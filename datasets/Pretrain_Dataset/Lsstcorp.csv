issuekey,title,description,storypoint
DLP-10,"Commissioning-ready Data Provenance System","Provenance System that can be connected with DRP and handle DRP-related provenance.",NULL
DLP-100,"Consistent multi-band deblending available","Per LDM-151 §5.3.2.2, it is a requirement that the deblender produce consistent results across multiple coadds and bands.    In this milestone we do not require a deblender which is capable of simultaneously processing multiple coadds. Rather, we require stack infrastructure such that consistent results may be achieved while deblending multiple bands in parallel using the single-epoch deblender. This requirement is satisfied by the [multi-band coadd processing workflow|https://confluence.lsstcorp.org/display/DM/S15+Multi-Band+Coadd+Processing+Prototype].",NULL
DLP-101,"Prototype detection on coadds available","Per LDM-151 §5.3.2.1, it is possible to detect objects on coadds. Multiple agent peaks are detected and merged ('culled') to reduce spurious detections due to noise or substructure.    Object detection is supported on the full range of coadds currently produced by the Image Coaddition Pipeline.    This milestone covers the state of object detection at the completion of the merger of the HSC codebase.",NULL
DLP-103,"Prototype multi-coadd peak association available","Per LDM-151 §5.3.2.1, it is possible to associated peaks and merge footprints detected on different coadds -- including in different bands -- to construct a unified set of footprints with associated peaks.    This is essential to the [multi-band coadd processing schema|https://confluence.lsstcorp.org/display/DM/S15+Multi-Band+Coadd+Processing+Prototype].",NULL
DLP-105,"Crowded field deblender available","Following the approach described in LDM-151 §5.3.2.2, the deblender has been extended to work in crowded fields by applying appropriate priors to the templates.",NULL
DLP-106,"Crowded field deblender","Deblending functionality available when PSFs substantially overlap over the field.    PSF determination gets harder in crowded fields, so we'll need to find a way to bootstrap that.    We'll need some way to divide-and-conquer extremely large blends (this will be necessary sometimes even in non-crowded fields to keep memory use bounded).  * Try to identify ""isthmuses""?  * Something based on sparse matrix diagonalization algorithms?",NULL
DLP-107,"Complete object detection on coadds","Develop a fully-functional detection system based on the HSC prototype.  ",NULL
DLP-108,"Final version of object detection on coadds available","Per LDM-151 §5.3.2.1, object detection is performed on a range of different types of coadd. Per this milestone, the production-quality detection system is in place in terms of selecting the coadds to be used, the thresholds adopted, binning strategy (NB LDM-151 requires recursive binning to detect extended objects), etc.",NULL
DLP-109,"Complete multi-coadd peak association","Building upon the work in DLP-104, develop the final peak & footprint association routines.    Note that we assume that detailed understanding of the coadds used in detection is required here, since the peak association requirements may vary with e.g. the binning strategy, and hence being blocked by DLP-108.",NULL
DLP-11,"Commissioning-ready AP/L1 Database","Commissioning-ready database that be capable of supporting Alert Production and L1 user analysis. Robust, including recovery from failures. Details of AP/L1 database are discussed in §3.1 of [LDM-135|http://ls.st/LDM-135*].",NULL
DLP-110,"Final version of multi-coadd peak association available","The complete multi-coadd peak & footprint association and merger functionality is now available. ",NULL
DLP-111,"Final version of multi-coadd deblender available","The complete multi-coadd deblender is now available as described in LDM-151 §5.3.2.2.",NULL
DLP-112,"Complete multi-coadd deblender","Develop the complete multi-coadd deblending functionality.  ",NULL
DLP-113,"Prototype sky coverage mask available","Per LDM-151 §5.3.2.3, we provide a sky coverage mask describing completeness and selection effects in the survey. Detailed requirements are expected to evolve based on experience with DES.",NULL
DLP-114,"Prototype sky coverage mask","* Many science cases need to make maps of different kinds of completeness and selection effects.  Frequently these will be used to create random catalogs that can be used in clustering algorithms.  * External code in this area includes Stomp, Mangle, Venice.  * We intend to learn from the experience of DES & HSC before making a decision.  ",NULL
DLP-115,"Final version of sky coverage mask available","The final version of the sky coverage mask functionality is available to the specification defined in LDM-151 §5.3.2.3 (when available).",NULL
DLP-116,"Complete sky coverage mask","Build on the work in DLP-114 to deliver the complete sky coverage mask functionality.  ",NULL
DLP-117,"Complete Object Detection & Deblending Pipeline Available","WBS 02C.04.05 is complete.",NULL
DLP-12,"Level 2 Database","Database catalog is needed to handle Level 2 Catalog data products described in §5.3 of [LSE-163|http://ls.st/LSE-163*]. This epic involves designing and building such catalogs. The work includes   * fine tuning baseline schema documented in LDM-153   * internal catalogs supporting Data Release Production   * building and releasing L2 catalog. Details of data loading are covered in [LDM-135 §8.15.2|http://ldm-135.readthedocs.org/en/master/#data-loading]",NULL
DLP-120,"Determine galaxy shear fitting parameters","We can estimate the performance of the final multifit galaxy modeling if we can measure a few numbers on simulations (those numbers are essentially factors that are multiplied to determine the computation time).    One number - the number of Monte Carlo samples - is extremely hard to put hard requirements on, because it requires essentially a full system and a huge amount of simulations to be measured.  But I think we can order-of-magnitude estimate it (just from intuition, really) at 20-200 samples.    This effort measures the other two sets of unknown numbers - the number of pixels typically included in the fit, and the order and number of shapelet terms in the PSF, using simulations and a placeholder galaxy fitting algorithm.    We also need to investigate other convolution approaches besides shapelets - depending on the speed vs. quality relationship for shapelets, those could become competitive.    Perry, a little Jim or successor,",NULL
DLP-13,"DRP-ready Internal DRP Database","Internal DRP catalog ready for supporting the DRP pipeline.",NULL
DLP-14,"Level 2 Qserv-based Catalog","Level 2 catalog produced by S18 DRP released and ready for querying through Qserv.",NULL
DLP-15,"Level 3 Database","Level 3 User Database. Related documentation: [LDM-135 §8.11|http://ldm-135.readthedocs.org/en/master/#level-3-user-tables-external-data]. Note that Level 3 has impact on the overall design of Qserv. In particular   * shared scans: appropriate Level 3 user tables will be scanned as part of each shared scan as needed to answer any in-flight user queries.   * data distribution: unlike L2 data sets, L3 are not immutable. Users can create/update/delete their L3 tables ad hoc at any time.",NULL
DLP-151,"Develop prototype PSF-matched image coaddition","Build coadds after matching input PSFs to a common predefined output PSF.  We need to figure out when we do this relative to warping in order to minimize noise covariance terms on the coadd.  ",NULL
DLP-152,"Initial PSF-matched image coaddition available","LDM-151 §5.2.1 requires that it must be possible to generate PSF-matched coadds. More detail on the requirements, included that the work should be performed using ""well known algorithms"", is provided in §5.2.2.1.    This initial version of PSF matched coaddition is functional and has a realistic data flow, but no requirements as to algorithic fidelity or computational performance.  ",NULL
DLP-153,"Develop prototype likelihood coaddition","Likelihood coadds are formally the optimal way to detect sources in multi-epoch data, but the output isn't a traditional coadd, so our traditional algorithms won't work on them.    We need to:  * Implement building likelihood coadds and detection on them.  * Test detection on likelihood coadds and compare to traditional detection methods  * Explore options for how to do downstream processing beyond detection    ",NULL
DLP-154,"Simplified likelihood coaddition available","LIkelihood coaddition is required by LDM-151 §5.2.1 and described in LDM-151 §5.2.2.1.    In this simplified version PSF is assumed to be constant and noise is assumed to be constant and uncorrelated.  Code must be fast enough to be used in production (no more than 3x slower than direct coaddition).    This basic implementation is appropriate for use in the detection and deblending pipeline (02C.04.05), but is not adequate for object characterization.  ",NULL
DLP-155,"Initial deep detection coadds available","Deep detection images are generated by combining coadds from multiple bands. In doing so, we use either the chi^2 scheme or a SED weighting scheme. In this milestone, we provide initial, functional versions of both, but with no specification as to their computational performance or algorithmic fidelity.    This work is required by LDM-151 §5.2.1 and described further in §5.2.2.1.",NULL
DLP-156,"Develop prototype chi^2 and filter-matched coaddition","Explore options for multi-band detection that go beyond simply merging independent detections in different bands.    Options include chi^2 coadds and SED-weighted combinations of per-band coadds.  ",NULL
DLP-157,"Develop prototype background-matched coaddition","Background matching theoretically gets us free background models on N-1 of N images in a patch of sky.  In practice it's been much harder to implement on non-drift-scan cameras than it was on SDSS, and we're not sure what the data flow or parallelization will look like.  ",NULL
DLP-158,"Initial background matched coaddition available","LDM-151 §5.2.1. requires that background matching must be implemented in the coaddition pipeline. It is further discussed in §5.2.2.3.    This initial prototype produces a functional version with a realistic data flow, but no requirements as to algorithic fidelity or computational performance.",NULL
DLP-159,"Complete background matched coaddition","Build on the work in DLP-157 to deliver a complete background-matched coaddition algorithm.  ",NULL
DLP-16,"Calibration Database","Related docs: [LSE-180: Level 2 Photometric Calibration for the LSST Survey|http://ls.st/LSE-180*]",NULL
DLP-160,"Refactored background matched coaddition available","Background matched coaddition is implemented as required and described in LDM-151 (§5.2.1 and §5.2.2.3 respectively). It follows the generic coaddition interface conventions as described in DLP-169.",NULL
DLP-161,"Initial DCR-corrected coaddition","We really have no idea how we're going to handle differential chromatic refraction in coadds (it's large enough we won't be able to ignore it; we can probably ignore other chromatic effects in coadds since we won't be using coadds for our highest-precision measurements).  We probably want to apply an SED-dependent pixel-level correction to input images, but to determine the SED at each point in the sky we'll need to at least know the SED of objects below the single-epoch detection limit, which requires coadds...",NULL
DLP-162,"Initial DCR-corrected coaddition available","Coaddition includes a prototype correction for differential chromatic refraction (and optionally other chromatic effects in the PSF and/or WCS).    At this stage, there is no quality requirement on the correction; we merely require the necessary bookkeeping to be in place with a reasonable algorithm.  This should allow subsequent work on DCR-corrected algorithms for detection and image subtraction with only incremental modifications to the DCR-corrected coaddition code (at least initially).    The baseline design described in LDM-151 does not make a specific requirement for DCR-corrected coaddition. However, other work suggests it is desirable and may be required both for deep detection and for differencing (e.g. _Report on Summer 2014 Production: Analysis of DCR_). See also the discussion in LDM-151 §5.2.2.7.",NULL
DLP-163,"Develop complete DCR-corrected coaddition","Build on the work started in DLP-161 to deliver the complete DCR corrected coaddition algorithm.    This work may involve only iterative improvement or a completely new algorithm.  ",NULL
DLP-164,"Refactored DCR corrected coaddition available","DCR correction on coadds is sufficient to ensure color-independent detection of both deep objects and transients.    This milestone is not currently required by LDM-151. However, we anticipate it being an emergent requirement of detection in deep coadds and/or difference images. It builds on the research provided for in DLP-162.    See also LDM-151 §5.2.2.7.",NULL
DLP-165,"Complete Image Coaddition Pipeline available","WBS 02C.04.04 is complete.",NULL
DLP-166,"Coaddition of undersampled images","As with PSF models, undersampled images break many of our algorithmic assumptions.    We don't currently know of any algorithms that are both computationally feasible and good enough from a science standpoint, so there's new work to be done here.    It's unlikely we'll find anything close to an optimal approach that's fast enough (very smart people have tried and only come up with very slow algorithms), so we probably want to focus on approximations whose effects we understand.  ",NULL
DLP-167,"Coaddition of undersampled images available","Slightly undersampled images can now be warped and coadded without aliasing.  Some loss of image quality is acceptable.    This is required by LDM-151 §5.2.1 and described by LDM-151 §5.2.2.2.",NULL
DLP-168,"Redefine and refactor coaddition pipeline interface","At this stage in development, we should have a good idea of the algorithms; here, we define the data flow and parallelization strategy we need to implement them.  ",NULL
DLP-169,"Coaddition pipeline interface available","This milestone indicates that all basic modes of operation of the coaddition pipeline have been prototyped. At this point, the final data flow required is determined, and can serve as an input both to other WBSs and to revision of the coaddition algorithms.",NULL
DLP-170,"Image artifact identification","Both background matching and image differencing could provide ways of identifying image artifacts (ghosts, satellite trails, etc).  We need do to evaluate which of these (maybe both) we want to use.  ",NULL
DLP-171,"Image artifact identification available","This represents a complete approach for dealing with image artifacts (define here as ""anything that shouldn't be included in a coadd"").    This work is required to meet the specifications laid down in LDM-151 §5.2.1 and is described further in LDM-151 §5.2.2.6.    Likely algorithmic approaches involve masking objects detected in difference image, background matching, or both.",NULL
DLP-198,"Complete PSF Estimation Pipeline available","This concludes 02C.04.03.",NULL
DLP-2,"Commissioning-ready Data Access Web Services","Data Access web services meta-epic",NULL
DLP-20,"Butler Framework","Butler framework that includes butler and mapper. Related document: [§3.2 in LDM-152|http://ldm-152.readthedocs.org/en/master/#baseline-design]",NULL
DLP-200,"Firefly refactoring","Firefly has been in development in more than five years, with the code bas in development for more than 15 years. The refactoring efforts in 2015 and 2016 will focus on two aspects: stream line the Java code and improve the quality and readability; and convert some components to JavaScript based code to take advantage of the JavaScript improvement in browser environment.",NULL
DLP-201,"First  version of LSST web UI ","First version of web UI implementation ready for LSST user community to access. It will have the base catalog and image query functions, using SQL to constraint the query. ",NULL
DLP-202,"Finish the GWT to JavaScript conversion for client side components","By end of S16, we would like to finish all the must-do conversion of GWT based client side components to be written in JavaScript, using React framework. This is in line with current web development technology trend. We want to position Firefly to take advantage of the improvement of JavaScript in browser, and development in the industry.",NULL
DLP-203,"Beta version of LSST web UI ready","By end of summer 2016,  the beta version of LSST web UI will be ready for LSST team to access. It will have the basic catalog query and image query capabilities.",NULL
DLP-204,"LSST web UI interoperates with workspace","LSST web UI will be able to inter operates with workspace, saving search results in the workspace.",NULL
DLP-205,"LSST web UI search external catalog","Web UI will be able to search the external catalogs, e.g. give user access to search VO registry to search the catalogs supporting TAP. ",NULL
DLP-206,"Web UI browse and search meta data ","capability to browse and search meta data, schema browser",NULL
DLP-207,"Extra LSST query capabilities in web UI","improvements to query interface, extra capabilities to specify the query of images and catalogs",NULL
DLP-208,"Second version of web UI ready","second version of LSST web UI ready for LSST user community",NULL
DLP-209,"Add one special UI view for specific science research interest","To accommodate the different needs of different science search interests, we want to introduce the special science view in web UI. To provide some special capabilities for one specific science research interest.",NULL
DLP-21,"Working Butler Framework","Working Butler framework that meets S16 needs of the apps team.",NULL
DLP-210,"Provide JS API for building specific science UI views ","We will provide several different specific UI views for different science research interests. But we can't provide all the specific views all the scientists needs. Providing some APIs will make it possible for different science working groups to build their own special UI views. ",NULL
DLP-211,"Improvement of current search capabilities","Improve on all the existing search capabilities, new features identified from testing users",NULL
DLP-212,"Improvements of all the web UI capabilities ","implement new features, bug fixes  identified in test. Improve usability, user experience. ",NULL
DLP-213,"Improvements on the JS APIs to build special UI views","Adding new JS API and new control parameters of APIs for developers  to build the customized UI.",NULL
DLP-214,"Accessing to L3 data","Provide users the capabilities to access the L3 data, their own and other collaborators. We need to pay attention to the ownership and it is closely tied to authentication and authorization.",NULL
DLP-215,"Correlate L3 data with parent data","Provide ways to associate L3 data to the LSST data that are used to generate it. For example, the L1 images that are used to make the L3 images, the sources that are used to calculate the values in L3 data source.",NULL
DLP-216,"Provide basic APIs to use Firefly visualization components","Provide basic APIs in http, JavaScript, Python to other developers to use Firefly visualization capabilities. The current clients are DM data products team for pipeline QA, LSST camera team for camera data visualization, SQUARE for SDQA tool.     Enable the interaction between tabular data (like catalogs) and images.",NULL
DLP-217,"Provide APIs for tabular data display using Firefly","Allow developers to use APIs to do column sorting, filtering",NULL
DLP-218,"Provide API for image display in Firefly","API will enable developers to display and manipulate images: zoom, stretch, color change, overlay regions, calculate distance between two points.   API will also provide mechanism for other developers to write their own callback functions to do other tasks. ",NULL
DLP-219,"Improvements of the Firefly APIs ","improvements and add more functions as our clients need. ",NULL
DLP-22,"Next-to-database User Analysis","Provide support for non-sql, ""next-to-database"" analysis. Related documentation: [§8.14 in LDM-135|http://ldm-135.readthedocs.org/en/master/#next-to-database-processing]",NULL
DLP-220,"Be able to access image cutout","Be able to access the image cutout through the DAX API. ",NULL
DLP-221,"Single focal plane image visualization in web UI and  through API ","Provide mechanism to display a single focal-plane image visualizaiton",NULL
DLP-222,"Provide time-domain data view","provide light curve plotting in Firefly visualization",NULL
DLP-223,"Provide image coaddition capability","provide web UI and API to enable coaddition of images",NULL
DLP-224,"provide mosaiking capability","provide web UI and APIs to enable user to mosaik images",NULL
DLP-225,"Basic 3D visualization","Provide basic 3D visualization of data",NULL
DLP-226,"Survey coverage display","Provide the display of survey coverage, maybe in different resolutions as needed",NULL
DLP-227,"Periodograms display capability","Be able to generate the periodogram from time series data and visualize it.",NULL
DLP-228,"moving object visualizaiton","visualization for moving objects: track, tracklets, proper motion ",NULL
DLP-229,"Image subtraction and blinking capability","Use the DM stack to do image subtraction using Firefly server extension capability, display the result properly in UI.  Blinking between two images in UI",NULL
DLP-23,"User-ready Next-Database Analysis System (Beta)","Next-to-database system ready for use by ""friendly"" users.",NULL
DLP-230,"Large area multi-resolution image display ","Resolve the issue ti display images over 10GB, e.g whole sky.  Looking into pre-generate the multi-resolution images to prepare for this. ",NULL
DLP-231,"Advanced light curve visualization","More options will be provided to users as needed to do the light curve visualization. ",NULL
DLP-232,"Advanced 3D visualizaiton","More options to control the data visualization in 3D.",NULL
DLP-233,"Light curve fitter","fitting light curves with a 'known' light curve shape - e.g., exoplanet transit, supernova explosion and decay, micro-lensing events, variable stars ...",NULL
DLP-234,"automated template fitter","automate the process of fitting light curves with a 'known' light curve shape - e.g., exoplanet transit, supernova explosion and decay, micro-lensing events, variable stars ...",NULL
DLP-235,"cross-matching visualizaiton","provide the capability to visualize the result of cross-matching catalogs, emphasizing the parameters that the algorithm used to do the cross-match. ",NULL
DLP-236,"Multi-variable correlation plots","Identify the correlations among the variables and provide the plotting capability to visualize them.",NULL
DLP-237,"all-sky movie","Display the all-sky image in a movie, to make the transition smoothly. ",NULL
DLP-238,"interactive celestial sphere","This is the 3D all sky display and interaction with users. ",NULL
DLP-239,"4D visualization","3D display with time as the 4th axis.",NULL
DLP-24,"Commissioning-ready LSST Database","LSST Database capable of supporting LSST Commissioning.",NULL
DLP-240,"Understand and define the requirements for Alert/notification toolkit","Have a clear definition of this tool",NULL
DLP-241,"Alert notification toolkit functions design","Design the functions of this tool, define relationship of this tool and alert brokers.",NULL
DLP-242,"Beta version of Alert subscription tool","Beta UI ready for users to subscribe to the alerts broadcasted by LSST mini broker.  Users can specify the filters to apply.",NULL
DLP-243,"Predefined alert filters","provide a set of predefined alert filters for users to choose from for subscribe to alerts",NULL
DLP-244,"provide mechanism for users to specify their own filters","web UI to enable users to specify their own filters when subscribe to the alerts",NULL
DLP-245,"Provide object association for a specified alert","capability to search and display the object associated with an alert, specified by alert ID or other criteria.",NULL
DLP-246,"Capability to access to alert history with filtering capability","UI to let users search on alert history by specifying alert ID or other criteria.",NULL
DLP-247,"First version of the alert subscription system ready","improve all the  basic functions listed in the beta version.",NULL
DLP-248,"Research the community discussion forum for alert system ","Doing research  in FY20 for the suitable candidates to be used for LSST alert discussion forum, working closely with community that are interested in alerts. Make a final recommendation to be adopted. ",NULL
DLP-249,"Beta API for alert subscription system","Define and provide the beta implementation of APIs to subscribe to the alert system",NULL
DLP-25,"Prototype Web Service for SUI","Implement a prototype Web Service, for use by the IPAC team for SUI related tests. Includes fully RESTful APIs, error handling, image stitching and basic metadata operations.",NULL
DLP-250,"second version of alert subscription system ","improvements on the basic functions, add more functions as needed ",NULL
DLP-251,"Enable the user community discussion forum for alert","Study what forums are available for community discussion. Make the recommendation and deploy it for community.",NULL
DLP-252,"API to access the alert history","Provide API to enable users to search the alert history, like given an alert ID, get  information related to it. ",NULL
DLP-253,"Alert annotation capabilities","Enable users to annotate,  persist, and view the annotations on alert.  Enable users to access annotation history on alert.  Note: The scope of the annotation is limited. ",NULL
DLP-254,"evaluate the system for user support","Evaluate helpdesk, knowledge database, ticketing systems, and online documentation system . Make a recommendation. ",NULL
DLP-255,"Deploy the system for user support","Deploy the help desk, knowledge database, ticketing systems, and online documentation system . ",NULL
DLP-256,"First draft definition of workspace","produce the first draft document to define the scope of workspace.   ",NULL
DLP-257,"Workspace functions design","Design workspace functions:   relationship with authentication and authorization system;  interoperability with SUI/tools;   interoperability with L3 data products;    VOspace protocol support?",NULL
DLP-258,"Beta version of workspace ","implementation of basic operation of workspace:  data object upload, download, manipulation, deletion  interoperability with SUI/tools",NULL
DLP-259,"First version of workspace implementation","improve all the functions listed in the beta version  new functions: interoperability with L3 data products;   provide APIs to enable scriptable operations",NULL
DLP-26,"Commissioning-ready Data Access Web Services","Data Access Web Services capable of supporting LSST Commissioning.",NULL
DLP-260,"second version of workspace implementaion","Improve all the functions in last version.  improve the interoperability among SUI/tools, workspace, and L3 data products.  improve the scriptable operations.     New functions if needed.  ",NULL
DLP-261,"Third version of workspace implementaiton","Improve all the functions in last version.  improve the interoperability among SUI/tools, workspace, and L3 data products.  improve the scriptable operations.  New functions if needed.",NULL
DLP-262,"ISR including all expected effects","This milestone will require identifying the expected effects.",NULL
DLP-263,"Full focalplane background modeling","This is needed by coaddition.",NULL
DLP-264,"Framework for representing astrometric pixel distortions","The current mechanism is to do a two fold process where smooth perturbations caused by the atmosphere and optics are represented as a single polynomial solution over the focalplane or chip.  The higher frequency distortions caused by sensor effects are modeled in a different, unrelated way.    Astrometry and all measurements need a way to represent these different distortions in a single self-consistent framework.  This leads to the notion that the general concept of a ""WCS"" needs to be redefined to be more general.",NULL
DLP-265,"Pixel-level intensity dependent PSF correction","This is important for photometry, but it is equally important for diffim since the zeropoint will vary between visits.  Templates will have to be built to a standard (zero-flux?) PSF, so this becomes vital to do at the pixel level.",NULL
DLP-266,"ISR operating at production level","ISR operating within required envelope for execution time and data quality with as-delivered sensors.",NULL
DLP-267,"Single frame measurement performing at production quality","Single frame measurements within requirements with as delivered sensors.",NULL
DLP-268,"Association of DIA sources with DIA and SS Objects","Query DIA object tables with DIA source positions and get associated records.  Query SSO tables for all objects with orbits consistent with DIA source positions.    This depends on DB interfaces.  This depends on Night MOPS efforts.",NULL
DLP-269,"DIA association operating at production level","This will depend on DB performance.    KPM -- Speed of full density query.",NULL
DLP-27,"Commissioning-ready Qserv","Qserv capable of supporting Commissioning. Details of Qserv are covered in LDM-135.",NULL
DLP-270,"Identify alert packet technology","Technologies exist for packaging and sending alerts, but current technologies have not been shown to scale to the LSST needs.  By W17 we need to have identified and rectified (using LSST effort perhaps) any shortcomings in either the event transport protocol or alert packaging standard.    KPM -- Packets per second on a reference implementation platform.",NULL
DLP-271,"Alert generation system at scale","We will be measuring DIA and SSO sources.  The information must be aggregated, packaged and published by a standard mechanism.  The publication system is after this milestone, but the two depend on each other for interfaces.    Depends on DIA source measurement suite and association.    KPM -- average and max alert packaging per sec.",NULL
DLP-272,"Simple alert filtering mechanism","The LSST will provide a mechanism for users to subscribe to the alert stream and place custom filters on that stream to then be forwarded on to other event processing systems.  This will be part of the LSST project ""broker.""  The broker will listen to the LSST project publisher.",NULL
DLP-273,"Alert publication system","The publication system will be the middleware between the packet generation and brokering systems.  The publication system will transmit the packaged alerts to multiple brokers for further dissemination.    KPM -- maximum and average alert throughput.",NULL
DLP-274,"Modernize difference imaging task","The current difference imaging task is a prototype and needs to be updated for modern interfaces.  It also will be made more modular.  This is in an effort to make the task easier to use in algorithm development and testing.",NULL
DLP-275,"Deliver document on effects of DCR","Differential chromatic refraction will be a major factor in how we end up doing template generation and difference imaging.  This will require defining the ways in which DCR impacts these systems.",NULL
DLP-276,"Dipole measurement suite","Dipole measurement (total flux, direction of dipole, etc.) will be useful in DIA measurement, but also in false positive detection and mitigation (aligned dipoles indicate astrometric failure).",NULL
DLP-277,"Difference image measurement suite","For tuning, development and testing we will need a full featured difference image measurement suite.  Algorithms will be very similar to single frame measurement except possibly in the case of dipoles and in the case of a pre-filtering mechanism for diffim.",NULL
DLP-278,"Measurement of DCR effects and mitigation","Define the strategies in order of priority to try to minimize the effects of DCR on measurement, coaddition, and diffim.",NULL
DLP-279,"Trailed source model","Objects that trail in a single image will have a trailed source model fit to them.  This will go into the SSO orbit determination.",NULL
DLP-280,"Recommend sparse field diffim algorithm","Of the various possible algorithms for doing difference imaging.  Identify the base line algorithm for use in sparse fields.",NULL
DLP-281,"Recommend template generation strategy","Depending on the suggested image differencing algorithm and the strategy for mitigating DCR effects, the template generation technique could vary widely (full sky templates to on-the-fly postage stamp templates).  The base line technique will significantly impact the required data taking strategy, particularly in commissioning and early years of the survey.",NULL
DLP-282,"Artifact pipeline","To beat down false positives, as many defects: scattered light, bleed trails, diffraction spikes, etc. will be detected and masked.  This will likely include a mixture of models, detection on single frame images, and detection of artifacts on image differences.",NULL
DLP-283,"Image differencing operating within requirements for sparse fields","Image difference pipeline working at production scale on as-delivered devices within requirements.  We are choosing to focus on sparse fields before crowded fields because the chromatic effects are more troublesome in crowded (confused) fields.  We will first get the algorithms working at high galactic latitude.    This depends on measurement suites, association, template generation, and image differencing algorithms.",NULL
DLP-284,"Recommend image differencing algorithm for crowded fields","The system must produce alerts over the entire sky.  We expect the challenges to be different depending on whether the images are crowded (confusion limited in the science image) or not.  This milestone is to define the baseline image differencing technique in crowded regions.",NULL
DLP-285,"Transient spuriousness engine","The goal is to remove as many false positives as possible at earlier stages of the pipeline, but we expect there to be some false positives that must be removed via a machine learning technique similar to the real-bogus system employed by some transient surveys.  The state of the art in machine learning will be used to deliver this system by this milestone.",NULL
DLP-286,"Deliver operational image differencing pipeline","Deliver the full image differencing pipeline working in all areas of the sky with base line algorithms for template generation, image differencing, DIA measurement and association and spuriousness determination.",NULL
DLP-287,"Pull relevant code from HSC to LSST","This is the UW's WBS, but there is similar work happening on the Princeton side.",NULL
DLP-288,"HSC merger complete","All relevant Hyper Suprime Cam code has been merged to LSST. HSC developers now primarily develop on the LSST codebase. Future HSC development that is not applicable to LSST will be carried out on a new fork, with frequent merges of LSST functionality.",NULL
DLP-289,"Merge HSC changes to LSST","All changes made in the (forked) Hyper Suprime Cam stack must be audited for applicability and, where appropriate, merged back to LSST.",NULL
DLP-290,"Residual PSF Ellipticity Correlations: TE1","Median residual PSF ellipticity correlations averaged over an arbitrary field of view for separations less than 1 arcmin shall be no greater than TE1.    LSR-REQ-0097.",NULL
DLP-292,"Scalable orbit prediction code","Orbit prediction code is needed by NightMOPS and the DIA association.  Code to do this exists in the domain, but has not been tested at LSST scale.  Potentially, licensing could also pose a problem.",NULL
DLP-293,"Fully functional NightMOPS","NightMOPS packages the orbit fitting code to be used in the pipelines.",NULL
DLP-294,"Scalable intra-night tracklet linking","This piece of the MOPS system is to identify all possible combinations of observations within a night that can be considered part of an orbit.",NULL
DLP-295,"Scalable orbit fitting code.","The MOPS system will feed tracks (multiple associated intra-night tracklets) to a fitting engine that will produce the most likely orbital elements for that track.  There is code in the domain to do this, but it has not been tested at LSST scale.",NULL
DLP-296,"Scalable collapse tracklets","The naive implementation of code to associate pairs of observations into tracklets will produce many redundant tracklets.  E.g. a triplet of observations of a single orbit will produce three distinct two observation tracklets.  This milestone will deliver code to collapse the many redundant tracklets into the optimal subset of tracklets.",NULL
DLP-297,"DayMOPS for non-trailed objects","All the pieces for DayMOPS will be in place by the beginning of FY19.  These will be hooked up over the course of FY19 to produce an operating DayMOPS pipeline for slow moving (non-trailed) sources.",NULL
DLP-298,"DayMOPS including trailed objects","Using information about the direction and length of trailed objects (even between snaps) will improve linking and orbit determination.",NULL
DLP-299,"Deliver recommended photometric self-calibration algorithm","The inputs to self-calibration need to be understood:  * What are the free parameters in the solution (i.e. what are the instrumental artifacts (if any) to be calibrated out in this step)?  * What are the inputs? Are they measurements from SFM or is it a richer set of inputs that include relative zeropoints as computed from coadd pipeline. ",NULL
DLP-300,"Photometric self-calibration API and minimal implementation","The API and prototype implementation will be up and running in the LSST execution framework in time for ComCam.",NULL
DLP-302,"API and minimal implementation for stack astrometry","This milestone delivers a solution the problem of relative astrometry of a stack of images to an external standard system or internal reference.    *This work is being done by IN2P3.",NULL
DLP-303,"Scalable stack astrometry","Ensure that the initial implementation of stack astrometry scales to LSST scales.",NULL
DLP-304,"Identify astrometric reference catalog","Some applications will require absolute astrometry.  For the external reference system, a suitable set of standards will be used.  Likely these will come from the GAIA mission.      Compare the standards against SRD-XXX to verify that the astrometric precision in the reference will not dominate the error budget for absolute astrometry.",NULL
DLP-305,"Astrometric stack calibration at scale","Deliver astrometric stack calibration at scale implemented in the LSST execution framework.",NULL
DLP-306,"Link tracklets","Nightly tracklets must be associated between nights.  These ""tracks"" are fed to the orbit fitting engine.",NULL
DLP-307,"Photometric Repeatability: procCalRep","The maximum allowed RSS contribution to the overall photometric repeatability of bright isolated point sources caused by errors introduced in the data processing pipelines.  OSS-REQ-0275 (LSE-30)",NULL
DLP-308,"Residual PSF Ellipticity Correlations: TE2","Median residual PSF ellipticity correlations averaged over an arbitrary field of view for separations between 1 and 5 arcmin shall be no greater than TE2.    LSR-REQ-0097.",NULL
DLP-309,"Absolute Astrometry: AA1","Median error in absolute position for each axis, RA & DEC, shall be less than AA1.    LSR-REQ-0094.",NULL
DLP-310,"Relative Astrometry: AM1","Median relative astrometric measurement error on 5 arcminute scales shall be less than AM1.    LSR-REQ-0094.",NULL
DLP-311,"Relative Astrometry: AM2","Median relative astrometric measurement error on 20 arcminute scales.    LSR-REQ-0094.",NULL
DLP-312,"Relative Astrometry: AM3","Median relative astrometric measurement error on 200 arcminute scales.    LSR-REQ-0094.",NULL
DLP-313,"Relative Astrometry: AB1","RMS difference between separations measured in the r-band and those measured in any other filter.    LSR-REQ-0094.",NULL
DLP-314,"DRP Computational Budget (DR1)","All DRP + AP pipelines.    LDM-138|Output|F4.",NULL
DLP-315,"Photometric repeatability: PA1gri","The rms of the unresolved source magnitude distribuition around the mean value.  This is for the g, r, and i bands.  LSR-REQ-0093",NULL
DLP-316,"Photometric repeatability: PA1uzy","The rms of the unresolved source magnitude distribution around the mean value.  This is for the u, z, and y bands.  LSR-REQ-0093",NULL
DLP-317,"Photometric Spatial Uniformity: PA3u","The distribution width (rms) of the internal photometric zero-point error in the u-band.  LSR-REQ-0093 from LPM-17",NULL
DLP-318,"Photometric Spatial Uniformity: PA3g","The distribution width (rms) of the internal photometric zero-point error in the g-band. LSR-REQ-0093 from LPM-17",NULL
DLP-319,"Photometric Spatial Uniformity: PA3y","The distribution width (rms) of the internal photometric zero-point error in the y-band. LSR-REQ-0093 from LPM-17",NULL
DLP-320,"Color Zero-point Accuracy: PA5","The accuracy of the absolute band-to-band zero-point transformations for main sequence stars.  This is for colors not using the u-band.  See LSE-17.",NULL
DLP-321,"Color Zero-point Accuracy: PA5u","The accuracy of the absolute band-to-band zero-point transformations for main sequence stars. This is for colors constructed using the u-band. See LSE-17.",NULL
DLP-322,"Absolute Photometry Accuracy: PA6","Accuracy of the transform between LSST photometric system to physical scale (e.g. AB magnitude scale).  See LSE-17.",NULL
DLP-323,"Moving Object Linkage Efficiency: orbitCompleteness","Minimum fraction of Solar System objects meeting reference criteria for which valid orbits shall be determined.  See LSE-30 (OSS-REQ-0160)",NULL
DLP-324,"Spuriousness Metric Efficiency: transCompletenessMin","Minimum completeness for transient science averaged over the survey.  See LSE-30 (OSS-REQ-0353)",NULL
DLP-325,"Spuriousness Metric Efficiency: transPurityMin","Minimum purity for transient science averaged over the survey. See LSE-30 (OSS-REQ-0353).",NULL
DLP-326,"Spuriousness Metric Efficiency: mopsCompletenessMin","Minimum completeness for Solar System object discovery averaged over one month of the survey.  See LSE-30 (OSS-REQ-0354).",NULL
DLP-327,"Spuriousness Metric Efficiency: mopsPurityMin","Minimum purity for Solar System object discovery averaged over a month of the survey.  See LSE-30 (OSS-REQ-0354).",NULL
DLP-328,"Computational Performance Metrics: OTT1","Maximum latency for release to the public of image and catalog data on likely optical transients.  See LPM-17.",NULL
DLP-329,"Computational Performance Metrics: AP computational budget","Processing power needed to maintain the alert production pipeline in steady state for typical observing conditions.  See LDM-138.",NULL
DLP-33,"Santiago - Boca Raton Early Integration Ready","The Santiago - Boca Raton path is configured and tested with simulated LSST image traffic and meets requirements.  Outage detection and diagnosis and catch-up after an outage are tested and meet requirements.  ",NULL
DLP-330,"Web UI portal able to support 100 concurrent users","The WEB UI needs to be responsive when there are many concurrent users accessing the data system at the same time.    This is achieved in cycles S17, S19, S10.",NULL
DLP-331,"Query response time from web UI","Web UI should give users timely and clear  indication about what is going on after a query is submitted. When the search results return from the data provider, it should be displayed for client within seconds.    The three epics are in S17, S19, S20.",NULL
DLP-332,"Image preparation time for web UI","The image needs to be prepared in the server side (convert to PNG or other image format) before it is displayed at the WEB UI. This is for the time it takes to have it ready for display.     This is achieved in cycles S17, S19, S20. ",NULL
DLP-333,"Image rendering time to display at web UI","When the image is transferred over to web, rendering time will depend on our software and of course the user's computer and browser. This is for the time it takes to show the image.     The KPM is achieved in S17, S19, S20.",NULL
DLP-34,"Santiago - Miami Early Integration Capability","The Santiago - Miami diverse path links are configured and tested with simulated LSST image traffic and meet requirements. Outage detection and diagnosis and catch-up after an outage are tested and meet requirements.  Fail-over on diverse paths are tested and meets requirements. ",NULL
DLP-35,"Santiago - Boca Raton Early Integration Capability","The Santiago - Boca Raton path is configured and tested with simulated LSST image traffic and meets requirements. Outage detection and diagnosis and catch-up after an outage are tested and meet requirements.",NULL
DLP-357,"Initial refinement of facility integration plan","Initial refinement of processing control component of facility integration plan",NULL
DLP-358,"Refinement of facility integration plan and traceability of science and operational requirements","Refinement of   processing control component of facility integration plan   and traceability of science and operational requirements",NULL
DLP-36,"Santiago - Miami Early Integration Ready","The Santiago - Miami diverse path links have been configured and tested with simulated LSST image traffic and meet requirements. Outage detection and diagnosis and catch-up after an outage have been tested and meet requirements.  Fail-over on diverse paths has been tested and meets requirements. ",NULL
DLP-37,"Summits-Gatehouse Fiber Ready","The fiber from the Summit to Gatehouse has been installed and tested.",NULL
DLP-38,"Mountain - Base Full System Integration Ready"," The end equipment for Full LSST System Integration has been installed, configured, and tested.",NULL
DLP-387,"Ongoing improvements to pipeline construction toolkit","TBD modifications/improvements.",NULL
DLP-39,"Mountain - Base Operations Ready","The Mountain - Base network has been integrated and tested with the full image data flowing to the replicators at the Base at operational cadence.",NULL
DLP-393,"Evaluate support of fault-recovery data sets in workflow systems","Checkpoint/restart services",NULL
DLP-40,"Mountain - Base Initial Capability Ready","The fiber and end transceivers will be installed for initial the use of the bandwidth",NULL
DLP-404,"Identify candidate security packages","Security and access control services",NULL
DLP-405,"Investigate CAS integration with Internet2 AAA tools","Security and access control services",NULL
DLP-406,"System to on-board community users","Security and access control",NULL
DLP-408,"Define initial basic dashboard services and web access","Dashboard and performance visualizations",NULL
DLP-409,"Implement dashboard with full capabilities","Dashboard and performance visualizations",NULL
DLP-41,"Mountain - Base Fiber Path Ready","Contracts between Telefonica and Reuna and Reuna and AURA are signed for the installation of the fiber from Gatehouse to C. Pachon",NULL
DLP-419,"Version 1 of TAP, ADQL, and VOEvent interfaces","VO interfaces",NULL
DLP-42,"Milestone 1 La Serena to Santiago Path Design Ready","The contracts for a dark fiber pair on the 3T fiber path between Reuna/Telefonica and Reuna/AURA have been signed. ",NULL
DLP-420,"Version 2 of TAP, ADQL, and VOEvent interfaces","VO interfaces",NULL
DLP-425,"Integrate 6 shelf 6 COB DAQ test stand (delivered by Camera Team) with integration cluster","Blocked by Camera team",NULL
DLP-426,"Deploy and verify DAQ test stand control software upgrade (delivered by Camera team)","Blocked by Camera team",NULL
DLP-427,"Integrate 14 Shelf 6 COB DAQ test stand (delivered by Camera team) with integration cluster","Blocked by Camera team",NULL
DLP-428,"Integrate OCS/TCS simulator (delivered by T&S team) with integration cluster","Blocked by T&S team",NULL
DLP-429,"Integrate OCS/TCS Software Release 3 (delivered by T&S team) with integration cluster","Blocked by T&S team",NULL
DLP-43,"Milestone 3 Fiber Delivery Plan Ready","REUNA/Telefonica provide plan to install fiber bundle from La Serena to Santiago on the 3T link, and secondary path via existing coastal fibers, including tests by Telefonica and Reuna/AURA",NULL
DLP-430,"Initial refinement of facility integration plan","Initial refinement of site infrastructure component of facility integration plan.",NULL
DLP-431,"Refinement of facility integration plan and traceability of science and operational requirements","Refinement of   site infrastructure component of facility integration plan   and traceability of science and operational requirements",NULL
DLP-432,"Initial prototype Archive Center system supporting L2 production","Archive Center Infrastructure",NULL
DLP-44,"Milestone 6 La Serena  to Santiago with test traffic flowing over LSST Lambda Ready","The end nodes have been obtained and installed and the 100Gbs Lambda configured for LSST from La Serena to Santiago",NULL
DLP-45,"Milestone 8 Delivery of ""functional Pre-operations Link"" (at least 4Gbps), Ready","The first phase limited bandwidth capability of the diverse path to Santiago has been configured by Reuna",NULL
DLP-46,"Milestone 10 Full Capacity over Diverse path Ready","The final phase full bandwidth for the diverse path from La Serena to Santiago has been configured by Reuna.",NULL
DLP-47,"Mountain - Base Fiber Path","Contracts between Telefonica and Reuna and Reuna and AURA are negotiated and signed for the installation of the fiber from Gatehouse to C. Pachon",NULL
DLP-470,"NCSA/CC-IN2P3 test",TBD,NULL
DLP-472,"Qserv Data Distribution","Working data distribution system capable of distributing (sharding) qserv-managed table chunks across a cluster of machines, and replicating them. Details are data distributions are discussed in §8.8 of [LDM-135|http://ls.st/LDM-135*].",NULL
DLP-473,"Async Queries in Qserv","Support asynchronous queries in Qserv.",NULL
DLP-474,"Qserv Monitoring","Tools for monitoring Qserv cluster, including statistics which shared scans are most popular.",NULL
DLP-48,"Mountain - Base Operations Capability","The Mountain - Base network will be integrated and tested with the full image data flowing to the replicators at the Base at operational cadence. ",NULL
DLP-480,"Finalized interfaces for multi-coadd peak association","Per LDM-151 §5.3.2.1 and DLP-103, the peak association procedure combines detections from multiple coadds to form a merged, unified catalog. As of this milestone, the pipeline flow and parallelization strategy which will be adopted for peak association is stable.",NULL
DLP-483,"Prototype multi-coadd deblender","As required in LDM-151 §5.3.2.2, the deblender is expected to ingest and consistently deblend across multiple coadds. This builds on the work in DLP-100 to incorporate this workflow into the deblender itself (rather than the surrounding infrastructure, as per the previous milestone). In the process, we define the pipeline data-flow and parallelization strategy which will be used for deblending. This milestone does not require a particular level of algorithmic fidelity or computational performance, and the deblender is not yet suitable for use in crowded fields.",NULL
DLP-484,"Divide-and-conquer routine for deblending","Per LDM-151 §5.3.2.2, an algorithm is available which will split up very large blends for deblending and measurement.",NULL
DLP-486,"Prototype merge & deduplication in tract & patch overlaps","Per LDM-151 5.3.2.1, we deduplicate objects which detected in the overlapping regions of tracts and patches.    This milestone represents the merger of a prototype implementation of this functionality from HSC.",NULL
DLP-487,"Complete merge & deduplication in tract & patch overlaps","TODO: This work may leverage the peak association routines developed in DLP-481 and/or DLP-109, thereby introducing a further dependency.",NULL
DLP-489,"Utility classes for chromaticity available","Requirements will be defined after the requirements-gathering step in the associated meta epic (DLP-490).",NULL
DLP-49,"Mountain - Base Fiber","The fiber from Mountain to Base installed and tested.  ",NULL
DLP-490,"Design & implement utility class for chromaticity","Will include a requirements gathering exercise to define success criteria for DLP-489.    Python/simple C++, astronomer, 2 months. ",NULL
DLP-491,"Reusable task interfaces stabilized","Mid-level {{Task}} (not limited to {{CmdLineTask}}) interfaces have been audited and refactored for consistency and ease-of-use.    This includes at least:   - Eliminate the use of DataRefs for input/output except where necessary, or provide alternative explicit signatures for all operations that don't involve DataRefs.   - Address code reuse issues where some options need to be provided as function arguments in some contexts and via Config in other contexts.   - Define and document interfaces (including required configuration fields) for ""retargetable"" subtask slots.    After these changes, the Task interfaces will not be frozen, but should remain stable for several major code releases except in certain documented cases that are slated for specific near-term rewrites.",NULL
DLP-492,"Refactor reusable subtask interfaces","Audit and refactor all {{Task}} classes we consider reusable components.  Some requirements are listed on DLP-491, but improvements should not be limited to that list.    Everybody contributes to auditing, implementation is pretty easy. ",NULL
DLP-494,"Design & implement full focal plane image characterization","This is a rewrite of {{CalibrateTask}} that goes beyond a simple refactoring; we need to provide support for components that need to be run over all CCDs in a visit as well as those that process individual CCDs individually, and we need to revisit the dependencies and ordering or components.    Astronomer, good understanding of algorithms, no in depth C++ but somebody with an eye for architecture.",NULL
DLP-495,"Define & implement top level pipeline tasks","Audit & improve HSC top-level drivers and integrate with existing LSST tasks. To include:  * Rewrite top-level pipeline tasks and Butler datasets.  * Remove ProcessCoaddTask.  * Ensure consistency between MPI drivers and individual CmdLineTasks.    Good overall pipeline understanding.",NULL
DLP-496,"Top level pipeline tasks defined and implemented","Top-level driver tasks and the associated pipeline flow and butler datasets audited and cleaned up.",NULL
DLP-498,"Provide HSC dataset for integration testing","Identify public available HSC data for initial integration tests, and, if needed, determine a temporary workaround if the amount of public HSC data is currently insufficient (either by running integration tests in Princeton, or by petitioning the HSC collaboration for limited temporary access to more data).  ",NULL
DLP-499,"Provide acceptance code for integration testing","Provide scripts to check the science quality of pipeline outputs for use in integration tests.    We assume that the initial test will be based on HSC data, with the requirements for science quality reflecting the current state of the HSC fork of the pipeline (which needs to be made explicit in the form of acceptance tests such as these).",NULL
DLP-5,"EFD Database","This epic covers work related to adding support for the EFD database for DRP.",NULL
DLP-50,"Mountain - Base Initial Capability","AURA will install, configure, and test end transceivers for the initial of the bandwidth.",NULL
DLP-500,"PSF KPMs at design level","Residual PSF ellipticity KPMs TE1 (DLP-290; LSR-REQ-0097) and TE2 (DLP-308; LSR-REQ-0097) are satisfied at the level required for FY20 (final design criteria). If possible, these measurements are made on production camera data.  ",NULL
DLP-501,"Test and improve PSFs on production camera data","Devote 1 person full time until we hit spec.    NB Production camera only available 15 Aug 2020.",NULL
DLP-502,"Prototype image artifact identification available","Per LDM-151 §5.2.1, outliers appearing in a subset of the input images should not be included when coadding. This milestone represents an initial version of that functionality. It will be ported from HSC, and is based on identifying discrete artifacts in differences between clipped and unclipped coadds.",NULL
DLP-503,"Prototype direct coaddition","Direct coaddition involves applying a relative astrometric solution (as produced by the Single Frame Pipeline, 02C.03.01) to input images, warping them to a common coordinate system, and coadding the pixels. This includes generating a combined PSF model based on the inputs.    This functionality is required by LDM-151 §5.2.1. It is described by LDM-151 §§5.2.2.1 & 5.2.2.5.    Per this milestone, it is possible to generate direct coadds but we impose no requirements as to computational performance or algorithmic fidelity.    In practice, this is a port of existing functionality from HSC, which is itself mostly just a refactoring of the existing LSST pipeline with relatively minor algorithmic improvements.",NULL
DLP-504,"Develop prototype DCR corrected coaddition","Develop algorithms to deal with DCR in coaddition, at least for the purposes of source detection, deblending, and image subtraction templates (final multifit-based measurement should address any concerns with DCR at that stage).  This will probably involve assigning some sort of per-pixel SEDs on the frames, but we'd like to make that choice reversible (which means detection cannot depend on the details of the assigned SED).  The algorithmic landscape is *very* uncertain.  ",NULL
DLP-505,"Final set of coaddition outputs for production defined (TODO: Add blockers from obj char, diffim, ...)","This is the canonical list of the coadd data products to be deployed as input to sizing model and pipeline flow baseline, including choices such as:   - cuts on seeing vs. depth   - cuts on observation date   - combinations of data from multiple bands   - type of coadd (likelihood vs. direct vs. PSF-matched)    We also need to decide here how long each coadd data product needs to be persisted; many will likely be used only for detection, then thrown away, while others may be needed for deblending, measurement, or image subtraction, or will be made available to users for download.",NULL
DLP-506,"Experimental determination of production coaddition outputs","Experiment with detection, deblending, and image subtraction algorithms to determine the suite of coadds we need to produce during a DRP.  ",NULL
DLP-51,"Mountain - Base Full System Integration Capability","The end equipment for Full System Integration is installed, configured, and tested.",NULL
DLP-511,"Refactored deep detection coadds available","Chi^2 and SED-matched multi-band coaddition is implemented as required and described LDM-151 (§§5.2.1 and 5.2.2.1 respectively). They follow the generic coaddition interface conventions as described in DLP-169.  ",NULL
DLP-513,"Refactored PSF-matched image coaddition available","PSF-matched coaddition is implemented as required and described LDM-151 (§§5.2.1 and 5.2.2.1 respectively). It follows the generic coaddition interface conventions as described in DLP-169.",NULL
DLP-514,"Develop complete likelihood coaddition","Per LDM-151 & Bosch on Measurement of Blended Objects in LSST, ultimate requirements for the production of ""advanced"" likelihood coadds will be determined in 02C.04.06 (Object Characterization Pipeline) based on the prototype developed here. This work will only be performed if required by Object Characterization.  ",NULL
DLP-515,"Final version of likelihood coaddition","Likelihood coadds that take into account PSF spatial variation and noise covariances as necessary to support final deep detection algorithms are available, with acceptable computational performance.    This may be achieved in one of three ways:   - Likelihood coaddition that accounts fully for the PSF and noise covariances is optimized to have acceptable computational performance.   - Experiments demonstrate that likelihood coadds that approximate or the PSF and/or noise covariances are sufficient for deep detection without loss in sensitivity.   - Experiments demonstrate that likelihood coadds are entirely unnecessary for deep detection.    Since a loss in coadd quality can generally be compensated by a lower detection threshold (with subsequent steps to remove spurious detections), this is essentially a determination of the tradeoff between dealing with spurious objects and computationally and algorithmically intensive coaddition.    Per LDM-151 & Bosch on Measurement of Blended Objects in LSST, ultimate requirements for the production of ""advanced"" likelihood coadds will be determined in 02C.04.06 (Object Characterization Pipeline) based on the prototype developed here. Further work on likelihood coaddition in this WBS (DLP-514, DLP-515) will only be performed if required by Object Characterization.",NULL
DLP-517,"Prototype for advanced likelihood coaddition available","Likelihood coaddition is available such that the full PSF model is used and noise covariance is at least approximately included.  No requirement on computational performance for this milestone.    Kernels for extended models should also be available.    Per LDM-151 & Bosch on _Measurement of Blended Objects in LSST_, ultimate requirements for the production of ""advanced"" likelihood coadds will be determined in 02C.04.06 (Object Characterization Pipeline) based on the prototype developed here. Further work on likelihood coaddition in this WBS (DLP-514, DLP-515) will only be performed if required by Object Characterization.",NULL
DLP-518,"Propose a workspace structure for discussion","Propose a workspace structure for discussion.    Xiuqin presented the SUI team discussion results at the DM AHM at SLAC. The slides and the discussion notes are here: https://confluence.lsstcorp.org/display/DM/Workspace+discussion",NULL
DLP-519,"Research the development framework for SUI","The research results is summarized here: https://confluence.lsstcorp.org/display/DM/Web+Technologies",NULL
DLP-52,"La Serena - Santiago Fiber Installed and tested on 3T","The fiber bundle from La Serena to Santiago on the 3T link is provisioned and tested by Telefonica and Reuna/AURA.",NULL
DLP-520,"Access  Qserv for catalog search in SUI prototype","Exercise the APIs provided by data access team to make sure the round-trip works. ",NULL
DLP-521,"Define the data access APIs with database team","The APIs are documented here: https://confluence.lsstcorp.org/display/DM/API",NULL
DLP-522,"SUI visualization prototype","Take IPAC Firefly package to prove that it satisfy the basic needs for visualization. ",NULL
DLP-526,"DMTC-0100-2000	Archive Center Integration Complete","Archive Center Integration Complete  ",NULL
DLP-528,"DMTC-0100-1350	Archive Site Ready for Equipment Configuration","This milestone represents the state that the Archive Center computer rooms at NCSA are available to host LSST equipment, and the contract vehicles for equipment procurement are in place.",NULL
DLP-529,"DMTC-0100-3200	Base - Archive Network Functional - 2 x 100 Gbps","This milestone represents the availability of both sides of the 100 Gbps ring connection Santiago and Miami as well as the 100 Gbps path connecting Santiago and Boca Raton.  Both the primary and secondary 40 Gbps La Serena - Santiago links will be available by this time.",NULL
DLP-53,"La Serena - Santiago with Traffic Flowing Over LSST Lambda ","The end nodes have been obtained and installed and the 100Gbs Lambda configured for LSST from La Serena to Santiago",NULL
DLP-530,"DMTC-0100-2600	Base - Archive Network Functional 1 Gbps","This is implemented as shared access on a 10 Gbps link provided by to AURA by REUNA and AmPath, LSST contributes partial funding for this link.",NULL
DLP-531,"DMTC-0100-3100	Base - Archive Network Functional 3 Gbps","This is a placeholder milestone to represent the ability to transfer data from the Base to the Archive as of the early integration phase. We have determined not to implement a 3 Gbps link, as the Santiago Miami 100 Gbps ring and the Santiago - Boca Raton 100 Gbps links will be available by this time.  The La Serena - Santiago primary link at 40 Gbps will be available by this time, but not the secondary link.",NULL
DLP-54,"La Serena - Santiago Diverse Path Limited Capability ","The first phase limited bandwidth of the diverse path to Santiago is configured by Reuna ",NULL
DLP-547,"DMTC-0100-3300	LSE-76 DM summit facility interface - Phase 3","[LSE-76|http://ls.st/lse-76*] is the ICD for the DM to Summit Facility interface.  This is an interface defining the physical requirements of DM at the Summit Facility.  It is an interface between DM and Telescope & Site.    At present this appears to be limited to the rack space, power, and cooling required for the Summit-Base network endpoint.  The rest of the Summit network is not formally a DM deliverable, though it is part of the integrated LSST network design described in [LSE-78|http://ls.st/lse-78*].    The completion of this milestone will reflect the conclusion of an agreement between DM and T&S on the details of the physical requirements, with these details added to LSE-76.",NULL
DLP-548,"DMTC-0100-4350	LSE-77 DM base facility interface - Phase 3","[LSE-77|http://ls.st/lse-77*] is the ICD for the DM to Base Facility interface.  This is an interface defining the physical requirements of DM at the Base Facility.  It is an interface between DM and Telescope & Site.    The interface primarily concerns itself with space, power, and cooling requirements for the DM equipment for the Base Center and the Chilean Data Access Center.    The completion of this milestone will reflect the conclusion of an agreement between DM and T&S on the details of the physical requirements, with these details either added to LSE-77 or reflected in other change-controlled documents.  In particular, the Base Facility Data Center Design Requirements document, [LSE-239|http://ls.st/lse-239*] reflects the roll-up of all subsystems' requirements for Base Facility accommodation.",NULL
DLP-549,"DMTC-0100-3800	LSE-78 all LSST network design - Phase 3","[LSE-78|http://ls.st/lse-78*] is the LSST Observatory Network Design document.  This effectively defines an interface among all the Observatory subsystems cooperating at the Summit and Base (as well as defining the DM data transport Base-Archive).    The completion of the Phase 3 milestone for LSE-78 will reflect the conclusion of an agreement between DM - the main implementing subsystem - with the other subsystems on the Observatory network plans to a sufficient level of detail to permit implementation to begin.  ",NULL
DLP-55,"La Serena - Santiago Diverse Path Final Capability ","The full bandwidth capability for diverse path from La Serena to Santiago is provisioned by Reuna",NULL
DLP-550,"DMTC-0100-0130	LSST Software Release  10.1 Complete: Ready for Science Verification","Release Highlights		Fully integrated DMS at DR1 scale; assembly of all data needed to begin operations	  Input Data		See commissioning plan	  Output Data		All L1 and L2 data products, sample L3 data products	  External Interfaces Available		All external interfaces live	  External Interfaces Provided		SUI v3 (?)	  Integration Tests Enabled			NA",NULL
DLP-551,"DMTC-0100-0110	LSST Software Release  8.1 Complete: Ready for Commissioning Camera","Release Highlights		Photometric calibration; Deblending and crowded field support; SUI; Production monitoring, control, and restart	  Input Data		DES images – full survey; Imsim delivery 4 (delivery 3 + additional year of time coverage, 2 filters)	  Output Data		R7.0 plus astrometric models in Object table	  External Interfaces Available		ComCam, full-capacity EFD simulator.	  External Interfaces Provided		Calibration Database, Camera lab testing archive assimilation, SUI v2 (?)""	  Integration Tests Enabled	OCS/CCS/DM full 24 hours, ComCam/DAQ at SLAC, ComCam/DAQ in Tucson	",NULL
DLP-552,"DMTC-0100-0120	LSST Software Release  9.1 Complete: Ready for Full Camera","Release Highlights		Integrated DMS at LSST-camera commissioning-support level and scale; assembly of all data needed to begin commissioning	  Input Data		See commissioning plan	  Output Data		All L1 and L2 data products	  External Interfaces Available		Full-store SDS testbed	  External Interfaces Provided			NA  Integration Tests Enabled			NA",NULL
DLP-554,"DMTC-0100-0080	LSST Software Release 4.0 Complete: R&D Code Base Refactored","Release Highlights		R&D code base mined for reusable components.  Stable interfaces for Middleware.  Lay foundation for debugging and visualization tools.  Input Data		NA  Output Data		NA  External Interfaces Available		NA  External Interfaces Provided		NA  Integration Tests Enabled		NA",NULL
DLP-555,"DMTC-0100-0082	LSST Software Release 5.0 Complete: Code Base Ready for Construction","Release Highlights		Image processing subset of apps framework; Single frame processing pipelines; Pipeline toolkit, simple orchestration and query services	  Input Data		DES images – first 2 yrs; ImSim delivery 1 (15x15 area, Opsim cadence for 1 yr); Images from LSST SDS test stand	  Output Data		Calibrated Science Images in Image Archive; Metadata and SDQA outputs in database	  External Interfaces Available		Camera Phase 3 interface definition, OCS Middleware v3, Initial OCS EFD implementation, DAQ software simulation  External Interfaces Provided			NA  Integration Tests Enabled		IOCS basic communication	",NULL
DLP-556,"DMTC-0100-0085	LSST Software Release 5.1 Complete: HSC Merge, UI Prototype, and Single Frame Processing","Release Highlights		Image processing subset of apps framework; Single frame processing pipelines; Pipeline toolkit, simple orchestration and query services	  Input Data		DES images – first 2 yrs; ImSim delivery 1 (15x15 area, Opsim cadence for 1 yr); Images from LSST SDS test stand	  Output Data		Calibrated Science Images in Image Archive; Metadata and SDQA outputs in database	  External Interfaces Available		Camera Phase 3 interface definition, OCS Middleware v3, Initial OCS EFD implementation, DAQ software simulation  External Interfaces Provided			NA  Integration Tests Enabled		IOCS basic communication	",NULL
DLP-557,"DMTC-0100-0090	LSST Software Release 6.1 Complete: Image Differencing and Coaddition","Release Highlights		Completion of apps framework; Initial releases of Image Differencing and Coaddition pipelines; Camera SDS interface; Initial Base DMCS for AP, query services user management	  Input Data		DES images – first 3 yrs; ImSim delivery 2 (delivery 1 plus full equatorial band, two over-the-pole stripes for 1 yr Opsim cadence); Photometric standards catalog	  Output Data		R5.0 plus Object, Source, DiaSource tables in database;  Transient Alerts	  External Interfaces Available		OCS Phase 3 interface definition, simulated images, partial-store Camera SDS testbed	  External Interfaces Provided		EFD query use cases, DM device simulators, Data Challenge results archive management, SUI v1 (?)	  Integration Tests Enabled	EFD native replication, DAQ catch-up interface",NULL
DLP-558,"DMTC-0100-0105	LSST Software Release 7.1 Complete: PSF Estimation, Deblending, Calibration","Release Highlights		PSF Estimation; Deblending; Operation with expected as-built image characteristics; Calibration telescope interface; Initial Archive DMCS for DRP and DAC DMCS	  Input Data		DES images – first 4 yrs; PS-1 detections for MOPS input; PS-1 images if public; Imsim delivery 3 (delivery 2 plus full sky, 1 filter, Opsim cadence for 1 yr)	  Output Data		R6.0 plus MovingObject table in database; calibrated photometry in Object and Source tables; galaxy shape model parameters in Object table	  External Interfaces Available		Calibration telescope, OCS command simulator, static EFD.	  External Interfaces Provided		DM quality telemetry, Full operational-level DM device simulators, L1 database implementation, Qserv analytics for Data Challenge results, Provenance storage and query  Integration Tests Enabled		EFD replication including blobs, OCS/CCS/DM start-of-night, visit, mini-nightContinuously-running Alert Production in Integration environment",NULL
DLP-559,"DMTC-0100-2700	Mountain - Base Network Functional 1 Gbps","This is a placeholder milestone to represent the ability to transfer data from the Mountain to the Base as of the early construction phase.  We have determined not to implement a full 1Gbps, as the current microwave links are sufficient for LSST early test purposes (we are leveraging information from DES data transfers in this time frame).  Instead, the first Mountain - Base fiber will be laid by Winter 16 (refer to DLP-37) and AURA will  light it with 10 Gbps (refer to DLP-40).",NULL
DLP-560,"DMTC-0100-2800	Mountain - Base Network Functional 2 Gbps","This is a placeholder milestone to represent the ability to transfer data from the Mountain to the Base as of the early construction phase. We have determined not to implement a full 2 Gbps, as the current microwave links are sufficient for LSST early test purposes (we are leveraging information from DES data transfers in this time frame). Instead, the first Mountain - Base fiber will be laid by Winter 16 (refer to DLP-37) and AURA will light it with 10 Gbps (refer to DLP-40).",NULL
DLP-561,"DMTC-0100-2900	Mountain - Base Network Functional 2 x 100 Gbps","This milestone represents the availability of both sides of the 100 Gbps link connecting the Mountain to the Base, including the fibers and the DWDM equipment.",NULL
DLP-564,"Able to use some data-access  APIs developed by database team","SUI team should be able to use some of the data access APIs provided by the database team to get catalog and image data. ",NULL
DLP-565,"Able to exercise all the data-access APIs developed by database team","SUI will continue to work with database team to exercise all the APIs for data access. All the issues should be worked out by the end of S16 cycle. ",NULL
DLP-566,"DMTC-0100-0090.0	LSST Software Release 6.0 Complete","Create, test, and release the Winter 2016 software release (v12.0).  ",NULL
DLP-567,"DMTC-0100-0105.0	LSST Software Release 7.0 Complete: ","Release Highlights		PSF Estimation; Deblending; Operation with expected as-built image characteristics; Calibration telescope interface; Initial Archive DMCS for DRP and DAC DMCS	  Input Data		DES images – first 4 yrs; PS-1 detections for MOPS input; PS-1 images if public; Imsim delivery 3 (delivery 2 plus full sky, 1 filter, Opsim cadence for 1 yr)	  Output Data		R6.0 plus MovingObject table in database; calibrated photometry in Object and Source tables; galaxy shape model parameters in Object table	  External Interfaces Available		Calibration telescope, OCS command simulator, static EFD.	  External Interfaces Provided		DM quality telemetry, Full operational-level DM device simulators, L1 database implementation, Qserv analytics for Data Challenge results, Provenance storage and query  Integration Tests Enabled		EFD replication including blobs, OCS/CCS/DM start-of-night, visit, mini-nightContinuously-running Alert Production in Integration environment",NULL
DLP-568,"DMTC-0100-0110.0	LSST Software Release  8.0 Complete: ","Release Highlights		Photometric calibration; Deblending and crowded field support; SUI; Production monitoring, control, and restart	  Input Data		DES images – full survey; Imsim delivery 4 (delivery 3 + additional year of time coverage, 2 filters)	  Output Data		R7.0 plus astrometric models in Object table	  External Interfaces Available		ComCam, full-capacity EFD simulator.	  External Interfaces Provided		Calibration Database, Camera lab testing archive assimilation, SUI v2 (?)""	  Integration Tests Enabled	OCS/CCS/DM full 24 hours, ComCam/DAQ at SLAC, ComCam/DAQ in Tucson	",NULL
DLP-569,"DMTC-0100-0120.0	LSST Software Release  9.0 Complete: ","Release Highlights		Integrated DMS at LSST-camera commissioning-support level and scale; assembly of all data needed to begin commissioning	  Input Data		See commissioning plan	  Output Data		All L1 and L2 data products	  External Interfaces Available		Full-store SDS testbed	  External Interfaces Provided			NA  Integration Tests Enabled			NA",NULL
DLP-57,"Mountain - Base Early Integration Ready","The end equipment has been installed and all specifications for equipment and fibers have been tested to requirements for Early integration of LSST system",NULL
DLP-570,"DMTC-0100-0130.0	LSST Software Release  10.0 Complete: ","Release Highlights		Fully integrated DMS at DR1 scale; assembly of all data needed to begin operations	  Input Data		See commissioning plan	  Output Data		All L1 and L2 data products, sample L3 data products	  External Interfaces Available		All external interfaces live	  External Interfaces Provided		SUI v3 (?)	  Integration Tests Enabled			NA",NULL
DLP-571,"DMTC-0100-29xx	Mountain - Base Network and Base Center Integration Complete","This milestone represents the demonstration that data flowing over the links connecting the Mountain to the Base and out to the Base buffer and replicators is functional per specifications.",NULL
DLP-572,"DMTC-0100-32xx	Base - Archive Network and Base and Archive Centers Integration complete","This milestone represents the availability and operation per specifications of all paths connecting the Base and Archive Centers.",NULL
DLP-573,"DMTC-0100-23xx	HQ Network Integration Complete","This milestone represents the availability of the fiber optic network connecting the HQ site in Tucson with the Base and Archive Centers.",NULL
DLP-574,"Evaluate best-of-breed tools.  Technology exploration.","Evaluate best-of-breed tools.  Technology exploration.",NULL
DLP-575,"Evaluate best-of-breed tools.  Technology exploration.","Evaluate best-of-breed tools.  Technology exploration.",NULL
DLP-576,"Evaluate best-of-breed tools.  Technology exploration.","Evaluate best-of-breed tools.  Technology exploration.",NULL
DLP-577,"Evaluate best-of-breed tools.  Technology exploration.","Evaluate best-of-breed tools.  Technology exploration.",NULL
DLP-578,"Evaluate best-of-breed tools.  Technology exploration.","Evaluate best-of-breed tools.  Technology exploration.",NULL
DLP-579,"Usability and developer efficiency","Internal and external developers can process precursor datasets (HSC, DECam, CFHT-LS, LSST lab data), generating and analyzing data products at small scale, with comparable difficulty to other toolkits.  Documentation exists that external developers can use to add new algorithms.  Problem reports can be turned into automated tests.",NULL
DLP-58,"Mountain - Base Early Integration Capability","The Mountain - Base end equipment for Early Integration has been installed, configured, and tested.",NULL
DLP-580,"End-to-end DRP 1: Calibrated data to analysis, minimal level","Start with pre-calibrated data from a precursor survey.  Run through all the DRP steps, at least at a placeholder level (generate data products of the right form but with no guarantees on content).  Load results into (limited-functionality) data access services.  Analyze results using (limited-functionality) SUI tools.",NULL
DLP-581,"End-to-end DRP 2: Raw data to analysis, scientific level","Start with raw data from a precursor survey. Run through all the DRP steps.  Generate data products with scientific performance comparable to the current state of the art. Load results into data access services. Analyze results using SUI tools.    ",NULL
DLP-582,"End-to-end DRP 3: Raw data to analysis, ComCam level","Start with raw data from ComCam (or a simulation thereof) in an amount expected from ComCam operations. Run through all the DRP steps, generating results at a scientific performance level suitable for validating LSST system performance. Load results into data access services able to handle ComCam operational loads. Analyze results using ComCam-capable SUI tools.",NULL
DLP-583,"End-to-end DRP 4: Raw data to analysis, survey level","Start with raw data from the LSST Camera (or a simulation thereof) in an amount expected for DR2. Run through all the DRP steps, generating results at SRD performance levels. Load results into data access services able to handle full operational loads. Analyze results using full SUI tools.",NULL
DLP-584,"End-to-end AP 1: Camera simulation to alerts, minimal level","Accept simulated OCS and CCS events.  Obtain pixel data from simulated Camera Data System.  Execute AP processing at least at a placeholder level (results of correct form).  Generate data for alerts but publish only to Level 1 Database.  Load results into Level 1 Database.",NULL
DLP-585,"End-to-end AP 2: Camera Test Stand to alerts, scientific level","Accept simulated OCS and CCS events. Obtain pixel data (at least at ComCam sizes/rates) from Camera Data System test stand. Execute AP processing at scientific performance comparable to the current state of the art. Generate data for alerts and publish to minimal broker.  Perform daily smoke tests.  Perform daily raw calibration image processing.  Load results into data access services. Analyze results using SUI tools.",NULL
DLP-586,"End-to-end AP 3: Camera Test Stand to alerts, ComCam level","Accept simulated OCS and CCS events. Obtain pixel data (at least at ComCam sizes/rates) from Camera Data System test stand. Execute AP processing at a scientific performance level suitable for validating LSST system performance. Generate data for alerts and publish to minimal broker. Perform daily smoke tests. Perform daily raw calibration image processing.  Load results into data access services able to handle ComCam operational loads. Analyze results using ComCam-capable SUI tools.",NULL
DLP-587,"End-to-end AP 4: Camera Test Stand to analysis, survey level","Accept simulated OCS and CCS events. Obtain pixel data at full-frame sizes/rates from Camera Data System test stand. Execute AP processing at SRD level. Generate data for alerts and publish to minimal broker. Perform daily smoke tests. Perform daily raw calibration image processing. Load results into data access services able to handle full operational loads. Analyze results using full SUI tools.",NULL
DLP-59,"Long-Haul Network Bandwidth","The Key Performance Metrics are the line speed bandwidth in gigabits per second (Gbps) over a long haul network link.  It can be measured by perfsonar-instrumented data transfer tests over sustained periods, using pre-cursor and LSST simulated data. Similarly it could be measured by the DWDM end node equipment to produce error rates seen by the equipment in the Forward Error Correction algorithm.",NULL
DLP-594,"Implement corrections for all expected effects for single frame ISR","Make sure correction for all expected ISR effects are implemented: bias, dark, flats, fringe, etc.  This does not include other more complicated corrections like those due to brighter-fatter effect.",NULL
DLP-595,"Implement system capable of representing all pixel distortions","There are all kinds of pixel distortions.  Traditional WCS objects assume smooth transformation from one system to another.  In general, this is not true since some pixel distortions can change quite quickly.    The solution will probably include some combination of low order transforms for going from focalplane to sky coordinates and higher order transforms that can go from a regular pixel grid to the distorted system.",NULL
DLP-596,"Implement full focalplane background modeling","The background for a single visit is driven by a similar atmosphere for the whole focalplane.  Therefore, we should be able to model the background more accurately by taking information from all chips at once to constrain the model.",NULL
DLP-597,"Implement pixel-level intensity dependent PSF correction","This will be important for very high fidelity difference images as the brighter-fatter effect can cause artifacts if the PSF intensity for stars in the template are different than the intensity for those in the science image.  The PSF matching will take some of this out, but it's likely that a pixel level correction will be necessary.",NULL
DLP-598,"Assemble all pieces and verify operations ready ISR","I think the distinction between DLP-266 and DLP-262 should be made obvious.    DLP-262 is really the standard ISR and this should include those corrections as well as other more subtle effects (brighter-fatter, treerings, etc.)",NULL
DLP-599,"Implement single frame processing at production level","Combine all correction and measurement tasks into a single pipeline for working with single science frames (two combined snaps for LSST).  ",NULL
DLP-6,"Data Provenance","System for capturing provenance for all LSST data products.",NULL
DLP-60,"La Serena - Santiago End Node Installation  and Test","Install the DWDM End nodes and amplifiers for the La Serena to Santiago Dark Fiber link",NULL
DLP-600,"Define APIs and implement initial algorithms","For coadds and multifit, we will need to register stacks of images to a common astrometric system (either an external system or to an internal reference system).",NULL
DLP-601,"Refine/redesign algorithm for stack astrometry to be scalable","Optimize initial algorithm to be scalable to LSST system.",NULL
DLP-602,"Assemble external reference catalog","We will want to tie astrometry to an absolute system in some cases.  Assemble and test a set of reasonable astrometric reference catalogs for LSST to use.",NULL
DLP-603,"Assemble pieces of stack registration into production pipeline","Bring together middleware, algorithm and reference catalogs into a single runnable pipeline.",NULL
DLP-604,"Review alert packet technology for applicability to LSST","There is currently (2015) a standard for VOEvents.  The current specification does not fulfill all  the requirements of the LSST: attaching images.    By 2017 one of three things will have to happen:  1. LSST changes requirements to fit standard.  2. The community adds to the standard sufficiently that LSST can use it unmodified.  3. LSST will have to take on the activity of extending the VOEvent standard to meet LSST needs.",NULL
DLP-605,"Implement a system that can package events at scale.","We have estimates of how many events we will have per visit.  These events must be packaged in 5 sec per visit.",NULL
DLP-606,"Implement machinery for publishing packaged events.","Once the alerts are packaged, they will need to pass through a publishing layer to be sent to the brokers.  This will likely either be a batch publication process or an addition to the packaging system so that alerts can be published in parallel depending on how the standards and protocols evolve.",NULL
DLP-607,"Implement simple broker","LSST has agreed to provide some amount of filtering on LSST alerts.  This will be one endpoint of the alert publication system.",NULL
DLP-608,"Refactor image differencing tasks","The image differencing tasks have not been kept up to date with recent changes.  This will refactor, reorganize, and modernize the implementation of the tasks without changing the algorithms substantially.    This will make further development and improvement more straightforward.",NULL
DLP-609,"Research effects of DCR in the context of LSST","Differential chromatic refraction (DCR) in the atmosphere will strongly impact how image differencing is carried out with LSST.  It seems likely that the template generation will be most heavily affected.  LSST will not have the luxury of waiting until late in the survey to build templates.  Thus, high quality templates must be built with very little data.  Doing this in the presence of very different observing conditions, airmass and parallactic angle, will be a major challenge.",NULL
DLP-61,"Milestone 5 La Serena - Santiago link Ready","The lInk will be now ready to start transferring test traffic",NULL
DLP-610,"Implement dipole measurement suite","Most measurement will be straightforward using the standard measurements, but difference images will potentially have dipoles which will cary extra information.  The measurement suite will need to measure the total flux, flux per lobe, separation of lobes, and direction of lobes at the very least.",NULL
DLP-611,"Research possible mitigation for DCR","DLP-275 has identified several potential mitigation techniques.  This project will produce a document that suggests the most effective one and produce measurements of residual DCR effects.",NULL
DLP-612,"Implement measurements suite for image differences","The full measurement suite will need to operate on all potential types of image differences.  This could include things like preconvolution differences.  A similar problem exists in measurement on likelihood coadds, so these efforts could inform each other.",NULL
DLP-613,"Research template generation","Using previous research on DCR and state of the art image differencing techniques, quantify the pros and cons of available template generation strategies in the presence of realistic observation conditions and cadences.",NULL
DLP-614,"Research and implement diffim for sparse fields","Using existing datasets and state of the art algorithms, find and implement the optimal image differencing algorithm for sparse, high galactic latitude fields.",NULL
DLP-615,"Implement trailed source model","Implement model to measure flux, direction, and speed of moving sources in image differences.  This is to include sources that move fast enough to be separated in combined snaps.",NULL
DLP-616,"Implement artifact pipeline","Implement the pipeline to identify and mask predictable artifacts.  This will include diffraction spikes, ghosts from bright stars, and potentially glints from reflected light.",NULL
DLP-617,"Research and implement diffim in crowded fields","Using existing datasets and state of the art algorithms, find and implement the optimal image differencing algorithm for dense, low galactic latitude fields.  This will involve woking with the delivered template generation algorithms.",NULL
DLP-618,"Implement sparse field diffim tasks","Chain together the relevant pieces to create a complete image differencing pipeline that can operate within requirements for sparse fields.",NULL
DLP-619,"Implement transient spuriousness engine","Use the state of the art machine learning techniques to implement an engine that will reject spurious diffim sources at the level in the requirements.",NULL
DLP-62,"Procurement and Purchase DWDM Equipment. Chile National","Acquire WAN networking equipment to establish 100Gbps optical circuits between Mountain to Base and La Serena to Santiago",NULL
DLP-620,"Implement operational image differencing pipeline","Implement the full image differencing pipeline including false positive elimination, in both sparse and dense fields.",NULL
DLP-621,"Implement scalable orbit prediction code","Ephemeris generation is used in DIA association as well as in the MOPS systems.  There are codes that do this in the domain, but they may not fill the need for LSST.  Part of this effort will be to determine the scalability, accuracy, and maintainability of current orbit prediction codes.",NULL
DLP-622,"Implement NightMOPS","NightMOPS identifies observations of previously known solar system objects.  Implement a task that will take the orbit prediction code, SSO database, and DIA sources and produce a list of objects associated with previously known SSOs.",NULL
DLP-623,"Implement a scalable version of tracklet linking","Observations taken within a night of moving objects will be linked into ""tracklets.""  These will later be fed into a system that will link single night tracklets into longer ""tracks.""  This system must be scalable to LSST volumes with predicted false positive rates.",NULL
DLP-624,"Implement task to collapse tracklets","Tracklets will generally be pairs of observations.  If more than two observations of a moving object are taken, the combinatorix dictate that there will be many more tracklets produced than actual objects.  This task is to reduce the tracklets to the minimum set of distinct tracklets.",NULL
DLP-625,"Implement task to link tracklets into tracks","Multiple observations are linked together into tracks.  The intra-night tracklets are linked into inter-night tracks.  These tracks are later fed to the orbit determination engine.",NULL
DLP-626,"Research and implement a scalable orbit fitting task","There is code in the community to fit orbits to a set of observations.  The current solutions need to be evaluated for scalability, accuracy and maintainability.  If there is no suitable solution in the domain, LSST must implement one.  This interface will take sets of tracks and return best fit orbits with uncertainties.",NULL
DLP-627,"Implement functioning DayMOPS for slow moving objects","DayMOPS is the system that takes observations and determines best fit orbits and identifies potential new solar system objects.  By using pre-covery (looking up previous observations and predicted locations) potential solar system objects can be verified.  This system will only take into account information derived from point source measurements.",NULL
DLP-628,"DayMOPS including trailed objects","Using the information form fast moving (trailed) objects can greatly improve the accuracy of the derived orbit.  Implement a system that will use trailed source model fits to enhance the orbit fitting where appropriate.",NULL
DLP-629,"Research photometric self-calibration techniques","This is most likely simply large matrix inversion to solve a large set of linear equations.  This should consider other techniques as well as research the performance implications of various implementation details.",NULL
DLP-63,"Milestone 16 Vendor selection Ready","Acquire WAN networking equipment from elected vendor to establish optical 100Gbps circuits between Mountain to Base and La Serena to Santiago",NULL
DLP-630,"Define API and produce minimal implementation for global photometric calibration","Implement the recommendation from DLP-299.",NULL
DLP-631,"Optimize photometric global self-calibration","Optimize photometric calibration such that it performs at scale within the LSST requirements on nominal hardwware.",NULL
DLP-632,"Implement association queries","Query database for DIA source associations and SS Object associations.",NULL
DLP-633,"Write queries for association","Write queries to associate DIA sources with DIA and SS objects.  Depends on query interface.",NULL
DLP-634,"Implement DIA association at production level","Implement production level DIA and SS object association.",NULL
DLP-635,"Qserv Query Cancellation","Improve Qserv code such that it can handle queries that are cancelled (for example, user presses ctrl-C in mysql client). Qserv should continue running under load with no resource leaks when queries are randomly killed.",NULL
DLP-636,"Performance on the Cerro Pachon to La Serena fiber link","A key  performance metric  measured in bit error rate or lossy packets. Ideally this should be minimum 1e-12 but seek 1e-15. It can be measured by perfsonar or the same DWDM.",NULL
DLP-637,"Refactor AFW R&D Code","In many places R&D code is still in place.  Bring instances of R&D code up to LSST production levels: add unit tests where missing, correct to coding standards, adhere to current pipeline conventions.",NULL
DLP-64,"Mountain - Base DWDM End Node Installation and Test","Installation of the LSST DWDM and nodes. Configure to 2 x 100Gbs Lambdas and test",NULL
DLP-640,"Re-design camera geometry classes","Current camera geometry classes make many assumptions about the camera that make it hard to use (sensors laid out in rectilinear grid).  It is also largely in C++.  Refactoring to Python will help other projects adapt it to their needs.",NULL
DLP-641,"Support the Camera Team","The camera team need to use the LSST DM stack to do analysis.  Specifically, they need code to turn the test stand data into images that can be passed through measurement and other pipelines.",NULL
DLP-643,"Generalize single frame astrometry","There is a strong dependency on astrometry.net for fitting astrometry on single frames.  This is a fine algorithm, but the framework must be more flexible to allow optimization and tuning down the road.",NULL
DLP-646,"Bandwidth on summit to base link","A key performance metric measured in bits per second to ensure we achieve 100Gs on the summit to base link",NULL
DLP-647,"Bandwidth measurement on La Serena to Santiago link","Measurement of the bandwidth on the La Serena to Santiago link",NULL
DLP-648,"Measurement of Bit Error Rate on La Serena to Santiago link","Measurement of packet loss or bit error rate which ideally should  be on the order of 1e-15 but 1e-12 is acceptable. ",NULL
DLP-649,"Shared Scans","Working implementation of shared scans capable of supporting both single-table scan, and synchronized multi-table scans. Demonstration involves:  # run simultaneously 5 full-table-scan queries on Object table and demonstrate they all complete in approx the time it takes to do a single pass through the table.  # run simultaneously 5 full-table-join queries (Object joined with Source) and demonstrate they all complete in approx the time it takes to do a single pass through the Source table plus the Object table.    Scale: 10% of DR1.    Details of shared scans can be found in §8.10 of [LDM-135|http://ls.st/LDM-135*].",NULL
DLP-65,"Mountain - Base LSST DWM End Node Installation and Test Ready","Install the LSST End Nodes at summit and base. Configure the nodes for 2 x 100Gbps Lambdas",NULL
DLP-650,"Qserv KPI Analysis","Required user query access performance characteristics are given in [LDM-135 §4.3|http://ldm-135.readthedocs.org/en/master/#query-access-related-requirements], and LDM-141. This epic capture work related to analyzing Qserv performance to make sure we are meeting the requirements.    # # Measure 99th percentile response time for the following query types:  ## low volume type A  ## low volume type B  ## low volume type C  ## low volume type D  ## high volume Object Scans  ## high volume ObjectExtra Scans  ## high volume Object/Source Scans  ## high volume Object/ForcedSource Scans  # Measure maximum query rate for the above query types    when run simultaneously with query mix on data set as shown in the [^queryMix.xlsx]    Measuring response time involves running specified mix of queries and showing Qserv can deliver designed query rate for low volume type A queries, even in presence of failures:   * worker failures in FY17   * master failures in FY18   * worker failures that involve loosing all replicas of some chunks in a given DAC in FY19    Measuring max query rate involves increasing query rate by scaling up each query type proportionally and determining maximum query throughput.",NULL
DLP-651,"Qserv Low Volume Type A Query Response Time","Measure 99th percentile response time for low volume type A queries when run simultaneously with query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-652,"Qserv Low Volume Type B Query Response Time","Measure 99th percentile response time for low volume type B queries when run simultaneously with query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-653,"Qserv Low Volume Type C Query Response Time","Measure 99th percentile response time for low volume type C queries when run simultaneously with query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-654,"Qserv Low Volume Type D Query Response Time","Measure 99th percentile response time for low volume type D queries when run simultaneously with query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-655,"Qserv High Volume Object Scan Response Time","Measure 99th percentile response time for Object Scan queries when run simultaneously with query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-656,"Qserv High Volume ObjectExtra Scan Response Time","Measure 99th percentile response time for high volume ObjectExtra scans when run simultaneously with query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-657,"Qserv Low Volume Object/Source Joins Response Time","Measure 99th percentile response time for low volume type B queries when run simultaneously with query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-658,"Qserv High Volume Object/ForcedSource Join Response Time","Measure 99th percentile response time for high volume Object/ForcedSource joins when run simultaneously with query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-659,"Qserv Query Rate","Measure maximum query rate for the query mix on data set as shown in the [queryMix.xlsx|https://jira.lsstcorp.org/secure/attachment/27075/27075_queryMix.xlsx]",NULL
DLP-66,"Mountain - Base Capacity Upgrade to 100Gbs for AURA","AURA acquires the first of the DWDM equipment and installs and utilizes for Traffic as the Backbone",NULL
DLP-665,"Final version of detection coadds available","Based on the detailed system requirements (DLP-505) and existing pipeline code, the final algorithms used for generating coadds used for detection are now operational at a level that enables us to hit SRD specification within the available computational budget.",NULL
DLP-666,"Final version of object characterization coadds available","Based on the detailed system requirements (DLP-505) and existing pipeline code, the final algorithms used for generating coadds used by the Object Characterization Pipeline are now operational at a level that enables us to hit SRD specification within the available computational budget.",NULL
DLP-668,"Refine algorithms for production of object characterization coadds","Identify the areas in which our existing implementations of the algorithms specified in DLP-505 are inadequate to enable us to reach the SRD specifications. Fix them.  ",NULL
DLP-669,"Refine algorithms for production of detection coadds","Identify the areas in which our existing implementations of the algorithms specified in DLP-505 are inadequate to enable us to reach the SRD specifications. Fix them.  ",NULL
DLP-67,"Mountain -Base Capacity upgrade Ready","The AURA backbone now utilizing 100Gbs Lambda for the backbone",NULL
DLP-670,"Physically motivated atmospheric PSF model","In addition to interpolating PSFs from bright stars, we incorporate a model of the atmosphere and the telescope optics to our PSF estimation as per LDM-151 §5.1.2.    This goes beyond the baseline design, but is a likely algorithmic enhancement depending on feedback from science collaborations and may be required to hit science requirements (depending on the results of DLP-88).  ",NULL
DLP-671,"Develop physically motivated atmospheric PSF model","Build a PSF model with physically motivated parameters (instead of just fitting and interpolating star images).    Requires expertise we do not currently have. Experts at DESC. If we can make it easy for them to contribute, that will help a lot. To do it ourselves, 6 months total effort.",NULL
DLP-674,"Narrow-band QE of all pixels available","Per LDM-151 §6.1.2.1, it is now possible to calculate the QE of all pixels at a 1 nm resolution.    Based on data from PhoSim.",NULL
DLP-678,"Atmospheric transmission at arbitrary focal plane position","Per LDM-151 §6.1.2.2, and using the fitted atmospheric model derived in DLP-679, it is possible to estimate the atmospheric transmission corresponding to any point on the LSST focal plane.",NULL
DLP-679,"Fit of atmospheric model to data from calibration telescope","Per LDM-151 §6.1.2.2, we can now fit an atmospheric model (DLP-680) to the spectrophotometric measurements from the calibration telescope (DLP-681) and other instrumentation (radiometer, GPS satellites, etc).",NULL
DLP-68,"Boca Raton - Chicago Early Integration","Provision the path from Boca Raton to NCSA",NULL
DLP-680,"Atmospheric model defined","LDM-151 §6.1.2.2 describes two possibilities for an atmospheric model:    * A version of MODTRAN;  * A PCA-like decomposition of the data.    As of this milestone, the model to be used in production has been defined.",NULL
DLP-681,"Spectrophotometric measurement pipeline for calibration telescope","Per LDM-151 §6.1.2.2, a pipeline is now available to reduce data from the calibration telescope and provide a wavelength- and intensity-calibrated 1-D spectrum.",NULL
DLP-682,"Optical ghost & glint catalog infrastructure","Per LDM-151 §6.1.2.4, the Calibration Products Pipeline now provides a means of maintaining a catalogue of optical ghosts and glints. Compiling the list of ghosts and glints itself is out of scope.    Developed using HSC data.",NULL
DLP-683,"On-telescope measurement of cross-talk matrix","Per LDM-151 §6.1.2.3, the Calibration Products Pipeline now provides tools for deriving the cross-talk matrix given in-dome measurements, likely from the Collimated Beam Projector, and delivering it to the SFM system (or elsewhere, as required).    Can be developed using data from DECam.    ",NULL
DLP-684,"Fringe frames available","Per LDM-151 §6.1.2.3, the Calibration Products Pipeline now provides tools for deriving fringe frames given white-light flats and measurements from the Collimated Beam Projector.    The requirements for fringe frames are expected to evolve during construction, and it may be that this milestone is made obsolete (but RHL thinks it unlikely).    This is not a high priority issue and can be handled late in construction. But: it is a nice self contained project, could be prototyped with HSC data, and might make a good ramp-up for a new algorithms person.",NULL
DLP-685,"Separate QE and pixel size variation models","Per LDM-151 §6.1.2.3, the QE and pixel size variations are separately modelled as a function of focal plane position (and possible time).",NULL
DLP-686,"Correct for pixel size variation on object scale","Per Per LDM-151 §6.1.2.3, an algorithm is available for correcting for pixel size variation on the scale of individual objects, e.g. by redistributing charge.    Developed with HSC/DECam data.",NULL
DLP-687,"Brighter-Fatter correction available","Per LDM-151 §6.1.2.3, an algorithm is available which compensates as far as is possible for the Brighter-Fatter effect on CCDs.    Developed using data from HSC/DECam.",NULL
DLP-688,"Models of residual Brighter-Fatter artifacts available","Per LDM-151 §6.1.2.3, any residual artifacts from the Brighter-Fatter effect after correction per DLP-687 are modelled and can be taken account of in future processing.    Developed using data from HSC/DECam.",NULL
DLP-689,"Algorithm for handling edge/bloom effects available","Per LDM-151 §6.1.2.3, an algorithm is available for handling edge/bloom effects on CCDs using charge redistribution and/or masking.    Developed and tested using data from DECam/HSC.",NULL
DLP-69,"Boca Raton - Chicago Early Integration Ready","Path provisioned from Boca Raton to NCSA",NULL
DLP-690,"Defect tracking","Per LDM-151 §6.1.2.3, an algorithm is available for recording and tracking defects on CCDs. We assume that this list of defects is provided to us (ie, we do not detect the defects), and remains constant throughout operations.    Developed using data from HSC and/or DECam.",NULL
DLP-691,"Charge Transfer (in)Efficiency compensation available","Per LDM-151 §6.1.2.3, an algorithm is available for handling charge transfer efficiency effects, including interpolation over bleed trails.    Prototyped using data from HSC.",NULL
DLP-692,"Instrumental sensitivity calculation on ComCam","The work to calculate instrumental sensitivity to date (DLP-677, DLP-676, DLP-675, DLP-674) have all been based on simulated data using PhoSim (or, in some cases, data from other telescopes). As of this milestone, it has been tested and integrated with the LSST system (assuming ComCam availability in W20).",NULL
DLP-694,"Prototype Brighter-Fatter correction available","Per LDM-151 §6.1.2.3, an algorithm is available for mitigating the Brighter-Fatter effect on CCDs. This prototype algorithm reflects research work done on HSC, and may not represent an optimal solution for LSST.",NULL
DLP-695,"Filter response measurement available","Per LDM-151 §6.1.2.1, code is available to support determining the measurement of filter response at 1 nm resolution based on in-dome measurements.    Development is possible using data from HSC.",NULL
DLP-696,"Single epoch Sersic model fitting","Per LDM-151 §5.4.1, the Object Characterization Pipeline shall be capable of fitting a hybrid galaxy-moving point source object to an object. Rather than addressing this complex model directly, we baseline an approach whereby we build up from a series of simpler model fits.    This milestone is the first step in that process: we are now able to fit a Sérsic profile to a single epoch.",NULL
DLP-697,"Single epoch CModel minimization","We continue to work towards a hybrid source model fit per LDM-151 §§5.4.1 & 5.4.2.7. See also §5.4.2.4.    It is now possible to fit a ""CModel"" source model: a linear combination of a de Vaucouleurs (Sérsic {{n=4}}) and an exponential (Sérsic {{n=1}}) component.",NULL
DLP-698,"Improved reliability single epoch CModel minimization","The CModel fitting described by DLP-697 & currently available in the LSST codebase is functional but has a number of known failure modes. Per this milestone, the CModel fit has undergone extensive testing for reliability and robustness and all known failures have been eliminated.",NULL
DLP-699,"Prototype framework for multi-epoch fitting","Per LDM-151 §§5.4.1 and 5.4.2.2, we use MultiFit to characterize objects by fitting over multiple epochs.    As of this milestone, a basic framework for experimenting with the MultiFit system is available. It provides a convenient vehicle for algorithmic exploration and experimentation, but it is not expected to survive in this form to production. ",NULL
DLP-7,"Alert Production and L1 User Database","Database catalog is needed to handle Level 1 Catalog data products described in §4.3 of [LSE-163|http://ls.st/LSE-163*]. This epic involves designing and building such catalogs. The work includes schema structure, partitioning, replication and fail over. Detailed design is outlined in §3.1 of [LDM-135|http://ldm-135.readthedocs.org/en/master/#alert-production-and-up-to-date-catalog].",NULL
DLP-70,"Miami - Chicago Early Integration Ready","Provisioning the circuit between Miami and NCSA",NULL
DLP-700,"Galaxy shear fitting parameters available","Per LDM-151 §5.4.1, we will fit a hybrid galaxy-point source model to sources for level 2 object characterization.    Achieving the SRD requirements with respect to the measurement of galaxy shear will require detailed understanding of the procedure for fitting. In particular, this milestone will establish the requirements for:    * The number of pixels typically included in the fit, and  * The order and number of shapelet terms in the PSF.    This milestone will also consider other, non-shapelet, approaches to convolution, benchmarking them on the same speed vs. quality metrics as shapelets.    This work will be carried out using simulations and a placeholder galaxy fitting algorithm.",NULL
DLP-701,"Advanced galaxy models available","The CModel galaxy models used in DLP-697 and DLP-698 are not adequate to meet the requirements specified in the SRD. Instead, we baseline the development of an advanced galaxy model as specified in LDM-151 §5.4.2.7.",NULL
DLP-702,"Moving point source model minimization","Per LDM-151 §§5.4.1 and 5.4.2.3, it will be possible to fit a moving point source model to a multi-epoch dataset using the multi-epoch fitting framework (DLP-699)",NULL
DLP-703,"Multi-epoch galaxy model minimization","We combine the CModel galaxy model (DLP-697) with the MultiFit framework (DLP-699), providing a prototype version of multi-epoch galaxy model fitting (LDM-151 §§5.4.1, 5.4.2.4, on the route to §5.4.2.7).",NULL
DLP-704,"Hybrid galaxy/moving point source model minimization","Per LDM-151 §§5.4.1, 5.4.2.2 and 5.4.2.7 we provide a hybrid galaxy/moving point source model which is usable within the MultiFit framework.",NULL
DLP-705,"Simultaneous galaxy model minimization","Per LDM-151 §5.4.1, it shall be possible to fit multiple objects simultaneously with galaxy models.",NULL
DLP-706,"Improved minimization techniques","We have reivisited the minimization techniques in use and optimize for speed and accuracy when used with our newly developed advanced galaxy models (DLP-701).",NULL
DLP-707,"Single-epoch galaxy model Monte Carlo sampling","Per LDM-151 §5.4.1, it must be possible to fit objects using Monte Carlo sampling. Here, we provide the first version of this capability, fitting our advanced galaxy model (DLP-701) to data in a single epoch.",NULL
DLP-708,"Simultaneous, hybrid model MultiFit","Per LDM-151 §§5.4.1, 5.4.2.2 and 5.4.2.7, it is now possible to perform numerical minimization and Monte Carlo sampling of multiple hybrid galaxy/moving point source models simultaneously in the MultiFit framework.",NULL
DLP-71,"Miami - Chicago Early Integration","Provisioned thee circuit between Miami and NCSA",NULL
DLP-711,"Production coadd galaxy color measurement","Per LDM-151 §5.4.2.14, experiments have been run to determine whether production galaxy color measurements will be based on direct or PSF-matched coadds.",NULL
DLP-712,"DCR in multi-epoch fitting","Per LDM-151 §§5.4.1 and 5.4.2.1, MultiFit based fitting now takes account of differential chromatic refraction.",NULL
DLP-713,"Improved framework for multi-epoch fitting [TODO: dependency on process middleware]","In DLP-699, we defined a placeholder framework to enable experimentation with MultiFit-type algorithms. These algorithms are now complete. In order to deploy them in production we need to work with the Process Middleware group to define a suitable framework for deployment in production.",NULL
DLP-714,"Baseline multi-epoch fitting system complete","The baseline MultiFit-based multi-epoch object characterization system can now be used with the trailed source model developed in DLP-279 (02C.03.04). The system described in LDM-151 §§5.4.1 and 5.4.2.2 is now complete.",NULL
DLP-715,"Fundamental centroid and shape measurements","Per LDM-151 §§5.4.2.8 and 5.4.2.9 centroids are measured for objects detected on coadds using an algorithm derived from SDSS, while adaptive moments are based on the algorithm described by Bernstein & Jarvis.",NULL
DLP-716,"Fundamental flux measurements","The fundamental measurements of flux specified by LDM-151 §5.4.1 can now be measured on coadds. Specifically, this includes:    * Aperture flux;  * Gaussian flux;  * PSF flux;  * Kron flux.    This milestone does not include Petrosian flux (DLP-717).",NULL
DLP-717,"Petrosian flux measurement","As specified by LDM-151 §5.4.1, Petrosian fluxes can now be measured on coadds.",NULL
DLP-718,"Basic extendedness criterion","Per LDM-151 §§5.4.1 and 5.4.2.12, we provide a basic extendedness measurement. The baseline calls for a comparison of the Gaussian and PSF fluxes (provided in DLP-716).",NULL
DLP-719,"Baseline coadd measurement system complete","The baseline system for performing measurements on coadds, as described in LDM-151 §5.4.1, is now available.",NULL
DLP-72,"Redesign and refactor Footprints","Execute Footprint redesign.    Strong coding, not an astronomer, Swig, C++.",NULL
DLP-720,"Likelihood coadd performance defined","Per LDM-151 §5.4.1, likelihood (""Kaiser"") coadds represent an alternative approach to measurement via MultiFit. However, their practical application is not yet well understood. This milestone represents an investigation into their utility; it indicates that sufficient work has been done on both the mathematical formalism and its practical application to simulated data (likely by producing equivalents to the algorithms described in DLP-715 and DLP-716 which apply to likelihood coadds) to inform future development (DLP-722).",NULL
DLP-722,"Select baseline design based on likelihood coadd experiments","Per LDM-151 §5.4.1, we have performed experiments in DLP-720 to establish the prospects for measurement on likelihood coadds. As of this milestone, we have chosen the final design for operations.",NULL
DLP-723,"Star galaxy separation (stretch)","The star-galaxy separation approach suggested by LDM-151 §5.4.2.13, based on a technique developed for HSC, is available. (Note that this is a stretch goal, not required by the SRD.)",NULL
DLP-724,"Forced per-epoch photometry","Per LDM-151 §5.4.1, we are now able to perform ""forced photometry"": fitting a point source to an image while holding the position, motion, shape and deblending parameters constant.",NULL
DLP-725,"Working Prototype of AP/L1 Database","Working prototype of database that will support Alert Production and Level 1 user queries. Fault tolerance not guaranteed.",NULL
DLP-726,"Forced photometry using MultiFit-derived templates","Following Bosch (_Measurement of Blended Objects in LSST_, §8.1), we use the outputs of the simultaneous multi-epoch hybrid-model minimization (DLP-708) to define per-visit deblended pixel values, and use them when performing forced photometry (DLP-724).",NULL
DLP-727,"Variability characterization based on forced measurements","Per LDM-151 §§5.4.1 & 5.4.2.16, we characterize the variability of forced sources.",NULL
DLP-728,"Allow for variability in multi-epoch fitting (stretch)","As a ""stretch goal"" following Following Bosch (_Measurement of Blended Objects in LSST_, §8.2.4) we include flux variability directly in the MultiFit-based fitting as an alternative to characterizing it based on forced photometry (DLP-727).",NULL
DLP-73,"Footprints redesigned and refactored","* New design proposed through RFC-37.  * Goals include both a cleaned up, easier to use API and better efficiency.  * Essentially a prerequisite for rewriting the low-level deblender code.",NULL
DLP-730,"Forced photometry available on difference images (stretch)","Per Bosch (_Measurement of Blended Objects in LSST_, §8.2.5) we can now perform forced photometry on difference images. This makes deblending much simpler, at the cost of increased complexity in understanding the image noise and PSF. ",NULL
DLP-732,"Forced photometry using coadd-derived templates (stretch)","Following Bosch (Measurement of Blended Objects in LSST, §8.2.6) we translate deblend templates derived on coadds to individual visit images as an alternative to using templates derived from MultiFit (DLP-726).    This approach may be mathematically complex, but has the possibility to better capture galaxy morphology than the baseline approach. We include it here as a ""stretch"" goal. ",NULL
DLP-733,"Complete Object Characterization Pipeline available","This concludes 02C.04.06.",NULL
DLP-734,"Photometric redshifts available","Per LDM-151 §§5.4.1 and 5.4.2.15 and DMS-REQ-0046, the Object Characterization Pipeline will calculate a photometric redshift for all objects.",NULL
DLP-736,"Performance and capability enhancements in support of catalog use cases","Per LDM-151 §3.1.2, development of the {{afw}} system will continue through development to support emergent requirements in 02C.04 WBS elements and to enhance performance.",NULL
DLP-737,"Performance and capability enhancements in support of catalog use cases","Per LDM-151 §3.1.2, development of the afw system will continue through development to support emergent requirements in 02C.04 WBS elements and to enhance performance.",NULL
DLP-738,"Performance and capability enhancements in support of catalog use cases","Per LDM-151 §3.1.2, development of the afw system will continue through development to support emergent requirements in 02C.04 WBS elements and to enhance performance.",NULL
DLP-739,"Performance and capability enhancements in support of catalog use cases","Per LDM-151 §3.1.2, development of the afw system will continue through development to support emergent requirements in 02C.04 WBS elements and to enhance performance.",NULL
DLP-74,"Develop interface for full focal plane PSF estimation","Includes adapting existing algorithms to the new interface, but not the development of new algorithms.    Related to calibrate task refactor in 02C.04.01",NULL
DLP-740,"Performance and capability enhancements in support of catalog use cases","Per LDM-151 §3.1.2, development of the afw system will continue through development to support emergent requirements in 02C.04 WBS elements and to enhance performance.",NULL
DLP-741,"Performance and capability enhancements in support of catalog use cases","Per LDM-151 §3.1.2, development of the afw system will continue through development to support emergent requirements in 02C.04 WBS elements and to enhance performance.",NULL
DLP-742,"Performance and capability enhancements in support of catalog use cases","Per LDM-151 §3.1.2, development of the afw system will continue through development to support emergent requirements in 02C.04 WBS elements and to enhance performance.",NULL
DLP-743,"Performance and capability enhancements in support of catalog use cases","Per LDM-151 §3.1.2, development of the afw system will continue through development to support emergent requirements in 02C.04 WBS elements and to enhance performance.",NULL
DLP-75,"Interface for full focal plane PSF estimation available","An interface is available which allows the user to request a a PSF model using information from all CCDs in a visit. The algorithmic fidelity and computational performance are not specified.    The model must cover the full focal plane (LDM-151 §5.1.1) and should make use of per-CCD PSFs (measured by the Single Frame Pipeline) and cut-outs of bright stars (LDM-151 §5.1.2; DLP-77).    This is the starting point for developing advanced PSF estimation algorithms which are defined by future milestones.",NULL
DLP-752,"Develop calibration algorithms based on dome flats and CBP","This covers all work required to perform on-telescope measurement of per-pixel QE with the exception of characterizing the filter transmission profiles.",NULL
DLP-755,"Algorithms for measuring narrowband pixel QE","Combine filter transmission profiles, CBP information and dome flats to produce a narrowband per-pixel quantum efficiency.",NULL
DLP-76,"Develop full focal plane PSF estimation based on per-CCD PSFs","Build a PSF model based on per-CCD PSFs and cutouts of bright stars.  ",NULL
DLP-760,"Development of comprehensive brighter-fatter correction routine","This work is hard to estimate; we'll get a better idea once we see how well DLP-694 performs in practice.",NULL
DLP-763,"Develop defect tracking system","Assuming agreement from the AP group and 02C.03.01, this is likely a minor variation on the code already deployed on HSC.",NULL
DLP-765,"Define atmospheric model","We baseline this as taking place before DLP-679 (the fit of data to the model), but in practice one images there will be a feedback process between the two.  ",NULL
DLP-766,"Produce spectrophotometric pipeline for calibration telescope","The baseline suggests that this can be ported from PFS, but the extent to which that is true remains to be seen.  ",NULL
DLP-77,"Full focal plane PSF estimation based on per-CCD PSFs and bright stars","Per LDM-151 §5.1.2 we model the PSF across the full focal plane by using per-CCD PSF estimates derived by the Single Frame Pipeline and cutouts of bright stars. We do not include any information beyond that available from the science CCDs.",NULL
DLP-775,"Experiment with likelhood coadds  [TODO: dependency on sims?]","To include both mathematical and practical development and testing.  ",NULL
DLP-776,"Develop star-galaxy separation techniques","Our base assumption is that this can be a port from HSC. The time requirement is significantly inflated if new LSST-side development is required to hit science goals.  ",NULL
DLP-778,"Clean up and test CModel fluxes","The CModel code is currently a mixture of work ported from HSC and some new development on LSST. It needs to be validated on real data (HSC, SDSS, CFHT). Failure modes should be eliminated.  ",NULL
DLP-78,"Incorporate existing wavefront estimation code to the DM stack","An initial Matlab implementation is available from elsewhere in LSST. If possible, we aim to incorporate that into the stack, rather than re-writing from scratch  ",NULL
DLP-781,"Develop moving point source minimization","Fit moving point-source models to multi-epoch data to measure proper motion and parallax.  Need to consider whether and how to use priors on motion based on magnitude.  ",NULL
DLP-783,"Develop single-epoch Monte Carlo galaxy fitting","Monte Carlo galaxy fitting produces outputs (samples) that don't fit into our current table library, and hence we can't implement it within the current plugin system. Once we have the support for samples in the table library (DLP-94), then writing the plugin is relatively simple.  Testing, debugging, and tuning the plugin is not simple. It's a lot of work.  ",NULL
DLP-784,"Simultaneous model fitting of blended objects using only galaxy models","Need to solve the same divide-and-conquer problem we encounter in the deblender (DLP-105).  ",NULL
DLP-785,"Develop hybrid model minimization","Stars can be blended with galaxies, but we don't want to simultaneously fit all combinatorial model assignment possibilities for each blend group.  Want a model that transitions smoothly from galaxy to star models (i.e. at zero radius, we free up the proper motion and parallax).  ",NULL
DLP-788,"Develop simultaneous hybrid MultiFit","At this point, we have all the basic components required for a complete MultiFit system. Assemble them.  ",NULL
DLP-790,"Develop complete MultiFit framework","We now have a complete MultiFit implementation, but we need to do the work to map that onto something we can actually run on the cluster.    Will include lots of input from process middleware.  ",NULL
DLP-796,"Compare algorithms and select production outputs","If we've hit the stretch goals, we have multiple options for forced measurement (DLP-726, DLP-730, DLP-732) and variability (DLP-727, DLP-728).    Compare these, select those which best hit science goals while remaining within the compute budget, and define the final set of algorithms which will be used in production.  ",NULL
DLP-799,"AP-ready Image and File Archive","Image and File Archive capable of supporting Alert Production and User Level 1 analysis.",NULL
DLP-8,"DRP-ready EFD Schema","DRP-ready version of the EFD schema.",NULL
DLP-80,"PSF estimation based on wavefront and camera metrology available","Per LDM-151 §5.1.2 we estimate the telescope & camera contribution to the PSF using wavefront sensor information and camera metrology. This is included in the full focal plane PSF estimation interface provided in DLP-77.",NULL
DLP-800,"Data Ingest into MySQL","Implement scripts for loading data produced by Summer 2015 pipelines into mysql. ",NULL
DLP-801,"Qserv uses C++-based Geometry Code","Port the geometry related code used by Qserv (it is currently written in python) to C+, and switch Qserv to the C+ based version.",NULL
DLP-802,"AP/L1 Database Design","Refreshed, complete design of database system capable of supporting Alert Production and Level 1 user database. Details are described in §4.3 of [LSE-163|http://ls.st/LSE-163*].",NULL
DLP-804,"Distributed Database and Table Management","Tools for managing distributed database and tables (creating, deleting, metadata for keeping track of them)",NULL
DLP-805,"Distributed Database and Table Management","Implementation of metadata for supporting distributed table and database management operations, including initial implementation of DROP TABLE. Further integration with data management system will be required when data distribution matures.",NULL
DLP-809,"Commissioning-ready L2 Database","Database system capable of supporting Level 2 Commissioning.    Database design is covered in LDM-135.",NULL
DLP-81,"Estimation of undersampled PSFs","Undersampled PSFs may occur when seeing is good, and break assumptions baked into our algorithms. We will need to develop an algorithm to estimate the PSF to avoid throwing away the data.     If we consider this requirement when writing our next-generation PSF modeling code, we may be able to get this functionality for free; this task will just be a matter of testing and verification in that case.   ",NULL
DLP-810,"Make Database Secure","Secured LSST Database (including level 1, 2 and 3). Covers securing database against all common attacks, such as SQL injection, DoS and others.",NULL
DLP-811,"L3 Database Support","Working version of database system capable of supporting Level 3 User analysis. Database aspects of L3 are discussed in §8.11 of [LDM-135|http://ls.st/LDM-135*].",NULL
DLP-812,"Detailed Design of Image and File Archive","A document with detailed design of Image and File Archive design, including all interactions with pipelines, users and SUI through butler, webservices etc. It should cover both writing/ingest and reading aspects.",NULL
DLP-813,"DRP-ready Image and File Archive","Image and File Archive capable of supporting Data Release Pipeline.",NULL
DLP-814,"Commissioning-ready Image and File Archive","Image and File Archive system capable of supporting LSST Commissioning.",NULL
DLP-815,"Commissioning-ready Deep Drilling Database","Database capable of supporting Deep Drilling for LSST Commissioning.",NULL
DLP-816,"Query Cost Estimate","System for estimating query cost.",NULL
DLP-817,"Multi-node Multi-query Integration Testing Harness","Extend the Qserv integration suite so that it can run on a distributed cluster.",NULL
DLP-818,CSS,"Built metadata system for keeping track of qserv-related metadata information about distributed databases and tables.",NULL
DLP-82,"Estimation of undersampled PSFs available","LDM-151 §5.1.2 requires the use of a forward-modelling algorithm to estimate the PSF in undersampled imaging. The resultant PSF should not be of lower quality than a well-sampled PSF.    Code that makes use of the PSF need not be updated to achieve this milestone (that is impossible for some algorithms, and difficult for others).",NULL
DLP-820,"Data Loader for DRP","Data loader for Qserv that is capable of loading data produced by DRP into Qserv. Data loader is discussed in §8.15.2 4.3 of [LDM-135|http://ls.st/LDM-135*].",NULL
DLP-823,"DRP-ready Qserv Query Support","Qserv capable of supporting all query types that are expected to be executed for DRP.",NULL
DLP-824,"Commissioning-ready Qserv Query Support","Qserv capable of supporting queries that will be run during commissioning.",NULL
DLP-825,"Qserv Query Result Caching","Qserv capable of caching query results, and pinning results on demand.",NULL
DLP-826,"Non-partitioned Table Support in Qserv","Qserv capable of supporting non-partitioned tables at scale. These tables might need to be replicated on each worker node, served through a dedicated server, or via shared file system. Deliverable: approach selected and implemented.",NULL
DLP-828,"Single-master Qserv Fault Tolerance","Qserv capable of surviving failures of worker nodes (workers must have sufficient replication). Fault tolerance aspects are discussed in §8.13 of [LDM-135|http://ls.st/LDM-135*].",NULL
DLP-829,"Multi-master Qserv Fault Tolerance","Qserv capable of surviving failures of master node. Fault tolerance aspects are discussed in §8.13 of [LDM-135|http://ls.st/LDM-135*].",NULL
DLP-83,"Implement color dependence in PSF model","Include chromaticity due to both atmosphere and the optics in the PSF model. See discussion in LDM-151 §5.1.2.  ",NULL
DLP-830,"Secondary Index","Efficient, scalable objectId --> chunkId index.  Indexing aspects are discussed in §8.7 of [LDM-135|http://ls.st/LDM-135*].",NULL
DLP-831,"Resource Management for DB","Resource management system capable of enforcing resource usage per user/team for databases.",NULL
DLP-832,"Qserv Software Migration Plan","Detailed plan how qserv software will be migrated in production at scale.",NULL
DLP-833,"Qserv Health Verification Tools","Tool for verifying if all Qserv services are up and running.",NULL
DLP-834,"Automated Testbed for Qserv","Build integration suite for testing Qserv. It should load data to vanilla mysql, to qserv, run queries and compare results.",NULL
DLP-835,"New xrootd client","Implement the new xrootd client that uses the new xrdssi interface, and integrate the client with Qserv code.",NULL
DLP-836,"New logging for DM","Implement log4cxx-based logging that will unify logging information from python and C++.",NULL
DLP-837,"AP-ready Butler Framework","Butler framework capable of supporting Alert Production needs. Butler documentation can be found in §3.2 of [LDM-152|http://ldm-152.readthedocs.org/en/master/#baseline-design].",NULL
DLP-838,"Prototype alert generation pipeline","In order to support the qserv team we need a prototype implementation of the alert generation pipeline by early FY17.",NULL
DLP-839,"Implement prototype alert generation pipeline","We need a prototype implementation of the alert generation pipeline for various reasons: end-to-end testing, integrations with e.g. qserv, etc.    This can be done in parallel with research into alert packet technology since the alert generation pipeline should be largely agnostic to the packaging technology.",NULL
DLP-84,"Redesigned Psf and Kernel classes available","Acceptance criteria will be defined by by the requirements-gathering step in DLP-85.  A minimum list of requirements is on DLP-85.",NULL
DLP-840,"Simple filter API definition","The SUI group need an API to program to by end of FY18.",NULL
DLP-841,"Define the API for the simple broker","In order to build the simple filter broker, we need an API that has buy in from the project.",NULL
DLP-842,"Revisit public interfaces with focus on ADQL","Deliverable: recommendation which public interfaces should be exposed to users from the Data Access Services, with particular focus on ADQL vs mysql-flavor.",NULL
DLP-844,"Resouce Management for Images","Resource management system capable of enforcing resource usage per user/team for images.",NULL
DLP-845,"Point source measurement on likelihood coadds","Per LDM-151 §5.4.1, it is required that point source measurement on likelihood coadds be available. This is required by the image differencing pipeline.",NULL
DLP-847,"Meet camera DAQ/OCS milestones for FY16","We will meet FY16 milestones for integrating DM with other subsystems (i.e., DMCS with OCS and CCS), as listed here: https://confluence.lsstcorp.org/display/SYSENG/OCS-CCS-DM+integration+timeline  and here:   https://confluence.lsstcorp.org/display/DM/Integration+timeline+summary+for+DM.",NULL
DLP-848,"Specifications package for development and support of data release processing complete","Complete design refinement and functional specification/breakdown of batch-system processing.",NULL
DLP-849,"Augment developer support by provisioning an integration cluster","Per the FY16 procurement contract, we will provision an integration cluster to augment developer support.",NULL
DLP-85,"Redesign & reimplement Psf and Kernel classes","This is a redesign of the Psf and Kernel class interfaces to support more complex algorithms in the future.  We'll need to do a more complete requirements gathering stage before we start working on a new design, but some known ones include:   - An interface for chromatic PSFs (evaluation for different SEDs), while continuing to support use cases that just want a sensible default SED to be selected.   - An interface that can handle uncertainty in the PSF model.  This is difficult to do correctly, as the uncertainty is correlated across different positions.  We may want to anticipate an importance sampling approach, for which we'd need to provide both a way to draw samples from the full, spatially-varying PSF model and a way to approximately draw samples from the PSF model realization at a point.   - Two-stage evaluation of PSF objects - once we know the position at which we want to evaluate the PSF, we may want to evaluate it in different ways:  with different SEDs, with different representations (images vs. shapelets), with or without uncertainty.   - We need to define the relationship between the Psf class hierarchy and the Kernel class hierarchy, as what we have now mostly arose historically.  I think most of it can probably be preserved, in that we'll want to keep Kernel as what we use directly to convolve images - but that means it *shouldn't* include chromaticity (though it could be the result of evaluating a Psf with a certain SED), and that may mean that an image subtraction kernel is not formally a Kernel (i.e. we may then need a new class hierarchy for those).   - We need to model the wings of the PSF, probably separately from the model of the PSF interior.  This should probably be made available as part of the Psf object.    The intent here is largely just to modify the interfaces; existing Psf implements may not be able to provide many of these features (i.e. carry uncertainty), but should have sensible fallback behavior (i.e. treat the PSF likelihood as a delta function).    C++, experience with the codebase, experience with PSF estimation -- Jim (alike).",NULL
DLP-850,"Prototype authentication/authorization system complete","We will have designed a prototype authentication/authorization system, demonstrated the technical feasibility, and implemented a prototype system.",NULL
DLP-851,"Evaluation package for Object Stores complete","We will have completed an evaluation package for Object Stores as a potential file storage system.",NULL
DLP-852,"Technical evaluation of inter-site data exchange","Technical evaluation of inter-site data exchange with the satellite processing center, coordinated through the JCC.",NULL
DLP-86,"Color dependent PSF modeling available","Described in LDM-151 §5.1.2.    Models predict the PSF model as a function of wavelength at a level TBD.    Whether DCR is included here or in a wavelength-dependent WCS is TBD.    Details of the acceptance test are somewhat problematic, because we can't currently identify a dataset that would make this realistically challenging:   - All precursor datasets are expected to have less chromatic dependence in their PSFs than LSST, to the point where we'd likely meet SRD requirements on those without even having a chromatic PSF model.   - The wavelength dependence we'll use to implement the PSF model is almost certainly exactly the same as what we'd use to create any simulations, unless PhoSim's atmospheric simulations improve considerably.",NULL
DLP-87,"Estimate PSF on simulated data","To assess the quality of our PSF measurement algorithms we will construct simulated images and measure PSFs.    Unclear if simulated data will really be available at a level required.  ",NULL
DLP-871,"Firefly server side code refactor ","Firefly has been in development in more than five years, with the code base in development for more than 15 years. The refactoring efforts will focus on  stream line the Java code and improve the quality and readability.",NULL
DLP-872,"Convert major portion of GWT in Firefly to pure JavaScript","Convert about 50% of GWT client in Firefly to pure JavaScript using React/FLUX (Redux) framework.",NULL
DLP-873,"Per-pixel QE based on CBP and dome screen flat","Per LDM-151 §6.1.2.1, we calculate broad-band per-pixel quantum efficiency based on measurements from the collimated beam projector and monochromatic illumination of the dome screen.    This is based on data from DECam and/or PhoSim.",NULL
DLP-874,"Sensitivity routines operational on DECam","Demonstrate the operation of the instrumental sensitivity system during an observing run on DECam in mid 2017.",NULL
DLP-875,"Deploy & test calibration system on DECam","Test the system as built on DECam.",NULL
DLP-876,"Boca Raton - Chicago Full Integration Ready","Path provisioned from Boca Raton to NCSA",NULL
DLP-877,"Miami - Chicago Full Integration Ready","Provisioning the circuit between Miami and NCSA",NULL
DLP-88,"PSF estimation system benchmarked on synthetic data","We have used the algorithms developed to date to estimate the PSF on high-quality simulated images and identified those areas in which our algorithms are limiting the quality of our science results.",NULL
DLP-89,"Test and Improve PSFs on ComCam data","Use ComCam data to tune and otherwise improve PSF measurement.    If everything goes according to plan, this may just be a matter of testing the PSF modeling code we'll have already built and tuning configuration parameters, but it's also possible we'll find that we need to overhaul large sections of that codebase, as this will be the first time we see how the as-built optical system really performs.    Devote 1 person full time as long as we have comcam (scheduled 4 Oct 2019 to 15 Aug 2020)  ",NULL
DLP-9,"Standalone Data Provenance Prototype","A working, non-optimized, standalone Data Provenance prototype. Provenance design and baseline schema is discussed in LDM-153.",NULL
DLP-90,"FY19 PSF KPMs satisfied","Residual PSF ellipticity KPMs TE1 (DLP-290; LSR-REQ-0097) and TE2 (DLP-308; LSR-REQ-0097) are satisfied at the level required for FY19. If possible, these measurements are made on ComCam data.    ",NULL
DLP-91,"Integrate spherical and image geometry libraries","The spherical and image geometry routines currently live in completely separate libraries, which is bad from a usability and consistency standpoint: spherical geometry code is present in both the pure-Python {{geom}} and the new C++ {{sphgeom}} package, while Euclidean geometry objects are in afw.geom.    There's a dependency issue here regarding mixing code which is required by qserv with that which is not; there is already some database code that depends on the spherical geometry libraries, and it's not clear if the database will ever need the Euclidean geometry objects.  In order to get the libraries interoperating well, however, it makes a lot of sense to have the spherical geometry library depend on the Euclidean library to a degree.  The best way to resolve this problem is probably to split the current Euclidean geometry library's low-level components that are of use to the spherical geometry library into a separate library, which the two higher-level libraries could then depend on.    C++, Swig,",NULL
DLP-92,"Integrated geometry library available","A set of libraries providing both spherical and image geometry is available, with consistent interfaces for similar operations and dependencies acceptable for use both in qserv and the science pipeline stack.",NULL
DLP-93,"Add support for persistable object fields to afw::table","This is actually a rather major overhaul of afw.table, focused on the goal of removing the plethora record types ({{SourceRecord}}, {{ExposureRecord}}, {{AmpInfoRecord}}, etc), most of which only exist to store on or more more complex objects (e.g. {{Footprints}} in {{SourceRecord}}) in addition to regular record values.  Having multiple record types greatly complicates the {{Catalog}}/{{Table}} classes - C++ isn't very good at handling polymorphic containers, and that's even harder when interfacing with Python is a concern.  Requiring a new pair of C++ {{Table/Record}} subclasses to add a first-class object to a record is also a huge amount of boilerplate and complexity for a problem that has come up more frequently than we originally anticipated.    The proposal here is that we add a new {{shared_ptr}} {{Key}} type, which would allow the {{BaseRecord}} class to hold arbitrary objects, and then remove all of its subclasses.  It's not quite as simple as that, of course:   - There are simplifications we can and should make to Catalog/Table if we only have one Record class.  We should remove the confusing Table class from public view, and could consider having different kinds of containers for Records.  We should certainly take this opportunity to ensure the data model is sufficiently compatible with {{Pandas}} and/or {{astropy.table}} that we can provide views using those libraries.   - We also use Record subclasses to provide convenience getters and setters to access items in those subclasses ""minimal schema"".  We'll need to find another mechanism for this.   - We'll need to work out the implications for persistence.  This has the potential to greatly simplify some aspects of table-based persistence for complex objects, but it will require some significant work on that part of the codebase as well.    C++, Jim has some specific design ideas but somebody else should get trained up.",NULL
DLP-94,"Persistable objects support in afw::table available","It is now possible to add fields to tables which can contain persistable objects.",NULL
DLP-95,"Add support for join iterators to afw::table","This adds limited support for one-to-one, one-to-many and (possibly) many-to-many relationships between afw::tables.  This is currently the major missing feature that gets in the way of using afw.table as an in-memory representation of SQL-style database tables in C++ and Python, and hence addressing it may allow more analysis code to be written independent of a future Butler's query interface.    We have a few specific use cases in mind:   - We need a way to store the results of galaxy fitting algorithms that involve Monte Carlo sampling of the likelihood (instead of greedy optimization).  This is an extra table of samples associated with each source (with the length of the table varying from source to source) - a straightforward one-to-many relationship.   - When analyzing forced photometry or multi-band coadd datasets, we need a way to express the one-to-many relationships between objects and sources.   - When doing spatial matches (both pairwise and N-way) we need a better way to express the results, the {{afw.table.Match}} objects currently in use are extremely limited.    One of the main design challenges here is the issue of ownership - linking tables together necessitates giving some records pointers to other records.  In some contexts, such as the one-to-many relationships needed for likelihood sampling, it's appropriate for a record from one table to ""own"" the records it's associated with from the other table.  In others, we may want to create lightweight relationship objects that own shared references the tables they join.    C++, needs to fit with DLP-93, extra 2 months of work on top of that.",NULL
DLP-96,"Join iterators support in afw::table available","It is now possible to conveniently iterate over one-to-one, one-to-many and many-to-one relationships between different tables.    This includes explicit support for the following three features:   - Output datatypes for source measurement algorithms that produce Monte Carlo samples (e.g. an additional table of sample records associated with each source).   - Data types that represent the relationship between reference objects and forced sources, for use in analysis code.   - Data types to express the results of N-way spatial matches, for use in analysis code.",NULL
DLP-97,"Overhaul single epoch deblender","The current deblender codebase is a mess and needs a rethink before we can make significantly improve it and develop towards multi-coadd deblending.    There are a number of enhancements in the SDSS deblender which have never been ported to HSC, but which are known to make a significant different. They should be included in this overhaul.    Specifically Bob.",NULL
DLP-98,"Overhauled single epoch deblender available","Per LDM-151 §5.3.2, the baseline deblender is based on the SDSS system. A number of improvements have been made on SDSS and HSC, and, in this milestone, we incorporate them to LSST.",NULL
DM-1000,"Test failures should cause a build failure","There are currently no red flashing lights when a test fails under scons --- just a ""failed"" printed to the screen, which is easy to miss, especially when building multithreaded.  A test failure should cause the entire scons build to fail, the same way a compile failure causes the build to fail.",NULL
DM-1003,"pipe_tasks tests non-deterministic","We have observed that pipe_tasks contains tests that are non-deterministic, since they use the numpy random number generator without seeding it.  This may explain errors from buildbot that we've been unable to reproduce.",NULL
DM-1008,"Request: approve log4cxx","We developed a new logging system (replacement for pex_logging), it has been packaged into a package called ""log"" (see [log.git|https://dev.lsstcorp.org/cgit/contrib/log.git]). It is based on [Apache's log4cxx|http://logging.apache.org/log4cxx/], which has been packages too, see [log4cxx,git|https://dev.lsstcorp.org/cgit/contrib/eupspkg/log4cxx.git]. Note that log4cxx depends on the [Apache Portable Runtime Project|https://apr.apache.org/], in practice on apr and apr-utils, which are already in ""LSST/external"" (so I assume they are already approved).  We need the approval for log4cxx. Thanks.",NULL
DM-1009,"Request: approve log package","We developed a new logging system (replacement for pex_logging), it has been packaged into a package called ""log"" (see [log.git|https://dev.lsstcorp.org/cgit/contrib/log.git])  Note, we will have a separate issue to enforce transition from pex_logging to log, so this ticket does not enforce the transition for everyone (we will be going ahead with the Qserv soon though).  We need the new package to be approved. Note it depends on log4cxx. Thanks.",NULL
DM-1019,"Make unit tests deterministic","The use of random numbers in unit tests without seeding the RNG means the tests may not be deterministic.",NULL
DM-1020,"Create Visualization of AP Simulator execution","I forgot to this this as a task when this started, but for the last week I've been working on an Alert Production Simulator visualization.  It's complete and can be viewed here:  http://lsst-web.ncsa.illinois.edu/~srp/",NULL
DM-1024,"Documentation and rename for forced measurement tasks","Move and improve docs from tickets/DM-976, rename forced cmd-line drivers to ForcedPhotImage, ForcedPhotCcd, ForcedPhotCoadd.",NULL
DM-1025,"Overall doxygen docs for measurement framework","Need new summary-level docs for the package, describing measurement plugin system, roles of tasks.  Should have links to task example docs.  Old summary-level docs focused on C++ algorithm framework should be removed but moved to a different page.",NULL
DM-1027,"sconsUtils.tests.run needs to check for test failures and return exit status","We want test failures to cause scons to return a non-zero exit status, instead of forcing people to look for .failed files or messages in the build output.",NULL
DM-1032,"Improve Butler usability, add features","The butler (including the {{CameraMapper}} class) requires updates to improve its functionality and usability and to remove legacy code, including use of {{pex_policy}}.",NULL
DM-1039,"'scons test' should return nonzero exit code when any test fails","When one or more unit tests fail, scons should return a nonzero exit code, indicating failure. Otherwise, products that fail unit tests will silently be installed by EUPS (and possibly other tools).",NULL
DM-1040,"Doxygen doesn't include obs packages","The doxygen pages (http://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/annotated.html) currently don't include any of the obs packages (e.g., obs_lsstSim, obs_cfht, obs_subaru).  These packages contain useful code, and their documentation should be merged in with that in Doxygen.  This may be difficult, given that we want to keep the obs packages separate in some manner from the rest.",NULL
DM-1042,"log::popContext() fails to pop top component","{{log::popContext()}} looks for dots in the logger name.  When the context was the top level component, there are no dots, and the function does nothing.  It should set the name to the empty string and the default logger to the root logger.",NULL
DM-1043,"Automate sending of OCS messages","We should have a way to automate the sending of OCS messages to the simulator, rather than sending the messages ""by hand"".  OCS messages would be sent out at the expected real intervals we've established.",NULL
DM-1044,"Restructure the user facing site containing the doxygen documentation ","This issue is focused on simplifying the users' web front-end to the doxygen documentation associated with specific builds. In particular:  * restructure the public website to simplify users' ability to find the document version  they are seeking.  This might be by mapping to a bNNN,  date or Release tag.  *  on each 'official' DM Release, capture the doxygen documentation under the name of the Release.",NULL
DM-1053,"eupspkg doesn't build optimised when SCONSFLAGS is set","LSST C++ components are being built without optimisation because of a bug in eupspkg which is triggered by an existing {{SCONSFLAGS}} environment variable that doesn't activate optimisation (e.g., no {{opt=3}}).  Specifically, eupspkg contains the line:  {code:sh} export SCONSFLAGS=${SCONSFLAGS:-""opt=3""}        # Default scons flags {code}  My {{SCONSFLAGS}} is already set (to {{-j 2 cc=clang}}, as I believe we recommend, or used to recommend, for Mac builds).  So the result of this line is that {{SCONSFLAGS}} is unchanged, which means optimisation isn't activated for the build.  An expression that always activates optimisation (which is what one wants for a production build) would be something like:  {code:sh} export SCONSFLAGS+="" opt=3"" {code}  Once this is fixed, a new release should be made.",NULL
DM-1056," DS9 use wrt Releases and/or Example Release Use","DS9 is being used in examples illustrating the use of DM Releases.  DS9 was formerly considered a developer-only 3rd party tool and not bundled in a release. Nor was it included in the DM Copyright statement listing the 3rd Party Tools required.  Dick raises the question whether it needs to be promoted in status  or just mentioned as a potentially useful user-supplied tool.",NULL
DM-1057,"CRs get through to astrometry solver","The CalibrateTask does not remove cosmic rays from the list of sources sent to AstrometryTask.  Either the calibrate or astrometry task needs to extend the config to include a list of flags to use to cull sources from the list.  ",NULL
DM-1064,"Separate master builds from the user triggered builds","Separate master builds from the user triggered builds in order to visually clarify the web display of builds.",NULL
DM-1065,"On triggering builds, display user and branches on waterfall display","On user-triggered builds, display the username and requested branches on waterfall display  thus allowing the user to identify their build more easily.",NULL
DM-1066,"Convert Buildbot to on-git change triggert instead of cron job trigger","Convert from the current cron job which initiates a trigger when any repo changes  to providing an on-git-change trigger for each repository.  Need clarification from [~ktl]  on meaning of product -  package based (lsst_distrib)  or project group based (sims, DB, DM-core)  * ""get the master builds triggered on git-change instead of via cron and separated by product""",NULL
DM-1069,"Measurement - Unit Tests","Gee 100%.",NULL
DM-1075,"Message about failed buildbot runs points to deleted log files","At the end of the buildbot failed message these lines appear: {quote} *** error building product afw. *** exit code = 1 *** log is in /lsst/home/lsstsw/build/afw/_build.log *** last few lines: :::::  + echo '*** Failed unit tests.' :::::  *** Failed unit tests. {quote}  This is not true.  As Robyn points out, the correct location of these files is implied by statements at the top of the message: {quote} Failed Package Info at	lsst-dev.ncsa.illinois.edu:/lsst/home/lsstsw/build/FailedLogs/328/ .......Log	*/_build.log {quote} by which is meant {quote} dev.ncsa.illinois.edu:/lsst/home/lsstsw/build/FailedLogs/328/afw/_build.log {quote}  It'd be nice if this were a little more explicit.",NULL
DM-1080,"Create iPython notebooks illustrating analysis with afw","There are various examples of using afw for analysis tasks, many in PowerPoint slides. A select few (image display and annotation, constructing color-color plots of source catalogs) would be much more useful to users in the form of iPython notebooks. This task is to craft two examples, assemble the supporting data that would be needed, and add them to the SWUG. ",NULL
DM-1081,"Enhance SWUG tutorial on SDSS processing","The tutorial on Processing SDSS Images in the SWUG is almost self-contained, apart from access to the database server at NCSA to:   - query the SeasonFieldQuality table - ingest processing metadata and source measurements  This can be solved by providing basic instructions for users to install (or access an existing) DB server, and by providing a mechanism for users to download and import the SeasonFieldQuality table. ",NULL
DM-1082,"Create a description of LSST data products for SWUG","Augment the SWUG chapter on Key LSST Data Concepts with a high-level description of the data products that are generated in production processing. The emphasis will be on data products that are produced with the current Stack, with only a lightweight summary of products that are anticipated in future releases. ",NULL
DM-1084,"Ressurect python dependancy checker for stack distribution","As discussed in the SAT call 2014-08-19  The problem is alerting stack users to potentially missing assumed third party dependancies. (The example was DS9). Robert The Good said he had a config-time checker that he could ressurect to check for third-party dependancies.   Robert said:  The configure stuff is in 	git@git.lsstcorp.org:LSST/DMS/devenv/lsst but may be/probably is very out of date.  I'll have a look at it and (a) add DS9 check (b) check with Dick in case he needs to document any extra steps. ",NULL
DM-1090,"Replace std::string const& fmt with char const* fmt in Log::log() and Log::vlog()","For standards compliance for {{va_start}}.  May require changes to places that these are called.",NULL
DM-1091,"Create a tutorial showing how to generate QA plots","This task is to create a tutorial that shows how to generate QA plots for processed datasets. Ideally it will use existing pipeQA software, or borrow from it. The intended audience includes both scientists with real datasets, and developers who want to evaluate the quality of the algorithms. ",NULL
DM-1092,"Design review new butler","Describe the interface and high-level implementation. Have the SAT review it.",NULL
DM-1093,"Implement file-based butler","Implement the new butler interface, including a mapper base class that supports single input files and output path templates.  A prototype will be delivered in S14.",NULL
DM-1094,"Implement directory-based butler","Add functionality to the mapper base class to support input directories with path templates and without registries.  A prototype will be delivered in S14.",NULL
DM-1095,"Implement registry-based butler","Add functionality to the mapper base class to support registry-based input directories.  A prototype will be delivered in S14.",NULL
DM-1096,"Implement calibration rendezvous for butler","Add functionality to the mapper base class to support calibration rendezvous.  A prototype will be delivered in S14.",NULL
DM-1097,"Implement database input for butler","Add functionality to the mapper base class to support input datasets from a database.  A prototype will be delivered in S14.",NULL
DM-1098,"Buildbot is not recognizing the exit return from a lock-exceeded situation","Re-edit: What I should have noted is that the error refinement mentioned below is occurring in DM-1430. It handles this Issue at no added cost.  Buildbot's error detection is failing to recognize both conditions when lsstsw returns in error either from the lsstsw flock or the eups lock.  The action should be to terminate the entire run in failure; instead it proceeds as though no error occurred and continues processing thru doxydoc generation and the demo end-to-end run.  This Issue is being expanded  (in DM-1430) to include more nuanced error messages than the current blanket: 'The build failed'.  The new error messages (and error handling) should differentiate on: 1) the error occurred during setup prior to the build; 2) the error occurred during the build; 3) the error occurred during the doxygen generation; 4) the error occurred during the I&T test.  The flock  error is characterized as being in the pre-build phase. The eups timeout error may occur in either the pre- or build phases.  (We leave for another Issue, the separation into new steps for 1&2, 3, 4.)",NULL
DM-1102,"Test Data - Replace Unit-Test Data Package","The {{afwdata}} package is used for many unit tests in {{afw}} and {{meas_algorithms}}, but I think we've largely stopped using it for new tests.  We instead tend to add new data packages or add small test datasets to the packages we're testing - fine solutions in isolation, but a big problem when we do this all the time.    I think there are several reasons we don't use {{afwdata}} more:   - It's poorly documented.   - Many of its files are obsolete: they use the old MaskedImage format, or they're outputs from older versions of PhoSim that may not be completely supported by obs_lsstSim anymore.   - There's no butler to organize the larger, more realistic datasets.   - There are no truth values for the simulated datasets.    I think we need to audit the way we use both {{afwdata}} and the ad-hoc test data files in individual code packages, gather requirements for other kinds of reusable data for unit tests, and put together a new package that tries to meet those needs without being too large.  Eventually I'd like to retire {{afwdata}} in favor of this new package entirely, and we might be able to merge it with some other packages that contain test data ({{obs_test}}?) as well.",NULL
DM-1119,"EUPS responsiveness is sluggish due to rapid growth of eups-versioned  packages","Eups is exhibiting a slowdown due to the rapidly increasing number of eups-versioned packages being maintained by it.  It has been suggested that, for the short term, the buildbot system create a periodic purge of eups tags associated with user branches (for those would considered too old for debugging use).  A longer term solution will be having separate build structures for mainline builds and user builds.",NULL
DM-1144,"sconsUtils test hangs when run from scons","A hang was noticed while building master of {{sconsUtils}} using {{lsst-build}}.  {{scons opt=3}}, run manually, hangs while running the new test case.  Running the new test case explicitly using {{python tests/testAll.py}} does not hang.  This appears to be because the test relies on the {{setup}} shell function which is not being made available when run under {{scons}}.",NULL
DM-1163,"Deal with optional packages in install","If a package being installed has an optional dependency that is available on the user's machine at install time, it will be pulled in to the install.  However, when that package is required as a dependency itself, that optional dependency isn't setup and the installation fails.  Specifically, [~ajc] reports:  {quote} Dont have any logs left but the message was DirectoryInstaller([""/Users/ajc/LSST/Software/lsst/DarwinX86/obs_sdss/8.0.0.0+5/bin""], [""bin""]) DirectoryInstaller([""/Users/ajc/LSST/Software/lsst/DarwinX86/obs_sdss/8.0.0.0+5/etc""], [""etc""]) eups expandtable: Processing /Users/ajc/LSST/Software/lsst/DarwinX86/obs_sdss/8.0.0.0+5/ups/obs_sdss.table: Product pykg_config is a dependency for pipe_tasks 8.0.0.1+3, but is not setup scons: *** [table] Error 9 scons: building terminated because of errors. + exit -5 eups distrib: Failed to build obs_sdss-8.0.0.0+5.eupspkg: Command: source /Users/ajc/LSST/Software/lsst/eups/bin/setups.sh; export EUPS_PATH=/Users/ajc/LSST/Software/lsst; (/Users/ajc/LSST/Software/lsst/EupsBuildDir/DarwinX86/obs_sdss-8.0.0.0+5/build.sh) >> /Users/ajc/LSST/Software/lsst/EupsBuildDir/DarwinX86/obs_sdss-8.0.0.0+5/build.log 2>&1 4>/Users/ajc/LSST/Software/lsst/EupsBuildDir/DarwinX86/obs_sdss-8.0.0.0+5/build.msg  exited with code 251 {quote}  Should the install be doing a {{setup -k}} instead of {{setup -j}}, to ensure all dependencies are satisfied?",NULL
DM-1165,"detecting dependency on boost (eups vs system)","Robert, can you remind us how to tell sconsUtil to use libraries that are not eups-related? (for example, system boost). Thanks. ",NULL
DM-1166,"internet-free distribution server","Hello Mario,  DM-602 allows to install Qserv from a local distribution server. This is useful for cluster installation which is often performed on internet-free machines.  Nevertheless, one of our requirement is that anonymous user can easily download distribution data from the distribution server, in order to setup a local distribution server (for example on a cluster shared filesystem).  The current solutions don't satisfy this :  - rsync require an account on lsst-dev - wget and curl can't easily  download recursively the content of a directory, and doesn't cover synchronisation - tarball (our current solution), doesn't cover synchronisation  A proposal is to set-up a read-only anonymous rsync account on lsst-dev.ncsa.illinois.edu which would allow anonybody to synchronize easily distribution data.  Would it be possible please, ot would you have a better solution ?  Thanks,  Fabrice",NULL
DM-1172,"Risk register update: reset all dates to match current schedule","Especially, move all DM Integration Complete trigger dates from 3/19 to 9/19.",NULL
DM-1173,"Change name of computing cluster in LDM-230 from Alert Production Cluster to Processing Cluster","The worker machines for nightly (and daily) computing will be used for much more than Alert Production.  We are renaming the cluster to the Processing Cluster.",NULL
DM-1174,"buildbot enhancement request: allow cancel","In starting a buildbot job to test DM-840 I ran into a few requests for enhancement:  - If one enters an email in simple format it rejects it and clears all other fields. Why not just leave everything as is so the user can correct it? I started a ""master"" build before I realized my branch information had been discarded. I also wish the form would accept simple email addresses; I think it is being overly picky.  - I'd like to be able to cancel jobs I start. Since you only use one normal login for most purposes it's a bit of a security risk, but enough to really worry about? I hope not. The consequences are pretty minor.  - The log files show normal completion in red. It's picky, but I think the files would be easier to read if red was reserved for problems.  The first problems will go away if you are able to authenticate using standard LSST usernames and passwords, which I know is somewhere on the ""to do"" list. That will be great since it avoids the need to specify an email address at all.",NULL
DM-1175,"Estimated completion time is wildly inaccurate","Is there a practical way to make the estimated completion time more accurate?",NULL
DM-1178,"Use PARENT coordinates in image-like objects","This has been done for all python interfaces as part of DM-763, but was deferred for the C++ side.  These tasks may not reflect the way this work will be done, but will serve as a starting point.",NULL
DM-1179,"testTruncatedGaussian.py fails due to a missing scipy symbol","The unit test testTruncatedGaussian.py fails on my Macs using the v9_2 stack, including its anaconda 1.8.1 with: {code}   File ""tests/testTruncatedGaussian.py"", line 117, in integrate2d     return scipy.integrate.dblquad(func, 0.0, scipy.integrate.Inf, AttributeError: 'module' object has no attribute 'Inf' {code}  I see this on master c50ad6f but I've seen it for awhile (and I thought I'd reported it before, but I couldn't find it on JIRA).",NULL
DM-1180,"Initialization for log ","Working on DM-1005 I found that initialization of log package causes me a lot of trouble. I hope we can improve that.  Currently log can be initialized in one of the following ways: - calling {{LOG_CONFIG(file_name)}} where file name has to be passed somehow to the initialization code, configures loggers according to given config file -  calling {{LOG_CONFIG()}} defines some basic configuration which kind of works but isn't usually what I want - not calling any of the above but setting environment variable {{LOG4CXX_CONFIGURATION}} to a config file name, which is equivalent to first option (this may not be intended use case but it works now) - if I don't do any of the above then log4cxx prints error messages and disables logging completely  Our use cases for log may be non-conventional but I have to implement them anyways. There are basically two cases where I have troubles: - plugins for xrootd (implemented as libraries), these do not have a simple way to pass configuration file name except through environment variable, I could use {{LOG4CXX_CONFIGURATION}} for that (without calling {{LOG_CONFIG()}} but I also want to produce reasonable output when I forget to set {{LOG4CXX_CONFIGURATION}}. Another complication is that there may be more than one plugin loaded at the same time, deciding which one has to initialize logging is complicated. - a (potentially large) number of unit tests. We could add {{LOG_CONFIG()}} to each one of them but I'd like to avoid that for a number of reasons.  So I think what I'd like to have is an option to not call {{LOG_CONFIG()}} which would still produce reasonable output even without {{LOG4CXX_CONFIGURATION}} (in other words make call to {{LOG_CONFIG()}} implicit). But I would also like to be able to change logger configuration via {{LOG4CXX_CONFIGURATION}} when {{LOG_CONFIG()}} is called (explicitly or implicitly).  I propose to modify log to do following: - any call to logger methods implicitly initializes it (via call to {{LOG_CONFIG()}}) if it has not been initialized. - if {{LOG_CONFIG()}} is called without file name and {{LOG4CXX_CONFIGURATION}} is set then it should be equivalent to {{LOG_CONFIG(getenv(""LOG4CXX_CONFIGURATION""))}}   Objections, suggestions?",NULL
DM-1181,"Adopt guidelines on git commit best practices","The question of what makes a ""good commit"" arises occasionally. I feel that the following document:  	https://wiki.openstack.org/wiki/GitCommitMessages#Structural_split_of_changes  presents a decent guideline. Pasting from it:  ================================ Structural split of changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~ The cardinal rule for creating good commits is to ensure there is only one ""logical change"" per commit. There are many reasons why this is an important rule:  * The smaller the amount of code being changed, the quicker & easier it is to review & identify potential flaws.  * If a change is found to be flawed later, it may be necessary to revert the broken commit. This is much easier to do if there are not other unrelated code changes entangled with the original commit.  * When troubleshooting problems using GIT's bisect capability, small well defined changes will aid in isolating exactly where the code problem was introduced.  * When browsing history using GIT annotate/blame, small well defined changes also aid in isolating exactly where & why a piece of code came from.  Things to avoid when creating commits ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ With that in mind, there are some commonly encountered examples of bad things to avoid  * Mixing whitespace changes with functional code changes. The whitespace changes will obscure the important functional changes, making it harder for a reviewer to correctly determine whether the change is correct. Solution: Create 2 commits, one with the whitespace changes, one with the functional changes. Typically the whitespace change would be done first, but that need not be a hard rule.  * Mixing two unrelated functional changes. Again the reviewer will find it harder to identify flaws if two unrelated changes are mixed together. If it becomes necessary to later revert a broken commit, the two unrelated changes will need to be untangled, with further risk of bug creation.  * Sending large new features in a single giant commit. It may well be the case that the code for a new feature is only useful when all of it is present. This does not, however, imply that the entire feature should be provided in a single commit. New features often entail refactoring existing code. It is highly desirable that any refactoring is done in commits which are separate from those implementing the new feature. This helps reviewers and test suites validate that the refactoring has no unintentional functional changes. Even the newly written code can often be split up into multiple pieces that can be independently reviewed. For example, changes which add new internal APIs/classes, can be in self-contained commits. Again this leads to easier code review. It also allows other developers to cherry-pick small parts of the work, if the entire new feature is not immediately ready for merge. Addition of new public APIs or RPC interfaces should be done in commits separate from the actual internal implementation. This will encourage the author & reviewers to think about the generic API/RPC design, and not simply pick a design that is easier for their currently chosen internal implementation.  The basic rule to follow is:      If a code change can be split into a sequence of patches/commits,     then it should be split. Less is not more. More is more. ================================  In light of increasingly allowing/encouraging rebasing to happen, I think we should set an expectation of what the commits should look like. I'd motion the SAT to put this onto the agenda as an addition to our coding standards.",NULL
DM-1182,"Adopt guidelines on contents and format of git commit messages","The question of what makes a ""good commit message"" occasionally comes up. I feel that this document:    http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html  presents a reasonable set of guidelines and I propose the SAT adopts it as a part of our software development standards.",NULL
DM-1183,"Allow rebasing of ticket/DM-NNN branches once DM-78 is done","I propose we:  * permit rebasing of tickets/DM-NNNN branches, once a gitolite hook designed to save the old branch (developed in DM-78) is finalized and deployed.  * require that, if needed, tickets/DM-NNNN branches are rebased to comply with DM-1181 and DM-1182 guidelines on commits and commit messages AND make the assessment/enforcement of compliance a part of the code review.",NULL
DM-1186,"Make LOG macros look like proper statements","Some log macros are now defined as  {code:cpp} if (something) { doSomething; }  {code} and this will cause unexpected behavior if people put semicolon after them. For example this code will break: {code:cpp}     if (data.empty())         LOG_DEBUG(""data is empty"");     else         doSomething(data); {code} It should be curable by placing  {code:cpp} do { ... } while(false) {code}  around it (and it will make closing semicolon required). ",NULL
DM-1189,"Add guards against overwriting eups tags in publish script","The ""publish"" script in ~lsstsw/bin will currently happily overwrite existing eups tags, for example, by running ""./bin/publish \-b b245 \-t current qserv"" I managed to destroy existing ""current"".  It'd be good to prevent that from happening, or adding guard (--forceOverwrite flag maybe?) if we need to allow it in some special circumstances.",NULL
DM-1190,"Review comments from DM-1067","I missed the GitHub review comments from Paul.  I have other cleanup tasks to do during this Sprint, and will do all of these fixes this week.  Will checkin to this issue when they are all done.",NULL
DM-1191,"Write a document giving reasons for moving to GitHub and preferring GitHub to Stash","As recommended by the SAT meeting on 2014-09-16, we need this document to promote the use of GitHub by other subsystems within the project.",NULL
DM-1194,"Participate in October meeting with Gregory","There's a teleconference in October with Gregory about the visualization requirements.   This is a fact gathering meeting to decide what hardware to support these requirements.  (tiled displays, etc).  IPAC is doing the software for this.",NULL
DM-1198,"Integration test are broken (likely by DM-198)","I'm trying to run integration test with the current master and they fail, here is what I see in the logs: {code} 2014-09-19 14:35:44,579 INFO Running : mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch LSST -e SELECT objectId,iauId,ra_PS,ra_PS_Sigma,decl_PS,decl_PS_Sigma,radecl_PS_Cov,htmId20,ra_SG,ra_SG_Sigma,decl_SG,decl_SG_Sigma,radecl_SG_Cov,raRange,declRange,muRa_PS,muRa_PS_Sigma,muDecl_PS,muDecl_PS_Sigma,muRaDecl_PS_Cov,parallax_PS,parallax_PS_Sigma,canonicalFilterId,extendedness,varProb,earliestObsTime,latestObsTime,meanObsTime,flags,uNumObs,uExtendedness,uVarProb,uRaOffset_PS,uRaOffset_PS_Sigma,uDeclOffset_PS,uDeclOffset_PS_Sigma,uRaDeclOffset_PS_Cov,uRaOffset_SG,uRaOffset_SG_Sigma,uDeclOffset_SG,uDeclOffset_SG_Sigma,uRaDeclOffset_SG_Cov,uLnL_PS,uLnL_SG,uFlux_PS,uFlux_PS_Sigma,uFlux_ESG,uFlux_ESG_Sigma,uFlux_Gaussian,uFlux_Gaussian_Sigma,uTimescale,uEarliestObsTime,uLatestObsTime,uSersicN_SG,uSersicN_SG_Sigma,uE1_SG,uE1_SG_Sigma,uE2_SG,uE2_SG_Sigma,uRadius_SG,uRadius_SG_Sigma,uFlags,gNumObs,gExtendedness,gVarProb,gRaOffset_PS,gRaOffset_PS_Sigma,gDeclOffset_PS,gDeclOffset_PS_Sigma,gRaDeclOffset_PS_Cov,gRaOffset_SG,gRaOffset_SG_Sigma,gDeclOffset_SG,gDeclOffset_SG_Sigma,gRaDeclOffset_SG_Cov,gLnL_PS,gLnL_SG,gFlux_PS,gFlux_PS_Sigma,gFlux_ESG,gFlux_ESG_Sigma,gFlux_Gaussian,gFlux_Gaussian_Sigma,gTimescale,gEarliestObsTime,gLatestObsTime,gSersicN_SG,gSersicN_SG_Sigma,gE1_SG,gE1_SG_Sigma,gE2_SG,gE2_SG_Sigma,gRadius_SG,gRadius_SG_Sigma,gFlags,rNumObs,rExtendedness,rVarProb,rRaOffset_PS,rRaOffset_PS_Sigma,rDeclOffset_PS,rDeclOffset_PS_Sigma,rRaDeclOffset_PS_Cov,rRaOffset_SG,rRaOffset_SG_Sigma,rDeclOffset_SG,rDeclOffset_SG_Sigma,rRaDeclOffset_SG_Cov,rLnL_PS,rLnL_SG,rFlux_PS,rFlux_PS_Sigma,rFlux_ESG,rFlux_ESG_Sigma,rFlux_Gaussian,rFlux_Gaussian_Sigma,rTimescale,rEarliestObsTime,rLatestObsTime,rSersicN_SG,rSersicN_SG_Sigma,rE1_SG,rE1_SG_Sigma,rE2_SG,rE2_SG_Sigma,rRadius_SG,rRadius_SG_Sigma,rFlags,iNumObs,iExtendedness,iVarProb,iRaOffset_PS,iRaOffset_PS_Sigma,iDeclOffset_PS,iDeclOffset_PS_Sigma,iRaDeclOffset_PS_Cov,iRaOffset_SG,iRaOffset_SG_Sigma,iDeclOffset_SG,iDeclOffset_SG_Sigma,iRaDeclOffset_SG_Cov,iLnL_PS,iLnL_SG,iFlux_PS,iFlux_PS_Sigma,iFlux_ESG,iFlux_ESG_Sigma,iFlux_Gaussian,iFlux_Gaussian_Sigma,iTimescale,iEarliestObsTime,iLatestObsTime,iSersicN_SG,iSersicN_SG_Sigma,iE1_SG,iE1_SG_Sigma,iE2_SG,iE2_SG_Sigma,iRadius_SG,iRadius_SG_Sigma,iFlags,zNumObs,zExtendedness,zVarProb,zRaOffset_PS,zRaOffset_PS_Sigma,zDeclOffset_PS,zDeclOffset_PS_Sigma,zRaDeclOffset_PS_Cov,zRaOffset_SG,zRaOffset_SG_Sigma,zDeclOffset_SG,zDeclOffset_SG_Sigma,zRaDeclOffset_SG_Cov,zLnL_PS,zLnL_SG,zFlux_PS,zFlux_PS_Sigma,zFlux_ESG,zFlux_ESG_Sigma,zFlux_Gaussian,zFlux_Gaussian_Sigma,zTimescale,zEarliestObsTime,zLatestObsTime,zSersicN_SG,zSersicN_SG_Sigma,zE1_SG,zE1_SG_Sigma,zE2_SG,zE2_SG_Sigma,zRadius_SG,zRadius_SG_Sigma,zFlags,yNumObs,yExtendedness,yVarProb,yRaOffset_PS,yRaOffset_PS_Sigma,yDeclOffset_PS,yDeclOffset_PS_Sigma,yRaDeclOffset_PS_Cov,yRaOffset_SG,yRaOffset_SG_Sigma,yDeclOffset_SG,yDeclOffset_SG_Sigma,yRaDeclOffset_SG_Cov,yLnL_PS,yLnL_SG,yFlux_PS,yFlux_PS_Sigma,yFlux_ESG,yFlux_ESG_Sigma,yFlux_Gaussian,yFlux_Gaussian_Sigma,yTimescale,yEarliestObsTime,yLatestObsTime,ySersicN_SG,ySersicN_SG_Sigma,yE1_SG,yE1_SG_Sigma,yE2_SG,yE2_SG_Sigma,yRadius_SG,yRadius_SG_Sigma,yFlags FROM Object WHERE objectId = 430213989148129 2014-09-19 14:35:44,752 INFO    stderr : -- ERROR 4110 (Proxy) at line 1: Qserv error: Unknown error InbandQueryAction -- {code} It looks like this error started to happen after DM-198 merge, I tried a commit right before that and it works OK. ",NULL
DM-1200,"Consider removing/reducing hardcoded paths in qserv run dir","Once a qserv run directory is created with its templated config file, it is frozen to the qserv version currently ""setup"" in eups via hardcoded paths in various scripts located in the run directory.  * It is unclear whether these scripts will work if another qserv eups-version is ""setup"". Are they tied to the particular directory, regardless of eups state? IOW, can the run directory be used without ""source loadLSST.sh"", provided the files are not touched?  If so, great! But the run directory should record which versions it was frozen with.  * If use of the run directory still requires the eups environment variables, then it should not be hardcoded with paths--it should switch versions if ""setup"" is called to change qserv versions. Arguably, this is dangerous, but it is only as dangerous as using existing data with new software, which for development, is extremely useful.",NULL
DM-1202,"ctrl_events unit tests are not deterministic","the unit tests in ctrl_events do not use unique topics, which can lead to unpredictable failures.  Also (and I'll be happy to split this into a separate ticket, if desired), the unit tests do not use the unit test framework. The tests would be easier to read and the test results easier to understand if they did. Better yet, many unit tests could be combined into a smaller number of tests.  Finally, a minor point, but many tests import packages they don't use. I recommend running the code through a linter.",NULL
DM-1203,"Set single time for all project software & common machines","So having had the quote-unquote-fun of trying to troubleshoot something that I triggered on a known Tucson timescale, on a machine running on CDT, with a log timestamped in GMT....  May I propose that all our timestamping and machine times are set to a single timezone?  To first order I don't care what it is.  To second order, I have a mild preference for Pacific, since I already have to keep it in my head due to it being ""Official Project Time"" and it means it will be equivalent to local for a lot of people for half the year. But really, Zulu time is fine, just... pick one. ",NULL
DM-1204,"IN2P3: Support CFHT in MySQL database","MySQL schema and database creation scripts should be modified to support CFHT data",NULL
DM-1205,"Migrate C++ Coding Standard from Trac to Confluence","Migrate the content of the LSST C++ Coding Standard from its location on the Trac/Wiki (https://dev.lsstcorp.org/trac/wiki/C%2B%2BStandard/References) to the DM Developer Guide (https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666). The current draft version is 1.6, which apparently needs further review by the SAT before it can be declared official. ",NULL
DM-1206,"Migrate the Python coding standard from Trac to Confluence","Migrate the content of the LSST Python Coding Standard from its location on the Trac/Wiki (https://dev.lsstcorp.org/trac/wiki/PythonCodeStandards) to the DM Developer Guide (https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard). The current draft version is 3.0, which was reviewed by the SAT before it can be declared official. Oddly, the Python 2->3 migration page was never incorporated into the standard; it should appear in a new subsection (Python 3 Idioms). ",NULL
DM-1207,"Migrate general coding standards docs from Trac to Confluence","Migrate the Trac documents on DM Coding Style Policy (https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy) as well as the top-level Standards and Policies page (https://confluence.lsstcorp.org/display/LDMDG/Standards+and+Policies) to to the DM Developer Guide. ",NULL
DM-1208,"Would like to be able to specify build flags to buildbot","It would be handy to be able to specify flags to scons when triggering a buildbot build. Occasionally we want to test a variant build (e.g. C++11).  One subtlety is that all packages will have to be rebuilt if any such flags are specified (for user-triggered builds buildbot tries to reuse existing packages when it can, and that would not be acceptable in this situation).",NULL
DM-1209,"C++11 support needs some work","sconsUtils has nominal support for C++11 but it needs some work including: - It should set CXXFLAGS not CCFLAGS when it sees --c++11 - It should check gcc version number and specify -std accordingly. In gcc 4.7 -std=c++0x triggers C++14 support, but gcc 4.4 does not support -std=c++11.",NULL
DM-1214,"anaconda is too outdated to work with ginga in a notebook","We are using anaconda 1.8.0 which doesn't ship with PyQT4 (2.0.1 does, which is, I think, the latest).  This prevents us from running ginga in an iPython notebook  This is a ""minor"" issue as we are not (yet?) using ginga, but the functionality is sufficiently interesting that I think we should consider it as  a consideration when deciding whether to upgrade anaconda.",NULL
DM-1219,"lsstsw not working on MacOS X","There are a number of small issues with lsstsw running on MacOS X: - It tries to deploy the Linux version of anaconda - It goes into a seemingly infinite loop running ""yes"" while cloning each package - It does not correctly detect that all unit tests passed  In addition, there is a known problem that only buildbot can get simulation code from stash; other users should edit settings.cfg.sh in lsstsw to remove the SBASE entry from REPOSITORY_PATTERN. I don't propose to fix that (at least on this ticket), since the workaround is about as clean as any fix would be, and this package is supposed to be temporary.  The fixes are as follows: - lsstsw: I added code to bin/deploy on tickets/DM-1219 that detects Mac and downloads the Mac version of anaconda. - eups: the ""yes"" problem is a bug in eups. Mario will commit a fix. - lsst_build: build.py is mis-testing that 0 unit tests failed. I committed a fix on tickets/DM-1219.",NULL
DM-1220,"eups and MANPATH","Using eups makes it hard to use {{man}} command (this is on NCSA cluster machines): {code} $ source ""/u2/salnikov/STACK/eups/bin/setups.sh"" $ setup xrootd $ man man No manual entry for man $ echo $MANPATH /u2/salnikov/STACK/Linux64/xrootd/master-g2798bbf748/share/man:/u2/salnikov/STACK/Linux64/git/1.8.5.2/share/man {code}  It looks like for {{man}} to find system man pages {{$MANPATH}} needs to contain empty path (meaning extra colon like {{/whatever:}}). Would it be possible for eups to always add an empty path to {{MANPATH}} when doing {{setup}}? ",NULL
DM-1221,"wcslib is being statically linked on Mac OSX","(Ported from Trac [#2878|https://dev.lsstcorp.org/trac/ticket/2878])  RHL recently discovered and asked me to investigate a problem with the wcslib 4.14 install (also present in 4.17) on Mac OSX: there is no lib/wcslib.dylib installed, so the library gets linked statically (wcslib.a) instead of dynamically. Because wcslib contains static data, linking statically from multiple libraries results in multiple copies of that static data, which is dangerous.  I've reported this upstream to Mark Calabretta.  Here's the patch I'm using to fix this issue on the HSC distribution server: {code} diff --git a/configure b/configure index 719600e..b975062 100755 --- a/configure +++ b/configure @@ -9839,7 +9839,7 @@ if test ""x$ac_cv_c_compiler_gnu"" = xyes ; then      SONAME=""libwcs.$SHVER.dylib""      SHRLD=""$SHRLD -dynamiclib -single_module""      SHRLD=""$SHRLD -compatibility_version $SHVER -current_version $LIBVER"" -    SHRLN= +    SHRLN=""libwcs.dylib""       case ""$build_cpu"" in      powerpc*) {code}",NULL
DM-1222,"Add FunctorKey for full ellipses","Add a FunctorKey for full afw::geom::ellipses::Ellipse objects, delegating to the existing QuadrupoleKey and PointKey.  Need to add convenience methods to all three of these for adding fields to a Schema.",NULL
DM-1223,"new plugin for PSF shapelet approximation","Add a meas_base-style measurement plugin for approximating the PSF with shapelets.",NULL
DM-1224,"Add FunctorKeys for ShapeletFunction and MultiShapeletFunction","Add FunctorKey objects that know how to map ShapeletFunction and MultiShapeletFunction objects to records.",NULL
DM-1225,"add RadialProfile class to manage multi-Gaussian Sersic approximations","This issue transfers a new class from the HSC fork, RadialProfile, back to the LSST side.  This is used downstream in meas_multifit in the galaxy-fitting algorithms.",NULL
DM-1231,"LSE-69: Bring to Phase 3","Reflects work needed in Summer 2015",NULL
DM-1243,"improve coverage of sciSQL plugin undeployment","sciSQL plugin undeployment procedure doesn't remove scisql and scisql_test databases, and its shared lib (libscisl-*-.so) from plugin directory.",NULL
DM-1247,"Enable building qserv_distrib instead of qserv in buildbot","We now have a package called qserv_distrib, which depends on qserv and qserv_testdata, (see DM-1147 for details), so buildbot should build that, instead of qserv. Thanks.",NULL
DM-1248,"lsst_build prepre.py VersionDbGit.commit sets dirty, which is ignored","lsst_build prepre.py VersionDbGit.commit sets a local variable named dirty, which is ignored. Other methods set self.dirty; is that what ""commit"" should also be doing? ",NULL
DM-1288,"bin/rebuild uses flock, which is not available on MacOS","bin/rebuild uses flock, which is not available on MacOS. This prevents lsstsw from being used for a MacOS buildbot.  Presumably one can use shlock (which is available on MacOS) or python's flock implementation instead. But...  I confess I'm still not sure why this lock is useful. If the idea is to prevent different builds from colliding for a shared stack, then surely eups locks are more important that this flock, since one can build in several different ways, and this flock only prevents building by lsstsw? If only lsstsw users see a stack then flock will be reliable, but as far as I know that does not apply to buildbot at this time. Even if it did apply to buildbot, I would think buildbot would prefer not to even attempt to launch two instances of lsstsw at the same time.  On the other hand it's not worth over-thinking this because lsstsw not intended to be used long-term. So a quick fix is probably fine.",NULL
DM-1289,"Often new tickets are rejected with ""component does not exist""","Often when I create a ticket JIRA has just the component I want, I press tab to accept it and continue writing my ticket, but at the end my attempt to Create the ticket fails with a complaint that the component does not exist. It certainly does exist. I'm not sure what the problem is -- maybe JIRA is adding an extra whitespace or something. Whatever it is, it is very frustrating. I have to keep trying to enter the component until JIRA will take it.",NULL
DM-1290,"Re-cast SWUG as Sphinx doc as pathfinder experiment","Re-cast SWUG as a Sphinx document to explore migration to this utility for DM documentation. Describe issues with conversion, weaknesses and strengths of Sphinx as an authoring tool for user-facing documents. Full integration with the DM Stack source documentation will require a tool such as Breathe. ",NULL
DM-1292,"Design tests for processCcd","Design the tests that should be done: how are calibrated frames compared, how are output catalogs compared.  Insert identified data in appropriate locations (camera in obs_test for example).  Work moved into DM-1293",NULL
DM-1300,"Delete lsst-dev:/nfs/lsst2/lsstsw-backup after Oct 10th 2014","/nfs/lsst2/lsstsw-backup contains a backup of ~lsstsw, that I made while merging and deploying DM-1062. Once it's determined the merge didn't break anything, delete this backup.",NULL
DM-1301,"Document unset MACOSX_DEPLOYMENT_TARGET on MacOS 10.9","On MacOS 10.9 the default C++ standard library has full C++11 support unless you have environment variable MACOSX_DEPLOYMENT_TARGET set to something older. In that case anything built with ""make"" will use the older library and anything built with scons (which ignores that env var) will use the newer library, resulting in segfaults.  I suggest that the need to unset MACOSX_DEPLOYMENT_TARGET be documented in instructions, including (if on MacOS 10.9) instructions printed by lsstsw and newinstall.sh.",NULL
DM-1303,"lsstsw kicks you out of your terminal session if it needs redeployment","I tried ""git pull"" on an existing copy of lsstsw so it would use the newer eups. When I try to setup I get this sensible message: {code} error: eups not in /Users/rowen/LSST/lsstsw/eups/current; rerun bin/deploy. {code} but then I am kicked out of my terminal. This seems extreme; wouldn't it suffice to print the message and leave me at the prompt?",NULL
DM-1304,"Compiler warnings on clang++ 6","Apple's clang++ 6.0 compiler produces a number of compiler warnings about DbStorageImpl.cc.  One set of complaints is as follows: {code} src/DbStorageImpl.cc:87:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions]     dafPer::IntegerTypeTraits<1>::mysqlType = MYSQL_TYPE_TINY;                                   ^ src/DbStorageImpl.cc:74:29: note: explicitly specialized declaration is here     static enum_field_types mysqlType;                             ^ src/DbStorageImpl.cc:89:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions]     dafPer::IntegerTypeTraits<2>::mysqlType = MYSQL_TYPE_SHORT;                                   ^ src/DbStorageImpl.cc:74:29: note: explicitly specialized declaration is here     static enum_field_types mysqlType;                             ^ src/DbStorageImpl.cc:91:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions]     dafPer::IntegerTypeTraits<4>::mysqlType = MYSQL_TYPE_LONG;                                   ^ src/DbStorageImpl.cc:74:29: note: explicitly specialized declaration is here     static enum_field_types mysqlType;                             ^ src/DbStorageImpl.cc:93:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions]     dafPer::IntegerTypeTraits<8>::mysqlType = MYSQL_TYPE_LONGLONG;                                   ^ src/DbStorageImpl.cc:74:29: note: explicitly specialized declaration is here     static enum_field_types mysqlType;                             ^ src/DbStorageImpl.cc:100:35: warning: first declaration of static data member specialization of 'mysqlType' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions]     dafPer::BoundVarTraits<bool>::mysqlType = MYSQL_TYPE_LONG;                                   ^ src/DbStorageImpl.cc:80:29: note: explicitly specialized declaration is here     static enum_field_types mysqlType;                             ^ src/DbStorageImpl.cc:101:47: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<bool>::isUnsigned = true;                                               ^ src/DbStorageImpl.cc:81:17: note: explicitly specialized declaration is here     static bool isUnsigned;                 ^ src/DbStorageImpl.cc:103:47: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<char>::isUnsigned = false;                                               ^ src/DbStorageImpl.cc:81:17: note: explicitly specialized declaration is here     static bool isUnsigned;                 ^ src/DbStorageImpl.cc:104:54: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<signed char>::isUnsigned = false;                                                      ^ src/DbStorageImpl.cc:81:17: note: explicitly specialized declaration is here     static bool isUnsigned;                 ^ src/DbStorageImpl.cc:105:56: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<unsigned char>::isUnsigned = true;                                                        ^ src/DbStorageImpl.cc:81:17: note: explicitly specialized declaration is here     static bool isUnsigned;                 ^ src/DbStorageImpl.cc:106:48: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<short>::isUnsigned = false;                                                ^ src/DbStorageImpl.cc:81:17: note: explicitly specialized declaration is here     static bool isUnsigned;                 ^ src/DbStorageImpl.cc:107:57: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<unsigned short>::isUnsigned = true;                                                         ^ src/DbStorageImpl.cc:81:17: note: explicitly specialized declaration is here     static bool isUnsigned;                 ^ src/DbStorageImpl.cc:108:46: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<int>::isUnsigned = false;                                              ^ src/DbStorageImpl.cc:81:17: note: explicitly specialized declaration is here     static bool isUnsigned;                 ^ src/DbStorageImpl.cc:109:55: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<unsigned int>::isUnsigned = true;                                                       ^ src/DbStorageImpl.cc:81:17: note: explicitly specialized declaration is here     static bool isUnsigned;                 ^ src/DbStorageImpl.cc:110:47: warning: first declaration of static data member specialization of 'isUnsigned' outside namespace 'persistence' is a C++11 extension [-Wc++11-extensions] template<> bool dafPer::BoundVarTraits<long>::isUnsigned = false;                                               ^ ... {code}  The other is simply that ""adding 'int' to a string does not append to the string. I have a fix for this on tickets/DM-1302, if you want it. (C++11 offers to_string, which is simpler, but we're not there yet).",NULL
DM-1307,"Add ""Level 1 Data Products"" to description of the use of the ""Additional Data""","Requested by Chuck (and overlooked in my edits): Add reference to ""additional data"" section to make clear that the DQA referred to is in particular aimed at the Level 1 Data Products, and that that is the rationale for the data latency requested.",NULL
DM-1308,"Fix LSE-140 docgen template to have correct headers","Currently I'm using the ""OSS Template"" for LSE-140.  This means that the .rtf resulting from a docgen always has to have its headers edited to reflect the correct document name and handle.  This is fairly difficult for me to fix, I believe, without EA administrator permissions.",NULL
DM-1339,"Unit tests failing","When building on MacOS 10.9 with clang 6 two shapelet unit tests are failing.  {code} tests/testMatrixBuilder.py  .F..... ====================================================================== FAIL: testConvolvedCompoundMatrixBuilder (__main__.MatrixBuilderTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMatrixBuilder.py"", line 310, in testConvolvedCompoundMatrixBuilder     self.assertClose(numpy.dot(matrix1D, coefficients), checkVector, rtol=1E-14)   File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/utils/9.2+9/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/50 elements differ with rtol=1e-14, atol=2.22044604925e-16 0.175869366369 != 0.175869366369 (diff=1.99840144433e-15/0.175869366369=1.13629876856e-14)  ---------------------------------------------------------------------- Ran 7 tests in 0.201s  FAILED (failures=1) {code} and {code} tests/testMultiShapelet.py  ...F... ====================================================================== FAIL: testConvolveGaussians (__main__.MultiShapeletTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMultiShapelet.py"", line 88, in testConvolveGaussians     self.compareMultiShapeletFunctions(msf3a, msf3b)   File ""/Users/rowen/LSST/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 107, in compareMultiShapeletFunctions     self.compareShapeletFunctions(sa, sb, rtolEllipse=rtolEllipse, rtolCoeff=rtolCoeff)   File ""/Users/rowen/LSST/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 86, in compareShapeletFunctions     rtol=rtolEllipse)   File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/utils/9.2+9/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/5 elements differ with rtol=1e-14, atol=2.22044604925e-16 2.44929359829e-16 != 1.13310777953e-15 (diff=8.881784197e-16/1.13310777953e-15=0.783842839795)  ---------------------------------------------------------------------- Ran 7 tests in 0.243s  FAILED (failures=1) {code}",NULL
DM-1351,"Process IDs can be longer than 16 bits","I'm seeing two unit test failures in ctrl_events on MacOS with clang 6. K-T diagnosed this as assuming process IDs are only 16 bits when in fact they can be longer.  {code} tests/DestinationId.py  Traceback (most recent call last):   File ""tests/DestinationId.py"", line 59, in <module>     assert processId == os.getpid() AssertionError {code}  and  {code} tests/OriginatorId.py  Traceback (most recent call last):   File ""tests/OriginatorId.py"", line 56, in <module>     assert processId == os.getpid() AssertionError {code} ",NULL
DM-1353,"Revisit design of slot system in SourceCatalog","An RFC email I sent to lsst-data proposing a change in how flags are handled in catalog slots generated more calls for a wider redesign than I anticipated, and I think this reflects broad (but perhaps not acute) problems in how we currently approach ""substitutable"" algorithms.  I'd like to have a review of our current approach and brainstorm new ones, and I think the SAT is the right venue for that.  Mandatory participants include at least [~jbosch], [~price], and [~rhl].",NULL
DM-1356,"Make eups work with the ksh","setups.sh assumes that you can export a shell function.  Not only is this not in posix, it's also unnecessary.  Please modify mksetups to remove the ""export -f ..."" statements.  Also, switch the [[ ... ]]] statements to use standard shell syntax.   Once these changes are made, setups.sh should work with all posix-compliant bourne shells (sh, bash, dash, ksh, zsh, ...) ",NULL
DM-1357,"buildbot use of demo should use git branch preferences","When we need to make modifications to the demo script and/or its expected results to account for a change in the pipeline, it'd be nice to be able to test that with buildbot on a branch before merging to master.  Currently, however, buildbot always uses the master branch of the demo package, rather than the branches provided when starting the build.",NULL
DM-1361,"Build with C++11","This ticket is for changes to our build system so that C++11 is used by default, rather than as an option  Necessary changes include: - boost must be built with C++11 - sconsUtils must always build with C++11 (right now it is an option)  This ticket allows us to make the necessary changes and test them, then merge to master. A fancier alternative is to create a way to tell the whole build system which C++ standard we want to use (one that would apply to packages built using bjam and perhaps make) and then modify sconsUtils to use that. However, I suspect that's too much work and would likely be fragile unless we could figure out a way to enforce a rebuild of all necessary packages when the setting was changed.  A centralized way to get the desired CXXFLAGS for the user's compiler would be helpful.",NULL
DM-1365,"lsst-build should set or prompt for git author and e-mail if they're not specified","{code} Traceback (most recent call last):   File ""/home/rowen/lsstsw/lsst_build/bin/lsst-build"", line 40, in <module>     args.func(args)   File ""/home/rowen/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 651, in run     version_db.commit(manifest, args.build_id)   File ""/home/rowen/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 516, in commit     git.commit('-m', msg)   File ""/home/rowen/lsstsw/lsst_build/python/lsst/ci/git.py"", line 59, in commit     return self('commit', *args, **kwargs)   File ""/home/rowen/lsstsw/lsst_build/python/lsst/ci/git.py"", line 36, in __call__     raise GitError(retcode, cmd, stdout, stderr) lsst.ci.git.GitError: Command '('git', 'commit', '-m', 'Updates for build b343.')' returned non-zero exit status 128. {code}  More info: http://notboring.org/devblog/2012/12/git-commit-fail-with-exit-code-128-unable-to-auto-detect-email-address/",NULL
DM-1366,"lsstsw uses too many cores on MacOS","When running vanilla lsstsw on MacOS it uses all available cores. This make the compute unusable for anything else (even simple text editing). Would you consider using all but one or two cores, instead? I've been manually configuring the latter for scons. That leaves the machine unresponsive while building external packages, but its usable while building scons packages.",NULL
DM-1367,"setup command not found","A recent change to eups (not exporting the setup and unsetup functions, 119236b) broke lsst_build: in particular, the build file ""_build.sh"" that it creates will not work because ""setup"" is undefined.  To see this, use lsstsw to try to build a stack. It fails on cfistio.  Robert suggests that _build.sh source setups.sh",NULL
DM-1368,"Variable eups shadows package eups","In file build.py a variable named ""eups"" is used, but that code also needs access to the package named eups. Please rename to the variable to avoid confusion and shadowing.",NULL
DM-1369,"Test issue - ignore","Just checking to see who the automatic assignee is for fresh bugs ",NULL
DM-1370,"Update anaconda","The current lsstsw installs anaconda 1.8.0, which is so old that pip doesn't work with it (due to a security vulnerability in openssh). Can we update to the current version of anaconda?",NULL
DM-1371,"sconsUtils sets -std=c++xx in CCFLAGS instead of CXXFLAGS","sconsUtils state.py tests a compiler for C++11 support and sets a compiler flag accordingly. Unfortunately it sets CCFLAGS instead of CXXFLAGS (my foolish error).",NULL
DM-1373,"Contradictory build summary status provided on a forced build","Build 1051  log at: http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/1051/ has contradictory statements regarding build initiation.  Build Properties of: owners 	[u'rowen <rowen@localhost>', u'everyman <everyman@localhost>'] 	A build was forced by 'rowen <rowen@localhost>': check C++11 support, A build was forced by 'everyman <everyman@localhost>': force build  And   Forced Build Properties of: Forced Build Properties: Name	Label	Value branches 		feature/CATSIM-121-add-alex-abate-s-igm-code-into email 		Bryce Kalmbach < jbkalmbach@gmail.com> owner 		everyman <everyman@localhost> reason 		force build  This is possibly due to the new on-source-change scheduler added last month in addition to the Forced Scheduler already in place.",NULL
DM-1374,"afw unit test background.py fails with anaconda 2.1.0","afw unit test background.py fails when using anaconda 2.1.0 as follows: {code} tests/background.py  .......F.............. ====================================================================== FAIL: testBadPatch (__main__.BackgroundTestCase) Test that a large bad patch of an image doesn't cause an absolute failure ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/background.py"", line 602, in testBadPatch     self.assertEqual(np.mean(bkgdImage[0:100, 0:100].getArray()), defaultValue) AssertionError: 10.000002 != 10  ---------------------------------------------------------------------- Ran 22 tests in 4.373s  FAILED (failures=1) {code}  (For the record, I think this is why the test fails on tioga ~rowen/lsstsw: I updated the anaconda to fix an fftw build failure)",NULL
DM-1378,"Prefix naming convention for DM repositories","An excellent suggestion from Chris Walter, which I endorse - we can either to it as part of the repo re-org, or just go ahead and do it as part of the full github transition (since there will have to be some handover downtime I assume?)   {quote} Not a question but a few comments from when I was thinking about this before.  First, I think it would be really great if everything transitions to GitHub.  Because of the way things are setup there, without a compact view of all of the repos I think some sort of good namespace for repo names is key.  Then you can just start to type the name of the repo in the search box and the github site immediately starts to narrow down the repos to ones with that name.  For the sims repos this ins't a problem since they already start with ""sims_"".  But the DM stuff doesn't.  So, if DM transitions too I think DM needs some sort of similar name space prefix.  Otherwise we will probably want separate organizations (LSST_Sims, LSST_DM etc) but I think one organization with descriptive name spaces is better. {quote}",NULL
DM-1379,"End-to-end demo should fail on inexact comparison to benchmark file.","This ticket has been split off from DM-1358 which was usurped by a bug fix embedded within the story details.  That bug fix was already pushed into the devenv/buildbot/DM-1358 branch.  So this totally separate task is being moved to this new Issue.  _______________________________________________________ The output of demo2012 results in an output file which is compared against a benchmarked file. Currently the comparison allows a deviation from the benchmark based ""on the the number of digits in the significands used in multiple precision arithmetic""; that number is currently set to 11.  An example using that setting is: @ Absolute error = 5.9973406400e-1, Relative error = 9.1865836000e-4 ##2566 #:25 <== 29.7751550737 ##2566 #:25 ==> 29.7478269835  Additionally, the current use returns: a 'Fatal' error if the code itself fails to execute correctly; a ""Warning' error if the any of the benchmarked quantities do not meet the comparison criteria; 'Success' if the comparison meets all criteria.  It is recommended that the comparison 'should be set to zero tolerance until we get some breakages and people discover what level of changes occurs as part of ""everyday"" development.  Those breakages should be on branch builds, of course, not on master.'  This Issue will:   *  reset the comparison metric to EQUALS. ",NULL
DM-1381,"fix testFacade - make the location random","It is supposed to test in random location, but seeding is not handled.",NULL
DM-1382,"Work on ideas for the deblender","Work on the deblender.  Initially this will be experiments to define the scope of the full project.  When we have more concrete ideas we will probably want to replace this epic with one called ""Deblender"".  JK: In PMCS this is Swinbank J ",NULL
DM-1385,"Process Objects in Multiple Bands/Epochs","In order to provide useful multi-band catalogues we need to generate a list of objects which are detectable in *any* band (where ""band"" may include a chi^2 image).  In order to deblend these objects we need to generate a merged set of Footprints and Peaks so that we can define what we mean by families.  JK: In PMCS this is Swinbank J ",NULL
DM-1391,"Compress uuid into mysql-table-name-compatible format","Discussed at hangout Oct 16, 2014. It'd be useful to embed uuid (or some other unique id) into each table name (and database name?), to avoid confusions when we do create/delete/create. Issue: mysql has a limit: 64 bytes, and uuid is 48 bytes, and it'd be nice to concatenate with actual table name, which puts us way to close to the 64 byte limit. We can compress using base64, but we don't want to introduce some weird characters in table names, which would force us to quote the names.",NULL
DM-1392,"Buildbot copies successful logs to FailedLogs","[Line 142 of lsstswBuild.sh|https://github.com/LSST/buildbot/blob/master/scripts/lsstswBuild.sh#L142] has an incorrect test that looks at the (nonexistent) output of {{grep}} instead of its exit status.  As a result, the build logs for any package that was built in the current run are copied into {{FailedLogs}}, not just those for failing packages.",NULL
DM-1393,"Add support for special characters in database and table names","At the moment special characters will confuse qserv. Example, if I create a database called `a@b*c$d`, and a table 'x', and I'll run ""select * from x"", I'll get:  {code} ERROR 4110 (Proxy): Qserv error: ""NoSuchDb:Database 'a@b*c$d' does not exist."" {code}  There are two ways to deal with it: a) simpler: detect that there is a special character in the name, and return a non-confusing error message saying that such name is disallowed b) proper: implement proper support, e.g., make sure database and table names are properly quoted everywhere.  In the short term, until this is fixed, we will put in the documentation covering Qserv limitations that special characters are disallowed.  ",NULL
DM-1397,"Traceback when trying to cancel a job","I tried to cancel a buildbot build (with permission) and got a traceback:  {code} web.Server Traceback (most recent call last): exceptions.KeyError: 'owner' /usr/lib64/python2.6/site-packages/Twisted-13.2.0-py2.6-linux-x86_64.egg/twisted/internet/defer.py:1099 in _inlineCallbacks 1098            else: 1099                result = g.send(result) 1100        except StopIteration: /usr/lib/python2.6/site-packages/buildbot/status/web/build.py:97 in performAction 96        authz = self.getAuthz(req) 97        res = yield authz.actionAllowed(self.action, req, self.build_status) 98 /usr/lib/python2.6/site-packages/buildbot/status/web/authz.py:142 in actionAllowed 141                if self.authenticated(request): 142                    return defer.succeed(check_authenticate(None)) 143                elif passwd != ""<no-password>"": /usr/lib/python2.6/site-packages/buildbot/status/web/authz.py:135 in check_authenticate 134                def check_authenticate(res): 135                    if callable(cfg) and not cfg(self.getUsername(request), *args): 136                        return False /usr/local/home/buildbot/master/master.cfg:368 in canStopBuild 367     buildInfo = build_status.getProperties() 368     owner = buildInfo[""owner""] 369     if owner.startswith(username) : /usr/lib/python2.6/site-packages/buildbot/process/properties.py:79 in __getitem__ 78        """"""Just get the value for this property."""""" 79        rv = self.properties[name][0] 80        return rv exceptions.KeyError: 'owner' {code}  Maybe I don't have permission to cancel it, but if so, the error handling could be improved.",NULL
DM-1398,"DateTime fails for old British dates","The {{dateTime.py}} test case fails for machines in the ""Europe/London"" time zone.",NULL
DM-1399,"Buildbot end-to-end demo should be pulled in from appropriate Git branch","Use of the Git repo: contrib/demos/lsst_dm_stack_demo  by Buildbot should acquire that repository from a user specified branch. Currently it acquires the repo only from the master branch.",NULL
DM-1401,"Compiler warning about memset in meas_modelfit","I see the following warnings when building meas_modelfit on MacOS 10.9 with clang 6: {code} python/lsst/meas/modelfit/modelfitLib_wrap.cc:4703:9: warning: destination for this 'memset' call is a pointer to dynamic class 'lsst::afw::geom::ellipses::Ellipse'; vtable pointer will be       overwritten [-Wdynamic-class-memaccess]         memset(v_def,0,sizeof(Type));         ~~~~~~ ^ python/lsst/meas/modelfit/modelfitLib_wrap.cc:4728:62: note: in instantiation of member function 'swig::traits_as<lsst::afw::geom::ellipses::Ellipse, swig::pointer_category>::as' requested       here     return traits_as<Type, typename traits<Type>::category>::as(obj, te);                                                              ^ python/lsst/meas/modelfit/modelfitLib_wrap.cc:5285:15: note: in instantiation of function template specialization 'swig::as<lsst::afw::geom::ellipses::Ellipse>' requested here         return swig::as<T>(item, true);                      ^ python/lsst/meas/modelfit/modelfitLib_wrap.cc:5520:42: note: in instantiation of member function 'swig::SwigPySequence_Ref<lsst::afw::geom::ellipses::Ellipse>::operator Ellipse' requested       here       seq->insert(seq->end(),(value_type)(*it));                                          ^ python/lsst/meas/modelfit/modelfitLib_wrap.cc:5542:6: note: in instantiation of function template specialization 'swig::assign<swig::SwigPySequence_Cont<lsst::afw::geom::ellipses::Ellipse>,       std::__1::vector<lsst::afw::geom::ellipses::Ellipse, std::__1::allocator<lsst::afw::geom::ellipses::Ellipse> > >' requested here             assign(swigpyseq, pseq);             ^ python/lsst/meas/modelfit/modelfitLib_wrap.cc:5597:47: note: in instantiation of member function 'swig::traits_asptr_stdseq<std::__1::vector<lsst::afw::geom::ellipses::Ellipse,       std::__1::allocator<lsst::afw::geom::ellipses::Ellipse> >, lsst::afw::geom::ellipses::Ellipse>::asptr' requested here         return traits_asptr_stdseq<std::vector<T> >::asptr(obj, vec);                                                      ^ python/lsst/meas/modelfit/modelfitLib_wrap.cc:4620:32: note: in instantiation of member function 'swig::traits_asptr<std::__1::vector<lsst::afw::geom::ellipses::Ellipse,       std::__1::allocator<lsst::afw::geom::ellipses::Ellipse> > >::asptr' requested here     return traits_asptr<Type>::asptr(obj, vptr);                                ^ python/lsst/meas/modelfit/modelfitLib_wrap.cc:34938:18: note: in instantiation of function template specialization 'swig::asptr<std::__1::vector<lsst::afw::geom::ellipses::Ellipse,       std::__1::allocator<lsst::afw::geom::ellipses::Ellipse> > >' requested here     res4 = swig::asptr(obj3, &ptr);                  ^ python/lsst/meas/modelfit/modelfitLib_wrap.cc:4703:9: note: explicitly cast the pointer to silence this warning         memset(v_def,0,sizeof(Type));                ^                (void*) 10 warnings generated. {code}",NULL
DM-1402,"Please add meas_modelfit to the list of components","Please add meas_modelfit to the list of components",NULL
DM-1403,"meas_modelfit requires gfortran to build","meas_modelfit provides a unit test written in fortran and and the build fails on MacOS if gfortran is not available. I don't believe gfortran is a permitted dependency. Certainly it does not show up on <https://confluence.lsstcorp.org/display/LSWUG/Prerequisites> for MacOS.  On the other hand, it does show up as a dependency for all the unix variants, which is odd.  If gfortran is permitted, then this ticket should probably be against that web page. If it snot permitted, please fix the package so that the package can be built without gfortran. Several options come to mind, including: - Convert that code to C code - Skip the test if gfortran is not available - If possible, pre-compute the output from the fortran test, instead of building and running it as part of the build",NULL
DM-1406,"Use LSST-specific include guards","I just ran into an issue with include guards when working in a package with both afw and [GalSim|https://github.com/GalSim-developers/GalSim] --- both use the same include guard names (e.g., {{IMAGE_H}}, {{ANGLE_H}}), which means some headers I needed weren't being included.  I suggest that these should be prepended with the package name and path (e.g., {{AFW_IMAGE_IMAGE_H}}, {{AFW_GEOM_ANGLE_H}}) or something similar to ensure they are distinguished.  This might be something for the SAT to discuss and set a policy on.  See [GalSim #608|https://github.com/GalSim-developers/GalSim/issues/608] for the flip side.",NULL
DM-1425,"""scons"" should build everything that ""scons install"" does","It turns out that ""scons install"" is more aggressive about hunting for SConscript files than ""scons"". I think this is a serious misfeature, as it makes it hard to debug build issues (they may show up when lsstsw builds the package but not when a user tries to do the same build directly).  I admit that ""scons install"" has to do some things more than ""scons"", but I don't believe that should include building targets using SConscript files.  See DM-1403 for an example of how this bit me.",NULL
DM-1430,"Buildbot's lsstswBuild.sh does not differentiate between errors arising prior to build start or during  build.","lsstswBuild.sh is the interface between Buildbot and the LSST build suite {lsstsw, lsst-build}.  Currently lsstswBuild.sh does not attempt to determine at what point in the rebuild process that the error occurred. It assumes that the error occurred during the actual build instead of during the setup leading to the build  (i.e. exit on multiple builds in progress, exit on failure of git access during the dependency tree build).  This Ticket is to  ensure that such pre-build failures are recognized and the error message reported is indicative of the failure cause.",NULL
DM-1435,"Test when Base daemons are down","Test the system when Base daemons are down to make sure the rest of the system works as expected.",NULL
DM-1436,"Test when Replicator Nodes are down","Test the system when Replicator Nodes are down to make sure the rest of the system works as expected.",NULL
DM-1437,"Test when Distributor Nodes are down","Test the system when Distributor Node daemons are down to make sure the rest of the system works as expected.",NULL
DM-1438,"Test when Archive daemons are down","Test the system when Archive daemons are down to make sure the rest of the system works as expected.",NULL
DM-1450,"Adjust timing of components of AP Simulator","Components of the AP simulator need to have their timing adjusted in a way that accurately reflects the running system.   This includes simulated processing time for jobs and the cadence at which messages are set.",NULL
DM-1451,"Adjust cadence of messages sent by automated OCS message transmitter","The automated OCS message transmitter currently sends out messages at regular intervals.  This was fine for testing, but this needs to be adjusted so the cadence of messages being sent by the automated script more accurately reflects how messages will come out of of the OCS.",NULL
DM-1457,"Decide on repo organization for data access services","(K-T writing for Jacek) There will be multiple distinct (sometimes interacting) services running in the Archive and DAC to provide access to image data, other data files, catalog data, and metadata.  Should these live in individual repositories or be grouped together?  Note that they will definitely live in separate products/packages; this is only about git.  Arguments for individual: * Simultaneous changes across multiple services will be rare. * Tagging each service individually is desirable. * History is ""cluttered"" with changes irrelevant to the service being worked on. * More frequent rebasing is necessary as master merges happen faster.  (Could be ameliorated by working mostlly in a fork and having less-frequent pull requests, but poses a greater threat to system integration.) * Repo == product/package is easier to manage and understand.  Arguments for combined: * Fewer repos to track and manage. * Clear top-level structure for LSST code. * See [Winter2015 Package Reorganization Planning|https://confluence.lsstcorp.org/display/DM/Winter2015+Package+Reorganization+Planning] and recent [SAT meeting notes|https://confluence.lsstcorp.org/display/DM/SAT+Meetings]",NULL
DM-1458,"SAT drill-down into completeness and systematics maps","Eli Rykoff at SLAC brought up the issue of completeness and systematics maps: how are they to be generated, how are they to be stored (compressed, since per-pixel values would be much too large)?  We have a little bit of a mention of fake objects in the [difference imaging|https://confluence.lsstcorp.org/display/DM/Level+1+Difference+Imaging+and+Moving+Object+Processing] design, but a detailed treatment of this is needed.",NULL
DM-1459,"Agility in Commissioning and early Production","Aaron Roodman at SLAC points out that we should expect to need to iterate quickly in Commissioning and early Production as we refine algorithms to meet the Science Requirements.    He feels these iterations are likely to involve publishing results to the community and obtaining feedback (including code fixes).  Trading off coverage in area or time and ""certification-level"" SDQA in favor of more and faster iterations on unofficial data subsets may be worth it, even to the extent of giving up a DR or two.",NULL
DM-1460,"Additional unit test conversion from meas_algorithms","This is a catch-all issue for any unit tests which got left in meas_algorithms.  It is hard in some cases to tell if a given test is intended to test something in meas_base (an algorithm or the framework), or something in meas_algorithms.  DM-1162 converted most of the obvious tests to meas_base, with the know exception of the ""forced"" tests in measureSources.py.",NULL
DM-1465,"Transfer code from design prototype","The design on the branch for DM-829 already has a lot of useful code - base classes, utility classes, and implementations; this issue is for bringing that over after the design review is complete.",NULL
DM-1466,"Convert simple C++ algorithms to new API","Convert all C++ algorithms that use the old C++ Algorithm concept.  This does not include PsfFlux or SdssShape (which will have been done already on DM-829/DM-1465), or ApertureFlux and CircularApertureFlux (which do not use the old algorithm concept).  This should be done in much the same way as PsfFlux - there's no need for a public Result or ResultKey object for any of these algorithms.  If any of them return outputs that aren't accounted for in any of the existing Result classes, the Algorithm itself should just hold a Key for them and use it to set the field in the record directly.",NULL
DM-1467,"Convert multiple-aperture flux algorithms","Convert the ApertureFluxAlgorithm and CircularApertureFluxAlgorithm classes to use the new C++ algorithm API.",NULL
DM-1468,"Remove old algorithm interface and utility classes","Remove the old classes that were used to wrap algorithms using the old C++ interface.  This includes (at least):  - The Input classes  - The Component classes and Result templates in Results.h  - The ComponentMapper classes and ResultMapper templates in ResultMappers.h  - The old WrappedSingleFrameAlgorithm and WrappedForcedPlugin classes.  We should double-check that meas_multifit still builds after this change; I'm pretty certain the CModel algorithm wrappers don't use any of the old stuff, but I'm not completely positive.",NULL
DM-1470,"improve check for configuration variables in mysql-proxy","QSERV_RPC_PORT value must be checked in mysql-proxy instead of in init.d scripts. Same thing for QSERV_UNIX_USER, if doable.",NULL
DM-1472,"buildbot/lsstsw handling of anaconda is confusing","Buildbot sources /home/lsstsw/bin/setup.sh, which manually adds /home/lsstsw/anaconda/bin to PATH.  IMO, that's confusing: we should be using EUPS any time we modify an environment variable, like PATH.  However, the real problem is that buildbot also tags whatever version of anaconda is current with the buildbot run's tag with the buildbot run's bXXX tag (I think; until I declared anaconda 2.1.0 today, there was only one declared version of anaconda, so I'm not sure if the current tag played a role).  In any case, buildbot was certainly declaring an anaconda version other than the one it was using with the buildbot run's tag.",NULL
DM-1474,"Port obs_subaru to new CameraGeom","Testing and improving the deblender is something that really needs to be done on real data of approximately LSST quality, and that makes HSC data a good choice.  But first we need to fix the bitrotted obs_subaru package so we can use the LSST stack (not its HSC fork) to process that data.",NULL
DM-1478,"Uniquify file names in tests","[Build #1582|http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/1582] appears to have failed because both {{tests/image.py}} and {{tests/mask.py}} write {{foo.fits}}.  The temporary files written by tests should have unique filenames.",NULL
DM-1479,"xpa install fails using parallel make ","The installation of xpa in the {{~lsstsw/stack}} stack on {{lsst-dev}} is missing various shared libraries.  Looking at the build log reveals that the install process attempted to copy them into the {{lib}} directory before it was created.  This is because the {{lib}} directory is created by the {{install-binaries}} installation step, not the {{shlib_install}} step.  While the {{Makefile}} could be made to be parallelizable, it is simpler to just turn off the parallelism in {{eupspkg.cfg.sh}}.",NULL
DM-1483,"Threads don't always die in ArchiveDMCS","I'm seeing an issue in the ArchiveDMCS where the number of threads continue to increase until the limit is exhausted, and I get:  error: can't start new thread  ",NULL
DM-1484,"Rework Terminator Thread","The Terminator thread is unnecessarily complex.   There's a simpler way to do what it does using Timer.",NULL
DM-1485,"boost install via eups distrib is broken","Installing boost using {{eups distrib install}} simply copies the untarred file, as it doesn't have the right builder.  How to reproduce: {code:hide-linenum} export EUPS_PKGROOT=http://sw.lsstcorp.org/eupspkg/| eups distrib install boost master-gbcb3a2454f --noclean setup boost master-gbcb3a2454f ls $BOOST_DIR {code}  Then, for more information, look at: {code:hide-linenum} less $EUPS_PATH/EupsBuildDir/boost-master-gbcb3a2454f/build.log {code}  This seems to be the telling sequence, at the end: {code:hide-linenum} + ./ups/eupspkg VERBOSE=1 FLAVOR=Linux64 config + ./ups/eupspkg VERBOSE=1 FLAVOR=Linux64 build eupspkg.build: no build system detected; assuming no build needed. + ./ups/eupspkg VERBOSE=1 FLAVOR=Linux64 install eupspkg.install: Copied the product into '/home/jbosch/eups/stack/Linux64/boost/master-g2e1a7fad74' {code}",NULL
DM-1486,"boost has unwanted code in eupspkg.cfg.sh","While working on DM-1361 Mario committed a change that I was supposed to remove, but I didn't. This ticket is to fix that up.",NULL
DM-1492,"scons gets confused by symlinks in root directory","SCons gets confused if you do: {{setup -r $(pwd)}} because that expands symlinks differently from how it does.  Fix is already present.",NULL
DM-1499,"make it clear that detectors do not know about pupil coordinates","The documentation in cameraGeom does not make it clear that PUPIL coordinates are special, i.e. that detectors do not know about them, and that you need to use the camera to transform camera points on the detector into pupil coordinates.  Either including this in the error message thrown when one asks for detector.getCenter(PUPIL) or adding it to the doxygen documentation should be a sufficient fix.",NULL
DM-1500,"Review LSE-66","Review LSE-66 for the CCS-DM-OCS joint meeting.",NULL
DM-1501,"Review LSE-67","Review LSE-67 for the CCS-DM-OCS meeting",NULL
DM-1502,"Review LSE-71","Review LSE-71 for the CCS-DM-OCS meeting.",NULL
DM-1503,"Write up of job start up issues","A utility which sends messages  at the cadence which the real OCS will use revealed that it’s possible that messages will arrive before the HTCondor jobs are ready to receive them.   Replicator jobs missed their messages and were not able to transmit the information given by a startReadout from the OCS.   The replicator job continues to wait until it eventually is killed off by it’s own internal timer.  During this time, the HTCondor job slot is still being utilized until the job is killed.    The distributor node is waiting for this information, and has worker jobs waiting for it to receive the raft (which is split up for each job).   The worker jobs also eventually are killed off, but also take up HTCondor slots during this time. Since old jobs in each of the HTCondor queues have not yet vacated, new jobs coming in are similarly delayed.  Now, instead of just one replicator job being delay another replicator job will also be delayed.  It’s likely that the second delayed job will be waiting on a different raft, causing even more workers to be delayed in the HTCondor queue for worker jobs.    This continues to cascade as time goes on.  I'm working on a write up that describes the issue and some alternatives we need to explore.",NULL
DM-1504,"Install SAL software ","Install SAL SDK software on a CentOS 32-bit OS.",NULL
DM-1507,"Add info about creating new repo to DM Dev Guide","It'd be good to add some information about creating new repo to the DM Developer Guide. See relevant discussion in comments for DM-1491.",NULL
DM-1511,"Document design prototype classes","I'm merging DM-1465 without completing the documentation for some classes in order to keep that merge from blocking Perry.  Will add the documentation later with this issue.",NULL
DM-1512,"Remove the offer to install git from newintall.sh","This came up in HipChat.  Here is the transcription: {quote} [6:29 AM] Simon Krughoff: @frossie Since we are working on newinstall.sh anyway, is it worth re-evaluating whether it should be asking to install git?  Compiling git in the absence of libssl (absent on many distros by default) causes the build to fail when fetching https repos. [6:29 AM] Tim Jenness: is there any reason not to simply make git a prereq and have people install it using whatever package manager they have? Or are you still supporting operating systems that have a git that is too old?  		6:30 AM 		Robert Lupton joined the room [6:30 AM] Simon Krughoff: I think the idea was the latter. 		I think at one point the installer needed git 1.8, but I don't think that's the case anymore. [6:31 AM] K-T Lim: It's not clear to me that users (as opposed to developers) should have to have git. [6:31 AM] Tim Jenness: I imagine Snow Leopard comes with an ancient git but I also imagine you don't want to be supporting Snow Leopard anyhow 		are we talking users or people building the software from source? [6:32 AM] Simon Krughoff: This is for anyone using newinstall.sh [6:32 AM] K-T Lim: Right now, users are a subset of the people building the software from source 		Because eups distrib install does the latter [6:33 AM] Tim Jenness: so it's building the software. So they need developer tools anyway. Why not git. It's trivial to use rpm or whatever to install git. [6:33 AM] K-T Lim: Many things are less trivial for people without root [6:33 AM] Simon Krughoff: That's true. 		Don't all distros come with git by default? [6:34 AM] Tim Jenness: I suppose it depends what ""default"" means. OS X does. If you spin up a ubuntu digital ocean minimalist VM does that have git? [6:35 AM] Simon Krughoff: We're really just talking about distros that are too old to have a modern git.  Since we are only supporting Mountain Lion and newer, I don't see why we can't draw a line somewhere reasonable for CentOS too. 		@timj That's a good point, but we already have a laundry list of prereqs that have to be installed on minimal distros anyway. [6:36 AM] K-T Lim: Anyway, I think the bottom line is that Simon is probably right and we should remove the git install from newinstall.sh [6:36 AM] Tim Jenness: Yes. That's what I meant about my earlier comment. You already aren't trying to install everything from scratch [6:37 AM] K-T Lim: We should test the install without any git available 		For developers, git should be a ""system dependency"" with a minimum version. [6:37 AM] Tim Jenness: As an aside, I'd really like eups to be able to ""git clone"" the repos during the build rather than download tar balls. It would make it a lot easier when I'm sitting in a build tree wondering whether a certain patch was installed or not. {quote}",NULL
DM-1513,"Creating a CameraMapper with outputRoot=root creates an unwanted _parent link","Creating a CameraMapper with outputRoot specified and set equal to root causes _parent to be created, which points to its containing directory.  I have a simple fix which I'll post once I have a ticket number.",NULL
DM-1522,"Fix error reporting from set command ","Currently running commands like  SET @@session.autocommit = 1;  results in: {code} +------------------------------------------+ | no-field-name                            | +------------------------------------------+ | Ignoring meaningless command (in Qserv). | +------------------------------------------+ {code}  SET should usually not return anything, so we should find a better way of reporting that the command is ignored (or even better, implement it properly...) ",NULL
DM-1523,"fix interaction of slots with SafeCentroidExtractor","SafeCentroidExtractor doesn't work properly for algorithms that are centroiders themselves (especially true for anything that could be used in the centroid slot).  Fixing this also requires setting up the slots when the Schema is defined (in the measurement task's constructor), not when the SourceTable object is created, which is something we probably should have done back when slot definitions were moved from SourceTable to Schema.",NULL
DM-1535,"Test OCS Middleware","Run basic tests to see that the install works properly.",NULL
DM-1536,"Test case gives Footprint.getShape() invalid arguments","{{Footprint.getShape()}} takes no arguments. However, if {{display}} is {{True}}, then {{afw/tests/footprint1.py}} attempts to run  {code:python} shape = foot.getShape(cen) {code}  with predictable results.  The (trivial) fix is forthcoming.",NULL
DM-1537,"lsst::afw::display::writeBasicFits can't handle boost::uint64_t","The following code  {code:python}         ds = afwDetect.FootprintSet(self.ms, afwDetect.Threshold(10))         objects = ds.getFootprints()          idImage = ds.insertIntoImage(True)         if display:             ds9.mtv(idImage, frame=2) {code}  (from {{afw/tests/footprint1.py}}) dies as follows  {code} ====================================================================== ERROR: testFootprintSetImageId (__main__.FootprintSetTestCase) Check that we can insert a FootprintSet into an Image, setting relative IDs ---------------------------------------------------------------------- Traceback (most recent call last):   File ""footprint1.py"", line 1000, in testFootprintSetImageId     ds9.mtv(idImage, frame=2)   File ""/raid/swinbank/src/afw/python/lsst/afw/display/ds9.py"", line 516, in mtv     _mtv(data, wcs, title, False)   File ""/raid/swinbank/src/afw/python/lsst/afw/display/ds9.py"", line 553, in _mtv     raise e NotImplementedError: Wrong number or type of arguments for overloaded function 'writeFitsImage'. (....) {code}  when {{display}} is {{True}}.  Fix is forthcoming.",NULL
DM-1546,"Propose to developers a unified way to build Qserv and  dependencies","partition, Qserv, qserv_testdata and possibly scisql doesn't relies on the same eups build procedure, i.e. commands {{setup -r .; scons; scons install}} have not exactly the same behaviour. This could be solved or at least, documented.",NULL
DM-1549,"lsst.afw.math.background.BackgroundList does not survive persist/unpersist","in obs_test, at least, the data product ""calexpBackground"" does not seem to survive the round trip of using the butler to persist and unpersist it.  I wrote this bit of obs_test to match obs_lsstSim, so I suspect the same issue exists there.  To see the problem, uncomment these lines in pipe_tasks test/testProcessCcd.py on tickets/DM-1293 (unless that has been merged to master). Then run the test using a recent master of obs_test. {code}             # calexpBackground = butler.get(""calexpBackground"", dataId)             # self.assertGreaterEqual(len(calexpBackground), 1)             # bg0Arr = calexpBackground[0][0].getImageF().getArray() {code} The call getImageF() will fail with a traceback that includes the explanation ""nxSample and nySample have too few points for requested interpolation style"". ",NULL
DM-1550,"The default home page on JIRA should show all tickets I'm reviewing","The default home page of JIRA shows tickets assigned to me, which is great, but not tickets I'm reviewing. There's a link for that, but I often forget to follow it.  If it's hard to show the tickets themselves, then a decent fall-back would be to show a count of *count* of the tickets I'm reviewing. If it's > 0 I'd know to click on the link.  Basically by default I think the home page should show users the tasks they are responsible for.",NULL
DM-1554,"Ensure mysql JDBC driver is properly supported","In DM-1539 we added basic support for jdbc, by redirecting some internal queries to the local mysqld used by the proxy. This includes queries that involve ""show variables"", ""select @@session.auto_increment_increment"", and ""set <various values>"". In a long run, we need to properly support these queries.",NULL
DM-1555,"Write simple python script to test API.","Write a simple variation of the generated (via: salgenerator hexapod_Actuators.idl sal python) script.",NULL
DM-1558,"If a command-line task fails, results cannot be returned","If a command-line task is run with doRaise false and doReturnResults true, and the task fails, a traceback results due to an uninitialized error.  The fix is trivial, though it requires coming up with a suitable value for the result field of the returned Struct. I propose to use None, but an empty Struct would also be a good choice.",NULL
DM-1559,"exceptions raised by CmdLineTasks result in unhelpful messages","An exception thrown by a command-line Task can cause {{TaskRunner.\_\_call\_\_}} to proceed all the way to its return statement without ever setting {{result}}, causing a confusing {{UnboundLocalError}}.  One possibility would be to return {{None}} instead, within the {{except}} block.",NULL
DM-1565,"sconsUtils should not report chat about the C++ compiler when invoked with -Q","{quote} $ scons -Q opt=3 lib python tests                  CC is clang version 6.0 Checking for C++11 support C++11 supported with '-std=c++11' $ {quote} With -Q the three lines about the C and C++ compiler should be omitted.  They are currently generated by log.info() commands.",NULL
DM-1583,"Add ability to cast (pointers to) PropertySet to PropertList","Before fixing DM-1524 (which is running into problems with boost::persistence) please add the ability to cast the return value of getMetadata to PropertyList ",NULL
DM-1592,"Protect meas_astrom from crashing if flags in badFlags listField do not exist","meas_astrom is crashing if flags in the badFlags listField parameters do not exist in the schema of the sources passed to the astrometry solver.  Should check every entry in the badFlags listField and ignore it if it is not present in the source table schema. Should also print a warning.",NULL
DM-1593,"Adapt and customize obs_cfht in order to be able to run any pipe_tasks","obs_cfht should be adapted in order to be able to run the standard pipe_tasks. ",NULL
DM-1599,"Look at persistence for PropertyList","Please mark this complete when you get a chance to look at DM-1524",NULL
DM-16,"Move existing algorithms from meas_algorithms to meas_base and refactor to the new API","See https://dev.lsstcorp.org/trac/ticket/3164",NULL
DM-1605,"Remove qserv-admin warning messages","If they are run immediatly after {qserv-configure.py --all}, integration tests display next warning messages:  {code} 2014-12-03 00:52:16,853 INFO Running : qserv-admin.py -c localhost:12181 -v 20 -f /home/fjammes/qserv-run/2014_11.0/var/log/qadm-case03.log 2014-12-03 00:52:17,712 INFO    stdout : -- qserv > qserv > qserv > ~ qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > qserv > ~ ~  -- 2014-12-03 00:52:17,713 INFO CSS meta successfully loaded for db : qservTest_case03_qserv 2014-12-03 00:52:17,713 INFO Configuring Qserv mono-node database 2014-12-03 00:52:17,714 INFO Non empty data chunks list : []  2014-12-03 00:52:17,716 INFO AvgForcedPhot table for empty chunk created /home/fjammes/stack/Linux64/mysqlpython/1.2.3+11/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py:75: Warning: Unknown table 'qservTest_case03_qserv__AvgForcedPhot'   while self.nextset(): pass 2014-12-03 00:52:17,718 INFO meta table created and loaded for AvgForcedPhot 2014-12-03 00:52:17,720 INFO AvgForcedPhotYearly table for empty chunk created /home/fjammes/stack/Linux64/mysqlpython/1.2.3+11/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py:75: Warning: Unknown table 'qservTest_case03_qserv__AvgForcedPhotYearly'   while self.nextset(): pass 2014-12-03 00:52:17,722 INFO meta table created and loaded for AvgForcedPhotYearly 2014-12-03 00:52:17,724 INFO RefObject table for empty chunk created /home/fjammes/stack/Linux64/mysqlpython/1.2.3+11/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py:75: Warning: Unknown table 'qservTest_case03_qserv__RefObject'   while self.nextset(): pass 2014-12-03 00:52:17,726 INFO meta table created and loaded for RefObject 2014-12-03 00:52:17,728 INFO RunDeepSource table for empty chunk created /home/fjammes/stack/Linux64/mysqlpython/1.2.3+11/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py:75: Warning: Unknown table 'qservTest_case03_qserv__RunDeepSource'   while self.nextset(): pass 2014-12-03 00:52:17,729 INFO meta table created and loaded for RunDeepSource 2014-12-03 00:52:17,731 INFO RunDeepForcedSource table for empty chunk created /home/fjammes/stack/Linux64/mysqlpython/1.2.3+11/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py:75: Warning: Unknown table 'qservTest_case03_qserv__RunDeepForcedSource'   while self.nextset(): pass 2014-12-03 00:52:17,733 INFO meta table created and loaded for RunDeepForcedSource 2014-12-03 00:52:17,736 INFO DeepForcedSource view for empty chunk created /home/fjammes/stack/Linux64/mysqlpython/1.2.3+11/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py:75: Warning: Unknown table 'qservTest_case03_qserv__DeepForcedSource'   while self.nextset(): pass 2014-12-03 00:52:17,738 INFO meta table created and loaded for DeepForcedSource 2014-12-03 00:52:17,740 INFO DeepSource view for empty chunk created /home/fjammes/stack/Linux64/mysqlpython/1.2.3+11/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py:75: Warning: Unknown table 'qservTest_case03_qserv__DeepSource'   while self.nextset(): pass {code}",NULL
DM-1606,"Catch lua plugin errors","mysql-proxy lua plugin errors have to be catched, to insure the luq script execution was successful.    For exemple error below wasn't catched at each time, letting user think their query was successfully executed.  {code}  [fjammes@clrlsstwn04 qserv]$ git diff c4dd922e798bfdb545145f09fc0b9cb95a82cadb 5e16bc9e98ece65c8f0b6c17cae68d7de477bc2e  diff --git a/core/modules/proxy/mysqlProxy.lua b/core/modules/proxy/mysqlProxy.lua  index c4b6ed5..7bad98a 100755  --- a/core/modules/proxy/mysqlProxy.lua  +++ b/core/modules/proxy/mysqlProxy.lua  @@ -186,6 +186,9 @@ function utilities()           -- convert new lines and tabs to a space           q = string.gsub(q, '[\n\t]+', ' ')     +        -- remove all spaces at the beginning  +        q = string.ltrim(q)  +           -- remove all spaces before/after '='           q = string.gsub(q, '[ ]+=', '=')  {code}",NULL
DM-1613,"Update Anaconda","1.8.0 to 2.1.0",NULL
DM-1623,"Document ""how to create new package""","I wasn't able to find documentation on how to create a new package, including things like where to find a template for a new package. ",NULL
DM-1628,"Fix permissions in Jira related to RFC","I am owning one RFC, and as a result I can't start a new sprint. The error message is: ""To start a sprint you must have Project Administrator permissions for all of the following projects: Data Management, Request For Comments.""",NULL
DM-1631,"Prevent fast-forward merges to master","Please implement a git hook to only allow pushes to master if they are: 1) non-fast-forward merge commits or 2) documentation fix commits with ""#master-doc-fix"" in the commit message.  This will prevent accidental fast-forward merges from confusing the master history.",NULL
DM-1634,"Come up with sketch of schema for Metadata Store","Need to come up with a sketch of schema. ",NULL
DM-1637,"cat test timeFuncs.py requires lsst10 connection and credentials","This test should only run if appropriate credentials are in place and perhaps only if the host is at NCSA.",NULL
DM-1638,"Enable logging in lua script","LUA_DEBUG_LEVEL has to be set from meta-configuration file and then passed to etc/sysconfig/qserv and to etc/init.d/mysql-proxy",NULL
DM-1639,"Check service startup before exiting init.d/qserv-czar ","On some systems (i.e. lsst-db2.slac.stanford.edu) it may happen that integration test launch query before startup of qserv-czar. A control (check socket availabiilty for example) should be performed in init.d/qserv-czar (see init.d/mysqld for example).",NULL
DM-1640,"Check Qserv scripts shell portability","For now Qserv install/configuration and startup script are only available for bash. They should be tested and updated to support other shell, unless it is decided Qserv requires bash.",NULL
DM-1643,"Modify eupspkg's generated build.sh to be reusable","{{eupspkg}} generates a {{build.sh}} file to actually do the build.  This script should be reusable to redo the build/install after patching any build problems, but it currently is not because it always tries to un-{{tar}} the source distribution (which has since disappeared since {{eups distrib}} retrieves it as a temporary file).  The script should 1) place a marker (e.g. a {{.something}} file) to indicate that it successfully extracted the sources and 2) if that marker exists, don't try to execute the {{tar}} command.",NULL
DM-1644,"Segfault on OS X Mavericks in testCoadds.py","When building pipe_tasks on OS X Mavericks (with lsstsw), I get a segfault in testCoadds.py. It also occurs if the test is run manually:  {code} mjuric@gallifrey:~/projects/l2/lsstsw/build/pipe_tasks/tests$ ./testCoadds.py > testCoadds.log 2>&1 Segmentation fault: 11 {code}  The contents of the log file is attached.",NULL
DM-1646,"Bug, cause unknown, caused CFITSIO to fail build twice before miraculously succeeding","cfitsio failed in the 'ld' phase in 2 sequential buildbot build attempts . It succeeded in the 3rd attempt but with a worrying message in the log.  The logs for the 3 sequential builds are located in: ~lsstsw/build/FailedLogs/{2406 2407 2408}/cfitsio The log in 2408 is the putative successful build of cfitsio; I created and loaded the log in 2408 to simplify review of the three runs.  -------------------------------------------------------------------------- Log 2406 error block: /usr/bin/ld: can not read symbols: File truncated /usr/bin/ld: BFD version 2.20.51.0.2-5.42.el6 20100205 internal error, aborting at merge.c line 876 in _bfd_merged_section_offset  /usr/bin/ld: Please report this bug.  collect2: ld returned 1 exit status make: *** [libcfitsio.so] Error 1 make: *** Waiting for unfinished jobs.... -------------------------------------------------------------------------- log 2407 error block: f77_wrap3.o: file not recognized: File truncated collect2: ld returned 1 exit status make: *** [libcfitsio.so] Error 1 make: *** Waiting for unfinished jobs.... <....> a - f77_wrap4.o make[1]: warning:  Clock skew detected.  Your build may be incomplete. make[1]: Leaving directory `/nfs/lsst/home/lsstsw/build/cfitsio' -------------------------------------------------------------------- Successful log 2408: ar: creating libcfitsio.a ar: libcfitsio.a: Error reading f77_wrap3.o: File truncated --------------------------------------------------------------------  Hipchat chatter on this topic  in room: 'Bot: Buildbot notification' on  Dec 8 at 9am  Essence: could be transient ssd/hardware problems; could be inadvisable default use of '-j #' during build.",NULL
DM-1649,"Research InfluxDB","Research http://influxdb.com/",NULL
DM-1650,"Fix email address on ""Getting Started""","According to https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM  ""Contact lsst-sysadmins _at_ lsst.org to set up the following...""  but I am getting:  {code} -------- Forwarded Message -------- Subject: 	Undeliverable: new employee Date: 	Mon, 8 Dec 2014 18:59:04 -0800 From: 	Mail Delivery Subsystem <MAILER-DAEMON@nospam3.slac.stanford.edu>  *Delivery has failed to these recipients or groups:*  lsst-sysadmins@lsst.org <mailto:lsst-sysadmins@lsst.org> Your message wasn't delivered due to a permission or security issue. It  may have been rejected by a moderator, the address may only accept email  from certain senders, or another restriction may be preventing delivery.  The following organization rejected your message: isgsa-1.lsst.org. {code}   lsst-sysadmins@lists.lsst.org is failing too:  {code} On 12/08/2014 07:04 PM, Mail Delivery Subsystem wrote:> *Delivery has failed to these recipients or groups:* >  > lsst-sysadmins@lists.lsst.org <mailto:lsst-sysadmins@lists.lsst.org> > The email address you entered couldn't be found. Please check the  > recipient's email address and try to resend the message. If the problem  > continues, please contact your helpdesk. >  > The following organization rejected your message: lists.lsst.org. {code}  What is the correct address? Can you fix it?  ",NULL
DM-1651,"Change afw fits.cc to read in COMMENT and HISTORY FITS cards.","It looks like the code that would read FITS history and comment cards is being hidden by a check for empty values. Changing the order of these statements should result in the history and comment cards being put in the PropertyList structure.",NULL
DM-1654,"Move buildbot to port 80 or 443","buildbot's Web interface currently runs on port 8010 of lsst-buildx.ncsa.illinois.edu.  Various networks, including unsecured visitor and public networks, block access to unusual ports.  It would avoid problems on such networks if buildbot were to move to port 80 (HTTP) or port 443 (HTTPS).",NULL
DM-1665,"add prose about the transition to operations  in the master security document that is consistent with LSE-70","LSE-70, the operations plan, contains decisions about the Cyber security. While not a complete program, it's now necessary to  revise the master plan to be consistent with the decisions embedded in that document. ",NULL
DM-1666,"Please improve eups support for multiple stacks","Eups present has poor support for multiple independent stacks on one computer, because too much information is shared in the single ~/.eups directory (such as manually declared packages).  This makes several things hard: - Creating a clean new stack. Unwanted information from the old stack infects the new stack. - Using different stacks for different purposes, such as an lsstsw stack to test potential releases against master, vs. a normal ""eups distrib install"" stack for working on new code.  I would strongly prefer not to share any packages between different stacks. They may not be binary compatible and I may be working on entirely different things in different stacks.  I suggest that user-specific eups information be different for each stack (each EUPS_PATH). One way to do this is have subdirectories of .eups that are based on EUPS_PATH and use those for manually declared packages and tags of packages.",NULL
DM-167,"Support non-astrometry.net.data catalog formats for photometry","Deal with reference catalogues that are in a range of formats.  N.b. this includes doing the matching. ",NULL
DM-1671,"Adopt Jim Bosch's afw fix for hscAstrom from the HSC project","Jim Bosch found that an important afw table feature was missing while converting hscAstrom on the HSC side. He has fixed the problem there and we need this fix on the LSST side.  On the HSC side the fix is u/jbosch/HSC-1056 in git@github.com:HyperSuprime-Cam/afw I believe all the relevant fixes are in commit 06aa434 (previous changes seem to have already been applied, though cameraGeom and exception names have not been updated on the HSC side).",NULL
DM-1672,"Update hscAstrom to work on our stack","Get hscAstrom running on our stack. Jim Bosch has already performed much of the work on the HSC side (git@github.com:HyperSuprime-Cam/hscAstrom.git) on branch u/jbosch/HSC-1056  Warning: this requires a change to afw: see DM-1671",NULL
DM-1678,"Handling of overlap parameter in new data loader","Fabrice tries to use new data loader but qserv returns errors for the data loaded with new loader. It looks like the reason is absent {{overload}} parameter at database level in CSS. There is an uncertainty where overlap parameter should be stored, I think overlap can be per-table parameter for new partitioner.  We need to figure out what new loader should load into CSS at database level and table level.",NULL
DM-1679,"camerGeom/utils.py uses local bboxes; why not parent?","cameraGeom/utils.py uses local bounding boxes instead of parent. Is this right? Wouldn't parent make more sense?",NULL
DM-1680,"Background.cc uses local bbox for _imgBBox; why not parent?","In afw Background.cc sets _imgBBox to the local bounding box. Why is this not the parent bounding box?",NULL
DM-1681,"forcedPhot.py uses local bbox; why not parent?","forcedPhot.py subsetReferences obtains the local bbox from an exposure; why not the parent bbox?  Also: tests/testRegister.py should probably use parent instead of local bboxes.",NULL
DM-1682,"ip_diffim utils.py creates a footprint from a local bbox; why not parent?","ip_diffim utils.py uses foot = afwDet.Footprint(exp.getBBox(LOCAL)); why not PARENT?",NULL
DM-1683,"Some tests compare local bboxes; why not parent?","isr.py tests in 4 cases that a local bbox of one image equals the local bbox of another; wouldn't parent tests be safer (though pickier)? ",NULL
DM-1684,"Two puzzling uses of local bboxes","PsfAttributes.cc uses a local BBox; why not a parent bbox: {code} double PsfAttributes::computeGaussianWidth(PsfAttributes::Method how) const {     /*      * Estimate the PSF's center.  This *really* needs to be rewritten to avoid using MeasureSources;      * we shouldn't need to instantiate source objects just to measure an adaptive centroid!      */     afwImage::MaskedImage<double> mi = afwImage::MaskedImage<double>(_psfImage);     typedef afwImage::Exposure<double> Exposure;     Exposure::Ptr exposure = makeExposure(mi);      afwDetection::Footprint::Ptr foot = boost::make_shared<afwDetection::Footprint>(exposure->getBBox(         afwImage::LOCAL)); {code}  Measure.cc MeasureSources::apply: uses local BBox; why not PARENT? {code} source.setFootprint(refFoot->transform(*referenceWcs, *wcs, exposure.getBBox(afw::image::LOCAL))); {code} ",NULL
DM-1686,"baseline.py question about local vs parent bbox","baseline.py contains this code: {code} if not opsfimg.getBBox(afwImage.LOCAL).overlaps(stampbb):     continue {code} Why is this a local bbox instead of parent? A comment would help if it's intentional, else please change it if not.",NULL
DM-1687,"Undefined names in measureMulti.py","modelfit/measureMulti.py has a number of bugs that are shown by pyflakes, including: - coaddInputCat is undefined in ""exposureSchema = coaddInputCat.getSchema()"" - psfCtrl is assigned to but not used (not a bug, but likely a misfeature) - coaddFootprint is undefined in ""calexpFootprint = coaddFootprint.transform"" and a few more",NULL
DM-1688,"CSS version must be defined only once","For now Qserv CSS version is defined 3 times, this is not good:  {code:bash} core/modules/css/constants.h:char const VERSION[] = ""1""; ///< Current supported version core/modules/css/Facade.cc:// NOTE: THIS NUMBER MUST MATCH VERSION DEFINED IN qservAdmin.py. core/modules/css/Facade.cc:const int VERSION = 1; core/modules/css/Facade.cc:const std::string VERSION_STR = boost::lexical_cast<std::string>(VERSION); qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-627 *) $ grep -rw VERSION admin/python/ admin/python/lsst/qserv/admin/qservAdmin.py:# NOTE: THIS NUMBER MUST MATCH VERSION DEFINED IN CSS/FACADE. admin/python/lsst/qserv/admin/qservAdmin.py:VERSION = 1 {code}",NULL
DM-1689,"processCcdSdss.py uses local bbox; should be parent?","processCcdSdss.py in makeExp uses a local bounding box to create the masked image. Why not a parent? I think we get away with this because the fpC files have xy0 = 0, but it's not safe.",NULL
DM-1690,"should testSelectFluxMag0.py use a parent bbox?","testSelectFluxMag0.py uses a local bounding box in the code {code} scaleFactorIm = imageScaler.getInterpImage(exposure.getBBox()) {code} should it be a parent bbox instead?",NULL
DM-1691,"Some unit tests silently are not run if afwdata not setup, and afwdata is not in the ups table","daf_butlerUtils contains two tests (ticket1580.py and ticket1640.py) that silently do nothing if afwdata is not setup. Also afwdata is not an optional dependency in the ups table file.  Please make the failure to run more visible (e.g. by printing a warning that is visible to scons) and, if permissible, add afwdata as an optional dependency to the ups table.",NULL
DM-1693,"Update czar for per-table overlap (Facade, RelationGraph)","The facade was lacking the ability to retrieve per-table overlap parameters. This ticket adds that capability. The RelationGraph code also relied on overlap when determining whether a query was evaluable. This ticket updates the RelationGraph code to use the per-table overlap info.",NULL
DM-1699,"Allow for CCDs with no defects in cameraMapper.py","Currently, the cameraMapper.py functions map_defects() and bypass_defects() throw RuntimeErrors if no defects for a given dataID are found.  Shouldn't a given detector be allowed to be defect-free?  ",NULL
DM-1717,"Reorganize eupspkg distribution server for scalability","{{eupspkg}} currently has all product eupspkgs (tarballs) in a single directory:   {code}  /products/{PRODUCT_NAME}-{VERSION}.eupspkg  {code}  This is not going to scale well as we go to many packages and many versions of each.  It already takes a long time to load {{https://sw.lsstcorp.org/eupspkg/products/}}.  Perhaps we should change to at least   {code}  /products/{PRODUCT_NAME}/{PRODUCT_NAME}-{VERSION}.eupspkg  {code}  ?    This change would have to happen in {{eups/python/eups/distrib/eupspkg.py}} and would be incompatible with the current distserver, obviously, although both could be maintained (even in the same URL hierarchy).",NULL
DM-1719,"After a build interruption during the lsst_build setup phase, the git-directory as left in an unusable state.","If the buildbot-initiated build is terminated abruptly (by whatever cause), a git-build directory may be left in an unusable state.  The current lsst_build does not recognize that the package needs to be removed and then refetched.  The result is that all subsequent builds of that package  using the same set of dependencies as the failed run - will continue to fail.  The solution currently is to go in to ~lsstsw/build and surgically remove just that one package directory. The next build run will  fetch a fresh copy from the git-repo and the buld will proceed as desired.  The better solution is to programmatically determine if the local copy of the directory is intact. If not, then reacquire the desired git-repo version.",NULL
DM-1723,"Improve data-loader cli and cfg files","Suggestions:  * provide --keep-input-data option, * replace --chunks-dir option with --work-dir, because it's also used for non partitioned data, and remove default value so that the user is aware where data are stored. * in cfg files part.id and dirColName seems to have the same semantics: double check and if yes, use only one parameter. * comment each field in an example cfg file and explain what is required by partitioner, duplicator, css or other loader components.",NULL
DM-1724,"Add unzipped-file-location option to data-loader","This option should be required only if there are compressed files given as input.",NULL
DM-1728," Support binary columns","In integration test case 03, tables  RunDeepSource, RunDeepForcedSource contains bits fields which are not supported by partitioner and Qserv.    Two solutions:  - change their type to tinyint  - support them in Qserv    Please note that none of these tables are queried in current integration tests (cf. DM-1493).",NULL
DM-1729,"Partition SimRefObject and add additional join queries in integration test case #01","Non-blocking story, coud be used to test RefMatch support in Qserv.  See Serge mail from qserv-l mailing thread: ""support for multi-column PK"":  Well, in testcase 01, SimRefObject and RefSrcMatch are not partitioned. So Qserv doesn’t really need to know anything about them for queries to work (at least from the query analysis point of view)..  If you did decide to partition SimRefObject, it would be its own director table, and would have dirTable = SimRefObject and dirColName equal to its PK (simRefObjectId I think it was).  RefSrcMatch is a match table, which can be used to join between 2 directors. In this case (I’m not sure where exactly the test case 01 data is from) it probably links up SimRefObject with the Source table. So it would have ZK metadata that looks roughly like:    {code} .../RefSrcMatch/match            1 .../RefSrcMatch/match/dirColName1 simRefObjectId .../RefSrcMatch/match/dirColName2 sourceId .../RefSrcMatch/match/dirTable1        SimRefObject .../RefSrcMatch/match/dirTable2        Source .../RefSrcMatch/match/flagColName flags .../RefSrcMatch/partitioning   \N .../RefSrcMatch/partitioning/subChunks        0 {code}   And, since partitioning match table records works differently from other kinds of records (each record is associated with 2 positions rather than just 1), you have to invoke sph-partition-matches rather than sph-partition on it. I don’t know if Andy’s loader supports that yet.",NULL
DM-1730,"newinstall.sh fails if ""/usr/bin/env python"" gives Python 3.4","[newinstall.sh|https://sw.lsstcorp.org/eupspkg/newinstall.sh] attempts to configure EUPS by running {{./configure ... --with-python=""$PYTHON""}}, where {{$PYTHON}} is derived from the environment or, if not set there, defaulting to {{/usr/bin/python}}.  EUPS, in turn, executes its own {{bin/mksetup}}. This file is currently only compatible with Python 2.x; it raises a {{SyntaxError}} under 3.x. It decides which Python to run by executing {{/usr/bin/env python}}: note that it _neither_ uses the user's {{$PYTHON}} directly _nor_ the contents of the {{--with-python}} argument.  In summary, if the user's environment is such that Python 3.4 appears on their path before 2.7, we will use that to execute {{mksetup}}, and it will fail, regardless of what options the user supplies to {{newinstall.sh}}. Furthermore, the failure is pretty uninformative:  {code} Installing EUPS (v1.5.7)...  done. newinstall.sh: line 225: /Users/deil/code/lsst/eups/bin/setups.sh: No such file or directory Installing Anaconda Python Distribution ...  newinstall.sh: line 240: eups: command not found {code}  (Note that the problem here occurs _before_ we ever get around to installing Anaconda, when we are reliant on whatever Python the system provides us with.)  There are really two closely related issues here:  * We should use the user-specified version of Python to execute {{mksetup}}; * We should detect a failure to install EUPS and report appropriately, rather than pressing on and failing to install Anaconda.  Both of these can be most conveniently handled from the EUPS side, and I'm about to open a PR against it, but, if & when that has been accepted, {{newinstall.sh}} will need an update to pick up the new version.  See also the [discussion & original report on dm-users|https://lists.lsst.org/pipermail/dm-users/2014-December/000529.html].",NULL
DM-1737,"Design and implement multi-czar unique query id (taskId)","Please see next discussion: https://github.com/LSST/qserv/commit/bcee8f2ebd348f6c260bf377d5a3ff09d68fc337#commitcomment-9161198  A counter per Qserv instance, kept centrally and persistent over restarts, is one alternative, but it could have performance problems with multiple czars. A counter per czar, still persistent, along with a czar identifier, would be better. Another could be a hash of the query; with anywhere from 160 to 512 bits, collisions should be negligible. I don't know if that would make the table name too long. ",NULL
DM-1740,"Add DYLD_LIBRARY_PATH to table for external packages","The following external packages (and one DMS package) add to LD_LIBRARY_PATH for Linux but do not add to DYLD_LIBRARY_PATH for MacOS X.  To the extent that these packages are for the Qserv server only, this may be acceptable, but in general we should try to have all packages portable whenever possible.  Accordingly, I plan to modify the table files for all but xrootd.  * libevent * log * log4cxx * protobuf * sqlite * xrootd * zookeeper",NULL
DM-1741,"ExampleCmdLineTask raises a FieldValidationError","{code} $ python examples/exampleCmdLineTask.py Traceback (most recent call last):   File ""examples/exampleCmdLineTask.py"", line 24, in <module>     ExampleCmdLineTask.parseAndRun()   File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/pipe_base/10.0+2/python/lsst/pipe/base/cmdLineTask.py"", line 428, in parseAndRun     config = cls.ConfigClass()   File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/pex_config/10.0+1/python/lsst/pex/config/config.py"", line 475, in __new__     field.__set__(instance, field.default, at=at+[field.source], label=""default"")   File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/pex_config/10.0+1/python/lsst/pex/config/config.py"", line 312, in __set__     raise FieldValidationError(self, instance, e.message) lsst.pex.config.config.FieldValidationError: Field 'doFail' failed validation: Value 0 is of incorrect type int. Expected type bool For more information read the Field definition at:   File ""/Users/jds/Projects/Astronomy/LSST/src/pipe_tasks/python/lsst/pipe/tasks/exampleCmdLineTask.py"", line 48, in ExampleCmdLineConfig     default = 0, And the Config definition at:   File ""/Users/jds/Projects/Astronomy/LSST/src/pipe_tasks/python/lsst/pipe/tasks/exampleCmdLineTask.py"", line 37, in <module>     class ExampleCmdLineConfig(pexConfig.Config): {code}  The issue is simply that a field with type {{bool}} has a default value of {{0}}. The fix is trivial; PR forthcoming.",NULL
DM-1745,"Minor fixes to SingleFrameMeasurement example","[~lauren] noticed some spots where examples/runSingleFrameTask.py wasn't working, because we'd failed to update the example code after some changes.  Changes were trivial and are already done, just need the issue for review.",NULL
DM-1746,"eups distrib install -n doesn't work anymore","The -n option (no-op) to {{eups distrib install}} doesn't seem to work anymore.  It looks likes some of our build files have a check that something was installed that runs even when we're intentionally not installing.  Apologies if this is actually a true EUPS issue, but I suspect it's something wrong with some of our build files. {code:hide-linenum} jbosch@tioga:~/stacks/LSST$ eups distrib install lsst_apps 10.0+8 -n   [  1/56 ]  apr 1.3.3.lsst2 ...(/home/jbosch/stacks/LSST/system/EupsBuildDir/Linux64/apr-1.3.3.lsst2/build.sh) >> /home/jbosch/stacks/LSST/system/EupsBuildDir/Linux64/apr-1.3.3.lsst2/build.log 2>&1 4>/home/jbosch/stacks/LSST/system/EupsBuildDir/Linux64/apr-1.3.3.lsst2/build.msg Build log file not copied as /home/jbosch/stacks/LSST/system/Linux64/apr/1.3.3.lsst2/ups does not exist (this shouldn't happen). eups distrib: Failed to build apr-1.3.3.lsst2.eupspkg: [Errno 2] No such file or directory: '/home/jbosch/stacks/LSST/system/EupsBuildDir/Linux64/apr-1.3.3.lsst2/build.msg' {code}",NULL
DM-1747,"qserv-admin needs mode where it exits on errors","qserv-admin.py is somewhat geared towards interactive usage. Upon startup, it can be fed a sequence of commands through stdin, and it attempts to execute them, printing to stdout if there are any errors. This is perfectly fine for humans running qserv-admin directly.  But qserv-admin.py is also called from the integration tester. The integration tester doesn't have any error parsing skills, so it doesn't check stdout. I think it may check an error code (?).  Thus the integration tester has no idea whether qserv-admin.py commands succeed or not.  I propose adding a switch to qserv-admin.py that instructs it to abort with an error code on any error. I'd also like to know whether the integration tester can understand error codes (I think so, but am not sure). ",NULL
DM-1748,"newinstall.sh misleading output if no network","newinstall.sh complains it differs from the server version if the network is down.   It should trap the fact that it can't reach the server anyway. ",NULL
DM-1749,"Fix float versus numpy.float32 type mismatch in example scripts.","Running any of:  * meas_base/examples/runSingleFrameTask.py * meas_algorithms/examples/measAlgTasks.py * pipe_tasks/examples/measurePsfTask.py  fail with the following error: {code:hide-linenum} NotImplementedError: Wrong number or type of arguments for overloaded function 'ImageF___isub__'. {code}  This is due to a type mismatch between the numpy.foat32 returned by numpy.median in the assignment: {code:hide-linenum} im -= numpy.median(im.getArray()))  {code} which occurs in all three above mentioned scripts.  The fix is to simply cast as float.",NULL
DM-1750,"Fix mismatch between field name separators in example scripts.","The example scripts: * meas_algorithms/examples/measAlgTasks.py * pipe_tasks/examples/measurePsfTask.py  are failing with:  {code} NotFoundError: 'Field or subfield withname 'flux.sinc_flux' not found with type 'D'.' {code}  They seem to getting confused with the new (underscores) versus old (periods) conventions for field name separators.  The fix is simply to {code} schema.setVersion(0) {code} immediately following the schema construction.",NULL
DM-1751,"minor cleanups to meas_base tests","This captures some miscellaneous minor cleanups I made to meas_base test code while working on DM-1725; I decided I didn't want to pollute the tiny but high-priority fix for that branch with the larger, mostly aesthetic changes here.",NULL
DM-1752,"Missing images in Doxygen","Observe the broken image in the [Doxygen documentation for {{meas_base}}|http://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/classlsst_1_1meas_1_1base_1_1sfm_1_1_single_frame_measurement_task.html#l00035].  This image is present in the {{meas_base}} repository, and appears correctly when running {{scons doc}} directly. However, it doesn't make it on to the web version of the docs.",NULL
DM-1756,"Create human-readable spec of default/required/optional parameters for db/table creation","Hey, we need a spec for db and table parameters, okay? This should be on paper rather than having bits of it scattered in our collective heads.  Let's put it in docs/source/schema.rst, okay?",NULL
DM-1757,"prevent worker directory creation when restarting xrootd","Restarting xrootd {code} ~/qserv-run/etc/init.d/xrootd stop ~/qserv-run/etc/init.d/xrootd start {code} seems to create worker/ in current directory.",NULL
DM-1758,"Spatial restrictors on child tables are not handled properly","Currently, attempting to run a query with a spatial restrictor on a partitioned child table crashes all workers. I think the czar should probably reject such queries, but if not it definitely needs to rewrite them in a different way.  Here's a sample query on child table DeepForcedSource:  {code:sql} select * from DeepForcedSource where qserv_areaspec_box(0.5, 1.1, 0.6, 1.2); {code}  Here's what I see in qserv-czar.log:  {code} 0112 17:43:02.647 [0x7f6d4971d700] INFO  root (app.py:495) - constraint= <lsst.qserv.czar.czarLib.Constraint; proxy of <Swig Object of type 'lsst::qserv::query::Constraint *' at 0x7f6d37c9f0c0> > 0112 17:43:02.654 [0x7f6d4971d700] INFO  root (spatial.py:35) - Using 85 stripes and 12 substripes. 0112 17:43:02.657 [0x7f6d4971d700] INFO  root (app.py:600) - Looking for indexhints in  [('qserv_areaspec_box', ['0.5', '1.1', '0.6', '1.2'])] 0112 17:43:02.659 [0x7f6d4971d700] INFO  root (app.py:610) - lookup got chunks: None 0112 17:43:02.661 [0x7f6d4971d700] INFO  root (metadata.py:43) - Using /db1/run/qserv-master/etc/emptyChunks.txt as default empty chunks file. 0112 17:43:02.676 [0x7f6d4971d700] INFO  root (metadata.py:46) - Using /db1/run/qserv-master/etc as empty chunks path 0112 17:43:02.678 [0x7f6d4971d700] INFO  root (metadata.py:90) - /db1/run/qserv-master/etc/empty_LSST.txt not found while loading empty chunks file. 0112 17:43:02.680 [0x7f6d4971d700] ERROR root (metadata.py:68) - Couldn't find /db1/run/qserv-master/etc/empty_LSST.txt, using /db1/run/qserv-master/etc/emptyChunks.txt. 0112 17:43:02.685 [0x7f6d35dc4700] DEBUG root (app.py:389) - reporting -1 100 Dispatch Query. 0112 17:43:02.685 [0x7f6d35dc4700] DEBUG root (build/qdisp/MessageStore.cc:49) - Msg: -1 100 Dispatch Query. 0112 17:43:02.685 [0x7f6d35dc4700] DEBUG root (build/ccontrol/userQueryProxy.cc:118) - EXECUTING UserQuery_submit(2) 0112 17:43:02.685 [0x7f6d35dc4700] INFO  root (build/qana/ScanTablePlugin.cc:128) - Squash scan tables: <2 chunks. 0112 17:43:02.685 [0x7f6d35dc4700] INFO  root (build/ccontrol/UserQuery.cc:270) - UserQuery::_setupMerger() 0112 17:43:02.686 [0x7f6d35dc4700] INFO  root (build/ccontrol/UserQuery.cc:176) - UserQuery beginning submission 0112 17:43:02.686 [0x7f6d35dc4700] INFO  root (build/qproc/QuerySession.cc:361) - Non-subchunked 0112 17:43:02.686 [0x7f6d35dc4700] DEBUG root (build/qproc/QuerySession.cc:381) - returning  queries:  0112 17:43:02.686 [0x7f6d35dc4700] DEBUG root (build/qproc/QuerySession.cc:383) - SELECT * FROM LSST.DeepForcedSource_7310 AS QST_1_ WHERE scisql_s2PtInBox(QST_1_.,QST_1_.,0.5,1.1,0.6,1.2)=1 0112 17:43:02.686 [0x7f6d35dc4700] DEBUG root (build/qproc/TaskMsgFactory2.cc:151) - no nextFragment 0112 17:43:02.687 [0x7f6d35dc4700] DEBUG root (build/qproc/TaskMsgFactory2.cc:153) - SELECT * FROM LSST.DeepForcedSource_7310 AS QST_1_ WHERE scisql_s2PtInBox(QST_1_.,QST_1_.,0.5,1.1,0.6,1.2)=1 0112 17:43:02.687 [0x7f6d35dc4700] DEBUG root (build/qdisp/Executive.cc:396) - Executive (0x1af6750) tracking id=1 0112 17:43:02.687 [0x7f6d35dc4700] INFO  root (build/qdisp/Executive.cc:172) - Exec add pth=/chk/LSST/7310 0112 17:43:02.687 [0x7f6d35dc4700] DEBUG root (build/qdisp/MessageStore.cc:49) - Msg: 7310 1200 Exec add pth=/chk/LSST/7310 Opening xroot://127.0.0.1:1094//chk/LSST/7310 ... {code}  The queries being dispatched are totally bogus. Unsurprisingly, xrootd.log contains:  {code} terminate called after throwing an instance of 'std::runtime_error'   what():  Couldn't get result {code}  Finally here's the metadata for the table:  {code} /DBS/LSST/TABLES/DeepForcedSource   READY /DBS/LSST/TABLES/DeepForcedSource/partitioning.json {""dirColName"": ""deepSourceId"", ""dirTable"": ""DeepSource"", ""subChunks"": ""0""} /DBS/LSST/TABLES/DeepForcedSource.json  {""compression"": ""0"", ""match"": ""0"", ""uuid"": ""7adf9d49-14d4-4d0d-a440-43437fcf3565""} {code}",NULL
DM-1759,"hang in testCmdLineTask.py","I'm seeing a hang in testsCmdLineTask.py when building on Ubuntu 12.04.  The problem appears to be in the testMultiprocess function; if I skip that, everything else works fine.  I don't see anything obvious in the output, aside from maybe the warning about missing data: {code:hide_linenum}  % tests/testCmdLineTask.py     : Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test.py' : Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test/test.py' : input=/mnt/data/jbosch/LSST/master/obs_test/data/input : calib=None : output=/tmp/tmpDF6Ihh CameraMapper: Loading registry registry from /tmp/tmpDF6Ihh/_parent/registry.sqlite3 .: Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test.py' : Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test/test.py' : input=/mnt/data/jbosch/LSST/master/obs_test/data/input : calib=None : output=/tmp/tmp4Lt3JD CameraMapper: Loading registry registry from /tmp/tmp4Lt3JD/_parent/registry.sqlite3 : Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test.py' : Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test/test.py' : input=/mnt/data/jbosch/LSST/master/obs_test/data/input : calib=None : output=/tmp/tmp4Lt3JD CameraMapper: Loading registry registry from /tmp/tmp4Lt3JD/_parent/registry.sqlite3 .: Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test.py' : Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test/test.py' : input=/mnt/data/jbosch/LSST/master/obs_test/data/input : calib=None : output=/tmp/tmpcROnCk CameraMapper: Loading registry registry from /tmp/tmpcROnCk/_parent/registry.sqlite3 .: Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test.py' : Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test/test.py' : input=/mnt/data/jbosch/LSST/master/obs_test/data/input : calib=None : output=/tmp/tmpQ49On1 CameraMapper: Loading registry registry from /tmp/tmpQ49On1/_parent/registry.sqlite3 test FATAL: Failed on dataId={'filter': 'r', 'visit': 3}: Failed by request: config.doFail is true .: Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test.py' : Config override file does not exist: '/mnt/data/jbosch/LSST/master/obs_test/config/test/test.py' : input=/mnt/data/jbosch/LSST/master/obs_test/data/input : calib=None : output=/tmp/tmpTesybC CameraMapper: Loading registry registry from /tmp/tmpTesybC/_parent/registry.sqlite3  WARNING: No data found for dataId=OrderedDict([('visit', 2), ('filter', 'r')]) {code}  It had been a while since I last built this code on my machine, so this may not be a particularly new bug.",NULL
DM-1760,"Fix PSF handling in SdssCentroidAlgorithm","Since as long ago as [{{b0d8591f}} on {{meas_algorithms}}|https://github.com/LSST/meas_algorithms/commit/b0d8591f0e28fd34c95f1548ebda5695e46d2a22#diff-e3030f91d8f27f723951db3f4d07ed73R453], the handling of the {{psf}} argument to {{SdssCentroid}} has been marked with the string ""FIXME"" and a note that the PSF should be provided as a configuration parameter. This code has now migrated to {{meas_base}} and its behaviour has changed (it now throws rather than assuming a default PSF if none is provided), but the basic issue remains: we should fix it.",NULL
DM-1763,"Use install_name_tool on anaconda when deploying on OSX","In order to make GalSim eups distrib installable on OSX, we need to use install_name_tool on anaconda's dynamic libraries to make sure that they point to the right place.  In most cases, this can be handled by changing anaconda's eupspkg.cfg.sh.    When lsstsw is deployed, it clones and builds anaconda independent of EUPS.  Therefore, the deploy script in lsstsw must also be changed to use install_name_tool in the case where it is being deployed on lsstsw.  There will be two branches associated with this ticket: One in lsstsw to alter the deply script. One in anaconda to alter eupspkg.cfg.sh",NULL
DM-1779,"Buildbot/Doxygen documents incorrect versions of some packages","[Build 3282|http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/3282] was requested including the {{u/pgee/DM-1659}} branch of {{meas_base}} starting on the evening of 13 January.  Subsequent hourly builds (e.g. [3296|http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/3296]) have continued to build documentation for that branch rather than reverting to documenting the {{master}} branch.  See the [discussion on HipChat|https://lsst.hipchat.com/history/room/520390/2015/01/14?q=buildbot&t=all#09:36:22] for details.",NULL
DM-1780,"JIRA configuration problems in RFC project","While working with RFC issues, I noticed the following problems:  - Comment text is is not parsed as wiki markup.  Fix: on https://jira.lsstcorp.org/plugins/servlet/project-config/RFC/fields, change the Renderer for Comment to ""Wiki Style Renderer"".  - When an RFC issue is closed and then Reopened, it is put into the ""Flagged"" state, not the ""Proposed"" state.  - Anonymous users can change the state of the RFC.",NULL
DM-1781,"Tcsh special alias cwdcmd causes the v10 loadLSST.csh to fail.","In {{loadLSST.csh}} the {{LSST_HOME}} environment variable is set by:  {code}  if ( ! ${?LSST_HOME} ) then    set LSST_HOME = `dirname ${this_script}`    set LSST_HOME = `cd ${LSST_HOME} && pwd`  endif  {code}  If {{cwdcmd}} is used to set the window title with something like this:  {code}  alias cwdcmd 'echo -n ""2;${HOST}:$cwd ^G""'  {code}  the {{cd}} command in the script echos those commands and does not set {{LSST_HOME}} properly.",NULL
DM-1793,"remove reference data members from FootprintFunctor","The FootprintFunctor class uses references for data members, which could cause memory problems if the class (or a subclass) is ever initialized with a temporary.  Fixing this would probably require changing the constructor to take a shared_ptr, however, so it would break a lot of downstream code.  I'd rather actually rewrite FootprintFunctor entirely (one of the goals for Epic DM-1107), but it's not clear when that will happen; if it slips too much, this issue is to remind us to fix at least this problem.",NULL
DM-1800,"formatHistory fails when writeSourceLine=False","When setting writeSourceLine=False in formatHistory, the code crashes for the latest version of pex_config. I've attached a script for duplicating the behavior. I've also attached a patch for a proposed fix.",NULL
DM-1801,"anaconda vendored libm.so.X causes link failures","The anaconda vendored version of libm.so.X has to be manually deleted in order to prevent link errors / test failures. This should be documented in a way this it's obvious for external end users.  It also needs to be handled for automated test builds in a clean sandbox environment.  It was mentioned on hipchat that [~rhl] requested upstream to remove libm.so over a year ago and this has not happened.  The best long term resolution may be to remove the ""default"" usage of anaconda all together.  [sqre@el7-jhoblitt-pot ndarray-10.0+1]$ cat ndarray-10.0+1/tests/.tests/ndarray-python.py.failed  tests/ndarray-python.py  Traceback (most recent call last):   File ""tests/ndarray-python.py"", line 3, in <module>     import ndarray_python_test ImportError: /home/sqre/stack/Linux64/anaconda/2.1.0/bin/../lib/libm.so.6: version `GLIBC_2.15' not found (required by /home/sqre/stack/Linux64/fftw/3.3.3/lib/libfftw3.so.3) ",NULL
DM-1821,"Clarify scope of DM data quality analysis requirement","Clarify in LSE-140 that the DM data quality analysis referred to is primarily that of the Level 1 data products.",NULL
DM-1823,"IN2P3: Design and implement monitoring tool for Qserv","{quote} Here's a good walk through of the kind of monitoring people are currently using for application level stuff.  http://www.infoq.com/articles/graphite-intro?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global  Graphite is a very common part of the tool chains people are setting up. A lot of the work in monitoring is moving away from a monlithic setup like nagios or ganglia into a flow of information in which you can swap components in/out as needed.  - Booker C. Bense  {quote}  ELK stack is also interesting: http://williamdurand.fr/2014/12/17/elasticsearch-logstash-kibana-with-docker/  And interesting infos here: http://monitorama.com/",NULL
DM-1838,"shrinkFootprint documentation & code are inconsistent","Per [~price] at DM-1545:  {code} /**  * Shrink a footprint isotropically by nGrow pixels, returning a FootprintSet.  */ PTR(Footprint) shrinkFootprint(Footprint const& foot, int nGrow, bool isotropic); {code}",NULL
DM-1842,"Fix encoding problem in qserv-czar.log","It seems qserv-czar.log can contains non-ascii strange characters:  {code} mysql --host 127.0.0.1 --port 4040 --user qsmaster qservTest_case03_qserv -e ""select * FROM Science_Ccd_Exposure_Metadata;""  vi ~/qserv-run/git/var/log/qserv-czar.log 0126 13:09:46.004 [0x7fea21ffb700] DEBUG root (build/qdisp/QueryRequest.cc:99) - Requesting [167] ^H^F^R^VqservTest_case03_qserv^X????^D*?^A                                              LSELECT * FROM qservTest_case03_qserv.Science_Ccd_Exposure_Metadata AS QST_1_^Z0r_67bad4e5adebc976dcfc96851c8a5536d_1234567890_0""Task=0x7fea0c0009e0 processing id=0  {code}  This can derail {{cat}} command and should be fixed.",NULL
DM-1846,"Measurement: Debias Centroids","We currently use an SDSS-derived centroiding algorithm in meas_algorithms, but one feature of the original SDSS code is absent.  The code fits a Gaussian to the object (it does it by convolving and looking for the maximum, but it's basically a full MLE fit given a Gaussian model).  The correct algorithm is to correlate with the PSF (i.e. convolve with its adjoint), and this can matter for asymmetric PSFs.  This can lead to discrepancies of around 20mas for 0.5"" seeing on Subaru.  SDSS corrected for this by applying the same centroiding algorithm to the PSF model, then calculating the first moment of the model (the centre of light of all the photons in the object), taking the difference to estimate the bias, then correcting the Gaussian-fitted centroid of the real object.  I removed this from the LSST code as it's degenerate with the astrometric solution --- except that it isn't when you're using coaddPSF as the bias means that the centroid-of-the-sum is not the same as the sum-of-the-centroids.  We should put it back, or switch to using the adjoint PSF.  My guess is that the SDSS approach is more robust.",NULL
DM-1863,"Repository renaming ICW move to Github","This ticket covers work associated with DM repository renaming, partly to satisfy the Github Transition Plan, partly to support current and future DM refactoring needs",NULL
DM-1869,"loadLSST.csh is broken","I think {{loadLSST.csh}} uses non-cshell syntax. E.g. it says   {code}  set EUPS_DIR = ""${LSST_HOME}/eups”  {code}  when apparently it should be   {code}  setenv EUPS_DIR ${LSST_HOME}/eups  {code}  The stack cannot be installed using this version because it immediately gets lost looking for EUPS. One can easily switch to bash and install just fine so this is not a major issue (but it would be good to least document it). ",NULL
DM-1886,"qserv-admin crash in DUMP EVERYTHING","running ""DUMP EVERYTHING file;"" command in qserv-admin crashes it: {noformat} qserv > DUMP EVERYTHING TEST; Traceback (most recent call last):   File ""/usr/local/home/salnikov/dm-xxx/bin/qserv-admin.py"", line 502, in <module>     main()   File ""/usr/local/home/salnikov/dm-xxx/bin/qserv-admin.py"", line 497, in main     CommandParser(connInfo).receiveCommands()   File ""/usr/local/home/salnikov/dm-xxx/bin/qserv-admin.py"", line 146, in receiveCommands     self._parse(cmd[:pos])   File ""/usr/local/home/salnikov/dm-xxx/bin/qserv-admin.py"", line 163, in _parse     self._funcMap[t](tokens[1:])   File ""/usr/local/home/salnikov/dm-xxx/bin/qserv-admin.py"", line 265, in _parseDump     self._impl.dumpEverything(dest)   File ""/usr/local/home/salnikov/dm-xxx/lib/python/lsst/qserv/admin/qservAdmin.py"", line 625, in dumpEverything     elif getattr(dest, 'write'): AttributeError: 'unicode' object has no attribute 'write' {noformat}",NULL
DM-1888,"scons install pass even if scons unit test fails","it seems that {{scons install}} (which run {{scons build}} ) returns 0 even if one {{scons build}} unit test fail. The normal behaviour would be that {{scons install}} require {{scons build}} to complete successfully.",NULL
DM-1895,"scons fails if a test has failed, even if it doesn't rerun tests","If scons decides it does not need to run unit tests it will still claim the build failed if there are any failed tests in tests/.tests.  To see this try the following: - setup afw but not afwData - put a fake failed test in afw/tests/.tests (e.g. rename an existing passed test log by adding .failed to the end) - scons  It will say that there is nothing to be done for .tests but proceed to tell you that the build failed because one test failed.  I think this is important to fix because it is very confusing and could cost developers a lot of time trying to fix tests that are not broken. I certainly wasted some time on it today. ",NULL
DM-19,"Update URLs in newinstall.sh","Update the newinstall.sh script to point to final repository locations, etc.",NULL
DM-1902,"Upgrade core/examples/obsolete/plotChunksCopies.tcl","plotChunksCopies.tcl, while dependent on output from obsolete code, is still the only way we have of visualizing the chunk layout. It should be ported to the current infrastructure.",NULL
DM-1922,"meas_base-10.0+2 test failures on ubuntu 14.04","The tests for meas_base-10+2 fail on ubuntu 14.04 while building lsst_distrib v10_0.",NULL
DM-1938,"Provide skeleton for RESTful access","Move I* related interfaces from webserv.git to here",NULL
DM-196,"research non-blocking log4cxx appenders","Default appenders in log4cxx are blocking (synchronous). This will dramatically impair performance of systems such as Qserv. We need to research the non-blocking appenders, and eventually switch to using them, in particular for production.",NULL
DM-1967,"make qserv-test-head.sh eups independant","eupspkg was used for parallel build, but it involves a dependency on git 1.8.4.x, so nproc, which is even bundled with minimalist Docker system will be used instead.",NULL
DM-1970,"Make DistortedTanWcs persistable","The new DistortedTanWcs class (DM-1969) is not persistable. It would be nice if it was.  One complicating factor is that DistortedTanWcs contains an arbitrary XYTransform, and that class hierarchy is not yet persistable.  One option is to persist it as a TAN-SIP WCS. This would probably be good enough (especially since DistortedTanWcs is only intended as an approximation before astrometry is fit) and it would be fairly easy to do. However, the lack of true round-trip would probably cause unwelcome surprises.  Even if true round-trip ability is implemented, it would probably be wise to also write a TAN-SIP approximation to the FITS files, so that utilities such as ds9 can take advantage of it. Presumably additional metadata could tell the LSST code to unpersist the WCS as a DistortedTanWcs instead of a TAN-SIP WCS (TanWcs). ",NULL
DM-1971,"ISR should write a DistortedTanWcs to the assembled exposure","The ISR task should write a DistortedTanWcs to the assembled exposure, instead of the pure TAN WCS that it starts with. (DistortedTanWcs is a plane TAN WCS plus a distortion model; the necessary distortion model can be directly read from the detector information in the camera geometry).  Warning: DistortedTanWcs cannot presently be persisted (DM-1970 is a ticket to fix this). Hence it is risky to put a DistortedTanWcs into a WCS. If the task then attempts to persist the exposure with that WCS (very likely if it persists the snaps) that will fail. Thus I am very nervous about doing this before at least a simplified version of DM-1970 has been implemented (e.g. persisting the WCS as a TAN-SIP WCS). However, others have recommended doing it now so I am not listing DM-1970 as a blocker.",NULL
DM-1972,"upgrade SWIG to 3.0.8 or later","SWIG 3.0.5 is now out and has several useful fixes w.r.t. 3.0.2 (which we are presently using) including:  - A bug we've had to work around involving templated methods of classes  - improved handling of new-style enums (they are no longer hoisted into the global namespace, which was a serious misfeature of SWIG 3.0.2)    I propose we try it out using buildbot (when we have some time), and if it works, we adopt it. Adopting it will help us relax the restrictions on what C++11 features can be used in C++ header files.",NULL
DM-1975,"inconsistent test failure in obs_lsstSim","Josh Meyers just saw a test failure in obs_lsstSim (tag v10_0) while trying to install the stack, but it went away when he tried it again.  I suspect a test that uses the same file as another, as we've seen in other packages.",NULL
DM-1976,"Fix SQL using join and ""select alias.fieldname""","Next query fails:  {code} mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case04_qserv -e ""SELECT sce.filterName, sce.field, sce.camcol, sce.run, s.deepForcedSourceId, s.ra, s.decl, s.x, s.y, s.psfFlux, s.psfFluxSigma, s.apFlux, s.apFluxSigma, s.modelFlux, s.modelFluxSigma, s.instFlux, s.instFluxSigma, s.shapeIxx, s.shapeIyy, s.shapeIxy, s.flagPixInterpCen, s.flagNegative, s.flagPixEdge, s.flagBadCentroid, s.flagPixSaturCen, s.extendedness FROM DeepForcedSource AS s, Science_Ccd_Exposure AS sce WHERE (s.scienceCcdExposureId = sce.scienceCcdExposureId) LIMIT 100"" {code}  Here's qserv-czar.log: {code} 0207 02:44:09.082 [0x7f7cd3fff700] DEBUG root (build/qdisp/MessageStore.cc:49) - Msg: 7140 2000 Complete (success) 0 1423273449 0207 02:44:09.082 [0x7f7cd3fff700] DEBUG root (build/qdisp/MessageStore.cc:49) - Msg: 7308 2000 Complete (success) 0 1423273449 0207 02:44:09.082 [0x7f7cd3fff700] DEBUG root (build/qdisp/MessageStore.cc:49) - Msg: 7310 2000 Complete (success) 0 1423273449 0207 02:44:09.088 [0x7f7cd3fff700] INFO  root (build/rproc/InfileMerger.cc:307) - Merging w/CREATE TABLE qservResult.result_4153079819 SELECT sce.filterName,sce.field,sce.camcol,sce.run,s.deepForcedSourceId,s.ra,s.decl,s.x,s.y,s.psfFlux,s.psfFluxSigma,s.apFlux,s.apFluxSigma,s.modelFlux,s.modelFluxSigma,s.instFlux,s.instFluxSigma,s.shapeIxx,s.shapeIyy,s.shapeIxy,s.flagPixInterpCen,s.flagNegative,s.flagPixEdge,s.flagBadCentroid,s.flagPixSaturCen,s.extendedness FROM qservResult.result_4153079819_m LIMIT 100 0207 02:44:09.088 [0x7f7cd3fff700] ERROR root (build/rproc/InfileMerger.cc:359) - InfileMerger sql error: Error applying sql. Error 1054: Unknown column 'sce.filterName' in 'field list' Unable to execute query: CREATE TABLE qservResult.result_4153079819 SELECT sce.filterName,sce.field,sce.camcol,sce.run,s.deepForcedSourceId,s.ra,s.decl,s.x,s.y,s.psfFlux,s.psfFluxSigma,s.apFlux,s.apFluxSigma,s.modelFlux,s.modelFluxSigma,s.instFlux,s.instFluxSigma,s.shapeIxx,s.shapeIyy,s.shapeIxy,s.flagPixInterpCen,s.flagNegative,s.flagPixEdge,s.flagBadCentroid,s.flagPixSaturCen,s.extendedness FROM qservResult.result_4153079819_m LIMIT 100  0207 02:44:09.088 [0x7f7cd3fff700] INFO  root (build/rproc/InfileMerger.cc:313) - Cleaning up qservResult.result_4153079819_m 0207 02:44:09.089 [0x7f7cd3fff700] INFO  root (build/rproc/InfileMerger.cc:325) - Merged qservResult.result_4153079819_m into qservResult.result_4153079819 0207 02:44:09.089 [0x7f7cd3fff700] INFO  root (build/ccontrol/UserQuery.cc:218) - Joined everything (success) 0207 02:44:09.091 [0x7f7cd3fff700] INFO  root (app.py:569) - Query exec (5) took 0.092488 seconds 0207 02:44:09.092 [0x7f7cd3fff700] INFO  root (app.py:574) - Final state of all queries success Runner running job 0207 02:44:09.096 [0x7f7cd35fe700] INFO  root (build/ccontrol/UserQuery.cc:251) - Discarded UserQuery(5) {code}",NULL
DM-1983,"Error conditions for SsiSession need to be sent properly","When error conditions are detected in SsiSession (unowned chunk, garbage request), all we do is ""return false;"". The right thing to do bundle up and send back a well-formed error message. The code to change is in SsiSession::ProcessRequest().   Estimated work: 4 sp, but does not include crafting tests, which may be tricky and perhaps not worth it.",NULL
DM-1984,"Fix testProcessCcd.py in pipe_tasks","There seem to be a problem with pipe_tasks. I run into it by doing ""eups distrib install obs_sdss"", pipe_tasks (version 10.0+1) fails. Once it fails I can reproduce it by doing:   * cd $EUPS_DIR/../EupsBuildDir/Linux64/pipe_tasks-10.0+1/pipe_tasks-10.0+1  * setup -k -r . * ./tests/testProcessCcd.py  and it fails with  {code} rocessCcd.measurement: Measuring 167 sources (167 parents, 0 children)  processCcd WARNING: Persisting background models processCcd: Matching icSource and Source catalogs to propagate flags. F. ====================================================================== FAIL: testProcessCcd (__main__.ProcessCcdTestCase) test ProcessCcdTask ---------------------------------------------------------------------- Traceback (most recent call last):   File ""./tests/testProcessCcd.py"", line 73, in testProcessCcd     self.assertAlmostEqual(bg0Arr.mean(), 325.4484, places=1) AssertionError: 321.26692337917484 != 325.4484 within 1 places  ---------------------------------------------------------------------- Ran 2 tests in 5.970s {code} ",NULL
DM-1985,"Reject queries involving qserv_areaSpec_box on non-partitioned tables","sql> select * from DeepForcedSource where qserv_areaSpec_box(0.4,0.9,0.6,1.1) LIMIT 3000 [2015-02-10 16:12:14] [42S02][1051] Unknown table 'result_4492663598' [2015-02-10 16:12:14] [Proxy][4120] Error during execution: -1 Ref=1 Resource(/chk/LSST/7140): 20150210-16:12:01, Error merging result, 1470, [1064] You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'QST_1_.,0.4,0.9,0.6,1.1)=1 LIMIT 3 (-1)  ",NULL
DM-1986,"Add support for queries involving qserv_area_spec on non-partitioned tables","(by Daniel) qserv_areaspec_box asks qserv to choose chunk/row coverage according to the director table. Generally, it detects the director table from the query, computes the region against the index, and then substitutes the right directive. But in this case, there is no director table, only a child table. Hence, even if we know what the director table is, its columns are not available to use in the query, unless we rewrite to add an equi-join with the director table. The best we can do in this case (without adding a join), is to use the parameters for determining chunk coverage, and then covering all rows in those chunks. The user could specify additional conditions to apply spatial filtering based on other (non-partitioning) columns.  Another thing we could do is to detect scisql region directives in the WHERE clause, but this expands the detection and dependence on scisql name/syntax (the first is in the angle distance condition when determining spatial join evaluability).  Anyway, what I'm trying to say is that the right thing to do isn't completely clear, and the possible options are not that simple to implement (though the ones I've listed are certainly possible, and should be much easier than refmatch support). ",NULL
DM-1993,"Rewrite afw::image::Wcs and subclasses","The afw.::image::Wcs class is not a well-written base class for Wcs, but is instead a thin layer on wcslib. Furthermore it contains a number of methods that I think can be eliminated or made free functions, including: - translateReferencePixel - rotateImageBy90 - flipImage - isFlipped (note that the middle two of these mutate the Wcs yet are marked ""const"".)  Here are concrete suggestions: * Change Wcs to an abstract base class for Wcs that knows nothing about wcslib * Make Wcs and its subclasses immutable * Think carefully about which methods we really need. Make other required behavior a free function. - Make a new subclass of Wcs that wraps wcslib - Rename TanWcs to TanSipWcs, for clarity. The current name is very confusing to those not familiar with the code.",NULL
DM-1995,"Visibility of ""Epic Status""","In addition to the normal Status value for all JIRA issues (and the associated workflow), JIRA-Agile epics have an additional field, the Epic Status.  This field is generally well hidden in JIRA issue displays and edit dialogs, but it can be brought to light in an issue filter (see the EpicStatusFilter.png attachment).  The Epic Status can pretty much only be changed, as far as I can see, by using the pop-up menu for epics in a JIRA-Agile board's Plan view (see the EpicMenu.png attachment for the ""Mark as Done"" action).  The latter is the only way that I can see to get an epic to disappear from the Plan view, and it is apparently completely unrelated to the ordinary Status.  This is pretty counter-intuitive.  The request here is to either, ideally, link the two so that either way of marking an epic as Done updates both status values, or at least to facilitate the display of the Epic Status in the normal display of epics as issues.",NULL
DM-2,"Document basic EUPS functionality","As a user, I need to know how to:  * Install EUPS itself * Install new packages * Setup packages * Selectively override a package being setup-ed ",NULL
DM-2004,"Remove pex_harness from lsst_distrib","Remove pex_harness from lsst_distrib so we can retire pex_harness. One package that may still rely on pex_harness is datarel",NULL
DM-2009,"Please add cbegin and cend to afw tables","It would be helpful if afw tables had the C++11 iterator methods cbegin and cend that return iterators-to-const.",NULL
DM-2029,"Update Confluence build instructions to match github move","The Build tool documentation   https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool  refers to   git clone git@git.lsstcorp.org:LSST/DMS/devenv/lsstsw.git  This should be updated to reflect the move to GitHub  git clone https://github.com/lsst/lsstsw.git   ",NULL
DM-2033,"Add XYTransform subclass to model sensor level anomalies","Initially, this development will be used to model the edge rolloff effect seen in LSST sensors owing to lateral field distortions from the guard drain around the perimeter of the sensors.  See the discussion here:  https://confluence.slac.stanford.edu/display/LSSTDESC/TWG+org+page%2C+Feb+2015+DESC+meeting  The edge rolloff can modeled to first order with separable functions in the x-and y-directions.  Here is an early analysis of the effect:  https://confluence.slac.stanford.edu/display/LSSTCAM/Edge+Roll+Off  I'll add a SeparableXYTransform class and an abstract Functor base class to afw::geom to allow for different kinds of 1D transformations.",NULL
DM-204,"MySQL wrappers","Finish implementing the mysql wrappers for python. Most is already implemented, but it can't be deployed because of logging. This is related to DMTF-16570-28",NULL
DM-2043,"Add cycle definitions to JIRA","For longer term planning purposes it'd be useful to have in Jira cycles up to 2020 (LDM-240 covers up to 2020). In jira, we currently have up to Winter 2016",NULL
DM-2056,"Copy of agile board can't be configured","I just made a copy of an existing board ""Database Team"", because I wanted to play with something, but the ""Copy of Database Team"" board that I created has been created with wrong permissions. Essentially, it has no administrator assigned, I can't delete it, and I can't configure it. I didn't expect that the copy I created won't be configurable by me. It looks like a bug to me.  I am going to create another board using ""create board"" now, so it is not blocking me.",NULL
DM-206,"Finalize selecting the database technology","The database technology has been selected (described in LDM-135), need to get it captured in the official documents.",NULL
DM-2064,"Implement a load-verification tool to insure data is correctly loaded","Once the data has been loaded, this tool has to its check correct state  Non exahustive list: - empty chunk file, - xrootd exported DB, - data  tables - overlap tables, - data_0123456789 tables - chunkId, subChunkId columns existence ...",NULL
DM-2067,"Revisit ownership of objects related to XrdSsi interface","Discussed at DB hangout 2015/02/18.   Need to revisit memory ownership of xrdssi objects.  Also, run Qserv through valgrind to double check for other unexpected memory leaks.",NULL
DM-2098,"Double-check WBS in db epics","K-T, I am a little hazy from time to time which tasks belong to which WBS. Can you double check I am on the right track? Jira filter that might help: {code} project = DM AND issuetype = Epic AND status = ""To Do"" AND component in (cat, ""Data Archive"", database, db, metaserv, imgserv, qserv, webserv, XLDB) ORDER BY cf[10500] ASC, summary ASC, cf[10501] DESC {code}",NULL
DM-2109,"F16 Replace mysql-proxy - czar xml-rpc protocol with REST","xml-rpc for communication between mysql proxy and czar is pretty sketchy and we always treated it as a temporary solution. Once RESTful API to qserv matures, we should replace the xml-rpc with REST. This was discussed at https://confluence.lsstcorp.org/display/DM/Qserv+Hangout+2015-02-04",NULL
DM-2126,"Compiler Warnings","There are several compiler warnings being generated when building qserv that should be fixed. Most should be considered bugs, but there are a few non-bugs such as intentional missing breaks. It is possible to squash warnings by line, which might be the right approach. More information here:  https://gcc.gnu.org/onlinedocs/gcc/Diagnostic-Pragmas.html   The warnings:  Catching by reference is recommended 'UserQueryError'  UserQuery.cc   /qserv_a/core/modules/ccontrol    line 247    Code Analysis Problem  Class 'Checker' has virtual method 'operator ()' but non-virtual destructor    ResourceUnit.h    /qserv_a/core/modules/global    line 103    Code Analysis Problem  Class 'ResRowBuffer' has virtual method 'fetch' but non-virtual destructor    RowBuffer.cc    /qserv_a/core/modules/mysql    line 110 Code Analysis Problem  Class 'RowBuffer' has virtual method 'fetch' but non-virtual destructor    RowBuffer.h    /qserv_a/core/modules/mysql    line 68 Code Analysis Problem  Invalid project path: Include path not found (/afs/slac.stanford.edu/u/sf/jgates/work/qserv/$LOG_DIR/include). qserv_a        pathentry    Path Entry Problem  Member '_dirty' was not initialized in this constructor QuerySession.h    /qserv_a/core/modules/qproc    line 157    Code Analysis Problem  Member '_fieldOffset' was not initialized in this constructor RowBuffer.cc    /qserv_a/core/modules/mysql    line 131    Code Analysis Problem  Member '_hasChunks' was not initialized in this constructor QuerySession.h    /qserv_a/core/modules/qproc    line 157    Code Analysis Problem  Member '_hasMerge' was not initialized in this constructor QuerySession.cc    /qserv_a/core/modules/qproc    line 86    Code Analysis Problem  Member '_hasSubChunks' was not initialized in this constructor QuerySession.h    /qserv_a/core/modules/qproc    line 157    Code Analysis Problem  Member '_hasSubChunks' was not initialized in this constructor QuerySession.h    /qserv_a/core/modules/qproc    line 157    Code Analysis Problem  Member '_limit' was not initialized in this constructor SelectStmt.cc /qserv_a/core/modules/query    line 92    Code Analysis Problem  Member '_order' was not initialized in this constructor OrderByClause.h    /qserv_a/core/modules/query    line 66    Code Analysis Problem  Member '_request' was not initialized in this constructor QueryResource.h    /qserv_a/core/modules/qdisp    line 53    Code Analysis Problem  Member '_requestType' was not initialized in this constructor QservPath.h    /qserv_a/core/modules/obsolete    line 39    Code Analysis Problem  Member '_session' was not initialized in this constructor QueryResource.h    /qserv_a/core/modules/qdisp    line 53    Code Analysis Problem  Member '_sessionId' was not initialized in this constructor UserQuery.cc    /qserv_a/core/modules/ccontrol    line 261    Code Analysis Problem  Member 'bufferSize' was not initialized in this constructor TransactionSpec.h    /qserv_a/core/modules/qdisp    line 52    Code Analysis Problem  Member 'chunkCount' was not initialized in this constructor QueryContext.h    /qserv_a/core/modules/query    line 64    Code Analysis Problem  Member 'counter' was not initialized in this constructor testProtocol.cc    /qserv_a/core/modules/proto    line 47    Code Analysis Problem  Member 'metaSession' was not initialized in this constructor testPlugins.cc    /qserv_a/core/modules/qana    line 51    Code Analysis Problem  Member 'needsMerge' was not initialized in this constructor QueryContext.h    /qserv_a/core/modules/query    line 64    Code Analysis Problem  No break at the end of case    MergingRequester.cc /qserv_a/core/modules/ccontrol    line 113    Code Analysis Problem  No break at the end of case    SchemaFactory.cc /qserv_a/core/modules/mysql    line 168    Code Analysis Problem  No break at the end of case    ValueFactorFactory.cc /qserv_a/core/modules/parser    line 118    Code Analysis Problem  No break at the end of case    ValueFactorFactory.cc /qserv_a/core/modules/parser    line 166    Code Analysis Problem  Statement has no effect 'GOOGLE_PROTOBUF_VERIFY_VERSION' testProtocol.cc    /qserv_a/core/modules/proto    line 112    Code Analysis Problem  Suspicious semicolon    AggregatePlugin.cc /qserv_a/core/modules/qana line 83    Code Analysis Problem ",NULL
DM-2140,"Please publish latest GalSim and tmv packages","I've updated, tagged, and built (via buildbot) new versions of two packages:  - tmv 0.72.lsst1  - GalSim 1.2.lsst2 Eups tag b748 includes both of these.  Please publish eups distrib packages of these (they're dependencies of meas_extensions_shapeHSM, which I've just resurrected from bitrot, and would now like to include with the rest of the stack).  Alternatively, feel free to reassign to me if you also give me lsstsw privileges.",NULL
DM-2142,"LPM-72: Upscope/downscope update","* Find upscopes, prefer those that come as late as possible. Focus on optimizations within DM. * Upscope ideas:   - Special programs?   - Restore alert production at base? ",NULL
DM-2144,"Update LDM-240","* Consider each cell as a list of milestones to be achieved by a certain date * Disregard UML use cases for now * WBS change disallowed ",NULL
DM-2162,"eups distrib install fails for usernames containing hyphens","eups distrib install fails for usernames containing hyphens.   Offending logic seems to be in lock.py  {code} def listLockers(lockDir, globPattern=""*"", getPids=False): """"""List all the owners of locks in a lockDir"""""" lockers = [] for f in [os.path.split(f)[1] for f in glob.glob(os.path.join(lockDir, globPattern))]: who, pid = re.split(r""[-.]"", f)[1:] {code}",NULL
DM-2169,"make eups versions for non-git-tagged packages sort","Whenever lsstsw builds a package from a version that doesn't have a git tag, the eups version number is set to something like ""master-a235b31"", which doesn't provide any indication about its ordering relative to tagged versions.  This is helpful when the version is not on master, and the version instead includes some indication of the branch name, but for versions on master, it'd be more helpful to use the output of ""git describe"" directly, as that will provide a number that is derived from the most recent tag, sorts properly almost all the time, and is still traceable back to the original git commit.  I'm not sure where this version generation code actually exists - I'm pretty sure it's not sconsUtils, but it could be lsstsw or eupspkg.  It might even be something one of these makes configurable externally.",NULL
DM-2170,"colorterms uses an undefined variable","Lines 99 of colorterms returns p, which is not yet defined. I think it should return ""primary"".",NULL
DM-2180,"qserv-check-integration -I argument doesn't work","If you launch either ""qserv-check-integration -I 04"" or ""qserv-check-integration -I=04"", it actually runs test case 01 (but the long form, ""qserv-check-integration --case-id=04"", works correctly.)",NULL
DM-2183,"lsstsw does not clone from the GitHub repositories","The code below is from lsstsw/etc/settings.cfg.sh  It appears that lsstsw is still cloning packages from cgit.  At the very least, BASE needs to be changed and the DMS/ subdirectories need to be removed from export REPOSITORY_PATTERN  # the location of source repositories BASE='git://git.lsstcorp.org/LSST' SBASE='https://stash.lsstcorp.org/scm' export REPOSITORY_PATTERN=""$BASE/sims/%(product)s.git|$BASE/DMS/%(product)s.git|$BASE/DMS/devenv/%(product)s.git|$BASE/DMS/testdata/%(product)s.git|$BASE/external/%(product)s.git|$SBASE/sim/%(product)s.git"" ",NULL
DM-2184,"Change imgserv to use db for all database access","Database access should use lsst/Db for all database access.",NULL
DM-2185,"Examine obs_ packages for outdated astrometry config fields","DM-1576 changes the default astrometry task, which changes some config overrides for astrometry. Examine the config overrides in the obs_ packages for outdated fields.",NULL
DM-2196,"edit repos.yaml to include sncosmo","we are adding sncosmo as an external package in the stack.  We need to edit lsstsw/etc/repos.yaml to point to the correct repository",NULL
DM-220,"S14 New XRootD client","Original XrdSsi announcement from AndyH: {quote}  I have created a git repo in /u/sf/abh/xssi/xrootd (in AFS space) that  contains the Scalable Service Interface (XrdSsi). In fact,  /u/sf/abh/xssi/xrootd/src/XrdSsi contains all of the code for client- and  server-side services. You can clone this repo and play. It is based on the  xrootd that resides at git head in the github repo.  Working examples on how to use the services can be found in  /u/sf/abh/xfix/test where source files starting with XrdSsiCl show how to  implement client-side services while files starting with XrdSsiSv show how  to implement server-side services. The mkcl and mksv scripts build the  client and server examples, respectively.  To actually load and use the server-side service, you must build a shared  library and use the following directives in the xrootd config file:  xrootd.fslib libXrdSsi.so xrootd.async off ssi.svclib <path to the service library>  Note that libXrdSsi.so must in in some path in your LD_LIBRARY_PATH or use  an absolute path. There are some additional ssi-specific directives but  are not necessary for you to get started. Note that the shared library  that implements the example service is ""/u/sf/abh/xssi/test/libSsiSv.so"".  What missing?  1) Example on how to implement an active stream. 2) Example on how to implement a fully parallel server-side service  session. 3) The object that interfaces with the cmsd (it's there but does nothing). {quote}",NULL
DM-2200,"eupspkg TaP packages cannot contain files like README.md","(Original problem discovered by [~rbiswas])  Now that the packages for external products are on GitHub, it would be nice for them to contain files like the near-standard ""README.md"".  Unfortunately, {{eupspkg.sh}}'s {{default_prep}} command explicitly inspects tarball-and-patch (TaP) packages to make sure there are no other files in the package root directory.  I think this is a little excessive.  There's a bypass if the environment variable {{TAP_PACKAGE}} is set, but the package's table file has not yet been setup, so it's not possible to set this variable there.  (The comments for the {{default_prep}} command suggest that the dependencies have been setup, but it's not clear that this is necessarily the case if invoked by {{lsst-build}}.)  I suggest that we check instead for a {{./.tap_package}} file to indicate definitively that this is a TaP package.",NULL
DM-2204,"merge lsstsw branch repos_option onto master","Please merge lsstsw:repos_option branch into master.   As a rule of thumb, while branches sometimes need to be deployed for testing, I'd like to see this is a transient process and either backed out or merged into master in ~48 hrous. ",NULL
DM-2212,"Implement tools to authenticate at NCSA","Based on results of evaluation in DM-3838, begin to implement authentication tools for NCSA resources.    Assignees: Bill Glick  Duration: December 2015 - February 2016",NULL
DM-2219,"Remove explicit de-duplication code from the a.net catalog loader","meas_astrom utils.cc has code that explicitly checks for duplicate stars loaded from a.net index files. However, my understanding is that we generate indices in such a way that prevents duplications and have done so for quite some time. Thus we can probably remove this code and thereby speed up loading of reference objects from a.net index files.",NULL
DM-2220,"Clean up memory management in meas_astrom's utils.cc","utils.cc contains code to load reference objects from a.net index files. Unfortunately it uses C-like memory management that appears to not be fully exception-safe. This should be cleaned up.",NULL
DM-223,"S14 database schema and tune up","Tune database performance and assist people running production as needed with all database related issues.",NULL
DM-2241,"Raw image definition and usage","SUI needs to serve raw data to the user community. We want to understand the use cases and definition of raw data. More specifically, what meta data will be available in the FITS file that we call raw image? ",NULL
DM-2242,"Logging fails for ""'%s'""","Logging fails when the following line is passed: {code} log.debug("" %s "") {code}  The error: {code}   File ""./tests/logTest.py"", line 114, in testComplexString     log.debug(""%s"")   File ""/home/becla/stack/repos/log/python/lsst/log/log.py"", line 100, in debug     log("""", DEBUG, fmt, *args, depth=2)   File ""/home/becla/stack/repos/log/python/lsst/log/log.py"", line 94, in log     _getFuncName(depth), frame.f_lineno, fmt % args) TypeError: not enough arguments for format string {code}",NULL
DM-225,"Review existing CSS design / prototype",https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS,NULL
DM-2260,"Broken links on ""Creating a Task"" page","All of the links on  https://confluence.lsstcorp.org/display/~shaw/Creating+a+Task  are broken (or useless), as they are of the form ""file:///Volumes/d0/LSST/tmp/"":  E.g.,  file:///Volumes/d0/LSST/tmp/pipe_tasks/doc/html/pipe_tasks_write_cmd_line_task.html",NULL
DM-2261,"The TAN-SIP WCS fitter does a poor job with significant distortion and for high order SIP","The tan-sip fitter in meas_astrom (lsst.meas.astrom.sip.makeCreateWcsWithSip) seems to fit poorly with significant distortion and for high order SIP.  Perhaps the simplest way to see this is to modify the existing test testCreateWcsWithSip.py to use higher orders and/or more distortion.  However, I have also attached sample code (partly based on that test) that shows the problem in more detail. The points it fits are uniformly distributed over a grid in the effective bbox, and so can be as dense as desired in order to provide enough information to fit to high order.  Notes: * The sample code requires the current master of afw, since it uses DistortedTanWcs. * I observe that fitting a pure TAN WCS succeeds with order 3 and 4, but fails with order 5 * I observe that fitting an offset radial distortion fails mildly at order 3 and spectacularly at orders 5 and 6 * The code continues even after a test fails, in order to show how badly the higher-order SIP polynomials fail. Hence it claims to succeed at the end, even though tests failed. * The sample code uses crpix=15000, 4000 to emulate a CCD that is far from the center of the field. The fit bbox is centered on that crpix. Nonetheless, one issue may be that TAN-SIP WCS (or at least our fitter) cannot handle such a large crpix. ",NULL
DM-2266,"OCS Integration","The OCS software will be arriving sometime over the summer,and we need to integrate it with the simulator.",NULL
DM-2267,"OCS messages from AP to the OCS","Implement the mechanisms to send DDS messages back to the OCS from the AP system. ",NULL
DM-2271,"RFCs with DM and non-DM components do not get reported to HipChat","RFC-27 is an example that was posted to both ""DM"" and ""LSST"" as components.",NULL
DM-2273,"Evaluate complex event processing software packages","We need to do a survey of complex event processing software packages including Esper, our own ctrl_evmon package, and other CEP packages we can find.",NULL
DM-2274,"Write up results and suggested ""next steps""","Write white paper on the evaluation of complex event processing packages.",NULL
DM-2275,"OCS software integration","Work with the OCS DDS software so that we can get it integrated with the DM stack applications.    The OCS team will be here sometime over summer 2015 to help us understand the requirements for the DDS software so we can do this.",NULL
DM-2276,"Apply for PyCHARM open-source licence","Form is here: https://www.jetbrains.com/eforms/openSourceRequest.action?licenseRequest=PCP05NS  LSST release info is here: https://www.jetbrains.com/eforms/openSourceRequest.action?licenseRequest=PCP05NS",NULL
DM-2284,"/ticket one more try to fix the team else fresh brainz tomorrow","Filed by Zapi 2015-03-07T10:32:14+0000",NULL
DM-2285,"/ticket okay will have to settle for assigning it to me for now","Filed by Zapi 2015-03-07T10:47:46+0000",NULL
DM-229,"S14 New logging for the whole DM software","Implement log4cxx-based logging that will unify logging information from python and C++.",NULL
DM-2296,"Rewriting column names in SQL queries derails Qserv","The following query will break Qserv:    {code}  SELECT objectId, objectId as o1.objectId FROM Object LIMIT 1  {code}    {code}  ERROR 4005 (Proxy) at line 1: Cannot connect to Qserv master;   Exception in RPC call: ...tack/Linux64/lua/5.1.4/share/lua/5.1/xmlrpc/http.lua:41:  connection refused  {code}    Here is what vanilla MySQL does:  {code}  ERROR 1064 (42000) at line 1: You have an error in your SQL syntax; check the manual  that corresponds to your MySQL server version for the right syntax to use near   '.objectId FROM   Object LIMIT 1' at line 1  {code}",NULL
DM-2297,"Associate documentation with new Mask planes","It's possible to add and remove mask planes dynamically, and their bit values get correctly written to FITS headers if that's how you choose to persist the Masks.  Unfortunately, the only way to guess what they mean is to scrutinise the name.  Please add the ability to associate a doc string with each mask plane, which should be persisted along with the data, and made available in sensible ways in the Mask API. ",NULL
DM-2298,"Provide a way to merge multiple afwTable catalogs with identical schema","Concatenating afwTable catalogs to create one catalog with contiguous memory isn't quite as simple as it should be.  One reason is that the reserve() member doesn't do what you might expect as it makes no guarantee of contiguous storage [this is why std::deque::reserve doesn't exist].  We should provide a utility function that does this right (Jim tells me that there is a right way -- I know a variety of ways that work that don't quite work when you allow for things like slots, or that require a deep copy of the result)   ",NULL
DM-2302,"Revisit db configuration for metaserv and dbserv ","Need to revisit configuration for dbserv and metaserv to insure we have a ""production"" setup separated from ""testing"", and that the testing never alters the production setup. This involves revisiting mysql account used by metaserv/dbserv.",NULL
DM-2303,"Odd and intermittent eups failure when expanding tables","Build 4581 http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/4581 failed on obs_sdss with an error I’ve never seen before:  eups expandtable: Processing /lsst/home/lsstsw/stack/Linux64/obs_sdss/tickets.DM-1576-g4aab34585a+1/ups/obs_sdss.table: could not find MARK  K-T searched for it and it appears to be an unpickling error. I resubmitted the job and it ran the next time. ",NULL
DM-2304,"Internally cite the Bernstein and Jarvis (2002) paper in the SDSS image moment shape algorithm code","The algorithm to ""Measure the image moments of source using adaptive Gaussian weights"" implemented in meas_base should reference the paper from which the algorithm is derived (Bernstein and Jarvis (2002) )  The reference is here:  http://adsabs.harvard.edu/abs/2002AJ....123..583B  ",NULL
DM-2308,"Improve hacks in PhotoCalTask for dealing with reference flux uncertainties","In reviewing DM-1576 Jim Bosch commented on two existing ugly hacks in PhotoCalTask for dealing with uncertainties in reference object flux: - The reference flux is not handled in a principled way - The square root of the flux is used as the estimated error if the reference flux error is not known  For now I have modified the code to ignore reference flux error. I will note, however, that any catalog that is suitable to be used for accurate photometric calibration ought to have known flux errors. So I am not convinced that the ugly hack of using sqrt(flux) is a very serious issue. Still, it might be smarter to ignore the reference flux error if it is unknown? For now I will ignore it in any case.",NULL
DM-2310,"recent change to tests/image.py segfaults on MacOS","The commit cb7650a adds a test to tests/image.py ""testDM882"" taht segfaults on my MacOS computer running 10.9.5",NULL
DM-2311,"Fix select expression """"SELECT *, NULL as poly""","The following (case04) query crashes Qserv:     {code:bash}  SELECT *, NULL as poly FROM Science_Ccd_Exposure LIMIT 3500     ERROR 4005 (Proxy) at line 1: Cannot connect to Qserv master; Exception in RPC call: ...tack/Linux64/lua/5.1.4/share/lua/5.1/xmlrpc/http.lua:41: closed  {code}",NULL
DM-2313,"Pass first protobuf header of xrootd response in xrootd meta","It would be easy to send the first byte below, and even the first protobuf header in xrootd metadata (i.e. out of band and packed with the message informing the client that a response is available on the server). This optimization would prevent performing a whole xrootd transaction to send only a few bytes.    This problem doesn't occur for following (i.e. non-first) data messages because they contains the header of the next message at their end.    AndyH needs first to implement the API in xrootd.    Extract of qserv-czar.log    {code:bash}  0312 18:42:05.489 [0x7f6594dfa700] INFO  root (build/qdisp/QueryRequest.cc:180) - ProcessResponse[data] with buflen=1 (more)                                                               0312 18:42:05.489 [0x7f6594dfa700] DEBUG root (build/ccontrol/MergingRequester.cc:78) - HEADER_SIZE_WAIT: Resizing buffer to 25                                                          Task 0x7f6588ea00b0 SetBuff Async Status=isReady                                                                                                                                           Task 0x7f6588ea00b0 sess=ok Status = 1 isReady                                                                                                                                           Task Handler calling ProcessResponseData.                                                                                                                                                  0312 18:42:05.490 [0x7f65957fb700] INFO  root (build/qdisp/QueryRequest.cc:180) - ProcessResponse[data] with buflen=25 (more)                                                              0312 18:42:05.490 [0x7f65957fb700] DEBUG root (build/ccontrol/MergingRequester.cc:90) - HEADER_WAIT: Resizing buffer to 2777471                                                            Task 0x7f6588ea00b0 SetBuff Async Status=isReady                                                                                                                                           Task 0x7f6588ea00b0 sess=ok Status = 1 isReady                                                                                                                                             Task Handler calling ProcessResponseData.                                                                                                                                                  0312 18:42:05.517 [0x7f65961fc700] INFO  root (build/qdisp/QueryRequest.cc:180) - ProcessResponse[data] with buflen=2777471 (more)        {code}",NULL
DM-2315,"-c on newinstall.sh is broken","The eups directory is made read only so the installer can't install over itself.  Either -c should detect this or the eups should be made read/write.  ",NULL
DM-2317,"Revisit simple parser code in qserv and metaserv","See qserv/admin/bin/qserv-admin.py and metaserv/bin/metaAdmin.py. The code could be unified. Perhaps we should consider python's built-in cmd module?",NULL
DM-2321,"many .i files contain invalid SWIG code","Many .i files contain outdated code that is not compatible with modern SWIG. They use %python {...} instead of %python %{...%}. I found the following files by searching packages through lsst_apps:  daf_base baseLib.i afw cameraGeomLib.i, coord.i, footprint.i, AffineTransform.i, Box.i, CoordinateBase.i, ellipsesLib.i, Separable.i, Extent.i, geomLib.i, LinearTransform.i, Span.i, TransformMap.i, image.i, imageLib.i, imageSlice.i, mask.i, maskedImage.i, background.i, spatialCell.i, Match.i, utils.i  I even see it in swig, which is interesting. Apparently {} is acceptable under some circumstances, but not all. I found the problem when I tried to build the stack using SWIG 3.0.5.  Note for posterity: after fixing this problem in daf_base, that package still builds fine and passes all its tests with SWIG 3.0.2. However, although it builds with SWIG 3.0.5 it fails one of its tests with many errors as follows. I think this is a known bug in SWIG 3.0.5 relating to C++ default argument values.  {code} Traceback (most recent call last):   File ""tests/PropertyList.py"", line 186, in testAddVector     apl.set(""ints"", v)   File ""/Users/rowen/LSST/lsstsw/build/daf_base/python/lsst/daf/base/baseLib.py"", line 4220, in _PL_setValue     return _propertyContainerSet(self, name, value, _PL_typeMenu, *args)   File ""/Users/rowen/LSST/lsstsw/build/daf_base/python/lsst/daf/base/baseLib.py"", line 4153, in _propertyContainerSet     return getattr(container, ""set"" + typeMenu[t])(name, value, *args)   File ""/Users/rowen/LSST/lsstsw/build/daf_base/python/lsst/daf/base/baseLib.py"", line 3555, in setInt     return _baseLib.PropertyList_setInt(self, name, value, comment, inPlace) NotImplementedError: Wrong number or type of arguments for overloaded function 'PropertyList_setInt'.   Possible C/C++ prototypes are:     lsst::daf::base::PropertyList::set< int >(std::string const &,int const &,bool)     lsst::daf::base::PropertyList::set< int >(std::string const &,int const &)     lsst::daf::base::PropertyList::set< int >(std::string const &,std::vector< int,std::allocator< int > > const &,bool)     lsst::daf::base::PropertyList::set< int >(std::string const &,std::vector< int,std::allocator< int > > const &)     lsst::daf::base::PropertyList::set< int >(std::string const &,int const &,std::string const &,bool)     lsst::daf::base::PropertyList::set< int >(std::string const &,int const &,std::string const &)     lsst::daf::base::PropertyList::set< int >(std::string const &,std::vector< int,std::allocator< int > > const &,std::string const &,bool)     lsst::daf::base::PropertyList::set< int >(std::string const &,std::vector< int,std::allocator< int > > const &,std::string const &)     lsst::daf::base::PropertyList::set< int >(std::string const &,int const &,char const *,bool)     lsst::daf::base::PropertyList::set< int >(std::string const &,int const &,char const *)     lsst::daf::base::PropertyList::set< int >(std::string const &,std::vector< int,std::allocator< int > > const &,char const *,bool)     lsst::daf::base::PropertyList::set< int >(std::string const &,std::vector< int,std::allocator< int > > const &,char const *) {code}",NULL
DM-2325,"Setup VM for Webserv Service","Webserv as a service Why needed? For SUI. To gain experience with running continuously. What is needed? 1 VM: 4 cores, 8 GB RAM, small local storage, access to ncsa /nfs When needed: ASAP Typical usage: running webserv and DataCat as a service, continuously",NULL
DM-2326,"Setup VM for Qserv as a service","Why needed? For SUI. To gain experience with running continuously. What is needed?    1/2 year: 4 VM, 8 cores, 16 GB RAM, 2 TB local storage, sudo to serve DC_W13_Stripe82 or equivalent   ~2 year time frame: 8-16 VMs, each: 8 cores, 16 GB RAM, 2 TB local storage, sudo    could go to much higher numbers (even like 50-100 VMs) if we have funding and if SUI would really find it useful. When needed soon Typical usage: 24x7 Qserv service for SUI",NULL
DM-2328,"buildbot often saves irrelevant artifacts after a failed run","After a failed run buildbot often saves artifacts from packages that it did not build yet (and perhaps sometimes from packages that it built successfully; I'm less sure about that and less worried about it).  http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/4665/steps/shell/logs/stdio  This run failed while building daf_base, but when it saved artifacts it saved afw, as well, even though afw had not yet been rebuilt. ",NULL
DM-2332,"Remove obsolete coord/Utils.h","This file is long obsolete; marked as ""remove this once ticket branch #1642 is done"". [The corresponding ticket|https://dev.lsstcorp.org/trac/ticket/1642] predates JIRA and was marked as done 3 years ago.",NULL
DM-2344,"absence of afwdata should not disable all afw tests","Some afw tests require afwdata, but most do not.  These should be disabled at a per-file (or per-test) level, not in sconsUtils.",NULL
DM-2345,"false positives in sconsUtils check for test failures","sconsUtils' check for test failures seems to blindly look at {{.tests/*.failed}}, without considering whether the tests were actually run on that build.      As a result, if a test test fails on one branch, and then we switch to a new branch where that test was removed, sconsUtils continues to report the test as a failure.    In addition, if we have test failures in one build, and then repeat the build and do not build the tests, sconsUtils continues to report test failures.  This creates the illusion that tests are being run (particularly a problem in conjunction with DM-609).",NULL
DM-2346,"SWIG code enclosed in %python %{ should be outdented to the left margin","In swig .i files code contained in %pythoncode %{...%} blocks must be outdented to the left margin for the code to be reliably correctly indented in the interface file and and for comments that start with # to be reliably handled as comments.  Comments may have leading characters deleted (including the #) which can lead to part of the comment being treated as code. Another failure mode is that the comment has the wrong indentation level. Fortunately these errors usually result a syntax error, or are innocuous.  Incorrectly indented code can lead to errors that are very hard to detect. One common result is that the added function appears as as a function definition inside another function. Hence the failure produces no python syntax errors, but the added function cannot be called.  The problem does not always lead to improperly indented code and in some cases I suspect the code is being correctly indented, since I doubt it would work otherwise. Even then it is safest to fix the problem, if only to provide a correct example for others.",NULL
DM-2350,"ProcessImageTask should be an abstract base class","ProcessImageTask is a bit confusing in that it is a command-line task but has no run method and must be subclassed to use. I think it would be clearer if it was an abstract base class and had an abstract run method.",NULL
DM-2351,"Clarify ability to recreate LSST (DM) Python objects from the data archive","The request (from the Level 3 perspective) is to clarify our expectations for which LSST Python object types can be readily recreated from data retrieved from the LSST Archive, and how explicitly the stack will support this recreation.  In other words, roughly, for what types do we support something like an object-relational mapping?  This is a key point in trying to understand what the Level 3 environment will be like for users.  I want to be sure that I understand any differences in what is possible (other than issues related solely to I/O performance/throughput) on resources at a DAC vs. on computers outside the LSST project-provided facilities.  For images, it's clear that a user can use the documented ability to retrieve LSST image data in FITS format to read that FITS data into an LSST DM Python image class.  Ideally this would all be through the Butler, and presumably this is possible for all classes of image data, including calibration images, that are available from the Archive (per the DPDD specification of the official data products).  It is much less clear to me whether any form of object-recreation, short of a full recomputation, is envisioned for the Python objects that lie behind our catalog entries (Object, Source, etc.) - or for derived metadata.  For instance, can the computed PSF model for an image be retrieved from the database and readily recreated in Python in a form satisfying our PSF API?  A WCS?  If recreation is supported, what will those interfaces look like? ",NULL
DM-2359,"Improve DbPool initialization","It currently takes already created Db object, it should take **kwargs and create it when needed.",NULL
DM-2360,"Define and respect an ""ignore-me"" mask plane in object detection","This is a request to create a mask bit indicating pixels that should be ignored for all purposes, including object detection.  The idea for this is images with large contiguous regions that contain no useful data.  Specifically, a region with this mask bit set would be ignored in the object detection stages.  A specific use case motivating this request is the use of the LSST DM software on co-adds from sets of individual dithers (potentially ISRed and combined outside DM stack).  These co-adds have effectively circular regions of useful validity, and corners that should be ignored.  In principle, the variance of these regions should also be set high, but the proper estimation of variance in regions of little data is not always robust (and quite frankly is often done wrong by the user -- including this one).  The FITS image array must be a rectangle, so the use of a mask bit (which can be visualized as a plane in the mask) to indicate that these regions should be ignored at all stages would be the clearest way of marking this.",NULL
DM-2362,"lsst-dev is unreachable","I'm unable to ssh into or ping lsst-dev from multiple different source networks.  I'm unaware of a scheduled maintenance period.  {code} $ ssh lsst-dev.ncsa.illinois.edu -v OpenSSH_6.6.1, OpenSSL 1.0.1k-fips 8 Jan 2015 debug1: Reading configuration data /home/jhoblitt/.ssh/config debug1: Reading configuration data /etc/ssh/ssh_config debug1: /etc/ssh/ssh_config line 56: Applying options for * debug1: Control socket ""/home/jhoblitt/.ssh/master/jhoblitt@lsst-dev.ncsa.illinois.edu:22"" does not exist debug1: Connecting to lsst-dev.ncsa.illinois.edu [141.142.237.30] port 22. debug1: connect to address 141.142.237.30 port 22: Connection timed out ssh: connect to host lsst-dev.ncsa.illinois.edu port 22: Connection timed out {code}",NULL
DM-2365,"Stray component in DM JIRA project","There is a stray component in the DM JIRA project, with the name ""start to outline an expandable Python framework for advanced users"".  Presumably this was created when someone typed an issue title into the component field, perhaps when DM-1906 was created.  Can it be removed from the components list?",NULL
DM-2366,"Outdated ""Agile"" link on main page sidebar for DM project","On the main page for the JIRA DM project, there is a sidebar, and on that sidebar the ""Agile"" link (which a naive outsider might easily click on):  https://jira.lsstcorp.org/browse/DM?selectedTab=com.pyxis.greenhopper.jira:greenhopper-project-panel-tab  points to something very outdated and potentially misleading.  Suggest removing the link, or redefining it to point to a reasonable default Agile board.",NULL
DM-2368,"GaussianCentroid should set an error","According to [{{GaussianCentroid.h}} in {{meas_base}}|https://github.com/lsst/meas_base/blob/master/include/lsst/meas/base/GaussianCentroid.h#L67] the {{GaussianCentroid}} algorithm ""does not currently set an error, though it should"".",NULL
DM-2369,"NaiveCentroid should probably report an error","According to [{{NaiveCentroid.h}} in {{meas_base}}| https://github.com/lsst/meas_base/blob/master/include/lsst/meas/base/NaiveCentroid.h#L59], the {{NaiveCentroid}} algorithm ""does not currently report an error, but it probably should"".",NULL
DM-2372,"mysql install fails on","There is a bug that causes SED to complain on some OSX builds.  The typical fix has been to add `export LANG=C` to the build script.  It turns out that is not always sufficient.  The fix is to add `export LC_CTYPE=C` as well.",NULL
DM-2374,"lsst-dev:/tmp has noexec set","/tmp on lsst-dev is mounted with the noexec flag and this breaks the build of at least mysql.  No doubt, the build of other packages will fail on the next rebuild.  Presumably this is a recent change as the last nightly succeeded.  Please revert this change immediately.",NULL
DM-2378,"Determine if MERGE_ERROR handling is correct","QueryRequest::ProcessResponse now gives up without retrying when it gets a MERGE_ERROR. Some research need to go into finding out what causes this error and if this is the correct course of action.",NULL
DM-2379,"Fix failure in afw statistics when using numpy arrays","Fix a failure in statistics when using numpy arrays.  Here is an example from Robert Lupton: {code} import numpy as np import lsst.afw.image as afwImage import lsst.afw.math as afwMath   def bug():     for arr in [afwImage.ImageF(10, 30),                 np.linspace(0, 10.0, dtype='float64'),                 np.linspace(0, 10.0, dtype=float),                 np.linspace(0, 10.0, dtype='float32')             ]:         print type(arr), afwMath.makeStatistics(arr, afwMath.MEAN).getValue()   if __name__ == ""__main__"":     bug() {code}  This fails with a complaint about incorrect arguments.  Paul Price said ""I think every place that takes std::vector<someNumericType> should also take an ndarray."" and offered the following link as an example of adding ndarray support to a piece of code. A commit shortly after that one shows how the .i file was updated to hide the vector version (since the ndarray version is used by Python). https://github.com/HyperSuprime-Cam/afw/commit/b8578746d69920bc1e1089cca4b4acb230f0e8d5  Jim Bosch proposes that we switch from std::vector to ndarray for array-like arguments. This is easy to make work with SWIG and it does so in a very efficient way (sharing memory). It is possible to offer both interfaces in C++ (ndarray and std::vector) but it's not clear there is any need, other than preserving backwards compatibility. If we do that, then hide the std::vector interface from Python. ",NULL
DM-238,"lsst-build --version-git-repo doesn't increment +N on a per-verson basis","The +N numbers should be incremented for each new (product, product_version, dependencies) tuple, not (product, dependencies).",NULL
DM-2384,"Upgrade boost to 1.59","We are at boost 1.55 and should try upgrading to 1.59, if only to reduce the number of register warnings when using C++11.",NULL
DM-2386,"unsafe use of namespaces in pex_policy","Dictionary.cc in pex_policy has ""using namespace std"" and ""using namespace boost"", which is unsafe and appears to be incompatible with boost 1.57.0 (the build fails with a warning about make_shared being ambiguous). ""using namespace boost"" should be removed.",NULL
DM-2388,"lsst-dev seems unusally slow","A CI run on lsst-dev has been in progress since 0947 CDT.  ~5 hours is much slower than is typically expected for a build to complete.  {code} lsstsw   16902  3007  0 09:47 ?        00:00:00 /bin/bash /home/lsstsw/buildbot-scripts/lsstswBuild.sh --branch tickets/DM-2386 --builder_name DM_stack --build_number 4910 {code}  The CPU load on the system is extremely low.  iostat does not show much of a local load or high awaits.  There are I/O timeouts in the dmesg but I don't have access to the syslog to find the associated time stamps as to when they occured.  {code} nfs: server lsst20.ncsa.illinois.edu not responding, still trying nfs: server lsst20.ncsa.illinois.edu OK INFO: task swig:1370 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. swig          D 0000000000000013     0  1370    592 0x00000080  ffff883fc83a9b68 0000000000000046 ffff883fc83a9ab8 ffffffffa054536e  ffff883fc83a9ae8 ffffffffa0545c10 ffff882051944840 ffff883fc83a9b18  ffff8820519448f0 ffff880d802032c8 ffff8840502d3058 ffff883fc83a9fd8 Call Trace:  [<ffffffffa054536e>] ? rpc_make_runnable+0x7e/0x80 [sunrpc]  [<ffffffffa0545c10>] ? rpc_execute+0x50/0xa0 [sunrpc]  [<ffffffff810aaa21>] ? ktime_get_ts+0xb1/0xf0  [<ffffffff811242d0>] ? sync_page+0x0/0x50  [<ffffffff8152a233>] io_schedule+0x73/0xc0  [<ffffffff8112430d>] sync_page+0x3d/0x50  [<ffffffff8152acff>] __wait_on_bit+0x5f/0x90  [<ffffffff81124543>] wait_on_page_bit+0x73/0x80  [<ffffffff8109eb80>] ? wake_bit_function+0x0/0x50  [<ffffffff8113a525>] ? pagevec_lookup_tag+0x25/0x40  [<ffffffff8112496b>] wait_on_page_writeback_range+0xfb/0x190  [<ffffffff81124b38>] filemap_write_and_wait_range+0x78/0x90  [<ffffffff811c084e>] vfs_fsync_range+0x7e/0x100  [<ffffffff811c093d>] vfs_fsync+0x1d/0x20  [<ffffffffa05e2930>] nfs_file_flush+0x70/0xa0 [nfs]  [<ffffffff8118ac5c>] filp_close+0x3c/0x90  [<ffffffff81077ddf>] put_files_struct+0x7f/0xf0  [<ffffffff81077ea3>] exit_files+0x53/0x70  [<ffffffff81079f1d>] do_exit+0x18d/0x870  [<ffffffff8107a658>] do_group_exit+0x58/0xd0  [<ffffffff8107a6e7>] sys_exit_group+0x17/0x20  [<ffffffff8100b072>] system_call_fastpath+0x16/0x1b INFO: task c++:1661 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. c++           D 0000000000000001     0  1661    577 0x00000080  ffff881f39ddbd20 0000000000000086 0000000000000000 ffff880000021f00  ffff8801aa4c8ae0 0000000000000000 00006aa06e9d01b7 ffffffff8117301c  ffff8801aa4c8ae0 0000000106fa0cda ffff8801aa4c9098 ffff881f39ddbfd8 Call Trace:  [<ffffffff8117301c>] ? transfer_objects+0x5c/0x80  [<ffffffff811b07e0>] ? mntput_no_expire+0x30/0x110  [<ffffffff8152a945>] schedule_timeout+0x215/0x2e0  [<ffffffff8128c199>] ? cpumask_next_and+0x29/0x50  [<ffffffff8106d175>] ? enqueue_entity+0x125/0x450  [<ffffffff8152a5c3>] wait_for_common+0x123/0x180  [<ffffffff81064b90>] ? default_wake_function+0x0/0x20  [<ffffffff8152a6dd>] wait_for_completion+0x1d/0x20  [<ffffffff81073f19>] do_fork+0x1d9/0x480  [<ffffffff81193d4b>] ? vfs_stat+0x1b/0x20  [<ffffffff81193d74>] ? sys_newstat+0x24/0x50  [<ffffffff81015e65>] sys_vfork+0x25/0x30  [<ffffffff8100b3d3>] stub_vfork+0x13/0x20  [<ffffffff8100b072>] ? system_call_fastpath+0x16/0x1b INFO: task c++:1663 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. c++           D 0000000000000000     0  1663    594 0x00000080  ffff88204e741d20 0000000000000082 0000000000000000 ffff880000021f00  ffff881f58483500 0000000000000000 00006aa11b2ee4c9 ffff880000032c80  ffff881f58483500 0000000106fa16d8 ffff881f58483ab8 ffff88204e741fd8 Call Trace:  [<ffffffff811b07e0>] ? mntput_no_expire+0x30/0x110  [<ffffffff8152a945>] schedule_timeout+0x215/0x2e0  [<ffffffff8128c199>] ? cpumask_next_and+0x29/0x50  [<ffffffff8106d175>] ? enqueue_entity+0x125/0x450  [<ffffffff8152a5c3>] wait_for_common+0x123/0x180  [<ffffffff81064b90>] ? default_wake_function+0x0/0x20  [<ffffffff8152a6dd>] wait_for_completion+0x1d/0x20  [<ffffffff81073f19>] do_fork+0x1d9/0x480  [<ffffffff81193d4b>] ? vfs_stat+0x1b/0x20  [<ffffffff81193d74>] ? sys_newstat+0x24/0x50  [<ffffffff81015e65>] sys_vfork+0x25/0x30  [<ffffffff8100b3d3>] stub_vfork+0x13/0x20  [<ffffffff8100b072>] ? system_call_fastpath+0x16/0x1b INFO: task c++:1666 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. c++           D 0000000000000008     0  1666    595 0x00000080  ffff88036bf8dd20 0000000000000082 0000000000000000 ffff882080010dc0  ffff88204e793540 0000000000000000 00006aa11a17d266 ffff882080021b40  ffff88204e793540 0000000106fa1514 ffff88204e793af8 ffff88036bf8dfd8 Call Trace:  [<ffffffff811b07e0>] ? mntput_no_expire+0x30/0x110  [<ffffffff8152a945>] schedule_timeout+0x215/0x2e0  [<ffffffff8128c199>] ? cpumask_next_and+0x29/0x50  [<ffffffff8106d175>] ? enqueue_entity+0x125/0x450  [<ffffffff8152a5c3>] wait_for_common+0x123/0x180  [<ffffffff81064b90>] ? default_wake_function+0x0/0x20  [<ffffffff8152a6dd>] wait_for_completion+0x1d/0x20  [<ffffffff81073f19>] do_fork+0x1d9/0x480  [<ffffffff81193d4b>] ? vfs_stat+0x1b/0x20  [<ffffffff81193d74>] ? sys_newstat+0x24/0x50  [<ffffffff81015e65>] sys_vfork+0x25/0x30  [<ffffffff8100b3d3>] stub_vfork+0x13/0x20  [<ffffffff8100b072>] ? system_call_fastpath+0x16/0x1b INFO: task c++:1669 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. c++           D 0000000000000008     0  1669    582 0x00000080  ffff880a3238bd20 0000000000000086 0000000000000000 ffff882080010dc0  ffff88067d696080 0000000000000000 00006aa0d2bcd607 ffff882080021b40  ffff88067d696080 0000000106fa136e ffff88067d696638 ffff880a3238bfd8 Call Trace:  [<ffffffff811b07e0>] ? mntput_no_expire+0x30/0x110  [<ffffffff8152a945>] schedule_timeout+0x215/0x2e0  [<ffffffff8128c199>] ? cpumask_next_and+0x29/0x50  [<ffffffff8106d175>] ? enqueue_entity+0x125/0x450  [<ffffffff8152a5c3>] wait_for_common+0x123/0x180  [<ffffffff81064b90>] ? default_wake_function+0x0/0x20  [<ffffffff8152a6dd>] wait_for_completion+0x1d/0x20  [<ffffffff81073f19>] do_fork+0x1d9/0x480  [<ffffffff81193d4b>] ? vfs_stat+0x1b/0x20  [<ffffffff81193d74>] ? sys_newstat+0x24/0x50  [<ffffffff81015e65>] sys_vfork+0x25/0x30  [<ffffffff8100b3d3>] stub_vfork+0x13/0x20  [<ffffffff8100b072>] ? system_call_fastpath+0x16/0x1b INFO: task c++:1675 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. c++           D 0000000000000001     0  1675    599 0x00000080  ffff88022d355d20 0000000000000082 0000000000000000 ffff880000021f00  ffff881f3cf4b500 0000000000000000 00006aa1057bbfc9 ffffffff8117301c  ffff881f3cf4b500 0000000106fa151f ffff881f3cf4bab8 ffff88022d355fd8 Call Trace:  [<ffffffff8117301c>] ? transfer_objects+0x5c/0x80  [<ffffffff811b07e0>] ? mntput_no_expire+0x30/0x110  [<ffffffff8152a945>] schedule_timeout+0x215/0x2e0  [<ffffffff8128c199>] ? cpumask_next_and+0x29/0x50  [<ffffffff8106d175>] ? enqueue_entity+0x125/0x450  [<ffffffff8152a5c3>] wait_for_common+0x123/0x180  [<ffffffff81064b90>] ? default_wake_function+0x0/0x20  [<ffffffff8152a6dd>] wait_for_completion+0x1d/0x20  [<ffffffff81073f19>] do_fork+0x1d9/0x480  [<ffffffff81193d4b>] ? vfs_stat+0x1b/0x20  [<ffffffff81193d74>] ? sys_newstat+0x24/0x50  [<ffffffff81015e65>] sys_vfork+0x25/0x30  [<ffffffff8100b3d3>] stub_vfork+0x13/0x20  [<ffffffff8100b072>] ? system_call_fastpath+0x16/0x1b INFO: task swig:1370 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. swig          D 0000000000000013     0  1370    592 0x00000080  ffff883fc83a9b68 0000000000000046 ffff883fc83a9ab8 ffffffffa054536e  ffff883fc83a9ae8 ffffffffa0545c10 ffff882051944840 ffff883fc83a9b18  ffff8820519448f0 ffff880d802032c8 ffff8840502d3058 ffff883fc83a9fd8 Call Trace:  [<ffffffffa054536e>] ? rpc_make_runnable+0x7e/0x80 [sunrpc]  [<ffffffffa0545c10>] ? rpc_execute+0x50/0xa0 [sunrpc]  [<ffffffff810aaa21>] ? ktime_get_ts+0xb1/0xf0  [<ffffffff811242d0>] ? sync_page+0x0/0x50  [<ffffffff8152a233>] io_schedule+0x73/0xc0  [<ffffffff8112430d>] sync_page+0x3d/0x50  [<ffffffff8152acff>] __wait_on_bit+0x5f/0x90  [<ffffffff81124543>] wait_on_page_bit+0x73/0x80  [<ffffffff8109eb80>] ? wake_bit_function+0x0/0x50  [<ffffffff8113a525>] ? pagevec_lookup_tag+0x25/0x40  [<ffffffff8112496b>] wait_on_page_writeback_range+0xfb/0x190  [<ffffffff81124b38>] filemap_write_and_wait_range+0x78/0x90  [<ffffffff811c084e>] vfs_fsync_range+0x7e/0x100  [<ffffffff811c093d>] vfs_fsync+0x1d/0x20  [<ffffffffa05e2930>] nfs_file_flush+0x70/0xa0 [nfs]  [<ffffffff8118ac5c>] filp_close+0x3c/0x90  [<ffffffff81077ddf>] put_files_struct+0x7f/0xf0  [<ffffffff81077ea3>] exit_files+0x53/0x70  [<ffffffff81079f1d>] do_exit+0x18d/0x870  [<ffffffff8107a658>] do_group_exit+0x58/0xd0  [<ffffffff8107a6e7>] sys_exit_group+0x17/0x20  [<ffffffff8100b072>] system_call_fastpath+0x16/0x1b INFO: task c++:1398 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. c++           D 0000000000000000     0  1398    601 0x00000080  ffff88036bf8bd20 0000000000000082 0000000000000000 ffff880000021f00  ffff88205024b540 0000000000000000 00006aaf749bb2d6 ffff880000032c80  ffff88205024b540 0000000106fb092b ffff88205024baf8 ffff88036bf8bfd8 Call Trace:  [<ffffffff811b07e0>] ? mntput_no_expire+0x30/0x110  [<ffffffff8152a945>] schedule_timeout+0x215/0x2e0  [<ffffffff8128c199>] ? cpumask_next_and+0x29/0x50  [<ffffffff8106d175>] ? enqueue_entity+0x125/0x450  [<ffffffff8152a5c3>] wait_for_common+0x123/0x180  [<ffffffff81064b90>] ? default_wake_function+0x0/0x20  [<ffffffff8152a6dd>] wait_for_completion+0x1d/0x20  [<ffffffff81073f19>] do_fork+0x1d9/0x480  [<ffffffff81193d4b>] ? vfs_stat+0x1b/0x20  [<ffffffff81193d74>] ? sys_newstat+0x24/0x50  [<ffffffff81015e65>] sys_vfork+0x25/0x30  [<ffffffff8100b3d3>] stub_vfork+0x13/0x20  [<ffffffff8100b072>] ? system_call_fastpath+0x16/0x1b INFO: task c++:1425 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. c++           D 0000000000000001     0  1425    576 0x00000080  ffff883f9c965d20 0000000000000082 0000000000000000 ffff880000021f00  ffff8840508b1500 ffffffff8117301c 00006aa3168c835d ffffffff8117301c  ffff8820534dc8c0 0000000106fa3976 ffff8840508b1ab8 ffff883f9c965fd8 Call Trace:  [<ffffffff8117301c>] ? transfer_objects+0x5c/0x80  [<ffffffff8117301c>] ? transfer_objects+0x5c/0x80  [<ffffffff8152a945>] schedule_timeout+0x215/0x2e0  [<ffffffff8128c199>] ? cpumask_next_and+0x29/0x50  [<ffffffff8106d175>] ? enqueue_entity+0x125/0x450  [<ffffffff8152a5c3>] wait_for_common+0x123/0x180  [<ffffffff81064b90>] ? default_wake_function+0x0/0x20  [<ffffffff8152a6dd>] wait_for_completion+0x1d/0x20  [<ffffffff81073f19>] do_fork+0x1d9/0x480  [<ffffffff81193d4b>] ? vfs_stat+0x1b/0x20  [<ffffffff81193d74>] ? sys_newstat+0x24/0x50  [<ffffffff81015e65>] sys_vfork+0x25/0x30  [<ffffffff8100b3d3>] stub_vfork+0x13/0x20  [<ffffffff8100b072>] ? system_call_fastpath+0x16/0x1b INFO: task c++:1661 blocked for more than 120 seconds.       Not tainted 2.6.32-504.12.2.el6.x86_64 #1 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message. c++           D 0000000000000001     0  1661    577 0x00000080  ffff881f39ddbd20 0000000000000086 0000000000000000 ffff880000021f00  ffff8801aa4c8ae0 0000000000000000 00006aa06e9d01b7 ffffffff8117301c  ffff8801aa4c8ae0 0000000106fa0cda ffff8801aa4c9098 ffff881f39ddbfd8 Call Trace:  [<ffffffff8117301c>] ? transfer_objects+0x5c/0x80  [<ffffffff811b07e0>] ? mntput_no_expire+0x30/0x110  [<ffffffff8152a945>] schedule_timeout+0x215/0x2e0  [<ffffffff8128c199>] ? cpumask_next_and+0x29/0x50  [<ffffffff8106d175>] ? enqueue_entity+0x125/0x450  [<ffffffff8152a5c3>] wait_for_common+0x123/0x180  [<ffffffff81064b90>] ? default_wake_function+0x0/0x20  [<ffffffff8152a6dd>] wait_for_completion+0x1d/0x20  [<ffffffff81073f19>] do_fork+0x1d9/0x480  [<ffffffff81193d4b>] ? vfs_stat+0x1b/0x20  [<ffffffff81193d74>] ? sys_newstat+0x24/0x50  [<ffffffff81015e65>] sys_vfork+0x25/0x30  [<ffffffff8100b3d3>] stub_vfork+0x13/0x20  [<ffffffff8100b072>] ? system_call_fastpath+0x16/0x1b  {code}",NULL
DM-2389,"SWIG 3 handles bools correctly, so a typemap in p_lsstSwig.i is no longer needed","utils p_lsstSwig.i contains a special typemap which forces python arguments to be truly bools (True or False) when calling C++ functions with bool arguments. That code is no longer required by SWIG 3.0, according to this release note:  {quote}  2014-03-06: wsfulton              [Python] Change in default behaviour wrapping C++ bool. Only a Python True or False              will now work for C++ bool parameters. This fixes overloading bool with other types.              Python 2.3 minimum is now required for wrapping bool.  ...  {quote}",NULL
DM-2393,"eups distrib install should not hold exclusive lock while compiling","When running eups distrib install it holds an ""exclusive lock"" on the EUPS dir.  Thus, trying to run 'setup' during this time results in the error message, e.g.,  setup: Unable to take shared lock on /Users/wmwv/stackdir: an exclusive lock is held by [user=wmwv, pid=53146]  I'm find with eups holding a lock when it's actually doing something to the EUPS directory, but it shouldn't hold a lock when it's compiling a particular package.  I note that despite the repeated statements that ""eups"" and ""eups distrib"" are entirely different things, they do share the same EUPS variables, and the same component in Jira.",NULL
DM-2394,"Fix queries contaning fields names prefixed by a table name/alias","Query:    {code:bash}  SELECT DISTINCT o1.objectId FROM Object o1 LIMIT 1  {code}    fails with the following error:  {code:bash}  130 qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-854)⟫ tail ~/qserv-run/git/var/log/qserv-czar.log   0325 10:30:04.014 [0x7f59b37fe700] INFO  root (build/rproc/InfileMerger.cc:307) - Merging w/CREATE TABLE qservResult.result_8184531634 SELECT DISTINCT o1.objectId FROM qservResult.result_8184531634_m LIMIT 1  0325 10:30:04.030 [0x7f59b37fe700] ERROR root (build/rproc/InfileMerger.cc:359) - InfileMerger sql error: Error applying sql. Error 1054: Unknown column 'o1.objectId' in 'field list' Unable to execute query: CREATE TABLE qservResult.result_8184531634 SELECT DISTINCT o1.objectId FROM qservResult.result_8184531634_m LIMIT 1    0325 10:30:04.030 [0x7f59b37fe700] INFO  root (build/rproc/InfileMerger.cc:313) - Cleaning up qservResult.result_8184531634_m  0325 10:30:04.031 [0x7f59b37fe700] INFO  root (build/rproc/InfileMerger.cc:325) - Merged qservResult.result_8184531634_m into qservResult.result_8184531634  0325 10:30:04.031 [0x7f59b37fe700] INFO  root (build/ccontrol/UserQuery.cc:202) - Joined everything (success)  0325 10:30:04.032 [0x7f59b37fe700] INFO  root (app.py:408) - Query exec (2) took 0.141981 seconds  0325 10:30:04.033 [0x7f59b37fe700] INFO  root (app.py:413) - Final state of all queries success  Runner running job  0325 10:30:04.039 [0x7f59b2ffd700] INFO  root (build/ccontrol/UserQuery.cc:235) - Discarded UserQuery(2)  {code}",NULL
DM-2395,"Change throughputs repo location in repos.yaml","The Simulation throughputs repository has been moved from gitolite to Github. The repos.yaml file needs to be update to reflect the change. ",NULL
DM-2405,"Measurement Framework Enhancements","Longer term, lower priority improvements to the measurement framework",NULL
DM-2407,"S15 Data Access and Database <--> Kevin","A place to capture changes related to Data Access and Database work that might require Kevin's attention and changes to our PMCS plan.",NULL
DM-2408,"Create new API to replace osslib in cmsd","Qserv uses the osslib API in the cmsd (of XRootD) to report db/chunk availability. With the new XrdSsi async interface, there was an attempt to unify this API with the RPC API, but it does not seem to make sense to unify two separate APIs in a single place. ",NULL
DM-2409,"Fix typo ""dimesiosn"" in SingleGaussianPsf schema","There's a typo in the schema for SingleGaussianPsf with ""dimensiosn"" instead of ""dimensions"" in the below.  Fix is trivial but needs to be tested to make sure it doesn't break existing persisted objects or tests.  private:     SingleGaussianPsfPersistenceHelper() :         schema(),         dimensions(             schema.addField< afw::table::Point<int> >(                 ""dimensiosn"", ""width/height of realization of Psf"", ""pixels""             )         ),         sigma(schema.addField<double>(""sigma"", ""radius of Gaussian"", ""pixels""))     {         schema.getCitizen().markPersistent();     } }; ",NULL
DM-2410,"Implement DOT-based graph vizualization for query IR","Developers have noted that it would be a help to have a visual representation of the query intermediate format. This ticket covers implementing an ability to output a graph (in DOT graphviz format) of a SelectStmt.",NULL
DM-2413,"buildbot@lsst-buildx:master is out of sync with lsst-sqre/buildbot-master","{code} [buildbot@lsst-buildx master]$ git remote -v origin	https://frossie@github.com/frossie/buildmaster.git (fetch) origin	https://frossie@github.com/frossie/buildmaster.git (push) {code}",NULL
DM-2416,"Study tracks to improve management of Qserv errors","Qserv contains lots of classes related to Error management. Would it be possible to unify these ones. For example having unique error code and error messages source files would be convenient for support and maintenance, (and in the possible long-term, internationalization).",NULL
DM-2418,"afw tests reuse the same filenames","footprint1.py, image.py, mask.py, and maskedImageIO.py all seem at first glance to use ""foo.fits"".  This causes problems when tests are run in parallel.  [~rowen], I believe you've cleaned up stuff like this in the recent past; could you take a quick look?",NULL
DM-2422,"instantiate uint32 bit Images and MaskedImages for LSST","The LSST camera will apparently write 18 bits of data per pixel, so we may end up storing images as uint32. We might as well be prepared by instantiating them.  This affects afw::math::Statistics and our SWIG interfaces as well. Note that we presently use these one-character suffixes in our SWIG wrappers: U=uint16, L=uint64. I'm not sure what we'd use for uint32, but if you want it to be obvious to the users I strongly suggest changing to U16, U32 and U64 for those cases where there is more than one option (hence not masks, which should stay U as long as we only have one mask pixel type).",NULL
DM-2428,"The sqlite package is a not being used and should not be built","The sqlite package is a holder from when we built our own python and/or our own pysqlite package (or whatever it was called). A python sqlite module is now built into python and we have no code that uses the sqlite package. Worse, on MacOS, building sqlite can cause problems such as code using the wrong sqlite library.",NULL
DM-2438,"Decide on owner/location of pixel-copy code for image stitching","Image stitching is currently using coadd_utils.Coadd.addExposure (which in turn calls lsst::coadd::utils::addToCoadd) to copy pixels from source images to a destination image. This is not the intended purpose of Coadd and there are significant short comings, notably pixels marked as edge are not copied, and pixels from the source image are added to existing pixels in the destination image.  New code needs to be written to copy valid pixels from a source image to the destination image only if the destination pixels is blank. This should probably not be part of the Coadd module as addToCoadd needs to be very efficient and this is something different than coadding. There is some question of whether this should go in imgserv or afw.  ",NULL
DM-244,"add aliases, FunctorKeys, and better slots to afw::table","Add FunctorKeys and aliases to afw::table::Schema objects, for more extensible slots and more.  See the discussion [here|https://dev.lsstcorp.org/trac/wiki/Winter2014/Design/AfwTable] and a (problematic) initial implementation from [#2351|https://dev.lsstcorp.org/trac/ticket/2351].",NULL
DM-2446,"allow tmv to build in parallel","The eupspkg.cfg.sh for the tmv package forces the build to only use one process.  Relax that condition to speed up the build.",NULL
DM-2453,"Fine-tune data access interfaces v2","Submitting async queries through POST- need to be able to specify location of results, type of results",NULL
DM-2464,"Several minor slips in Wcs documentation","The Doxygen documentation for {{lsst::afw::image::wcs}} contains a number of minor mistakes and typos. Perhaps most egregiously, it [repeatedly|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_01_07.01.48/classlsst_1_1afw_1_1image_1_1_wcs.html#a0af8218e58db172c39f8e7a6004c81a0] [claims|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_01_07.01.48/classlsst_1_1afw_1_1image_1_1_wcs.html#a58c33b803b01a091621c59a3d46166f8] that functions called {{pixelToSky}} ""convert from celestial coordinates to pixel coordinates"". Further, a lot of documentation is duplicated, because it's included in both the header and the implementation (often in somewhat contradictory ways).  ""Minor"" doc fixes don't normally need tickets or review; since I'm changing quite a few lines at once here, I think it's worth filing and getting a second pair of eyes to check the changes.",NULL
DM-2465,"Inequality semantics of Coords are still confusing","Despite the discussion & supposed fix at DM-2347, the same confusion still occurs. Although we agreed to {{%useValueEquality}} for {{Coord}}, that code was never committed.",NULL
DM-2471,"astrometry has a bug in retrieving TAN_PIXELS and it exposes the need for a better API","meas_astrom astrometry.py has two errors in how it retrieves the TAN_PIXELS coordinate system: {code} pixelsToTanPixels = exposureInfo.getDetector().getTransformMap().get(TAN_PIXELS) if pixelsToTanPixels:     ... {code} The first error is using a CameraSysPrefix instead of a CameraSys to specify the transform; the second error is in assuming get will return None if the system is not present. Unfortunately the correct code is ugly and suggests the need for a better API: {code} detector = exposureInfo.getDetector() tanPixCamSys = detector.getCameraSys(TAN_PIXELS) if tanPixCamSys in detector.getTransformMap():     pixelsToTanPixels = detector.getTransformMap().get(tanPixCamSys)     ... {code}  TransformMap can only handle CameraSys and fixing that would be hard, so I propose instead to add two overloaded methods to Detector, both of which can accept a CameraSys or a CameraSysPrefix: - getTransform(sys) - hasTransform(sys)  The resulting code will be: {code} detector = exposureInfo.getDetector() if detector.hasTransform(TAN_PIXELS):     pixelsToTanPixels = detector.getTransform(TAN_PIXELS)     ... {code} ",NULL
DM-2472,"Winter2015 release won't build on latest Apple machines","I'm having issues building the stack on the latest Apple machine (Apple S1 CPU, 8GB of storage). The script install won't download, and it's not even clear how to start the download process. I'm confused. Please advise on how to proceed.  PS: Having a few instances of this hardware to run CI on would be helpful and avoid such problems for the end-users. The higher-end variants would probably be suited for the purpose. I propose acquiring a few once they're generally available.",NULL
DM-2473,"Tests in meas_astrom trigger race conditions in EUPS","I tried to build the stack from scratch, and got a failure in one of the meas_astrom tests (attached). It's similar to the DM-2303 failures, where Eups aborts because the cache pickle file is invalid.  This happens because: a) tests in meas_astrom are run concurrently (good) b) at least 11 of them instantiate EUPS (ok) c) on every creation of an instance of Eups class, it checks whether it's Products cache is valid; if it isn't it rebuilds it and writes it out to the disk for later reading. Because the write is non-atomic, this creates a race condition where if another instance of Eups tries to read the cache file before the writing has finished, it will see it as corrupted. And that is what happens here.  While there are known issues w. EUPS locking, I think this kind of race should not exist even in the absence of locking (i.e., readers should not have to lock). The fix is to make the write atomic, using the usual write-to-temporary-and-then-move pattern. In pseudocode: {code} fp = NamedTemporaryFile(delete=False, dir=cachedir) pickle.dump(fp, cache) fp.close() os.rename(fp.name, cacneFn) {code}  Also, the caching code should not abort on invalid cache, but fall back to rebuilding the cache instead (and maybe issue a warning).  Once this implemented I think it may be possible to remove the workaround developed for DM-2303.",NULL
DM-2476,"add a unit test for clipImage","The function lsst.afw.image.utils.clipImage was broken and is not used anywhere. I fixed it just enough to make the pyflakes linter happy (work on tickets/DM-2471 that I expect to have merged in a day or two), but it still needs a unit test.",NULL
DM-2481,"Fix intermittent race condition in worker","An intermittent crash noticed by [~jgates] and [~fritzm] seems to be a result of a race condition. {code} The integration tests hung on the first attempt at 2015-04-02 11:39:03,271 - lsst.qserv.tests.benchmark - INFO - Launch 1051_nn.sql against qserv 2015-04-02 11:39:03,564 - lsst.qserv.tests.benchmark - INFO - Launch 3006_selectAs.sql against qserv I did a ^c, restarted qserv, and the test have run without issue a couple of times. {code}  and  {code} 0402 12:09:13.840 [0x7f8fb31fa700] WARN  Foreman (build/wdb/QueryAction.cc:97) - QueryAction overriding dbName with qservTest_case05_qserv terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >'   what():  boost: mutex lock failed in pthread_mutex_lock: Invalid argument {code} ",NULL
DM-2482,"configuration of log object can throw exception which needs to be catchable","During the configuration of properties, log can instantiate appender classes which can throw exceptions.   In order to propagate this up to Python the following code needs to be added to the logLib.i Swig file:  {quote} %lsst_exceptions() %include ""lsst/pex/exceptions/exceptionsLib.i"" {quote}",NULL
DM-2483,"funpack and fpack are not build in cfitsio builds","The utilities funpack and fpack are included with cfitsio, but are not built in the stack version of cfitsio.  This is important because DECam data from the NOAO Science Archive are compressed with fpack.  The cfitsio library can read fz files, but other utilities can't.  I suggest we just build them since we have the ability to do so.",NULL
DM-2488,"In statistics, IsFinite should check for finite (!(NaN || Inf)) rather than just !NaN","In math/Statistics.cc, the test for Bad Numbers is: {code}     /**      * @brief A boolean functor to check for NaN (for templated conditionals)      */         class CheckFinite {     public:         template<typename T>         bool operator()(T val) const {             return !lsst::utils::isnan(static_cast<float>(val));         }     }; {code}  The comment should be changed to cover Inf as well as NaN, and the test changed to lsst::utils::isfinite",NULL
DM-249,"Relink the astrometry_net_data datasets into the new 8.0.0.0 stack","THe new 8.0.0.0 Winter2014 (pre) Release stack does not have the astrometry_net_data datasets connected to the versions as was done in the old stack.  If this is not the correct model going forward, then this tasks becomes a redesign task.",NULL
DM-2490,"please make eups deletable","lsstsw installs eups in a way that makes it hard to delete. This is annoying, especially on shared systems where users don't have sudo. If there is some way to change this without breaking lsstsw it would be much appreciated. ",NULL
DM-2494,"more shapelet unsanctioned access to display","In addition to DM-2492, we've found another unguarded import of matplotlib.pyplot, which can apparently cause problems on some (but not all) display-less systems.",NULL
DM-2495,"S15 Science User Interface  <--> Kevin","To communicate to Kevin for anything needs his attention.    5/26/2015 Moved epic DM-1156 and DM-1160  to Winter 16  5/31/2015 DM-2847 is created for Sumer 15    8/15/2015, moved DM-1874 to W16  8/15/2015, moved  DM-1160  back to S15 since we need to exercise the dax APIs    8/15/2015  added a new epic. DM-3469	Prepare for S15 end-to-end system and Firefly release",NULL
DM-2496,"AstrometryTask does not update the source catalog","The AstrometryTask should update the coords in the source catalog with the fit WCS. In fact that should be done in FitTanSipWcsTask.",NULL
DM-2499,"Jira integration with lsst-sqre Github Organization","Jira should pickup branches and other commands from PRs & commit messages in any of the lsst-sqre repos.",NULL
DM-251,"Integration of Dcr with Wcs","Subclassing of Wcs to include the effects of Dcr in the astrometric representation.",NULL
DM-2516,"Apparent race condition in daf_persistence unit tests","Buildbot run 5307 failed in a way that is very suspicious: two daf_persistence unit tests that both write to table DbStorage_Test_1 failed (there are 4 such tests, two C++ tests that failed and two .py tests that passed).  Should those tests be changed so each test class uses its  own table?  The code I changed should have no affect on daf_persistence and redoing the buildbot run passed (though admittedly with very minor changes to other packages).",NULL
DM-2517,"Implement RFC-31 constants package","Implement a constants package as described in the consensus for RFC-31.",NULL
DM-252,"Integration of Dcr with Psf","Subclassing of Psf to create a Dcr-aware version of Psf modeling.  There will likely be testing that extends beyond the scope of this task.",NULL
DM-2525,"Please stop auto-subscribing users to new repos","Please stop auto-subscribing users to new repos added to github. I have no need to watch any of them and the constant parade of emails is really annoying. Every user who agrees has to manually unsubscribe from each new repo. It gets old fast.",NULL
DM-2527,"CameraMapper.getEupsProductName should be pure virtual","CameraMapper.getEupsProductName is complicated and can run quite slowly (e.g. on lsst-dev). It makes more sense to just have subclasses override the method to return the appropriate string and not offer a usable default implementation (e.g. by making the default implementation raise NotImplementedError or by making CameraMapper an abstract base class (which is a good idea, in any case, but more work).  Fixing this is trivial, but requires updating all obs_ packages. A less disruptive alternative is to implement this change in two steps: 1) Deprecate the method by documenting it should be overridden and by having the default implementation print a deprecation warning. Also fix those obs_ packages we maintain. 2) In a later release make the default implementation unusable.",NULL
DM-2529,"Add HCS's pure TAN WCS fitter to the astrometry task","HSC's hscAstrom astrometry fitter runs a pure TAN fitter after matching sources to reference objects, then passes the resulting WCS to the TAN-SIP fitter. meas_astrom's astrometry task is very similar to HSC's fitter, but works poorly. One notable difference is that meas_astrom's astrometry task does not run a pure TAN fitter, so we will try adding that.  I'm not sure this will help (since our initial guess for a WCS is already pretty good, and our TAN-SIP fitter already can handle TAN terms), but it's an easy thing to try.  I plan to make it part of the FitTanWcsTask, so all WCS fitting is done at one time in one place (but by calling two bits of C++ code).",NULL
DM-253,"Integration of Dcr with Coadd","Subclassing of coaddition that use Dcr during the coadd process.  Likely will involve edits to the warping process more than to the coadd process.",NULL
DM-2531,"fix bare pointers in objects","There are a number of places where third party library calls are made to objects that should be referenced as shared_ptr or unique_ptr.  Clean all of those up, and any other bare pointer references.",NULL
DM-2539,"Finish up ZFS tuning and implementation","details tbd",NULL
DM-254,"Integration of Dcr with Image Differencing","Creation of a Dcr-aware ImageDifferencingTask.  This may include more edits to the image warping than to the image differencing itself, depending on the implementation of Dcr compensation.   This may also end up forking off into other storylines if we decide we want to test multiple approaches to the problem.",NULL
DM-2540,"Research NFS Optimization","Research if we can/should optimize NFS for LSST usage. Fairly loose ended... can we increase performance (syncs, cache, etc), security, robustness, etc? Estimate spending 10-20 hours of work with result being a wiki page of suggestions, limitations, etc.",NULL
DM-2542,"Remove obsolete log dependency","Since we decided to rely on standard python logging in python-only code instead of our own logging, the imgserv and webserv don't need to depend on log. Fix the .table files.",NULL
DM-2548,"logging documentation is confusing, due to having two packages","The logging documentation for the LSST stack is confusing. There are two packages, listed very far apart from each other, with no indication of why there are two packages nor which one should be used for our code.  I strongly suggest that each package's main.dox file be updated to address this issue. It should clearly mention the existence of other package and give a rough idea of which one is to be used.  Here is the original description of this ticket, which may clarify the initial comments:  The pex_logging main page, which has a useful overview and tutorial, seems to not match the actual implementation anymore, at least for the python interface. For instance main.dox http://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/log.html says:  In Python, the following logging functions are available in the lsst.log module. These functions take a variable number of arguments following a format string in the style of printf(). The use of *args is recommended over the use of the % operator so that formatting of log messages that do not meet the level threshold can be avoided.  I didn't actually expect pex_logging to contain such a free function (though that's what the documentation says) but I did expect the Log function to contain that method. It doesn't. Instead it has a logdebug method (different name) that does not seem to accept the arguments described above (though naturally it does accept a single string).  Unless I'm missing something the overview needs some significant work.",NULL
DM-255,"afw.geom.Point can't be built with numpy types","Building against the anaconda on lsst-dev causes construction of Point2I (and other point types) with numpy dtypes to fail with the standard exception: {code} NotImplementedError: Wrong number or type of arguments for overloaded function {code} I did not know that was not allowed.  If I am doing it others are as well.  I think this should be addressed in some way before W14 release.  How to repeat on lsst-dev: {code:sh} setenv EUPS_DIR ~lsstsw/stack/ source ~lsstsw/eups/bin/setups.csh setup afw setup anaconda echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}  Using the old 7_3 stack you can do a similar test: {code:sh} source /lsst/DC3/stacks/default/loadLSST.csh setup -t v7_3 afw echo ""import lsst.afw.geom as ag\nimport numpy\nprint ag.Point2I(5,5)\nval = numpy.int64(5)\nprint ag.Point2I(val,val)"" > test.py python test.py {code}",NULL
DM-2550,"Task.display mis-handles non-dict values of display","Task.display raises an exception if display is not a dict, due to a simple mistake in a test.  Yes the method is deprecated, but it is still being used and the fix is trivial.  Note that the test in question has another minor issue, but one that should be left alone: the variable ""display"" can mean too many different things; in particular it can be a boolean or a frame number (or a dict whose key is the name and whose value is a boolean or a frame number. The problem is with ""boolean or fame number"": there is no simple, robust way to distinguish between a boolean that might be 0 or False and a valid frame number 0. The current code treats 0 as False, and I like that: it's simple robust and it will not unexpectedly raise an exception.",NULL
DM-257,"Ask to change assignee on issue drag-and-drop on the board","On 3/17/14, 10:07 , Robert Lupton the Good wrote:> I don't seem to be able to change a status (e.g. to Ready to Merge) and change the assignee simultaneously in JA plan (i.e. by dragging).  The Atlassian docs imply that this is possible;  is this something about our workflow? >   A ""Screen"" needs to be defined, that would pop up whenever the issue is moved to a column. I'll open an issue and we'll get to it. ",NULL
DM-2575,"anaconda install fails on MacOS due to trying to upgrade ""system""","The installer for anaconda attempts to upgrade the ""system"" package to work around a known bug, but unfortunately that package doesn't exist on MacOS.",NULL
DM-258,"Enable JIRA e-mail interface","It should be possible to reply to JIRA issue e-mail messages, and have those replies added as comments to the relevant issue.",NULL
DM-2584,"LOE work done to accomplish the networks from Chile to NCSA","jkantor, rlambert",NULL
DM-2588,"Catch RuntimeError on matplotlib.pyplot import and remove more unprotected/unnecesary pyplot imports","Catch RuntimeError in addition to ImportError when testing for matplotlib with things like:  {code:python}  from matplotlib import pyplot  import matplotlib.pyplot as plt  {code}    I have some weird issue on my laptop where     {code:python}  from matplotlib import pyplot  {code}  throws a RuntimeError when DISPLAY is unset.  scons unsets DISPLAY when compiling.    This bug has allowed me to notice a few unncessary imports of pyplot, particular in test cases.",NULL
DM-2596,"Add missing versions to DM-Long-range Planning Project in JIRA  ","We currently only have W15, S15, W17. Please add W16, S16, S17, W18... and so on until end of construction.",NULL
DM-2604,"Update qserv.table with flask dependency","Recently added code in wmgr module uses flask, but qserv.table misses flask as a dependency, need to add it.",NULL
DM-261,"Anaconda Python built with incorrect gcc at NCSA","At NCSA, the recent Anaconda Python claims to have been built against gcc 4.1, not the gcc 4.4 we're using for the rest of the stack:  {noformat} [jbosch@lsst-dev ~]$ setup anaconda -v Unable to lock /lsst/home/lsstsw/stack; proceeding with trepidation Setting up: anaconda                        Flavor: Linux64    Version: 1.8.0 [jbosch@lsst-dev ~]$ python Python 2.7.5 |Anaconda 1.8.0 (64-bit)| (default, Nov  4 2013, 15:30:26)  [GCC 4.1.2 20080704 (Red Hat 4.1.2-54)] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. {noformat}  I'm guessing this is because we're using a binary Anaconda install, but I'm still a little worried this will lead to trouble downstream as we're building our packages with a different compiler than was used to build the Python we're building against.",NULL
DM-2616,"log4cxx hardcodes "".so"" as shared library extension","log4cxx's [{{ups/log4cxx}}|https://github.com/lsst/log4cxx/blob/master/ups/log4cxx.cfg] contains the line {{libs=\[""liblog4cxx.so""\]}}. On OS X, the library is called {{liblog4cxx.dylib}}. Trying to build anything based on logc4xx therefore fails:  {code} c++ -o lib/liblog.dylib -dynamiclib -Wl,-install_name -Wl,liblog.dylib src/logInterface.os src/Log.os -Llib -L/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/utils/10.0+1/lib -L/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/pex_exceptions/10.0+1/lib -L/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/base/10.0+2/lib -L/Users/jds/Projects/Astronomy/LSST/s tack/DarwinX86/boost/1.55.0.1.lsst2/lib -L/Users/jds/Projects/Astronomy/LSST/src/log4cxx/0.10.0.lsst2-2-gaae52b5/lib -L/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/config -lutils -lpex_exceptions -lbase -lboost_regex -lpthread -llog4cxx.so ld: library not found for -llog4cxx.so clang: error: linker command failed with exit code 1 (use -v to see invocation) {code}  Simply removing the {{.so}} should fix this.",NULL
DM-2620,"case05 in testdata is missing empty chunks information.","It looks like case05 is missing emptychunks info. I guess it's missing the generation step that the other testcases already have.  Ideally, the czar would create the empty chunks info, but that is a much larger project.  <code> 0424 17:54:27.238 [0x7f6cce458700] INFO  root (app.py:600) - Looking for indexh ints in  [('sIndex', ['qservTest_case05_qserv', 'Object', 'objectId', '43332784 0428032'])] 0424 17:54:27.241 [0x7f6cce458700] INFO  root (metadata.py:43) - Using /usr/loc al/home/danielw/t1/run/etc/emptyChunks.txt as default empty chunks file. 0424 17:54:27.244 [0x7f6cce458700] INFO  root (metadata.py:90) - /usr/local/hom e/danielw/t1/run/etc/emptyChunks.txt not found while loading empty chunks file. 0424 17:54:27.247 [0x7f6cce458700] INFO  root (metadata.py:46) - Using /usr/loc al/home/danielw/t1/run/etc as empty chunks path 0424 17:54:27.251 [0x7f6cce458700] INFO  root (app.py:284) - cids are  () 0424 17:54:27.254 [0x7f6cce458700] INFO  root (app.py:610) - lookup got chunks:  [] 0424 17:54:27.273 [0x7f6cce458700] INFO  root (metadata.py:90) - /usr/local/hom e/danielw/t1/run/etc/empty_qservTest_case05_qserv.txt not found while loading e mpty chunks file. 0424 17:54:27.276 [0x7f6cce458700] ERROR root (metadata.py:68) - Couldn't find  /usr/local/home/danielw/t1/run/etc/empty_qservTest_case05_qserv.txt, using /usr /local/home/danielw/t1/run/etc/emptyChunks.txt. 0424 17:54:27.279 [0x7f6cce458700] ERROR root (app.py:377) - Unexpected error: (<class 'lsst.qserv.czar.app.DataError'>, DataError(), <traceback object at 0x7: <code>",NULL
DM-2626,"qserv-configure produces no output","Fabrice, something happened to qserv-configure.py in the recent commits, I'm trying to run it from master branch with -a option and it produces no output except one question, it's not clear if it's doing anything or not: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ qserv-configure.py -a  WARNING : Do you want to erase all configuration data in /usr/local/home/salnikov/qserv-run/2015_04 ? [y/n] y [salnikov@lsst-dbdev4 dm-2617]$  {noformat}  If I delete existing run directory manually then ip runs and shows some info: {noformat} [salnikov@lsst-dbdev4 dm-2617]$ rm -fr /usr/local/home/salnikov/qserv-run/2015_04 [salnikov@lsst-dbdev4 dm-2617]$ qserv-configure.py -a INFO:root:Qserv configuration tool ======================================================================= INFO:root:Creating meta-configuration file: /usr/local/home/salnikov/qserv-run/2015_04/qserv-meta.conf INFO:root:Reading meta-configuration file /usr/local/home/salnikov/qserv-run/2015_04/qserv-meta.conf INFO:root:Defining main directory structure INFO:root:Qserv directory structure creation succeeded INFO:root:Qserv symlinks creation for externalized directories succeeded INFO:root:Creating configuration files in /usr/local/home/salnikov/qserv-run/2015_04/etc and scripts in /usr/local/home/salnikov/qserv-run/2015_04/tmp INFO:root:Creating configuration from templates INFO:root:Running configuration scripts INFO:lsst.qserv.admin.commons:Run shell command: /usr/local/home/salnikov/qserv-run/2015_04/tmp/configure/mysql.sh ...................... {noformat} ",NULL
DM-2637,"Coord should be abstract","afw.coord.Coord appears to be meant as an abstract base class, but it can be instantiated. When a Coord is instantiated it's not clear what the various coordinate conversion methods should do. I think it would be best to make sure that Coord could not be instantiated. ",NULL
DM-2639,"Standardize primary method names, run/runDataRef, across PipeTasks","DM-1299 changed the API of {{IsrTask}} so that {{run}} takes exposures and {{runDataRef}} takes a DataRef.  This was implemented per the interest in RFC-26 to make {{run}} the method of reuse across our tasks, which seemed reasonable. The problem here is that {{CmdLineTask.parseAndRun}} calls ""run,""  and we cannot trivially run {{IsrTask}} as a {{CmdLineTask}}.  If we want {{run}} to be the method of reuse and {{runDataRef}} to be the {{CmdLineTask}}: * {{parseAndRun}} should be updated to call {{runDataRef}} * Remaining {{run}} methods that take {{dataRefs}} should be renamed to {{runDataRef}} in all PipeTasks. ",NULL
DM-2640,"Update AssembleCcdTask to handle missing amps","Think about what {{AssembleCcdTask}} should do when passed incomplete AmpInfo tables. Options are: * Raise an exception * Fill gaps with NaNs (which subsequently get interpolated over by IsrTask)  Considerations: * Amps will fail   ",NULL
DM-2641,"Remove utils::eups::productDir","As noted in RFC-47, the utils package includes a function utils::eups::productDir. This is only used by a few tests and examples in afw. We don't seem to have a strong need to support this for C++ code and we would prefer that it be python-only.  I am somewhat nervous about this change, since I find it very useful to have unit tests and even examples run without arguments. The afw tests can probably be fixed by moving some tests to python and deleting redundant tests. If not, then I am not happy with this change.  I think we'll just have to live with C++ examples requiring command-line arguments. However, I suggest checking whether the examples are still needed, and whether they could be rewritten in Python.",NULL
DM-2644,"Add side-door to xrdssi so xrootd can export one path as a std xrootd","On an xrootd server that speaks xrdssi, we would like to designate a path to be handled in the standard xrootd way. This allows xrdssi servers to publish files in the xrootd manner, thus providing a simple transport mechanism for workers to pull raw data from each other.",NULL
DM-2645,"qserv-configure.py should allow a user to have multiple run directories.","It is surprising that qserv-configure.py is even aware of other run directories, but when trying to create another run directory completely independent of an existing one, it fails: {code}  Do you want to update symbolic link /usr/local/home/danielw/t1/run/etc/my-client.cnf to /usr/local/home/danielw/t1/run2/etc/my-client.cnf? [y/n] n CRITICAL:root:Symbolic link to client configuration unmodified. Exiting. {code}   ",NULL
DM-265,"Acquire D2/D3 CFHTLS dataset covering the Stripe82 area","DM wants to upgrade the LSST stack to work with the CFHTLS dataset (again). In particular, the  CFHTLS data which overlaps SDSS Stripe 82.    This task will acquire the  area of D2 CFHTLS which overlays Stripe82 and install it in /nfs/lsst/ImageData/CFHTLS-DEEP",NULL
DM-2650,"A unit test assumes etc is unwritable","pex_policy tests/testBadPAFWrite.cc has a test that assumes /etc is unwritable and fails if a write to it succeeds. We know of at least one incident where this condition was not met.  I think it is worth fixing this so our stack can more reliably be installed on other systems. However, I don't think it is worth spending very much time on fixing the test because pex_policy is deprecated. So I propose to simply delete this one test, rather than attempt to rewrite it in a more robust fashion.  In other words, this is important merely because it may prevent installation of the software stack.",NULL
DM-2651,"Handle queries similar to ""select 1""","queries like ""select 1"", or ""select 1+1"" work in plain mysql, but fail in qserv. Qserv will currently fails with  {code} ERROR 4110 (Proxy) at line 1: Qserv error: 'ParseException:Parse error(ANTLR):unexpected token: ;:' {code}",NULL
DM-266,"Version numbers of external packages are the same as in old releases","E.g., cfitsio 3310 build with eupspkg should really be 3310+1, as they're a different package with potentially different build process/flags.",NULL
DM-267,"Install Syntax Highlighter Plugin","We desperately need Python code highlighting.  ""diff"" highlighting would also be useful.  https://marketplace.atlassian.com/plugins/jira.plugin.syntaxhighlighter.macro.syntaxplugin",NULL
DM-2670,"Assess our success at measuring Constant Shear","This is an extremely plastic issue, which I put in to remind us that the ultimate goal for S15 is to figure out how well we are doing at shear measurement.  And how much time it will take to make that assessment will depend on iterative changes in the measurement algorithm, how quickly we can converge on the correct parameterization of the algorithm, and of course, on how long the framework we set up to run these tests takes to do its assessment.",NULL
DM-2677,"Think about using MariaDB pluggable authentication","In MariaDB the authentication of users is delegated to plugins. That means that instead of managing user accounts in mysql-centric way, we could e.g. integrate it with LDAP. This should help build a better single-sign-on system. We should explore it, and things whether we want to go in that direction.  Related reading: [MariaDB pluggable authentication|https://mariadb.com/kb/en/mariadb/development-pluggable-authentication/]",NULL
DM-268,"Create a safe archive for the astrometry_net_data and other test datasets","Eups package astrometry_net_data co-opted the individual versions  to hold different unique datasets (i.e. not just different versions of the same dataset).  Currently the different datasets are not backed-up nor are they installed in git repositories so the loss of a dataset  thru misadventure or other failure requires the recreation of the dataset from whence it came.  This Issue should deal with setting up a failsafe archival mechanism.  Edit (by mjuric): As K-T points out in the first comment, this is true of all other test data sets. This issue is intended to cover the needed work for all of them.",NULL
DM-2685,"Understand Bulk Loading and Replica Mgmt in other systems","It'd be worth taking a look how other systems (e.g. SciDB, maybe Cassandra) deal with bulk data loading and replica management.",NULL
DM-2686,"Query parser/analysis/rewriting","Suggested by Daniel (this should probably be more than one ticket): * Streamline query intermediate format (incl. Serge's suggestions) * Revisit QueryContext (split into QueryContext[incoming usercontext, read-only: username, db, auth\] and QueryClipboard[derived namespacing, info passed between plugins) * Think about nested namespace handling for subqueries * Revisit analysis framework in light of current functionality and future optimization opportunity (if chunk totally contained in user area spec, eliminate area spec from chunkquery for that chunk). * Is there a better way to materialize an analyzed query into many chunk queries?",NULL
DM-269,"fix ifs in qserv eups scripts","Hi Fabrice,  I started playing with qserv eups-based install process and learning  eups along the way. I just went through the first step by checking out u/fjammes/eupspkg branch and running install.sh script. I decided  to look around for what eups does and how it arranges things. Looking  at table files in instalation area I observed one interesting thing  that I don't quite understand. Basically some table files have 'if'  blocks that do not make much sense to me and some have unbalanced braces. Here an example of latter:  === stack/ups_db/Linux64/qserv/6.0.0rc1/ups/mysqlpython.table === {code} if (type == exact) {	    setupRequired(python          -j system)    setupRequired(mysql           -j 5.1.65) } else {    setupRequired(python system)    setupRequired(mysql 5.1.65) } }                   # <----- extra brace???  envPrepend(PYTHONPATH, ${PRODUCT_DIR}/lib/python) {code} =================================================================  and here is an example of strange ifs:  === stack/ups_db/Linux64/qserv/6.0.0rc1/ups/luaexpat.table === {code} # luasocket build file relies strongly # on lua 5.1 if (type == exact) { if (type != exact) {    setupRequired(lua -j 5.1.4)    setupRequired(expat -j 2.0.1) } } else {    if (type == exact) {       setupRequired(lua             -j 5.1.4)       setupRequired(expat           -j 2.0.1)    } else {       setupRequired(lua 5.1.4)       setupRequired(expat 2.0.1)    } } {code} ==============================================================  I tried to read about this ""type == exact"" ifs in  https://dev.lsstcorp.org/trac/wiki/EupsTips and it looks to me that  table files that come from http://datasky.in2p3.fr/qserv/distserver/tables/ are not supposed to have those ifs, they are added when table files are installed. Maybe eups gets confused by the existing ifs in table files when it does installation?  Cheers, Andy",NULL
DM-2697,"Research Deep Engine","This is a new MySQL storage engine that apparently uses adaptive algorithms and can scale to tables with very high row counts; it may be an interesting choice for the object ID -> partition mapping table.",NULL
DM-27,"Announce Winter 2014 release","Announce Winter 2014 release to lsst-dm-stack-users mailing list.",NULL
DM-270,"test new issue","I have a bug to report!  Iain  Iain Goodenow LSST Project Office (LSSTPO) IT Systems Administrator igoodenow@lsst.org 520.318.8385 www.lsst.org  ",NULL
DM-2700,"AFW tests complain about a FutureWarning","{{testUtils.py}} in AFW triggers a number of warnings when using the {{!=}} operator for a numpy array: {code} warper.py:/nfs/home/lsstsw/build/afw/python/lsst/afw/image/testUtils.py:96: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future. {code} The fix is to use {{is not None}} instead.",NULL
DM-2701,"daf_persistence unit test fails if you have a db_auth file with inadequate credentials","If users have an .lsst/db_auth.py file but don't have log-in credentials for host/port: lsst10.ncsa.uiuc.edu:3306, the daf_persistence unit tests fail.  If users do not have any .lsst/db_auth.py file, the offending unit tests are skipped (which is why this problem only just came up, when the UW sims teams all added db_auth.py files to their computers).",NULL
DM-2702,"The new astrometry task gives poor results on CFHT data","The new lsst.meas.astrom.AstrometryTask gives markedly poorer results than the older astrometry_net-based ANetAstrometryTask on CFHT data, as per Dominique Boutigny's demo on lsst-dev /lsst8/boutigny/valid_cfht",NULL
DM-2706,"Adopt a scheme for securely managing passwords","We keep passing various credentials around for qsMaster etc. It'd be better to keep it more secure, perhaps using a tool like: https://www.vaultproject.io/ ?",NULL
DM-2713,"Check for connection leaks in integration tests","It'd be good to have some code that is checking if we are leaking connections. An easy way would be to just run  {code} show status like 'Threads_connected' {code} run some queries, and check the values for Threads_Connected again",NULL
DM-2714,"Make valgrind happier","`valgrind --tool=memcheck --leak-check=full ` runs on the unit tests should be clean(er) than they are now.",NULL
DM-2719,"SIP fitter (CreateWcsWithSip) is unstable when fitting with polynomials of order >3-4","CreateWcsWithSip is unable to fit sip polynomials of order > 3-4.  The problem is that the fitter is using the pixel coordinates as input leading to very small coefficients to compensate the huge numbers associated to the highest powers. The solution is to normalize the inputs to the [-1,+1] range and to rescale the SIP coefficients accordingly.",NULL
DM-2730,"Qserv code cleanup","random code cleanup: trailing white spaces, extraneous "";"" at the end of queries, spellings etc.",NULL
DM-2731,"Make an initial EUPS package for Firefly","If firefly will be used as a local display tool (e.g., a replacement for ds9), it should be possible to install it (and any dependencies it needs to bring in) with ease. Wrapping it as an EUPS package is one way to get that done.",NULL
DM-2732,"freetype 2.5.2 in anaconda crashes mac os X plotting with pdf and usetex=True","sims_maf fails a test when building sims_maf 1.0.1   Test testMetricBundle.py fails when plotting a pdf with usetex=true.   Embedding TeX font cmmi10 - fontinfo={'basefont': 'CMMI10', 'fontfile': u'/opt/local/share/texmf-texlive/fonts/type1/public/amsfonts/cm/cmmi10.pfb', 'encodingfile': None, 'effects': {}, 'dvifont': <matplotlib.dviread.DviFont object at 0x11510fa70>} python(46475,0x7fff743a0310) malloc: *** error for object 0x112189520: pointer being freed was not allocated *** set a breakpoint in malloc_error_break to debug Abort  The problem is described in https://github.com/matplotlib/matplotlib/issues/4359 and comes from an issue with freetype 2.5.2 in anaconda.   Solution was to  conda install freetype=2.4.10 conda install matplotlib=1.4.3=np19py27_0 ",NULL
DM-2739,"integration test is too picky to be very useful","The package lsst_dm_stack_demo is used by buildbot as an integration test by comparing an output file to an existing expected output data file. Unfortunately there are several problems with the test: - There is no statistical summary of the measured change, and in particular there is no report of the maximum error. When this test breaks we usually see tiny changes in many values. But if one or a few values have big changes (large enough to be really scary) we may not see this. In other words it is far too easy for minor innocuous changes to hide an crucial large change. - The data is written out using a number of digits that varies. I am not sure of all the causes of variability, but we have observed different files created on different systems. A recent commit (DM-2719) apparently caused the number of digits to change on buildbot, though I find it very hard to believe. The numbers match out to the number of digits written, but the number changed, resulting in a failed integration test. - Even if the data was written out to a consistent number of digits, the test should probably rely on fewer digits, as meaningless minor changes can cause failures.  The result of is that we have many false negatives, which makes us take failures less seriously and wastes our time trying to figure out what is wrong when nothing is wrong.",NULL
DM-2741,"Improve the Executive","Some parts of the Executive are difficult to follow and it is believed that there are some changes that can be made to make it clearer. These include:   Linking QueryResource and QueryRequest together via multiple inheritance, making them members of the same class, or something similar.  Unchain the retryFunc objects and have the Executive control the retry attempts directly.  Have Finished() called whenever cleanup() is called. ",NULL
DM-2742,"Contract payment due to Fernando Liello","Payment of second invoice to Fernando Liello of $25,000",NULL
DM-2746,"buildbot fails due to xpa not found","buildbot is failing because xpa is no longer a required package in the ups table but it's still mentioned in the cfg file (thanks to K-T for the diagnosis).",NULL
DM-2747,"Make a Github Release with lddt_dm_demo tarball during relase","Exctend sq-codekit",NULL
DM-2749,"Add instructions on how to upgrade eups in a stack","As long as eups is a component of our packaging and distribution system, we need to provide instructions on how to upgrade it in an existing stack installation (and we should suggest that users upgrade to 1.5.9, particularly if they have a shared stack installation).  Note that the vast majority of stacks do not have eups managing eups, which has been suggested at times.  I think that having eups manage eups is more complex than is necessary for most people who will only upgrade occasionally.  I believe the process amounts to mostly ""move your existing one aside, make sure you're using the right Python, download/clone, configure, make install"", but there may be some subtleties that we should make sure we are getting right.  [~jbosch] and [~rhl] seem like the obvious sources of information here. ",NULL
DM-2753,"migrate astrometry_net_data repo from gitolite -> github","This repo is ~2MB in size and one of only 3 repos that lsstsw is pulling from gitolite.  It should be migrated to github towards the goal of moving DM completely away from gitolite.",NULL
DM-2760,"Data loader fails to decompress compressed data","In some condition data loader decides to not uncompress data files before loading them directly into database table. No errors are generated in this case, but the result is funny.",NULL
DM-2761,"Integration test case04/0011_selectDeepCoadd.txt fails in master","Running inegration test with current master I see failure: {noformat} ERROR:lsst.qserv.tests.benchmark:Broken queries list in /usr/local/home/salnikov/qserv-run/2015_05/tmp/qservTest_case04/outputs/qserv: ['0011_selectDeepCoadd.txt'] CRITICAL:root:Test case #04 failed {noformat}  This started happening after DM-2734 merge, should have something to do with the per-table configuration.",NULL
DM-2767,"Fix an error in sensor location reporting","The location information of which machine has which sensor (for wavefront images) is reported incorrectly to the archive DMCS.",NULL
DM-277,"datarel.utils.getPsf() fails","[87d8d5a|https://dev.lsstcorp.org/cgit/LSST/DMS/datarel.git/commit/?id=87d8d5a76c63e2fb840af3d3ebedfd41e5a86986] modified {{utils.py}} to provide a function to get a PSF from an exposure.  Unfortunately, while one typo (""imageOrgin"" -> ""imageOrigin"") was fixed in the process, another one occurred (""dataId=dataId"" was left out).  This causes any attempt to retrieve a PSF using this function to fail for lack of required data identifier keys.",NULL
DM-2771,"afw can not be built without EUPS","After manually removing the product dependencies, sconsUtils still attempts to import eups.    {code}  $ AFW_DIR=. scons   scons: Reading SConscript files ...  Unable to import eups; guessing flavor  Checking who built the CC compiler...error: no result  CC is gcc version 4.8.3  Checking for C++11 support  Checking whether the C++ compiler worksyes  C++11 supported with '-std=c++11'  Checking for C++ header file tr1/unordered_map... yes  Setting up environment to build package 'afw'.  Unable to import eups; guessing flavor  Could not find EUPS product dir for 'afw'; using /home/vagrant/afw.  Doxygen is not setup; skipping documentation build.  ImportError: No module named eups:    File ""/home/vagrant/afw/SConstruct"", line 3:      scripts.BasicSConstruct(""afw"")    File ""/home/vagrant/foo/python/lsst/sconsUtils/scripts.py"", line 56:      versionModuleName, noCfgFile=noCfgFile)    File ""/home/vagrant/foo/python/lsst/sconsUtils/scripts.py"", line 106:      SCons.Script.SConscript(os.path.join(root, ""SConscript""))    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 609:      return method(*args, **kw)    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 546:      return _SConscript(self.fs, *files, **subst_kw)    File ""/usr/lib/scons/SCons/Script/SConscript.py"", line 260:      exec _file_ in call_stack[-1].globals    File ""/home/vagrant/afw/lib/SConscript"", line 6:      import eups  {code}",NULL
DM-2772,"datarel tests depend on EUPS","`tests/SConscript` imports eups:    https://github.com/lsst/datarel/blob/master/tests/SConscript#L4",NULL
DM-2773,"testing_endtoend tests depend on EUPS","`tests/SConscript`    https://github.com/lsst/testing_endtoend/blob/master/tests/SConscript#L4",NULL
DM-2774,"Remove eups dependency from tests","In RFC-44 we agreed that tests should not have a direct dependency on eups. Currently there are two distinct uses of eups in tests:    1. {{import eups}} when eups is not actually required.  2. Locating of test data using {{eups.productDir}}. DM-2635 provided a general utility function for obtaining the location of packages.    eups is also used in some non-test code (8 packages). In many cases the above applies and those fixes can be wrapped up in this ticket.",NULL
DM-2776,"Remove version functions that rely on SVN","In DM-2635 support for SVN version guessing was removed. Turns out that ip_diffim does call the removed function. Additionally, pex_logging has a version function that relies on {{$HeadURL$}}. Both these version functions should be removed as Sconsutils now writes the version information in a standardized way for each package.",NULL
DM-2788,"component for pex_policy","There is no jira component for `pex_policy",NULL
DM-279,"ForcedPhotTask.run() fails","The last commit (in July 2013) did this: {code:diff} --- a/python/lsst/pipe/tasks/forcedPhot.py +++ b/python/lsst/pipe/tasks/forcedPhot.py @@ -152,8 +152,7 @@ class ForcedPhotTask(CmdLineTask):        @timeMethod      def run(self, dataRef): -        inputs = self.readInputs(dataRef) -        exposure = inputs.exposure +        exposure = dataRef.get(exposureName)            expBits = dataRef.get(""ccdExposureId_bits"")          expId = long(dataRef.get(""ccdExposureId"")) @@ -165,18 +164,6 @@ class ForcedPhotTask(CmdLineTask):          self.measurement.run(exposure, sources, references=references)          self.writeOutput(dataRef, sources)   -    def readInputs(self, dataRef, exposureName=""calexp"", psfName=""psf""): -        """"""Read inputs for exposure - -        @param dataRef         Data reference from butler -        @param exposureName    Name for exposure in butler -        @param psfName         Name for PSF in butler - -        """""" -        return Struct(exposure=dataRef.get(exposureName), -                      psf=dataRef.get(psfName), -                      ) -      def makeSources(self, references, idFactory):          """"""Generate sources to be measured {code}  {{exposureName}} is now undefined.  It looks like it should be replaced by {{""calexp""}}.",NULL
DM-2791,"lsstsw does not deply anaconda on Macs","the deploy script for lsstsw does not contain the    	if ! PATH=""$PREFIX/bin:$PATH"" conda list --no-pip --json system >/dev/null; then          echo ""No system package to upgrade""    fix, which accounts for the fact that there is no 'system' package to update on Macs.  This prevents lsstsw from properly deploying.",NULL
DM-2796,"obs_subaru cannot be built due to a missing hsc header file","Building obs_subaru on a standard LSST stack fails with an error:  {code}  src/DistEstXYTransform.cc:26:10: fatal error: 'hsc/meas/match/distest.h' file not found  #include ""hsc/meas/match/distest.h""  {code}    It also has a warning that would be nice to have fixed:  {code}  src/Crosstalk.cc:95:66: warning: explicitly assigning a variable of type 'std::size_t' (aka 'unsigned long') to itself [-Wself-assign]              PixelVector const& ctx1List = getCrosstalkX1(i, nxAmp=nxAmp);  {code}    It may not be essential to fix this, but it would be very handy because it would allow us to update obs_subaru when we make changes in dependencies (as I am doing right now with DM-1578)",NULL
DM-281,"Check sinc aperture threshold and caching","I've just made it so the multiple-aperture photometry code calls the sinc photometry code for small apertures, on DM-115.  I've set the threshold for when to switch algorithms (maxSincRadius) to 10 pixels, but we should experiment with different values to see where it starts mattering.  We should also make certain that the way I'm using the sinc code doesn't get in the way of the caching system.  From reading the code, it looks fine, but I'd appreciate the opinion of an expert.",NULL
DM-282,"Make sure new metadata exceptions do not crash a qserv czar","Versions affected: git: ae099d7eeeb1996a  tickets/2939 implemented additional strictness in the metadata interface so that exceptions are thrown when callers misuse (or use lazily) the API. This ticket covers elimination of the current faulty code.",NULL
DM-2828,"mysqlclient installation failure on Mac OS","When helping folks install the stack on machines running OS X 10.8 and 10.9, {{mysqlclient}} failed with an error from sed about an ""illegal byte sequence"".    This seems to be the same error as DM-2372 in the {{mysql}} package and the solution is likely identical. Certainly, on the machines in question, we were able to complete the stack installation by invoking:  {code}  $ LC_ALL=C eups distrib install ...  {code}  However, I am not able to reproduce this on my own machine so cannot verify the fix.",NULL
DM-283,"Add rotation to text in ds9.dot()","Since sensors are typically symmetric, it's hard to tell which orientation a ccd is in from looking at it in ds9.  If I add rotation to text, it will be easier to tell that a chip is rotated in the focal plane system.",NULL
DM-2831,"ConfigDictField gets upset if the contained Config has a constructor","I'm making a config with a field that is a ConfigDict. If that ConfigDict value type is a Config that has a constructor then I cannot set the ConfigDict field, even though I can construct the underlying config. I'm puzzled how the ConfigDict can even tell that the value type has a constructor, but clearly it can.    I have attached a simple demo (too simple to show any reason for having a constructor in the contained config).",NULL
DM-2832,"Config constructor is unsafe if the config contains a ConfigDictField","The constructor for Config accepts keywords arguments to set fields. That is a wonderful feature (and one I did not know about until today). In many cases it appears to be safe, in that providing an invalid field name results in an exception. However, I have found a situation where data provided for an invalid field name is silently ignored: when the Config contains a single field that is a ConfigDictField. The attached file provides a demo.",NULL
DM-284,"XXX Qserv needs logic to determine if it is in a consistent state","Want to check if Qserv as a whole is runnable and consistent, and whether each component is runnable and consistent. That includes things like checking if port numbers are not in use, whether schema of the data in the database is consistent with associated metadata in zookeeper etc. ",NULL
DM-2846,"remove hardcoded username from doc/publish.sh","The title says it all... the latest commit done in DM-2630 introduced a hardcoded user name, please fix it.",NULL
DM-2848,"Test GPFS Server for Performant Access to Condo Storage","Use a GFPS VM server (provided temporarily by NCSA SET) to test and evaluate GPFS server functionality.  The idea being to figure out some initial use cases for GPFS and determine hardware requirements for actual implementation.",NULL
DM-285,"OS X Xcode update breaks boost build","From an e-mail conversation with Lynne:  Well, look at that .. yes I did upgrade on March 12. and now have the versions which are not compatible.   clang --version Apple LLVM version 5.1 (clang-503.0.38) (based on LLVM 3.4svn) Target: x86_64-apple-darwin13.1.0 Thread model: posix  At least there’s a patch it sounds like?   On Mar 20, 2014, at 10:24 PM, Mario Juric <mjuric@lsst.org> wrote:  > On 3/21/14, 0:10 , Lynne Jones wrote: >> Hi Mario,  >> >> On mavericks (with macports, but system clang, in /usr/bin/clang), I’m getting an error building boost. >> >> Build log file attached.  >> > > Let me guess, you've recently updated or installed Xcode? > > https://github.com/Homebrew/homebrew/issues/27396 > > Fsck. Will this release ever end... :). > > - M > >> >> >> >> >> I’ll try it with anaconda, but I’m thinking that this should be unrelated to python (?).  >> >> Lynne >> >> >> On Mar 20, 2014, at 9:35 PM, Mario Juric <mjuric@lsst.org> wrote: >> >>> I just pushed out a (hopefully final) release candidate of Winter'14. >>> >>> Try: >>> >>> curl -O http://sw.lsstcorp.org/eupspkg/newinstall.sh >>> bash newinstall.sh >>> >>> and follow the instructions (either mac or Linux, there's no difference >>> any more). Once the basic stuff is set up, instead of installing >>> lsst_apps, install pex_config (it'll be clear once you see the message >>> that newinstall.sh process ends with). >>> >>> Let me know if it didn't work. >>> >>> - M. ",NULL
DM-2857,"dateTime.py test in daf_base could fail based on system activity","When I tried to install sims the other day, I got the attached error in one of the daf_base unit tests.  The test calls two different methods of calculating the system time and compares them.  Obviously, my computer was running something else at the time and the lag between the two calls was long enough to cause a failure in the unit test.  I immediately started the build up again and everything worked fine.  Other users might not know to do that.  Perhaps the test should try itself several times and only declare a failure if all attempts fail.    For good measure: this was on OSX 10.10.3",NULL
DM-286,"Polygon.cc fails to build on RHEL6-32","Line 27 has {{unsigned long}} for the type of the template parameter specifying the number of dimensions.  Boost defines this as {{std::size_t}}.  On 32-bit machines, these are not the same.",NULL
DM-2860,"Add support for ORDER BY expressions","Parser doesn't support function in ORDER BY clause:  {code}   order by ABS(taiMidPoint)  {code}    {code}  SELECT objectId, taiMidPoint FROM Source order by ABS(taiMidPoint) ASC  ERROR 4110 (Proxy) at line 1: Qserv error: 'ParseException:Parse error(ANTLR):unexpected token: (:'  {code}",NULL
DM-287,"Jira and Confluence open a new page when I log in","I don't know where to file this.  Can someone point me in the right direction?  When I get auto logged out of Jira or Confluence, I click the login button to log back in.  As expected, a username/password box come up in that window.  When I type in my username and password, a brand new window (not tab) is opened with my authenticated session.  This is pretty annoying as it leave the username/password window open in the other tab.  I'm using Chrome (33.0.1750.117) on Scientific Linux 6.",NULL
DM-2874,"Add support for polygon-based searches","Qserv should support the following type of queries:    {code}  -- Create a binary representation of the search polygon  SET @poly = scisql_s2CPolyToBin(359.9601, 2.5952,                                 0.0398, 2.5952,                                 0.0398, 2.6748,                                 359.9601, 2.6748);    -- Compute HTM ID ranges for the level 20 triangles overlapping  -- @poly. They will be stored in a temp table called scisql.Region  -- with two columns, htmMin and htmMax  CALL scisql.scisql_s2CPolyRegion(@poly, 20);    -- Select reference objects inside the polygon. The join against  -- the HTM ID range table populated above cuts down on the number of  -- SimRefObject rows that need to be tested against the polygon  SELECT refObjectId, isStar, ra, decl, rMag  FROM SimRefObject AS sro INNER JOIN      scisql.Region AS r ON (sro.htmId20 BETWEEN r.htmMin AND r.htmMax)  WHERE scisql_s2PtInCPoly(ra, decl, @poly) = 1;  {code}    This 3-step process can be hidden inside qserv, without exposing it to user. Ideally, we should be able to run it behind the scene, and avoid changing/extending the existing API between user and the proxy    This story has been transfered from https://dev.lsstcorp.org/trac/ticket/2056",NULL
DM-2875,"Centroid shift check uses wrong value","On Mar. 11, a change from a getter method to a direct variable reference in SdssShape.cc introduced a typo that leads to the centroid shift check failing in almost all circumstances: {{fabs(shape->x - ycen0) > shiftmax}}.  It is surprising that the unit tests did not discover this.",NULL
DM-2876,"Provide a Python function that returns the names (or at least the enum/numbers) of all the set flags in a given schema","Currently, if there is a general failure reported in the flags of a measurement algorithm there is no convenient was to quickly scan all of the flags to determine what is causing the error.  This is a request for a python convenience function that returns the names (or at least the enum/numbers) of all the set flags in a given schema.  ",NULL
DM-2878,"Inspect Tololo to Pachon path","Somyl will go to ",NULL
DM-288,"afw build fails on MacOS 10.9 due to undefined symbols","Lynne Jones reports that the latest release candidate fails to build on her Mac OS 10.9, with the following error:  {code} c++ -o examples/analyticKernel examples/analyticKernel.o -Llib -L/Users/lynnej/lsst/DarwinX86/wcslib/4.14+3/lib -L/Users/lynnej/lsst/DarwinX86/cfitsio/3310+2/lib -L/Users/lynnej/lsst/DarwinX86/gsl/1.15+2/lib -L/Users/lynnej/lsst/DarwinX86/minuit2/5.22.00+2/lib -L/Users/lynnej/lsst/DarwinX86/xpa/2.1.14+2/lib -L/Users/lynnej/lsst/DarwinX86/fftw/3.3.2+2/lib -L/Users/lynnej/lsst/DarwinX86/daf_persistence/8.0.0.0+1/lib -L/Users/lynnej/lsst/DarwinX86/mysqlclient/5.1.65+3/lib -L/Users/lynnej/lsst/DarwinX86/pex_policy/8.0.0.0+1/lib -L/Users/lynnej/lsst/DarwinX86/pex_logging/8.0.0.0+1/lib -L/Users/lynnej/lsst/DarwinX86/daf_base/8.0.0.0+1/lib -L/Users/lynnej/lsst/DarwinX86/utils/8.0.0.0+1/lib -L/Users/lynnej/lsst/DarwinX86/pex_exceptions/8.0.0.0+1/lib -L/Users/lynnej/lsst/DarwinX86/base/8.0.0.0+1/lib -L/Users/lynnej/lsst/DarwinX86/boost/1.55.0.1/lib -L/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/config -lafw -lwcs -lcfitsio -lgsl -lgslcblas -lMinuit2 -lxpa -lfftw3 -ldaf_persistence -lboost_serialization -lmysqlclient_r -lpex_policy -lpex_logging -lboost_filesystem -lboost_system -ldaf_base -lutils -lpex_exceptions -lbase -lboost_regex -lpthread 1 warning generated. Undefined symbols for architecture x86_64:   ""lsst::afw::math::GaussianFunction2<double>::getPersistenceName() const"", referenced from:       vtable for lsst::afw::math::GaussianFunction2<double> in analyticKernel.o   ""lsst::afw::math::GaussianFunction2<double>::write(lsst::afw::table::io::OutputArchiveHandle&) const"", referenced from:       vtable for lsst::afw::math::GaussianFunction2<double> in analyticKernel.o ld: symbol(s) not found for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) scons: *** [examples/analyticKernel] Error 1 3 warnings generated. scons: building terminated because of errors. {code}  This appears to be due to the lack of template instantiations for {{GaussianFunction2}} (and {{DoubleGaussianFunction2}}.",NULL
DM-2880,"Create transformation task for CalibrateTask outputs","The tasks defined in DM-2191 are probably not sufficiently general to handle the {{icSrc}} dataset generated by {{CalibrateTask}}. An {{icSrc}} specific task will need to be defined.    [~jbosch] is anticipating a {{CalibrateTask}} rewrite at some point; we should keep this issue on hold until the plan there is more concrete.",NULL
DM-2886,"scons erroneously rebuilds when command-line targets change","{{scons}} seems to think some headers are no longer dependencies when scons is run with the {{examples}} or {{install}} targets. This can result in rebuilding entire packages when {{scons install}} is run after {{scons}} is run.    This has been observed in both afw and coadd_utils.    The workaround for this is to always specify {{lib}} first in your scons targets, even if it's implicit. In other words, always do:    {noformat}  scons lib python  {noformat}  or  {noformat}  scons lib examples  {noformat}  even though {{examples}} and {{lib}} depend on {{lib}}.    I've experimented with adding this to sconsUtils by messing with {{BUILD_TARGETS}} and {{COMMAND_LINE_TARGETS}}, but it seems to process the dependencies before we have a chance to modify the order of these, so that's not going to work.    This bug has been filed upstream with scons: http://scons.tigris.org/issues/show_bug.cgi?id=2798    Ported from TRAC Ticket 1800 (https://dev.lsstcorp.org/trac/ticket/1800)",NULL
DM-2888,"Allow OR after spatial constraint","One of the restrictions is: ""OR is not allowed after qserv_areaspec_* constraint"": We should eventually allow ORs of qserv_areaspec_* constraints themselves",NULL
DM-2889,"Allow sampling based on BIT operations on objectId in WHERE ","For sampling in full table scans, and possibly for other selections where the objectId is decomposed into bitfields, WHERE clauses will want to use formulas applied to objectId.",NULL
DM-2891,"meas.algorithms.utils uses measurement algorithms that are no longer available","meas.algorithms.utils uses GaussianCentroid and SdssShape, but now that they have moved to meas_base the code no longer works.    Please fix this.  I'd prefer to leave the functionality to visualise PSFs in meas_algorithms, but if necessary file an RFC to move it elsewhere.  ",NULL
DM-2894,"Study how to solve ""ORDER BY FUNC(field)""","This problem is complex. This ticket aims at breaking it down in simpler tasks. ",NULL
DM-2896,"update lsstsw anaconda version to 2.2.0","Per RFC-54, the reference anaconda version used by lsstsw will be updated to 2.2.0.  No changes to newinstall.sh should be necessary.",NULL
DM-2901,"Improve error message triggered by selecting unknown column","Selecting a column that does not exist, for example:   {code}  SELECT nonExistingColumn   FROM Object  {code}    Generates an obscure error message:    {code}  ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150608-19:30:36, Error in result data., 1,   Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150608-19:30:36, State error (unrecognized), 0,   Ref=3 Resource(/chk/qservTest_case01_qserv/6800): 20150608 (-1)  {code}    This story involves changing the code so that we produce a more obvious error. BTW, MySQL will produce   {code}  ERROR 1054 (42S22): Unknown column '<name here>' in 'field list'  {code} ",NULL
DM-2902,"Fix problem with extra paranthesis in GROUP BY","The following query:    {code}  SELECT zNumObs, COUNT(*) AS theCount  FROM Object  WHERE qserv_areaspec_box(0.1, -6, 4, 6)  GROUP BY (zNumObs)  {code}    Fails with an error:  {code}  ERROR 4110 (Proxy): Qserv error: 'ParseException:Parse error(ANTLR):unexpected token: (:'  {code}    The extra paranthesis around zNumObjs are causing the parser to fail. ""GROUP BY zNumObs"" works just fine.",NULL
DM-2904,"Define procedure for setting metadata on measurement SourceCatalog","It is not clear how the metadata can be set on a SourceCatalog which is output by SingleFrameMeasurement.  There is an algMetadata which can be optionally input to the init() method of the measurement algorithm, but it is never set on the sources output.  In fact, I believe that this input parameter is only there for consistency with the meas_algorithms SourceMeasurementTask.    Should meas_base sfm.py be altered to use this algMetadata parameter, or is there some other was (maybe setMetadata) that metadata can be set on the source catalog output from the SingleFrameMeasurementTask.run() method?",NULL
DM-2906,"migrate qserv_testdata repo from gitolite -> github","This repo is 304MiB (GH limit is 1GiB) and all objects are under the 100MiB GH size limit.  Two objects are > 50MiB triggering a GH warning but it is advisory only.      Discussion on #qserv a few weeks ago suggested that the GH limitations were acceptable for at least the near future of this repo.  Are there any objections or concerns about migrating this repo from gitolite -> github?    {code}  [master] ~/tmp/qserv_testdata $ git remote add origin git@github.com:jhoblitt/qserv_testdata.git  [master] ~/tmp/qserv_testdata $ git push -u origin master  Counting objects: 1193, done.  Delta compression using up to 8 threads.  Compressing objects: 100% (627/627), done.  Writing objects: 100% (1193/1193), 182.04 MiB | 4.00 MiB/s, done.  Total 1193 (delta 565), reused 1118 (delta 513)  remote: warning: GH001: Large files detected.  remote: warning: See http://git.io/iEPt8g for more information.  remote: warning: File datasets/case03/data/RunDeepForcedSource.txt.gz is 69.67 MB; this is larger than GitHub's recommended maximum file size of 50 MB  remote: warning: File case03/data/RunDeepForcedSource.txt.gz is 72.27 MB; this is larger than GitHub's recommended maximum file size of 50 MB  To git@github.com:jhoblitt/qserv_testdata.git  * [new branch]      master -> master  Branch master set up to track remote branch master from origin.  {code}",NULL
DM-2907,"Please add popup for closing tickets","Can we have a popup for closing tickets?  I often want to comment on the code review when moving a ticket to Done  ",NULL
DM-2912,"obs_LsstSim camera data is not self-consistent","The attached scripts use the afw.cameraGeom.Camera provided by obs.lsstSim.LsstSimMapper as well as a camera generated directly from the description/focalplanelayout.txt and description/focalplanelayout.txt from obs_LsstSim to generate pixel coordinates from a catalog of stars.  These ought to return identical pixel coordinates.  They do not.    If you run cameraTest.py you will get a catalog that lists the pixel coordinates in the control camera (the camera provided by LsstSimMapper), the pixel coordinates in the test camera, and the difference between the two.    Note: the scripts use CatSim code, so sims_catUtils will need to be set up.    Instructions for accessing the CatSim databases are here    https://confluence.lsstcorp.org/display/SIM/Accessing+the+UW+CATSIM+Database    ",NULL
DM-2916,"Add support for multiple secondary indexes","related code in Facade::getSecIndexColNames()",NULL
DM-2918,"the test for processCcd should test astrometry and color term correction","The unit test for processCcd presently tests neither astrometry nor applying colorterms in PhotoCalTask. Tests for both of these would be very helpful, but will require adding a significant amount of data.",NULL
DM-292,"newinstall.sh empty directory check is too strict","newinstall.sh should ignore files beginning with dot when checking if directory is empty. E.g., OS X creates .DS_Store files in empty directories whenever they're touched by Finder, which confuses the users.",NULL
DM-2924,"Support EQUINOX being ""J2000.0"" as well as 2000.0","I share the concern that we're going to spend a *lot* of effort on handling illegal WCS headers, but this one's easy, and we already have a partial fix for the SDSS headers which use a string but no ""J"" prefix.    Skymapper uses ""J2000.0"" not 2000.0 for its EQUINOX.  Our code silently converts this to float using atof, and thus interprets it as 0.0 and deduces FK4 (which we don't support).  ",NULL
DM-2926,"Formatting a Wcs object to FITS ignores PVi_j cards","The routine in WcsFormatter.cc generatePropertySet ignores any PVi_j cards.  This means that we are unable to round-trip e.g. TPV headers.  ",NULL
DM-2933,"Foreman may not register all task thread","ForemanImpl::RunnerMgr::getNextTask can start threads without having them registered with ForemanImpl::RunnerMgr::_running. The scheduler (see wched::BlendScheduler) can return a queue with several tasks. getNextTask starts a new thread for all tasks in the queue but only returns the task at front, and only that task is registered to _runner.",NULL
DM-2937,"Buildbot ""force build"" form is too picky","The buildbot insists on names of the form  {code}  Robert Lupton <rhl@astro.princeton.edu>  {code}  despite having already asked for a fullname;  it clears the form if it's unhappy (fortunately, back works).    Can we make it less picky?  ",NULL
DM-294,"Add instantiations in afw for meas_algorithms","There are a few undefined symbols in meas_algorithms due to missing explicit instantiations in afw.  I'll give this a shot.",NULL
DM-2941,"afwdata wasn't tagged with 10.1","This might be because it is still in gitolite rather than on GitHub, or it might be because it's data and not code.",NULL
DM-2942,"Is ""LSST Stack Installation"" page in LSWUG useful?","https://confluence.lsstcorp.org/display/LSWUG/LSST+Stack+Installation  seems to be a subset of  https://confluence.lsstcorp.org/display/LSWUG/Getting+Started+with+the+LSST+Software+Stack  with significant differences (e.g. no mention of CernVM FS).  The former  is only linked from the top page of LSWUG (in the child tree) and from  https://confluence.lsstcorp.org/display/LSWUG/Using+the+LSST+Stack.      Should these be reconciled somehow?",NULL
DM-2943,"Update leap second table","There is a leap second coming up in July 2015 and it needs to be added to {{DateTime.cc}}.",NULL
DM-2944,"SourceMeasurementTask still referenced in our stack","SourceMeasurementTask is gone, but we still have code that refers to it, including:  {code}  /Users/rowen/LSST/lsstsw/build/meas_algorithms/python/lsst/meas/algorithms/debugger.py:     21: from lsst.meas.algorithms.measurement import SourceMeasurementTask     26:     measurement = ConfigurableField(target=SourceMeasurementTask, doc=""Measurements"")    /Users/rowen/LSST/lsstsw/build/meas_algorithms/python/lsst/meas/algorithms/detection.py:    209: The example also runs the SourceMeasurementTask; see \ref meas_algorithms_measurement_Example for more explanation.    /Users/rowen/LSST/lsstsw/build/meas_deblender/examples/utils.py:     15: class DebugSourceMeasTask(measAlg.SourceMeasurementTask):     41:         measAlg.SourceMeasurementTask.preMeasureHook(self, exposure, sources)     74:         measAlg.SourceMeasurementTask.postMeasureHook(self, exposure, sources)     80:         measAlg.SourceMeasurementTask.preSingleMeasureHook(self, exposure, sources, i)    102:         measAlg.SourceMeasurementTask.postSingleMeasureHook(self, exposure, sources, i)    /Users/rowen/LSST/lsstsw/build/meas_deblender/python/lsst/meas/deblender/deblendAndMeasure.py:     31: from lsst.meas.algorithms import SourceMeasurementTask     50:         target = SourceMeasurementTask,    /Users/rowen/LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/calibrate.py:    180: <DT> initialMeasurement \ref SourceMeasurementTask_ ""SourceMeasurementTask""    189: <DT> measurement \ref SourceMeasurementTask_ ""SourceMeasurementTask""    /Users/rowen/LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/imageDifference.py:     36: from lsst.meas.algorithms import SourceDetectionTask, SourceMeasurementTask, \    104:         target=SourceMeasurementTask,    /Users/rowen/LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/measurePsf.py:    136: The example also runs SourceDetectionTask and SourceMeasurementTask; see \ref meas_algorithms_measurement_Example for more explanation.  {code}    I will handle pipe_tasks calibrate.py as part of DM-435.",NULL
DM-2946,"Add SDSS Colorterms to obs_lsstSim","I created some lsst to sdss conversions using the Kurucz SEDs and the sims_photUtils methods. A notebook located here: https://github.com/jbkalmbach/lsst_colorTransform/blob/master/LSST-SDSS%20Color%20Transformations.ipynb contains plots and more information.",NULL
DM-2947,"Data loader fails to load secondary index in multi-node case","Data loader crashes in multi-worker mode when it stores the data in secondary index table.",NULL
DM-2950,"Update subtask name in processImage (astrometer -> solver)","Due to a subtask renaming from *astrometer* to *solver* in DM-2740 (see [commit|https://github.com/lsst/meas_astrom/commit/5c938ef73777b582e45099c0562d8d035299e757#diff-1e094f9740933d4958721c5a9eedcaef]), the associated call to *astrometer* in {{processImage}} in {{pipe_tasks}} needs to be changed to *solver*.    This relates to DM-2329 (and will likely be restructured there).",NULL
DM-2957,"Make daf::base::PropertySet::conform() handle mis-matched types more gracefully","The member function PropertySet::conform() requires that the types of all fields common to the two sets be identical, throwing an exception when this condition is violated.    It would be useful to have a way to merge sets which currently throw this exception;  the case that precipitated this issue was inheriting FITS headers from an PDU where the PDU type for EQUINOX was (incorrectly) string, and the HDU (correctly) used float.  Another case is when LTV1 in the PDU is 0.0, and we set it to 0    One option would be an extra bool to discard the earlier set of values;  another would be a way of returning the list of offending keys for the caller to handle as they will.  ",NULL
DM-2968,"We duplicate information between WCS fits headers and WCS afwTables in Exposures","When we write Exposures we currently write WCS information to two places:  # the FITS headers  # an extension containing an afwTable    The theory behind this duplication is that the FITS headers may be only an approximation to the full Wcs in the extension, but we should think about whether this is the right way to support external users (e.g. should we only write the extension, but provide the ability to export the FITS-versions?  This plays into the FITS/HDF5/... discussion)",NULL
DM-2969,"We need to be able to export a wider range of Wcs types to Exposure's Wcs afwTable extension","Currently, only TAN and TAN-SIP WCSs can be persisted to the Wcs extension when we write Exposures to FITS files.   We need to extend this to support at least the commonly-encountered WCS projections.    I'm using WCS here to mean Calabretta-and-Greisen WCSs, as in general our {{Wcs}} objects may not map onto the FITS standard.  We will need to be able to persist more general (and probably composed) maps to disk.    ",NULL
DM-2970,"Implement the ability to create a Footprint enclosing a particular point","It is useful to be able to find the Footprint above/below a given Threshold that includes a particular pixel.  In particular, this may be used to remove the low-significance peaks that are typically found near large objects (with shallow brightness profiles) as their profile passes through the detection threshold.  ",NULL
DM-2971,"Make the Wcs ctor private/protected, and make sure that makeWcs accepts the same arguments as the ctor","Calling the Wcs constructor directly results in a base-class object, whereas we need derived classes whenever appropriate (e.g. we can currently only persist TanWcs).  The function {{makeWcs}} does this correctly, but calling the ctor directly doesn't/can't.  Please  make the Wcs() ctor private or protected to avoid this problem.    When switching the pipe_tasks mocks to use makeWcs not Wcs it became clear that the  set of arguments that Wcs() accepts is different from makeWcs().  We should probably make sure that the two APIs are consistent.  E.g.  {code}  wcs = lsst.afw.image.Wcs(crval, crpix, cd.getMatrix())  {code}  but  {code}  wcs = lsst.afw.image.makeWcs(crval, crpix, *cd.getMatrix().flatten())  {code}    ",NULL
DM-2988,"modify lsstswBuild.sh to print build/ftest failures instead of archiving them","This is needed when running under jenkins as devs will not have (or want) access to all of the slaves in the build farm.",NULL
DM-299,"Qserv should catch syntax problems earlier","Right now, Qserv can only catch syntax problems by dispatching to a worker and finding out later that there is a problem.  We should be able to catch syntax problems and typos much earlier. It should be simple to dispatch a query to null table on the master (that are schema-compliant). If no rows are returned, it passes the syntax check. This should trap most of the syntax errors.  We can do better if we are willing to write the code that checks column and table access. And we can do it, given rich-enough metadata. But this is a much greater amount of code. The first strategy is a matter of doing short-circuit dispatch to pre-prepared tables on a mysqld on the master, which should be tractable.  (transferred from trac ticket 2977)",NULL
DM-2994,"eups declare -t -c bug?","sims_maf_contrib is a github repo we encourage people to use for general (non-LSST) contributions to sims_maf.   as such, it's git cloned and then usually put into something like ~/lsstRepos/sims_maf_contrib  (vs. ~/lsst/ .. etc for eups distributed packages).   After it's cloned, doing :  cd ~/lsstRepos/sims_maf_contrib  eups declare -r . -c   results in a nonsense path being stored in eups.   Example:  [lucy:~/lsstRepos/sims_maf_contrib] lynnej% pwd  /Users/lynnej/lsstRepos/sims_maf_contrib  [lucy:~/lsstRepos/sims_maf_contrib] lynnej% eups list sims_maf_contrib  eups list: Unable to find product sims_maf_contrib  [lucy:~/lsstRepos/sims_maf_contrib] lynnej% eups declare -r . -c  [lucy:~/lsstRepos/sims_maf_contrib] lynnej% setup sims_maf_contrib  setup: Table file not found for sims_maf_contrib tag:current (DarwinX86): /Users/lynnej/lsst/epos/sims_maf_contrib/ups/sims_maf_contrib.table    What does seem to work is using the tag '$USER':  [lucy:~/lsstRepos/sims_maf_contrib] lynnej% eups undeclare sims_maf_contrib  [lucy:~/lsstRepos/sims_maf_contrib] lynnej% eups declare -r . -t $USER  Warning: path /Users/lynnej/lsstRepos/sims_maf_contrib is absolute, not relative to EUPS_PATH  [lucy:~/lsstRepos/sims_maf_contrib] lynnej% setup sims_maf_contrib -t $USER -t sims  [lucy:~/lsstRepos/sims_maf_contrib] lynnej%     I think this is a bug. I'm not entirely sure who to assign it to, so I've started with SQuaRE.",NULL
DM-2995,"logTest fails because wait() doesn't check for EINTR","I encountered unit test failure when building log on OS X 10.10:    {code}  (_build)mjuric@gallifrey:~/anaconda/conda-bld/work$ cat ./tests/.tests/*.failed  tests/logTest    Running 7 test cases...  waitpid: Interrupted system call  Running 7 test cases...  (_build)mjuric@gallifrey:~/anaconda/conda-bld/work$  {code}    I think the problem is due to {{errno==EINTR}} not being checked in the test code, as described on http://stackoverflow.com/questions/10160583/fork-multiple-processes-and-system-calls",NULL
DM-3,"Mirror LSST git repositories to github","To enable seamless interaction with the community, LSST code repositories should be automatically mirrored to github.  Any github user must be able to fork our repositories as necessary and build using the source from github.  They should be able to open github issues, which will be monitored by us and transitioned to Jira as necessary.  They should be able to post pull requests, which we would merge with our code (after proper review and vetting).",NULL
DM-30,"Expose details and TBDs in all aspects of DM system design","This epic represents a series of SAT meetings to drill down into all aspects of DM system design to expose details, assumptions, and remaining TBDs.  An emphasis will be placed on data inputs, flows, and outputs; fine points of science pipeline algorithms are less important.",NULL
DM-3000,"AUP Meeting with Iain","Discussed beginning project to acknowledge AUP receipt for new and existing accounts.",NULL
DM-3006,"fix postbuild ""cleanup"" script","Occasionally, when killing a lsstswBuild.sh job rebuild stays running and blocks the build slave until it exists.  An attempt to install a postbuild script to kill off any ""zombie"" builds was attempted but was unsuccessful.  This will likely become an on-going operational burden unless resolved.",NULL
DM-3007,"ganglia monitoring of build slaves","We should expose general build slave performance metrics to make it both easy for developers get a rough idea of the resource requirements a build requires and for the SQRE to judge health and tuning of the system.  A default ganglia should cover these requirements and be easy to automate.",NULL
DM-3008,"support fetching repos.yaml via http","The current repos.yaml scheme requires a locally copy be available for lsst_build to read.  This implies that any changes to this file need to be rapidly rolled out to multiple build slaves.    It will be more robust and require less engineering to retrieve the repos.yaml file on demand via https from the source repo (on github).  This will allow changes to take effect essentially as they are merged on github.",NULL
DM-3009,"hiera-facation of some lsststack::lsstsw params to ease testing","It would simply testing if git repo/branch parameters for lsststack::lsstsw could be supplied via hiera. Puppet's databindings do not [yet] provide automatic parameter lookup for defined types.  The lsststack::lsstsw defined type will need to be ""instrumented"" manually.",NULL
DM-301,"transfer trac tickets to jira","Daniel, please go through the existing tickets:   https://dev.lsstcorp.org/trac/query?status=assigned&status=deferred&status=inStandardsReview&status=inTicketWork&status=needInfo&status=new&component=qserv&groupdesc=1&group=status&col=id&col=summary&col=component&col=status&col=owner&col=type&col=priority&col=milestone&order=priority  and transfer to jira these that makes sense to transfer, attaching to the right epic or story. (In the meantime I'll continue doing the same, but in some cases you know better what the status of these issues is or where they should belong, so I'll skip these) ",NULL
DM-3010,"automated EC2 snapshots","Create a jenkins job with stored ec2 credentials to snapshot the master instance daily and maintain them for a period of at least 30 days.",NULL
DM-3015,"Add -headerpad_max_install_names to linker flags on OS X","This proposes to add -headerpad_max_install_names to enable production of relocatable binaries by post-processing with tools such as install_name_tool to modify the RPATH.    The patch has been proposed in https://github.com/lsst/sconsUtils/pull/4",NULL
DM-3017,"Remove deprecated mjd() method from daf_base","The {{mjd}} method has been deprecated since 2010. Remove it.",NULL
DM-3018,"Update dev quick-start guide for new qserv_testdata repo","Quick start guide for developers still points qserv_testdata to the old repo, it needs to be changed to the new github location.",NULL
DM-302,"find(Key) fails in empty afw::table::Schema","The assertion in line 230 of {{afw/tests/testSchema.py}} triggers a variety of behaviors (segfaults, assertion failures, successes) on my RHEL6-32 box.  This appears to be because {{SchemaImpl::find(Key)}} doesn't check to see if the {{lower_bound()}} return value is {{map::end}}.  Or am I doing something wrong?",NULL
DM-303,"Investigate why explicit instantiations are needed when they weren't before","There are a few undefined symbols in meas_algorithms due to missing explicit instantiations in afw.  I'll give this a shot.",NULL
DM-3039,"Draft changes to LDM-230","Given the new state model and mandatory command set for top-level subsystems, update LDM-230 to match",NULL
DM-304,"Rename SqlConfig to MySqlConfig","All classes in sql module start with sql. All classes in mysql module starts with mysql... except the sqlConfig class. I find it very confusing, and I'm going to add ""missing"" ""my"" in front, unless someone (Daniel?) has strong objections.",NULL
DM-3040,"KPM Measurement: Residual PSF Ellipticity Correlations (TE1), FY15","This will be measured on precursor data (almost certainly from HSC, via DM-2380) using the following procedure:   - Run visit-level processing (i.e. {{ProcessCcdTask}}, probably via the new HSC driver ported on DM-3368).   - Compute the residual ellipticity correlation function separately in each visit.  Bob Armstrong has code we can probably use for this (and will be at LSST2015 to help).  The _stile_ project (https://github.com/msimet/Stile) may be another solution (and perhaps a better long-term one).   - Compute the median correlation function (at separation bins) over multiple visits.    This will be done in whichever of r or i band has better public HSC data.    The same measurement procedure will be used for TE2 (DLP-308) in S15 (DM-3047).",NULL
DM-305,"Repackage ingest code out of datarel package","Ingest code may be useful for LSST Stack users but shouldn't require things like the orchestration software.  Package it by itself, and perhaps push it (and its dependencies, likely including {{cat}}, down from {{datarel} and {{lsst_distrib}} into {{lsst_apps}}.",NULL
DM-3057,"KPM Measurement: Relative Astrometry (AM1), FY15","This will be measured on precursor data (almost certainly from HSC, via DM-2380) using the following procedure:   - Run visit-level processing (i.e. ProcessCcdTask, probably via the new HSC driver ported on DM-3368).   - Run relative astrometric calibration (i.e. meas_mosaic, DM-2674).   - Select bright stars and match across visits (new scripts, mostly delegating to existing code). Just selecting the stars used for PSF determination would probably work.   - Generate pairs of objects with separation near the target (5' for this issue, 20' or 200' for DM-3064 and DM-3071).  Bin widths will have to be determined, as that's not included in the requirement specification.  Will require new code.   - Plot separation deltas (difference from per-pair mean separation) vs. magnitude; measure RMS and outliers in magnitude bins.    This requirement is specified only for r and i band.    The same procedure will be used for the AM1 (DLP-310), AM2 (DLP-311, DM-3064), and AM3 (DLP-312, DM-3071) measurements for this cycle.",NULL
DM-306,"Add C++ operator==() to DateTime","{{lsst.daf.base.DateTime}} has a Python {{\_\_eq__()}} method, but it doesn't have a C++ {{operator==()}} method.  There doesn't seem to be a good reason for this.",NULL
DM-3064,"KPM Measurement: Relative Astrometry (AM2), FY15","See DM-3057 for measurement procedure.",NULL
DM-307,"Eigen alignment problems in cameraGeom on RHEL6-32","Several tests (exposure.py, testCameraGeom.py, testDetector.py, testOrientation.py) fail on RHEL6-32 due to Eigen alignment problems: {noformat} python: /home/ktl/dmstack/Linux/eigen/3.1.1+2/include/Eigen/src/Core/DenseStorage.h:56: Eigen::internal::plain_array<T, Size, MatrixOrArrayOptions, 16>::plain_array() [with T = double, int Size = 4, int MatrixOrArrayOptions = 0]: Assertion `(reinterpret_cast<size_t>(array) & 0xf) == 0 && ""this assertion is explained here: "" ""http://eigen.tuxfamily.org/dox-devel/TopicUnalignedArrayAssert.html"" "" **** READ THIS WEB PAGE !!! ****""' failed. {noformat}  I believe the problems are due to the {{Matrix2d}} in Orientation and the Orientation instance variable in Detector.",NULL
DM-3071,"KPM Measurement: Relative Astrometry (AM3), FY15","See DM-3057 for measurement procedure.",NULL
DM-308,"Floating point problems on RHEL6-32","The following tests fail in very strange floating-point-related ways on my RHEL6-32 box.  It's not at all clear why the integer size should affect the floating point math.  * angle.py: {noformat}   File ""tests/angle.py"", line 213, in testWrap     self.assertLess(nearAngRad - refAngRad, math.pi) AssertionError: 3.1415926535897931 not less than 3.141592653589793 {noformat}  * background.py: {noformat}   File ""tests/background.py"", line 602, in testBadPatch     self.assertEqual(np.mean(bkgdImage[0:100, 0:100].getArray()), defaultValue) AssertionError: 10.00000078125 != 10 {noformat}  * polygon.py: {noformat} python: src/geom/Polygon.cc:116: double<unnamed>::pixelOverlap(const BoostPolygon&, int, int): Assertion `area <= 1.0' failed. {noformat}  In addition, {{Calib::operator==()}} fails in {{testExposureTable.py}}, but that can be worked around by adding {{DateTime::operator==()}} (see DM-306).",NULL
DM-3107,"sconsUtils doesn't recognise gcc by a different name","It is common to have multiple compilers available with their versions embedded in the name (e.g., {{gcc-4.8}}); these can be selected through the {{CC}} and {{CXX}} environment variables.  However, sconsUtils doesn't recognise a renamed gcc because it uses the binary name in the output from {{$CC --version}} rather than just {{gcc}}, and the regexes don't pick this up.    This is a pull request for fixed regexes.",NULL
DM-3112,"Invalid read in afw::math::Interpolate --- off-by-one bug","[~nlust] discovered an invalid read in afw::math::makeInterpolate by running valgrind on a test for PSFEx.    {code}  ==23324== Invalid read of size 8  ==23324==    at 0x1B09ACE4: lsst::afw::math::makeInterpolate(std::vector<double, std::allocator<double> > const&, std::vector<double, std::allocator<double> > const&, lsst::afw::math::Interpolate::Style) (Interpolate.cc:74)  ==23324==    by 0x1B0A9B16: lsst::afw::math::BackgroundMI::_setGridColumns(lsst::afw::math::Interpolate::Style, lsst::afw::math::UndersampleStyle, int, std::vector<int, std::allocator<int> > const&) const (BackgroundMI.cc:168)  ==23324==    by 0x1B0AC7A9: boost::shared_ptr<lsst::afw::image::Image<float> > lsst::afw::math::BackgroundMI::doGetImage<float>(lsst::afw::geom::Box2I const&, lsst::afw::math::Interpolate::Style, lsst::afw::math::UndersampleStyle) const (BackgroundMI.cc:339)  ==23324==    by 0x1B0AA0E8: lsst::afw::math::BackgroundMI::_getImage(lsst::afw::geom::Box2I const&, lsst::afw::math::Interpolate::Style, lsst::afw::math::UndersampleStyle, float) const (BackgroundMI.cc:453)  {code}    On inspection, the loop is running off the end of the array: when {{i = x.size() - 1}} (i.e., the last element), then it's trying to access x[i+1], which is off the end.",NULL
DM-3113,"prepare v0.2.0 release","A fair number of changes have been merged since the 2015-06-29 ""release"" of demo.ci.lsst.codes.  The documentation and deployment notes need to be updated. The upstream packer templates have been updated and the ""from scratch"" deployment process needs to be run through to verify ""reproduciblity"" before making a new ""production"" release.     Note that the hostname will likely be changed to `ci.lsst.codes` in the next deployment.",NULL
DM-3115,"Clarify c++ standard for named constants","As discussed in RFC-63, it'd be good to clarify in our C++ standard (section 3.3) that the ""must be all caps"" rule only applies to const variables that represent constants (i.e. those that would be set using an enum or #define in C); it does not apply to variables that happen to be determined at their point of definition.",NULL
DM-3116,"Jenkins build may not start if not already authenticated","I have seen the following issue several times, though I am not sure if it is 100% reproducible:  - In HipChat I open Bot: Jenkinds Demo  - I follow the link in the room description  - I enter the data for my run and press ""Build""  - I am taken to a page asking me enter my GitHub credentials, which I do  - I am taken back to the Jenkins page, but my job has not started and the page's fields are restored to their default values    At this point I can re-enter the parameters for my run and press Build and it all works. So this is not a big deal. But it is a silent failure and it can confuse users (it certainly has confused me).    I think the ideal behavior would be for Jenkins to start the build after authentication. But if that is too hard, then would it be possible for Jenkins to retain the parameters I had set and to warn me that I have to press Build again to start the build?    Alternatively, could the system could force me to authenticate before I fill out the build fields and press Build?",NULL
DM-3117,"write integration testing comments for meeting notes","Update https://confluence.lsstcorp.org/display/SYSENG/2015+July+08-10+CCS-DAQ-OCS-DM+Workshop+IV    with comments about how the integration testing went.",NULL
DM-3123,"nginx / jenkins log management","Explicit logrotate of logs that are likely to grow signifigantly with time (eg., nginx http logs) and possibly ingest logs into a management system such as the ELK stack.",NULL
DM-3124,"migrate from fork to an official jenkins github-oauth release","My work to modify the jenkins github-oauth plugin for our needs was merged by upstream last week and a new official release has been made.  We should test and then migrate to an official release of this plugin.",NULL
DM-3127,"email notification for failed builds","As with buildbot, add the functionality to receive notification email when a submitted build fails.",NULL
DM-3128,"build on additional platforms","Possible platforms of interest:    Fedora 22 - gcc 5.1  Ubuntu 14.04  OpenBSD|FreeBSD  OSX",NULL
DM-3129,"selectable python version in Jenkins","The ability to select a python env; essentially 2.7 or 3.3+.",NULL
DM-3131,"newinstall.sh job","Daily or weekly builds of newinstall.sh on various platforms.  Possibly a bridge to producing usable docker images.",NULL
DM-3132,"CI the CI","A jenkins job to test building a new jenkins-master from lsst-sqre/sandbox-jenkins-demo.",NULL
DM-314,"Stack build of base 8.0.0 hangs unexpectedly","During an attempt to build the Stack v8.0 Release Candidate on a new iMac box running:  - MacOSX 10.9.2 - Clang v5.1 (clang-503.0.38) Target: x86_64-apple-darwin13.1.0 - XCode v5.1  the process hung trying to build base 8.0.0.0+1. (Trace to be attached.) I had previously installed gfortran, and the Anaconda v1.9.1 python distribution. Oddly, building the stack using the newinstall.sh script, selecting the installer-provided Anaconda v1.8.0 python distro, the build completed successfully. ",NULL
DM-3143,"Fix warnings from processCcd.py on HSC","When running processCcd.py on HSC data, I get the following warnings:  {code}  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  : Empty WCS extension, using FITS header  {code}    I expect these are from reading the bias/dark/flat as {{Exposure}} when they were written as {{Image}}.  Probably just need to tweak the HscMapper.paf file.",NULL
DM-3145,"Can not see github links when logged out","The links on JIRA tickets to Github branches and pull requests only seem to be visible if the user is logged in. This restriction seems unnecessary given the public nature of our source repositories and causes me confusion sometimes when I click on a link to a ticket and can't find the code (this is especially so in my RSS client which has a built in browser that has not been authenticated with JIRA). Can we remove the restriction?",NULL
DM-3149,"Provide a useful error message when multiprocessing test fails","I stumbled upon the following problem when building pipe_base:  {code}  $ cat tests/.tests/testCmdLineTask.py.failed  ....  CameraMapper: Loading registry registry from /tmp/tmpYDSBOR/_parent/registry.sqlite3  ..  ======================================================================  ERROR: testMultiprocess (__main__.CmdLineTaskTestCase)  Test multiprocessing at a very minimal level  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testCmdLineTask.py"", line 140, in testMultiprocess      ""-j"", ""5"", ""--id"", ""visit=2"", ""filter=r""])    File ""/home/mjuric/miniconda/conda-bld/work/python/lsst/pipe/base/cmdLineTask.py"", line 435, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/home/mjuric/miniconda/conda-bld/work/python/lsst/pipe/base/cmdLineTask.py"", line 185, in run      pool = multiprocessing.Pool(processes=self.numProcesses, maxtasksperchild=1)    File ""/home/mjuric/miniconda/envs/_build/lib/python2.7/multiprocessing/__init__.py"", line 232, in Pool      return Pool(processes, initializer, initargs, maxtasksperchild)    File ""/home/mjuric/miniconda/envs/_build/lib/python2.7/multiprocessing/pool.py"", line 138, in __init__      self._setup_queues()    File ""/home/mjuric/miniconda/envs/_build/lib/python2.7/multiprocessing/pool.py"", line 234, in _setup_queues      self._inqueue = SimpleQueue()    File ""/home/mjuric/miniconda/envs/_build/lib/python2.7/multiprocessing/queues.py"", line 354, in __init__      self._rlock = Lock()    File ""/home/mjuric/miniconda/envs/_build/lib/python2.7/multiprocessing/synchronize.py"", line 147, in __init__      SemLock.__init__(self, SEMAPHORE, 1, 1)    File ""/home/mjuric/miniconda/envs/_build/lib/python2.7/multiprocessing/synchronize.py"", line 75, in __init__      sl = self._semlock = _multiprocessing.SemLock(kind, value, maxvalue)  OSError: [Errno 13] Permission denied    ----------------------------------------------------------------------  Ran 8 tests in 0.391s    FAILED (errors=1)  {code}    The cause of the failure is the fact /dev/shm wasn't readable (or mounted) on the particular system (e.g., see: http://stackoverflow.com/questions/2009278/python-multiprocessing-permission-denied). It would be nice to catch this exception and let the user know this is what may be going on.",NULL
DM-315,"Build Winter2014 LSST/DM Stack binaries for distribution","Build the Winter2014 Stack binaries for distribution, for all supported platforms. Platforms include all of those supported for Summer 2013, plus Mac OSX 10.9 (Mavericks). ",NULL
DM-3150,"CalibrateTask should optionally not compute initial astrometry","For processing data which already has a good WCS (such as CFHT and SDSS) it is unnecessary to perform an initial fit of astrometry before computing the PSF. Furthermore it is risky to do so because the astrometry fitter may fail.    At present CFHT handles this by overriding the CalibrateTask with its own variant task and a variant run method. However, the amount of code duplication required is unacceptable.    To avoid this I suggest we add the ability to disable the initial astrometric fit.    I also think we should Refactor CalibrateTask so its run method is much simpler and easier to override. But even if we do that (and perhaps it should be a separate ticket) it is such a common need to not run the initial astrometry that I think one should not have to override the run method to do it.  ",NULL
DM-3152,"Pop up comment window for transition to ""invalid""","When I put DM-2194 into ""Invalid"" state, I expected to get a popup where I could enter a comment to explain why the ticket is invalid, but I didn't.  This then required additional spam to provide the explanation.",NULL
DM-3156,"Support computing statistics on a scalar afw table field","It would be handy if our afw math statistics package could compute statistics on a given field of an afw table. As it stands, doing this in Python, at least, is messy: one must copy the data (since tables cannot be assumed to be contiguous) and then compute the statistics on the result.",NULL
DM-3159,"DipoleClassificationConfig.setDefaults returns before doing anything","[{{lsst.ip.diffim.dipoleMeasurement.DipoleClassificationConfig.setDefaults}}|https://github.com/lsst/ip_diffim/blob/master/python/lsst/ip/diffim/dipoleMeasurement.py#L54] has a {{return}} *before* the business logic. That can't be right.",NULL
DM-316,"Port obs_cfht to new CameraGeom","Port the current CFHT camera description to the new current CameraGeom framework.  As a first go, I'll translate the existing paf.  ",NULL
DM-3165,"""SELECT DISTINCT foo FROM LSST.Filter AS f"" shouldn't require a merge step","This query is non-chunked so the QuerySession.needMerge flag should be set to false    (grep ticket number in core/ to get the unit test where this issue appear)",NULL
DM-3166,"Suppress merge query for non-chunked query","See testQueryAnaOrderBy.cc:    {code:c++}  BOOST_AUTO_TEST_CASE(OrderByAggregateNotChunked) {      std::string stmt = ""SELECT SUM(photClam) FROM Filter ORDER BY filterId"";      std::string expectedParallel = ""SELECT SUM(photClam) AS QS1_SUM FROM LSST.Filter AS QST_1_"";      // FIXME merge query is not useful here      std::string expectedMerge = ""SELECT SUM(QS1_SUM)"";      auto querySession = check(qsTest, stmt, expectedParallel, """", expectedMerge);  }  {code}    Here, the non-chunked query should be issued on only one node, so the merge query is not useful. This need to be confirmed and suppressed.",NULL
DM-3168,"JIRA browser windows hang when changing ticket state if the comment field is empty","I find that when using JIRA to change the state of a ticket, if I don't specify anything for the comment field then after I press the button to change state, the window hangs. The state has changed, as I can find by accessing JIRA in a different tab, so it's just a cosmetic annoyance.    I often run into this because I often supply a comment for a state change first, then change state (e.g. I'll paste a review and clean it up just right, then put the ticket into ""reviewed"" state), since I don't like cleaning up text in modal dialog boxes.    This is on MacOS 10.9.5 using the current version of Safari. I have tested with other browsers or operating systems.",NULL
DM-317,"S14 Qserv: Basic productivity by advanced users","A running qserv installation should be usable: a) for a finite (non-zero) number of tasks, b) for at least one science-usable query, and  c) without complete confusion, by non qserv devs.  If the user remains within the supported queries, the system should be robust enough that if a qserv component crashes, then it either recovers automatically, or is easily restarted. From those same user inputs, the system should not get into an unknown or problem state that requires developer attention. ",NULL
DM-3184,"NoiseReplacer should be a context manager","I suggest that NoiseReplacer be made a context manager, so that its final cleanup is automatically and reliably performed, even if the code using it raises an exception.",NULL
DM-3187,"Add support for non-aliased fields in ORDER BY clause","Next clause    {code:sql}  SELECT f1 from Source as s ORDER BY s.id  {code}    isn't supported by Qserv, indeed merge table has an id, not s.id column.    Adding ORDER BY fields in SELECT clause using alias would solve this:    {code:sql}  SELECT f1, s.Id as __QSERV_ORDER_BY_1 from Source as s ORDER BY __QSERV_ORDER_BY_1  {code}    Nevertheless {{SELECT * FROM resultTable ORDER BY __QSERV_ORDER_BY_1}} query on mysql-proxy would have to be replaced with something like {{SELECT f1 FROM resultTable ORDER BY __QSERV_ORDER_BY_1}} (this select would take all columns except the __QSERV_ORDER_BY ones).",NULL
DM-3191,"Provide a (warning free) means of loading a FITS image plane with WCS","Since DM-2599 we issue one or more warnings when end-user code attempts to instantiate an {{Exposure}} or {{MaskedImage}} from a FITS file which does not contain the expected {{EXTTYPE}} metadata.    However, given the deprecation of {{DecoratedImage}} (see e.g. DM-3190), this means there's no supported way to load a single image plane with WCS from disk. (Likely, the end user would need to separately read image data and WCS, then combine them to form an {{Exposure}}).    This is a straightforward use case which we should support.",NULL
DM-3195,"One cannot add related links while editing RFC","While editing RFCs in JIRA I cannot find the field that allows me to add related links. That means I have to go to the ticket I want to link and link it there. Roundabout, but it works.",NULL
DM-3197,"Upgrade scons to v2.3.5","In DM-2905 scons was upgraded to v2.3.4. Version 2.3.5 was released shortly thereafter with some minor fixes.",NULL
DM-3198,"Reduce danger of circular imports in Python due to afw table persistence","persistence using afw table is prone to causing Python circular imports in python. When folks at HSC added table persistence to afw.geom.Polygon they found that this caused a circular dependency in Python. They resolved this moving Polygon a level deeper. That worked, but has the unfortunate effect of moving Polygon to a surprising namespace and making it harder to access.    This ticket asks for changes that reduce the danger of circular dependencies and allow a more natural namespace for Polygon and future classes that are persisted using afw table.    I do not fully understand the issue, but it I saw it while moving the HSC code over, and what I observed is that ""import lsst.afw.geom"" imported afw table, which in turn imported afw.coord.Coord, which in turn imported afw geom. I'm not sure why adding Polygon persistence caused this, nor how we got away with the current design before Polygon adding persistence.",NULL
DM-320,"rename testCppParser, rename control, merger modules","rename testCppParser to testQueryAnalysis  rename control -> ccontrol (czar control) rename merger -> rproc (result processing) ",NULL
DM-3200,"Base package fails to build under OS X El Capitan","I tried to build the stack on a Mac running El Capitan Public beta 2, and it gave three errors in base.      {noformat}   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                   Dload  Upload   Total   Spent    Left  Speed  100 12671  100 12671    0     0  18798      0 --:--:-- --:--:-- --:--:-- 18799    LSST Software Stack Builder  =======================================================================    Detected git version 2.3.2 (Apple Git-55). OK.      In addition to Python 2.7, some LSST packages depend on recent versions of numpy,  matplotlib, and scipy. If you don't have all of these, the installation may fail.  Using the Anaconda Python distribution will ensure all these are set up.    Anaconda Python installed by this installer will be managed by LSST's EUPS  package manager, and will not replace or modify your system python.    Would you like us to install Anaconda Python distribution (if unsure, say yes)? yes    Installing EUPS (v1.5.9)...  done.  Installing Anaconda Python Distribution ...     [  1/1  ]  anaconda 2.2.0                                             done.   Installing the basic environment ...     [  1/5  ]  doxygen 1.8.5                                              done.     [  2/5  ]  python 0.0.3 ...               Using externally provided Python 2.7.9 :: Anaconda 2.2.0 (x86_64).    [  2/5  ]  python 0.0.3                                               done.     [  3/5  ]  scons 2.3.4                                                done.     [  4/5  ]  sconsUtils 10.1-4-gce72cec+1                               done.     [  5/5  ]  lsst 10.1-1-g84e9557+13                                    done.   Creating startup scripts (bash) ... done.  Creating startup scripts (ksh) ... done.  Creating startup scripts (csh) ... done.  Creating startup scripts (zsh) ... done.    Bootstrap complete. To continue installing (and to use) the LSST stack  type one of:    source ""/Users/srp/stack/lsst/loadLSST.bash""  # for bash  source ""/Users/srp/stack/lsst/loadLSST.csh""   # for csh  source ""/Users/srp/stack/lsst/loadLSST.ksh""   # for ksh  source ""/Users/srp/stack/lsst/loadLSST.zsh""   # for zsh    Individual LSST packages may now be installed with the usual `eups  distrib install' command.  For example, to install the science pipeline  elements of the LSST stack, use:    eups distrib install lsst_apps    Next, read the documentation at:        https://confluence.lsstcorp.org/display/LSWUG/LSST+Software+User+Guide    and feel free to ask any questions via our mailing list at:        https://lists.lsst.org/mailman/listinfo/dm-users                                           Thanks!                                                  -- The LSST Software Teams                                                         http://dm.lsst.org/    srps-Mac-Pro:stack srp$ pwd  /Users/srp/stack  srps-Mac-Pro:stack srp$ ls  install.sh	lsst  srps-Mac-Pro:stack srp$ cd lsst  srps-Mac-Pro:lsst srp$ ls  DarwinX86	eups		loadLSST.csh	newinstall.sh  EupsBuildDir	eupsbuild.log	loadLSST.ksh	site  _build		loadLSST.bash	loadLSST.zsh	ups_db  srps-Mac-Pro:lsst srp$ source loadLSST.bash  srps-Mac-Pro:lsst srp$ eups distrib install lsst_apps    [  1/51 ]  cfitsio 3360.lsst1                                         done.     [  2/51 ]  doxygen 1.8.5 (already installed)                          done.     [  3/51 ]  eigen 3.2.0                                                done.     [  4/51 ]  fftw 3.3.3                                                 done.     [  5/51 ]  gsl 1.16.lsst1                                             done.     [  6/51 ]  minuit2 5.28.00                                            done.     [  7/51 ]  mysqlclient 5.1.73.lsst1-1-gb8bcc43                        done.     [  8/51 ]  python 0.0.3 (already installed)                           done.     [  9/51 ]  swig 3.0.2.lsst1                                           done.     [ 10/51 ]  xpa 2.1.15.lsst2                                           done.     [ 11/51 ]  boost 1.55.0.1.lsst2+3                                     done.     [ 12/51 ]  mysqlpython 1.2.3+17                                       done.     [ 13/51 ]  numpy 0.0.1+5 ...               Using externally provided numpy v1.9.2.    [ 13/51 ]  numpy 0.0.1+5                                              done.     [ 14/51 ]  scons 2.3.4 (already installed)                            done.     [ 15/51 ]  wcslib 4.14+7                                              done.     [ 16/51 ]  astrometry_net 0.50.1+6                                    done.     [ 17/51 ]  matplotlib 0.0.1+5 ...               Using externally provided matplotlib v1.4.3.    [ 17/51 ]  matplotlib 0.0.1+5                                         done.     [ 18/51 ]  pyfits 3.2.4.lsst1+3                                       done.     [ 19/51 ]  sconsUtils 10.1-4-gce72cec+1 (already installed)           done.     [ 20/51 ]  astrometry_net_data 10.0+23                                done.     [ 21/51 ]  base 10.1-1-g8080078+14 ...    ***** error: from /Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/build.log:  c++ -o lib/libbase.dylib -dynamiclib -Wl,-install_name -Wl,libbase.dylib src/ModuleImporter.os -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lpthread  swig -o tests/testModuleImporterLib_wrap.cc -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 -c++ -python tests/testModuleImporterLib.i  c++ -o python/lsstcppimport_wrap.os -c -std=c++11 -g -DLSST_HAVE_TR1=0 -DLSST_LITTLE_ENDIAN=1 -O3 -Wall -Wno-unused-function -fPIC -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 python/lsstcppimport_wrap.cc  c++ -o tests/ptr tests/ptr.o -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lbase -lpthread  running tests/ptr... c++ -o tests/testModuleImporter1 tests/testModuleImporter1.o -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lbase -lpthread  c++ -o tests/testModuleImporterLib_wrap.os -c -std=c++11 -g -DLSST_HAVE_TR1=0 -DLSST_LITTLE_ENDIAN=1 -O3 -Wall -Wno-unused-function -fPIC -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 tests/testModuleImporterLib_wrap.cc  sh: line 1: 70182 Trace/BPT trap: 5       ""tests/ptr"" >> ""tests/.tests/ptr"" 2>&1  failed  buildConfig([""doc/doxygen.conf""], [""doc/doxygen.conf.in""])  running tests/testModuleImporter1... sh: line 1: 70192 Trace/BPT trap: 5       ""tests/testModuleImporter1"" >> ""tests/.tests/testModuleImporter1"" 2>&1  failed  doxygen /Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/base-10.1-1-g8080078+14/doc/doxygen.conf  /Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/base-10.1-1-g8080078+14/doc/mainpage.dox:61: warning: unable to resolve link to `lsst.pipe.base.cmdLineTask.CmdLineTask' for \link command  c++ -o python/_lsstcppimport.so -bundle -F/ -undefined suppress -flat_namespace python/lsstcppimport_wrap.os -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lbase -lpthread -ldl -lpython2.7  c++ -o tests/_testModuleImporterLib.so -bundle -F/ -undefined suppress -flat_namespace tests/testModuleImporterLib_wrap.os -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lbase -lpthread -ldl -lpython2.7  running tests/testModuleImporter2.py... failed  3 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  + exit -4  eups distrib: Failed to build base-10.1-1-g8080078+14.eupspkg: Command:  	source /Users/srp/stack/lsst/eups/bin/setups.sh; export EUPS_PATH=/Users/srp/stack/lsst; (/Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/build.sh) >> /Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/build.log 2>&1 4>/Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/build.msg   exited with code 252    srps-Mac-Pro:lsst srp$   {noformat}    In the file /Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/build.log, near the end, we have:      {noformat}  Checking whether the C++ compiler works... yes  C++11 supported with '-std=c++11'  Checking for C++ header file tr1/unordered_map... no  Setting up environment to build package 'base'.  Checking whether int64_t is long ... no  Determining RTLD values...ok  scons: done reading SConscript files.  scons: Building targets ...  c++ -o src/ModuleImporter.os -c -std=c++11 -g -DLSST_HAVE_TR1=0 -DLSST_LITTLE_ENDIAN=1 -O3 -Wall -Wno-unused-function -fPIC -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 src/ModuleImporter.cc  Cannot guess fingerprint without .git directory; will be set to '0x0'.  swig -o python/lsstcppimport_wrap.cc -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 -c++ -python python/lsstcppimport.i  c++ -o tests/ptr.o -c -std=c++11 -g -DLSST_HAVE_TR1=0 -DLSST_LITTLE_ENDIAN=1 -O3 -Wall -Wno-unused-function -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 tests/ptr.cc  cd python && m4 -Dm4_RTLD_GLOBAL=8 -Dm4_RTLD_NOW=2 < lsst64defs.py.m4 > /Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/base-10.1-1-g8080078+14/python/lsst64defs.py  c++ -o tests/testModuleImporter1.o -c -std=c++11 -g -DLSST_HAVE_TR1=0 -DLSST_LITTLE_ENDIAN=1 -O3 -Wall -Wno-unused-function -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 tests/testModuleImporter1.cc  c++ -o lib/libbase.dylib -dynamiclib -Wl,-install_name -Wl,libbase.dylib src/ModuleImporter.os -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lpthread  swig -o tests/testModuleImporterLib_wrap.cc -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 -c++ -python tests/testModuleImporterLib.i  c++ -o python/lsstcppimport_wrap.os -c -std=c++11 -g -DLSST_HAVE_TR1=0 -DLSST_LITTLE_ENDIAN=1 -O3 -Wall -Wno-unused-function -fPIC -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 python/lsstcppimport_wrap.cc  c++ -o tests/ptr tests/ptr.o -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lbase -lpthread  running tests/ptr... c++ -o tests/testModuleImporter1 tests/testModuleImporter1.o -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lbase -lpthread  c++ -o tests/testModuleImporterLib_wrap.os -c -std=c++11 -g -DLSST_HAVE_TR1=0 -DLSST_LITTLE_ENDIAN=1 -O3 -Wall -Wno-unused-function -fPIC -Iinclude -I/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/include -I/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/include/python2.7 tests/testModuleImporterLib_wrap.cc  sh: line 1: 70182 Trace/BPT trap: 5       ""tests/ptr"" >> ""tests/.tests/ptr"" 2>&1  failed  buildConfig([""doc/doxygen.conf""], [""doc/doxygen.conf.in""])  running tests/testModuleImporter1... sh: line 1: 70192 Trace/BPT trap: 5       ""tests/testModuleImporter1"" >> ""tests/.tests/testModuleImporter1"" 2>&1  failed  doxygen /Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/base-10.1-1-g8080078+14/doc/doxygen.conf  /Users/srp/stack/lsst/EupsBuildDir/DarwinX86/base-10.1-1-g8080078+14/base-10.1-1-g8080078+14/doc/mainpage.dox:61: warning: unable to resolve link to `lsst.pipe.base.cmdLineTask.CmdLineTask' for \link command  c++ -o python/_lsstcppimport.so -bundle -F/ -undefined suppress -flat_namespace python/lsstcppimport_wrap.os -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lbase -lpthread -ldl -lpython2.7  c++ -o tests/_testModuleImporterLib.so -bundle -F/ -undefined suppress -flat_namespace tests/testModuleImporterLib_wrap.os -Llib -L/Users/srp/stack/lsst/DarwinX86/boost/1.55.0.1.lsst2+3/lib -L/Users/srp/stack/lsst/DarwinX86/anaconda/2.2.0/lib/python2.7/config -lbase -lpthread -ldl -lpython2.7  running tests/testModuleImporter2.py... failed  3 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  + exit -4  {noformat}      ",NULL
DM-3201,"Interpolation allows non-positive variance","The {{interpolateOverDefects}} function allows the variance to be non-positive in two ways:  * The interpolation is not constrained to be positive.  * The {{fallbackValue}} used for the variance is identical to that for the image, and may be non-positive.    If the variance can be interpolated to zero, a significant area (e.g., pixels on a coadd that have no corresponding inputs) may be detected as a source, leading to excessive memory usage when we produce heavy footprints.    This is a transfer of [HSC-1186|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1186] (unresolved at time of writing).",NULL
DM-3203,"ds9.mtv silently fails","While debugging, I tried the age-old technique:  {code}  import lsst.afw.display.ds9 as ds9  ds9.mtv(exposure)  {code}    This does not work, and there is no indication of failure.  I have been told that I need package ""display_ds9"".  This requirement has not been advertised, and there is nothing on the standard output that indicates this.",NULL
DM-3208,"sconsUtils ""probe"" refactoring","Per the discussion on: https://github.com/lsst/sconsUtils/pull/6 , it would be better if scons native checks were implemented to problem for eups and doxygen. See the review comments for details.",NULL
DM-3219,"router interfaces to subnets stuck in ""Down"" status","This true of both attempting to add interfaces to a router with an external gateway and one without.",NULL
DM-3221,"horizon instance console is broken","The horizon instance console tab never displays/connects to the instance's console.  If the hypervisor/nova configuration doesn't support this (which would be unfortunate), the tab should at least be disabled.",NULL
DM-3222,"horizon https certificate is self signed","A public CA signed certificate needs to be in place before allowing general users on the system so as not to train them to click through certificate errors.  Also note that the self signed cert has a validity period of 5 years.  Per recent policy change, public CAs will only accept CSRs for a period of 3 years or less.    {code:java}  $ openssl s_client -showcerts -connect nebula.ncsa.illinois.edu:443  CONNECTED(00000003)  depth=0 C = US, ST = Illinois, L = Urbana, O = University of Illinois, OU = National Center for Supercomputing Applications, CN = nebula.ncsa.illinois.edu, emailAddress = bmather@ncsa.illinois.edu  verify error:num=18:self signed certificate  verify return:1  depth=0 C = US, ST = Illinois, L = Urbana, O = University of Illinois, OU = National Center for Supercomputing Applications, CN = nebula.ncsa.illinois.edu, emailAddress = bmather@ncsa.illinois.edu  verify return:1  ---  Certificate chain   0 s:/C=US/ST=Illinois/L=Urbana/O=University of Illinois/OU=National Center for Supercomputing Applications/CN=nebula.ncsa.illinois.edu/emailAddress=bmather@ncsa.illinois.edu     i:/C=US/ST=Illinois/L=Urbana/O=University of Illinois/OU=National Center for Supercomputing Applications/CN=nebula.ncsa.illinois.edu/emailAddress=bmather@ncsa.illinois.edu  -----BEGIN CERTIFICATE-----  MIIEhzCCA2+gAwIBAgIJANo5p6oSmiJOMA0GCSqGSIb3DQEBCwUAMIHZMQswCQYD  VQQGEwJVUzERMA8GA1UECAwISWxsaW5vaXMxDzANBgNVBAcMBlVyYmFuYTEfMB0G  A1UECgwWVW5pdmVyc2l0eSBvZiBJbGxpbm9pczE4MDYGA1UECwwvTmF0aW9uYWwg  Q2VudGVyIGZvciBTdXBlcmNvbXB1dGluZyBBcHBsaWNhdGlvbnMxITAfBgNVBAMM  GG5lYnVsYS5uY3NhLmlsbGlub2lzLmVkdTEoMCYGCSqGSIb3DQEJARYZYm1hdGhl  ckBuY3NhLmlsbGlub2lzLmVkdTAeFw0xNTA3MTUxMjMzMjFaFw0yMDA3MTMxMjMz  MjFaMIHZMQswCQYDVQQGEwJVUzERMA8GA1UECAwISWxsaW5vaXMxDzANBgNVBAcM  BlVyYmFuYTEfMB0GA1UECgwWVW5pdmVyc2l0eSBvZiBJbGxpbm9pczE4MDYGA1UE  CwwvTmF0aW9uYWwgQ2VudGVyIGZvciBTdXBlcmNvbXB1dGluZyBBcHBsaWNhdGlv  bnMxITAfBgNVBAMMGG5lYnVsYS5uY3NhLmlsbGlub2lzLmVkdTEoMCYGCSqGSIb3  DQEJARYZYm1hdGhlckBuY3NhLmlsbGlub2lzLmVkdTCCASIwDQYJKoZIhvcNAQEB  BQADggEPADCCAQoCggEBAMKx61A4RpXv2VSBnoNWn8ff/mmnQIkwcg0y3o0YPhWn  +410pkcf0/8OGRHMEm5IAZvj4Xaj/MnDaHJpHj9ljLdDjGPEQpMJW6NQjBapI08V  HVn91I6tnOakvkBtBbv8auW8/Th+05hqhjGpiUCppIZxUB0Utpyh/P1HabCSquTH  wWoAs9t0SjvfXtVDqFITjbEzjfOIcX79CTr3+I2QPeqkoBLhVGkBdJ2aMOwk8jeI  VAYpx4Mcak3flTe6/3PzIoMjYckrS53k3pSD5GIwUlJl1TEh0+QBVc8tyzgO5icp  1D0yDyyYx9VMPR/KnoiXdooK7r4ay15G2o1eitleKJ0CAwEAAaNQME4wHQYDVR0O  BBYEFJSUX5FWniad5rKia1nrhFLtSWMHMB8GA1UdIwQYMBaAFJSUX5FWniad5rKi  a1nrhFLtSWMHMAwGA1UdEwQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAKW2MYBw  BqrgGU/PK+LP2cbtyJv9J76TI16z9K3dc3qrGcfROnfLk5NsIPG9ZaXz/OWdpkan  5keZGyAQ/B0HzWKqE8n3TZPQsCV14H+fWK/9N1yG3ad7P9Tlmt70hgr9b9wq/M1v  YJpmdEgES45ZenMqn19WpJVTwBY8C8ERgg/KRh9FEm2BynDMp1r0sb5SvBCXiI+J  enagJb/vDwJc4hkRE1+A82vdCfDFwliQTJNe3yq9ymcWFT+3b0EEgHAjPLtL1F6p  k8KtchtAaH0oIDxIhNPyHz3rseeqTHITw5gNRFc5s7xDTj49RyJlKTcEJilkCi5J  jcf4KMOgpWdpzNc=  -----END CERTIFICATE-----  ---  Server certificate  subject=/C=US/ST=Illinois/L=Urbana/O=University of Illinois/OU=National Center for Supercomputing Applications/CN=nebula.ncsa.illinois.edu/emailAddress=bmather@ncsa.illinois.edu  issuer=/C=US/ST=Illinois/L=Urbana/O=University of Illinois/OU=National Center for Supercomputing Applications/CN=nebula.ncsa.illinois.edu/emailAddress=bmather@ncsa.illinois.edu  ---  No client certificate CA names sent  Server Temp Key: ECDH, prime256v1, 256 bits  ---  SSL handshake has read 1854 bytes and written 331 bytes  ---  New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-GCM-SHA256  Server public key is 2048 bit  Secure Renegotiation IS supported  Compression: NONE  Expansion: NONE  SSL-Session:      Protocol  : TLSv1.2      Cipher    : ECDHE-RSA-AES128-GCM-SHA256      Session-ID: 6A41C5413E48D4A3B3F36EFE01F674DD77D5E29DE0828CD7DAB4E32D3A29D83E      Session-ID-ctx:       Master-Key: B70254484C41418995F201421F3821CA48E97B82E7CB21A20132F3853166EF79D5162E60B4B17DDF1568E80A3820ADC7      Key-Arg   : None      Krb5 Principal: None      PSK identity: None      PSK identity hint: None      TLS session ticket lifetime hint: 300 (seconds)      TLS session ticket:      0000 - 40 b6 7d 9e 9d 09 f7 b4-8d 56 39 77 f8 91 66 00   @.}......V9w..f.      0010 - ee 10 a8 dd d4 47 42 26-8f a9 1b 94 db 60 c4 30   .....GB&.....`.0      0020 - aa 73 b4 4c 0f af a2 63-f6 6e 3e fd 71 62 20 c4   .s.L...c.n>.qb .      0030 - b2 e8 5b b2 6a 94 b2 32-6a 28 e1 e3 df 4b 04 82   ..[.j..2j(...K..      0040 - 5d 2a c4 42 90 1f bc c7-1a b1 89 74 6d bf 7d 3d   ]*.B.......tm.}=      0050 - 3a 27 20 f9 eb 6c 00 be-e5 db f2 33 41 e8 f6 28   :' ..l.....3A..(      0060 - 07 a7 28 ad 05 cf cf 51-c0 40 6f b4 d4 04 ce 12   ..(....Q.@o.....      0070 - 3c 46 36 c4 85 bd 5a 23-92 9f 7b 2c 71 94 4e 0f   <F6...Z#..{,q.N.      0080 - 76 f8 e8 e0 f8 49 45 6a-59 a1 f2 3a be 40 3c d4   v....IEjY..:.@<.      0090 - 26 9e 82 ae 2f 16 44 5f-5f 4c b6 24 7b a7 8d 0c   &.../.D__L.${...      00a0 - 2c 03 ae b8 ba a0 6b e0-10 9d 8a b1 64 b0 4f 21   ,.....k.....d.O!      00b0 - 3e 37 58 31 9b fc 55 2e-22 65 88 8c 58 c4 90 76   >7X1..U.""e..X..v        Start Time: 1438035605      Timeout   : 300 (sec)      Verify return code: 18 (self signed certificate)  ---  DONE    {code}  ",NULL
DM-3225,"unable to access instances via ssh","I've tried a number of different security acls, including wide open to everything, and uploading my own known pristine version of cirros and I am unable to connect to either the public centos 7 and cirrus images and the cirros image I uploaded.  Two way tcp communication is definitely functioning, the issue seems to be with the instance.  Perhaps there's an issue with reverse DNS resolution or something more exotic such as the kernel not having enough entropy available to generate the session keys before ssh times out.  I have tried changing the ssh client ConnectTimeout without any change in behavior.    {code:java}  $ ssh 141.142.200.94 -l cirros -vvv  OpenSSH_6.9p1, OpenSSL 1.0.1k-fips 8 Jan 2015  debug1: Reading configuration data /home/jhoblitt/.ssh/config  debug1: Reading configuration data /etc/ssh/ssh_config  debug1: /etc/ssh/ssh_config line 3: Applying options for *  debug1: Control socket ""/home/jhoblitt/.ssh/master/cirros@141.142.200.94:22"" does not exist  debug2: ssh_connect: needpriv 0  debug1: Connecting to 141.142.200.94 [141.142.200.94] port 22.  debug2: fd 3 setting O_NONBLOCK  debug1: fd 3 clearing O_NONBLOCK  debug1: Connection established.  debug3: timeout: 7199947 ms remain after connect  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_rsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_rsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_dsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_dsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_ecdsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_ecdsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_ed25519 type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_ed25519-cert type -1  debug1: Enabling compatibility mode for protocol 2.0  debug1: Local version string SSH-2.0-OpenSSH_6.9  debug1: Remote protocol version 2.0, remote software version dropbear_2012.55  debug1: no match: dropbear_2012.55  debug2: fd 3 setting O_NONBLOCK  debug1: Authenticating to 141.142.200.94:22 as 'cirros'  debug3: hostkeys_foreach: reading file ""/home/jhoblitt/.ssh/known_hosts""  debug3: hostkeys_foreach: reading file ""/etc/ssh/ssh_known_hosts""  debug1: SSH2_MSG_KEXINIT sent  debug1: SSH2_MSG_KEXINIT received  debug2: kex_parse_kexinit: curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1,diffie-hellman-group1-sha1  debug2: kex_parse_kexinit: ecdsa-sha2-nistp256-cert-v01@openssh.com,ecdsa-sha2-nistp384-cert-v01@openssh.com,ecdsa-sha2-nistp521-cert-v01@openssh.com,ssh-ed25519-cert-v01@openssh.com,ssh-rsa-cert-v01@openssh.com,ssh-dss-cert-v01@openssh.com,ssh-rsa-cert-v00@openssh.com,ssh-dss-cert-v00@openssh.com,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519,ssh-rsa,ssh-dss  debug2: kex_parse_kexinit: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,arcfour256,arcfour128,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,rijndael-cbc@lysator.liu.se  debug2: kex_parse_kexinit: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,arcfour256,arcfour128,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,rijndael-cbc@lysator.liu.se  debug2: kex_parse_kexinit: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1,hmac-md5-etm@openssh.com,hmac-ripemd160-etm@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-96-etm@openssh.com,hmac-md5,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96  debug2: kex_parse_kexinit: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1,hmac-md5-etm@openssh.com,hmac-ripemd160-etm@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-96-etm@openssh.com,hmac-md5,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96  debug2: kex_parse_kexinit: none,zlib@openssh.com,zlib  debug2: kex_parse_kexinit: none,zlib@openssh.com,zlib  debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit: first_kex_follows 0   debug2: kex_parse_kexinit: reserved 0   debug2: kex_parse_kexinit: diffie-hellman-group1-sha1,diffie-hellman-group14-sha1  debug2: kex_parse_kexinit: ssh-rsa,ssh-dss  debug2: kex_parse_kexinit: aes128-ctr,3des-ctr,aes256-ctr,aes128-cbc,3des-cbc,aes256-cbc,twofish256-cbc,twofish-cbc,twofish128-cbc  debug2: kex_parse_kexinit: aes128-ctr,3des-ctr,aes256-ctr,aes128-cbc,3des-cbc,aes256-cbc,twofish256-cbc,twofish-cbc,twofish128-cbc  debug2: kex_parse_kexinit: hmac-sha1-96,hmac-sha1,hmac-md5  debug2: kex_parse_kexinit: hmac-sha1-96,hmac-sha1,hmac-md5  debug2: kex_parse_kexinit: none  debug2: kex_parse_kexinit: none  debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit: first_kex_follows 0   debug2: kex_parse_kexinit: reserved 0   debug1: kex: server->client aes128-ctr hmac-sha1 none  debug1: kex: client->server aes128-ctr hmac-sha1 none  debug1: kex: diffie-hellman-group14-sha1 need=20 dh_need=20  debug1: kex: diffie-hellman-group14-sha1 need=20 dh_need=20  debug1: sending SSH2_MSG_KEXDH_INIT  debug2: bits set: 1017/2048  debug1: expecting SSH2_MSG_KEXDH_REPLY        Connection to 141.142.200.94 timed out  {code}    ",NULL
DM-3226,"horizon API access tab shows URLs without FQDNs","The service endpoint URLs should be disabled with a FQDN.     {code:java}  http://nebula.ncsa.illinois.edu:9292  {code}   instead of  {code:java}   http://nebula:9292  {code}   ",NULL
DM-3229,"Remove Reply-To from RFC emails","The email that is sent to {{dm-devel}} when an RFC is filed includes a {{Reply-To}} header which means that if people reply to the email the response is lost to JIRA. After discussions with [~ktl] and [~gpdf] we came to the conclusion that it is best if RFC discussions are associated with the JIRA ticket (given how often we need to go back and refer to a discussion) and that the {{Reply-To}} header leads to possible confusion. If we remove that header responses will at least end up in JIRA.    We may want to consider removing the ""where to discuss"" option from the RFC system but that's for another debate.",NULL
DM-3231,"use pkg-config to link against thirdparty dependencies","A number of the third-party dependencies provide a .pc file which is installed as part of the standard eupspkg build.  While not all of these are linked against directly (some are deps of deps), this appears to be reasonable place to start an incremental conversion from sconsUtils .cfg -> pkg-config.    .pc files which are already present:    {code:java}  ./Linux64/cfitsio/3360.lsst1/lib/pkgconfig/cfitsio.pc  ./Linux64/wcslib/4.14+7/lib/pkgconfig/wcslib.pc  ./Linux64/apr/1.3.3.lsst2/lib/pkgconfig/apr-1.pc  ./Linux64/libevent/2.0.16-stable-1-g7e53c5e/lib/pkgconfig/libevent.pc  ./Linux64/libevent/2.0.16-stable-1-g7e53c5e/lib/pkgconfig/libevent_pthreads.pc  ./Linux64/libevent/2.0.16-stable-1-g7e53c5e/lib/pkgconfig/libevent_openssl.pc  ./Linux64/fftw/3.3.3/lib/pkgconfig/fftw3.pc  ./Linux64/anaconda/2.2.0/lib/pkgconfig/cairo-fc.pc  ...  ./Linux64/anaconda/2.2.0/lib/pkgconfig/zlib.pc  ./Linux64/gsl/1.16.lsst1/lib/pkgconfig/gsl.pc  ./Linux64/apr_util/1.3.4.lsst2/lib/pkgconfig/apr-util-1.pc  ./Linux64/mysqlproxy/0.8.2+7/lib/pkgconfig/mysql-proxy.pc  ./Linux64/mysqlproxy/0.8.2+7/lib/pkgconfig/mysql-chassis.pc  ./Linux64/log4cxx/0.10.0.lsst3/lib/pkgconfig/liblog4cxx.pc  ./Linux64/protobuf/2.4.1.lsst1+3/lib/pkgconfig/protobuf.pc  ./Linux64/protobuf/2.4.1.lsst1+3/lib/pkgconfig/protobuf-lite.pc  ./Linux64/activemqcpp/10.1/lib/pkgconfig/activemq-cpp.pc  {code}    The `pkgconfig` package will need to be added to the system deps list.  It should be generally present on any linux distro.  We should double check that this is also the case with homebrew.    `pkg-config` uses `$PKG_CONFIG_PATH to search for .pc files.  A snippet similar to the following will need to be added to the .table files for eups products which provide a .pc.  {code:java}  envPrepend(PKG_CONFIG_PATH, ${PRODUCT_DIR}/lib/pkgconfig)  {code}    scons has some minimal support for parsing the output from pkg-config but compile flag merging will need to be tested. E.g.    https://bitbucket.org/scons/scons/wiki/UsingPkgConfig  ",NULL
DM-3235,"qserv missing direct dependencies","qserv currently uses log4cxx, lua, and mysql directly but does not declare them in its .table file.",NULL
DM-3236,"Deleting a remote branch is not noticed by Jenkins","In working on DM-3182 I created a branch tickets/DM-3182 of meas_deblender. I then made further changes to the main code that eliminated the need for changes to meas_deblender so I deleted its remote branch tickets/DM-3182 using:  {code}  localhost$ git push origin --delete tickets/DM-3182  To git@github.com:lsst/meas_deblender.git   - [deleted]         tickets/DM-3182  {code}  I then ran Jenkins again and it still found that branch of meas_deblender instead of using master. This may be user error on my part or a quirk of lsstsw. It's not a big deal -- I worked around it using ""git revert"" to undo the changes on that ticket branch (thereby recreating the remote branch, of course). But if it is actually a bug in lsstsw then I'd like to at least report it so we have a record of it in case anyone else stumbles across it.",NULL
DM-3238,"Add qserv-restart.sh","I am tired of running {qserv_run_dir}/bin/qserv-stop.sh followed by {qserv_run_dir}/bin/qserv-start.sh, and then {qserv_run_dir}/bin/qserv-status.sh, I'm going to create a trivial script that does it all in one shot.",NULL
DM-328,"mysql-proxy fails to stop","One thing Andy Salnikov noticed when he wanted to stop everything after running tests:  $ qserv-stop.sh Stopping qserv-master:                                     [  OK  ] Stopping mysql-proxy:                                      [FAILED] Stopping qms:                                              [  OK  ] Stopping xrootd:                                           [  OK  ] Shutting down MySQL.                                       [  OK  ]  Apparently mysql-proxy fails to stop. Andy checked around and see that var/run/mysql-proxy.pid file is empty, apparently init script did not write mysql-proxy PID there when it started proxy. Andy have to kill it by hand now.  Bill and Fabrice observed the same problem from time to time.",NULL
DM-329,"No way to retrieve an HDU from an MEF with the butler","Specifying a template location for an {{lsst.afw.image.Image}} including a cfitsio bracketed extension ""\[\{hdu}]"" fails (because the extension is stripped off in {{Butler.datasetExists()}} but nowhere else, in particular not in {{CameraMapper._parentSearch}} where an {{os.path.exists()}} can never succeed for such locations).  There is code in {{lsst::daf::persistence::FitsStorage}} to use a location ending in ""#\{hdu}"", but that has all the same problems.  A better solution would seem to be to keep the path location just a pathname and pass the HDU or other information needed for cfitsio in the {{additionalData}}, with {{FitsStorage}} adding the bracket expression (or in this case not even doing that, just setting {{_hdu}} for {{getHdu()}} which is used in {{lsst::afw::formatters::ImageFormatter}}).",NULL
DM-3324,"KPM Measurement: Photometric repeatability: PA1uzy, FY15","This will be measured in the same way as described on DM-3338.",NULL
DM-3331,"KPM Measurement: Photometric repeatability: PA1gri, FY15","This will be measured in the same way as described on DM-3338.",NULL
DM-3338,"KPM Measurement: Photometric Repeatability: procCalRep, FY15","This will be measured on precursor data (almost certainly from HSC, via DM-2380) using the following procedure:   - Run visit-level processing (i.e. {{ProcessCcdTask}}, probably via the new HSC driver ported on DM-3368).  This must include aperture correction (DM-85).   - Run relative photometric calibration (i.e. {{meas_mosaic}}, DM-2674).   - Select bright stars and match across visits (new scripts, mostly delegating to existing code).  Just selecting the stars used for PSF determination would probably work.   - Plot magnitude deltas (difference from per-object mean magnitudes) vs. magnitude; measure RMS and outliers in magnitude bins.    Given the limited amount of public HSC data available, this will probably be restricted to at most three bands (gri).    This measurement will be the same as that for PA1uzy (DLP-316), and PA1gri (DLP-315) in this cycle (DM-3324, DM-3331), since we currently do not have a way of differentiating between pipeline-contributed errors and the complete error budget.",NULL
DM-3350,"Possible problem with attribution when replying to JIRA email","In DM-3159 two comments are attributed to Robyn Allsman yet it seems clear that Robyn did not author those comments (especially given one of them is signed ""Andy""). It seems that both these comments were submitted by replying to a JIRA email. Is it possible that some defaulting happens if the JIRA user can not be inferred from the email message?",NULL
DM-3360,"Minor cleanup related to max_hostname","Check return value from  {{gethostname()}}.    Cleanup [this|https://github.com/lsst/qserv/blob/master/core/modules/wdb/QueryAction.cc#L67].",NULL
DM-3361,"Fix czar crash and improve error message for queries that select ""dec""","""dec"" is a reserved word in mysql, so queries like ""SELECT dec FROM t"" will never work, mysql will return syntax error, and not ""unknown column""    {quote}  mysql> select dec from t;  ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'dec from t' at line 1  {quote}    {quote}  select decll from t;  ERROR 1054 (42S22): Unknown column 'decll' in 'field list'  {quote}    Qserv is having troubles in parser, and the error is:    {quote}  ERROR ccontrol.UserQueryFactory (build/ccontrol/UserQueryFactory.cc:109) - Invalid query: ParseException:Parse error(ANTLR):unexpected token: dec:  {quote}    It'd be nice to improve the error message, and fix the problem because it currently crashes czar!  ",NULL
DM-3379,"LSST All-hands Talk","Unsure if I am giving this talk.  If so, it will have to be remotely.",NULL
DM-3389,"LDM-151 has out of date WBS numbers and URLs","LDM-151 has outdated numbers for calibration pipelines: specifically the Photometric and Astrometric calibration pipelines are not in 02C.03 and the Calibration Products Pipeline is associated with 02C.01.02.04 and not 02C.04.02.    Additionally some URLs are broken because they do not take into account the move to Github.",NULL
DM-3395,"Add W14, S14 options to ""Fix Versions"" field in DLP project","[~jkantor] requests that we ""capture past milestones that are completed (S14, W15)"" in the JIRA DLP project ([source|https://confluence.lsstcorp.org/display/DM/DM+Leadership+Team+Meeting+2015-08-03]). Currently, though, the earliest ""Fix Version"" we can set for a milestone in DLP is W15. Can you please enable W/S14 as well?    (I'm actually not sure if this should go on the IT issue tracker rather than DM; please correct me if so.) ",NULL
DM-3402,"Fix exception handling in meas_deblender","This got pulled over in DM-1944 as a cherry-pick from [HSC-1126|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1126].  The error checking needs to be updated due to behavioral changes to exceptions on the LSST side.",NULL
DM-3403,"Fix parallel build in generating defects","Building obs_subaru can fail with errors like the following:  {code}  'import sitecustomize' failed; use -v for traceback  'import sitecustomize' failed; use -v for traceback  'import sitecustomize' failed; use -v for traceback  Traceback (most recent call last):    File ""bin/genDefectFits.py"", line 12, in <module>  Traceback (most recent call last):    File ""bin/genDefectFits.py"", line 12, in <module>      from lsst.obs.hsc import HscMapper    File ""/nfs/home/price/LSST/obs/subaru/python/lsst/obs/hsc/__init__.py"", line 2, in <module>  Traceback (most recent call last):    File ""bin/genDefectFits.py"", line 12, in <module>      from lsst.obs.hsc import HscMapper    File ""/nfs/home/price/LSST/obs/subaru/python/lsst/obs/hsc/__init__.py"", line 2, in <module>      from lsst.obs.hsc import HscMapper    File ""/nfs/home/price/LSST/obs/subaru/python/lsst/obs/hsc/__init__.py"", line 2, in <module>          from .transformRegistry import *  from .transformRegistry import *    File ""/nfs/home/price/LSST/obs/subaru/python/lsst/obs/hsc/transformRegistry.py"", line 3, in <module>    File ""/nfs/home/price/LSST/obs/subaru/python/lsst/obs/hsc/transformRegistry.py"", line 3, in <module>      from .transformRegistry import *    File ""/nfs/home/price/LSST/obs/subaru/python/lsst/obs/hsc/transformRegistry.py"", line 3, in <module>      from lsst.obs.subaru.subaruLib import HscDistortion, DistortionPolynomial  ImportError: No module named subaruLib      from lsst.obs.subaru.subaruLib import HscDistortion, DistortionPolynomial      from lsst.obs.subaru.subaruLib import HscDistortion, DistortionPolynomial  ImportError: No module named subaruLib  ImportError: No module named subaruLib  scons: *** [hsc/defects/2013-01-31/defects.dat-fits] Error 1  scons: *** [hsc/defects/2014-06-01/defects.dat-fits] Error 1  scons: *** [hsc/defects/2014-04-03/defects.dat-fits] Error 1  scons: building terminated because of errors.  {code}    This is due to a missing dependency for building the defects FITS files.    This is a pull request for a simple fix.",NULL
DM-3405,"lsstsw does not deploy git ","lsstsw failed to deploy git on Mac 10.10.4   with GCC 5.1.0",NULL
DM-3406,"Consolidate display code in astrometry","Prompted by DM-3209:    The {{ANetAstrometryTask}} uses 3 different means for debugging displays:  * {{lsst.pipe.base.Task.display}}, which is an old old mechanism for displaying debugging info (I think it was in Pipette before we converted that to pipe_base + pipe_tasks).  It's not used much (either in code or activated), and its capability is pretty much superseded by what's in {{lsst.meas.astrom.display}}, except that it integrates the use of {{lsstDebug}}.  * {{lsst.meas.astrom.anetAstrometry.showAstrometry}}, which appears to be intended for use in WCS fitting loops, since it calculates fit statistics.  * {{lsst.meas.astrom.display.displayAstrometry}}, which is the most complete display code, though it's clearly lacking some features (e.g., fit statistics provided by {{showAstrometry}}, integration with {{lsstDebug}} or some other facility for activation).    It might be helpful to consolidate these.  The existence of multiple ways to do things likely indicates that we haven't yet found the best way.  I suggest that we need some basic functions to display source catalogs and match lists in afw (more basic than {{displayAstrometry}} but building on the display code in afw because I don't want to have to write multiple lines to display a source catalog).  Astrometry is likely a useful area to consider when designing display tools for debugging.",NULL
DM-3407,"Rename hipchat rooms","Requirement: hide the test room better.     Room currently labeled 'Bot: Jenkins Demo': rename to 'Bot: Jenkins CI' [or Bot: SQuaRE CI if you want to throw any future Travis messages into the same room?]    Room currently labeled 'Bot: Jenkins Demo Test': rename to 'Bot: sqre-test' so we can use it for multiple end tests. ",NULL
DM-3408,"Discussion / chat systems: auto-links to LSST documents and change requests","In HipChat, the appearance of a string that can be recognized as an issue in the linked JIRA system (e.g., ) triggers the automatic posting of additional information on that issue into the current room, including a link to the issue.    While occasionally annoying, on the whole this seems to be a useful feature, and many of us have discussed a desire to extend this behavior to Docushare documents and LSST change requests - i.e., to have a mention of a document (e.g., ""LSE-30"") trigger a followup posting/link with the title, and perhaps additional information such as the current preferred version and its date, and (if different) latest version and its date.    Similarly, it would be nice to have a mention of ""LCR-xxx"" trigger a followup with the status of the corresponding change request.    This would substantially facilitate conversation about traceability to documents and would allow newcomers to more easily follow such conversations among experts.    This behavior would be useful in HipChat/slack as well as Discourse.      In Confluence it would also be useful, but most likely have to be done via macro (by analogy to the current JIRA issue macro); however, it has been rumored that Atlassian does not facilitate the writing of such macros for third-party systems.",NULL
DM-3413,"Custom sort order on ""Fix Version"" field does not work correctly for the S21 and W21 values","The S21 and W21 values of Fix Version appear to sort ahead of S15 and W15.",NULL
DM-3420,"Fix problem with JOIN and LIMIT","{quote}  SELECT o.deepSourceId FROM Object o, Source s WHERE qserv_areaspec_box(0.7, 1.5, 0.71, 1.51) and o.deepSourceId = s.objectId;  +------------------+  | deepSourceId     |  +------------------+  | 4363051117580101 |  | 4363051117580101 |  | 4363042527641676 |  | 4363042527641647 |  | 4363042527641676 |  | 4363042527641647 |  | 4363042527641676 |  | 4363042527641693 |  | 4363042527641693 |  | 4363042527641693 |  | 4363051117580100 |  | 4363051117580100 |  | 4363051117580100 |  | 4363051117580095 |  | 4363051117580101 |  | 4363042527641647 |  | 4363042527641693 |  | 4363042527641693 |  | 4363042527641647 |  | 4363042527641676 |  | 4363042527641647 |  +------------------+  21 rows in set (2.88 sec)    SELECT o.deepSourceId FROM Object o, Source s WHERE qserv_areaspec_box(0.7, 1.5, 0.71, 1.51) and o.deepSourceId = s.objectId limit 10;  ERROR 4120 (Proxy): Error executing query using qserv.  {quote}",NULL
DM-3423,"cameraGeom.utils not updated for Robert's new display mechanism","The camera display tools are broken since the update by Robert to the new mechanism for switching different displays in and out.  A particular example is that in the showCcd method, if the None-defaulted keyword argument 'display' is not overridden, there is a 'NoneType has not attribute mtv' exception.",NULL
DM-3424,"Calming down the JIRA issue macro in HipChat (and any other future such macros)","There is a device in place that pastes a small status report on a JIRA issue into a HipChat room after it detects a reference to an issue.  This is quite useful.  However...  When an issue is repeatedly mentioned, it becomes most annoying.  It would be nice to implement some level of chatter reduction for this macro.  The most useful thing, I think, would be if further references to the issue were ignored until at least X subsequent messages of any kind had been posted to the relevant room.  Depending on how the macros work, that could be very difficult to implement (the macro might have no way of knowing that), so a fallback of a delay of Y minutes might get close to the same behavior.   (Conceivably a change to the information displayed by the macro could also break the suppression, but I think that would be overkill.)",NULL
DM-3430,"Always allow declaring and undeclaring a package","It is possible for eups to get into a state whereby it is impossible to declare, undeclare or setup a package. I'm not sure how I got into this situation, but I think one can reproduce the problem by taking a properly declared package and breaking it by deleting its information in ups_db, e.g. the file ups_db/<package>/<version>.version.    In this situation eups will refuse to declare, undeclare or setup the package and the error messages explaining what is wrong are too terse, making it difficult to diagnose and fix the problem. Fortunately at least one of commands does produce a message mentioning ups_db.    It would be ideal if both ""eups declare"" and ""eups undeclare"" worked in this situation, e.g. if they were robust enough to manage incomplete existing information. If that was the case then the current message produced by ""setup"" would probably suffice to suggest a solution, though a bit more information would be nice.  ",NULL
DM-3431,"Improve database unit tests in the absence of a running server","There is a number of packages (daf_ingest, obs_sdss, obs_lsstSim, daf_persistence) that contains unit tests which communicate with a database server. When the usual database server at NCSA is not up, or not reachable, the tests are skipped.    It should be possible to do something more useful via mocking. For example, it may be possible to record and serialize a passing unit test's database interactions with a live instance, and to play back those interactions via mocks of the DBAPI connection and cursor classes when no live database is available.",NULL
DM-3441,"Clean up and document PSFEx","Technical Debt amassed by porting HSC code to LSST necessitates that PSFEx and meas_extensions_psfex be cleaned up a bit. This will involve things such as converting variable names to camel case everywhere and writing markup compatible document strings. Additionally the README of the file needs to be updated.",NULL
DM-3443,"Some centroid errors are missing in source catalog","SdssCentroid_xySigma is missing and there is no error associated to sdssShape_x / sdssShape_y",NULL
DM-3445,"Overhaul the way the right colorterm dictionary is selected","There are some configuration issues associated with reference catalogs, including:    1) Determining the source of the reference catalog. This information is used in two ways:  - mapping the filter used to take an exposure to a filter in a reference catalog, for matching sources to reference objects (e.g. for astrometry and photometry)  - picking the correct set of colorterms based on the filter and type of CCD used for the science image and filters in the reference catalog  At present this information is encoded in the eups version name of the astrometry_net_data package used. This requires the use of eups and assumes astrometry.net index files. This is the most serious immediate issue.    2) Colorterm correction data is saved in obs_* packages as config files. This makes it harder to include a temporal component.    This ticket is a request for a cleaner solution.    The plan has been to wait for ""the new butler"", e.g. DM-2404 (taking this use case into account in its design) before designing the solution. However, if we need something sooner, one possibility for fixing the first problem is to put the source of the reference catalog into metadata attached to the reference catalog.",NULL
DM-3446,"Make JIRA labels visible in RFC project","The current configuration of the RFC project does not allow labels to be displayed in issue views.  It is possible to set labels, but not see them outside the specific ""edit labels"" dialog.    I'd like to use labels to help organize the issues to be addressed at AHM2015.  See the label I placed on RFC-84 as a test.    Can this property of the RFC project be changed?",NULL
DM-3454,"Odd error message in getDistortedWcs","lsst.afw.image.utils.getDistortedWcs complains as follows if the provided exposure has no WCS:  ""exposure must have a WCS to use as an initial guess"". It should not say anything about an initial guess. This is presumably a leftover from when the code was part of meas_astrom.    Thanks to [~price] for pointing this out.",NULL
DM-346,"Can't install sconsUtils with eupspkg","DM-58 require packaging of db library, which relies on sconsUtils.  That's why i try to install sconsUtils with next command, but it fails :  {code:shell} fjammes@clrlsst-dbmaster-vm:~/src/qserv (tickets/DM-58) $ eups distrib install sconsUtils 6.2.0.0_19_g755151c0a5+9ff2ee7baa --repository=""http://lsst-web.ncsa.illinois.edu/~mjuric/pkgs""   [  1/4  ]  doxygen 1.8.2                                              done.    [  2/4  ]  python system (already installed)                          done.    [  3/4  ]  scons system (already installed)                           done.    [  4/4  ]  sconsUtils 6.2.0.0_19_g755151c0a5+9ff2ee7baa ...  ***** error: from /opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/build.log: python                system     	current setup scons                 system     	current setup + ./ups/eupspkg VERBOSE=0 FLAVOR=Linux64 fetch + ./ups/eupspkg VERBOSE=0 FLAVOR=Linux64 prep + setup --type=build -k -r . ++ /opt/qserv/eups/bin/eups_setup --type=build -k -r . + eval export 'PATH=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/bin:/opt/qserv/stack/Linux64/doxygen/1.8.2/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/fjammes/go/bin:/home/fjammes/bin:/opt/qserv/eups/bin;' export 'PYTHONPATH=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/python:/opt/qserv/eups/python;' export 'SETUP_SCONSUTILS='\''sconsUtils' LOCAL:/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa -f Linux64 -Z '(none)'\'';' export SCONSUTILS_DIR=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa ++ export PATH=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/bin:/opt/qserv/stack/Linux64/doxygen/1.8.2/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/fjammes/go/bin:/home/fjammes/bin:/opt/qserv/eups/bin ++ PATH=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/bin:/opt/qserv/stack/Linux64/doxygen/1.8.2/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/fjammes/go/bin:/home/fjammes/bin:/opt/qserv/eups/bin ++ export PYTHONPATH=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/python:/opt/qserv/eups/python ++ PYTHONPATH=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/python:/opt/qserv/eups/python ++ export 'SETUP_SCONSUTILS=sconsUtils LOCAL:/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa -f Linux64 -Z (none)' ++ SETUP_SCONSUTILS='sconsUtils LOCAL:/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa -f Linux64 -Z (none)' ++ export SCONSUTILS_DIR=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa ++ SCONSUTILS_DIR=/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa + ./ups/eupspkg VERBOSE=0 FLAVOR=Linux64 config + ./ups/eupspkg VERBOSE=0 FLAVOR=Linux64 build scons: Reading SConscript files ... SCons 2.1 or greater required, but you have SCons 2.0.1 + exit -4 eups distrib: Failed to build sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa.eupspkg: Command: 	source /opt/qserv/eups/bin/setups.sh; export EUPS_PATH=/opt/qserv/stack; (/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/build.sh) >> /opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/build.log 2>&1 4>/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_19_g755151c0a5+9ff2ee7baa/build.msg  exited with code 252 {code}  scons can't be installed, indeed it seems it eups package is outdated (please look at REPOSITORY variable in its pkginfo file below):  {code:shell} fjammes@clrlsst-dbmaster-vm:~/src/qserv (tickets/DM-58) $ cat /opt/qserv/stack/EupsBuildDir/Linux64/scons-2.1.0_3_gc22e6b1da4+12ac34fb5a/scons-2.1.0_3_gc22e6b1da4+12ac34fb5a/ups/pkginfo  PRODUCT='scons' VERSION='2.1.0_3_gc22e6b1da4+12ac34fb5a' FLAVOR='generic' SOURCE='git' SHA1='c22e6b1da407adaf1eefaa8893ec7b0524ce21aa' REPOSITORY='git://git.lsstcorp.org/contrib/eupspkg/scons' REPOVERSION='c22e6b1da407adaf1eefaa8893ec7b0524ce21aa'  {code}  Furthermore while trying to install sconsUtils  6.2.0.0_16_gc9d884655a+759c3944a1, scons succeed to install but an other error happen :  {code:shell} fjammes@clrlsst-dbmaster-vm:~/src/qserv (tickets/DM-58) $ eups distrib install sconsUtils 6.2.0.0_16_gc9d884655a+759c3944a1 --repository=""http://lsst-web.ncsa.illinois.edu/~mjuric/pkgs""    [  1/4  ]  doxygen 1.8.2 (already installed)                          done.    [  2/4  ]  python system (already installed)                          done.    [  3/4  ]  scons 2.1.0+2b5a2f1b52 (already installed)                 done.    [  4/4  ]  sconsUtils 6.2.0.0_16_gc9d884655a+759c3944a1 ...  ***** error: from /opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_16_gc9d884655a+759c3944a1/build.log: ++ export 'SETUP_PYTHON=python system -f Linux64 -Z /opt/qserv/stack' ++ SETUP_PYTHON='python system -f Linux64 -Z /opt/qserv/stack' + setup --just --type=build scons 2.1.0+2b5a2f1b52 ++ /opt/qserv/eups/bin/eups_setup --just --type=build scons 2.1.0+2b5a2f1b52 + eval export 'PATH=/opt/qserv/stack/Linux64/scons/2.1.0+2b5a2f1b52/bin:/opt/qserv/stack/Linux64/doxygen/1.8.2/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/fjammes/go/bin:/home/fjammes/bin:/opt/qserv/eups/bin;' export 'SCONS_DIR=/opt/qserv/stack/Linux64/scons/2.1.0+2b5a2f1b52;' export 'SETUP_SCONS='\''scons' 2.1.0+2b5a2f1b52 -f Linux64 -Z '/opt/qserv/stack'\'';' export SCONS_DIR_EXTRA=/opt/qserv/stack/ups_db/Linux64/scons/2.1.0+2b5a2f1b52 ++ export PATH=/opt/qserv/stack/Linux64/scons/2.1.0+2b5a2f1b52/bin:/opt/qserv/stack/Linux64/doxygen/1.8.2/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/fjammes/go/bin:/home/fjammes/bin:/opt/qserv/eups/bin ++ PATH=/opt/qserv/stack/Linux64/scons/2.1.0+2b5a2f1b52/bin:/opt/qserv/stack/Linux64/doxygen/1.8.2/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/fjammes/go/bin:/home/fjammes/bin:/opt/qserv/eups/bin ++ export SCONS_DIR=/opt/qserv/stack/Linux64/scons/2.1.0+2b5a2f1b52 ++ SCONS_DIR=/opt/qserv/stack/Linux64/scons/2.1.0+2b5a2f1b52 ++ export 'SETUP_SCONS=scons 2.1.0+2b5a2f1b52 -f Linux64 -Z /opt/qserv/stack' ++ SETUP_SCONS='scons 2.1.0+2b5a2f1b52 -f Linux64 -Z /opt/qserv/stack' ++ export SCONS_DIR_EXTRA=/opt/qserv/stack/ups_db/Linux64/scons/2.1.0+2b5a2f1b52 ++ SCONS_DIR_EXTRA=/opt/qserv/stack/ups_db/Linux64/scons/2.1.0+2b5a2f1b52 + eups list -s doxygen               1.8.2      	b63 b61 b41 b48 b137 b301 b303 b302 b304 current b58 b59 b38 b39 b34 b53 b36 b37 b30 b57 b54 b33 b142 setup python                system     	current setup scons                 2.1.0+2b5a2f1b52 	b63 current b61 b48 setup + ./ups/eupspkg VERBOSE=0 FLAVOR=Linux64 fetch fatal: reference is not a tree: c9d884655a24deb7cf4b96ee8af4753d737b1377 + exit -1 eups distrib: Failed to build sconsUtils-6.2.0.0_16_gc9d884655a+759c3944a1.eupspkg: Command: 	source /opt/qserv/eups/bin/setups.sh; export EUPS_PATH=/opt/qserv/stack; (/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_16_gc9d884655a+759c3944a1/build.sh) >> /opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_16_gc9d884655a+759c3944a1/build.log 2>&1 4>/opt/qserv/stack/EupsBuildDir/Linux64/sconsUtils-6.2.0.0_16_gc9d884655a+759c3944a1/build.msg  exited with code 255 {code}  indeed the specified commit hash doesn't seem to exists in sconsUtils git repo : {code:bash} fjammes@clrlsst-dbmaster-vm:~/tmp $ git clone git://git.lsstcorp.org/LSST/DMS/devenv/sconsUtils.git Initialized empty Git repository in /home/fjammes/tmp/sconsUtils/.git/ remote: Counting objects: 2064, done. remote: Compressing objects: 100% (1272/1272), done. remote: Total 2064 (delta 839), reused 1248 (delta 477) Receiving objects: 100% (2064/2064), 336.90 KiB | 285 KiB/s, done. Resolving deltas: 100% (839/839), done. fjammes@clrlsst-dbmaster-vm:~/tmp $ cd sconsUtils/ fjammes@clrlsst-dbmaster-vm:~/tmp/sconsUtils (master) $ git checkout c9d884655a24deb7cf4b96ee8af4753d737b137 error: pathspec 'c9d884655a24deb7cf4b96ee8af4753d737b137' did not match any file(s) known to git. {code}  Two questions here : - if there's effectively a problem in scons/sconsUtils packaging, is it please possible to fix it ? - would it be possible to define a ""current"" tag for sconsUtils (like git), in order to ease its install and not to have to specified its version in Qserv install script ?  Thanks, and have a nice day,",NULL
DM-3461,"Bug fixes for psfex","Some bugs were uncovered when trying to run psfex in conjunction with obs_subaru.",NULL
DM-3465,"tests/testCoadd.py often segfaults on MacOS","Recently I've been unable to build pipe_tasks because tests/testCoadd.py usually segfaults. This is on MacOS X 10.9.5 and [~krughoff] reports also seeing it on MacOS X 10.9 (corrected). I am using lsstsw and thus its anaconda.    I have built several times on linux without problems.    Note that this is seen with ""rebuild pipe_tasks"", which does not build or setup psfex or the related meas_extension_psfex (so it is probably not related to DM-3463)    [~jbosch] is guessing it may be related to DM-1954.",NULL
DM-3466,"Fix how loadLSST.zsh sets $LSST_HOME","newinstall.sh bootstraps a loadLSST.zsh that incorrectly sets the $LSST_HOME to be `.` rather than a full path.",NULL
DM-3473,"Add transformed source entries to HSC mapper policy","Need to add entries for    * {{transformed_src}}  * {{transformed_src_schema}}  * {{transformSrcMeasurement_config}}  * {{transformSrcMeasurement_metadata}}    to {{HscMapper.paf}} to enable the processing of HSC data through the measurement transformation framework.",NULL
DM-3476,"calcmom in SdssShape.cc has poor error reporting","calcmom does much of the work in SdssShape.cc, but it returns -1 if it fails for any reason. Please enhance it to raise an exception with an informative error message. That would also simplify the code that calls it, and make that code more robust.    Other code cleanups are also worth considering, since calcmon is C code with a n old C-style interface.",NULL
DM-3498,"Add support for caching results from queries","Currently, when query is done and results returned to user, the results are deleted. In the initial version of async queries we will do the same. In the long term, it'd be good to cache the results, especially for expensive queries,",NULL
DM-350,"cameraGeom.Camera.transform bugs","Camera.transform uses undefined variable cameraSys in an error message.  Also, transform calls findDetectors and findDetectors calls transform. Reading the code it is not clear to me how an infinite loop is avoided; some documentation on this would help. It might be cleaner to have an internal version of transform that did not call findDetectors, and have findDetectors and transform both call that as needed.  Also I suggest that findDetectors check that the coordinate system exists and report the error separately than if not exactly one detector is found.  (Also, CameraSys is imported but not used and a few lines are too long.)",NULL
DM-351,"CameraPoint should support stream << in C++ and __repr__ in Python","{{CameraPoint}} should support {{stream <<}} in C++ and {{\_\_repr\_\_}} in Python",NULL
DM-3510,"JIRA keeps showing All when viewing tickets; please change the default to Comments","Most times when I view a ticket I find the list of updates in All mode, which shows a lot of things I don't care about. Is there some way to get it to always default to Comments?",NULL
DM-3517,"unrepeatable build failure in base","The attached bug report is from a unit test failure that occurred in base when trying to build the sims packages.  The failure did not happen to me.  It happened once to Seth Digel, and once to Michael Reuter.  In both cases, simply restarting the build ""resolved"" the issue (i.e. the build completed on the second attempt with no changes made to the code).",NULL
DM-352,"regex bug in afw::table FITS reader","A regular expression bug in afw::table FITS reading gets in the way of loading FITS binary tables with columns that do not include the optional ""repeat count"" in the TFORM keyword.  In other words, we can't read fields that have FITS headers with e.g.: {code} TFORM: ""K       "" {code} instead of  {code} TFORM: ""1K      "" {code}  CFITSIO apparently always does the latter, which is why we've never seen this bug before, but both are valid FITS.",NULL
DM-353,"Stack installation can leave packages not tagged current","I installed a clean Winter2014 on my Mac in a new directory using the standard procedure (from a fresh login; my old stack was NOT setup): {code} mkdir lsst_home2 cd lsst_home2 curl -O http://sw.lsstcorp.org/eupspkg/newinstall.sh bash newinstall.sh {code} It completed without errors, but when I tried to setup some packages it complained that it could not find suitable versions. It turned out that many packages in my new stack were missing the ""current"" tag. Those packages all had one thing in common: I had a git version checked out and declared in my old stack using version name ""git"" and tag ""rowen"" (my username). Many, perhaps all, of those git packages were also tagged ""current"" in my old stack.  I have no idea how my new stack could have learned anything about packages declared in the old stack. The only LSST environment variables that I normally have defined are $LSST_HOME and $LSST_GIT, and I'm pretty sure I updated the former for the new stack before starting the installation.  Neither my old or new stack has a developer sandbox (since it is on my personal work computer).  My `.eups/startup.py` contains just one line: {code} hooks.config.Eups.userTags += [""test""] {code} and I do not actually use that tag anymore (I always use ""rowen"", instead) so I should probably comment that out. ",NULL
DM-3532,"Test new lsst::log logging framework","Try out the new lsst::log framework in a few cases, including some tasks and some C++ code that uses lsst::pex::logging::Debug and/or [T]Trace, preferably including multiple logs in one file.    For the record: many packages, including afw use [T]Trace in C++ code. Few packages use Debug at all, and meas_modelfit and xrootd are the only two I found that use it extensively.",NULL
DM-3537,"Allow metric to be non-numeric","I believe we agreed last week (Tim, Jeff, Jacek) that metric value does not have to be numeric, it can be a string. This was triggered by complex metric that I have in Qserv where we want to use a different mix of queries as a metric, so I am thinking about putting there something like ""100LV+56HV"" (where LV - low volume queries, and hv - high volume queries).     To do that I need someone to remove the restriction on the metric value, we are currently enforcing it to be numeric.",NULL
DM-3541,"Please support better config for lsstsw, including an option to use git@ URLs","It would be very helpful to developers if the lsstsw tool supported a means of configuration that was not in a file checked into the git repository. That way we could update lsstsw at will without losing our configuration.    The two config items I care most about are:  - Use git@ URLs for developers instead of http::, so developers can easily push changes. The present workaround is to manually edit repos.yaml to change all URLs, but this is clumsy and breaks every time the repos.yaml file needs an update.  - The number of cores to use for a build.    One possible solution for the first item is to use a symbol in repos.yaml that gets replaced with the appropriate prefix depending on the config option.  ",NULL
DM-3542,"Buildbot is down","Buildbot stopped posting anything to its HipChat room on 20 August. At time of writing, the site at http://lsst-buildx.ncsa.illinois.edu:8010 is inaccessible. Looks like it needs resurrecting.",NULL
DM-356,"Incorporate obs_file into the official repositories and make it user friendly","It should be easy for an end-user to use obs_file to perform photometry and astrometry on an image that's had its signature removed.",NULL
DM-357,"Include obs_decam into official LSST repositories","As DECam data will be one of the precursor data sets used to assess the quality of LSST code, we need this module in LSST-controlled repositories.",NULL
DM-3584,"Port Qserv to OS X/Clang","Now that the Qserv dependencies work on OS X (DM-1662) this ticket details the issues that arise from attempting to build Qserv on OS X using the clang compiler.    A preliminary investigation finds:    * {{site_scons/state.py}} enables the {{-rpath-link}} linker option which is not supported on OS X.  * {{-Wno-unused-local-typedefs}} is not supported by clang (see DM-869).  * {{-pthread}} link option is ignored.  * {{libqserv_css}} links against {{log4cxx}} but that library is not listed explicitly in {{core/modules/SConscript}}.  * The python/swig bindings {{_cssLib.so}} do not link properly.    Also clang issues a fatal compiler error from within {{core/modules/mysql/MySqlConnection.h}}:    {code}  In file included from build/mysql/MySqlConnection.cc:29:  build/mysql/MySqlConnection.h:73:23: error: implicit instantiation of undefined template 'std::__1::basic_string<char, std::__1::char_traits<char>,        std::__1::allocator<char> >'      const std::string getError() const { assert(_mysql); return std::string(mysql_error(_mysql)); }                        ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/iosfwd:188:33: note: template is declared        here      class _LIBCPP_TYPE_VIS_ONLY basic_string;                                  ^  In file included from build/mysql/MySqlConnection.cc:29:  build/mysql/MySqlConnection.h:73:65: error: implicit instantiation of undefined template 'std::__1::basic_string<char, std::__1::char_traits<char>,        std::__1::allocator<char> >'      const std::string getError() const { assert(_mysql); return std::string(mysql_error(_mysql)); }                                                                  ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/iosfwd:188:33: note: template is declared        here      class _LIBCPP_TYPE_VIS_ONLY basic_string;                                  ^  {code}    *Compiler warnings*    {code}  In file included from build/sql/statement.cc:32:  build/sql/Schema.h:74:1: warning: 'Schema' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Schema {  ^  build/sql/statement.h:35:1: note: did you mean struct here?  class Schema; // Forward  ^~~~~  {code}    {code}  build/mysql/MySqlConnection.cc:147:41: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]      std::string killSql = ""KILL QUERY "" + boost::lexical_cast<int>(threadId);                            ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  build/mysql/MySqlConnection.cc:147:41: note: use array indexing to silence this warning      std::string killSql = ""KILL QUERY "" + boost::lexical_cast<int>(threadId);                                          ^                            &             [                                   ]  {code}",NULL
DM-3589,"Upgrades to overscan correction in ip_isr","The issues in this ticket pertain to the function {{overscanCorrection}} in {{isr.py}} in repository {{ip_isr}}.    When testing code from and HSC port, I was able to break the code in some ways which may occur in real observations. In the course of investigating the breakages I noticed several places the code could be improved to not only fix the breakages, but make it more robust in general.    * If {{fitType}} is set to {{mean}} or {{median}}, appropriate masks should be used for clipping/defect identification; currently only the image portion is passed to {{afwMath.makeStatistics}}.  * Expand the suspect bit mask to be applied to rows with defects, even if the {{fitType}} is {{mean}} or {{median}} (This may not be necessary, but there is no way to know the true value of the bias int the row)  * If there are too few overscan pixels per row, the percentiles used for clipping are skewed, and high values are not appropriately masked out, perhaps use some information from surrounding rows, and/or {{ampMaskImage}} mask information. If high pixels are left in the collapsed bias array, they will affect subsequent fitting. This may additionally occur if an entire row is affected, and thus the median value of the row is the median of bad data.    If the fit to the background level is poor, or in some way skewed it may be possible to introduce high frequency variation in the bias level of the sky background on a row by row basis. I propose to change the fitting process such that instead of collapsing the array down to one dimension before fitting for the bias function, we fit using all of the pixels in the overscan, with the pixels in a row representing a sample along the column dimension. Each of the pixels can be weighted in the joint fit according to the distance away from the median (or mean) value of the row (or some other weighting function; it may be important to down-weight the highest quartile more than the lowest for example). In this way cosmic rays and other isolated pixels will be ""clipped"" by being down weighted in the fit, and the fit overall should be more accurate and robust.    Alternatively, some clipping may be necessary in the collapsed array, to ensure rows which contain bleeding are not mistakenly identified as signal. This issue can be demonstrated by inserting a high value or row into the unit test. {{OverscanCorrection}} treats this as a valid value or row and does not mask the row. Subsequent fits for bias are then affected.     These issues may not be as important with a large number of pixels per row, but the boundary between when it would matter or not matter has not been explored. If we proactively fix the issue with a different routine we can avoid weird behavior to be fixed in future data releases.",NULL
DM-3598,"coordinate transformations in afwCameraGeom.camera convert NaN's into 0.0","The coordinate transformations between camera-based coordinate systems (e.g. between PUPIL and FOCAL_PLANE) returns Point2D(0, 0) if either coordinate of the input Point2D is set to NaN.  I do not know that this is incorrect behavior, but line 66 of include/lsst/afw/geom/XYTransform.h says that an exception is supposed to be thrown if the point being transformed is outside of the domain of the transformation    See the attached example script.",NULL
DM-360,"Move v7.3 installation instructions to separate version of SwUG","The latest LSST Software User Guide contains installation instructions for both v7.3 of the stack, and also v8.0 (just released). The instructions for v7.3 should be made less prominent, although they are still useful until:  1) the binaries are published for v8.0 2) all supported cameras have been updated to use the new CameraGeom (to be released in v8.1)  The eventual procedure will be to publish new versions of the SwUG for each release of the LSST Stack. ",NULL
DM-361,"EUPS-distributed git doesn't build with http support if curl headers are not present","KSK reported that the git package built by EUPS doesn't have https support:  {quote} git clone http://dev.lsstcorp.org/git/LSST/afw.git afw Cloning into 'afw'... fatal: Unable to find remote helper for 'http' [krughoff@magneto ~/lsststack]$ git version git version 1.8.5.2 {quote}  This is most likely due to:  http://stackoverflow.com/questions/8329485/git-clone-fatal-unable-to-find-remote-helper-for-https  After some discussion on HipChat, a proposed course of action was to package curl and add it as a dependency of EUPS-distributed git.",NULL
DM-362,"No general way to get multiple compiler flags through sconsUtils","Setting {{CCFLAGS}} in the environment does nothing.  Setting {{CCFLAGS}} on the {{scons}} command line and including {{--setenv}} works, but can't easily be applied to all executions of {{scons}}.  Setting {{CCFLAGS}} in an {{SCONSFLAGS}} environment variable works for a single flag but not for more than one.  (Luckily we've only needed a single value for {{LINKFLAGS}} in the past.)  Setting {{ARCHFLAGS}}, which get added to both {{CCFLAGS}} and {{LINKFLAGS}}, also works for a single flag but not more than one.  The problem with the last option above appears to be that {{ARCHFLAGS}} is appended as a single-element list.  It's probably best to {{split()}} it before conversion to a list. ",NULL
DM-3629,"Finalize utility class for chromaticity","Complete the work started in DM-3578 (W16).",NULL
DM-363,"Evaluate bots for HipChat","hubot: http://hubot.github.com errbot: http://gbin.github.io/err/  Others: http://help.hipchat.com/knowledgebase/articles/64359-running-a-hipchat-bot",NULL
DM-364,"Package and build cuda_toolkit and cuda_sdk","With the transition to the new distserver, we've stopped building the GPU-accelerated code, because we don't package the cuda_toolkit and cuda_sdk dependencies. Those should be turned into EupsPkg packages (and git repositories!), and re-enabled (by removing them from exclusions.txt in lsstsw.git).",NULL
DM-3649,"Fix warnings in log test program","The {{logTest.cc}} test in {{log}} issues compiler warnings concerning {{tmpnam}}. These should be fixed by using {{mkstemp}}.",NULL
DM-3655,"LSST_CFG_PATH support broken because of recent sconsUtils change","Building {{lsst_apps -t w_2015_35}} fails on my laptop:  {code}  SCONSFLAGS=""opt=3 -Q cc=clang"" eups distrib install lsst_apps -t w_2015_35    [  1/53 ]  cfitsio 3360.lsst1 (already installed)                     done.     [  2/53 ]  doxygen 1.8.5 (already installed)                          done.     [  3/53 ]  eigen 3.2.0 (already installed)                            done.     [  4/53 ]  fftw 3.3.3-1-g8fdba61 (already installed)                  done.     [  5/53 ]  gsl 1.16.lsst1 (already installed)                         done.     [  6/53 ]  minuit2 5.28.00 (already installed)                        done.     [  7/53 ]  mysqlclient 5.1.73.lsst1-1-gb8bcc43 (already installed)    done.     [  8/53 ]  python 0.0.1 (already installed)                           done.     [  9/53 ]  swig 3.0.2.lsst1 (already installed)                       done.     [ 10/53 ]  xpa 2.1.15.lsst2 (already installed)                       done.     [ 11/53 ]  boost 1.55.0.1.lsst2+3 (already installed)                 done.     [ 12/53 ]  mysqlpython 1.2.3+17 (already installed)                   done.     [ 13/53 ]  numpy 0.0.1 (already installed)                            done.     [ 14/53 ]  scons 2.3.5 (already installed)                            done.     [ 15/53 ]  wcslib 4.14+7 (already installed)                          done.     [ 16/53 ]  astrometry_net 0.50.1+6 (already installed)                done.     [ 17/53 ]  matplotlib 0.0.1 (already installed)                       done.     [ 18/53 ]  pyfits 3.2.4.lsst1+3 (already installed)                   done.     [ 19/53 ]  sconsUtils 10.1-7-ga43ce92+1 (already installed)           done.     [ 20/53 ]  astrometry_net_data 10.0+30 (already installed)            done.     [ 21/53 ]  base 10.1-1-g8080078+19 (already installed)                done.     [ 22/53 ]  geom 10.0+29 (already installed)                           done.     [ 23/53 ]  psfex master-ge3d792d24f ...    ***** error: from /Users/lsst/products/EupsBuildDir/DarwinX86/psfex-master-ge3d792d24f/build.log:    ""_fftwf_execute"", referenced from:        _fft_conv in fft.os        _fft_rtf in fft.os        _fft_ctf in fft.os    ""_fftwf_free"", referenced from:        _fft_conv in fft.os    ""_fftwf_malloc"", referenced from:        _fft_conv in fft.os        _fft_rtf in fft.os    ""_fftwf_plan_dft_2d"", referenced from:        _fft_ctf in fft.os    ""_fftwf_plan_dft_c2r_2d"", referenced from:        _fft_conv in fft.os    ""_fftwf_plan_dft_r2c_2d"", referenced from:        _fft_conv in fft.os        _fft_rtf in fft.os  ld: symbol(s) not found for architecture x86_64  clang: error: linker command failed with exit code 1 (use -v to see invocation)  scons: *** [lib/libpsfex.dylib] Error 1  + exit -4  eups distrib: Failed to build psfex-master-ge3d792d24f.eupspkg: Command:          source /Users/rhl/eups/bin/setups.sh; export EUPS_PATH=/Users/lsst/products; (/Users/lsst/products/EupsBuildDir/DarwinX86/psfex-master-ge3d792d24f/build.sh) >> /Users/lsst/products/EupsBuildDir/DarwinX86/psfex-master-ge3d792d24f/build.log 2>&1 4>/Users/lsst/products/EupsBuildDir/DarwinX86/psfex-master-ge3d792d24f/build.msg   exited with code 252  {code}    The linker command is:  {code}  clang -o lib/libpsfex.dylib -dynamiclib -Wl,-install_name -Wl,libpsfex.dylib -Wl,-headerpad_max_install_names src/levmar/lmbc.os src/levmar/Axb.os src/wcs_utils.os src/sample_utils.os src/fits/fitsmisc.os src/dummies.os src/diagnostic.os src/levmar/lm.os src/levmar/lmblec.os src/levmar/lmbleic.os src/field_utils.os src/fft.os src/psf.os src/field.os src/makeit2.os src/context.os src/misc.os src/homo.os src/wcs/poly.os src/xml.os src/prefs.os src/vignet.os src/pca.os src/levmar/misc.os src/levmar/lmlec.os -Llib -L/Users/lsst/products/DarwinX86/fftw/3.3.3-1-g8fdba61/lib -lfftw3 -llapackstub  {code}  and the symbols do seem to be defined in the library:  {code}  nm -oa /Users/lsst/products/DarwinX86/fftw/3.3.3-1-g8fdba61/lib/* 2>/dev/null | grep fftwf_free  /Users/lsst/products/DarwinX86/fftw/3.3.3-1-g8fdba61/lib/libfftw3f.3.dylib: 00000000000cb4d0 T _fftwf_free  /Users/lsst/products/DarwinX86/fftw/3.3.3-1-g8fdba61/lib/libfftw3f.a:malloc.o: 0000000000000010 T _fftwf_free  /Users/lsst/products/DarwinX86/fftw/3.3.3-1-g8fdba61/lib/libfftw3f.dylib: 00000000000cb4d0 T _fftwf_free  {code}    So maybe the linker's finding some other fftw?",NULL
DM-3668,"ingestImagesDecam.py stopped working between b1597 and b1630","ingestImagesDecam.py no long builds registry.sqlite3 with some recent changes of the stack. Output messages:     {code:java}  ingestImagesDecam.py . --mode=link instcal/*fits.fz  : Loading config overrride file '/Users/hchiang2/decam/playground/obs_decam/config/ingest.py'  : Config override file does not exist: '/Users/hchiang2/decam/playground/obs_decam/config/decam/ingest.py'  : input=/Users/hchiang2/decam/dataTempInstcal/data2  : calib=None  : output=None  CameraMapper WARNING: Unable to locate registry registry in root: /Users/hchiang2/decam/dataTempInstcal/data2/registry.sqlite3  CameraMapper WARNING: Unable to locate registry registry in current dir: ./registry.sqlite3  CameraMapper WARNING: No registry loaded; proceeding without one  ingest.parse WARNING: Unable to find value for ccdnum (derived from CCDNUM)  ingest.parse WARNING: Error reading instcal/c4d_130922_234714_ooi_i_d1.fits.fz extensions set(['S7'])  /Users/hchiang2/decam/dataTempInstcal/data2/instcal/c4d_130922_234714_ooi_i_d1.fits.fz --<link>--> /Users/hchiang2/decam/dataTempInstcal/data2/0237628/instcal0237628.fits.fz  /Users/hchiang2/decam/dataTempInstcal/data2/dqmask/c4d_130922_234714_ood_i_d1.fits.fz --<link>--> /Users/hchiang2/decam/dataTempInstcal/data2/0237628/dqmask0237628.fits.fz  /Users/hchiang2/decam/dataTempInstcal/data2/wtmap/c4d_130922_234714_oow_i_d1.fits.fz --<link>--> /Users/hchiang2/decam/dataTempInstcal/data2/0237628/wtmap0237628.fits.fz  Traceback (most recent call last):    File ""/Users/hchiang2/decam/playground/obs_decam/bin/ingestImagesDecam.py"", line 3, in <module>      DecamIngestTask.parseAndRun()    File ""/opt/lsst/DarwinX86/pipe_tasks/10.1-35-ga8dd78f+3/python/lsst/pipe/tasks/ingest.py"", line 353, in parseAndRun      task.run(args)    File ""/Users/hchiang2/decam/playground/obs_decam/python/lsst/obs/decam/ingest.py"", line 67, in run      self.register.addRow(registry, info, dryrun=args.dryrun, create=args.create)    File ""/opt/lsst/DarwinX86/pipe_tasks/10.1-35-ga8dd78f+3/python/lsst/pipe/tasks/ingest.py"", line 311, in addRow      conn.execute(sql, values)  AttributeError: 'RegistryContext' object has no attribute 'execute'  {code}    ",NULL
DM-3669,"Please remove chat about ""EUPS integration"" with -Q","When you added messages about ""EUPS integration"" you failed to make the message generation obey {{-Q}} (cf. DM-1565).  Please fix this.  ",NULL
DM-3676,"log package should not depend on utils","The {{log}} package does not depend on {{utils}} for anything other than some minor Swig includes.",NULL
DM-3680,"Add Scons buildblot slave to aid their port to Python 3","I have been talking to the {{scons}} developers about supporting Python 3. They state that one of the impediments to supporting Python 3 is that they do not have any buildbot slaves currently using python 3.    Would it be possible for SQuaRE to set up a small system (say, running Fedora so that python 3 is native) that can be a buildbot slave for the {{scons}} project?    Instructions for activating the slave can be found at [https://bitbucket.org/scons/scons/wiki/InstallingBuildbotSlaves]",NULL
DM-3689,"hipchat notification of discourse activity","It would be nice to have a hipchat channel with notifications of discourse activity.  As a power up, perhaps new topics under certain categories, I'm thinking specifically of DM Notifications, could generate posts to select general HC channels -- similar to how RFC notifications are currently handled.    A quick google search turns up this plugin for integration:    https://github.com/binaryage/discourse-hipchat-plugin",NULL
DM-3696,"Gracefully catch incorrect syntax in ""SELECT * AS x FROM t""","The following command:  {code:bash}  ""SELECT * AS N FROM qservTest_case01_qserv.Object LIMIT 10""  {code}    crashes Qserv czar. The error message in the log:  {code}  ERROR ccontrol.UserQueryFactory (build/ccontrol/UserQueryFactory.cc:117)   - Invalid query: ParseException:Parse error(ANTLR):unexpected token: AS:  {code}    Running the same query with plain mysql results in:  {code}  ERROR 1064 (42000) at line 1: You have an error in your SQL syntax; check   the manual that corresponds to your MySQL server version for the right syntax   to use near 'AS N FROM qservTest_case01_qserv.Object LIMIT 10' at line 1  {code}",NULL
DM-3699,"Upgrade Eigen to remove ""register"" warnings during builds","The current compile/link system results in numerous uninteresting warnings about ""register"" not being supported by C++11 when we build DM stack code. These are so numerous as to make it hard to detect useful warnings. These are caused by boost and Eigen.    Some possible solutions:  - Upgrade boost and Eigen. This would be optimal (assuming that newer versions have indeed stopped using ""register""), but may be difficult. Boost, in particular, seems to require several patches to build successfully.  - Suppress the warnings by explicitly testing if the compiler accepts a flag to suppress them and use that flag if so. If we go this route we should explicitly undo it once we upgrade boost and Eigen.  - Suppress the warnings using -isystem, as per [~swinbank]. Again, undo this once we upgrade boost and Eigen.    We have lived with this for a long time, but it is very unpleasant. At this point I'd like to push it up in priority and accept solutions that paper over the problem if necessary.    There is another class of warning that is less numerous but still a nuisance: warnings about using an outdated API for numpy. I am not sure where those come from, and there are few enough that I feel we can live with them if no easy solution presents itself. I suspect fixing them will require a different ticket.    DM-869 is related, but appears to focus on a different class of warnings from boost (one that I don't usually see).    *Update:* Ticket has been updated to just refer to Eigen as the boost upgrade was covered in DM-2384 and Eigen is the remaining source of register warnings.",NULL
DM-3705,"Update ndarray to use current numpy API","When we build software that uses SWIG and ndarray we get {{warning ""Using deprecated NumPy API}}. Please update ndarray so we are using the current API.    If in the interim you want to suppress the warnings, I suggest issuing a separate ticket for that. I have mixed feelings about suppressing the warnings. On the one hand false warnings do make it harder to spot real problems. On the other hand these warnings are not very numerous (building afw only results in 8 of them) and so are fairly easy to ignore.",NULL
DM-3754,"eupspkg install failure mysqlclient: ""must supply either home or prefix/exec-prefix -- not both""","When installing w_2015_36, I get the following error installing mysqlclient as part of {{eups distrib install -t w_2015_36}}:  {code}  eupspkg.install: deleting existing install in '/Users/wmwv/lsst/DarwinX86/mysqlpython/1.2.3+17'  running install  error: must supply either home or prefix/exec-prefix -- not both  + exit -5  eups distrib: Failed to build mysqlpython-1.2.3+17.eupspkg: Command:  {code}    This looks like something about Python install defaults.  I would be most happy to take suggestions for things to check.    {{build.log}} attached.  ",NULL
DM-376,"Testing of inclusion on CameraTransformMap should raise False regardless of type","Testing for inclusion in a CameraTransfromMap object raises a TypeError if the object being tested is not of type CameraSys.  This is confusing in python land because one doesn't expect lists to be typed.  How to repeat: {code:py} >>> import lsst.afw.cameraGeom as cameraGeom >>> transformMap = cameraGeom.CameraTransformMap() >>> None in transformMap Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""/nfs/lsst/home/krughoff/lsst/lsst_devel/Linux64/LSST/DMS/afw/python/lsst/afw/cameraGeom/cameraGeomLib.py"", line 960, in __contains__     return _cameraGeomLib.CameraTransformMap___contains__(self, *args) ValueError: invalid null reference in method 'CameraTransformMap___contains__', argument 2 of type 'lsst::afw::cameraGeom::CameraSys const &' {code}",NULL
DM-3763,"Be friendlier to New Users on Discourse","Please adjust New Users thresholds so that the following common use case on Discourse will work:    1. I have a question/thread I'm really interested in so I join.  2. I post answers, questions, and links.  I'm probably going to be heavily involved in this thread (it's why I bothered to create an account, after all).    The current defaults allow me only 3 replies, posting of 2 URLs, and no attachments.  I don't know what the exact values should be, but the current defaults don't seem to work for what would seem to me to be a pretty common use case.",NULL
DM-3767,"I'm getting 2-3 copies of every comment to JIRA tickets I'm watching","In the last few days or so I've been getting 2-3 copies of every comment posted to any ticket I'm watching. Did JIRA change? Are my settings messed up? It's not a big deal, but it's rather annoying to have to wade through all the duplicates and avoid accidentally skipping messages I care about.",NULL
DM-3781,"The obs_test camera should have more than one CCD","I just tried the showCamera code from afw DM-2437 and discovered that obs_test only has one CCD.  This makes it hard to test various things as there's a qualitative difference between a scalar (0, 1) and a list (2 or more).    Please add more CCDs to test_obs;  I'd propose 6 in a 2x3 pattern but even 2 would be much better than 1.  ",NULL
DM-3789,"Upgrade fftw to v3.3.4","The current version of fftw (3.3.3) does not build if someone has texinfo-5 installed. This ticket covers upgrading to version 3.3.4 which includes the required fixes. {{fftw}} is marked as a ""u"" third party package so is allowed to be upgraded periodically. The release notes for 3.3.4 indicate that there are only minor fixes in this release so we are not expecting any compatibility issues.",NULL
DM-379,"Add Reviewer to JIRA ""My"" filters","If we're going to keep assignees constant and assign Reviewers as issues go into Review status, the various filters that pick out ""My"" issues, including the Agile board Quick Filters configurable only by the administrator, should likely include Reviewers in addition to Assignee so that people can easily track the issues they are supposed to be reviewing.  An alternate would be to provide a different filter showing just those In Review issues for which the current user is a Reviewer.  (It is desirable to keep the Assignee for the issue not only so that Story Points can be tracked reasonably but also so that the Assignee is aware that it is fundamentally his/her responsibility to get the code reviewed by someone.)",NULL
DM-3790,"Top level product for obs_decam ","We have an action to allow people to pull obs_decam and a working stack without including obs_decam in lsst_distrib. And we want to CI it.     The strawman plan is to make a new TLP for decam. SQuaRE will discuss. ",NULL
DM-3793,"Upgrade wcslib to 5.9","Upgrade wcslib to version 5.9, as per RFC-89. This should not involve any API changes.",NULL
DM-3795,"/lsst/stack on lsst-dev is broken","Viz:    {code}  [swinbank@lsst-dev ~]$ . /lsst/stack/loadLSST.sh  /lsst/stack/eups-1.5.9/bin/eups_setup: line 6: /home/lsstsw/anaconda/bin/python: No such file or directory  /lsst/stack/eups-1.5.9/bin/eups_setup: line 6: /home/lsstsw/anaconda/bin/python: No such file or directory  /lsst/stack/eups-1.5.9/bin/eups_setup: line 6: /home/lsstsw/anaconda/bin/python: No such file or directory  {code}    This was discussed on HipChat yesterday (9 Sept); it's still happening as of this morning.  Not clear if this is just a glitch related to the upcoming release or if there's something intentional going on.",NULL
DM-3799,"Fix compiler detection in Qserv for non-gcc compiler","In DM-3772 the compiler tests were modified to support {{gcc-X.Y}} syntax but the patch resulted in an additional capture group being created for the gcc case which did not exist in the other cases. The fix is to make the additional parentheses non-capturing.",NULL
DM-3805,"uint type is non-standard","{{core/modules/util/common.h}} in Qserv uses the type {{uint}} when it should be using {{unsigned int}}. {{uint}} is technically not part of the C++ standard although many compilers do define it.",NULL
DM-3807,"Build documenteer package to support stack documentation","Build a Python package (ideally Python 3.4+ compatible, but absolutely Python 2.7 compatible) containing Sphinx and docutils extensions to support the LSST Stack documentation. This package should be sufficiently generic that other open source projects can find & use it.",NULL
DM-3809,"Create a fenced code snippet Sphinx extension","A reST directive for Sphinx that shows (and highlights) source code from an external source code file. The snippet can be either the whole file, or more particularly, just part of the file. Named fences defined in the source code's comments can be used to extract a source code snippet. (Comment fences are better than specifying line number ranges since line numbers change too frequently).    Source code could be specified multiple ways    * Filesystem path  * Remote git path + branch + file path  * Python namespace path    Part of lsst_sphinx_kit",NULL
DM-381,"Implement proposed components of alert processing","Prototype of alert processing as described in LDM-230, section 2.",NULL
DM-3810,"Create reST directives for LSST document URLs","Create a set of restructured text directives to make it easy to link to LSST DM URLs from inside reST text.    e.g. {{:dm-rfc:}} and {{:ticket:}}    or astronomy literature: {{:arxiv:}} and {{:ads:}}.",NULL
DM-3812,"Please provide a command-line way of getting the history of a config parameter","The   {code}  --show config=[glob]  {code}  option to e.g. {{processCcd.py}} is very useful, but sometimes you find yourself wondering where that value came from.  As discussed in http://community.lsst.org/t/how-do-i-get-the-history-of-a-configuration-object/162 you can get this information if you're willing to write some boiler-plate python but it'd be really nice to support via a {{--show}}  option.  Something like  {code}  --show configHistory[=glob]  {code}    [~ktl] recommends starting from https://raw.githubusercontent.com/lsst/pex_config/ecacb7eafca021afb4c9b9e4852bcf9e4a9ede45/bin/configDoc.py",NULL
DM-3813,"Build a DM Design Document Platform on GitHub with CI","Build a platform for DM design documents with requirements    * Each design document has it's own repo on GitHub  * Content of the design document is in reST with images as appropriate  * Documents are continuously built on commit and rendered via readthedocs.org    A predecessor to this work was DM-3546. This is a meta story to coordinate individual stories in this effort related to doc infrastructure and content conversion.",NULL
DM-382,"Update ActiveMQ broker to expire topics.","The ActiveMQ broker doesn't automatically expire topics once they're created.  When running the ctrl_events tests, topics are created that are guaranteed to be only used once, so that simultaneous tests don't send messages to each other's topics.  This doesn't really have an impact in most cases, but if the ActiveMQ broker is up for a very long time, the number of topics will continue to increase.  In recent versions of ActiveMQ, this is a configurable.   Topics that haven't been used (meaning, no clients for that topic) for a certain length of time can automatically be deleted.",NULL
DM-3829,"Enable sconsUtils to output contents of failed tests as well as reporting the number of failures","Sometimes the output of {{scons}} when a multi-threaded build is executing makes it hard to determine which tests are failing and which are passing without looking in the {{.tests}} directly after the build completes. In a test build with {{eups distrib}} on a virtual machine scanning the directory is not always trivial.    The proposal is for {{sconsUtils}} to be modified to dump the contents of the failed test output to standard output itself. It may be appropriate to restrict this verbose output to when an appropriate environment variable is set.",NULL
DM-383,"pipe_tasks return error w/help option","Various pipeline processing tasks (e.g. processCcd.py), when run with the ""--help"" option, return the error:   <taskname>: error: Must specify input as first argument  The task help is correctly printed prior to the error message, however, so this is really only an annoyance. ",NULL
DM-3848,"ssh key injection appears to fail","Instance ID c479bb88-1deb-44d0-af02-3c9320b88e67 appears to have failed to properly inject the ssh user's public keys.  3 other instances launched from the same image were accessible as expected.    {code}  $ ssh -l cirros 141.142.208.105 -vvv  OpenSSH_6.9p1, OpenSSL 1.0.1k-fips 8 Jan 2015  debug1: Reading configuration data /home/jhoblitt/.ssh/config  debug1: Reading configuration data /etc/ssh/ssh_config  debug1: /etc/ssh/ssh_config line 3: Applying options for *  debug1: Control socket ""/home/jhoblitt/.ssh/master/cirros@141.142.208.105:22"" does not exist  debug2: ssh_connect: needpriv 0  debug1: Connecting to 141.142.208.105 [141.142.208.105] port 22.  debug2: fd 3 setting O_NONBLOCK  debug1: fd 3 clearing O_NONBLOCK  debug1: Connection established.  debug3: timeout: 7199948 ms remain after connect  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_rsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_rsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_dsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_dsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_ecdsa type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_ecdsa-cert type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_ed25519 type -1  debug1: key_load_public: No such file or directory  debug1: identity file /home/jhoblitt/.ssh/id_ed25519-cert type -1  debug1: Enabling compatibility mode for protocol 2.0  debug1: Local version string SSH-2.0-OpenSSH_6.9  debug1: Remote protocol version 2.0, remote software version dropbear_2012.55  debug1: no match: dropbear_2012.55  debug2: fd 3 setting O_NONBLOCK  debug1: Authenticating to 141.142.208.105:22 as 'cirros'  debug3: hostkeys_foreach: reading file ""/home/jhoblitt/.ssh/known_hosts""  debug3: record_hostkey: found key type RSA in file /home/jhoblitt/.ssh/known_hosts:260  debug3: load_hostkeys: loaded 1 keys from 141.142.208.105  debug3: hostkeys_foreach: reading file ""/etc/ssh/ssh_known_hosts""  debug3: order_hostkeyalgs: prefer hostkeyalgs: ssh-rsa-cert-v01@openssh.com,ssh-rsa-cert-v00@openssh.com,ssh-rsa  debug1: SSH2_MSG_KEXINIT sent  debug1: SSH2_MSG_KEXINIT received  debug2: kex_parse_kexinit: curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1,diffie-hellman-group1-sha1  debug2: kex_parse_kexinit: ssh-rsa-cert-v01@openssh.com,ssh-rsa-cert-v00@openssh.com,ssh-rsa,ecdsa-sha2-nistp256-cert-v01@openssh.com,ecdsa-sha2-nistp384-cert-v01@openssh.com,ecdsa-sha2-nistp521-cert-v01@openssh.com,ssh-ed25519-cert-v01@openssh.com,ssh-dss-cert-v01@openssh.com,ssh-dss-cert-v00@openssh.com,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519,ssh-dss  debug2: kex_parse_kexinit: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,arcfour256,arcfour128,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,rijndael-cbc@lysator.liu.se  debug2: kex_parse_kexinit: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,arcfour256,arcfour128,aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,aes256-cbc,arcfour,rijndael-cbc@lysator.liu.se  debug2: kex_parse_kexinit: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1,hmac-md5-etm@openssh.com,hmac-ripemd160-etm@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-96-etm@openssh.com,hmac-md5,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96  debug2: kex_parse_kexinit: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1,hmac-md5-etm@openssh.com,hmac-ripemd160-etm@openssh.com,hmac-sha1-96-etm@openssh.com,hmac-md5-96-etm@openssh.com,hmac-md5,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96  debug2: kex_parse_kexinit: none,zlib@openssh.com,zlib  debug2: kex_parse_kexinit: none,zlib@openssh.com,zlib  debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit: first_kex_follows 0   debug2: kex_parse_kexinit: reserved 0   debug2: kex_parse_kexinit: diffie-hellman-group1-sha1,diffie-hellman-group14-sha1  debug2: kex_parse_kexinit: ssh-rsa,ssh-dss  debug2: kex_parse_kexinit: aes128-ctr,3des-ctr,aes256-ctr,aes128-cbc,3des-cbc,aes256-cbc,twofish256-cbc,twofish-cbc,twofish128-cbc  debug2: kex_parse_kexinit: aes128-ctr,3des-ctr,aes256-ctr,aes128-cbc,3des-cbc,aes256-cbc,twofish256-cbc,twofish-cbc,twofish128-cbc  debug2: kex_parse_kexinit: hmac-sha1-96,hmac-sha1,hmac-md5  debug2: kex_parse_kexinit: hmac-sha1-96,hmac-sha1,hmac-md5  debug2: kex_parse_kexinit: none  debug2: kex_parse_kexinit: none  debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit:   debug2: kex_parse_kexinit: first_kex_follows 0   debug2: kex_parse_kexinit: reserved 0   debug1: kex: server->client aes128-ctr hmac-sha1 none  debug1: kex: client->server aes128-ctr hmac-sha1 none  debug1: kex: diffie-hellman-group14-sha1 need=20 dh_need=20  debug1: kex: diffie-hellman-group14-sha1 need=20 dh_need=20  debug1: sending SSH2_MSG_KEXDH_INIT  debug2: bits set: 1027/2048  debug1: expecting SSH2_MSG_KEXDH_REPLY  debug1: Server host key: ssh-rsa SHA256:B37vFXX1BXnIkkKsMc35yTTDmnJMJTA2915ddFy/J+w  debug3: hostkeys_foreach: reading file ""/home/jhoblitt/.ssh/known_hosts""  debug3: record_hostkey: found key type RSA in file /home/jhoblitt/.ssh/known_hosts:260  debug3: load_hostkeys: loaded 1 keys from 141.142.208.105  debug3: hostkeys_foreach: reading file ""/etc/ssh/ssh_known_hosts""  debug1: Host '141.142.208.105' is known and matches the RSA host key.  debug1: Found key in /home/jhoblitt/.ssh/known_hosts:260  debug2: bits set: 990/2048  debug2: set_newkeys: mode 1  debug1: SSH2_MSG_NEWKEYS sent  debug1: expecting SSH2_MSG_NEWKEYS  debug2: set_newkeys: mode 0  debug1: SSH2_MSG_NEWKEYS received  debug1: Roaming not allowed by server  debug1: SSH2_MSG_SERVICE_REQUEST sent  debug2: service_accept: ssh-userauth  debug1: SSH2_MSG_SERVICE_ACCEPT received  debug2: key: .ssh/id_rsa_github (0x7fe3ecfefb70),  debug2: key: /home/jhoblitt/.ssh/id_rsa ((nil)),  debug2: key: /home/jhoblitt/.ssh/id_dsa ((nil)),  debug2: key: /home/jhoblitt/.ssh/id_ecdsa ((nil)),  debug2: key: /home/jhoblitt/.ssh/id_ed25519 ((nil)),  debug1: Authentications that can continue: publickey,password  debug3: start over, passed a different list publickey,password  debug3: preferred publickey,keyboard-interactive,password  debug3: authmethod_lookup publickey  debug3: remaining preferred: keyboard-interactive,password  debug3: authmethod_is_enabled publickey  debug1: Next authentication method: publickey  debug1: Offering RSA public key: .ssh/id_rsa_github  debug3: send_pubkey_test  debug2: we sent a publickey packet, wait for reply  debug1: Authentications that can continue: publickey,password  debug1: Trying private key: /home/jhoblitt/.ssh/id_rsa  debug3: no such identity: /home/jhoblitt/.ssh/id_rsa: No such file or directory  debug1: Trying private key: /home/jhoblitt/.ssh/id_dsa  debug3: no such identity: /home/jhoblitt/.ssh/id_dsa: No such file or directory  debug1: Trying private key: /home/jhoblitt/.ssh/id_ecdsa  debug3: no such identity: /home/jhoblitt/.ssh/id_ecdsa: No such file or directory  debug1: Trying private key: /home/jhoblitt/.ssh/id_ed25519  debug3: no such identity: /home/jhoblitt/.ssh/id_ed25519: No such file or directory  debug2: we did not send a packet, disable method  debug3: authmethod_lookup password  debug3: remaining preferred: ,password  debug3: authmethod_is_enabled password  debug1: Next authentication method: password  cirros@141.142.208.105's password:   {code}",NULL
DM-3854,"Anaconda package incorrectly assumes all Linux is x86","Our Anaconda package checks if it's running on Linux. If so, and the architecture is {{x86_64}}, it installs {{Anaconda-2.2.0-Linux-x86_64.sh}}; if the architecture is anything else, it attempts to install {{Anaconda-2.2.0-Linux-x86.sh}}. This fails on non-x86 machines:  {code}  installer.sh: line 397: /root/lsst/Linux/anaconda/2.2.0/pkgs/python-2.7.9-2/bin/python: cannot execute binary file: Exec format error  ERROR:  cannot execute native linux-32 binary, output from 'uname -a' is:  Linux jds 3.2.34-30 #17 SMP Mon Apr 13 15:53:45 UTC 2015 armv7l armv7l armv7l GNU/Linux  + exit -5  eups distrib: Failed to build anaconda-2.2.0.eupspkg: Command:          source /root/lsst/eups/bin/setups.sh; export EUPS_PATH=/root/lsst; (/root/lsst/EupsBuildDir/Linux/anaconda-2.2.0/build.sh) >> /root/lsst/EupsBuildDir/Linux/anaconda-2.2.0/build.log 2>&1 4>/root/lsst/EupsBuildDir/Linux/anaconda-2.2.0/build.msg  exited with code 251  {code}  I don't believe Anaconda ships a package that will run on {{armv7l}}, but we should tell the user that rather than wasting time downloading and failing to install software that will never work.",NULL
DM-3856,"matplotlib sometimes fails to load during tests","During some multi-threaded builds {{meas_algorithms}} is sometimes failing when loading the matplotlib font cache. We think this is due to two tests simultaneous using matplotlib infrastructure and the second one sometimes attempts to load the font cache when the first is using it.    {code}     import matplotlib.colorbar    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/colorbar.py"", line 34, in <module>      import matplotlib.collections as collections    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/collections.py"", line 27, in <module>      import matplotlib.backend_bases as backend_bases    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/backend_bases.py"", line 56, in <module>      import matplotlib.textpath as textpath    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/textpath.py"", line 19, in <module>      import matplotlib.font_manager as font_manager    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/font_manager.py"", line 1415, in <module>      _rebuild()    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/font_manager.py"", line 1402, in _rebuild      pickle_dump(fontManager, _fmcache)    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/font_manager.py"", line 952, in pickle_dump      with open(filename, 'wb') as fh:  IOError: [Errno 2] No such file or directory: u'/tmp/matplotlib-vagrant/fontList.cache'  {code}    Currently matplotlib import failures are trapped as follows:    {code:python}  try:      import matplotlib.pyplot as plt      import matplotlib.colors  except ImportError:      plt = None  {code}    but this only protects the case where matplotlib is not installed. It does not protect against a failure of an existing matplotlib to load.    The matplotlib project has come across this problem itself when using Travis ([https://github.com/matplotlib/matplotlib/commit/f0888de10e8740200c7cb8387a3733d4fe6af967]) and they disable the font cache when {{$TRAVIS}} is set.    To fix this we can therefore either fix every import to also trap an {{IOError}} or else modify {{sconsUtils}} to set the TRAVIS environment variable when it runs the tests.",NULL
DM-3867,"Add coadd related data products to DecamMapper","This ticket will add any missing data products required for coaddition to the obs_decam mapper. (And make any updates should problems arise) ",NULL
DM-3868,"obs_decam:tests/getRaw.py should skip test if testdir not set up","In the one unit test, DM-3462 introduced an implicit dependence on TESTDATA_DECAM_DIR.    {code}  +        datadir = os.getenv(""TESTDATA_DECAM_DIR"")  +        assert datadir, ""testdata_decam is not setup""  {code}    If someone git clones this repository and builds it, this test will fail unless they've explicitly downloaded the test dataset and setup it up.  This is not the behavior we want.  We want it to check for the test data and skip the test if not set up.",NULL
DM-3889,"Multi-threaded matplotlib test issue in meas_algorithms","A rare failure of matplotlib in meas_algorithms:    {code}  [ 59/71 ]  meas_algorithms 11.0.rc2-1-gc0593fd+6 ...     ***** error: from /home/vagrant/stack/EupsBuildDir/Linux64/meas_algorithms-11.0.rc2-1-gc0593fd+6/build.log:      import matplotlib.colorbar    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/colorbar.py"", line 36, in <module>      import matplotlib.contour as contour    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/contour.py"", line 27, in <module>      import matplotlib.texmanager as texmanager    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/texmanager.py"", line 89, in <module>      class TexManager:    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/texmanager.py"", line 124, in TexManager      mkdirs(texcache)    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/site-packages/matplotlib/cbook.py"", line 1003, in mkdirs      os.makedirs(thispart, mode)    File ""/home/vagrant/stack/Linux64/anaconda/2.2.0/lib/python2.7/os.py"", line 157, in makedirs      mkdir(name, mode)  OSError: [Errno 17] File exists: '/tmp/matplotlib-vagrant'  The following tests failed:  /home/vagrant/stack/EupsBuildDir/Linux64/meas_algorithms-11.0.rc2-1-gc0593fd+6/meas_algorithms-11.0.rc2-1-gc0593fd+6/tests/.tests/psfSelectTest.py.failed  1 tests failed  {code}    This is a race between multiple matplotlib processes trying to create a directory tree.    {{matplotlib}} upstream has been notified in [https://github.com/matplotlib/matplotlib/issues/2046]",NULL
DM-3890,"Race condition in daf_persistence tests","Rarely daf_persistence can fail when credentials are available and the tests run in parallel.    {code}  $ cat DbStorage_2.py.failed  tests/DbStorage_2.py    1442423057628685824  Traceback (most recent call last):    File ""tests/DbStorage_2.py"", line 97, in <module>      assert not db.next()  AssertionError  {code}    {code}  $ cat DbStorage_1.py.failed  tests/DbStorage_1.py    1442423057628685824  Traceback (most recent call last):    File ""tests/DbStorage_1.py"", line 88, in <module>      assert db.getColumnByPosDouble(0) == 1.23456  AssertionError  {code}",NULL
DM-390,"Build redirector","This is described in LDM-230, sections 2.3.2 and 2.3.3, including accepting distributor publications of which rafts/CCDs it owns, and replying to worker requests for CCDs.",NULL
DM-3904,"meas_astrom SIP test can fail if tests run all in one process","When running the {{meas_astrom}} test suite with {{nose}} the following error occurs:    {code}  ======================================================================  ERROR: testBigXy0 (createWcsWithSip.CreateWcsWithSipCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 81, in testBigXy0      res = self.astrom.determineWcs2(cat, bbox=bbox)    File ""/Users/timj/work/lsstsw/src/meas_astrom/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 436, in determineWcs2      wcs,qa = self.getBlindWcsSolution(sourceCat, **kwargs)    File ""/Users/timj/work/lsstsw/src/meas_astrom/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 524, in getBlindWcsSolution      raise RuntimeError(""Unable to match sources with catalog."")  RuntimeError: Unable to match sources with catalog.  -------------------- >> begin captured stdout << ---------------------  Catalog size 186    --------------------- >> end captured stdout << ----------------------    ======================================================================  ERROR: testLinearXDistort (createWcsWithSip.CreateWcsWithSipCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 98, in testLinearXDistort      self.singleTestInstance(self.filename, distort.linearXDistort)    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 109, in singleTestInstance      cat = self.loadCatalogue(self.filename)    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 165, in loadCatalogue      res = self.astrom.determineWcs2(cat, bbox=bbox)    File ""/Users/timj/work/lsstsw/src/meas_astrom/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 436, in determineWcs2      wcs,qa = self.getBlindWcsSolution(sourceCat, **kwargs)    File ""/Users/timj/work/lsstsw/src/meas_astrom/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 524, in getBlindWcsSolution      raise RuntimeError(""Unable to match sources with catalog."")  RuntimeError: Unable to match sources with catalog.  -------------------- >> begin captured stdout << ---------------------  linearXDistort    --------------------- >> end captured stdout << ----------------------    ======================================================================  ERROR: testLinearYDistort (createWcsWithSip.CreateWcsWithSipCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 102, in testLinearYDistort      self.singleTestInstance(self.filename, distort.linearYDistort)    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 109, in singleTestInstance      cat = self.loadCatalogue(self.filename)    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 165, in loadCatalogue      res = self.astrom.determineWcs2(cat, bbox=bbox)    File ""/Users/timj/work/lsstsw/src/meas_astrom/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 436, in determineWcs2      wcs,qa = self.getBlindWcsSolution(sourceCat, **kwargs)    File ""/Users/timj/work/lsstsw/src/meas_astrom/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 524, in getBlindWcsSolution      raise RuntimeError(""Unable to match sources with catalog."")  RuntimeError: Unable to match sources with catalog.  -------------------- >> begin captured stdout << ---------------------  linearYDistort    --------------------- >> end captured stdout << ----------------------    ======================================================================  ERROR: testQuadraticDistort (createWcsWithSip.CreateWcsWithSipCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 106, in testQuadraticDistort      self.singleTestInstance(self.filename, distort.linearYDistort)    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 109, in singleTestInstance      cat = self.loadCatalogue(self.filename)    File ""/Users/timj/work/lsstsw/src/meas_astrom/tests/createWcsWithSip.py"", line 165, in loadCatalogue      res = self.astrom.determineWcs2(cat, bbox=bbox)    File ""/Users/timj/work/lsstsw/src/meas_astrom/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 436, in determineWcs2      wcs,qa = self.getBlindWcsSolution(sourceCat, **kwargs)    File ""/Users/timj/work/lsstsw/src/meas_astrom/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 524, in getBlindWcsSolution      raise RuntimeError(""Unable to match sources with catalog."")  RuntimeError: Unable to match sources with catalog.  -------------------- >> begin captured stdout << ---------------------  linearQuadraticDistort    --------------------- >> end captured stdout << ----------------------    ----------------------------------------------------------------------  Ran 77 tests in 96.588s    FAILED (SKIP=10, errors=4)  {code}  (This requires the corresponding patches from the DM-3901 ticket branch to have been applied).    Running in subprocess with:  {code}  nosetests --processes=4 --process-timeout=240 tests/*.py  {code}  leads to all tests passing. As does running just the {{createWcsWithSip.py}} test on its own with nose.",NULL
DM-391,"Build data preload and puller worker batch job","This is described in LDM-230, section 2.3.3.  Includes: preloading the template image, master calibration images, DIAObjects DIASources, and SSObjects;  retrieving both sets of crosstalk-corrected pixels and associated telemetry/metadata. ",NULL
DM-3914,"Continuous Deployment of the LDM-151 DM Applications Design to GitHub Pages with Travis CI","This will demonstrate how LaTeX-based DM design documents can be continuosly deployed to the web with Travis CI onto GitHub pages. tex4ht is the HTML rendering engine.",NULL
DM-3915,"Unused local typedef warning in Qserv","Now that DM-3803 has fixed most of the warnings there is one residual warning in Qserv:  {code}  core/modules/qana/RelationGraph.cc:956:44: error: unused typedef 'VecIter' [-Werror,-Wunused-local-typedef]      typedef std::vector<Vertex*>::iterator VecIter;                                             ^  {code}    The fix is simple:  {code:diff}  diff --git a/core/modules/qana/RelationGraph.cc b/core/modules/qana/RelationGraph.cc  index a826c04..79a2fd9 100644  --- a/core/modules/qana/RelationGraph.cc  +++ b/core/modules/qana/RelationGraph.cc  @@ -953,7 +953,6 @@ void RelationGraph::rewrite(SelectStmtPtrVector& outputs,                               QueryMapping& mapping)   {       typedef std::list<Vertex>::iterator ListIter;  -    typedef std::vector<Vertex*>::iterator VecIter;          if (!_query) {           return;  {code}",NULL
DM-3925,"eups publish overly inclusive","DId this    {code:java}    publish -b b1694 -t v11_0_rc3 lsst_distrib  {code}    and unexpectedly ended up with a bunch of sims packages in:    https://sw.lsstcorp.org/eupspkg/tags/v11_0_rc3.list    Could be publish, could be lsstsw, could be buildbot, could be my user error - filing so I don't forget, not an action item for anyone in SQuaRE at this point.    Data point: happened for 10.1 too. ",NULL
DM-3927,"Spring clean team repos","  Input is being gathered here:  http://ls.st/sli    [Team membership affects the application of the tagging policy]    ",NULL
DM-3933,"get('calexp_md') gives error for decam calexps","To reproduce:  {code}  >>> import lsst.daf.persistence as dafPersist  >>> butler = dafPersist.Butler('/lsst8/image_diff/calexp_dir_Blind14A_10/')  CameraMapper: Loading registry registry from /lsst8/image_diff/calexp_dir_Blind14A_10/registry.sqlite3  >>> butler.get('calexp', visit=289493, ccdnum=15)  <lsst.afw.image.imageLib.ExposureF; proxy of <Swig Object of type 'boost::shared_ptr< lsst::afw::image::Exposure< float,lsst::afw::image::MaskPixel,lsst::afw::image::VariancePixel > > *' at 0x7f5882437de0> >  >>> butler.get('calexp_md', visit=289493, ccdnum=15)  Traceback (most recent call last):    File ""<stdin>"", line 1, in <module>    File ""<string>"", line 1, in __repr__    File ""/home/lsstsw/stack/Linux64/daf_persistence/11.0.rc2+10/python/lsst/daf/persistence/readProxy.py"", line 41, in __getattribute__      subject = oga(self, '__subject__')    File ""/home/lsstsw/stack/Linux64/daf_persistence/11.0.rc2+10/python/lsst/daf/persistence/readProxy.py"", line 136, in __subject__      set_cache(self, get_callback(self)())    File ""/home/lsstsw/stack/Linux64/daf_persistence/11.0.rc2+10/python/lsst/daf/persistence/butler.py"", line 236, in <lambda>      location, dataId)    File ""/home/lsstsw/stack/Linux64/daf_butlerUtils/11.0.rc2+15/python/lsst/daf/butlerUtils/cameraMapper.py"", line 317, in <lambda>      afwImage.readMetadata(location.getLocations()[0]))    File ""/home/lsstsw/stack/Linux64/afw/11.0.rc2-2-g04d2804+2/python/lsst/afw/image/imageLib.py"", line 1146, in readMetadata      return _imageLib.readMetadata(*args)  lsst.pex.exceptions.wrappers.TypeError:     File ""src/PropertySet.cc"", line 858, in virtual void lsst::daf::base::PropertySet::_add(const string&, boost::shared_ptr<std::vector<boost::any> >)      LTV2 has mismatched type {0}  lsst::pex::exceptions::TypeError: 'LTV2 has mismatched type'  {code}",NULL
DM-3934,"Research RocksDB","http://rocksdb.org/  https://github.com/facebook/rocksdb    In particular, let's check if it could be used for secondary index (see DM-736)",NULL
DM-3948,"Jenkins page for starting jobs could be more informative","The Jenkins page for starting builds https://ci.lsst.codes/job/stack-os-matrix/build?delay=0sec could use more information, including:  - The meaning of the branch entry field, e.g. text such as the following (adapted from https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool): ""Jenkins will attempt to checkout the first specified branch (both in the package and its dependencies), and fall back to additional branches, if any are specified, finally falling back to the default branch (which is usually master)""  - Ideally it would not allow one to try to enter info or start a job until logged in (probably already a separate ticket) but assuming that is hard enough to take awhile, adding a bold warning that you must log in before entering data or trying to start a build would be very helpful.",NULL
DM-3950,"Unify API of Source/Star Selectors"," {{selectStars()}} is the primary method in {{lsst.meas.algorithms.SecondMomentStarSelector}} and all other StarSelectors in meas_algorithms. The primary method in {{lsst.ip.diffim.DiaCatalogSourceSelector}} is {{selectSources}}. Source selectors should have the same API to facilitate using as swappable subtasks in {{ImageDifferenceTask}}.    The simplest solution is to rename the method in {{DiaCatalogSourceSelector}} to {{selectStars}}.  ",NULL
DM-3953,"Enhance pixel flags measurement algorithm","The goal of the pixel flags measurement algorithm is to reduce the image mask information around a source.  The current algorithm does this on two scales: the 3x3 pixels around the peak, and the entire source footprint.  I suggest that these are merely the extrema of interesting scales, and that the algorithm should be updated to record all mask bits on additional length scales.    As an example of why this might be necessary, consider the {{CLIPPED}} mask bit, which is set when pixels are rejected from one of the input images to a coadd, signalling that the {{CoaddPsf}} may be bad at that point.  Simply detecting it within the source footprint does not mean that my source may be adversely affected (as the footprint may extend beyond the important pixels of an object), but not detecting it within the 3x3 around the peak doesn't mean that my source has not been adversely affected (as the source may be extended).      I suggest recording the bits present within each of the following scales:  * 3x3 around the peak (as currently);  * pixels within a footprint containing a large fraction (e.g., 95%) of the flux of the PSF;  * pixels within a fixed aperture (e.g., 2 arcsec); and  * pixels within the source footprint (as currently).",NULL
DM-396,"Investigate observing status during HTCondor Processing","It would be worth while to investigate the best way to check on status of running jobs during HTCondor processing.  There are some tools to do this within Pegasus, and if we intend to use it, they would be useful, but it still needs to be investigated.  During a previous Data Challenge, we were able to tap into the job office by observing the DM Messages (ctrl_events) that were being sent while it operated.  We were able to use this to build a web page display of processing while it happened.  This was very useful because were able to observe jobs and submissions directly, and see that things were either working, failing, or taking to long to process.   Without this, we had to go through the log/output files by hand before realizing things were going sideways. ",NULL
DM-397,"Opaque error message from processCcdSdss.py","After downloading SDSS DR7 data products to my local machine, and setting up pipe_base, obs_sdss, etc., and building a repository with genInputRegistry.py, I attempted to run:   % processCcdSdss.py $DATA_DIR/ --id run=1755 filter=r camcol=3 field=411 --output $DEMO_DIR/calexp_dir --configfile $DEMO_DIR/processConfig.py  where $DATA_DIR points to my repository.   The processing terminated (error log attached). At Paul's helpful suggestion, I noticed that the ""psField"" data product was absent; once supplied the task completed normally. The problem here is that AFAIK there is no tool to validate that a repository is complete and correct for a given camera, and the resulting error message from running the processCcd task is obtuse. ",NULL
DM-3970,"ChoiceField validation includes duplicate in error message","For example:  {code}  lsst.pex.config.config.FieldValidationError: ChoiceField 'op' failed validation: Value foo is not allowed.          Allowed values: {mul, mul, add, sub, div, None}  {code}  (Note the repeated {{mul}}).",NULL
DM-3972,"Convert Qserv restrictor names to const int in C++ code","Qserv restrictors should be converted from string to int in Qserv C++ code for more efficiency and better maintenance.",NULL
DM-398,"ActiveMQ configuration into a git repository","The ActiveMQ configuration files which are changed from the standard ActiveMQ download are only on the system itself.  They should be in a git repository so we can track changes.   ",NULL
DM-3982,"CalibrateTask is incompatible with older astrometry tasks","[~lauren] and I recently discovered an incompatibility between {{CalibrateTask}}'s schema-handling and the older astrometry tasks, such as {{ANetAstrometryTask}} (the problem affects any astrometry task that utilizes its {{schema}} constructor argument).    The problem is that astrometry is called twice in {{CalibrateTask}}:   - Just before PSF estimation, in order to get matches to support {{CatalogStarSelector}}.  This call is passed {{sources1}}, a catalog containing only the initial, pre-PSF measurements.   - After the second measurement stage, to determine the {{Wcs}} and get matches to feed {{PhotoCal}}. This call is passed {{sources}}.    The problem is that {{sources.schema != sources1.schema}}, and the astrometry subtask is initialized with {{schema1 == sources1.schema}}.  So if the astrometry task needs to fields it added to the schema (the default {{AstrometryTask}} does not), it will likely fail in the second run, as it's being given a catalog that doesn't correspond to the schema it was initialized with.    Comments in the code indicate that there's a desire that future {{AstrometryTask}} s will not take a schema argument, and that would mean that they could be called repeatedly with different schemas (as we are doing).  I'm not sure that's viable; I think it'd probably be appropriate for {{AstrometryTask}} to set flags indicated the sources it's using (or considered using), as the other calibrate subtasks do.    Instead, I think we're probably best off creating two separate astrometry subtasks, each with a different schema, and using the appropriate subtask for each input/output catalog.    Clearly we need to come up with a better solution when in the upcoming {{CalibrateTask}} redesign; two subtasks is obviously clunky.  Whether this is high-priority before then depends on how much we're dependent on {{ANetAstrometryTask}} as a fallback option, and I haven't been following that conversation closely.",NULL
DM-399,"HTCondor local configuration changes should be in git","The HTCondor local configuration files are only on the systems they operate on.  They should be in a git repository so we can track changes. ",NULL
DM-3995,"Provide vSphere licensing quote","Provided Josh Hobblitt quote for vSphere licensing.",NULL
DM-4002,"Add doc dir and main.dox to obs_test","No obs_* packages have a doc dir. I suggest starting with obs_test since it is perhaps less obvious what it is used for than the others and in some ways acts as a template.",NULL
DM-4006,"Disable unicode support in boost","If the {{libicu-devel}} package is installed on a Linux system {{boost}} will enable Unicode support. This can lead to binary incompatibility issues when moving between Linux flavors where {{libicu.so}} can have slightly different major version variants that are not binary compatible. Since Unicode is not currently required by the stack the suggestion is to disable Unicode in {{boost}} to ensure that the stack builds are more predictable.",NULL
DM-4008,"Switch from boost::shared_ptr to std::shared_ptr","Switch from boost::shared_ptr to std::shared_ptr. This also requires switching boost::make_shared to std::make_shared.    This includes  • In utils p_lsstSwig.i change from {{%include ""boost_shared_ptr.i""}} to {{%include ""std_shared_ptr.i""}}. Note that the shared_vec macro defined in p_lsstSwig.i appears to not need any changes (though I do wonder if it is still required).  • Change the PTR and CONST_PTR macros  • Change .cc, .h and .i files; I found 1093 matches across 344 files (excluding qserv files, since qserv already switched)    Note that qserv already switched",NULL
DM-401,"lsstsw/etc/settings.cfg.sh  doesn't prevent inadvertant updates to git repo.","RE: lsstsw/etc/settings.cfg.sh  > >     if [[ $USER != 'lsstsw' && $(hostname) != >     'lsst-dev.ncsa.illinois.edu <http://lsst-dev.ncsa.illinois.edu>' ]]; >     then >             export NOPUSH=1 >     fi >  should use || instead of && .   This will ensure that only user 'lsstsw' running on lsst-dev will be able to git-push upstream changes to the  production stack's versiondb.",NULL
DM-4010,"db tests randomly fail with python egg installation error","[~jgates] reports a {{db}} test failure when attempting to extract files from the SQLAlchemy egg. This seems to be identical to the problem reported in DM-2752 but with a different package involved.",NULL
DM-4012,"DbStorage_*.py tests fail","A fresh    {code}eups distrib install --tag 2015_09 dax_webserv{code}    on lsst-dev    gives me    {code}  [ 39/59 ] daf_persistence 11.0-1-g7418c06 ...    ***** error: from /home/becla/stack/EupsBuildDir/Linux64/daf_persistence-11.0-1-g7418c06/build.log:  c++ -o python/lsst/daf/persistence/_persistenceLib.so -Wl,-rpath-link -Wl,/nfs/home/becla/stack/EupsBuildDir/Linux64/daf_persistence-11.0-1-g7418c06/daf_persistence-11.0-1-g7418c06/lib:/home/becla/stack/Linux64/pex_policy/11.0+1/lib:/home/becla/stack/Linux64/pex_logging/11.0+1/lib:/home/becla/stack/Linux64/dax_metaserv/11.0-1-g26026e2+6/lib:/home/becla/stack/Linux64/dax_dbserv/11.0+7/lib:/home/becla/stack/Linux64/daf_base/11.0+1/lib:/home/becla/stack/Linux64/utils/11.0-1-g47edd16/lib:/home/becla/stack/Linux64/pex_exceptions/10.0+46/lib:/home/becla/stack/Linux64/log/11.0/lib:/home/becla/stack/Linux64/dax_webservcommon/11.0/lib:/home/becla/stack/Linux64/base/11.0/lib:/home/becla/stack/Linux64/log4cxx/0.10.0.lsst5/lib:/home/becla/stack/Linux64/astrometry_net/0.50.1+11/lib:/home/becla/stack/Linux64/wcslib/4.14.lsst1+3/lib:/home/becla/stack/Linux64/boost/1.59.lsst1/lib:/home/becla/stack/Linux64/apr_util/1.3.4.lsst2/lib:/home/becla/stack/Linux64/mysqlclient/5.1.73.lsst2/lib:/home/becla/stack/Linux64/minuit2/5.28.00.lsst1/lib:/home/becla/stack/Linux64/gsl/1.16.lsst2/lib:/home/becla/stack/Linux64/fftw/3.3.4.lsst1/lib:/home/becla/stack/Linux64/cfitsio/3360.lsst3/lib:/home/becla/stack/Linux64/apr/1.3.3.lsst2/lib:/opt/rh/devtoolset-3/root/usr/lib64:/opt/rh/devtoolset-3/root/usr/lib -shared python/lsst/daf/persistence/persistenceLib_wrap.os -Llib -L/nfs/home/becla/stack/Linux64/mysqlclient/5.1.73.lsst2/lib -L/nfs/home/becla/stack/Linux64/pex_policy/11.0+1/lib -L/nfs/home/becla/stack/Linux64/pex_logging/11.0+1/lib -L/nfs/home/becla/stack/Linux64/daf_base/11.0+1/lib -L/nfs/home/becla/stack/Linux64/utils/11.0-1-g47edd16/lib -L/nfs/home/becla/stack/Linux64/pex_exceptions/10.0+46/lib -L/nfs/home/becla/stack/Linux64/base/11.0/lib -L/nfs/home/becla/stack/Linux64/boost/1.59.lsst1/lib -L/home/becla/stack/Linux64/anaconda/2.2.0/lib/python2.7/config -ldaf_persistence -lboost_serialization -lmysqlclient_r -lpex_policy -lpex_logging -lboost_filesystem -lboost_system -ldaf_base -lutils -lpex_exceptions -lbase -lboost_regex -lpthread -lutil -ldl -lpython2.7  /home/becla/stack/Linux64/anaconda/2.2.0/lib/python2.7/config/libpython2.7.a(posixmodule.o): In function `posix_tmpnam':  -------src-dir-------/Python-2.7.9/./Modules/posixmodule.c:7575: warning: the use of `tmpnam_r' is dangerous, better use `mkstemp'  /home/becla/stack/Linux64/anaconda/2.2.0/lib/python2.7/config/libpython2.7.a(posixmodule.o): In function `posix_tempnam':  -------src-dir-------/Python-2.7.9/./Modules/posixmodule.c:7522: warning: the use of `tempnam' is dangerous, better use `mkstemp'  running tests/Persistence_1.py... running tests/DbStorage_2.py... running tests/LogicalLocation.py... running tests/DbStorage_1.py... running tests/mapperImport.py... running tests/butlerSubset.py... running tests/butlerPickle.py... running tests/mapper.py... running tests/DbAuth.py... running tests/Persistence_2.py... running tests/butlerAlias.py... passed  passed  passed  passed  passed  passed  passed  passed  failed  failed  passed  2 tests failed  {code}    The tests that failed:    {code}  cat DbStorage_1.py.failed  tests/DbStorage_1.py    1444064938486882048  Traceback (most recent call last):  File ""tests/DbStorage_1.py"", line 88, in <module>  assert db.getColumnByPosDouble(0) == 1.23456  AssertionError  {code}    {code}  cat DbStorage_2.py.failed  tests/DbStorage_2.py    1444064938486882048  Traceback (most recent call last):  File ""tests/DbStorage_2.py"", line 97, in <module>  assert not db.next()  AssertionError  {code}    I cc'ed Tim - maybe it is similar to the intermittent problems we were seeing with sqlalchemy / db?",NULL
DM-4018,"boost-1.59.lsst1 doesn't build with gcc-4.9 on debian:jessie","Next error happen on debian/jessie with gcc-4.9, running in Docker:    {code}  gcc.compile.c++ bin.v2/libs/log/build/gcc-4.9.2/release/link-static/log-api-unix/threading-multi/formatter_parser.o  g++: internal compiler error: Killed (program cc1plus)  Please submit a full bug report,  with preprocessed source if appropriate.  See <file:///usr/share/doc/gcc-4.9/README.Bugs> for instructions.        ""g++""  -ftemplate-depth-128 -O3 -finline-functions -Wno-inline -Wall -pthread -m64 -fno-strict-aliasing -ftemplate-depth-1024 -fvisibility=hidden -std=c++11 -DBOOST_ALL_NO_LIB=1 -DBOOST_CHRONO_STATIC_LINK=1   -DBOOST_FILESYSTEM_STATIC_LINK=1 -DBOOST_LOG_SETUP_BUILDING_THE_LIB=1 -DBOOST_LOG_USE_AVX2 -DBOOST_LOG_USE_NATIVE_SYSLOG -DBOOST_LOG_USE_SSSE3 -DBOOST_LOG_WITHOUT_EVENT_LOG -DBOOST_SPIRIT_USE_PHOENIX_V3=1 -DBOOS  T_SYSTEM_NO_DEPRECATED -DBOOST_SYSTEM_STATIC_LINK=1 -DBOOST_THREAD_BUILD_LIB=1 -DBOOST_THREAD_DONT_USE_CHRONO=1 -DBOOST_THREAD_POSIX -DBOOST_THREAD_USE_LIB=1 -DDATE_TIME_INLINE -DNDEBUG -D_GNU_SOURCE=1 -D_XOPEN_  SOURCE=600  -I""."" -c -o ""bin.v2/libs/log/build/gcc-4.9.2/release/link-static/log-api-unix/threading-multi/formatter_parser.o"" ""libs/log/src/formatter_parser.cpp""    ...failed gcc.compile.c++ bin.v2/libs/log/build/gcc-4.9.2/release/link-static/log-api-unix/threading-multi/formatter_parser.o...  {code}",NULL
DM-402,"lsstsw/bin/rebuild is too verbose for logs destined for real-time web display ala buildbot","The elapsed-time meter on curl operations is extremely user-unfriendly on the larger packages (e.g. afwdata).  During testing of DM-271, the following was successfully used in lsstsw/bin/deploy  but not permanently installed: {code:bash} diff --git a/bin/deploy b/bin/deploy index 8729ad2..762f3d7 100755 --- a/bin/deploy +++ b/bin/deploy @@ -15,10 +15,13 @@ export PATH=""$LSSTSW/anaconda/bin:$PATH"" export PATH=""$LSSTSW/lfs/bin:$PATH"" export PATH=""$LSSTSW/bin:$PATH"" +unset ELAPSED_METER +test -t 2 && ELAPSED_METER=""-#"" + test -f ""$LSSTSW/anaconda/.deployed"" || ( # Anaconda echo ""::: Deploying Anaconda"" cd sources - curl -# -L -O http://repo.continuum.io/archive/Anaconda-1.8.0-Linux-x86_64.sh + curl $ELAPSED_METER -L -O http://repo.continuum.io/archive/Anaconda-1.8.0-Linux-x86_64.sh bash Anaconda-1.8.0-Linux-x86_64.sh -b -p ""$LSSTSW/anaconda"" touch ""$LSSTSW/anaconda/.deployed"" ) @@ -26,8 +29,8 @@ test -f ""$LSSTSW/anaconda/.deployed"" || ( # Anaconda test -f ""$LSSTSW/lfs/.git.deployed"" || ( # git echo ""::: Deploying git"" cd sources - curl -# -L -O https://git-core.googlecode.com/files/git-1.9.0.tar.gz - curl -# -L -O https://git-core.googlecode.com/files/git-manpages-1.9.0.tar.gz + curl $ELAPSED_METER -L -O https://git-core.googlecode.com/files/git-1.9.0.tar.gz + curl $ELAPSED_METER -L -O https://git-core.googlecode.com/files/git-manpages-1.9.0.tar.gz tar xzf git-1.9.0.tar.gz {code}",NULL
DM-4020,"Remove #!/usr/bin/env from pipe_tasks library code","Many tasks in pipe_tasks start with #!/usr/bin/env..., yet are not executable.",NULL
DM-403,"add maf packages to dist server","We are trying to make the catalogs and operations packages installable via the eups distrib install mechanism.  This involves some third party packages and a few pure python lsst packages.  The third party packages are: contrib/pyephem contrib/pymssql contrib/palpy contrib/healpy  Three repositories will also need to be renamed to adhere to the package name == repository name requirement: LSST/sims/catalogs/generation ==> LSST/sims_catalogs_generation LSST/sims/catalogs/measures ==> LSST/sims_catalogs_measures LSST/sims/operations/maf ==> LSST/sims_operations_maf",NULL
DM-4033,"modelfit_CModel code occassionally tries to use uninitialized pointer","startDetermineFitRegion returns an uninitialized PTR(footprint) where it handles the condition:   !footprint.contains(pixel).  This subsequently causes an assert.",NULL
DM-4037,"Require non-empty doc string for config parameters","Currently, {{doc=None}} is allowed for config parameters.  We should not be allowing for configuration parameters with no description, so the existence of a doc string should be imposed.",NULL
DM-4038,"Update boost usage documentation","With the agreement to switch many of our boost usage to the modern C++11 equivalent, we also need to update the recommendations for how {{boost}} is used by developers. This ticket involves transferring the trac document from [https://dev.lsstcorp.org/trac/wiki/TCT/BoostUsageProposal] to Confluence (or where the current documentation lives) and updating it to reflect the outcome of RFC-100.",NULL
DM-404,"lsst/bin/rebuild does not accept parameters specifying the ordered list of branchs desired","Although the underlying software of 'rebuild' allows specification of a sequence of '-refs <branch>' indicating the search order of alternate branches to use during git repo recovery,  the forward facing 'rebuild' command does not include them.  A trivial approach was taken in order to test feasibility of user specification using buildbot's interface. In the BB web interface, the user specifies both the  parameter name (ref)  and  the parameter value (the blank separated strings of branches to use). For example:  {code}          param[name] = ref          param[value] = tickets/DM-271 tickets/DM-23 master {code}  This was then massaged by the buildbot interface to provide the following  string to  'rebuild':  {code} ""--ref tickets/DM-271 --ref tickets/DM-23 --ref master"" {code} During feasibility testing, 'rebuild' didn't accept any input parameters so a simple  {code} REF_LIST=$@ {code} sufficed to capture the refs and then use them via: {code} lsst-build prepare $REF_LIST  --exclusion-map=..... {code}",NULL
DM-4041,"Improve installation and cron-like configuration for community_mailbot","Per review comments for DM-3690, the installation and configuration of the community_mailbot can be improved    - avoid the runner script by using the full virtual environment path of the installed script  - investigate alternatives to relying on cron",NULL
DM-4060,"GPFS Tuning","Coordinating with storage expertise to test and tune GPFS from Condo to dev environment, NFS from GPFS into dev environment and GPFS/NFS into OpenStack.",NULL
DM-4066,"Please update Jenkins' numpy to version 1.10","DM-4063 shows a breakage induced by numpy 1.10. Once that ticket is fixed, please update the numpy in Jenkins so this problem is being properly tested.    Meanwhile we will rely on the kindness of others to test DM-4063",NULL
DM-4072,"master not recognized as a valid tag","I can't seem to clone a tag to master anymore, and in fact something has gone wonky with the tag ""master"":  {code}  localhost$ eups tags --clone b951 master  eups tags: Unsupported tag(s): master    # I can't even list master anymore:  localhost$ eups list -t master  Ignoring unsupported tags in VRO: master  eups list: Unsupported tag(s): master    # and indeed master is not listed in global.tags; should it be?  localhost$ echo $EUPS_PATH  /Users/rowen/LSST/lsstsw/stack  localhost$ cat /Users/rowen/LSST/lsstsw/stack/ups_db/global.tags   current stable latest b860 b861 b862 b863 b864 b865 b866 b867 b868 b869 b870 b871 b872 b873 b874 b875 ... b950 b951    # and of course there are packages tagged b951  localhost$ eups list -t b951  afw                   11.0-5-g97168e0 	b950 b951  afwdata               8.0.0.1+29 	b949 b948 b946 b945 b944 b943 b941 b950 b951 current  astrometry_net        0.50.1+8   	b949 b948 b946 b945 b944 b943 b941 b950 b934 b951 b935 current b933  ...    # this is with eups version 1.5.8  localhost$ eups --version  EUPS Version: 1.5.8  {code}    Any idea how my stack got into this state and how best to fix it?  ",NULL
DM-4073,"investigate whether slowdown in numpy recarray access affects DM code","Numpy 1.10.1 significantly slows down access to numpy recarrays using field names    https://github.com/numpy/numpy/issues/6467    This impacts the sims code.  It is unclear whether this is something DM has to worry about.  This ticket is to investigate whether or not DM has a problem.    Note: the discussion in the numpy issue claims that a fix is on the way.",NULL
DM-4077,"Support external processing inputs in mapper and argument parser","RFC-95 proposes new features in the Butler (probably actually implemented in {{CameraMapper}}) and the {{CmdLineTask}} argument parser to support running the pipeline on intermediate data products produced by external pipeline, such as the DECam community pipeline.    This issue represents the implementation of that feature.",NULL
DM-4088,"Update logger to use db/SqlAlchemy","The Logger.py file uses df_persistence to insert log messages.  Switch this to use db/SqlAlchemy.",NULL
DM-4091,"Support registry-less repos in obs_decam, CameraMapper","When trying to load decam images from a registry-less repository, there are two places where the mapper tries to use the registry. These can be avoided, since the information it is looking for can be loaded out of the Exposure itself.    {{DecamMapper._extractDetectorName()}} searches for the registry, but the function that calls this is {{CameraMapper._setCcdDetector()}}, which has access to the Exposure that contains the detector name already. Overriding {{_setCcdDetector()}} in obs_decam will avoid this registry lookup.    {{CameraMapper._setFilter()}} uses the registry, but if we are loading an already-existing file, the filter has already been set. I think we can skip re-setting the filter in that case.    ",NULL
DM-4096,"Please unify the various scripts for displaying cameras","{{afw}} contains the script {{showCamera.py}} which claims to ""show the layout of CCDs in a camera"".    {{obs_subaru}} also contains a script called {{showCamera.py}} which seems to do much the same job but including distortion. It doesn't work with the current version of {{cameraGeom}}.     {{obs_subaru}} also contains a script called {{displayCamera.py}} which claims to ""display the Subaru camera"". It makes it possible to plot the focal plane in either DS9 or Matplotlib (the latter looking very similar to the output of {{afw}}'s {{showCamera.py}}).    Please eliminate the apparent duplication between these scripts. If all three are really necessary, document why; if one or more can be removed with no loss of functionality, do so.",NULL
DM-4097,"Document the fact that interpolateOverDefects does not use the PSF","As [described on Discourse|https://community.lsst.org/t/use-of-psf-when-interpolating-over-defects/259], {{meas_algorithm}}'s {{interpolateOverDefects}} takes a {{Psf}} as one of its arguments but does not use it. This behaviour is intentional, but should be clearly documented, both in the function itself and flowing up to relevant task documentation.",NULL
DM-4101,"Compiler warnings in Crosstalk.cc","I see the following compiler warnings when building obs_subaru:  {code}  src/Crosstalk.cc:95:66: warning: explicitly assigning a variable of type 'std::size_t' (aka 'unsigned long') to itself [-Wself-assign]              PixelVector const& ctx1List = getCrosstalkX1(i, nxAmp=nxAmp);                                                              ~~~~~^~~~~~  src/Crosstalk.cc:96:66: warning: explicitly assigning a variable of type 'std::size_t' (aka 'unsigned long') to itself [-Wself-assign]              PixelVector const& ctx2List = getCrosstalkX2(i, nxAmp=nxAmp);                                                              ~~~~~^~~~~~  2 warnings generated.  {code}  ",NULL
DM-4103,"clone of ""public"" repo still prompts for lfs server credentials","$ git clone https://github.com/jhoblitt/lfstest.git  Cloning into 'lfstest'...  remote: Counting objects: 16, done.  remote: Compressing objects: 100% (15/15), done.  remote: Total 16 (delta 0), reused 16 (delta 0), pack-reused 0  Unpacking objects: 100% (16/16), done.  Checking connectivity... done.  Downloading atk-2.8.0-4.el7.x86_64.rpm (233.25 KB)  Username for 'https://lfstest3.s3-us-west-2.amazonaws.com': jhoblitt  Password for 'https://jhoblitt@lfstest3.s3-us-west-2.amazonaws.com':   ",NULL
DM-4104,"Document that <<= is deprecated","Update our documentation to replace Image {{<<=}} pixel copy operator with {{[:] =}} for Python code and {{set(rhs)}} for C++ code (or whatever RFC-102 decides), with a note that {{<<=}} exists but is deprecated.",NULL
DM-4106,"github user authentication should be cached","Currently, all POST operations result in a github API call.  This can result in running into the github 5K/hour API call limit on a per user basis.  It also adds latency to each lfs API call.  A workaround would be to cache user authentication on a short time frame (perhaps 15min) .    {code:java}  App 24366 stderr: 2015-10-16 13:24:48 - Octokit::TooManyRequests - GET https://api.github.com/user: 403 - API rate limit exceeded for jhoblitt. // See: https://developer.github.com/v3/#rate-limiting:  {code}  ",NULL
DM-4108,"Aws.config should not be directly modified when not using ceph","The current behavior breaks the s3 endpoint select and results in an error unless {{LFS_CEPH_ENDPOINT}} is defined.    A simple conditional should fix it:  Eg.:    {code:java}  if GitLfsS3::Application.settings.ceph_s3    Aws.config.update(      endpoint: ENV['LFS_CEPH_ENDPOINT'],      access_key_id: ENV['AWS_ACCESS_KEY_ID'],      secret_access_key: ENV['AWS_SECRET_ACCESS_KEY'],      force_path_style: true,      region: 'us-east-1',      # ssl_ca_bundle: '/usr/local/etc/openssl/cert.pem' # Required for brew install on a mac.    )       end    {code}  ",NULL
DM-4109,"lfs client unable to resync with server?","If the s3 bucket used by the lfs server is manually deleted, the lfs client seems to be unable to determine that it needs to repush existing objects.  {{lfs fsck}} {{lfs push ...}}, etc. seems to be oblivious to the state change.",NULL
DM-4110,"need procedure for changing lfs ""remotes""","This is essentially the same problem as DM-4109, but a different use case.    {code:java}  $ git clone https://github.com/jhoblitt/lfstest.git  Cloning into 'lfstest'...  remote: Counting objects: 16, done.  remote: Compressing objects: 100% (15/15), done.  remote: Total 16 (delta 0), reused 16 (delta 0), pack-reused 0  Unpacking objects: 100% (16/16), done.  Checking connectivity... done.  Downloading atk-2.8.0-4.el7.x86_64.rpm (233.25 KB)  Downloading augeas-1.1.0-17.el7.x86_64.rpm (37.20 KB)  Downloading augeas-libs-1.1.0-17.el7.x86_64.rpm (331.73 KB)  Downloading bison-2.7-4.el7.x86_64.rpm (578.00 KB)  Downloading blas-3.4.2-4.el7.x86_64.rpm (398.22 KB)  Downloading bzip2-devel-1.0.6-12.el7.x86_64.rpm (217.88 KB)  Downloading cairo-1.12.14-6.el7.x86_64.rpm (696.79 KB)  Downloading cmake-2.8.11-4.el7.x86_64.rpm (6.69 MB)  Downloading cpp-4.8.3-9.el7.x86_64.rpm (5.93 MB)  Downloading flex-2.5.37-3.el7.x86_64.rpm (292.45 KB)  Downloading fontconfig-2.10.95-7.el7.x86_64.rpm (227.97 KB)  Downloading fontpackages-filesystem-1.44-8.el7.noarch.rpm (9.88 KB)  Checking out files: 100% (14/14), done.  $ git config --file .gitconfig lfs.url https://git-lfs.lsst.codes  $ git lfs push origin master  $ git lfs status  On branch master  Git LFS objects to be pushed to origin/master:      Git LFS objects to be committed:      Git LFS objects not staged for commit:      $ git lfs fsck  Git LFS fsck OK    {code}    ",NULL
DM-4111,"doc & demostrate complete backup and restore procedure","Backups could be done via git-lfs clone/push if DM-4110 is resolved, direct object replication, or complete backup of the ceph object storage.  Restore from complete loss must be demonstrated.",NULL
DM-4114,"pyflakes warnings on obs_subaru","Two files in obs_subaru make the pyflakes python linter very unhappy, and the right fixes may take a bit of thought:  - python/.../subaru/ccdTesting.py  - python/.../subaru/crosstalkYagi.py    Also, I suspect python/.../subaru/isr.py.orig should be deleted  ",NULL
DM-4126,"Include examples in CI","Several packages in the stack include example code which is neither included as part of the build nor exercised in the unit tests. See, for example [{{afw}}|https://github.com/lsst/afw/tree/master/examples] and [{{pipe_tasks}}|https://github.com/lsst/pipe_tasks/tree/master/examples]. This means it's all too easy for folks to forget to update these examples when they make changes elsewhere. It would be great if we could execute the examples and demonstrate that they still work as part of the CI system.",NULL
DM-4128,"errors durinig lfs fetch do not cause a non-zero exit status","If any error of any kind occurs during an lfs fetch operation (git clone/etc.) git still exits with status 0.  This is a show stopper for automated tasks, such as lsst_build, which are reliant on the exit status to indicate problems.    This may already be fixed on the master branch for the lfs client.    https://github.com/github/git-lfs/issues/731",NULL
DM-4130,"--id does not iterate over the data id's as expecteded ","When using {{ingestCatalog.py}} (from daf_ingest) I would expect it iterates over all visits and ccdnums available in the output data repository    The command line used is the the following:  {code}  ingestCatalog.py bulge/output  --host lsst10.ncsa.illinois.edu --database afausti_S12_sdss_u_afausti_2015_1019_170306 --table bulgeSources --dstype src --id  {code}  and I got:  {code}  raise RuntimeError, ""No unique lookup for %s from %s: %d matches"" % (newProps, newId, len(lookups))  RuntimeError: No unique lookup for ['ccdnum'] from {'visit': 205343}: 61 matches  {code}  I need at least {{""--id ccdnum=1..61""}} to iterate over the visits     Talking with K-T he said:    {quote}  K-T Lim·Oct-19 6:56 PM: Hm.  You shouldn't, I don't think.  This looks like the *levels problem* we were having before.  {quote}",NULL
DM-4132,"Support afw.PARENT and afw.LOCAL in afw image slicing","We support {{\[..., ...\]}} as syntactic sugar for BBox operations with images, for example  {code}  L = afwImage.ImageF(100, 200)  R = afwImage.ImageF(100, 100)  L[10:20, 100:130] = R[30:40, 20:40]  {code}  or  {code}  Lbox = afwGeom.BoxI(afwGeom.PointI(10, 100), afwGeom.PointI(20, 130))  Rbox = afwGeom.BoxI(afwGeom.PointI(30, 20), afwGeom.PointI(40, 40))    L[Lbox] = R[Rbox]  {code}  However, there is no syntax to specify whether LOCAL or PARENT BBoxes should be used.  Please support e.g.  {code}  L[10:20, 100:130, afwImage.PARENT] = R[30:40, 20:40, afwImage.LOCAL]  L[Lbox, afwImage.PARENT] = R[Rbox, afwImage.PARENT]  {code}  with default afwImage.PARENT as the default. The code to do this is all in {{python/lsst/afw/image/imageLib.i}}).  Note that there is no support for a third index, so I don't think that this is a dangerous restriction on future extensions.",NULL
DM-4139,"broken code in meas_algorithms utils","meas.algorithms.utils has some broken code, including:    {{namp}} is not defined here (or anywhere) in {{showPsfResiduals}}:  {code}          if showAmps:              nx, ny = namp  {code}    {{dmin}} may be unassigned in {{showPsfCandidates}}:  {code}                      if len(catalog) == 1:                          source = catalog[0]                      else:               # more than one source; find the once closest to (xc, yc)                          for i, s in enumerate(catalog):                              d = numpy.hypot(xc - s.getX(), yc - s.getY())                              if i == 0 or d < dmin:                                  source, dmin = s, d  {code}    Furthermore many variables are assigned and then never used. Try a linter such as pyflakes.",NULL
DM-414,"Event monitor enhancements","This is work that can be done for the event monitor.",NULL
DM-4140,"For registry-free butler, look up information in related data type.","description from DM-3591: Read information (in particular the observation time and length) out of an input dataset's file representation in order to provide rendezvous with calibration data in another repository (that does have a registry). Today, that read is handled by genInputRegistry.py so that the butler doesn't need to look into the dataset itself. If there's no registry, such a read will be necessary.",NULL
DM-4141,"cmdLineTasks should provide proper unix return codes","When a cmdLineTask fails it doesn't appear to return a non-0 exit code to the shell, making it hard to write shell scripts that chain commands together.    Please fix this.    E.g.  {code}  $ bin/assembleCoadd.py /lustre/Subaru/SSP --rerun yasuda/SSP3.8.5_20150810_cosmos:rhl/brightObjectMasks --id tract=9813 patch=5,5 filter=HSC-I --selectId ccd=0..103 visit=1238..1246:2 -c doMaskBrightObjects=True && echo ""Success""   ...  2015-10-22T02:44:13: assembleCoadd FATAL: Failed in task initialization:     Your Eups versions have changed.  The difference is:   ---   +++   @@ -48 +48 @@  -obs_subaru                     HSC-3.11.0a_hsc  /data1a/ana/products2014/Linux64/obs_subaru/HSC-3.11.0a_hsc  +obs_subaru                     LOCAL:/home/rhl/LSST/obs/subaru rev:ef3c892f clean-working-copy  @@ -55 +55 @@  -pipe_tasks                     LOCAL:/home/rhl/LSST/pipe/tasks-HSC-1342 rev:84b0f3c4 2 files changed, 5 insertions(+), 6 deletions(-)  +pipe_tasks                     LOCAL:/home/rhl/LSST/pipe/tasks-HSC-1342 rev:9e8ed18b 2 files changed, 47 insertions(+), 42 deletions(-)  @@ -60 +59,0 @@  -pyflakes                       git              /home/rhl/Src/pyflakes  Please run with --clobber-config to override    Success  {code}",NULL
DM-4149,"Condo Payments","Discussion of yearly fees, remaining term, plan forward.",NULL
DM-4158,"Allow configuring more statistical options for assembleCoadds.py ","The assembleCoadd.py task has a configuration option doSigmaClip which chooses between MEAN and MEANCLIP. Please replace this with an option to specify the algorithm to be used. Note that afwMath.stringToStatisticsProperty can be used to convert a string to an enum value. In particular, this would allow me to specify a MEDIAN stack if desired (e.g. to make pretty RGB images)  ",NULL
DM-4159,"It seems impossible to configure a configuration via a config option","We just came across a problem with configuration on the HSC side (https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1342).    There is an algorithm, {{flags.pixel}}, to set flags based on bits set in the Mask.  I just added an option to set a new bitplane (by default ""BRIGHT_OBJECT""), and when it's enabled you would also like to add flags such as {{flags.pixel.bright.object.any}} (the fact that the _ in the name becomes . is a different topic).    However, this appears to be impossible.  By the time the config parameter is parsed we've finished creating the measurement task, and its config is frozen so I cannot add the extra bits.  The workaround is to add it in a task override file (in this case in obs_subaru) but this is unsatisfactory as it's not just in a different file but a different package.",NULL
DM-416,"Convert event monitor to Python","Currently written in Java with the API exposed in a Jython interface. It would be better to re-write this in Python in order to take advantage of Python packages that Jython doesn’t support.   Moving to pure Python would also allow for the interfaces to be simplified, which would make scripts easier to write.",NULL
DM-4161,"Audit & update (or remove) meas_algorithms/shapelet","The code in [{{meas_algorithms:include/lsst/meas/algorithms/shapelet}}|https://github.com/lsst/meas_algorithms/tree/master/include/lsst/meas/algorithms/shapelet] and [{{meas_algorithms:src/shapelet}}|https://github.com/lsst/meas_algorithms/tree/master/src/shapelet] has been almost untouched in several years, does not comply with our coding standards, does not seem to relate to the {{shapelet}} package (despite the overlap in name), and is not obviously used by any other code in the stack.    Please establish if this code is still relevant and useful. If it is, bring it up-to-date with current practices. If not, remove it.",NULL
DM-4163,"Implement a polynomial XYTransform","Please implement one or more polynomial versions of XYTransform, ncluding a 2-d Chebyshev. Use cases include warping (DM-4162) and describing CCD pixel non-uniformities.    I suspect the hard part will be making these invertible.",NULL
DM-4167,"create a singleFileMapper and make it work with a CmdLineTask","Be able to write a command line task that has a single image as an input (via the dataId most likely), do something to that image, and then have 2 or more outputs. e.g. produce an output image and an afw table. This should execute from the command line with 2 variations:  1. provide only the name of the input image (and have defaults for where the outputs would go)  2. provide path for each output to go to  The task needs to support running with a posix/sqlite registry as well as the above mechanism. (Will probably need to create a new mapper something like ""singleFileMapper"" and need a way to specify this. having a _mapper file is not ideal, would like a rule e.g. ""if no _mapper is found, try the singleFileMapper, or maybe create an command line argument to the pipe_base argument parser to indicate the singleFileMapper).",NULL
DM-4169,"Butler support for database-query dataset types","Add the ability to configure butler to point to a database server (this would be delivered as the repository root - so that will have to be changed to be more generic. it could be a URL, where the name, port number, etc are all encoded in the url), and a table in that server, and a template sql query that the dataIds would get filled into.    Regarding authorization for the connection in the URL, we might want to use our dbAuth class (in daf_persistence) for that.    The example is to refactor obs_sdss SelectSdssImageTask so that instead of talking directly to the database (see code), it would use the butler to query for the data it is getting out of the database. ",NULL
DM-4173,"Butler: add support for write-once-compare-same outputs","Want to be able to write to a file and be sure that the file is 1. locked and 2. if that content exists, make sure that it is the same (and if it's not the same then respond - no op, or throw)    Examples of target files are policy (soon to be ""formerly paf"", see DM-4171), and config, and schema files.    write function should look like the butler.put method, but also take a comparison method. Comparison could be optional and default to \_\_eq\_\_    (this existed in trac: https://dev.lsstcorp.org/trac/ticket/2789)",NULL
DM-418,"Implement multiple sources","Add the ability to take events from multiple sources simultaneously. Those sources could be from the same source type (DM Events/Messages, but different topics), or completely different sources, working together",NULL
DM-4182,"Butler: add dataset type that supports dataId list for bidirectional lookup of dataId to exposure ID","right now we can go from data ID to exposure ID, but not the other way.  there’s no dataset type of dataId in the butler right now. if we could retrieve dataIds as a butler python type that would help.    the mapping to support the bidirectional lookup gets hard coded lookup into cameraMapper subclasses.",NULL
DM-4186,"Internal discussion with Finance about process","Discussing process, invoicing, generating longer term purchasing contracts, tying purchase approvals through quotes and into iBuy (or other purchasing mechanism). Discussion of previously approved funds.",NULL
DM-4187,"Networking design","Discussions of how to integrate condo into new and existing infrastructure. ",NULL
DM-4198,"When building boost warn user if user-config.jam or site-config.jam exists","Building boost can fail if a user-config.jam or site-config.jam exist and have options which conflict with the lsst build configuration process. Introduce a warning message if either of these files are found to notify the user.",NULL
DM-4199,"Add a topic age filter to guard against forwarding old posts","There should be a setting on forward_discourse that ensures that topics older than X hours are not forwarded. This will ensure that even if the cache hasn’t been seeded, old topics won’t be forwarded.",NULL
DM-4200,"Change default table version to 1","This was supposed to be done in Sprint 2.  The work was done, but we are still waiting on tests for ip_diffim and possibly ap before integrating to master",NULL
DM-4203,"Query ""SELECT count(*) as COUNT FROM Object WHERE objectId IN (500)"" crash Qserv","Next error is printed in czar log:  {code:bash}  2015-10-28T14:07:35.572Z [0x7fa0edd05700] ERROR ccontrol.UserQueryFactory (core/modules/ccontrol/UserQueryFactory.cc:101) - Invalid query: ParseException:Parse error(ANTLR):unexpected token: COUNT:  {code}    And    {{SELECT count(*) as N FROM Object WHERE objectId IN (88, 23, 19, 77, 62, )}}  cause:  {code:bash}2015-10-28T14:35:22.675Z [0x7f01c6aeb700] ERROR ccontrol.UserQueryFactory (core/modules/ccontrol/UserQueryFactory.cc:101) - Invalid query: ParseException:Parse error(ANTLR):unexpected token: objectId:  {code}  ",NULL
DM-4204,"db.utils.listDbs() crashes","Trying to use one of {{listDbs()}} method in {{db.utils}} module causes crash:  {noformat}    File ""/usr/local/home/salnikov/dm-3192/lib/python/lsst/qserv/wmgr/dbMgr.py"", line 141, in listDbs      dbs = utils.listDbs(dbConn)    File ""/u2/salnikov/STACK/Linux64/db/1.1.0-2-gea74ebb/python/lsst/db/utils.py"", line 171, in listDbs      return inspect(engine).get_schema_names()  NameError: global name 'engine' is not defined  {noformat}    Looks like there is a misspelled variable name in that method.  ",NULL
DM-4205,"ChunkInventory crashes on inconsistent database","If someone drops database on worker without updating {{qservw_worker.Dbs}} table then on next restart of xrootd chunk inventory code crashes:  {noformat}  [2015-10-28T12:05:47.373-0500] [0x7fdc4179a720] INFO  root (core/modules/xrdsvc/SsiService.cc:72) - SsiService starting...  [2015-10-28T12:05:47.374-0500] [0x7fdc4179a720] DEBUG root (core/modules/wpublish/ChunkInventory.cc:80) - Launching query : SELECT db FROM qservw_worker.Dbs  [2015-10-28T12:05:47.376-0500] [0x7fdc4179a720] ERROR root (core/modules/wpublish/ChunkInventory.cc:164) - SQL error: Can't list tables for db qservTest_case01_qserv because the database does not exist.   xrootd: core/modules/wpublish/ChunkInventory.cc:165: void {anonymous}::doDb::operator()(const string&): Assertion `ok' failed.  {noformat}    It's worth making it a bit more robust.  ",NULL
DM-4207,"Remove stray pdb","There's a stray {{pdb}} in {{CoaddDataIdContainer.makeDataRefList}}.",NULL
DM-4211,"Prevent use of secondary index for SQL predicate ""objectId op val"" if op is not '='","Queries that involve secondary index but use  ""!="", "">="", ""<="", '<', '>' instead of ""="", or ""IN"" work incorrectly. Example ""select * from Object where objectId > 123"" will be treated as ""select * from Object where objectId = 123"".",NULL
DM-4216,"lfs 1.0.1/1.0.2 clone/pull hang indefinitely","1.0.1 on ec2 instance  {code}  [build0@jenkins-el6-1 ~]$ git lfs version  git-lfs/1.0.1 (GitHub; linux amd64; go 1.5.1; git a999185)  [build0@jenkins-el6-1 ~]$ time git clone -c filter.lfs.required -c filter.lfs.smudge='git-lfs smudge %f' -c filter.lfs.clean='git-lfs clean %f' -c credential.helper='!f() { cat > /dev/null; echo username=; echo password=; }; f' https://github.com/lsst/afwdata-cowboy.git   Cloning into 'afwdata-cowboy'...  remote: Counting objects: 411, done.  remote: Total 411 (delta 0), reused 0 (delta 0), pack-reused 411  Receiving objects: 100% (411/411), 54.88 KiB | 0 bytes/s, done.  Resolving deltas: 100% (51/51), done.  Checking connectivity... done.  Downloading CFHT/D2/sdss.dat (110.68 KB)      ^Cwarning: Clone succeeded, but checkout failed.  You can inspect what was checked out with 'git status'  and retry the checkout with 'git checkout -f HEAD'      Exiting because of ""interrupt"" signal.      real	5m53.733s  user	0m0.045s  sys	0m0.063s  {code}    1.0.2 on desktop in tucson  {code}  [master] ~/tmp $ git lfs version  git-lfs/1.0.2 (GitHub; linux amd64; go 1.5.1; git 0566698)  [master] ~/tmp $  time git clone -c filter.lfs.required -c filter.lfs.smudge='git-lfs smudge %f' -c filter.lfs.clean='git-lfs clean %f' -c credential.helper='!f() { cat > /dev/null; echo username=; echo password=; }; f' https://github.com/lsst/afwdata-cowboy.git  Cloning into 'afwdata-cowboy'...  remote: Counting objects: 411, done.  remote: Total 411 (delta 0), reused 0 (delta 0), pack-reused 411  Receiving objects: 100% (411/411), 54.88 KiB | 0 bytes/s, done.  Resolving deltas: 100% (51/51), done.  Checking connectivity... done.  Downloading CFHT/D2/sdss.dat (110.68 KB)    ^Cwarning: Clone succeeded, but checkout failed.  You can inspect what was checked out with 'git status'  and retry the checkout with 'git checkout -f HEAD'      Exiting because of ""interrupt"" signal.    real	3m20.946s  user	0m0.094s  sys	0m0.049s    {code}    1.0.0 on desktop in tucson  {code}  $ git clone -c credential.helper='!f() { cat > /dev/null; echo username=; echo password=; }; f' https://github.com/lsst/afwdata-cowboy.git afwdata-cowboy2  Cloning into 'afwdata-cowboy2'...  remote: Counting objects: 411, done.  remote: Total 411 (delta 0), reused 0 (delta 0), pack-reused 411  Receiving objects: 100% (411/411), 54.88 KiB | 0 bytes/s, done.  Resolving deltas: 100% (51/51), done.  Checking connectivity... done.  Downloading CFHT/D2/sdss.dat (110.68 KB)  Downloading CFHT/D2/template.dat (4.25 MB)  Downloading CFHT/D4/cal-53535-i-797722_1.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_1_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_2.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_2_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_3.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_3_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_4.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_4_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_5.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_5_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_6.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_6_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_7.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_7_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_8.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_8_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_9.fits (93.60 MB)  Downloading CFHT/D4/cal-53535-i-797722_9_tmpl.fits (84.34 MB)  Downloading CFHT/D4/cal-53535-i-797722_small_1.fits (658.12 KB)  Downloading CFHT/D4/cal-53535-i-797722_small_1_tmpl.fits (658.12 KB)  Downloading DC3a-Sim/sci/v26-e0/v26-e0-c011-a00.sci.fits (10.02 MB)  Downloading DC3a-Sim/sci/v26-e0/v26-e0-c011-a10.sci.fits (10.02 MB)  Downloading DC3a-Sim/sci/v5-e0/v5-e0-c011-a00.sci.fits (10.02 MB)  Downloading DC3a-Sim/sci/v5-e0/v5-e0-c011-a10.sci.fits (10.02 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C00.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C01.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C02.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C03.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C04.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C05.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C06.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C07.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C10.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C11.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C12.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C13.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C14.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C15.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C16.fits (9.86 MB)  Downloading ImSim/bias/v0/R23/S11/imsim_0_R23_S11_C17.fits (9.86 MB)  Downloading ImSim/calexp/v85408556-fr/R23/S11.fits (155.43 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C00.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C01.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C02.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C03.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C04.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C05.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C06.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C07.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C10.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C11.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C12.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C13.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C14.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C15.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C16.fits (9.86 MB)  Downloading ImSim/dark/v1/R23/S11/imsim_1_R23_S11_C17.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C00.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C01.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C02.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C03.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C04.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C05.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C06.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C07.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C10.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C11.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C12.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C13.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C14.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C15.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C16.fits (9.86 MB)  Downloading ImSim/flat/v2-fr/R23/S11/imsim_2_R23_S11_C17.fits (9.86 MB)  Downloading ImSim/icMatch/v85408556-fr/R23/S11.fits (19.69 KB)  Downloading ImSim/icSrc/v85408556-fr/R23/S11.boost (1.62 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C00.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C01.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C02.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C03.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C04.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C05.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C06.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C07.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C10.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C11.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C12.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C13.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C14.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C15.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C16.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s0/R23/S11/C17.fits (9.84 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C00.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C01.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C02.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C03.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C04.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C05.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C06.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C07.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C10.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C11.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C12.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C13.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C14.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C15.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C16.fits (9.83 MB)  Downloading ImSim/postISR/v85751839-fr/s1/R23/S11/C17.fits (9.83 MB)  Downloading ImSim/postISRCCD/v85751839-fr/s0/R23-S11.fits (155.39 MB)  Downloading ImSim/postISRCCD/v85751839-fr/s1/R23-S11.fits (155.38 MB)  Downloading ImSim/psf/v85408556-fr/R23/S11.boost (11.99 KB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C00_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C01_E000.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C02_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C03_E000.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C04_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C05_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C06_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C07_E000.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C10_E000.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C11_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C12_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C13_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C14_E000.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C15_E000.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C16_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E000/R23/S11/imsim_85408556_R23_S11_C17_E000.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C00_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C01_E001.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C02_E001.fits.gz (1.01 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C03_E001.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C04_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C05_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C06_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C07_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C10_E001.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C11_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C12_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C13_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C14_E001.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C15_E001.fits.gz (1.03 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C16_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85408556-fr/E001/R23/S11/imsim_85408556_R23_S11_C17_E001.fits.gz (1.02 MB)  Downloading ImSim/raw/v85751839-fr/E000/R23/S11/imsim_85751839_R23_S11_C00_E000.fits.gz (1.03 MB)  Downloading ImSim/raw/v85751839-fr/E001/R23/S11/imsim_85751839_R23_S11_C00_E001.fits.gz (1.03 MB)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C00.boost (306 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C01.boost (301 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C02.boost (301 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C03.boost (301 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C04.boost (307 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C05.boost (297 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C06.boost (300 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C07.boost (296 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C10.boost (305 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C11.boost (299 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C12.boost (300 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C13.boost (299 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C14.boost (299 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C15.boost (305 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C16.boost (301 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s0/R23/S11/C17.boost (298 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C00.boost (306 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C01.boost (308 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C02.boost (298 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C03.boost (299 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C04.boost (300 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C05.boost (301 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C06.boost (307 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C07.boost (300 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C10.boost (305 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C11.boost (307 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C12.boost (301 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C13.boost (300 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C14.boost (300 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C15.boost (305 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C16.boost (301 B)  Downloading ImSim/sdqaAmp/v85408556-fr/s1/R23/S11/C17.boost (301 B)  Downloading ImSim/sdqaCcd/v85408556-fr/s0/R23/S11.boost (425 B)  Downloading ImSim/sdqaCcd/v85408556-fr/s1/R23/S11.boost (422 B)  Downloading ImSim/src/v85408556-fr/R23/S11.boost (2.06 MB)  Downloading ImSim/visitim/v85751839-fr/R23-S11.fits (155.39 MB)  Downloading Statistics/v1_i1_g_m400_s20_f.fits (4.00 MB)  Downloading Statistics/v1_i1_g_m400_s20_u16.fits (2.01 MB)  Downloading Statistics/v1_i2_g_m400_s20_f.fits (4.00 MB)  Downloading Statistics/v1_i2_g_m400_s20_u16.fits (2.01 MB)  Downloading Statistics/v2_i1_p_m9_f.fits (4.00 MB)  Downloading Statistics/v2_i1_p_m9_u16.fits (2.01 MB)  Downloading Statistics/v2_i2_p_m9_f.fits (4.00 MB)  Downloading Statistics/v2_i2_p_m9_u16.fits (2.01 MB)  Downloading data/871034p_1_MI.fits (74.88 MB)  Downloading data/871034p_1_img.fits (18.74 MB)  Downloading data/cal-53535-i-797722_1_small.fits (708.75 KB)  Downloading data/fpC-002570-r6-0199_sub.fits (120.94 KB)  Downloading data/fpC-005902-r6-0677_sub.fits (120.94 KB)  Downloading data/med.fits (3.36 MB)    Downloading data/medexp.fits (3.36 MB)  Downloading data/medsub.fits (303.75 KB)  Downloading data/medsubswarp1lanczos2.fits (120.94 KB)  Downloading data/medswarp1bilinear.fits (1.34 MB)  Downloading data/medswarp1lanczos2.fits (1.34 MB)  Downloading data/medswarp1lanczos3.fits (1.34 MB)  Downloading data/medswarp1lanczos4.fits (1.34 MB)  Downloading data/medswarp1nearest.fits (1.34 MB)  Downloading data/small.fits (658.12 KB)  Downloading data/small_MI.fits (495.00 KB)  Downloading data/small_img.fits (137.81 KB)  Downloading test/cal-53535-i-797722_1.fits (93.60 MB)  Checking out files: 100% (226/226), done.  {code}",NULL
DM-4217,"Make the glob in --show config=xxx case insensitive if ""xxx"" is all lower case","See RFC-108    The only difference is that I plan to make {{\-\-show config=xxx}} be case-insensitive only if the pattern xxx is all lower-case.  If people really want to match ""background"" lower-case only, they can say {{--show config=xxx:NOIGNORECASE}}",NULL
DM-4218,"cmdLineTask can raise an exception logging a fatal message about dataIds","When {{TaskRunner.\_\_call\_\_}} fails it attempts to log a message  {code}  task.log.fatal(""Failed on dataId=%s: %s"" % (dataRef.dataId, e))  {code}  Unfortunately, dataRef can be a list, in which case this statement throws an exception itself and the real problem is lost.  ",NULL
DM-4233,"Fix OCSP stapling in nginx web server config","An error is reported in nginx because OCSP stapling isn't configured correctly. Fix this through using the {{ ssl_trusted_certificate}} option.",NULL
DM-4244,"unable to attach cinder volumes","We are unable to attach a cinder volume to a running instance.  Volume creation succeeds but the attach seems to time out and fail after several minutes.  This has been tested via horizon, the cli, and the API.  Eg.  {code}  openstack server add volume ceph0-jhoblitt 1d5afee8-1742-4622-b350-fb746b84813b  {code}  ",NULL
DM-4249,"Revisit alignment of data access docs with milestones","Update data access docs, primarily LDM-135 and LDM-153. The work includes integrating the docs with data access epics.",NULL
DM-4250,"Fix logic error in verify when pushing.","* When files are verified, and they are not public on a public ceph s3 server, make them public.  * TLDR; Added {{not}} to the related {{if}} statement.    See:    https://github.com/lsst-sqre/git-lfs-s3/pull/2    https://github.com/lsst-sqre/git-lfs-s3-server/pull/3",NULL
DM-4257,"Heisenbug building meas_algorithms","Observed once [so far] building v11_0 with distrib install on centos 7 on a nebula m1.xlarge.  Packer destroyed the VM upon error so no triage could be done.    {code}    openstack:      openstack:      openstack: ***** error: from /home/vagrant/stack/EupsBuildDir/Linux64/meas_algorithms-11.0+1/build.log:      openstack: running tests/measure.py... running tests/testDetection.py... running tests/cr.py... running tests/CoaddPsf.py... running tests/testPsfAttributes.py... running tests/testCoaddBoundedField.py... running tests/testPsfDetermination.py... running tests/CoaddApCorrMap.py... failed      openstack: running tests/testLoadReferenceObjects.py... passed      openstack: passed      openstack: passed      openstack: passed      openstack: passed      openstack: running tests/testDoubleGaussianPsf.py... running tests/ticket2986.py... running tests/psfIO.py... running tests/psfSelectTest.py... running tests/Interp.py... passed      openstack: running tests/testGaussianPsfFactory.py... passed      openstack: passed      openstack: passed      openstack: running tests/testPsfCandidate.py... passed      openstack: passed      openstack: passed      openstack: passed      openstack: passed      openstack: passed      openstack: 1 tests failed      openstack: scons: *** [checkTestStatus] Error 1      openstack: scons: building terminated because of errors.      openstack: + exit -4      openstack: eups distrib: Failed to build meas_algorithms-11.0+1.eupspkg: Command:      openstack: source /home/vagrant/stack/eups/bin/setups.sh; export EUPS_PATH=/home/vagrant/stack; (/home/vagrant/stack/EupsBuildDir/Linux64/meas_algorithms-11.0+1/build.sh) >> /home/vagrant/stack/EupsBuildDir/Linux64/meas_algorithms-11.0+1/build.log 2>&1 4>/home/vagrant/stack/EupsBuildDir/Linux64/meas_algorithms-11.0+1/build.msg      openstack: exited with code 252      openstack: [ 43/53 ]  meas_algorithms 11.0+1 ...  {code}",NULL
DM-426,"Coordinate naming of trapped messages in a better way","Messages trapped by Conditionals are automatically assigned the name ""$msg"", and can be referred to as ""$msg[1]"" or ""$msg[2]"", depending on which Conditional in the chain trapped the message.   This is an artifact of the way the declarative objects were named in the Chain sequence.  One suggestion might be to have an Assignment object and allows messages to be signed whatever name we want.     A better solution would be to see if they can be just assigned directly to variables, but this only would work if there's a re-write in Python.",NULL
DM-4260,"nebula API token expiration period is too short","The API tokens issued by keystone expire after ~ 1 hour.  This is problematic for some automation tools, such as a packer, that do not expect to have to reauthorize after such a short period of time.  Between the short token TTL and DM-4237, there are significant difficulties in trying to make automated images avaiable.    {code}  ==> openstack-centos-7: Error terminating server, may still be around: Expected HTTP response code [202 204] when accessing [DELETE http://nebula.ncsa.illinois.edu:8774/v2/8c1ba1e0b84d486fbe7a665c30030113/servers/aacabacd-e0bc-4a4b-9892-0f2e474727c5], but got 401 instead  ==> openstack-centos-7: Authentication required  {code}",NULL
DM-427,"Add a Reader to directly access HTCondor message logs","Add a Reader object to handle messages in HTCondor logs.    This would require having the Reader translate multiple message types (see ctrl_stats).  This would also require interfacing the mechanism HTCondor uses to read these files, since HTCondor logs roll over from one file to the next.  We really shouldn't try to roll our own code to read those logs ourselves.",NULL
DM-4297,"obs_decam tests/getId.py fails ","In DM-4191, I included a change in {{DecamMapper._computeCcdExposureId}}: https://github.com/lsst/obs_decam/commit/08086d2a961ba7cef34f4a33acba1f6be356e25f    but missed to update the unit test {{getId.py}} accordingly.        More about that previous change in DecamMapper._computeCcdExposureId:   It actually wasn't directly relevant to ISR processing but was from conversations in HipChat obs_decam room Oct 21, 2015, 1:05 PM CDT     ",NULL
DM-4300,"Parser error kills czar","Running some simple query causes parser error and czar crash:  {noformat}  # client side  $ mysql -h127.0.0.1 -P4040  mysql> SELECT 1;  ERROR 4005 (Proxy): Unable to run lua xmlrpc client, message: ...inux64/lua/5.1.4.lsst1/share/lua/5.1/xmlrpc/http.lua:41: closed    $ ~/qserv-run/2015_11/bin/qserv-status.sh   INFO: Qserv execution directory : /home/salnikov/qserv-run/2015_11  MySQL running (16353)                                      [  OK  ]  xrootd is running (16386)                                  [  OK  ]  mysql-proxy is running (16422)                             [  OK  ]  qserv-czar is dead but PID file exists                     [FAILED]    see /home/salnikov/qserv-run/2015_11/var/run/qserv-czar.pid  qserv-wmgr is running (16450)                              [  OK  ]  {noformat}    Czar log:  {noformat}  2015-11-10T11:31:29.906-0600 [0x7f4471cd1700] ERROR ccontrol.UserQueryFactory (core/modules/ccontrol/UserQueryFactory.cc:101) - Invalid query: ParseException:Parse error(ANTLR):unexpected token: 1:  {noformat}    Core backtrace:  {noformat}  Program terminated with signal SIGSEGV, Segmentation fault.  #0  0x00007f44693cce4e in std::__shared_ptr<lsst::qserv::query::OrderByClause, (__gnu_cxx::_Lock_policy)2>::operator bool (      this=0x30) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/shared_ptr_base.h:1056  1056	      { return _M_ptr == 0 ? false : true; }  Missing separate debuginfos, use: debuginfo-install expat-2.0.1-11.el6_2.x86_64 keyutils-libs-1.4-5.el6.x86_64 krb5-libs-1.10.3-42.el6.x86_64 libcom_err-1.41.12-22.el6.x86_64 libselinux-2.0.94-5.8.el6.x86_64 nss-softokn-freebl-3.14.3-23.el6_7.x86_64 openssl-1.0.1e-42.el6.x86_64  (gdb) bt  #0  0x00007f44693cce4e in std::__shared_ptr<lsst::qserv::query::OrderByClause, (__gnu_cxx::_Lock_policy)2>::operator bool (      this=0x30) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/shared_ptr_base.h:1056  #1  0x00007f44693ccb7c in lsst::qserv::query::SelectStmt::hasOrderBy (this=0x0) at core/modules/query/SelectStmt.h:120  #2  0x00007f446943148d in lsst::qserv::qproc::QuerySession::getProxyOrderBy (this=0x2bcbb90)      at core/modules/qproc/QuerySession.cc:179  #3  0x00007f446926eaa6 in lsst::qserv::ccontrol::UserQueryFactory::newUserQuery (this=0x2c83fb0, query=""SELECT 1;"", defaultDb="""",       resultTable=""qservResult.result_27994039885"") at core/modules/ccontrol/UserQueryFactory.cc:118  #4  0x00007f446928abb3 in _wrap_UserQueryFactory_newUserQuery (args=0x7f4461023db8) at build/czar/czarLib_wrap.cc:8668  {noformat}",NULL
DM-4305,"assembleCoadd broken","A recent update to assembleCoadd to bring over changes to do clipped coadds breaks coadd generation.  There are two specific problems.    1. There is an infinite recursion because of SafeClipAssembleCoaddTask calling its own constructor in the \_\_init\_\_ method.  2. The overridden assemble method does not adhere to the original assemble call signature, so when the default run method is called by ParseAndRun, it raises an exception.    Additionally I find the flow fairly confusing as the overridden assemble method is called by the default run method which then calls the default assemble method on the parent class.",NULL
DM-4308,"Missing qservCssData db crash Qserv","Missing  qservCssData db crashes Qserv when issues a sql query through czar:    The error message should be more clear and written to qserv-czar.log instead of console (console log should be thrown away in production).    {code}  ==> /qserv/run/var/log/qserv-czar-console.log <==  terminate called after throwing an instance of 'lsst::qserv::css::CssError'    what():  Error from mysql: (-999) Error connecting to mysql with config:[host=, port=0, usr=qsmaster, pass=, dbName=qservCssData, socket=/qserv/run/var/lib/mysql/mysql.sock  {code}    Error reporting for empty or non-consistent qservCssData should also be improved.",NULL
DM-431,"Implement ""shelf life"" for unfulfilled chains.","A ""shelf life"" timer should be implemented to optionally expire Chains that are unfulfilled.  Currently Chains are retained indefinitely.  This shouldn't be a required or automatic expiration, since some Chains may be very long-lived.   It also shouldn't be implemented as an Exception, since that term has its own implications.  This should be based on either the number of messages seen before expiring, or the amount of time since the last Conditional was matched.  It probably should be a superclass of Exception.",NULL
DM-4318,"Mention self-merge for repos.yaml in documents","The conclusion of RFC-75 asks for the process to be documented in [https://confluence.lsstcorp.org/display/LDMDG/Adding+a+new+package+to+the+build].  Please do so.",NULL
DM-4319,"obs_subaru uses envAppend when the common usage is envPrepend","In DM-868 we fixed the table files to use {{envPrepend}} rather than {{envAppend}}. {{obs_subaru}} was not around at the time so it still needs to be fixed.",NULL
DM-432,"Remote access to Event Monitor process","Add remote access to the Event Monitor to monitor.  Since the Event Monitor is meant to be a long-lived process, there should be a good way of getting information from it while it's running.   We could watch to see which rules were in place, which Chains were active, and what their current state is, and for general debugging.  The JMX would be a good way to explore some of this, but if the event monitor were re-written, it couldn't be used.  Besides, it requires the use of jconsole, and it wouldn't be as easy to read/use as watching this information through a browser, or the command line.",NULL
DM-4320,"Use NCSA's Ceph with git-lfs","We set up our own Ceph cluster on Nebula to meet our needs for git-lfs.    In NCSA's OpenStack User Group meeting they offered their Ceph object store to back s3.lsst.codes. Ideally we'll continue to use the librados and radosgw but through configuration use their Ceph object store instead of our own.",NULL
DM-4321,"mergeCoaddDetections.py does not raise correctly","mergeCoaddDetections.py passes a list of dataRefs to the {{\_\_call\_\_}} method on {{CmdLineTask}}.  If there is an exception in the {{run}} method, of {{MergeCoaddDetectionsTask}}, this causes an incorrect exception since the {{dataRef}} in the {{except}} block does not have a {{dataId}} attribute (since it's a list).    I don't know if this is a bug in {{CmdLineTask}}, or in mergeCoaddDetections.",NULL
DM-4322,"Amplifier readout noise set to crazy values for some old CFHT images","In cfhtIsrTask.py the readout noise for amp A and amp B is set  from the values stored in the raw image fits header identified by the keywords RDNOISEA and RDNOISEB. Typically the noise level is ~5 electrons. For some old CFHT images acquired in 2004, RDNOISEA/B is equal to 65535. In these cases, the parameter identified by the keyword RDNOISE is nevertheless correct.    cfhtIsrTask.py should be modified to use RDNOISE instead of RDNOISEA/B when this situation happens.",NULL
DM-4327,"ActiveMQCPP does not build on OS X El Capitan","Our version, 3.5.0, of ActiveMQCPP from 2012 does not build on OS X El Capitan because El Capitan no longer ships with {{openssl}}. Apple have deprecated {{openssl}} for a few releases and now it's gone. Apple recommends using their crypto libraries or installing a third party {{openssl}}.    Unfortunately the current ActiveMQCPP also does not work with El Capitan crypto libraries so it looks like we may have to start shipping our own {{openssl}} (unlike {{qserv}} we can't simply switch to Apple crypto digest code as activemqcpp uses a lot of {{openssl}} code).    Without this package, {{lsst_distrib}} will not build on El Capitan.",NULL
DM-4328,"AFW tests should run and skip if afwdata is missing","Currently, the AFW tests are completely skipped at the {{scons}} layer if {{afwdata}} can not be located. This is bad for two reasons:    1. Surely there are tests in AFW that can be usefully run without access to data files.  2. When we switch to a proper test harness (e.g. DM-3901) an important metric is the number of tests executed compared with the number of tests skipped.    Each test file (or even each test) should determine itself whether it should be skipped based on afwdata availability. This should not be a global switch.",NULL
DM-4330,"Upgrade activemqcpp","Our {{activemqcpp}} package is 3 years out of date. On the third-party packages list it is annotated as a ""u"" package so can be updated without RFC. [~spietrowicz] will check that it integrates properly with the broker at NCSA.",NULL
DM-4333,"Research/Write up a workflow for getting sims_maf_contrib running","Prompted by a email request from John Gizis.",NULL
DM-4334,"libevent does not build on OS X El Capitan","{{libevent}} does not build on OS X El Capitan because it attempts to build with {{openssl}}. [~fritzm] has agreed that disabling SSL on OS X will not cause any problems.",NULL
DM-4344,"pipe_tasks 11.0-14-ga314014+5 fails a test on os/x 10.10.5","Running  {code}  SCONSFLAGS=""-j 6 opt=3"" eups distrib install pipe_tasks 11.0-14-ga314014+5  {code}  on my laptop running ox/x 10.10.5 results in a test failure:  {code}  F.  ======================================================================  FAIL: testEdge (__main__.interpolationTestCase)  Test that we can interpolate to the edge  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testInterpImageTask.py"", line 127, in testEdge      validateInterp(miInterp, useFallbackValueAtEdge, fallbackValue)    File ""tests/testInterpImageTask.py"", line 90, in validateInterp      self.assertAlmostEqual(val0, fallbackValue, 6)  AssertionError: 2.5515668 != 2.5515660972664267 within 6 places  {code}  ",NULL
DM-4345,"OSX CI/jenkins build slaves","We are too frequently encountering OSX specific build issues that aren't caught by CI on Linux distros.  Establishing OSX jenkins build slaves has been blocked since the beginning of CY15 by lack of avaiable resources.",NULL
DM-4350,"Extend lsst_dd_rtd_theme to show DOI, abstract, etc","DM-4324, which created lsst-technote-bootstrap, extended the metadata that can be associate with technical notes (DOIs, docushare URLs, github URLs, etc.). The Sphinx theme now needs to be extended to show this information.",NULL
DM-4358,"obs_subaru tests should skip within the tests if test data can not be located","Currently, the {{obs_subaru}} tests are completely skipped at the scons layer if the test data can not be located. This is bad for two reasons:  1. Are there any tests that can be run even if test data is missing?.  2. When we switch to a proper test harness (e.g. DM-3901) an important metric is the number of tests executed compared with the number of tests skipped.  Each test file (or even each test) should determine itself whether it should be skipped based on afwdata availability. This should not be a global switch.",NULL
DM-4359,"lsstsw deploy on OS X El Capitan fails with git / openssl","I just installed OS X 10.11.1 and tried to install a brand new {{lsstsw}} environment.    I’ve done    {code:bash}  git clone https://github.com/lsst/lsstsw.git  cd lsstsw  ./bin/deploy  {code}    At this point it does a lot of things, but stumbles on installing its special snowflake git.    The error message is    {code:bash}  ./git-compat-util.h:214:10: fatal error: 'openssl/ssl.h' file not found  #include <openssl/ssl.h>  {code}    The full output is below.    Do I need to install an openssl separately on my El Capitan system?    {code:bash}  ::: Deploying git  ######################################################################## 100.0%  ######################################################################## 100.0%  configure: Setting lib to 'lib' (the default)  configure: Will try -pthread then -lpthread to enable POSIX Threads.  configure: CHECKS for site configuration  checking for gcc... gcc  checking whether the C compiler works... yes  checking for C compiler default output file name... a.out  checking for suffix of executables...  checking whether we are cross compiling... no  checking for suffix of object files... o  checking whether we are using the GNU C compiler... yes  checking whether gcc accepts -g... yes  checking for gcc option to accept ISO C89... none needed  checking how to run the C preprocessor... gcc -E  checking for grep that handles long lines and -e... /usr/bin/grep  checking for egrep... /usr/bin/grep -E  checking for ANSI C header files... yes  checking for sys/types.h... yes  checking for sys/stat.h... yes  checking for stdlib.h... yes  checking for string.h... yes  checking for memory.h... yes  checking for strings.h... yes  checking for inttypes.h... yes  checking for stdint.h... yes  checking for unistd.h... yes  checking for size_t... yes  checking for working alloca.h... yes  checking for alloca... yes  configure: CHECKS for programs  checking whether we are using the GNU C compiler... (cached) yes  checking whether gcc accepts -g... (cached) yes  checking for gcc option to accept ISO C89... (cached) none needed  checking for inline... inline  checking if linker supports -R... no  checking if linker supports -Wl,-rpath,... yes  checking for gar... no  checking for ar... ar  checking for gtar... no  checking for tar... tar  checking for gnudiff... no  checking for gdiff... no  checking for diff... diff  checking for asciidoc... no  configure: CHECKS for libraries  checking for SHA1_Init in -lcrypto... yes  checking for curl_global_init in -lcurl... yes  checking for XML_ParserCreate in -lexpat... yes  checking for iconv in -lc... no  checking for iconv in -liconv... yes  checking for deflateBound in -lz... yes  checking for socket in -lc... yes  checking for inet_ntop... yes  checking for inet_pton... yes  checking for hstrerror... yes  checking for basename in -lc... yes  checking for gettext in -lc... no  checking libintl.h usability... no  checking libintl.h presence... no  checking for libintl.h... no  configure: CHECKS for header files  checking sys/select.h usability... yes  checking sys/select.h presence... yes  checking for sys/select.h... yes  checking sys/poll.h usability... yes  checking sys/poll.h presence... yes  checking for sys/poll.h... yes  checking for inttypes.h... (cached) yes  checking for old iconv()... no  configure: CHECKS for typedefs, structures, and compiler characteristics  checking for socklen_t... yes  checking for struct itimerval... yes  checking for struct dirent.d_ino... yes  checking for struct dirent.d_type... yes  checking for struct passwd.pw_gecos... yes  checking for struct sockaddr_storage... yes  checking for struct addrinfo... yes  checking for getaddrinfo... yes  checking for library containing getaddrinfo... none required  checking whether the platform regex can handle null bytes... yes  checking whether system succeeds to read fopen'ed directory... no  checking whether snprintf() and/or vsnprintf() return bogus value... no  configure: CHECKS for library functions  checking libgen.h usability... yes  checking libgen.h presence... yes  checking for libgen.h... yes  checking paths.h usability... yes  checking paths.h presence... yes  checking for paths.h... yes  checking libcharset.h usability... yes  checking libcharset.h presence... yes  checking for libcharset.h... yes  checking for strings.h... (cached) yes  checking for locale_charset in -liconv... yes  checking for setitimer... yes  checking for library containing setitimer... none required  checking for strcasestr... yes  checking for library containing strcasestr... none required  checking for memmem... yes  checking for library containing memmem... none required  checking for strlcpy... yes  checking for library containing strlcpy... none required  checking for uintmax_t... yes  checking for strtoumax... yes  checking for library containing strtoumax... none required  checking for setenv... yes  checking for library containing setenv... none required  checking for unsetenv... yes  checking for library containing unsetenv... none required  checking for mkdtemp... yes  checking for library containing mkdtemp... none required  checking for mkstemps... yes  checking for library containing mkstemps... none required  checking for initgroups... yes  checking for library containing initgroups... none required  checking for POSIX Threads with ''... yes  configure: creating ./config.status  config.status: creating config.mak.autogen  config.status: executing config.mak.autogen commands  GIT_VERSION = 2.2.2      * new build flags      * new link flags      * new prefix flags      GEN common-cmds.h      CC hex.o      CC ident.o      CC kwset.o  In file included from In file included from ident.chex.c::81:  :  In file included from In file included from ./cache.h./cache.h::44:  :  ./git-compat-util.h./git-compat-util.h:214::21410::10 : fatal error: fatal error: 'openssl/ssl.h' file'openssl/ssl.h'  notfile  foundnot  found  In file included from kwset.c:37:  In file included from ./cache.h:4:  ./git-compat-util.h:#include <openssl/ssl.h>214  :10:         ^  fatal error: 'openssl/ssl.h'#include <openssl/ssl.h>  file not         ^  found  #include <openssl/ssl.h>           ^      CC levenshtein.o  In file included from levenshtein.c:1:  In file included from ./cache.h:4:  ./git-compat-util.h:214:10: fatal error: 'openssl/ssl.h' file not found  #include <openssl/ssl.h>           ^  1 error generated.  1 error generated.  make: *** [levenshtein.o] Error 1  make: *** Waiting for unfinished jobs....  1 error generated.  make: *** [hex.o] Error 1  make: *** [ident.o] Error 1  1 error generated.  make: *** [kwset.o] Error 1  {code}",NULL
DM-4361,"obs_sdss fails tests trying to import six","Installing {{lsst_apps -t w_2015_45}} fails with    {code}  ERROR: ImportError: cannot import name six [unknown]  Traceback (most recent call last):    File ""tests/getId.py"", line 29, in <module>      from lsst.obs.sdss import SdssMapper    File ""/Users/lsst/products/EupsBuildDir/DarwinX86/obs_sdss-11.0-3-g1664b15+1/obs_sdss-11.0-3-g1664b15+1/python/lsst/obs/sdss/__init__.py"", line 24, in <module>      import makeCamera    File ""/Users/lsst/products/EupsBuildDir/DarwinX86/obs_sdss-11.0-3-g1664b15+1/obs_sdss-11.0-3-g1664b15+1/python/lsst/obs/sdss/makeCamera.py"", line 30, in <module>      from lsst.obs.sdss.convertOpECalib import SdssCameraState    File ""/Users/lsst/products/EupsBuildDir/DarwinX86/obs_sdss-11.0-3-g1664b15+1/obs_sdss-11.0-3-g1664b15+1/python/lsst/obs/sdss/convertOpECalib.py"", line 4, in <module>      from lsst.obs.sdss.yanny import yanny as Yanny    File ""/Users/lsst/products/EupsBuildDir/DarwinX86/obs_sdss-11.0-3-g1664b15+1/obs_sdss-11.0-3-g1664b15+1/python/lsst/obs/sdss/yanny.py"", line 35, in <module>      from astropy.extern import six  ImportError: cannot import name six  {code}",NULL
DM-4371,"Migrate remaining large gitolite repos to lfs","These repos were too large to move as part of the initial gitolite -> github migration.  They now need to be migrated to git-lfs.    https://dev.lsstcorp.org/cgit/LSST/DMS/    LSST/DMS/testdata/isrdata.git  LSST/DMS/testdata/multifit.git  LSST/DMS/testdata/sst.git  LSST/DMS/testdata/testdata_cfht.git    This spreadsheet: http://ls.st/md3 needs to be updated after the migration and the new canonical URLs should be announced on community since currently none of these repos are referenced from {{repos.yaml}}.",NULL
DM-4374,"makeCoaddTempExp has the wrong default for bgSubtracted","Currently, the makeCoaddTempExp command line task warps calexps with background in them.  The point was that background matching in SDSS works quite well, but unfortunately, it doesn't work nearly as well for non-driftscan data.    I believe the default should be changed to {{bgSubtracted=True}}.    Also, from HipChat discussion:  {quote}\[Nov-17 8:40 PM\] Paul Price: I only know that the current defaults \[for makeCoaddTempExp.py\] are s....l.....o.....w.....  \[Nov-17 8:42 PM\] Paul Price: @KSK Here's what I recommend: warpAndPsfMatch.warp.warpingKernelName=""lanczos3"" warpAndPsfMatch.warp.cacheSize=1000000{quote}    I believe these defaults should also be changed.  ",NULL
DM-4378,"Improve SConscript product discovery in AFW and coadd_utils","As part of DM-3200 the use of {{lsst.utils}} was removed from {{scons}} configuration files and replaced by a call to {{sconsUtils}}. The fix was non-optimal so this ticket makes the trivial change to use the correct API.",NULL
DM-4380,"Technote Platform Wishlist","* fix weird index sublist expansion bug  * with the current template, it's difficult to tell the difference between ===,---,^^^, sections  * add a .gitignore template  * add some sort of .travis.yml template  * possibly have travis push the master branch to readthedocs?  ",NULL
DM-4384,"nebula resource exhaustion","I've been having trouble spawning instances on nebula today (as I was having late last week) due to core or memory quota limits.  This makes development rather frustrating as one can't even burn images else where and upload them due to DM-4237.    Is there sufficient available capacity to request an increase to our quota?    Is there an ETA for when LSST purchased hardware will be integrated with Nebula?",NULL
DM-4385,"Remove stringToAny from the utils package","I accidentally left stringToAny in Utils.cc when implementing DM-2635, even though I removed it from Utils.h and it is not used anywhere. Remove it and mark RFC-47 Done.",NULL
DM-4388,"lsst_distrib build on new CentOS instance fails","I started a new instance, m1.xlarge on Nebula and did the following:    {quote}sudo yum install blas bison bzip2 bzip2-devel cmake curl flex fontconfig freetype-devel gcc-c++ gcc-gfortran gettext git glib2-devel libXext libXrender libXt-devel libcurl-devel libuuid-devel make ncurses-devel openssl-devel patch perl perl-ExtUtils-MakeMaker readline-devel zlib-devel{quote}    {quote}  git config --global user.email ""nospamwanted@illinois.edu""  git config --global user.name ""Steve Pietrowicz""  {quote}    {quote}  git clone https://www.github.com/lsst/lsstsw.git  cd lsstsw   ./bin/deploy   . /home/centos/lsstsw/bin/setup.sh  rebuild lsst_distrib  {quote}    And got the following error on the build of meas_modelfit:    {quote}         meas_modelfit: 2015_10.0-1-g9cc7517+6 .........................................................ERROR (158 sec).  *** error building product meas_modelfit.  *** exit code = 2  *** log is in /home/centos/lsstsw/build/meas_modelfit/_build.log  *** last few lines:  :::::  [2015-11-19T20:29:14.552746Z] passed  :::::  [2015-11-19T20:29:36.567961Z] passed  :::::  [2015-11-19T20:29:36.593865Z] Failed test output:  :::::  [2015-11-19T20:29:36.598799Z] tests/testShapeletPsfApproxPlugins.py  :::::  [2015-11-19T20:29:36.598831Z]   :::::  [2015-11-19T20:29:36.599547Z] The following tests failed:  :::::  [2015-11-19T20:29:36.602614Z] /home/centos/lsstsw/build/meas_modelfit/tests/.tests/testShapeletPsfApproxPlugins.py.failed  :::::  [2015-11-19T20:29:36.602900Z] 1 tests failed  :::::  [2015-11-19T20:29:36.604177Z] scons: *** [checkTestStatus] Error 1  :::::  [2015-11-19T20:29:36.606360Z] scons: building terminated because of errors.  {quote}    the contents of that .failed file are basically empty:    {quote}  [centos@srp1 lsstsw]$ cat /home/centos/lsstsw/build/meas_modelfit/tests/.tests/testShapeletPsfApproxPlugins.py.failed  tests/testShapeletPsfApproxPlugins.py    [centos@srp1 lsstsw]$   {quote}  ",NULL
DM-4389,"Fix deprecations from git-lfs client","The {{git-lfs}} client is deprecating some features that we are using with v2.0.    They are deprecating {{.gitconfig}} in favor of {{.lfsconfig}}. They are also removing the {{git-lfs init}} command. We will need to update documentation, all repos that use git-lfs and CI system.    See: https://github.com/github/git-lfs/issues/839",NULL
DM-439,"Evaluate VOEvent software for simulator","This depends on whether or not we're going to be sending ""real"" VOEvents, or if we're simulating that too:  There appear to be several different VOEvent software packages out there.  We need to evaluate and select which package we'll use.  We'll want to consult with Ray about this.",NULL
DM-4390,"lfs pull breaks with valid github credentials","It appears that s3.lsst.codes is not properly handle http credentials.  A blank username/password is able to download lfs objects but supplying valid github username/password credentials fails.  Eg.    {code:java}    $ git clone -c lfs.batch=false -c filter.lfs.require -c filter.lfs.smudge='git-lfs smudge %f' -c filter.lfs.clean='git-lfs clean %f'  https://github.com/lsst/afwdata.git afwdata2  Cloning into 'afwdata2'...  remote: Counting objects: 1784, done.  remote: Total 1784 (delta 0), reused 0 (delta 0), pack-reused 1784  Receiving objects: 100% (1784/1784), 232.26 KiB | 0 bytes/s, done.  Resolving deltas: 100% (471/471), done.  Checking connectivity... done.  Downloading CFHT/D2/sdss.dat (110.68 KB)  Username for 'https://git-lfs.lsst.codes': jhoblitt  Password for 'https://jhoblitt@git-lfs.lsst.codes':   Username for 'https://s3.lsst.codes': jhoblitt  Password for 'https://jhoblitt@s3.lsst.codes':   Error accessing media: CFHT/D2/sdss.dat (711e334cd5a247721d4603a7876e3166c58b77307e65102ea769cb0788633bb2)    Errors logged to .git/lfs/objects/logs/20151120T092457.812846521.log  Use `git lfs logs last` to view the log.  error: external filter git-lfs smudge %f failed 2  error: external filter git-lfs smudge %f failed  fatal: CFHT/D2/sdss.dat: smudge filter lfs failed  warning: Clone succeeded, but checkout failed.  You can inspect what was checked out with 'git status'  and retry the checkout with 'git checkout -f HEAD'    $ cat afwdata2/.git/lfs/objects/logs/20151120T092457.812846521.log  git-lfs/1.0.2 (GitHub; linux amd64; go 1.5.1; git 0566698)  git version 2.4.3    $ git-lfs smudge CFHT/D2/sdss.dat  Error accessing media: CFHT/D2/sdss.dat (711e334cd5a247721d4603a7876e3166c58b77307e65102ea769cb0788633bb2)    Client error: https://s3.lsst.codes/lsst-sqre-prod-git-lfs/data/711e334cd5a247721d4603a7876e3166c58b77307e65102ea769cb0788633bb2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=XIGV9O00257RB4BM5EQ3%2F20151120%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20151120T162452Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&X-Amz-Signature=4504341d987915a4890ebc5ae83e767df8e9e9e6c7563966e7f3429be8d69053  goroutine 1 [running]:  github.com/github/git-lfs/lfs.Stack(0x0, 0x0, 0x0)  	/Users/rick/go/src/github.com/github/git-lfs/lfs/errors.go:557 +0x80  github.com/github/git-lfs/commands.logPanicToWriter(0x7f6cd90321e8, 0xc8200341d8, 0x7f6cd9032da8, 0xc8204783b0)  	/Users/rick/go/src/github.com/github/git-lfs/commands/commands.go:184 +0xf7f  github.com/github/git-lfs/commands.logPanic(0x7f6cd9032da8, 0xc8204783b0, 0x0, 0x0)  	/Users/rick/go/src/github.com/github/git-lfs/commands/commands.go:148 +0x421  github.com/github/git-lfs/commands.handlePanic(0x7f6cd9032da8, 0xc8204783b0, 0x0, 0x0)  	/Users/rick/go/src/github.com/github/git-lfs/commands/commands.go:123 +0x4e  github.com/github/git-lfs/commands.LoggedError(0x7f6cd9032da8, 0xc8204783b0, 0x8e24c0, 0x1e, 0xc82014dc98, 0x2, 0x2)  	/Users/rick/go/src/github.com/github/git-lfs/commands/commands.go:73 +0x82  github.com/github/git-lfs/commands.smudgeCommand(0xab9e60, 0xc820011930, 0x1, 0x1)  	/Users/rick/go/src/github.com/github/git-lfs/commands/command_smudge.go:80 +0xd89  github.com/github/git-lfs/vendor/_nuts/github.com/spf13/cobra.(*Command).execute(0xab9e60, 0xc820011880, 0x1, 0x1, 0x0, 0x0)  	/Users/rick/go/src/github.com/github/git-lfs/vendor/_nuts/github.com/spf13/cobra/command.go:477 +0x403  github.com/github/git-lfs/vendor/_nuts/github.com/spf13/cobra.(*Command).Execute(0xabab60, 0x0, 0x0)  	/Users/rick/go/src/github.com/github/git-lfs/vendor/_nuts/github.com/spf13/cobra/command.go:551 +0x46a  github.com/github/git-lfs/commands.Run()  	/Users/rick/go/src/github.com/github/git-lfs/commands/commands.go:88 +0x23  main.main()  	/Users/rick/go/src/github.com/github/git-lfs/git-lfs.go:34 +0x12e    ENV:  LocalWorkingDir=/home/jhoblitt/tmp/afwdata2  LocalGitDir=.git  LocalGitStorageDir=.git  LocalMediaDir=.git/lfs/objects  TempDir=.git/lfs/tmp  ConcurrentTransfers=3  BatchTransfer=false  GIT_DIR=.git  $ git-lfs version  git-lfs/1.0.2 (GitHub; linux amd64; go 1.5.1; git 0566698)  {code}    ",NULL
DM-4392,"Save logs from failed qserv unit tests in Jenkins","Qserv saves logs from failed unit tests in a non-standard location and they are not currently visible in jenkins. It would be nice to include those logs (if they exist) in jenkins output. The logs are in multiple directories inside {{build/}}, the pattern to search for the logs is {{build/\*/\*.utest.failed}}.  ",NULL
DM-4394,"Update dependencies in obs_decam table file","The {{obs_decam}} table file needs to be updated. Firstly the dependencies listed in the table file are not complete. Secondly the table file sets {{LD_LIBRARY_PATH}} despite there being no C++ code in the package.",NULL
DM-440,"adjust comment pop-ups for issue workflow changes","Currently, when we switch from To-Do to In Progress, we get a box asking for a comment.  I think this is unnecessary, but not a serious problem.  However, we do not get such a box when we switch from In Review to Review Complete, and here I think having a comment box is highly desirable: I virtually always want to make a comment when I complete a review, and I'm assuming doing it in a pop-up box cuts down on unnecessary notifications.",NULL
DM-4403,"Cache by topic ID rather than the topic slug","If a user changes the title of a topic, the slug changes and the topic will be re-sent by the mailbot. The topic integer ID is a more reliable unique identifier.",NULL
DM-4404,"Re-evaluate MPI comm workaround","In ctrl_pool's pool.py, we have a Comm class that appears to work around an OpenMPI bug.  In his review for DM-2983, [~mcarras2] requests that we check whether this is still necessary.",NULL
DM-4405,"Can we include the Eigen ""unsupported"" stuff?","I'm using Splines and LevenbergMarquardt stuff from the Eigen 'unsupported' modules. Is it possible to include the unsupported modules in the LSST stack?",NULL
DM-4409,"Create a Unit Test to check if stars are being reserved from PSF fitting correctly.","DM-3692 cherry-picked the HSC ticket HSC-966 which allows for a set fraction of stars to be reserved from PSF fitting. Write a unit test to check that this functionality works correctly.",NULL
DM-4411,"Port HSC code to propagate merge.peak","Changes to deblender to propagate the ""merge.peak"" flag for parent objects as well as children. Relevant commit is 8aa3e8afdea938d5ff93292bc183bc5f39060842 from meas_deblender",NULL
DM-4413,"Add emacs and tmux packages to puppet-lsststack","Add emacs and tmux as convenience tools to puppet-lsststack packages.",NULL
DM-442,"processCcd.py --show fails to check that an argument is provided","Running {code}processCcd.py ... --show{code} without any argument to the {{--show}} option processes the data rather than printing an error message and exiting. ",NULL
DM-4430,"obs_cfht MegacamMapper.paf needs to be updated","MegacamMapper.paf needs to be updated in order to be compatible with the latest assembleCoadd task (deep_safeClipAssembleCoadd_config entry) and with the simultaneous astrometry (wcs entry)",NULL
DM-4441,"Support multi-mode fringe subtraction","Some of the current implementation assumes only a single fringe frame.  Several places in FringeTask and/or IsrTask may need to be updated to support fringe correction with multiple fringe frames.",NULL
DM-4461,"Week end 11/28/15","Support for lsst-dev cluster, OpenStack, and accounts  for week ending November 28, 2015.",NULL
DM-4468,"Decommissioning old equipment (week end 11/28/15)","* Contacted SUI & QServ teams to see about their schedule for moving off old hardware  ",NULL
DM-451,"Setup ApertureFlux for testing using ProcessCcd","Similar to DM-441, but for Aperture Flux  Should include tests of major config options to be sure that they do something reasonable.  Should include test of at least one exception (flag)",NULL
DM-4518,"Neutron - Maximum number of ports exceeded.","When creating a new instance I received the following error:    bq. Error: Maximum number of ports exceeded (HTTP 403) (Request-ID: req-c57843e6-dee1-410b-bbb1-426628b15bdc)    and    bq. Error: [u'The number of defined ports: %(ports)dis over the limit: %(quota)d', {u'ports': 51, u'quota': 50}] (HTTP 403) (Request-ID: req-c15f53ee-5ab5-4bb8-9ad7-3d6c8d65acf5)    I tried a different network and that didn't solve it. So it looks like the ports quota is project/tenant based.",NULL
DM-4519,"Create and implement system where datasets can refer to other datasets and provide access and caching via Butler","Some of our datasets, particularly exposures, have many subcomponents, such as WCSs, PSFs, and even bounding boxes. So far, we've implemented this by considering these to be ""pieces"" of an exposure to be extracted without loading the full exposure. The current pipeline flow is starting to demonstrate some of the shortcomings of this approach, however:    The WCS of an exposure can also be logically considered to be part of other datasets, such as the source table derived from that exposure. (The same is true of the PSF, photometric calibration, background, etc., but I'll use WCS as my example here).  Ubercal creates a new WCS to be associated with each exposure, and while these may be stored separately on disk, we'd like to be able to create a new dataset that would combine the calexp pixel data with the ubercal WCS etc. (See also Dataset Flavors and Versioning, below).  Some complex serialized objects, such as CoaddPsf themselves contain many other objects, in this case the Psfs and Wcss of the exposures that went into a coadd. Considering that a the CoaddPsf of a neighboring patch will contain many of the same Psfs and Wcss, it's highly desirable to store each of these only once within a data repo, and moreover, to be able to use an existing in-memory object instead of re-reading it from disk and creating a duplicate.    Possible implementations include:  # links embedded in fits files  # associations recorded in the registry  # other sym link system",NULL
DM-4520,"Butler support for ""git branch style"" data repositories ","Right now we can can chain repositories through the _parent mechanism.  Want to be able to think of a chain as a ""branch"" and specify the branch along with the repository root. Furthermore, want to be able to refer to a _parent of the branch tip by name, if starting a new branch from a previous tip.",NULL
DM-4522,"git on lsst-dev is too old for git-lfs","[Git-lfs|https://git-lfs.github.com/] requires git v1.8.2.    {{lsst-dev}} has git v1.7.1.    I don't really have a solution. I don't know how this fact didn't come up earlier. Is there another version of git on lsst-dev?",NULL
DM-4528,"DPDD: Object construction should not be required to consider single-epoch Sources","LSE-163 §5.1 indicates that the Object construction routine is required to consider single-epoch Sources:  {quote}  The master list of Objects in Level 2 will be generated by associating and deblending the list of single-epoch source detections and the lists of sources detected on coadds.  {quote}    Per [discussion on community.lsst.org|https://community.lsst.org/t/access-patterns-from-qserv-shared-scan-perspective/418/11], I suggest that this should not be a hard requirement.",NULL
DM-4530,"eups install fails when `python` binary is python 3.x","The EUPS build fails running {{mksetup}} when {{python}} is a 3.x variant.  Per [pep-0394|https://www.python.org/dev/peps/pep-0394/#id13] 2.x only code should use {{python2}} in the shebang.    {code:java}    In bin    make[1]: Entering directory `/home/travis/build/lsst/lsstsw/sources/eups-1.5.9/bin'    chmod u+w /home/travis/build/lsst/lsstsw/eups/1.5.9/bin      File ""./mksetup"", line 37        print ""Writing a csh startup script"";                                           ^    SyntaxError: Missing parentheses in call to 'print'    make[1]: *** [install] Error 1  {code}  ",NULL
DM-4531,"Qserv returns incorrect results for some expressions","I'm seeing something strange with some simple queries in qserv:  {noformat}  mysql> select sum(coord_ra) from qservTest_case03_qserv.RunDeepForcedSource;  +------------------+  | SUM(QS1_SUM)     |  +------------------+  | 15566265.2327842 |  +------------------+  1 row in set (0.63 sec)    mysql> select sum(coord_ra*coord_ra) from qservTest_case03_qserv.RunDeepForcedSource;  +------------------+  | SUM(QS1_SUM)     |  +------------------+  | 15566265.2327842 |  +------------------+  1 row in set (0.61 sec)  {noformat}    Even though expressions are different, the result returned by query is identical.    Looking at czar log there is something fishy going on there, looks like parser or something else replaces {{sum(coord_ra*coord_ra)}} with just {{sum(coord_ra)}} for unexplained reason:  {noformat}  [2015-12-03T11:36:13.530-0600] [0x7f87a7d0f700] DEBUG qproc.QuerySession (core/modules/qproc/QuerySession.cc:113) - Query Plugins applied:   QuerySession description:    original: select sum(coord_ra*coord_ra) from qservTest_case03_qserv.RunDeepForcedSource    has chunks: 1    chunks: []    needs merge: 1    1st parallel statement: SELECT sum(coord_ra) AS QS1_SUM FROM qservTest_case03_qserv.RunDeepForcedSource_%CC% AS QST_1_    merge statement: SELECT SUM(QS1_SUM)    ScanTable: qservTest_case03_qserv.RunDeepForcedSource  {noformat}  ",NULL
DM-4532,"define and implement mechanism to iterate over partial dataIds","[~npease]: TODO this needs better definition, I need to discuss it more with [~ktl]",NULL
DM-4533,"Read and apply constant and linear terms from TAN-SIP header in TanWcs.cc","While the SIP definition in : http://fits.gsfc.nasa.gov/registry/sip/SIP_distortion_v1_0.pdf specifies that the constant and linear polynomial coefficients should not be used in the direct transformation this turns to be an issue when deriving the SIP coefficients from a simultaneous astrometry fit taking into account several CCD at the same time. In such cases, forcing the constant and linear terms to be 0 leads to numerical instabilities and it is not even obvious that a solution always exists.  On top of this, we remark that several applications (ds9, wcstools) are reading those coefficients.    After discussion with [~jbosch], [~rhl] and [~astier] we think that it is bette to read and apply the constant and linear terms while decoding the TAN-SIP WCS at least until we implement  a more general distortion encoding mechanism.  ",NULL
DM-4535,"Execute stack copyright/license conversion","Run new codekit to change license/copyright to RFC-45 style across the entire stack.",NULL
DM-4539,"Improve dataId transformation","Right now it is hard (impossible) to know when a dataId has been transformed (via _transformId(...) in a CameraMapper subclass) and it's not specified (or documented) when the transform will or should happen.    Problems can occur when writing a subclass method that takes dataIds; there's no way to know if the data has yet been transformed.    Good documentation is also strongly requested.",NULL
DM-4541,"Implement HDF5 serialization for AFW objects","Initially it could be done in AFW the way that fits serialization is done now.  Ultimately it should be moved to a different package, and FITS (and HDF5) dependencies should be removed from AFW.",NULL
DM-4542,"Refactor butler dispatch (object serialization) to be pluggable.","refactor the switch-statement behavior in butler that serializes to different formats for fits, pickle, etc into a generic pluggable dispatch framework.    affects code in daf_persistence/src (c++ that we want to get rid of)  daf_persistence python; hard coded stuff in butler  potentially affects bypass_* functions in the mapper classes.    We must consider that transport protocol is a factor in this, targets include:  * stream  * disk  * database  * in-memory storage    We need to do some design work and will need to create stories for implementing the different targets.",NULL
DM-4543,"Add HDF5 support to butler dispatch mechanism ","After refactoring butler dispatch (serialization) functionality, add HDF5 serialization support to that.",NULL
DM-4548,"Create DAX URL from butler dataId","Need a function to create a DAX URL from a given butler dataId. Return just the path part of the url, not the ""http://<server>"" part of the url; the caller must add this part (to indicate which img serv or db serv).     Also add convenience functions to support:  * list of dataId to list of dax url  * butlerSubset instance to list of dax url    It probably wants to be free functions in a new file in daf_butlerUtils.",NULL
DM-4549,"Investigate creating a VO data link URL from butler dataId","similar to DM-4548 but create a VO data link url instead.",NULL
DM-455,"test harness for startup scripts","We need a unit test that would automatically detect problems with starting/stopping services. An example of a problem that we should automatically catch is in DM-328.",NULL
DM-4550,"Add SDSS anti-shredding algorithm to deblender","[~rhl] identifies this as a priority on HipChat ([HSC room, 2015-12-04, 18:17 Eastern|https://lsst.hipchat.com/history/room/496148/2015/12/04?q=shredding&mid=2f679603-beca-4c8c-90de-f106efebe12f&ts=1449271028&t=all]).",NULL
DM-4551,"Butler policy settings via pipe tasks argument parser","Extend pipe tasks argument parser to take butler policy settings to send to the butler",NULL
DM-4552,"Design the authorization management through or around Butler","We need to understand how authentication information is passed through or around the Butler to background services it may access.    per [~price] If butler uses authorization, the auth must be able to be passed between instances of butler; current use case is to pickle butler, send it to another process, and unpickle. It is possible we will choose another serialization or re-instantiation (such as instantiate-from-configuration) scheme, but whatever we choose the auth must be able to be used by the 2nd (and so forth) instantiated butler.",NULL
DM-4554,"Basic Remote Butler functionality","Initial implementation of a Butler front end that has (or uses) a ""remote mapper"" to communicate with a Butler service running remotely. Gets should return DAX URL. Puts should go to the local repository.",NULL
DM-456,"orca provenance recording","When the .paf to config switch happened, it was decided to push off the recording of provenance to a later date.  The old mechanism was set up to record software package names, version, and directories.  It also recorded the .paf files for the policies that were used to configure the pipelines.  We need a mechanism to record this information for software packages and the Config files used to configure the runs.  I suspect this still belongs in ctrl_orca, but we might consider shifting this to ctrl_execute because it has additional information that I think we'd like to have as part of the record of provenance of the run.  The tests for this in ctrl_orca broke when this change over happened, so those need to be fixed as well.",NULL
DM-457,"Please provide permission to delete attachments from issues in JIRA","It's possible to add attachments, but not delete them.  For example, I just added some to DM-104 but it turned out that jira can't really handle tiff, so I'd like to replace them.... but I can't.  See https://confluence.atlassian.com/display/DOC/Deleting+an+Attachment ",NULL
DM-458,"Add ability to add screenshots to issues in JIRA","Currently you can attach files to issues, but not screenshots.  This is a nuisance.  Fortunately there's a free plugin that claims to support this;  can you install it?  https://marketplace.atlassian.com/plugins/com.atlassian.plugins.jira-html5-attach-images",NULL
DM-459,"event logging via orca needs to be reworked","I suspect this might be supplanted by the new logging system, but if it isn't, I wanted to get this issue documented here.  The version of the bin/Logger.py object included and started by Orca needs to be re-worked, if we intend to Orca to handle putting messages in the logging table of the database.   The current mechanism used is to have a process (bin/Logging.py) launched which monitors the logging topic, and puts messages into the database.  When there are large numbers of messages, this becomes a big bottleneck. In previous iterations of the pipelines, (DC2 and possibly DC3), the Logging process would have a significant backlog of messages to process long after the pipelines ended.  No messages were lost, but it did take a very long time for the queue to be drained.  In order to speed up the db inserts, the current version uses a shared memory file which would be used to write database records.  This in turn is sent to the database.  This isn't portable, and we just need a better mechanism to deal with this issue.  If the logging messages continue to be translated into DM events/messages, another solution to explore is to have the ActiveMQ broker insert those messages itself.  There are hooks in ActiveMQ to do this, but I haven't explored this.  The hooks would have to be written in Java.  This should use the DbAuth from daf_persistence, rather than having its own too.",NULL
DM-4592,"Error handling in Firefly","When we have time, we may want to go through Firefly to see if we want to make the error handling to behave the same way.   ",NULL
DM-4593,"background monitor for Firefly","This task is composed of:  - define what jobs need to be put in background job monitor  - defining data structure for background jobs  - converting server-side code to use messaging for job statuses  - creating action, action creator and reducing functions to handle status updates  - creating an advance  component to handle backgrounded jobs  - should have minimized view as well as full view.",NULL
DM-4594,"Make bookmark for a webpage in Firefly","user gets on the web portal, does some searches, manipulates the image and/or catalogs, does the XY plot for certain columns, adjusted colors for the image display. Now user wants to remember the state and shares it with someone.      user can make a bookmark of the current state of the web page and come back to it later. ",NULL
DM-4597,"DR-to-DR Object association requirements","In discussion with [~ktl], it's clear that he regards it as a requirement that there be an association between Objects from different data releases (and this seems entirely reasonable to me, for what that's worth). However, this requirement is not documented anywhere. Please add it to the DPDD.",NULL
DM-4598,"Requirements for detection of low surface brightness objects","In discussion with [~ktl] a requirement for binning to detect extended low-surface-brightness objects came up (he reckons this was done for SDSS). However, it doesn't seem to be included in any of our requirements documentation. Please add it.",NULL
DM-4599,"Fix publication URL in technote bootstrap to lsst.io","Update bootstrap to use *.lsst.io URL for publication URL.",NULL
DM-4602,"Support LaTeX/PDF documents with technote infrastructure","Rather than trying to turn Word/LaTeX documents into true HTML documents, lets allow those documents to remain true to their PDF form while also being technotes.    Technote HTML will wrap the PDF document, allowing a reader to download the PDF.  The PDF will be stored by Git (perhaps even Git LFS). It will be up to the author to build the PDF.    By adopting/extending the standard technote project format, we retain the standard metadata.yaml that will allow technotes to be indexed.",NULL
DM-4605,"provenance needs to be re-added to ctrl_orca","When all pex_policy was removed from ctrl_orca to move to pex_config, the functionality provided by ctrl_provenance was pulled because it only used pex_policy.   This needs to be re-added.",NULL
DM-4606,"Support getting coordsys from a Coord","Add {{getCoordSystem()}} to {{lsst::afw::Coord}}. Also add {{CoordSystem}} enum {{UNKNOWN = -1}} (without changing the values of the existing enums) and make {{Coord}} a virtual base class.    See RFC-118 for details. This ticket is intended to implement RFC-118 and so will be updated as required once RFC-118 is settled. (I filed this ticket before the RFC and then realized an RFC was required).",NULL
DM-4607,"epoch handled inconsistently in Coord","{{IcrsCoord}} can be constructed with an epoch, which makes sense as a date of observation (e.g. for handling proper motion).    However {{afwCoord.makeCoord(ICRS, ...}} raises an exception if an epoch is provided: ""ICRS has no epoch, use overloaded makeCoord with args (system, ra, dec)"". This makes no sense to me and is needlessly restrictive.    {{GalacticCoord}} is the other coordinate system for which an epoch is not necessarily useful, perhaps even less useful than for {{IcrsCoord}} because proper motion is less likely to be wanted. However, it is handled more consistently in that {{GalacticCoord}} cannot be constructed with an epoch and {{makeCoord(GALACTIC...)}} reports the error above. Note that {{GalacticCoord.getEpoch()}} is supported and returns 2000.0.     Given that {{GalacticCoord}} reports an epoch, I suspect we should allow setting the epoch, again for proper motion (no matter how unlikely to be wanted) and for consistency (simplifying unit tests that loop over coordinate system type and epoch).    In any case, I hope we can treat {{IcrsCoord}} and {{GalacticCoord}} the same way.",NULL
DM-4608,"Add OSX El Capitan boilerplate to templates package","The templates package, the prototypical stack package, did not receive the LSST_LIBRARY_PATH update required by OSX El Capitan (see [DMTN-001|http://dmtn-001.lsst.io] for details) ",NULL
DM-4618,"Make LSSTSW install work with tcsh","LSSTSW installation currently only works with bash.  I would like to use tcsh instead.",NULL
DM-4619,"Add README, CONTRIBUTING and GitHub summaries to each package","- The README file to each Stack package should follow a standardized template (to be developed in this ticket). This README should, at minimum, state the Python modules and tasks provided, a short description, and instructions for installing this package to a Stack (through links to pipelines.lsst.io). Additional content can also be included in a README, such as the example runs included in the validate_drp README. The README should also direct users to community.lsst.org and its support forum.  - The GitHub description line for each Stack package should be filled in. I currently propose a template {{python module — description}}, e.g. {{lsst.afw — LSST astronomy framework, including table and image processing tools}}.  - A CONTRIBUTING file should also be included in each repo to help both internal and external contributors. GitHub shows a link to this file at when new issues and PRs are made (https://github.com/blog/1184-contributing-guidelines). The CONTRIBUTING document should link to centrally maintained documentation in pipelines.lsst.io and developer.lsst.io.    It may be necessary to RFC templates for this content during the course of this ticket.",NULL
DM-4620,"Write doxygen documentation for all commandLineTasks in pipe_tasks","There is very little documentation on comandLineTasks which are the main stack utilities that are used for processing data.  Doxygen documentation (or similar) should be written for these that give the inputs, outputs, options and general outline of the steps in each program.  Also add a list of the important/often-used config options.",NULL
DM-4621,"Serialization to FITS needs to be documented","Serialization to FITS needs to be documented. There needs to be an ICD, a document that explains how the FITS file is laid out and how it maps from afw objects to FITS files.  ",NULL
DM-4622,"Add a meas_extension for DAOPHOT","Add a meas_extension for DAOPHOT to allow for simultaneous PSF photometry fitting.  This is especially important for dealing with crowded fields.  The current version of the stack does not work well in crowded fields.",NULL
DM-4623,"Add simultaneous PSF-fitting of overlapping objects","The current version of the stack does not perform simultaneous PSF-fitting of overlapping objects like most crowded field photometry codes do (e.g. DAOPHOT, DoPHOT, etc.).  The stack needs its own implementation of these algorithms.  But we should use DAOPHOT as a reference (DM-4622).",NULL
DM-4624,"Documentation on how to create a new obs package","There is currently very little documentation on how to create a new obs package.  Russell gave a presentation at the 2015 DM bootcamp on this that would be a good starting point.",NULL
DM-4626,"Write a tutorial on how to create a sky map","Creating a sky map for a data set is tricky and there are not good instructions. It would be great to have a tutorial that describes the various projections and how to set up a sky map for a given problem.    Here are some existing docs courtesy of Yusra:  -Doxygen: https://lsst-web.ncsa.illinois.edu/~buildbot/doxygen/x_masterDoxyDoc/skymap.html  -original design doc: https://dev.lsstcorp.org/trac/wiki/DM/SAT/SkyMap",NULL
DM-4627,"Fix reportPatches","reportPatches.py is supposed to report which patches a visit (or set of visits) overlaps, but it currently doesn't work.  The main error line is:    sqlite3.OperationalError: no such column: tract",NULL
DM-4629,"FITS files need to be self-documenting","The stack serialized FITS files need to be self-documenting, i.e. the HDUs need to have documentation in the headers that describe the data.  Otherwise people using the FITS files don't know what's in the files and how to interpret them.  This is especially true for people working outside the stack environment, like the IDL wrapper/workflow that Nidever is developing for the verification datasets effort.  ",NULL
DM-4633,"Add a clobber option to each command line task to deal properly with reprocessing data","Currently, command line tasks overwrite all existing files.  It would be useful to have an option (e.g., clobber) that specifies whether existing should be overwritten or not.  clobber = 0  leave existing output files alone  clobber = 1  overwrite all existing files (what's currently being done)    It would be useful if this condition is checked very early so little time is ""wasted"" by doing processing that won't result in an output file.  If someone wants the processing to happen (and use the outputs in the logs, for example) then they can use the ""doWriteOutput=False"" option and then clobber option would be ignored.",NULL
DM-4634,"Rename makeCaddTempExp to something less obscure","The makeCoaddTempExp command line tasks is an essential component of the DRP processing that warps images to a predefined projection on the sky.  The name is quite long and obscure.  It would be useful to rename it to something simpler and more self-explanatory like ""makeWarpedImage"".",NULL
DM-4635,"Split processCcd into separate ISR and calibration command line tasks","It would be very useful to have a command line tasks that just performs ISR and then have the rest of the steps in processCcd go in a separate ""calibrate"" task.  Since Russell is already rewriting the calibrate task, it would be natural (and easier) for this change to happen now.",NULL
DM-4636,"Make makeCoaddTempExp output warped files be at the chip level","The current output of makeCoaddTempExp is a file for each tract/patch/visit, note that chip was not included in that list even though the input calexps are at the chip level.  This means that generally all of the chips for a given visit will be in the warped file for a patch.  However, you can select a subset of chips in the --selectId and only those chips will about in the output file.  If you run the same patch/visit again but now with a different chip selected, then the output file gets overwritten with the new chip only.  This is a confusing situation and should be fixed.    One way to fix this would be only allow visits to be specified in --selectID in makeCoaddTempExp.  That way it's clear that it's always many input chip-level calexps to one visit-level warped image and there's no confusion.    However, my preferred option would be to have the outputs of makeCoaddTempExp be at the chip level as well.  This way the main purpose of the task is to warp an image (chip-level calexps) to a patch in the sky; one input and one output.  It simplifies the logic and will also make it easier for workflow/middleware/orchestration systems to keep track of things.  There'll be more output files (than before) but they are all in the same tract/patch/ directory and it should be easy to ""scoop"" them up in assembleCoadd (although I've never used it) and then coadd them.  ",NULL
DM-4638,"remove old generatedDag.py file","There's an old version of generatedDag.py laying around in the ctrl_orca directory, which I believe is obsolete.   This should be verified to be sure nothing is using it, and then remove this script if it is indeed obsolete.",NULL
DM-4640,"TAN-SIP WCS fitter should raise an exception if the fit is bad ","This ticket is related to DM-3549",NULL
DM-4654,"remote storage/sharing of terraform state","Allow terraform state to be shared between multiple developers for redeployment/maintenance of the CI system.",NULL
DM-4658,"Port code of conduct to new docs","Port: https://confluence.lsstcorp.org/display/LDMDG/Team+Culture+and+Conduct+Standards?src=contextnavpagetreemode",NULL
DM-4659,"Documentation for processCcd","To verify and test the DM science pipelines we are running verification datasets through both the AP and DRP pipelines which essentially means a handful of commandLineTasks.  This is challenging because many of the people working on the verification datasets are new to DM and there is very little documentation on the main commandLineTasks.  Therefore, we request that Doxygen documentation (or similar) should be written for these tasks that give the inputs, outputs, options and general outline of the steps in each program. Also, a list of the important/often-used config options would be extremely useful especially for tasks that hundreds of config options.    This ticket is specifically for processCcd.",NULL
DM-4660,"Documentation for makeCoaddTempExp","To verify and test the DM science pipelines we are running verification datasets through both the AP and DRP pipelines which essentially means a handful of commandLineTasks.  This is challenging because many of the people working on the verification datasets are new to DM and there is very little documentation on the main commandLineTasks.  Therefore, we request that Doxygen documentation (or similar) should be written for these tasks that give the inputs, outputs, options and general outline of the steps in each program. Also, a list of the important/often-used config options would be extremely useful especially for tasks that hundreds of config options.    This ticket is specifically for makeCoaddTempExp.  ",NULL
DM-4661,"Documentation for assembleCoadd","To verify and test the DM science pipelines we are running verification datasets through both the AP and DRP pipelines which essentially means a handful of commandLineTasks.  This is challenging because many of the people working on the verification datasets are new to DM and there is very little documentation on the main commandLineTasks.  Therefore, we request that Doxygen documentation (or similar) should be written for these tasks that give the inputs, outputs, options and general outline of the steps in each program. Also, a list of the important/often-used config options would be extremely useful especially for tasks that hundreds of config options.    This ticket is specifically for assembleCoadd.  ",NULL
DM-4662,"Documentation for processCoadd","To verify and test the DM science pipelines we are running verification datasets through both the AP and DRP pipelines which essentially means a handful of commandLineTasks. This is challenging because many of the people working on the verification datasets are new to DM and there is very little documentation on the main commandLineTasks. Therefore, we request that Doxygen documentation (or similar) should be written for these tasks that give the inputs, outputs, options and general outline of the steps in each program. Also, a list of the important/often-used config options would be extremely useful especially for tasks that hundreds of config options.    This ticket is specifically for processCoadd.",NULL
DM-4663,"Documentation for forcedPhotCcd.py and forcedPhotCoadd.py","To verify and test the DM science pipelines we are running verification datasets through both the AP and DRP pipelines which essentially means a handful of commandLineTasks.  This is challenging because many of the people working on the verification datasets are new to DM and there is very little documentation on the main commandLineTasks.  Therefore, we request that Doxygen documentation (or similar) should be written for these tasks that give the inputs, outputs, options and general outline of the steps in each program. Also, a list of the important/often-used config options would be extremely useful especially for tasks that hundreds of config options.    This ticket is specifically for forcedPhotCcd.py and forcedPhotCoadd.py  ",NULL
DM-4664,"Documentation for imageDifference.py","  To verify and test the DM science pipelines we are running verification datasets through both the AP and DRP pipelines which essentially means a handful of commandLineTasks.  This is challenging because many of the people working on the verification datasets are new to DM and there is very little documentation on the main commandLineTasks.  Therefore, we request that Doxygen documentation (or similar) should be written for these tasks that give the inputs, outputs, options and general outline of the steps in each program. Also, a list of the important/often-used config options would be extremely useful especially for tasks that hundreds of config options.    This ticket is specifically for imageDifference.  Some documentation exists in the code but is insufficient and needs improvement.  ",NULL
DM-4669,"Remove obs_decam initial PSF FWHM config override value","obs_decam config override has calibrate.initialPsf.fwhm=2.0 arcsec.  This is far too large.  We decided on the RFD discussion on ""initial PSF FWHM"" to remove this config override and use the default value in the calibrate task.  The idea is to tweak the calibrate default value to make it work pretty much for all data.",NULL
DM-4670,"Add explicit EUPS packages for pyfits dependencies","{{pyfits}} depends on two external packages: {{stsci-distutils}} and {{d2to1}}. These modules were not put in EUPS packages because it may not have been noticed that {{pyfits}} attempts to download them itself when it is built.    Even though these packages are only required at build time and so are not used by the stack itself, it is more reliable to package them up in EUPS so that they can be installed without requiring calls to PyPI. Historically some people have had issues installing the stack because of issues with downloading these packages (generally certificate failures). [see hipchat room Software Development on 2015-12-15 at 1:30pm PST.]",NULL
DM-4674,"Prototype web SSO server","The Web SSO Server supports authentication using LSST credentials (passwords and second factor tokens). It provides single sign-on across multiple LSST web applications via the SAML and OpenID Connect standards.    Example Implementations: Shibboleth IdP, WS02 Identity Server, CILogon, Globus, ADFS, SimpleSAMLphp",NULL
DM-4675,"Prototype web SSO client (service provider)","The Web SSO Client provides a shared front-end for authentication to LSST web applications. It supports authentication using LSST credentials via the Web SSO Server as well as authentication using external credentials via federated identity providers (e.g., InCommon members, GitHub, Google). It implements the SAML, OpenID Connect, and OAuth 2.0 standards.    Example Implementations: Shibboleth SP, mod_auth_openid, WS02 Identity Server, SimpleSAMLphp",NULL
DM-4676,"Demonstrate InCommon authentication","Demonstrate authentication from incommon.org identity providers to the prototype web SSO server (DM-4674)",NULL
DM-4684,"Adapt desc/Twinkles cookbook doc as a Stack coaddition + forced photometry tutorial","Simon has a sweet tutorial on taking OpSim outputs and doing forced photometry on them. We should adapt this into a tutorial for the stack docs.    See https://github.com/DarkEnergyScienceCollaboration/Twinkles/blob/master/code/twinkles_cookbook.md",NULL
DM-4685,"FY18 Advanced Shared Scan Optimizations","Advanced performance optimizations and tuning of the shared scans. This includes:  *  Reschedule user queries on a different scheduler. Two possible parts to this are detecting slow queries and moving them to a slower/lower priority scan scheduler and the other would be having the czar tell the workers to move all tasks for a user query to a specific scheduler. These require unique user query ids. ",NULL
DM-4686,"obs_cfht needs to be updated to support multiband processing","* MegacamMapper.paf and megacamMapper.py need to be updated to handle multiband processing.  * a filter priority list should be set up for mergeCoaddDetections",NULL
DM-4687,"Better documentation of how TestCase gets extended","It is difficult to determine what assert methods might be available when writing tests because {{lsst.utils.tests.TestCase}} can be extended via the {{@lsst.utils.tests.inTestCase}} decorator. It would be very useful to have documentation that describes where to look for what extensions have been applied to a given package. {{utils.tests.TestCase}} should at least have a list of such places, if not direct links or some other way to document them.",NULL
DM-469,"Setting up build slaves","Configure buildbot slave on lsst-dev. Initially the owner will be 'lsstsw2'.",NULL
DM-4693,"undeclared dependencies on astropy.io and yaml","While installing the stack on a new Ubuntu 14.04 box, using system Python, I encountered test/build problems that seem to indicate undeclare dependencies:     - daf_persistence requires a {{yaml}} python module   - daf_butlerUtils requires {{astropy.io}}    It was easy for me to work around these problems (with .deb packages and pip), so this is not a blocker for me, but I'm worried that we're accidentally subverting our dependency management system with some things provided by anaconda.    (Assigning to [~tjenness] for now since he seems likely to have been involved in any previous conversations or existing tickets on this subject, and hence may be able to quickly triage).",NULL
DM-4715,"Use wkhtmltopdf to include a PDF preview of technotes for archival in Zenodo","[wkhtmltopdf|http://wkhtmltopdf.org] is a headless command line tool for converting HTML pages into PDFs (using webkit). We can use this tool at Zenodo upload time to include a PDF preview of the technote for those browsing the lsst-dm collection on Zenodo.    The conversion should probably be done by the [technorati|http://github.com/lsst-sqre/technorati] microservice so that authors don’t have to install wkhtmltopdf.    The [techno|https://github.com/lsst-sqre/techno] authoring tool can trigger a PDF build on technorati, rendezvous the PDF into the Zenodo upload for the user, then delete the local PDF so that it never ends up being added to the Git repository",NULL
DM-4725,"Implement zenodio.upload","Upload an artificat into a Zenodo Community collection using Zenodo’s REST API format. See https://zenodo.org/dev    Part of the [zenodio|https://github.com/lsst-sqre/zenodio] Python package. This tool will be used by our technote platform to automate the process of publishing technotes.",NULL
DM-4726,"Implement a techno init command to create new technotes","Currently new technotes are created with [lsst-technote-bootstrap|https://github.com/lsst-sqre/lsst-technote-bootstrap] and cookiecutter. It’s time to make technote authoring *even easier* by creating a custom tool to automate more of the process. This is the role of {{techno}}, and this ticket will create the first subcommand in {{techno}}.    This ticket will    1. Create the {{techno}} Python project  2. Create an {{init}} command that uses {{cookiecutter}}’s python api, dynamically suggests the current date, and creates a repo on GitHub.",NULL
DM-4727,"Create techno archive command to publish a technote to zenodo","{{techno archive}} will    1. Gather metadata  2. Reserve a DOI with Zenodo using the REST API  3. Git tag the version  4. Have Read the Docs publish the tagged version.  5. Trigger a PDF build of the technote  6. Package the built HTML along with the build PDF  7. Publish the technote and its metadata to Zenodo with the REST API  ",NULL
DM-4732,"butler parentSearch incorrectly returns '[]' instead of 'None' in one case.","The contract for `daf.butlerUtils.CameraMapping.parentSearch` states that `None` will be returned if no matches are found.    But in one of the options  `            if os.path.realpath(pathPrefix) != os.path.realpath(root):`  the code says `return []` instead of `return None`.    This is then not caught properly by `daf.butlerUtils.mapping.map`, which was checking just for the return value `is not None`.      The behavior of parentSearch is clearly an error (it's againt the documentation for the function).    I would also suggest that `daf.butlerUtils.mapping.map` should check more robustly for `if newPath:` rather than `if newPath is not None:`.    Thus we explicitly set up this if/else to return `None` if `not paths` instead of `[]`.    ",NULL
DM-475,"Create LSSTsim data for processing","Create appropriate LSST PhoSim data to use for an example of processing such data. The dataset need not be large in spatial extent (perhaps a raft, or even a single CCD), but should include the following properties:  - single area of sky (though with arbitrary rotation), consistent with selected FPA spatial extent - at least 3 passbands  - Reasonable spread in brightness (saturation limit, down to single-visit depth) - include stars with representative colors; including galaxies is optional - span a reasonable range in time (perhaps a year)  The above properties would result in a dataset that could be used to illustrate a variety of processing and visualization demonstrations.   It may be possible to use an existing lsstSim dataset (even if not all of the above criteria are met), and a published catalog to begin building the demonstration. ",NULL
DM-4750,"git push of an LFS didn't show password prompt","I get Username/Password prompty for s3.lsst.codes on `git clone` but not on `git push`.    E.g.,    {code}  [serenity validation_data_cfht] git push -u origin master  Git LFS: (0 of 5 files) 0 B / 692.19 MB                                                                                              P  Git LFS: (0 of 5 files) 0 B / 692.19 MB                                                                                              P  {code}    just repeats with a new line, perhaps in response to a timeout.    When I finally hit enter, the files proceeded to upload.",NULL
DM-4752,"Build on Mac very slow due to running fc-list","Builds on MacOS 10.11 have been painfully slow (for instance 23 minutes to build {{afw}} instead of the more typical 12 minutes, 30 minutes to rebuild {{ip_diffim}}, 20 minutes to rebuild {{meas_astrom}}) and much of this time is spent running fc-list in 8 cores.    I suspect {{matplotlib}} is triggering this process, but I have not verified it.    I see this using lsstsw to build a fresh stack and with manual builds.    A workaround is to repeatedly kill {{fc-list}}, e.g. with this bash script:  {code}  while sleep 1; do pkill fc-list; done  {code}    I checked my fonts with Apple's Font Book and found a few dozen with ""minor errors"" that I deleted, but nothing serious. I'm still seeing the problem.",NULL
DM-4757,"LOGS usage gcc 5 incompato ","Gcc 5.2.1 chokes on passing an ostringstream to LOGS; requires explicit .str().   This usage seems only to have occurred in one file, so fix was trivial.",NULL
DM-476,"Document how users can identify Stripe 82 data in a sky region","The tutorial on Co-Add processing on SDSS data presumes that users can identify all Stripe 82 data that overlaps with the sky region of interest. Currently DM users can do this through an internal database, which records a variety of parameters such as image quality, the RA,Dec of the image corners, and so on. This task is to document either an external way to do that, or suggest a way that external users can access the DM database for this purpose. ",NULL
DM-4769,"Details for racks","Gathered details for racks, pdus, etc.  Research new options for PDUs",NULL
DM-477,"Send email when a buildbot run fails.","When an operation initiated on the buildslave fails, either send email to  [lsst-data] (or equivalent developer list) if it was an lsst-build failure  or to the buildbot nanny if it was an error when setting up the build process. ",NULL
DM-4770,"Week end 12/05/15","Support for lsst-dev cluster, OpenStack, and accounts  for week ending December 5, 2015.",NULL
DM-4776,"Openstack expansion setup and configuration (week end 12/05/15)","* Moved systems to make room for openstack expansion",NULL
DM-4779,"Queries with unrecognized function hang","A query that has stored function which is not supported hangs for ever. Example query that will run into that issue:    {code}SELECT objectId FROM Source where xxx(1, 2) < 0.1{code}    The message in czar log:    {code}  TaskKill[2016-01-08T17:45:57.963-0600] [0x7f71a29f0700] DEBUG rproc.InfileMerger (core/modules/rproc/InfileMerger.cc:276) - Executing InfileMerger::merge(sizes=59, 132, rowcount=0, errCode=0hasErrorMsg=1)  : [0x7f7264007bc0] [2016-01-08T17:45:57.963-0600] [0x7f71a29f0700] ERROR rproc.InfileMerger (core/modules/rproc/InfileMerger.cc:280) - Error in response data: [0] Error(s) in result for chunk #7308: [1370] execute command denied to user 'qsmaster'@'localhost' for routine 'mysql.xxx'  {code}",NULL
DM-478,"Allow user-triggered builds initiated from the Buildbot web form and which allow input of the git-refs to be used during the build","Users will be allowed to initiate builds from the Buildbot web form. They will initially use an htpasswd login/password. It is anticipated the authentication method will eventually be transitioned into LDAP - but not during this pass.  The users will be able to specify a prioritized list of git refs specifying the branches to be scanned when acquiring a package from a git repository.  May also want to allow the user to tag the build with some user-memorable eups-tag so (1) acquiring the build manifest and/or  (2) setting up the identical stack for debug, are simple operations.",NULL
DM-4787,"Copyright sample template is from 2014","The default Copyright notice on Confluence says '2008-2014'.     a)  This should be 2008-2016  b)  Given that this hasn't been updated since 2014, is this page still current?    https://www.lsstcorp.org/LegalNotices/LsstSourceCopyrightNotice.txt  ",NULL
DM-479,"A stack rebuild should be automatically initiated whenever git repository master branch has been updated.","When the master branch of any LSST stack source repository has been updated, buildbot should initiate a stack rebuild of the master branch.",NULL
DM-4792,"Add support for VIEWs in Qserv","For now it is a placeholder that will help us remember we need to handle VIEWS. More details coming.",NULL
DM-4795,"Support per-mapper default filter name","A {{CameraMapper}} subclass should be able to specify a default filter name to be used when the filter is not found in {{self.filters}}. This will allow processing images with filter names that have not been previously anticipated, as might be desired for onsite analysis.    This is a port of work performed on [HSC-987|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-987].",NULL
DM-4796,"Allow ISR to fall back to a different filter when loading calibrations","The ISR task configuration should enable the user to specify a ""fallback"" filter which will be used when calibrations for a particular filter aren't available.    This is a port of work performed on [HSC-987|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-987].",NULL
DM-4797,"SafeClipAssembleCoaddConfig.setDefaults() does not call the correct superclass","{{assembleCoadd.py}} in {{pipe_tasks}}  {code:python}  637 class SafeClipAssembleCoaddConfig(AssembleCoaddConfig):  [...]  670     def setDefaults(self):  671         # The numeric values for these configuration parameters were empirically determined, future work  672         # may further refine them.  673         pexConfig.Config.setDefaults(self)  674         self.clipDetection.reEstimateBackground = False  {code}    Line 673 should call to the superclass's {{setDefaults()}} method; instead, it is bypassed and we jump straight to {{pexConfig.Config.setDefaults()}}.",NULL
DM-48,"S14 Design/hackathon week, scheduled for Apr 28 - May 2 @ SLAC","Prep and run the Qserv Design Week, including all logistics, planning, agenda etc.",NULL
DM-4800,"Use simpler method of converting from Mask to FootprintSet","Our current method of converting a {{Mask}} to a {{FootprintSet}} is memory intensive. A more efficient version is available on [HSC-1231|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1231].",NULL
DM-4803,"Coordinates in output source files should be in degrees","Currently, coordinates (coord_ra and coord_dec) in source catalogs (FITS files) are in radians with not units or documentation in the file itself.  The astronomical convention is to use degrees for coordinates (or hours for RA, but that is falling out of favor).  I think that we should follow this standard and make it a LSST convention to always use degrees for coordinates in ""external"" files that people might load from other packages (the internal storage of coordinates is a separate issue).  Also, we should put the units in the file for documentation purposes and so it will be easier to interpret the results on ingest.    This might need a RFC but since I'm not sure if there's anyone available to work on this right now I'm just going to file a ticket.",NULL
DM-4804,"AFW math makeStatistics can't handle numpy arrays with non default datatypes","AFW math makeStatistics fails when a numpy array does not use the system default datatype:  example:    {code}  In [1]: import lsst.afw.math as afwMath    In [2]: import numpy as np    In [3]: vector = np.arange(10.0)    In [4]: vector  Out[4]: array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])    In [5]: afwMath.makeStatistics(vector, afwMath.MEDIAN).getValue()  Out[5]: 4.5    In [6]: vector = np.arange(10.0, dtype='float32')    In [7]: vector  Out[7]: array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.], dtype=float32)    In [8]: afwMath.makeStatistics(vector, afwMath.MEDIAN).getValue()  ---------------------------------------------------------------------------  NotImplementedError                       Traceback (most recent call last)  <ipython-input-8-6d3f57153d0b> in <module>()  ----> 1 afwMath.makeStatistics(vector, afwMath.MEDIAN).getValue()    /Users/nate/lsstsw/lsstsw/stack/DarwinX86/afw/2015_10.0-14-g7c5ed66/python/lsst/afw/math/mathLib.pyc in makeStatistics(*args)     5419     makeStatistics(lsst::afw::math::MaskedVector< int > const & mv, vectorF vweights, int const flags) -> Statistics     5420     """"""  -> 5421   return _mathLib.makeStatistics(*args)     5422 class Interpolate(_object):     5423     """"""Proxy of C++ lsst::afw::math::Interpolate class""""""    NotImplementedError: Wrong number or type of arguments for overloaded function 'makeStatistics'.  {code}    ",NULL
DM-481,"Unit tests install directory","Hello,  Tests target seems to be correctly defined in core/modules/SConstruct (cf. getTests() function), but tests doesn't seems to be nor built or runned.  In DM-58 branch i've introduced a few line of code which now build the tests.  For now, tests binaries are located in build/moduleName/ directory, do you think we should let the tests here or install it :  - in the Qserv install directory ? - in a tests/ subdirectory of Qserv install directory ? - other solution ? - do we set an option to scons procedure in order to build, and run, these unit tests ?  Thanks,  Fabrice",NULL
DM-4812,"Improve parser in mysqlproxy","MySQL proxy relies on simple regexp to decide if a query should be routed to qserv, processed locally, or perhaps ignored. It'd be nice to have a more robust parser there. This story involves writing such parser for proxy.",NULL
DM-4813,"Skip obs_cfht butler mapper test if datadir isn't found.","All of the {{test*}} functions in {{tests/testButler.py}} were disabled if the {{datadir}} wasn't found except for {{testPackageName}}.  However, that test uses the butler as well, so if there's no {{datadir}} then that test must be skipped as well.  Fix is to do:    {code}      def testPackageName(self):          if not self.runTests:              return          self.assertEqual(self.butler.mapper.packageName, ""obs_cfht"")  {code}",NULL
DM-4816,"Please add a way to get a Filter's canonical name","We support aliases for filters (e.g. NAOJ calls Sloan r ""W-S-R+""), but there's no way to ask for the canonical name of a filter. Please rectify this.    (This issue originally filed by [~rhl] as [Trac #2113|https://dev.lsstcorp.org/trac/ticket/2113]; refer to further discussion there.)",NULL
DM-4818,"Unable to install `psycog2` using `lsstsw","Build {{ci_hsc}} fails on {{psycog2}} dependency    {code}  [serenity lsstsw] rebuild psycog2               psycog2: Traceback (most recent call last):    File ""/Users/wmwv/lsstsw/lsst_build/bin/lsst-build"", line 41, in <module>      args.func(args)    File ""/Users/wmwv/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 746, in run      manifest = p.construct(args.products)    File ""/Users/wmwv/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 709, in construct      self._add_product_tree(products, name)    File ""/Users/wmwv/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 680, in _add_product_tree      ref, sha1 = self.product_fetcher.fetch(productName)    File ""/Users/wmwv/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 296, in fetch      raise Exception(""Failed to clone product '%s' from any of the offered repositories"" % product)  Exception: Failed to clone product 'psycog2' from any of the offered repositories  {code}     (e.g., building {{obs_subaru}} with {{lsstsw}} {{a4d9de0}} and {{lsst_build}} {{015c01a}} current fails on the {{psycopg2}} dependency).",NULL
DM-4819,"investigate distributing automated lsst_apps builds via conda packages","Initial investigation starting with Mario's conda-lsst work.  Note that this work was started on 2016-01-12 but is being moved to its own ticket.",NULL
DM-4828,"Qserv Integration Tests","This epic captures all stories in backlog related to qserv integration tests.",NULL
DM-483,"Update LSST overview paper","Need to update the data management section of the LSST overview paper for the PST/Zeljko. Needs to be finished by ~end of April.",NULL
DM-4832,"Add new HSC-I2 filter","Add filter definitions and colour terms for the new HSC filter.    This is a port of [HSC-1374|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1374].",NULL
DM-4838,"Fix overlapping validity range for calibs","Per DM-3766, [~hchiang2] spotted a problem in the way that {{obs_subaru}}'s {{genCalibRegistry.py}} adjusts {{validStart}} and {{validEnd}} to avoid conflicts. This has been fixed in {{pipe_tasks}}'s {{ingestCalibs.py}}, but is still an issue with {{obs_subaru}}'s {{genCalibRegistry.py}}.    It's not clear if the latter is actually still relevant (DM-4740); on the assumption it is, the fix from [HSC-1362|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1362] should be applied.",NULL
DM-484,"Separate meas_photocal from meas_astrom","The SFM photometric calibration is currently part of the meas_astrom product (although imported as {{lsst.meas.photocal}}).  It needs to become its own top-level product.  We should take the opportunity to rewrite it (along with meas_astrom).  For example, it currently hardwires algorithm names rather than using configs to specify types of algorithm.  ",NULL
DM-4843,"MeasureMergedCoaddSourcesTask: do not initialize astrometry unless we need it","{{MeasureMergedCoaddSourcesTask}} only uses an astrometry subtask if {{doWriteMatches}} is {{True}}. Please don't initialize it unless it's required.    This work was originally part of [HSC-1343|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1343].",NULL
DM-4844,"Make it possible to run MeasureMergedCoaddSourcesTask without propagating flags","Propagating flags requires matching against CCD visit catalogues or astrometry.net, which is inconvenient if the relevant data files aren't available. Please make it optional.    This is a port of functionality implemented on [HSC-1343|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1343]; see also DM-4843 for other work on that ticket.",NULL
DM-4845,"Revert changes to variance in deblending","In DM-2914 (specifically, in [73ed9bc8|https://github.com/lsst/meas_deblender/commit/73ed9bc816a3fd5613b8c5c2e0eb1ce77b22bb7e]) a change was made to the calculation of variance.    This is incorrect: while deblending does split up the noise part of the pixel values as it splits up the signal, we don't want to account for that in the variance plane, because we're even more uncertain about value of deblended pixels than we are unblended pixels. Please revert it.    This is a port of [HSC-1334|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1334].",NULL
DM-4846,"MeasureMergedCoaddSourcesTask should not derive a new astrometric solution","{{MeasureMergedCoaddSourcesTask}} needs an astrometry task to match sources, but it does not need to derive a new astrometric solution. Indeed, doing so is not merely wasteful, but incorrect: the derived solution is not persisted, but source positions calculated based on it are, and cannot therefore be reproduced    This is a port of [HSC-1318|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1318].",NULL
DM-485,"Reconsider design of afw::table::SchemaMapper","The current afwTable::SchemaMapper is probably capable of doing anything you need, but the only way to do it is to go to Jim's office and ask.  We should reconsider the overall API to make easy things easy.  The problem is made worse by the fact that some of Jim's tricks don't show up in doxygen as they are in swig extend blocks.  (e.g. to copy a table adding a column for a new type of flux you need something like: {code:python} def copyCatalogAddingField(srcCat, extraFluxColumn):     """"""Copy a catalog srcCat, adding (but not setting) an extra flux column""""""     scm = afwTable.SchemaMapper(srcCat.getSchema())     scm.addMinimalSchema(srcCat.getSchema(), True)     scm.addOutputField(afwTable.Field[""D""](extraFluxColumn, ""The name used by the PhotoCal code""))     scm.addOutputField(afwTable.Field[""D""](extraFluxColumn + "".err"", ""error for %s"" % extraFluxColumn))     scm.addOutputField(afwTable.Field[""Flag""](extraFluxColumn + "".flags"",                                               ""flags for %s"" % extraFluxColumn))      cat = afwTable.SourceCatalog(scm.getOutputSchema())     #     # Define the needed slots     #     cat.table.defineCentroid(srcCat.getCentroidDefinition())     for define, getDefinition in [(cat.table.defineApFlux,    srcCat.table.getApFluxDefinition),                                   (cat.table.defineInstFlux,  srcCat.table.getInstFluxDefinition),                                   (cat.table.defineModelFlux, srcCat.table.getModelFluxDefinition),                                   (cat.table.definePsfFlux,   srcCat.table.getPsfFluxDefinition),]:         try:             define(getDefinition())         except Exception as e:             pass      cat.extend(srcCat, True, scm)      return cat {code} ",NULL
DM-4853,"Software dependency docs should be generated from stack puppet config","Use the Puppet config at https://github.com/lsst-sqre/puppet-lsststack/blob/master/manifests/params.pp to generate the stack dependency documentation. This work involves parsing the puppet file and adding content to reStructuredText, probably as a documenteer Sphinx extension.",NULL
DM-4854,"Make astrometry_net_data return an error on use","The current astrometry_net_data package in github does not actually provide any data. It is a placeholder so that any packages which depend on it won't fail on setup. However, any attempted usage of the package will fail, and it is unclear to the user why this (intentional) failure is occurring.    This ticket will 1) add a README to the package to explain that it is a placeholder and 2) add an andConfig.py file that will print an error message to the user if they attempt to use it for astrometry.  ",NULL
DM-4855,"Meta ticket for HSC MPI high level task port","This ticket incorporates work to be done in [DM-3368|https://jira.lsstcorp.org/browse/DM-3368], [DM-3369|https://jira.lsstcorp.org/browse/DM-3369], and [DM-3370|https://jira.lsstcorp.org/browse/DM-3370]. Much of the work may depend on each other, and testing will be made much easier if the work for each of these tickets could share a common branch. This will avoid a situation where a given package may need to be set up on two different ticket branches simultaneously. ",NULL
DM-4859,"DECam postISRCCD products of south CCDs have shifted wcs","With the current master and default config, astrometry fails when processing raw data of south CCDs. Warning message:   {code:java}  processCcd.calibrate WARNING: Unable to perform astrometry (Unable to match sources): attempting to proceed  processCcd.calibrate WARNING: Failed to determine photometric zero-point: No matches available  {code}      Matching fails because the wcs of the input {{postISRCCD}} exposures are shifted by a large amount.  {{processCcd.py}} astrometry can run successfully with the config {{calibrate.astrometry.matcher.maxOffsetPix=1400}}.     This plot !patchesCmp35a.png|thumbnail! shows the wcs of 2 neighboring CCDs, N4 (35) and S4 (28), by drawing the ra/dec boxes from {{raw}} (black), {{postISRCCD}} (blue), and {{calexp}} (red).      The shift appears because the first amplifier in a detector is assumed to be the amplifier at the lower left corner. The assumption is not true for all south chips of the current DECam camera geometry setup, and hence introduces a large shift in the wcs of the postISRCCD products.       One way to fix this is to modify the amplifier segments file {{obs_decam/decam/segmentfile.txt}} and ensure the lower-left amplifier is the first amplifier of a detector.  With the changes in {{obs_decam}} of this ticket, {{processCcd.py}} can run with the default config (including those in master obs_decam/config/processCcd.py and  isr.assembleCcd.setGain=False DM-4232). The same wcs plot becomes !patchesCmp35b.png|thumbnail!      Note that the {{raw}} image (black) include prescan/overscan regions. {{postISRCCD}} (blue) is trimmed. Otherwise they share the same ra/dec boxes as they should. !illustration-sky.png|thumbnail! is an illustration of the two CCDs in sky orientation (not to scale).",NULL
DM-486,"Test issue","This is a test issue, please ignore it.",NULL
DM-4860,"SourceDetectionTask can corrupt image","SourceDetectionTask can silently corrupt the provided exposure by fitting and subtracting an invalid background model that that results in the image being all nans. I believe the task should realize that the background model is invalid and raise an exception in this situation.    I first saw the problem with an image that had several hundred counts of background. Detection detected a single source and the results were as described. I have attached code demonstrating the issue. Setup `meas_algorithms` and run `showBackgroundBug.py` to see the problem.  ",NULL
DM-4863,"FY17 Webserv Improvements","This epic captures all future stories related to webserv/metaserv/imgserv that are not assigned to any cycle-related epic.",NULL
DM-4881,"Check that image and mask have the same size when computing statistics","If the image and mask don't have the same size, we can get a NAN result if all pixels in the lower-left are masked. This patch add a check on the size of all images to catch accidental user errors.    This is a (partial) port of work performed on [HSC-1290|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1290] (for the HSC-1290 changes to {{meas_extensions_shapeHSM}}, see DM-3384).",NULL
DM-4883,"Refactor mysql and sql modules","sql and mysql modules in qserv/core need to be refactored / reorganized.",NULL
DM-4890,"collection of stories done by IRSA/IPAC ","This is an epic to collect the stories done by IRSA group in IPAC as part of their contribution to Firefly development.",NULL
DM-4891,"Upgrade doxygen to latest version","The flex fixes manually applied in DM-4728 are now part of an official doxygen release. This ticket concerns updating the LSST doxygen to that version (1.8.11) and testing the builds.    Doxygen is tagged ""u"" on the DM Third party packages page on confluence so an RFC is not required before initiating an upgrade.",NULL
DM-4896,"Add PyYAML EUPS package","Add PyYAML EUPS packages.",NULL
DM-4900,"Generate Doxygen XML output via sconsUtils","We’re using [Breathe|http://breathe.readthedocs.org/en/latest/] to bridge C++ API references built with Doxygen in to the Sphinx documentation site builder. Breathe uses Doxygen’s XML output. This is a trivial modification to the existing scons documentation build target.",NULL
DM-4902,"add an ""update"" feature to lsstsw","There have been a few uesrs reporting difficulty with updating lsstsw on the HC #square channel.  A new convenience script to automate this process or improvement to the deploy script would be helpful for this use-case.",NULL
DM-4905,"PsfexStarSelector uses an undefined variable","{{PsfexStarSelector}} contains the following code. The variable {{vignet}} is not defined and I have not found a likely-looking alternative:  {code}          #-- ... and check the integrity of the sample          if maxbadflag:              nbad = np.array([(v <= -psfex.psfex.cvar.BIG).sum() for v in vignet])  {code}  ",NULL
DM-4910,"PO security meeting","Security planning/meeting with PO.",NULL
DM-4913,"lsst_dm_stack_demo processCcd warning","A series of new ""warnings"" has appeared in the output of the stack demo as of b1856 on 2016-01-15.  Should these ""warnings"" be fatal?    http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/9283/steps/shell/logs/stdio    {code:java}  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797076: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797157: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797161: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797163: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797167: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797177: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797190: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797195: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797198: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797199: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797201: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797202: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797212: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797222: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797224: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797226: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797227: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797242: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797256: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797317: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797325: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797396: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797555: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797584: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797615: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797658: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797659: The center is outside the Footprint of the source record  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797672: The center is outside the Footprint of the source record    {code}  ",NULL
DM-4914,"mysqlclient product should build from the same sources as mariadb","I think it makes sense, in terms of defensive packaging, for the {{mysqlicent}} and {{mysql (maraidb)}} products to be built from the same source tarball and to be upgraded syncronously in the future.",NULL
DM-4918,"rename anaconda package -> lsst-dm/legacy-anaconda","After DM-2993 is completed, the anaconda package should be renamed as {{legacy-}} to discourage any future use.",NULL
DM-4922,"document minimum python version + minimal env","The minimum python 2.7.x version, I believe, is essentially unknown. I propose that we document 2.7.10 as the minimum supported version.    We also need to articulate what we expect to be present in the python env.  At this point, I think that is essentially {{setuptools}} and the python header files (python.h).",NULL
DM-4924,"Please provide transparent access to compressed files","I just noticed that if we compress files on disk (using gzip in this case) that the butler can't read them, although cfitsio can. That is,  {{calexp = butler.get(""calexp"", dataId)}}  fails but  {{calexp = afwImage.ExposureF(butler.get(""calexp_filename"", dataId))}}  works.    SDSS's butler-equivalent checked for a variety of suffixes. for example  {{["""", "".gz"", "".fz""]}}  and handled this transparently, searching for files with one of these suffixes appended until a match was found.    Please provide this, or equivalent, functionality.",NULL
DM-4927,"ci_hsc doesn't produce readable processCcd_metadata","When I run {{scons}} in {{ci_hsc}}, it fails as follows:  {code}  partial(["".scons/sfm-903334-16""], [""DATA/registry.sqlite3"", ""DATA/CALIB"", "".scons/sfm""])  : Validating dataset processCcd_config for {'ccd': 16, 'visit': 903334}  CameraMapper: Loading registry registry from /raid/swinbank/src/ci_hsc/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /raid/swinbank/src/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  : processCcd_config exists: PASS  : processCcd_config readable (<class 'lsst.pipe.tasks.processCcd.ProcessCcdConfig'>): PASS  : Validating dataset processCcd_metadata for {'ccd': 16, 'visit': 903334}  : processCcd_metadata exists: PASS  scons: *** [.scons/sfm-903334-16] Exception : input stream error  Traceback (most recent call last):    File ""/nfs/home/lsstsw/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Action.py"", line 1062, in execute      result = self.execfunction(target=target, source=rsources, env=env)    File ""/raid/swinbank/src/ci_hsc/python/lsst/ci/hsc/validate.py"", line 116, in scons      return self.run(*args, **kwargs)    File ""/raid/swinbank/src/ci_hsc/python/lsst/ci/hsc/validate.py"", line 92, in run      self.validateDataset(dataId, ds)    File ""/raid/swinbank/src/ci_hsc/python/lsst/ci/hsc/validate.py"", line 67, in validateDataset      print(data.__class__)    File ""/home/lsstsw/stack/Linux64/daf_persistence/2015_10.0-6-g5a5f333+5/python/lsst/daf/persistence/readProxy.py"", line 41, in __getattribute__      subject = oga(self, '__subject__')    File ""/home/lsstsw/stack/Linux64/daf_persistence/2015_10.0-6-g5a5f333+5/python/lsst/daf/persistence/readProxy.py"", line 136, in __subject__      set_cache(self, get_callback(self)())    File ""/home/lsstsw/stack/Linux64/daf_persistence/2015_10.0-6-g5a5f333+5/python/lsst/daf/persistence/butler.py"", line 279, in <lambda>      callback = lambda: self._read(pythonType, location)    File ""/home/lsstsw/stack/Linux64/daf_persistence/2015_10.0-6-g5a5f333+5/python/lsst/daf/persistence/butler.py"", line 452, in _read      location.getCppType(), storageList, additionalData)    File ""/home/lsstsw/stack/Linux64/daf_persistence/2015_10.0-6-g5a5f333+5/python/lsst/daf/persistence/persistenceLib.py"", line 1430, in unsafeRetrieve      return _persistenceLib.Persistence_unsafeRetrieve(self, *args)  Exception: input stream error  scons: building terminated because of errors.  {code}",NULL
DM-4935,"Create JIRA project for RFDs","Over time, it has become apparent that mixing RFD (request for discussion) requests with RFCs is causing confusion, and the custom RFC template and workflow are not as appropriate for RFDs.    Creating a separate RFD project will disentangle these.  The RFD workflow might look like ""Proposed"", ""Scheduled"", ""Done"".  There should be a clear field for the proposed time slot.",NULL
DM-4941,"Make NaiveDipoleCentroid fill the centroid slot","The centroid slot is currently not being filled when processing difference images. The NaiveDipoleCentroid plugin centroids the positive and negative lobes of the dipole, but does not define an ""overall"" centroid. This change adds a new ""center"" key that will be filled by the midpoint of the positive and negative lobes if both are present, or if only one lobe is present, then it uses that centroid.",NULL
DM-4947,"Buildbot fails in afw on a pthread_creat issue","Upon running buildbot on branch DM-4692, which built successfully on Jenkins, I have twice received failures in afw. One of the cases is:    {code}  :::::  [2016-01-28T23:38:27.571221Z] Failed test output:  :::::  [2016-01-28T23:38:27.577021Z] tests/convolve.py  :::::  [2016-01-28T23:38:27.577036Z]   :::::  [2016-01-28T23:38:27.577062Z] OpenBLAS: pthread_creat error in blas_thread_init function. Error code:11  :::::  [2016-01-28T23:38:27.577254Z] The following tests failed:  :::::  [2016-01-28T23:38:27.579376Z] /nfs/home/lsstsw/build/afw/tests/.tests/convolve.py.failed  :::::  [2016-01-28T23:38:27.579479Z] 1 tests failed  :::::  [2016-01-28T23:38:27.580101Z] scons: *** [checkTestStatus] Error 1  :::::  [2016-01-28T23:38:27.584614Z] scons: building terminated because of errors.  {code}    The other failure occurred in a different test but still with pthread_creat involved. [~rowen] believes that this is related to a similar problem that he saw on a UW machine involving process limits being exceeded. He can describe the issue better than I can.    The full output is available at http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/9320/steps/shell/logs/stdio. ",NULL
DM-4953,"Please eliminate misleading warning when running scons on OSX with SIP","On a Mac with OSX 10.11 and SIP enabled, every time I run {{scons}} on an LSST package I get the message:    {code}  $ scons  scons: Reading SConscript files ...  Could not import lsstcppimport; please ensure the base package has been built (not just setup).    EUPS integration: enabled  [etc]  {code}    It's not fatal, and is not indicative of a fundamental problem. But it is alarming, especially for new users who will assume this means something has broken when they first hit this message. (I did.)",NULL
DM-4954,"Building issues with ci_hsc on OSX with SIP","Because the scons file in ci_hsc mixes lsst code with build code, simply typing scons to build the package on a system with sip will fail. The work around is to not let scons find the python version, but to specify it manually. The workaround is trivial, anyone on os x with sip can simply type:  {code:bash}  `which python` `which scons`  {code}  on the command line to build the package",NULL
DM-4974,"conda git-lfs package","It would be convenient for end users to have the git-lfs client available as a conda package. There does not appear to be an existing recipe. ",NULL
DM-4975,"jira checklist plugin license error","An error from the checklist plugin is displayed when creating a new ticket.",NULL
DM-4976,"update conda build docker container","* allow sudo to be run by the 'conda' user (easiset to allow any user)  * modify as needed to be used as a dynamically launched jenkins build slave  * attempt to trim down the installed set of system packages that duplicate deps fulfilled by conda packages",NULL
DM-4977,"pin conda package versions in set up conda-lsst","At least the conda-build package has a version that will break conda-lsst.  In the interest of making the package build as reproducible as is reasonably possible, I'd like to add a conda package spec or environment file to pin the versions of critical packages.",NULL
DM-4978,"develope strategy to handle afwdata as a conda package","At present, conda-lsst completely masks out the build of {{afwdata}}.  This prevents many of the unit tests from running when building a package.  At a minimum, we should make an afwdata conda package available for end users to install for boot strapping a development env.  Ideally, we could make it a build time dep only for the eups products which need it for unit tests.",NULL
DM-4979,"atomic conda channel publishing","Due to concerns about conda not correctly comparing some lsst product versions, conda channels should only publish a single version of each package (which at this point, should probably all be built together).  We need some simple plumbing to allow ""atomic"" channel updates.  Such as putting new builds into a new directory and changing a symlink",NULL
DM-4980,"add repos.yaml support to conda-lsst","In order to ensure that ""historical"" builds can be reproduced, {{repos.yaml}} support should be added to {{conda-lsst}}.  Perhaps automatically fetching the lastest version from the github raw blob url.",NULL
DM-4981,"add git-lfs client to conda-lsst build env","The usage of LFS is growing across DM's repos.  The git-lfs client should be added to the conda-lsst build env and support for dealing with the helper cache added to conda-lsst.",NULL
DM-4982,"audit conda package usage of eups thirdpart packages","Check to see which, if any, eups package for ""third party"" software can be replaced with existing conda packages.",NULL
DM-4984,"programatic way of determining latest bXXXX number","We need a programmatic means of determining the latest build id for that ""master"" branch that encompasses all top level products we want to product conda packages of.  Ideally, this would be the latest ""master"" build that completed successfully but that may be non-trivial to determine with the current CI sestup.",NULL
DM-4986,"build a ""production"" www server for conda channel(s)","Build a ""production"" conda channel server based on the existing prototype work: https://github.com/lsst-sqre/sandbox-pkg    In particular, a backup mechanism is required.",NULL
DM-4987,"jenkins job to build conda packages","Create a jenkins job to build and publish a complete channel of conda packages.  It should be possible for this job to be scheduled to automatically build packages from the latest master bxxxx build.",NULL
DM-4988,"automate runnning the stack demo on conda packages","Automate running the lsst stack demo on conda packages before they are published to a public conda channel.",NULL
DM-5,"Change Source/Object Terminology","The Data Products review committee has recommended to change what we call 'Source' to something that is less likely to cause confusion (e.g., 'Detection').  This issue captures the need to: * Decide on the new terminology * Perform a massive rename throughout the source code to adopt it.",NULL
DM-5000,"Add robustness to dipole measurement task","Add robustness – add functionality to test for # of dipoles and abort dipole fitting if above config threshold. Also flag saturated stars/ bad pixels prior to fitting.      Coordinate with DM-3515. Not sure if this one belongs in this epic or that one.",NULL
DM-5001,"Design plans for reimplementation of diffim task - separate point source measurement from dipole measurement.","Design plans for reimplementation of diffim task - separate point source measurement from dipole measurement.    Again, not sure if this belongs in this epic or the refactor diffim epic.",NULL
DM-5003,"MariaDB 10.1.10 build failure on OSX 10.11","Attempting to build MariaDB 10.1.10 (current LSST master) fails on my system as follows:    {code}  [2016-02-02T21:53:56.215123Z] [ 86%] Building CXX object storage/spider/CMakeFiles/spider.dir/spd_copy_tables.cc.o  [2016-02-02T21:53:56.418704Z] [ 86%] Building CXX object storage/connect/CMakeFiles/connect.dir/json.cpp.o  [2016-02-02T21:53:56.542279Z] /Users/jds/Projects/Astronomy/LSST/lsstsw/build/mariadb/storage/connect/json.cpp:597:7: error: assigning to 'PSZ' (aka 'char *') from incompatible type 'const char *'  [2016-02-02T21:53:56.542351Z]                 str = (err) ? NULL : ""Ok"";  [2016-02-02T21:53:56.542392Z]                     ^ ~~~~~~~~~~~~~~~~~~~  [2016-02-02T21:53:56.551663Z] 1 error generated.  [2016-02-02T21:53:56.554608Z] make[2]: *** [storage/connect/CMakeFiles/connect.dir/json.cpp.o] Error 1  [2016-02-02T21:53:56.554664Z] make[2]: *** Waiting for unfinished jobs....  {code}    My system is:    {code}  [jds@magpie ~]$ sw_vers && c++ --version  ProductName:	Mac OS X  ProductVersion:	10.11.3  BuildVersion:	15D21  Apple LLVM version 7.0.2 (clang-700.1.81)  Target: x86_64-apple-darwin15.3.0  Thread model: posix  {code}    This is upstream [MDEV-9322|https://mariadb.atlassian.net/browse/MDEV-9322]; it's fixed in 10.1.11 or can be trivially patched.",NULL
DM-5004,"Please add component validate_drp","Please add the following Components to JIRA:    - validate_drp  - validation_data_cfht  - validation_data_decam  ",NULL
DM-5007,"pyfits 3.4 upgrade broke obs_subaru build","The upgrade to pyfits 3.4 appears to have broken the defect list construction, which is part of the obs_subaru build:  {code}  bin.src/genDefectFits.py:56: PyfitsDeprecationWarning: The new_table function is deprecated as of version 3.3 and may be removed in a future version.            Use :meth:`BinTableHDU.from_columns` for new BINARY tables or :meth:`TableHDU.from_columns` for new ASCII tables instead.    table = pyfits.new_table(cols)  Traceback (most recent call last):    File ""bin.src/genDefectFits.py"", line 86, in <module>      genDefectFits(args.cameraName, args.defectsFile, args.targetDir)    File ""bin.src/genDefectFits.py"", line 58, in genDefectFits      table.header.update('NAME', ccd)    File ""/tigress/pprice/lsstsw/stack/Linux64/pyfits/3.4.0/lib/python/pyfits-3.4-py2.7-linux-x86_64.egg/pyfits/header.py"", line 1017, in update      'value, and comment string.' % idx)  ValueError: Header update sequence item #0 is invalid; the item must either be a 2-tuple containing a keyword and value, or a 3-tuple containing a keyword, value, and comment string.  scons: *** [hsc/defects/2013-01-31/defects.dat-fits] Error 1  scons: building terminated because of errors.  {code}    When I hack the environment to use pyfits 3.3, it works fine.",NULL
DM-5009,"mysqlproxy fails to build due to system-supplied lua","Building {{mysqlproxy}} on my system fails as follows:    {code}  [2016-02-03T05:19:38.375065Z] libtool: compile:  cc -DHAVE_CONFIG_H -I. -I.. -I/Users/jds/Projects/Astronomy/LSST/lsstsw/stack/DarwinX86/mariadb/10.1.10/include/mysql -I/Users/jds/Projects/Astronomy/LSST/lsstsw/stack/DarwinX86/mariadb/10.1.10/include/mysql/.. -I/opt/local/include/glib-2.0 -I/opt/local/lib/glib-2.0/include -I/opt/local/include -I/Users/jds/Projects/Astronomy/LSST/lsstsw/stack/DarwinX86/lua/5.1.4.lsst1/include -D_REENTRANT -I/opt/local/include/glib-2.0 -I/opt/local/lib/glib-2.0/include -I/opt/local/include -D_REENTRANT -I/opt/local/include/glib-2.0 -I/opt/local/lib/glib-2.0/include -I/opt/local/include -DPLUGINDIR=\""/Users/jds/Projects/Astronomy/LSST/lsstsw/stack/DarwinX86/mysqlproxy/0.8.5+2/lib/mysql-proxy/plugins\"" -DEXEC_PREFIX=\""/Users/jds/Projects/Astronomy/LSST/lsstsw/stack/DarwinX86/mysqlproxy/0.8.5+2\"" -I/Users/jds/Projects/Astronomy/LSST/lsstsw/stack/DarwinX86/libevent/2.0.16.lsst2/include -g -O2 -MT libmysql_chassis_la-chassis-plugin.lo -MD -MP -MF .deps/libmysql_chassis_la-chassis-plugin.Tpo -c chassis-plugin.c  -fno-common -DPIC -o .libs/libmysql_chassis_la-chassis-plugin.o    ...    [2016-02-03T05:19:38.446373Z] lua-load-factory.c:122:59: error: too few arguments to function call, expected 5, have 4  [2016-02-03T05:19:38.446442Z]         return lua_load(L, loadstring_factory_reader, &factory, s);  [2016-02-03T05:19:38.446505Z]                ~~~~~~~~                                          ^  [2016-02-03T05:19:38.446554Z] /opt/local/include/lua.h:280:1: note: 'lua_load' declared here  [2016-02-03T05:19:38.446605Z] LUA_API int   (lua_load) (lua_State *L, lua_Reader reader, void *dt,  [2016-02-03T05:19:38.446614Z] ^  [2016-02-03T05:19:38.446664Z] /opt/local/include/luaconf.h:242:18: note: expanded from macro 'LUA_API'  [2016-02-03T05:19:38.446690Z] #define LUA_API         extern  [2016-02-03T05:19:38.446712Z]                         ^  [2016-02-03T05:19:38.447470Z] lua-load-factory.c:146:65: error: too few arguments to function call, expected 5, have 4  [2016-02-03T05:19:38.447537Z]         ret = lua_load(L, loadstring_factory_reader, &factory, filename);  [2016-02-03T05:19:38.447593Z]               ~~~~~~~~                                                 ^  [2016-02-03T05:19:38.447932Z] /opt/local/include/lua.h:280:1: note: 'lua_load' declared here  [2016-02-03T05:19:38.447992Z] LUA_API int   (lua_load) (lua_State *L, lua_Reader reader, void *dt,  [2016-02-03T05:19:38.448002Z] ^  [2016-02-03T05:19:38.448051Z] /opt/local/include/luaconf.h:242:18: note: expanded from macro 'LUA_API'  [2016-02-03T05:19:38.448074Z] #define LUA_API         extern  [2016-02-03T05:19:38.448092Z]                         ^  [2016-02-03T05:19:38.448387Z] 2 errors generated.  [2016-02-03T05:19:38.452084Z] make[3]: *** [libmysql_chassis_la-lua-load-factory.lo] Error 1  [2016-02-03T05:19:38.452134Z] make[3]: *** Waiting for unfinished jobs....  {code}    Note that I have a system-supplied lua 5.3.1 in {{/opt/local}}, which is taking precedence over the LSST-supplied lua 5.1.4.lsst1, even though the latter is set up. Clearly, it shouldn't. This newer lua has a different API, and hence the breakage.",NULL
DM-501,"Think about how we handle calibration products","We currently specify a calibration directory and choose calib products by date.  This isn't flexible enough (e.g. I want to try a new flat but the old biases are OK --- I shouldn't need to build a link farm)",NULL
DM-5016,"No way to ask Jenkins CI to build all top level products","When upgrading third party packages or packages very low in the stack, it is sometimes useful to run a Jenkins CI run on all known top level packages. This is much more robust than requiring someone to remember all the relevant packages (and will be made worse if we start to add {{obs_}} packages as discrete top level packages as in DM-3790). It was initially thought that this would happen if no products were specified at all (Buildbot must be doing something like this) but this does not currently work with the Jenkins implementation (Products gets the value {{-skip_docs}}).",NULL
DM-5017,"Make validate_drp examples/runDecamTest.sh take ","On my Mac examples/runDecamTest.sh master took an hour. It would be very helpful if it was much faster.",NULL
DM-5020,"Update ci_hsc and testdata_subaru to use both .gitconfig and .lfsconfig","[SQuaRE suggests|http://developer.lsst.io/en/latest/tools/git_lfs.html#creating-a-new-git-lfs-enabled-repository] that Git LFS repositories should provide both {{.gitconfig}} and {{.lfsconfig}} files with identical contents. Please make it so.",NULL
DM-5021,"Stack in lsst-dev:/lsst/stack is broken","Viz:  {code}  [swinbank@lsst-dev ~]$ source /lsst/stack/loadLSST.sh  [swinbank@lsst-dev ~]$ setup -t b1894 lsst_apps  [swinbank@lsst-dev ~]$ eups list -s lsst_apps  python    11.0+98       b1894 setup  [swinbank@lsst-dev ~]$ python -c'import lsst.utils'  RuntimeError: module compiled against API version a but this version of numpy is 9  Traceback (most recent call last):    File ""<string>"", line 1, in <module>    File ""/home/lsstsw/stack/Linux64/utils/2015_10.0-3-g530cd83+11/python/lsst/utils/__init__.py"", line 23, in <module>      from utilsLib import *    File ""/home/lsstsw/stack/Linux64/utils/2015_10.0-3-g530cd83+11/python/lsst/utils/utilsLib.py"", line 34, in <module>      _utilsLib = swig_import_helper()    File ""/home/lsstsw/stack/Linux64/utils/2015_10.0-3-g530cd83+11/python/lsst/utils/utilsLib.py"", line 30, in swig_import_helper      _mod = imp.load_module('_utilsLib', fp, pathname, description)  ImportError: numpy.core.multiarray failed to import  {code}    ({{b1894}} being the latest weekly at time of writing).",NULL
DM-5031,"Enable external code contributions ","We need to switch existing code to a more permissive license (MIT or Apache) as we   want to accept patches from the community only under more permissive licenses.      We should develop a policy explaining to people who’d like to contribute exactly what   to do and what are boundary conditions.     ",NULL
DM-5032,"Please add an elliptical Gaussian PSF","It would be very helpful to have an elliptical Guassian PSF available. One place I would use it is in {{InsertGaussianPsfTask}}, to insert an elliptical Gaussian PSF model at the beginning of each iteration of detect/measure/fit PSF in CharacterizeImageTask.    Two things to consider:  - Does it make sense to modify {{lsst::meas::algorithms::SingleGaussianPsf}} to be elliptical (e.g. add another constructor), rather than adding a new class?  - Do we want an elliptical double Gaussian PSF? I have no immediate need, but it does seem a logical thing to have. Again, if one is wanted, would it make more sense to provide a new class or expand {{DoubleGaussianPsf}}?",NULL
DM-5044,"Please add getDimensions method to Psf","At the moment it appears that the only way to obtain the dimensions of a PSF is to call psf.computeImage().getDimensions(). I would like a more direct method that doesn't require creating the image, such as psf.getDimensions()",NULL
DM-5045,"LSST Emacs config repo not found","The Confluence page mentions an Emacs configuration repo for use by lsst devs: https://confluence.lsstcorp.org/display/LDMDG/Emacs+Support+for+LSST+Development    The repo link, git://git.lsstcorp.org/LSST/DMS/devenv/build.git, does not resolve.    Does this repo still exist somewhere? Is it relevant? Should be document/suggest Emacs configuration in some other way?",NULL
DM-5046,"system prereq ""preflight"" check script","It would be useful to provide a ""preflight"" sanity check script for end users that verifies (the extent that is reasonably possible) that their system has all of the necessary stack prerequisites.",NULL
DM-5047,"newinstall.sh should allow Python 3 to be used","{{newinstall.sh}} needs to be able to generate a system configured to use Python 3. This may require some special conda environment selection and other subtleties.",NULL
DM-5048,"lsstsw should allow Python 3 to be used","{{lsstsw}} needs to be able to use python 3.    It may be that DM-3129 leads to this by default.",NULL
DM-5051,"Prevent force pushing to master","Prevent force-push to master for lsst repos using protective branches. See RFC-121 for full details",NULL
DM-5060,"Create an overview/flowchart doc of DM communication platforms","I see this as being an overview of what methods DM uses to communicate, and what method should be chosen for any circumstance.    See the page https://confluence.lsstcorp.org/display/DM/Communication+and+Links in the DM Confluence space",NULL
DM-5061,"Provide documentation on EUPS for LSST Developers","Following DM-5013, it was requested to provide introductory documentation on EUPS for LSST developers.",NULL
DM-5062,"Write introduction to documentation writing","This will be a page in http://developer.lsst.io to provide an overview of DM’s documentation needs. The page will start to compile links to resources on technical writing, notes on DM's English style, a ‘this-not-that’ table of usage.",NULL
DM-5063,"Page with links to DM documents and code resources for Developer Guide","Add a page to the Getting Started section of developer.lsst.io that lists important DM documents, websites, and where to find code.",NULL
DM-5064,"Port Orchestration page from DM Confluence to developer.lsst.io","Port https://confluence.lsstcorp.org/display/DM/Orchestration",NULL
DM-5065,"Port ‘Getting started with stack development’ page from DM’s Confluence","Original URL is https://confluence.lsstcorp.org/display/DM/Getting+started+with+stack+development    This page might be appropriate for the Science Pipelines documentation.",NULL
DM-5066,"Port Doxygen Tips and Tricks from DM Confluence","Original URL is https://confluence.lsstcorp.org/pages/viewpage.action?pageId=11108550    This page should be ported into the “Writing Docs” section of developer.lsst.io",NULL
DM-5067,"Port ‘How to document a task’ from DM Confluence","Original URL is https://confluence.lsstcorp.org/display/DM/How+to+document+a+Task    This page may be most appropriate for the LSST Science Pipelines documentation.",NULL
DM-5076,"Meeting with AMD","Met with AMD about future server roadmap",NULL
DM-5081,"Add mask plane to indicate no brighter fatter correction around edges ","The brighter fatter correction convolves the image with a kernel to do the correction.  In the border region of the image where the convolution is not valid there is no correction applied.  We need to add a bit in the mask plane to indicate this.",NULL
DM-5082,"Implement code to generate kernel for brighter fatter correction.","The code to generate the kernel used to correct for the brighter fatter effect was written by Will Coulton, a grad student at Princeton.  It was never implemented on the HSC side although we used the output to apply the correction.  We need this code in LSST in order to generate kernels for cameras other than HSC.  I have attached Will's code that he used for HSC.",NULL
DM-5083,"Please add ""force rebuild"" option to lsstsw rebuild command","One difficulty in developers using lsstsw is that we cannot force a rebuild. One of the great strengths of lsstsw for developers is being able to build the code in a set of repos on a particular tag; in this situation it is often more useful to the developers to have the code in the cloned repos rebuilt than have the code installed in the stack. But that can go terribly awry if one builds things in the wrong order, leaving a host of unbuilt git clones. For instance last night I did the following:  - rebuild a ticket  - rebuild master  at that point my git clones were at master, when I wanted them on the ticket, and there was nothing I could think to do about it except start with a fresh stack (which is a major headache due to all the packages missing from lsstsw). I am still digging out of the mess.    Of course forced rebuild is useful for other situations, as well, such as confirming that we can build the stack. Some packages have intermittent build errors and doing an occasional fresh build is often the only way to catch those.",NULL
DM-5087,"Runtime numpy warnings now appearing","I am seeing a number of {{RuntimeWarnings}} coming out of tests. They have the form:  {code}  RuntimeWarning: invalid value encountered in less  RuntimeWarning: invalid value encountered in greater  RuntimeWarning: invalid value encountered in true_divide  {code}  [~jbosch] speculates that these are coming from {{numpy}} 1.10 when {{NaN}} values are encountered. The warning should probably just be disabled.    Full details:  {code}  ip_diffim/tests/.tests/ImagePsfMatch.py:ip_diffim/python/lsst/ip/diffim/diffimTools.py:73: RuntimeWarning: invalid value encountered in less  ip_diffim/tests/.tests/ImagePsfMatch.py:ip_diffim/python/lsst/ip/diffim/diffimTools.py:73: RuntimeWarning: invalid value encountered in greater  ip_diffim/tests/.tests/SnapPsfMatch.py:ip_diffim/python/lsst/ip/diffim/diffimTools.py:73: RuntimeWarning: invalid value encountered in less  ip_diffim/tests/.tests/SnapPsfMatch.py:ip_diffim/python/lsst/ip/diffim/diffimTools.py:73: RuntimeWarning: invalid value encountered in greater  meas_algorithms/tests/.tests/testPsfDetermination.py:meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py:156: RuntimeWarning: invalid value encountered in less  meas_deblender/tests/.tests/testInclude.py:miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  meas_extensions_psfex/tests/.tests/testPsfexPsf.py:meas_algorithms/2016_01.0+efcb378250/python/lsst/meas/algorithms/objectSizeStarSelector.py:156: RuntimeWarning: invalid value encountered in less  pipe_tasks/tests/.tests/testCoadds.py:miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  pipe_tasks/tests/.tests/testProcessCcd.py:miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  pipe_tasks/tests/.tests/testTransform.py:miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  pipe_tasks/tests/.tests/ticket-2155.pyminiconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in true_divide  sims_catUtils/tests/.tests/testObservationMetaDataGenerator.py:......sims_utils/2.1.0-2-gcea7c0b+68e7d4995d/python/lsst/sims/utils/WcsUtils.py:190: RuntimeWarning: invalid value encountered in arccos  {code}    ",NULL
DM-5093,"Fix proto unit tests compilation","Unit tests for proto are not being built or run after related files have changed. ",NULL
DM-5108,"meas_modelfit testMixture test fails on anaconda 2.5","I recently upgraded my Anaconda to version 2.5 on Mac OS X El Capitan. Rebuilding {{lsst_apps}} triggers a new test failure in {{meas_modelfit}}:  {code}  F.....  ======================================================================  FAIL: testDerivatives (__main__.MixtureTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testMixture.py"", line 172, in testDerivatives      doTest(g, x)    File ""tests/testMixture.py"", line 168, in doTest      self.assertClose(analyticGradient, numericGradient, rtol=1E-6)    File ""/Users/timj/work/lsstsw/stack/DarwinX86/utils/2016_01.0+0b596edbb3/python/lsst/utils/tests.py"", line 368, in assertClose      testCase.assertFalse(failed, msg=""\n"".join(msg))  AssertionError: 1/3 elements differ with rtol=1e-06, atol=2.22044604925e-16  -7.41146749152e-07 != -7.41145697331e-07 (diff=1.05182109542e-12/7.41146749152e-07=1.41918060981e-06)  {code}    Switching to the previous anaconda (2.4 I think) the numbers above are:  {code}  -7.41146749152e-07 != -7.41146239432e-07 (diff=5.0971982368e-13/7.41146749152e-07=6.87744801233e-07)  {code}    The following patch fixes it:  {code:diff}  diff --git a/tests/testMixture.py b/tests/testMixture.py  index a070748..804495e 100755  --- a/tests/testMixture.py  +++ b/tests/testMixture.py  @@ -165,7 +165,7 @@ class MixtureTestCase(lsst.utils.tests.TestCase):               analyticGradient = numpy.zeros(n, dtype=float)               analyticHessian = numpy.zeros((n,n), dtype=float)               mixture.evaluateDerivatives(point, analyticGradient, analyticHessian)  -            self.assertClose(analyticGradient, numericGradient, rtol=1E-6)  +            self.assertClose(analyticGradient, numericGradient, rtol=1.5E-6)               self.assertClose(analyticHessian, numericHessian, rtol=1E-6)              for x in numpy.random.randn(10, g.getDimension()):  {code}  but I have no idea how reasonable that is. It is obviously disconcerting that updating the numerical library can change our tests again.",NULL
DM-5123,"develop strategy for handling environments with Intel's MKL library","Per discussion with [~tjenness] and a bit of symbol table inspection, the root cause of the issue observed in DM-5105 is likely due to the Intel MKL library duplicating symbols which are present in fftw, blas, etc..  Eg.,    https://software.intel.com/en-us/node/522277    After discussion with CA sales, it has been clarified that the new mkl conda packages are under different terms than the previous MKL Optimized ""add-ons"".      http://docs.continuum.io/anaconda/eula    The new license is permissive and allows redistribution.  It is reasonable to assume that MKL will be present on more LSST enduser's  systems in the future and we need a strategy for managing linking to prevent symbol conflicts.  A few possibilities are:    * remove fftw, openblas, etc. from the stack and link solely with MKL (unknown if it is a 100% replacement and this would tie us to the conda package with the permissive eula)  * officially declare MKL as unsupported   * create library stubs (ie., symlinks) so that {{-lfftw}}/etc. will continue to work and re-plumb how EUPS handles optional dependencies so we can switch between openblas/fftw and MKL  * modify our scons probing logic to be able to select between the various library options and re-plumb EUPS optional dependency handling    ",NULL
DM-5126,"newinstall.sh should directly install miniconda","Instead of using the miniconda2}} EUPS product.  {{newinstall.sh}} should directly install the conda env similar to {{lsstsw/bin/deploy}}.",NULL
DM-5134,"Please document ""appropriate teams""","According to the [Developer Guide|http://developer.lsst.io/en/latest/build-ci/new_package.html], when new packages are added to the build ""access must be granted to the appropriate teams"". It turns out this is important not just for access control but also because it relates the to tagging and release process, so it really does matter.    However, at time of writing we have [nine teams|https://github.com/orgs/lsst/teams] defined, and no obvious guide to what they're for or which are ""appropriate"" for what (we might guess that ""Data Management"" was a good team to add packages to, but ... who really knows?).    Please update the documentation so it's clear where new packages should be added.",NULL
DM-5141,"source loadLSST.csh gives error if DYLD_LIBRARY_PATH is not set","New installation (as of 2/12 -- all the way from newinstall.sh, so it grabs the new eups 2.0.1)  Error is:   {code}  [ewok:~/lsst] lynnej% source loadLSST.csh  DYLD_LIBRARY_PATH: Undefined variable.  setup: Command not found.  {code}  This is new for me. (note that I do not have a DYLD_LIBRARY_PATH variable set before doing loadLSST.csh)  If I do the following:  {code}  [ewok:~/lsst] lynnej% setenv DYLD_LIBRARY_PATH   [ewok:~/lsst] lynnej% source loadLSST.csh  {code}  ",NULL
DM-5146,"generate daf_persistence Policy files need to be formatted for readability","right now the Policies get written as yaml all just dumped out in a line. The dumper need to format files for readability.",NULL
DM-5149,"w_2016_07 ""newinstall.sh"" nebula images & docker containers","Plumbing and/or documentation additions necessary for another SQRE member to use {{packer-newinstall}} to test the {{w_2016_07}} weekly tag for this week and to build the binary products. ",NULL
DM-515,"Please add comment pop-up in workflow change to Done","Like DM-440, but we should also have a comment box when we move to Done, as that's when we often record how we've responded to trivial review comments.",NULL
DM-5157,"lsstsw bin/deploy fails trying to install ""cairo""","lsstsw bin/deploy fails on my OS X El Capitan system as follows, using master, commit d623a4e:  {code}  localhost$ bin/deploy  ::: Deploying Miniconda 3.19.0 for MacOSX-x86_64  ######################################################################## 100.0%  PREFIX=/Users/rowen/UW/LSST/testlsstsw/miniconda  installing: _cache-0.0-py27_x0 ...  installing: python-2.7.11-0 ...  installing: conda-env-2.4.5-py27_0 ...  installing: openssl-1.0.2d-0 ...  installing: pycosat-0.6.1-py27_0 ...  installing: pyyaml-3.11-py27_1 ...  installing: readline-6.2-2 ...  installing: requests-2.9.0-py27_0 ...  installing: sqlite-3.8.4.1-1 ...  installing: tk-8.5.18-0 ...  installing: yaml-0.1.6-0 ...  installing: zlib-1.2.8-0 ...  installing: conda-3.19.0-py27_0 ...  installing: pycrypto-2.6.1-py27_0 ...  installing: pip-7.1.2-py27_0 ...  installing: wheel-0.26.0-py27_1 ...  installing: setuptools-18.8.1-py27_0 ...  Python 2.7.11 :: Continuum Analytics, Inc.  creating default environment...  installation finished.  Fetching package metadata: ....  Error: No packages found in current osx-64 channels matching: cairo 1.12.18 6     You can search for this package on anaconda.org with         anaconda search -t conda cairo 1.12.18 6     You may need to install the anaconda-client command line client with         conda install anaconda-client  localhost$   {code}",NULL
DM-5158,"dtype argument ignored by GroupView methods","{{GroupView.aggregate}} and {{GroupView.apply}} both accept an argument {{dtype}} which is ignored. The fix is trivial.",NULL
DM-5159,"Please use angle and Coord where possible","validate_drp would be easier to follow and safer if it took advantage of lsst.afw.geom.Angle and lsst.afw.coord.IcrsCoord. For instance {{averageRaDecFromCat}} could return an IcrsCoord and positionRms could use coord1.angularSeparation(coord2) and handle wraparound and other effects simply and safely.",NULL
DM-5168,"newinstall python version checking seems wrong (to me)","In https://github.com/lsst/lsst/pull/19 the version check for Python was changed such that it switched from using the python from the path to the python set in the {{$PYTHON}} variable. I don't think this is the right thing to do because when the stack is built or run the python that is used will be the python in the PATH and not a {{$PYTHON}} python. Am I wrong? {{sconsUtils}} is definitely using {{python}} to run the tests and is not trying to read an environment variable. This means that any version checking we do must be checking the default path python.",NULL
DM-5170,"mariadbclient libssl linking problems on Linux","I have a build failure in daf_persistence due to mariadbclient not linking to libssl/libcrypto on a linux machine. Can we make the ssloptions fix for OSX apply generically, or would that break something else?    Here's the daf_persistence error that I received:  {code}  daf_persistence: 2016_01.0-1-gf47bb69 ERROR (33 sec).  *** error building product daf_persistence.  *** exit code = 2  *** log is in /astro/apps6/opt/lsstStacks/lsstsw/build/daf_persistence/_build.log  *** last few lines:  :::::  [2016-02-17T23:03:47.312613Z] /astro/apps6/opt/lsstStacks/lsstsw/stack/Linux64/mariadbclient/10.1.11/lib/libmysqlclient_r.so: undefined reference to `SSL_CTX_ctrl'  :::::  [2016-02-17T23:03:47.312663Z] /astro/apps6/opt/lsstStacks/lsstsw/stack/Linux64/mariadbclient/10.1.11/lib/libmysqlclient_r.so: undefined reference to `ERR_get_error'  :::::  [2016-02-17T23:03:47.312765Z] /astro/apps6/opt/lsstStacks/lsstsw/stack/Linux64/mariadbclient/10.1.11/lib/libmysqlclient_r.so: undefined reference to `SSL_new'  :::::  [2016-02-17T23:03:47.312815Z] collect2: error: ld returned 1 exit status  :::::  [2016-02-17T23:03:47.312834Z] scons: *** [tests/DbStorage_1] Error 1  :::::  [2016-02-17T23:03:47.735155Z] /astro/apps6/opt/lsstStacks/lsstsw/miniconda/lib/python2.7/config/libpython2.7.a(posixmodule.o): In function `posix_tmpnam':  :::::  [2016-02-17T23:03:47.735289Z] -------src-dir-------/Python-2.7.11/./Modules/posixmodule.c:7631: warning: the use of `tmpnam_r' is dangerous, better use `mkstemp'  :::::  [2016-02-17T23:03:47.735345Z] /astro/apps6/opt/lsstStacks/lsstsw/miniconda/lib/python2.7/config/libpython2.7.a(posixmodule.o): In function `posix_tempnam':  :::::  [2016-02-17T23:03:47.735400Z] -------src-dir-------/Python-2.7.11/./Modules/posixmodule.c:7578: warning: the use of `tempnam' is dangerous, better use `mkstemp'  :::::  [2016-02-17T23:03:47.764830Z] scons: building terminated because of errors.  {code}    Here's what my mariadb lib linkage looks like, with the missing libs:    {quote}[16:32:53 parejkoj@magneto: /astro/apps6/opt/lsstStacks/lsstsw/stack/Linux64/mariadbclient/10.1.11/lib]  $ ldd libmysqlclient.so.18.0.0  	linux-vdso.so.1 =>  (0x00007ffdb63b4000)  	libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f088ab10000)  	libz.so.1 => /lib64/libz.so.1 (0x00007f088a8f9000)  	libssl.so.1.0.0 => not found  	libcrypto.so.1.0.0 => not found  	libdl.so.2 => /lib64/libdl.so.2 (0x00007f088a6f4000)  	librt.so.1 => /lib64/librt.so.1 (0x00007f088a4ec000)  	libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00007f088a1e6000)  	libm.so.6 => /lib64/libm.so.6 (0x00007f0889f61000)  	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007f0889d4b000)  	libc.so.6 => /lib64/libc.so.6 (0x00007f08899b7000)  	/lib64/ld-linux-x86-64.so.2 (0x00007f088b2af000){quote}",NULL
DM-5172,"W16/X16/F16 SUIT communication with Kevin","To record the changes made to W16 and S16 epics and stories.    2/17/2016  Moved the following epics to F16:        DM-3594,  DM-3604,     2/18/2016  Moved the following epics to X16:       DM-3625 DM-3647    2/18/2016  Moved the following stories to F16:      DM-2783,     2/17/2016  Moved the following stories to F16 (nov. 2016)       DM-3643",NULL
DM-5176,"Running runQuery.py for a long time causes mysql-proxy to crash","mysql-proxy crashes for unknow reason:    {code:bash}  qserv@ccqserv125:~$ ps x     PID TTY      STAT   TIME COMMAND       1 ?        Ss     0:00 /bin/sh -c /qserv/scripts/start.sh      16 ?        S      0:00 /bin/sh /qserv/scripts/start.sh     266 ?        S      8:11 tail -F /qserv/run/var/log/mysqld.log /qserv/run/var/log/xrootd-console.log /qserv/run/var/log/worker/xrootd.log /qserv/run/var/lo    1117 ?        Ss     0:00 bash   14848 ?        S      0:00 /bin/sh /qserv/stack/Linux64/mariadb/10.1.11/bin/mysqld_safe --defaults-file=/qserv/run/etc/my.cnf --datadir=/qserv/data/mysql --p   14965 ?        Sl     5:52 /qserv/stack/Linux64/mariadb/10.1.11/bin/mysqld --defaults-file=/qserv/run/etc/my.cnf --basedir=/qserv/stack/Linux64/mariadb/10.1.   15018 ?        Sl     0:07 /qserv/stack/Linux64/xrootd/2015_11.0/bin/xrootd -c /qserv/run/etc/lsp.cf -l /qserv/run/var/log/xrootd.log -n worker -I v4   15054 ?        Sl     0:08 /qserv/stack/Linux64/xrootd/2015_11.0/bin/cmsd -c /qserv/run/etc/lsp.cf -l /qserv/run/var/log/cmsd.log -n worker -I v4   15152 ?        S      0:33 python /qserv/stack/Linux64/qserv/2016_01-10-gebc3522/bin/watcher.py -c /qserv/run/etc/qserv-watcher.cnf -v   15182 ?        S      0:05 python /qserv/stack/Linux64/qserv/2016_01-10-gebc3522/bin/qservWmgr.py -c /qserv/run/etc/qserv-wmgr.cnf -v   22132 ?        R+     0:00 ps x  {code}    Here's the proxy log:  {code:bash}  2016-02-18 00:28:28: (critical) plugin proxy 0.8.5 started  2016-02-18 00:28:28: (debug) max open file-descriptors = 1048576  2016-02-18 00:28:28: (message) proxy listening on port :4040  2016-02-18 00:28:28: (message) added read/write backend: 127.0.0.1:13306  2016-02-18 00:29:03: (debug) [network-mysqld.c:1134]: error on server connection (fd: 130 event: 2). closing client connection.  2016-02-18 00:32:17: (debug) proxy-plugin.c:229: connecting to 127.0.0.1:13306 timed out after 2.00 seconds. Trying another backend.  2016-02-18 00:32:17: (critical) proxy-plugin.c.1865: Cannot connect, all backends are down.  2016-02-18 00:32:17: (debug) last message repeated 2 times  2016-02-18 00:32:17: (debug) [network-mysqld.c:1134]: error on a connection (fd: -1 event: 0). closing client connection.  2016-02-18 05:48:05: (debug) last message repeated 23 times  2016-02-18 05:48:05: (debug) [network-mysqld.c:1134]: error on a connection (fd: -1 event: 0). closing client connection.  2016-02-18 07:29:58: (debug) last message repeated 1 times  2016-02-18 07:29:58: (debug) [network-mysqld.c:1134]: error on a connection (fd: -1 event: 0). closing client connection.  2016-02-18 08:28:44: (debug) last message repeated 1 times  2016-02-18 08:28:44: (debug) [network-mysqld.c:1134]: error on a connection (fd: -1 event: 0). closing client connection.  2016-02-18 08:31:27: (debug) last message repeated 1 times  2016-02-18 08:31:27: (debug) [network-mysqld.c:1134]: error on server connection (fd: 396 event: 1). closing client connection.  2016-02-18 08:31:27: (debug) [network-mysqld.c:1134]: error on server connection (fd: 380 event: 1). closing client connection.  2016-02-18 08:31:27: (debug) [network-mysqld.c:1134]: error on server connection (fd: 374 event: 1). closing client connection.  {code}",NULL
DM-5177,"error in command setup command line in","There's a mistake on the page:    http://developer.lsst.io/en/latest/services/lsst-dev.html#load-the-lsst-environment    The line:   ""source ~lsstsw/eups/bin/setups.sh   # bash users""    should be    ""source ~lsstsw/eups/current/bin/setups.sh   # bash users""",NULL
DM-5180,"fix vagrant-nebula & sqr-002 vagrant doc issues","The vagrant provider needs to be specified and vagrant 1.8.x has been released since the documentation was written.  This breaks the openstack provider.  The fix is trivial but we need to publish a patched version for end-users.",NULL
DM-5183,"patch eigen to fix UF_long in prep for meas_simastrom merge","Our current version of eigen (3.2.5) is incompatible with recent versions of SuiteSparse due to a change from UF_long to suitesparse_long. This was fixed in eigen 3.3-beta1, but is a 3-line patch.    I'm going to add the patch to our eigen product, so we won't have to run the 3.3 beta (which may break other things).",NULL
DM-5184,"Issue with patches section of third-party packages developer doc","[~Parejkoj] reports:    {quote}The ""patches"" section of the third party packages page is problematic:  http://developer.lsst.io/en/latest/build-ci/third_party.html#the-patches-directory  Since the files themselves aren't checked in to git, but rather the tarballs, the git command listed there won't work.  {quote}",NULL
DM-5195,".gitignore doc/xml directories in all Stack packages","Now that XML is being generated by doxygen (DM-4900) we need to prevent developers from accidentally committing the generated XML. This ticket will systematically go through all stack packages and add `doc/xml/` to .gitignore files.",NULL
DM-5199,"Fail early if the compiler does not support -std=c++11","In DM-1361 {{sconsUtils}} was modified to insist on support C\+\+ 11 features. The tests turned out to be too permissive as some GCC compilers that support {{-std=c\+\+0x}} understand some C\+\+ 11 features but not all of them. The proposal is simply to remove the {{-std=c\+\+0x}} option and fail if the compiler does not support {{-std=c\+\+11}}.",NULL
DM-5207,"sncosmo fails installation with {{error: must supply either home or prefix/exec-prefix -- not both}}","Building {{sncosmo}} on a   Mac, Yosemite, Homebrew-installed Python, {{--user}} installation target for {{pip}}  fails with the below.    This failure naively sounds related to DM-3754, but the fix for {{eups}} in that ticket was adopted, so perhaps the ending message {{error: must supply either home or prefix/exec-prefix -- not both}} comes from something else.    {code}              sncosmo: 1.2.0+1 ERROR (12 sec).  *** error building product sncosmo.  *** exit code = 1  *** log is in /Users/wmwv/lsstsw/build/sncosmo/_build.log  *** last few lines:  :::::  [2016-02-21T00:25:55.878985Z] + '[' -d tests/.tests ']'  :::::  [2016-02-21T00:25:55.879057Z] + eupspkg PRODUCT=sncosmo VERSION=1.2.0+1 FLAVOR=generic install  :::::  [2016-02-21T00:25:56.305441Z] The requested path 'astropy_helpers' for importing astropy_helpers does not exist, or does not contain a copy of the astropy_helpers package.  Attempting download instead.  :::::  [2016-02-21T00:25:56.305535Z] Downloading astropy_helpers; run setup.py with the --offline option to force offline installation.  :::::  [2016-02-21T00:25:56.551976Z] /usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.  :::::  [2016-02-21T00:25:56.552045Z]   warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')  :::::  [2016-02-21T00:25:58.039846Z] /Users/wmwv/lsstsw/build/sncosmo/.eggs/astropy_helpers-1.1.1-py2.7.egg/astropy_helpers/setup_helpers.py:102: AstropyDeprecationWarning: Direct use of the adjust_compiler function in setup.py is deprecated and can be removed from your setup.py.  This functionality is now incorporated directly into the build_ext command.  :::::  [2016-02-21T00:25:58.039907Z]   'command.', AstropyDeprecationWarning)  :::::  [2016-02-21T00:25:58.063052Z] running install  :::::  [2016-02-21T00:25:58.063429Z] error: must supply either home or prefix/exec-prefix -- not both  {code}",NULL
DM-5210,"newinstall.sh downloads to /tmp/conda_packages.txt , causing user conflicts","We are seeing that ""newinstall.sh""  performs a curl of  conda_packages.txt within the MiniConda installation   to  /tmp/conda_packages.txt    .     This file is always left there (I believe), or at the very least can be stranded there with a failed or truncated installation.    This can 'lock out' of others, causing build failures say on a machine like lsstdev if a file like     [daues@lsst-dev ~]$ ls -l /tmp/conda_packages.txt   -rw-rw-r-- 1 moeyensj moeyensj 928 Feb 23 10:03 /tmp/conda_packages.txt    gets left behind. The error looks like :    {code}    installing: yaml-0.1.6-0 ...  installing: zlib-1.2.8-0 ...  installing: conda-3.19.0-py27_0 ...  installing: pycrypto-2.6.1-py27_0 ...  installing: pip-7.1.2-py27_0 ...  installing: wheel-0.26.0-py27_1 ...  installing: setuptools-18.8.1-py27_0 ...  Python 2.7.11 :: Continuum Analytics, Inc.  creating default environment...  installation finished.  WARNING:      You currently have a PYTHONPATH environment variable set. This may cause      unexpected behavior when running the Python interpreter in Miniconda2.      For best results, please verify that your PYTHONPATH only points to      directories of packages that are compatible with the Python interpreter      in Miniconda2: /raid/daues/test2/work2/Linux64/miniconda2/3.19.0.lsst2  Warning: Failed to create the file conda_packages.txt  ######################################################################## 100.0%  curl: (23) Failed writing body (0 != 928)  + exit -5  eups distrib: Failed to build miniconda2-3.19.0.lsst2.eupspkg: Command:  	source /raid/daues/test2/work2/eups/bin/setups.sh; export EUPS_PATH=/raid/daues/test2/work2; (/raid/daues/test2/work2/EupsBuildDir/Linux64/miniconda2-3.19.0.lsst2/build.sh) >> /raid/daues/test2/work2/EupsBuildDir/Linux64/miniconda2-3.19.0.lsst2/build.log 2>&1 4>/raid/daues/test2/work2/EupsBuildDir/Linux64/miniconda2-3.19.0.lsst2/build.msg   exited with code 251  {code}       it would seem more prudent for the file to be downloaded to   /tmp/conda_packages_$\{USER\}.txt  or /tmp/conda_packages_$$.txt  or something similar.     ",NULL
DM-5211,"""SELECT y_flagNegative from Object where y_flagNegative != (1)"" crash Qserv master","Next query crash Qserv master:    {code:bash}  -bash-4.2$ time mysql --host ccqserv125 --port 4040 --user qsmaster LSST -e ""SELECT y_flagNegative from Object where y_flagNegative != (1)"";  ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query  {code}    One may suppose it's because too large results.",NULL
DM-5212,"Add configurable limitations in qana for scans","There are some queries that should not be allowed and queries to various shared scans should be limited to certain tables or a maximum number of joins. This needs to be done via configuration and/or css. This should probably be done in qana.",NULL
DM-5214,"Test shared scans with Pan-starrs data","Setup css and configuration of qserv for shared scans to work with pan-starrs data and determine what aspects of shared scans need to be improved.",NULL
DM-5215,"Provision ls.st short links for Technotes","There is a request to provision ls.st short links for Technotes. E.g. ls.lst/sqr-000 links to sqr-000.lsst.io.",NULL
DM-5216,"Clarify use of whitespace in reStructuredText Style Guide","There was a user request to clarify the role of whitespace in delimiting directives from paragraphs. This ticket will devise and implement the changes to http://developer.lsst.io/en/latest/docs/rst_styleguide.html    ",NULL
DM-5217,"Document process for reStructuredText-based Design Documents","The process of developing Design Documents in a branch and then merging to master upon approval should be documented in developer.lsst.io",NULL
DM-5220,"xrootd cmake probe for python finds the wrong executable","I am attempting to build {{xrootd}} in a minimal EL5 environment with miniconda.  The system python 2.4 executable is present but without the {{devel}} package (no headers).    The python probes in {{cmake/XRootDFindLibs.cmake}} are finding the miniconda libs but the system python executable.    {code}  -- Found PythonLibs: /home/conda/conda-lsst/miniconda/envs/_build/lib/libpython2  .7.so (found suitable version ""2.7.11"", minimum required is ""2.4"")   -- Found PythonInterp: /usr/bin/python2.4 (found suitable version ""2.4.3"", minim  um required is ""2.4"")   {code}    As a result, the include paths are not setup correctly and the build fails looking for {{Python.h}}    {code}  cc -fno-strict-aliasing -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fPIC -I/home/conda/conda-lsst/miniconda/conda-bld/work/src -I/home/conda/conda-lsst/miniconda/conda-bld/xrootd-build/src -I/usr/include/python2.4 -c /home/conda/conda-lsst/miniconda/conda-bld/work/bindings/python/src/PyXRootDCopyProcess.cc -o build/temp.linux-x86_64-2.4/home/conda/conda-lsst/miniconda/conda-bld/work/bindings/python/src/PyXRootDCopyProcess.o -g  [ 73%] Linking CXX executable xrdmapc  In file included from /home/conda/conda-lsst/miniconda/conda-bld/work/bindings/python/src/PyXRootDCopyProcess.hh:28:0,                   from /home/conda/conda-lsst/miniconda/conda-bld/work/bindings/python/src/PyXRootDCopyProcess.cc:26:  /home/conda/conda-lsst/miniconda/conda-bld/work/bindings/python/src/PyXRootD.hh:32:20: fatal error: Python.h: No such file or directory   #include <Python.h>                      ^  compilation terminated.  error: command 'cc' failed with exit status 1  {code}    It is not clear to me at this point if this an issue with environment variable setup or {{cmake}}'s {{PythonInterp}} code but I am suspecting the later. Google reveals a large number of results for similar problems.  This bug appears to be the most active (among many) in the {{cmake}} bug tracker.    https://cmake.org/Bug/view.php?id=14809",NULL
DM-5221,"what should xrootd be linking against for LSST usage?","xrootd's cmake configuration is probing for several optional libraries.  Should we be linking against any of these for normal LSST usage?  Note that we already have an eups package for {{libevent}}.    {code}  -- Could NOT find LibXml2 (missing:  LIBXML2_LIBRARIES LIBXML2_INCLUDE_DIR)   ...  -- Could NOT find fuse (missing:  FUSE_INCLUDE_DIR FUSE_LIBRARY)   -- Could NOT find LibEvent (missing:  LIBEVENT_LIB LIBEVENT_INCLUDE_DIR)   -- Could NOT find ceph (missing:  RADOS_INCLUDE_DIR RADOS_LIBS)   {code}",NULL
DM-5223,"evaluate jenkins jira plugin",https://wiki.jenkins-ci.org/display/JENKINS/JIRA+Plugin,NULL
DM-5224,"bare OSX bootstrap instructions","It would be extremely useful to have a developer OSX bootstrapping guide starting from a bare OSX image.  Covering cli based Xcode installation, home brew packages, etc. that can be essentially cut'n'pasted.",NULL
DM-5225,"implement a slack command to open a jira ticket.","It would be very nice to be able to issue jira tickets from a slash command in slack.  One use case that should be covered is that Jacek would like to be able to at-mention a list of people to give the CAMs a todo (e.g. to do monthly reports).    I guess is should take at-mentions at assignees (default to self), a string for a summary and maybe an argument for story points (?).",NULL
DM-5226,"compile list of who works for DM","We need a list of who works for DM, % of time, and role for each person.",NULL
DM-5235,"X16 resource loaded plan ready for Kevin","Each tcam should have a resource loaded plan, ready to load to PMCS by 3/10",NULL
DM-5245,"Extract resource loading from DRP/DLP meta-epics and store elsewhere","The DRP meta-epics currently have associated time estimates that are used for resource loading. Please extract them and ensure they are recorded elsewhere.",NULL
DM-5253,"WCS match-fit iteration should use simple matching after first round","The current astrometric fitting iterates over a match-then-fit cycle using MatchOptimisticB each time. This matcher finds sets of stars (currently defaults to 6 stars per set), which is designed for finding a image's position and orientation in a large reference catalog, but this process is less suitable for when we already have a close alignment between the reference catalog and the source catalog on subsequent iterations of the match-fit cycle. This cycle also reduces the maximum match radius for subsequent iterations, which results in entire sets being rejected unnecessarily (which contributes to the instability of the fitting).    MatchOptimisticB should instead be run once to find the overall location and rotation of the image in the reference catalog, then subsequent iterations should use a simplistic matcher as the fitter refines the WCS solution. If MatchOptimisticB fails on the first iteration, the image is likely unrecoverable even with subsequent iterations.  ",NULL
DM-5254,"Set Team field for stories without a team","Per RFC-145, let's go through the stories in DM and assign Team to each of them. To see these stories, you can use https://jira.lsstcorp.org/issues/?filter=14106    I am expecting each tcam to go through the list and deal with stories that fall into your wbs.     (you might want to sort by reported or assignee to quickly identify most of them)",NULL
DM-526,"Upgrade EUPS to 1.3.1 (when it's released)","This is a tracking issue to make sure we upgrade EUPS and that next EUPS release incorporates bugfixes of importance to us:  * https://github.com/RobertLuptonTheGood/eups/issues/14 ",NULL
DM-5263,"static sqlite3 registry does not sufficiently allow for the possibility of new dataset type and dataIds that may be added to repository via butler.put","need a way for lookup to work on items that are put in the repo after the registry has been created. ",NULL
DM-5266,"Extra line inadvertently left in unit test","While reworking the test cases associated with DM-5206 in response to review, I accidentally failed to remove an obsolete line of code. I'll do so on this ticket instead.",NULL
DM-5268,"Allow pluggable db access in daf_persistence","It was noticed by [~rhl] on HipChat that {{daf_persistence}} depends on {{mariadbclient}} for persisting database tables (he was having trouble building {{mariadbclient}} on OS X because of the {{cmake}} dependency). There was a question as to whether database persistence should be implemented as a plug in system or use a more generic and portable DB abstraction layer. This is partly motivated by AFW depending on {{daf_persistence}} thereby causing any dependencies here to have a broad impact. ",NULL
DM-5269,"Add unicode testing description to DM Dev Guide","At the Joint Technical Meeting at Santa Cruz the following decision was made regarding Unicode in tests:  {quote}  Focus on adding Unicode test cases for strings that would come from outside.  {quote}  This ticket relates to updating the relevant documentation to indicate this.",NULL
DM-5272,"log package no longer builds on OS X","The thread-local logging changes from DM-4756 break the {{log}} package on OS X:  {code}  scons: Building targets ...  g++ -o src/Log.os -c -std=c++11 -g -DLSST_LITTLE_ENDIAN=1 -O0 -Wall -Wno-unused-function -fPIC -Iinclude -I/Users/timj/work/lsstsw/stack/DarwinX86/boost/1.59.lsst5+fbf04ba888/include -I/Users/timj/work/lsstsw/stack/DarwinX86/log4cxx/0.10.0.lsst6+1cc8a4a234/include -I/Users/timj/work/lsstsw/miniconda/include/python2.7 src/Log.cc  src/Log.cc:382:5: error: thread-local storage is not supported for the current target      thread_local static bool threadInit = false;      ^  1 error generated.  {code}  ",NULL
DM-529,"Investigate utility of scisql + HTM indexing for region searches in Qserv","It might be advantageous to use scisql support for fast point-in-region searches when executing point-in-polygon (and similar) searches via Qserv.   Since Qserv already limits the execution of such queries to just those chunks overlapping the search region, it may not be worth doing this (unless chunks are really large).  This very low priority issue corresponds to ticket 2056 from Trac, and serves as a reminder that we should do some performance testing to figure out whether to pursue this potential optimization or not. In particular, testing performance on a 1 million row chunk that is 60-80% covered has been suggested.",NULL
DM-5293,"Correctly test both Wcs and TanWcs for EQUINOX output","In DM-5206 I modified the way WCS headers are written when persisting FITS files. I attempted to write a test case that checked this was being done correctly for both {{Wcs}} and {{TanWcs}}, but I mistakenly managed to check {{TanWcs}} twice. Please correct the test.",NULL
DM-5295,"Refactor ProcessDiffim","This ticket continues DM-3704 and refactors processDiffim.",NULL
DM-5299,"data model for persisting long metadata to FITS","AFW currently uses the HIERARCH FITS header keyword to encapsulate >8 character header keys, but that is not the intended usage of that keyword, and its behavior changes in cfitsio >3.37. We need to decide how we map our longer metadata into the limited namespace in FITS if we want to be able to roundtrip information to FITS, and we have to be careful not to use behavior outside the FITS standard and common extensions.    There was some discussion about this here: https://community.lsst.org/t/safer-handling-of-long-fits-keywords/456/5    In particular, the tests that failed when attempting to update cfitsio were from afw: mask.testWriteFits() and image.testDM882(). There may be other places that don't behave correctly beyond those.",NULL
DM-5303,"Document policy question on use of eduPersonAffiliation for granting data rights","As a follow-on to discussions at the Feb 2016 JTM, document the policy question(s):    Should LSST grant data rights based on eduPersonAffiliation values set by university identity providers (via InCommon/eduGAIN)?    Explain the eduPersonAffiliation values and controls around those values.    Deliver this document to Beth & Don and other stakeholders.",NULL
DM-5304,"Use psfex as the default PSF determiner","{{MeasurePsfTask}} uses ""pca"" as its default PSF determiner. At a meeting about DM-4692 changes to {{ProcessCcdTask}} it was suggested that the newer ""psfex"" PSF determiner would be a better choice.",NULL
DM-5305,"Add a star selector based on PSF","[~jbosch] suggests that we have a star selector based on the PSF model. He asked me to file this ticket and assign it to him. Jim: feel free to edit this description to explain the need in more detail.    This would be a good candidate for an efficient way to identify sources suitable for aperture correction DM-5309",NULL
DM-5306,"PcaPsfDeterminer is overly aggressive in rejecting stars","In debugging DM-4692, we found that {{PcaPsfDeterminer}} was very aggressive in rejecting outliers from individual principal components, which lead to unconstrained solutions in large regions of particular images (validation_data_cfht visit 849375 ccd 22 is a good example). This also caused significant changes in the PSF model when small numbers of psf candidate stars were added or dropped from the input catalog. We believe this behavior lead to ~10 milimag variations in PSF photometry. We were able to reduce this sensitivity by setting an ad-hoc work around configuration of {{spatialReject=6.0}}, but we did not study this in depth.",NULL
DM-5309,"Please provide an efficient way to identify sources suitable for aperture measurement","At present the way we identify sources suitable for aperture measurement is to use sources that the PSF determiner used to determine the PSF. This clumsily entangles measurement with PSF measurement, instead of making measurement self-contained. This leads to awkwardnesses such as the inability to measure aperture correction if we don't fit a PSF model.    The architecture would be much cleaner if we had an efficient way to identify sources that are suitable for measuring aperture correction, and if our measurement task used that.",NULL
DM-5310,"Re-add fake source handling to ProcessCcdTask","DM-4692 does not implement fake source handling in ProcessCcdTask, and that feature should be added back in.",NULL
DM-5325,"LSST IAM Design Doc v0.2","Revise our LSST IAM Design Doc based on our work over the Oct 2015 - Mar 2016 period.",NULL
DM-5326,"propagateVisitFlags.py doesn't work with DECam and lsstSim data","The ccd data identifier ""ccd"" is hard-coded here:   https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/propagateVisitFlags.py#L182  {code:java}  ccdSources = butler.get(""icSrc"", visit=int(v), ccd=int(c), immediate=True)  {code}    But obs_decam uses ""ccdnum"" instead of ""ccd"", and obs_lsstSim uses ""sensor"". The use of ""ccd"" can be camera specific.  ",NULL
DM-5332,"Initial evaluation of auth options with nginx","DAX deployments may be based on nginx. We need to do some basic research about available nginx authentication options, since to-date we've been focused on Apache HTTPD.",NULL
DM-5335,"measMulti.py has undefined variables","{{meas_modelfit}}'s {{measMulti.py}} uses several undefined variables (as found by the pyflakes linter). The other {{.py}} files should also be checked (though {{measureCoadd.py}} passes).",NULL
DM-534,"meas_algorithms fails to build because afw/math/detail/SrcPosFunctor is gone","afw ticket #2214 included renaming math/detail/SrcPosFunctor to something clearer. Unfortunately it turns out that meas_algorithms was using it (even though it was buried in a detail namespace, and thus not part of the public API) and no longer builds. A proper fix would include hoisting the renamed class out of detail (if the renamed afw class should be public) or recoding meas_algorithms to not use it (if it should be private).",NULL
DM-5354,"Think about how to deal with comments to Technotes","[~afausti] requests a system for commenting on technotes (like you can comment on Confluence pages).    My leading idea is to build a system where topics on Community.lsst.org that mention a technote get linked to from the technote’s sidebar (this would happen dynamically).    (During the technote writing/formulation stage, GitHub Pull Request-based comments should be used.)",NULL
DM-5366,"Please make bin/deploy install anaconda, not miniconda","Most developers who install {{lsstsw}} will want {{anaconda}} instead of {{miniconda}}. I suggest either switching entirely or offering an easy way for a user to specify one or the other -- preferably something that lives outside of {{lsstsw}}, e.g. an environment variable or a config file in {{~/.config}}",NULL
DM-5369,"ip_diffim has unlisted dependencies","{{ip_diffim}} has a dependency on Minuit2 that is not listed. Missing dependencies:    * {{daf_base}}  * {{daf_persistence}}  * {{meas_algorithms}}  * {{Minuit2}}  * {{pex_exceptions}}  * {{pex_logging}}  * {{pex_policy}}  ",NULL
DM-5373,"Check with science collaborations about L1 scans","Per RFC-133, we should check with science collaborations whether we are not missing any killer use case by not enabling shared scans on L1 catalogs.",NULL
DM-5378,"Tool that enforces valid Team field in JIRA","Per RFC-145 we need to catch stories without valid Team field.",NULL
DM-5382,"Update template repository with new copyright rules","The template git repository needs to be updated to use the copyright approach agreed to in RFC-45.",NULL
DM-5383,"Update developer docs with Copyright instructions","The adoption of RFC-45 requires the documentation is updated to reflect this new approach to copyright tracking.",NULL
DM-5386,"Please document ""TAP packages"" and associated policy","[Discussion on clo|https://community.lsst.org/t/how-do-we-update-eups/478/14] indicates that not only does EUPS (distrib) use a {{TAP_PACKAGE}} environment variable (and maybe a {{.tap_package}} file), but also there is [policy|https://community.lsst.org/t/how-do-we-update-eups/478/14] about how they should be set.    However, there doesn't seem to be any documentation covering this, and certainly the policy is not recorded on our [instructions for distributing third party packages|http://developer.lsst.io/en/latest/build-ci/third_party.html]. Please ensure that it is.",NULL
DM-5391,"Please provide email notifications from Jenkins","I would like email notification of the completion of Jenkins jobs that I submit (but no others). It would especially helpful if the email included a link to the console log and a summary of success or failure.    As it stands, it is painful to figure out when a job succeeds or fails. I have to either watch endless irrelevant messages go by on the Jekins HipChat room or, when the job stars, immediately launch a browse window to watch the console log. Neither is very satisfactory, especially if I need to go somewhere else while the job runs.    The current system would be acceptable if the jobs ran quickly, but most of mine take an hour.    A fine alternative (in case it is easier) is to have a copy of the HipChat notification of jobs I submit sent directly to me on HipChat. That will trigger a notification if I'm connected to HipChat or an email if not, so it would be fine.",NULL
DM-5409,"mpi4py does not build on OS X El Capitan","Attempting to build {{mpi4py}} with the {{mpi}} and {{mpich}} EUPS packages, does not seem to work with clang on OS X:  {code}  /Users/timj/work/lsstsw/stack/DarwinX86/mpich/3.1.4+da39a3ee5e/bin/mpicc -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -DPyMPI_MISSING_MPI_Type_create_f90_integer=1 -D PyMPI_MISSING_MPI_Type_create_f90_real=1 -D PyMPI_MISSING_MPI_Type_create_f90_complex=1 -I/Users/timj/work/lsstsw/miniconda/include/python2.7 -c _configtest.c -o _configtest.o  /Users/timj/work/lsstsw/stack/DarwinX86/mpich/3.1.4+da39a3ee5e/bin/mpicc -arch x86_64 _configtest.o -L/Users/timj/work/lsstsw/miniconda/lib -Lbuild/temp.macosx-10.5-x86_64-2.7 -o _configtest  Undefined symbols for architecture x86_64:    ""___muldc3"", referenced from:        import-atom in libpmpi.dylib    ""___mulsc3"", referenced from:        import-atom in libpmpi.dylib    ""___mulxc3"", referenced from:        import-atom in libpmpi.dylib  ld: symbol(s) not found for architecture x86_64  {code}    I thought that this all built fine when I reviewed DM-2983 so it's possible that there is an interaction with Homebrew or it is something about El Capitan (I would have tested DM-2983 on Yosemite). [~hchiang2] reports the same problem on her OS X El Cap system.    This error does seem to be related to Fortran complex numbers.",NULL
DM-5411,"lsstsw can't find cairo 1.12.18.6 in osx-64 conda channel","I sat down to do a fresh lsstsw install today and failed with the following message:    {code}  Python 2.7.11 :: Continuum Analytics, Inc.  creating default environment...  installation finished.  Fetching package metadata: ....  Error: No packages found in current osx-64 channels matching: cairo 1.12.18 6    You can search for this package on anaconda.org with        anaconda search -t conda cairo 1.12.18 6    You may need to install the anaconda-client command line client with        conda install anaconda-client  {code}    FWIW, I actually don't really want to use the miniconda install at all.  I'd rather just use my Python.  So this problem isn't a current blocker as I worked around this by   {code}  touch miniconda/.deployed  {code}     and commenting out the {{# export PATH=""$LSSTSW/miniconda/bin:$PATH""}} line in {{bin/setup.sh}}.",NULL
DM-5421,"Add --show history option to cmdLineTask","{{pex_config}} is able to report where a config parameter is set.  Please add a command line option {{--show history=config.parameter.name}} to the cmdLineTask parser.    The implementation will probably want to use something like:  {quote}  import lsst.pex.config.history as pch  pch.Color.colorize(False)  print pch.format(config.calibrate.astrometry.solver, ""matchingRadius"")  {quote}  ",NULL
DM-5429,"Add plugins for approximating the PSF as Gaussians with ngmix","Create a new ""meas_extensions_ngmix"" package, and add SFM and Forced plugins to approximate the PSF using a mixture of Gaussians using ngmix's E-M fitter.    There is example code to actually run ngmix in this mode in ngmix's README.md, but the tricky part is probably figuring out how to extract the outputs and write them to records.  If at all possible, we should use the same format as modelfit_ShapeletPsfApprox, so either algorithm could be used to feed CModel (and other ngmix algorithms, when modelfit_ShapeletPsfApprox is configured to use only zeroth-order fitting).    This will also require figuring out the failure modes and defining the appropriate flags.  Given that ngmix has no C++ interface, this work should be entirely Python, and hence it may be desirable to address DM-4009 somehow first.    This issue is almost certainly too large in terms of story points, but anything smaller doesn't really have a very well-defined end-point.  Maybe subtasks?",NULL
DM-5433,"Add {{lsst_ci}} as default build target for Jenkins","Add {{lsst_ci}} as default build target for Jenkins.    I think this is a simple edit to one line.    Notes of interest:  * {{lsst_ci}} explicitly requires {{lsst_distrib}} in its table file so one could actually replace the {{lsst_distrib}} default target with {{lsst_ci}}, but as we transition I might actually suggest leaving both in place to provide a bit of clarity.  * I'm not currently requesting adding this to the default {{lsstsw}} build product.",NULL
DM-5439,"Centralize configuration","At present validate_drp has a lot of duplication between the various yaml files, including nearly identical twins for normal and quick examples. Much of the duplication could be eliminated by making the validation code a normal task and saving the configuration parameters as a normal config. Advantages include:    - Properties that appear may be camera-specific, such as {{brightSnr}}, {{medianAstromscatterRef}} and {{medianPhotoscatterRef}}, can be set in the usual way: using a config override in the appropriate {{obs_}} package. That said, in many cases reasonable defaults exist based on the quality of our algorithms. The current values used for {{CFHT}} and {{DECam}} are identical; if those are good values then no overrides are needed for those cameras.  - It uses our standard for processing data, including standard methods of specifying data and configuration.",NULL
DM-5440,"Offer a way to set the value of a field to a given value for all records","I would like a simple (one-line) way to a field of a catalog to a given value. This should work for discontiguous catalogs and flag fields (unlike the numpy interface). It should preferably work field names (not just ""keys""), since the overhead for lookup occurs only once for setting all fields.    One use case is RFC-154, where we want to set an ""is star"" flag field to True for all instances of a subset of a source catalog.",NULL
DM-5442,"Subaru NB0656 filter definition missing","DM-5289 removed the definition of NB0656 but left it in the list of {{HscMapper.filters}}. This has broken Jenkins. Please restore the definition.",NULL
DM-5453,"Failure in ci_hsc: syntax error in obs_subaru's config/measureCoaddSources.py","The Jenkins build of {{ci_hsc}} is failing:    {code}  [2016-03-15T00:41:10.959702Z]   File ""/home/build0/lsstsw/stack/Linux64/obs_subaru/tickets.DM-5453-g53dd454477/config/measureCoaddSources.py"", line 14  [2016-03-15T00:41:10.959725Z]     config.astrometry.refObjLoader.load(getPackageDir(""obs_subaru""), ""config"", ""filterMap.py""))  [2016-03-15T00:41:10.959747Z]                                                                                               ^  [2016-03-15T00:41:10.959755Z] SyntaxError: invalid syntax  [2016-03-15T00:41:11.009328Z] scons: *** [.scons/measure] Error 1  [2016-03-15T00:41:11.011114Z] scons: building terminated because of errors.  {code}",NULL
DM-5454,"Afw tests/tickets2707.py fails on OS X","Recent afw builds on OS X are causing tests/ticket2707.py to fail with:    {code}  ======================================================================  FAIL: testMatchXy (__main__.MatchXyTest)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/ticket2707.py"", line 76, in testMatchXy      self.assertEquals(m.first.getId() + self.nobj, m.second.getId())  AssertionError: 18L != 16L    ======================================================================  FAIL: testMatchXyMatchControl (__main__.MatchXyTest)  Test using MatchControl to return all matches  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/ticket2707.py"", line 113, in testMatchXyMatchControl      self.assertEquals(m.first.getId() + self.nobj, m.second.getId())  AssertionError: 18L != 16L    ----------------------------------------------------------------------  {code}    [~rowen] has also seen this failure on his mac, but a build on lsst-dev does not reproduce it. This test had been largely disabled until DM-5280. The changes to Match.cc in that ticket appear not to be responsible, as reverting that particular file still causes the test to fail.",NULL
DM-5456,"Update Python coding standard based on RFC-162","RFC-162 proposes that the Python coding standard should be rewritten in terms of PEP-8. [~ktl] has requested that the proposed new text of the coding standard be submitted for his approval prior to incorporation into developer.lsst.io, presumably as a pull request. This ticket covers that rewriting.",NULL
DM-5459,"Provide a way of presenting the per-parameter documentation included in the config in a visually 'nice' way","The currrent Doxygen documentation generates documentation for options in the config class of tasks. However, the quality of the documentation generated leaves a lot to be desired: Doxygen merely embeds a block of code that just happens to include the help string but also includes a lot of unnecessary code. This makes the config parameters hard to find, hard to parse (by humans) and makes them very inconvenient to use when documenting source code. Provide some method for presenting the config options in a visually pleasing manner.",NULL
DM-5460,"Allow butler to use python classes without readFits","{{ObjectMaskCatalog}}, defined in [{{pipe_tasks}}'s {{objectMasks.py}}|https://github.com/lsst/pipe_tasks/blob/e866fdf2ea428bdadf5fea3a03dba008012d28ba/python/lsst/pipe/tasks/objectMasks.py#L8], (ab)uses [its {{readFits}} method|https://github.com/lsst/pipe_tasks/blob/e866fdf2ea428bdadf5fea3a03dba008012d28ba/python/lsst/pipe/tasks/objectMasks.py#L36] to persuade the Butler to initialize a Python class from a non-FITS datasource. Please provide a supported way to get the Butler to read non-FITS data.",NULL
DM-5461,"lsstsw rebuild --help is unhelpful","Running {{rebuild --help}} on lsstsw produces a list of command line flags, but doesn't explain what any of these flags do, even though several of them are very important (like {{-u}} for updating repos.yaml). Having a full set of help strings for each option would significantly improve the usability of lsstsw. ",NULL
DM-5466,"obs_decam calibration ingest uses fragile relative paths","When ingesting calibration files with obs_decam, the current configuration in the mapper policy is to save in the registry the path from the current working directory to the ingested files. This is very fragile and can only work if {{ingestCalibs.py}} is run from within the calibration repository itself, and there are numerous ways in which this can still fail. The ingest script and mapper policy file should be updated to specify a path template, and to symlink or copy the calibration files into the repository as is done for ingesting images.",NULL
DM-5467,"Distribute bad pixel masks in obs_decam","Processing Decam data from raw images currently requires downloading the bad pixel mask files from NOAO, then ingesting them into the calibration repository. It would be much more convenient if we included these with obs_decam itself, pending the ok to do so from NOAO. Our understanding is that we could create a default calibration repository in obs_decam, which would be the parent of users' calibration repos. We will also likely want to convert the provided files from mask images into tables of bounding boxes.",NULL
DM-5500,"Minor fixes to ingestCalibs.py ","I hit three minor snags when trying to ingest Decam calibration products:    1. After the change in DM-4236 to by default require output repositories for all CommandLineTasks, `ingestCalibs.py` needs to be switched to use `InputOnlyArgumentParser` to avoid this requirement.    2. sqlite returns unicode strings, which are not supported by our logging routines. Log messages that may include output from sqlite need to be sanitized with `str()` just to be safe.    3. A validity period is required for importing calibration files, but no default was given in the argument parser.",NULL
DM-5504,"Backlog for QA Visualisation Tasks","  At this point, this epic holds backlog items so that they do not get lost. ",NULL
DM-5522,"SQuaRE hackday projects","This is a bucket epic for relatively low-priority work that is appropriate for the ""many hands"" approach.     Please do not schedule in a cycle. Stories that actually get serviced will be allocated in an appropriate cycled epic. ",NULL
DM-5524,"MegaCam vertical overscan dimensions off by one","[~boutigny] discovered, following the merge of new functionality in ISR that masks the vertical overscan, that the vertical overscan extent is too large by one pixel for all CCDs.  They should be 1024x32 instead of 1024x33.",NULL
DM-5525,"Regions checked for saturation need refining","The regions searched for saturation are ""RawBBox"", ""RawHorizontalOverscanBBox"" and ""RawVerticalOverscanBBox"".  I see two problems:  1. The ""RawBBox"" includes the ""RawHorizontalOverscanBBox"" and ""RawVerticalOverscanBBox"".  2. The ""RawPrescanBBox"" is missing.    I think it should be sufficient to just do ""RawBBox"", as it appears to be a master region, encompassing all the others.  This will help avoid problems due to bad definitions of the generally unused ""RawVerticalOverscanBBox"", etc., e.g., DM-5524.",NULL
DM-5527,"dm.lsst.org installation instructions are broken for El Capitan","http://dm.lsst.org/ instructs new users to install version 11 of the stack with {{eups distrib}}. This will fail for anybody using OSX >= 10.11. That's not a good new-user experience.",NULL
DM-5528,"Remove background matching from pipe_tasks","The background matching introduced into pipe_tasks a couple of years ago works only on drift-scan observations and is not applicable to pointed observations (e.g., DECam, HSC, LSST). Making background matching work with pointed observations will take a lot more care and coordinated processing across patches due to the smaller scales involved (arcminutes for CCDs vs many tens of degrees for SDSS runs). However, background matching is currently enabled by default in the stack, which necessitates overrides (which are often neglected, at least initially, by the new user) for pointed observations, which is the vast majority of current use cases.    Given that the current background matching code is only applicable to a single camera supported in our stack, [we have agreed|RFC-137] to remove it and disable {{MakeCoaddTempExpConfig.bgSubtracted}} by default.",NULL
DM-5529,"Make afw::Coord pure virtual and add SphPoint","Implement RFC-131:    Make {{afw::geom::Coord}} pure virtual (so it cannot be instantiated) and to add a new class {{afw::geom::SphPoint}} that represents a spherical point.    {{Coord}} should contain a {{SphPoint}} rather than inherit from it. This allows {{Coord.offset}} and {{Coord.rotate}} to take a {{SphPoint}} argument, without the confusion of allowing the user to provide a {{Coord}} and wondering what is done with the coordinate system and epoch information.    When implementing, look for opportunities to integrate with sphgeom package, such as borrowing SphPoint from there.    Also be careful to integrate with the upcoming WCS overhaul.    Only support distance if this makes the code simpler in some way (such as being provided by code borrowed from elsewhere). It is explicitly not a requirement.",NULL
DM-5549,"Create a compact, time-modifield listing of our GitHub repos","It would be nice to have a compact listing of our GitHub repos like we used to have on gitolite.     You can still see the old one at https://dev.lsstcorp.org/cgit/.     I think the ability to have it be alphabetically ordered is good and bonus points for some sort of pseudo directory structure based on metadata (based on name or description).  ",NULL
DM-555,"Replicator jobs submission","On receipt of the startIntegration event, the Base DMCS launches replicator jobs, one for each science raft (21) and one more job which will handle wavefront sensor images.  The jobs are launched with raft ids, with ""raft"" 22 being a special case for the wavefront image job.",NULL
DM-5557,"Store background model with exposure","Implement RFC-161: store an exposure's background model with the exposure",NULL
DM-556,"Event Command tool.","Write a command line tool that sends the startIntegration event to the Base DMCS, which is listening for event on the OCS Topic.  This tool also should be able to send the startReadout event to the Replicator jobs.",NULL
DM-557,"Job interaction with replicator, and data pull.","Sequence:  1) On startup, each job has a raft assigned to it by the Base DMCS.  The job subscribes and waits for the startReadout event, which contains an image id. 2) Receive a startReadout event from the command line tool. 3) Receive the event with the startReadout information, and extracts the image id. 4) Pull the data indicated in the event, calling getCameraImage(image id, raft).",NULL
DM-5570,"Move data in obs_test to git-lfs","I would like to add more data to obs_test: calexp and source catalogs associated with the raw data (to provide data for examples). Before doing so, I am wondering if it makes sense to save the existing data (and new data) as git-lfs data. The existing raw data has been altered at least once, and I suspect calexp and source catalogs may be updated more often.",NULL
DM-5571,"Write overview of X16 plan for Data Access and DB","Write a short overview of the planned work of your team in X16 and add it to your page under: https://confluence.lsstcorp.org/display/DM/X16. Audience that you are writing it for: the DM team.",NULL
DM-5572,"Write overview of X16 plan for SQuaRE","Write a short overview of the planned work of your team in X16 and add it to your page under: https://confluence.lsstcorp.org/display/DM/X16. Audience that you are writing it for: the DM team.",NULL
DM-5573,"Write overview of X16 plan for NCSA team","Write a short overview of the planned work of your team in X16 and add it to your page under: https://confluence.lsstcorp.org/display/DM/X16. Audience that you are writing it for: the DM team.",NULL
DM-5574,"Write overview of X16 plan for DRP team","Write a short overview of the planned work of your team in X16 and add it to your page under: https://confluence.lsstcorp.org/display/DM/X16. Audience that you are writing it for: the DM team.  ",NULL
DM-5575,"Write overview of X16 plan for AP team","Write a short overview of the planned work of your team in X16 and add it to your page under: https://confluence.lsstcorp.org/display/DM/X16. Audience that you are writing it for: the DM team.  ",NULL
DM-5576,"Write overview of X16 plan for Architecture team","Write a short overview of the planned work of your team in X16 and add it to your page under: https://confluence.lsstcorp.org/display/DM/X16. Audience that you are writing it for: the DM team.",NULL
DM-5577,"Write overview of X16 plan for Base Site and Networks team","Write a short overview of the planned work of your team in X16 and add it to your page under: https://confluence.lsstcorp.org/display/DM/X16. Audience that you are writing it for: the DM team.  ",NULL
DM-5583,"Compute size of alerts for sizing model","LDM-141 has a nominal size for alert records in the database and does not try to compute the size of alert messages (whether sent via binary or XML protocol).  The former can be better estimated using the requirements in the DPDD; the latter should be estimated to determine the outbound network bandwidth to brokers.    If the size of alert messages grows too large, a possible requirements change could be implemented to limit the DIASources in each alert to those seen in the previous year.",NULL
DM-5587,"daf_persistence fails to build due to mariadbclient not finding openssl libraries","When mariadbclient builds, it finds the openssl libraries in either miniconda or anaconda (or possibly other versions of openssl libraries anywhere on the system).   However, when mariadbclient attempts to run later (such as when building daf_persistence), it can no longer find the libraries it used to build itself.   Mario tells me it has to do with the libraries being linked into mariadbclient using 'rpath', but then rpath not being expanded somewhere in the files associated with mariadbclient .. in conda-build, these rpaths are expanded to the full values so everything works (and maybe this would be a solution - to make eups expand these rpath values, but then he kind of frowned).     Anyway - this is a problem for users attempting to install the stack. The current answer appears to be ""tell users to install openssl"" but ..  On redhat, even after doing 'yum install openssl' and 'yum install openssl-devel', the yum installed libraries were not used by mariadbclient (probably because they were version 1.0.1 instead of 1.0.0, which is what our mariadbclient is looking for). ",NULL
DM-5589," lsst.afw.display.get_display() should raise warning if display_ds9 is not setup","{code:python}  # exp is an instance of `lsst.afw.image.ExposureF  * import lsst.afw.display as display  disp = display.getDisplay()  disp.mtv(exp)  {code}    runs without any complains or indication about a problem but fails to display anything on ds9, even when it is already open.  As [~rowen] pointed out, the problem can be solved by setting up display_ds9 before running the code (other display managers will work too).    It would be nice to log a warning that informs the user of this requirement when {{disp.mtv}} is called without any display manager being set up.  ",NULL
DM-5592,"Update shapeHSM table file to define LSST_LIBRARY_PATH","The table file does not set the {{LSST_LIBRARY_PATH}} environment variable, and it should. Please update the table file accordingly.",NULL
DM-5596,"Remove obsolete install scripts from ~/src/qserv/admin/tools/","Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.",NULL
DM-5597,"Remove obsolete install scripts from ~/src/qserv/admin/tools/","Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.",NULL
DM-5598,"Remove obsolete install scripts from ~/src/qserv/admin/tools/","Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.",NULL
DM-5599,"Remove obsolete install scripts from ~/src/qserv/admin/tools/","Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.",NULL
DM-5600,"Remove obsolete install scripts from ~/src/qserv/admin/tools/","Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.",NULL
DM-5601,"Remove obsolete install scripts from ~/src/qserv/admin/tools/","Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.",NULL
DM-5602,"Remove obsolete install scripts from ~/src/qserv/admin/tools/","Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.",NULL
DM-5603,"Remove obsolete install scripts from ~/src/qserv/admin/tools","Internet-free install procedure has never been used for real install and seems to be replace in practice by Docker procedure. So related scripts and documentation should be removed.",NULL
DM-5606,"Advisory Messages","Add Advisory Message receiving support.  This allows monitoring of consumer/producer activity by receiving messages on standard topics supported by the broker.    See:    http://activemq.apache.org/advisory-message.html    for more information.    Support will be added for consumed messages and consumer connectivity.   ",NULL
DM-5615,"Add pipe_drivers to lsstsw","Please make it possible to build pipe_drivers as part of lsstsw.",NULL
DM-562,"Replicator node association with distributor node","On startup, the replicator node creates and maintains a connection to its preconfigured distributor node.",NULL
DM-5629,"x16 LOE: service support for LSST developers using NCSA resources","Service management for NCSA resources, including lsst-dev, Nebula OpenStack, and FY16 capabilities after deployment.",NULL
DM-563,"Replicator jobs send job info to distributor node","The replicator jobs receive the visit id, exposure sequence number within the visit, and raft id from the Base DMCS.  This information is passed from the replicator job to a process on the duplicator node.",NULL
DM-5630,"x16 LOE: sys admin support for operating LSST development resources at NCSA","General system administration support for operating lsst-dev, Nebula OpenStack cluster, and FY16 capabilities after deployment.",NULL
DM-5634,"Add pipe_drivers to lsst_distrib","Please add pipe_drivers to lsst_distrib.    Does this need an RFC?",NULL
DM-5635,"not flagged NaN in sdssCentroid","In some rare cases base_SdssCentroid_x(y)Sigma can be NaN while base_SdssCentroid_flag is False",NULL
DM-5637,"Add automated linting for GitHub pull requests","As part of RFC-162 adoption of a {{flake8}} configuration file for checking of conformance of python code to the coding style, we need to implement automated linting reports when pull requests are created.",NULL
DM-5639,"Set up auto-responder no-reply@community.lsst.org address","The Community Mailbot forwards message notifications using the no-reply@community.lsst.org address. While this, along with the email template that points readers to the conversation on Community, prevents 99% of users from replying to the forwarded message, it can be a poor experience for anyone who accidentally does reply to the Mailbot’s forwarding messages.    This ticket will investigate an implement an autoresponder for the no-reply@community.lsst.org email address to tell users they’ve made a mistake and to kindly post their reply onto the Community forum itself.",NULL
DM-5644,"Technote Platform Development","This technote supports the development of Sphinx-based platforms for technical notes and design documents written in reStructuredText. Note that some of the Sphinx extension development here may benefit software documentation projects. This Epic also relates to LSST the Docs support as they related to technote/design doc automation.    *Provisional goals:*    - Automated provisioning of new technical notes on GitHub and LSST the Docs.  - Improved web design that can effectively support printed PDF depositions for Zenodo/DocuShare.  - Automated workflow for uploading technotes to Zenodo (or other DOI provider) and ADS  - Sphinx extension support for Jupyter notebook content  - Sphinx extension support for bibliographies that integrate with BibTex (using [sphinxcontrib.bibtex|https://github.com/mcmtroffaes/sphinxcontrib-bibtex]) with custom extensions to support getting bibliographic data dynamically/automatically from ADS.",NULL
DM-5646,"Backlog Epic for pipelines.lsst.io","This Epic covers the development of our software documentation projects; original content is covered in a separate Epic.    Provisional goals for this epic:    - Provision and deliver of software documentation projects with _LSST the Docs_ for LSST software products.  - Migrate stack package {{doc/}} directories to the new Sphinx format expected by ltd-mason.  - Establish and implement patterns for API reference sections  - Establish content pattern for Stack package documentation (including updating Stack package templates)  - Establish pattern for documenting tasks  - Establish pattern for including tutorials (including notebooks, sample data, and CI of tutorials)  - Sphinx theme web design and branding",NULL
DM-565,"Add Workers to simulator - v4","Workers added to the simulator.  Assume: Workers nodes have already in the pool.  Sequence:  * Base DMCS receives nextVisit event * Base DMCS launches worker jobs, one for each CCD (189) and four more for wavefront sensors. * Each worker job launches with visit id, the number of exposures to be taken, a foresight pointing, a filter id, and a CCD id.   * Worker computes the spatial region that covers the expected area of the CCD plus a margin.    * Worker retrieves the template image (by filter and airmass), Objects (from the last Data Release), DiaObjects, past DiaSources, and SSObjects that overlap the region using the Data Access Client Framework.  It also retrieves the master calibration images appropriate for that CCD and filter.  The following two steps are done twice in normal science mode;  there may be other configurations were this is done once, or more than twice. * Worker jobs contact the Archive DMCS via a blocking call to determine the distributor for the image and raft. * Worker jobs contact the distributor (from the previous step) to request the image via a blocking call.  Finally: * Worker jobs send VOEvents",NULL
DM-5656,"Fix Slurm submission options","The following commit has not found its way from HSC to LSST, and the bug it fixes is still present in LSST:  {code}  commit 4d85e691848004a6e32d638a2c3e7600a05f83e1  Author: Paul Price <price@astro.princeton.edu>  Date:   Fri Sep 11 09:18:49 2015 -0400        SlurmBatch: fix submission options            Had confused 'options' (which get put in the submission script)      and 'submit' (which get put in the command-line), so that      --batch-submit arguments were being ignored.  {code}",NULL
DM-5661,"Please provide weekly & release tags of obs_*","Please add the {{lsst_ci}} top-level package to the system used to tag weeklies. This will make it possible to include it in the shared stack provided on development machines (DM-5435), which is an important user request ([1|https://community.lsst.org/t/shared-stack-on-lsst-development-machines/641/6], [2|https://community.lsst.org/t/shared-stack-on-lsst-development-machines/641/11]).",NULL
DM-5662,"New chat room for notifications from Jenkins branch builds","I think it would make it significantly easier to notice when ""the build"" is broken if notifications for branch builds (that often fail) went to a different chat room than those for master builds (that are expected to succeed).  Is it difficult to implement this?",NULL
DM-5664,"Delete or document and test config/psfex.py","The file {{config/psfex.py}} has no documentation and is bit rotting. If you feel it should be kept then please document it and add a simple unit test that loads it and runs data using it. If it is not needed, then please get rid of it.",NULL
DM-567,"Collect/Develop ops use cases for the development cluster","Need to develop the rationale for the acquisition of Dev and I&T cluster hardware in FY15 and early years. There's some uncertainty as to whether we need a large cluster that early, or should it be deferred to FY16.  Issue opened as a follow-up to personal e-mail discussion with subject ""discussions with mario last night"". See that thread for some comments/proposals. ",NULL
DM-5678,"Add LSST the Docs Keeper API routes for creating new API users","The ADMIN_USER permission should allow a user to create other API users. The user should be able to GET itself and see its own permission set (but not other API users). Also add general documentation on permissions levels for HTTP users.",NULL
DM-5679,"Change LSST the Docs Keeper response content based on API user authorization level","Make it possible for some routes to be semi-opened. For example, a user who administers a Product might see 'private' infrastructural details about the Product that an anonymous user would not see. A way to do this is described in https://github.com/miguelgrinberg/Flask-HTTPAuth/issues/17#issuecomment-76668519",NULL
DM-568,"Base DMCS launch sequence for workers","Implement the following sequence: 1) The Base DMCS waits for a nextVisit event 2) Upon receiving this event, the Base DMCS either creates a DAG with 189 jobs with appropriate arguments (see DM-565 for description).  This uses the DAG creator from DM-569. 3)  Base DMCS launches this DAG to be executed on HTCondor.",NULL
DM-5680,"makePsfCandidate needs to be easier and safer to use","The following code can fail at getMaskedImage:  {code}              try:                  psfCandidate = algorithmsLib.makePsfCandidate(star, exposure)              except Exception as err:                  self.log.warn(""Failed to make a psfCandidate from star %d: %s"" % (star.getId(), err))                  continue                                # The setXXX methods are class static, but it's convenient to call them on              # an instance as we don't know Exposure's pixel type              # (and hence psfCandidate's exact type)              if psfCandidate.getWidth() == 0:                  psfCandidate.setBorderWidth(self.config.borderWidth)                  psfCandidate.setWidth(self.config.kernelSize + 2*self.config.borderWidth)                  psfCandidate.setHeight(self.config.kernelSize + 2*self.config.borderWidth)                im = psfCandidate.getMaskedImage().getImage()  {code}  with an error such as:  {code}  Box2I(Point2I(-2,1120),Extent2I(41,41)) doesn't fit in image 2048x4176...  {code}  It is easy to wrap more of this code in a try/except block (and that is what we will do for now in starSelector.py) but it would be nice if the code was more robust and complained when the bad values were set, instead of later when the masked image was requested.",NULL
DM-5682,"Add meas_extensions_simpleShape to lsst_apps","In DM-5284 [~nlust] added a new {{meas_extensions_simpleShape}} algorithm. This should be part of the {{lsst_apps}} top-level package. Please make it so.",NULL
DM-5683,"List all dependencies for meas_extensions_simpleShape","In DM-5284, {{meas_extensions_simpleShape}} was added to the stack. However, not all of its dependencies were included in the table file. Full listing of dependencies is required following RFC-52. Please add them. ([~tjenness] suggests this should include at least {{numpy}}, {{utils}}, {{pex_config}} and {{pex_exceptions}}).",NULL
DM-5684,"Unused variables in meas_extensions_psfex library code","meas_extensions_psfex library code contains many errors and warnings. Much of this will be fixed in DM-5663, but some fixes are less obvious. Those should be done on this ticket. Errors include undefined variable {{vignet}} in {{psfexStarSelector.py}}, several others in psfexPsfDeterminer and many in {{utils.py}}",NULL
DM-5687,"Parallelize reference catalog ingestion","It's very likely that we'll need the reference ingestion and indexing to happen in parallel, otherwise it will be too slow.  The base version was implemented in DM-1579.",NULL
DM-569,"HTCondor Job Creation","Write an HTCondor Job creation object/utility that will be used to write HTCondor Job files which will be used to submit to the HTCondor cluster.   This should be able to be used to create/submit/monitor replicator jobs and worker jobs.",NULL
DM-5690,"dm_dev_guide exclude_patterns for snippets incorrect","{{dm_dev_guide:conf.py}} currently reads:  {code:python}  exclude_patterns = ['README.rst', '_build', 'development/docs/snippets']  {code}  There is no {{development}} directory. Suggest it should be:  {code:python}  exclude_patterns = ['README.rst', '_build', 'docs/snippets']  {code}    This causes a warning on build:  {code}  docs/snippets/figure.rst:None: WARNING: duplicate label fig-example-figure-label  {code}",NULL
DM-570,"Write object/utility to send VOEvents","Write an object (or possibly a command line tool) which will send VOEvents on behalf of the worker job. This can initially be using DM Messages, but will transition to real VOEvents once we've evaluated the VOEvent software.",NULL
DM-5706,"Set up a docker registry at IN2P3","A local docker registry at CC-IN2P3 would improve performance when running Qserv from Docker, and would also prevent using Squid proxy for all nodes to access the internet.    A discussion between Square Team and CC-IN2P3 might be useful to clarify requirements before setting up registry, do we need:    1. a registry with :     - full internet access    - authentication for Qserv team    - 100 GB storage on a local device    2. A docker hub mirror registry seems easier to set up and maintain, and might be enough:  [https://docs.docker.com/v1.6/articles/registry_mirror/]    ",NULL
DM-5707,"Set up a docker registry at IN2P3","A local docker registry at CC-IN2P3 would improve performance when running Qserv from Docker, and would also prevent using Squid proxy for all nodes to access the internet.    A discussion between Square Team and CC-IN2P3 might be useful to clarify requirements before setting up registry, do we need:    1. a registry with :     - full internet access    - authentication for Qserv team    - 100 GB storage on a local device    2. A docker hub mirror registry seems easier to set up and maintain, and might be enough:  [https://docs.docker.com/v1.6/articles/registry_mirror/]    ",NULL
DM-5708,"Set up a docker registry at IN2P3","A local docker registry at CC-IN2P3 would improve performance when running Qserv from Docker, and would also prevent using Squid proxy for all nodes to access the internet.    A discussion between Square Team and CC-IN2P3 might be useful to clarify requirements before setting up registry, do we need:    1. a registry with :     - full internet access    - authentication for Qserv team    - 100 GB storage on a local device    2. A docker hub mirror registry seems easier to set up and maintain, and might be enough:  [https://docs.docker.com/v1.6/articles/registry_mirror/]    ",NULL
DM-5709,"Set up a docker registry at IN2P3","A local docker registry at CC-IN2P3 would improve performance when running Qserv from Docker, and would also prevent using Squid proxy for all nodes to access the internet.    A discussion between Square Team and CC-IN2P3 might be useful to clarify requirements before setting up registry, do we need:    1. a registry with :     - full internet access    - authentication for Qserv team    - 100 GB storage on a local device    2. A docker hub mirror registry seems easier to set up and maintain, and might be enough:  [https://docs.docker.com/v1.6/articles/registry_mirror/]",NULL
DM-571,"please provide environment or config-file defaults for lsst-build options","I'd like to be able to run lsst_build without having to specify --exclusion-map and --version-git-repo on the command-line every time.  While I can do that with an alias, I think it should be something built-in.  Ideally, for --exclusion-map, there'd be a search path and some sort of syntax for having higher-priority config locations override lower-priority options.  (As you might have guessed, this will be the first of many feature requests for lsst-build as I try to switch from my own bot tool and find things that seem to be missing).",NULL
DM-5710,"Set up a docker registry at IN2P3","A local docker registry at CC-IN2P3 would improve performance when running Qserv from Docker, and would also prevent using Squid proxy for all nodes to access the internet.    A discussion between Square Team and CC-IN2P3 might be useful to clarify requirements before setting up registry, do we need:    1. a registry with :     - full internet access    - authentication for Qserv team    - 100 GB storage on a local device    2. A docker hub mirror registry seems easier to set up and maintain, and might be enough:  [https://docs.docker.com/v1.6/articles/registry_mirror/]",NULL
DM-5712,"Reduce driver tasks' duplicated config override in the obs packages","pipe_driver tasks glue multiple steps into one, and the config override files from these steps are typically wanted. Currently it is done by going through all subtasks in the config override files of the obs pacakges.  For example see {{config/multiBandDriver.py}} and {{config/CoaddDriver.py}} in {{obs_subaru}} and {{obs_decam}}.  Please investigate how to push this up into the framework, and add a feature so to reduce duplicate code in the obs packages. ",NULL
DM-5713,"Set up Docker Registry for in2p3 Qserv cluster","A local docker registry at CC-IN2P3 would improve performance when running Qserv from Docker, and would also prevent using Squid proxy for all nodes to access the internet.    A discussion between Square Team and CC-IN2P3 might be useful to clarify requirements before setting up registry, do we need:    1. A docker hub mirror registry which seems easy to set up and maintain, and might be enough:  [https://docs.docker.com/v1.6/articles/registry_mirror/]    Or:    2. a stand-alone registry with :   - full internet access   - authentication for Qserv team    Common requirement for both proposals would be:  - stand-alone virtual machine  - registry port reachable for all Qserv cluster  - 100 GB storage on a local device    ",NULL
DM-5714,"Please allow upload of .txt and .log to discourse","I would like to be able to upload log files to discourse. At present it only seems to allow pdf and various image formats. (Yes I could turn a log file into pdf, but I'd rather not have to do that).",NULL
DM-5718,"Reduce verbosity of WCS fitter","The WCS fitter is too chatty:  {code}  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 85 stars  processCcd.calibrate.astrometry.matcher: Purged 1314 unusable sources, leaving 40 usable sources  processCcd.calibrate.astrometry.matcher: Matched 23 sources  processCcd.calibrate.astrometry.wcsFitter: Updating centroids in refCat  processCcd.calibrate.astrometry.wcsFitter: Updating coords in sourceCat  processCcd.calibrate.astrometry.wcsFitter: Updating distance in match list  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 85 stars  processCcd.calibrate.astrometry.matcher: Purged 1314 unusable sources, leaving 40 usable sources  processCcd.calibrate.astrometry.matcher: Matched 22 sources  processCcd.calibrate.astrometry.wcsFitter: Updating centroids in refCat  processCcd.calibrate.astrometry.wcsFitter: Updating coords in sourceCat  processCcd.calibrate.astrometry.wcsFitter: Updating distance in match list  processCcd.calibrate.astrometry.matcher: filterStars purged 0 reference stars, leaving 85 stars  processCcd.calibrate.astrometry.matcher: Purged 1314 unusable sources, leaving 40 usable sources  processCcd.calibrate.astrometry.matcher: Matched 21 sources  processCcd.calibrate.astrometry.wcsFitter: Updating centroids in refCat  processCcd.calibrate.astrometry.wcsFitter: Updating coords in sourceCat  processCcd.calibrate.astrometry.wcsFitter: Updating distance in match list  {code}    The ""Updating"" messages don't provide any useful information for the user, and only clutter the logs.",NULL
DM-572,"Fault tolerance for AP Simulator - v5","Add components that aid in fault tolerance of components in the simulator.  * Build replicator/distributor heartbeat/lease * Build replicator and distributor output checks * Build multiple replicator pools",NULL
DM-5721,"Further optimize dipole fitting task as necessary","There are additional paths for optimization and robustification of the dipole measurement task. This is a ticket for such work as need arises.    One necessary task is that Zeljko is concerned about the performance of DipoleFitTask in crowded fields, particularly in flux measurement. This ticket will involve testing it in crowded fields, and investigating constraining only positions using the pre-subtraction images, and fitting the flux only in the diffim.",NULL
DM-5727,"Need framework for documenting cycle work","It'd be good to have a framework for documenting work done in each cycle (beyond ""just"" release notes).",NULL
DM-573,"Build replicator/distributor heartbeat/lease","The idea here is to have some kind of system of monitoring at regular intervals to detect when a machine can no longer be contacted.",NULL
DM-5730,"mariadb build failure on OS X with Macports lz4 library","I was able to build {{eups distrib install -t w_2016_15 lsst_distrib}} fine on my OS X El Capitan laptop, but it failed on my desktop on {{mariadb}}:  {noformat}  [100%] Building CXX object storage/mroonga/CMakeFiles/mroonga.dir/lib/mrn_value_decoder.cpp.o  [100%] Building CXX object storage/mroonga/CMakeFiles/mroonga.dir/lib/mrn_database_repairer.cpp.o  [100%] Linking CXX shared module ha_mroonga.so  ld: library not found for -llz4  clang: error: linker command failed with exit code 1 (use -v to see invocation)  {noformat}    The difference between my laptop and my desktop is that I had liblz4 installed via Macports on the desktop, but not on the laptop. At some point in the build log for mariadb, the lz4 library was found on the desktop, but not on the laptop.    Uninstalling liblz4 from my desktop now allows mariadb to build.",NULL
DM-5731,"Design layout for including code in technotes","Including code and Jupyter notebooks in technotes is a great idea for reproducibility. Users are already establishing cowpaths in their own technotes for this. This ticket will design a layout to make it easy to support code with technotes in the future (such as running CI dynamically deriving figures+tables by running code, and including notebooks in the technotes themselves. This ticket will also document this layout and provide guidance on how to include code in technotes.",NULL
DM-5732,"Outline structure for KPM CoDR document ","- Canonical table of KPMs and the document reference they flowdown like (kind of like the ldm240 tab 2)  LSE-029     - For each KPM we specify:    * Verification Owner: the person or group that performs the tests     * Verification Technique: How the KPM will be tested conceptually    * Verification Code: What code will be used for this test [blank for CoDR]    * Verification Implementation: Which group is delivering  any code that has to be written specifically for this test specially [blank for CoDR]    * Verification Data: How can the KPM test be evaluted prior to real data being available (percursor data - maybe a hierarchy of preference HSC, DECAM etc - , simulated data)      - Process    * SQuaRE will compile the testing report as part 02C09    * An outline of process (obtain from MJ) on signing off on the report of this testing     ",NULL
DM-5733,"Lead internal (DM) review process of KPM CoDR draft","This story captures the process of getting DM comments on the document draft and applying any changes required to bring it to final form. ",NULL
DM-5735,"butler.subset problems with obs_lsstSim","Using a obs_lsstSim data repo,  Butler.subset complains about a missing raw_skyTile table.  Here is my test code:    {code}  bash-4.1$ cat butler_test.py  import lsst.daf.persistence as dp    data_repo = '/nfs/farm/g/lsst/u1/users/tonyj/Twinkles/run1/985visits'    butler = dp.Butler(data_repo)    print butler.mapper    base_dataId = dict(raft='2,2', sensor='1,1', tract=0)  dataId = dict(filter='u', visit=1000185)  dataId.update(base_dataId)  src = butler.get('src', dataId=dataId)  print src    uband_dataId = dict(filter='u')  uband_dataId.update(base_dataId)  datarefs = butler.subset('src', dataId=uband_dataId)  print datarefs  {code}    and the output:    {code}  bash-4.1$ python butler_test.py  CameraMapper: Loading registry registry from /nfs/farm/g/lsst/u1/users/tonyj/Twinkles/run1/985visits/_parent/registry.sqlite3  <lsst.obs.lsstSim.lsstSimMapper.LsstSimMapper object at 0x7ffe21e7add0>  <lsst.afw.table.tableLib.SourceCatalog; proxy of <Swig Object of type 'lsst::afw::table::SortedCatalogT< lsst::afw::table::SourceRecord > *' at 0x7ffe2143bba0> >  Traceback (most recent call last):    File ""butler_test.py"", line 17, in <module>      datarefs = butler.subset('src', dataId=uband_dataId)    File ""/nfs/farm/g/desc/u1/LSST_Stack_2016-02-23/lsstsw/stack/Linux64/daf_persistence/2016_01.0-2-g3878625+3/python/lsst/daf/persistence/butler.py"", line 377, in subset      return ButlerSubset(self, datasetType, level, dataId)    File ""/nfs/farm/g/desc/u1/LSST_Stack_2016-02-23/lsstsw/stack/Linux64/daf_persistence/2016_01.0-2-g3878625+3/python/lsst/daf/persistence/butlerSubset.py"", line 90, in __init__      level, fmt, self.dataId):    File ""/nfs/farm/g/desc/u1/LSST_Stack_2016-02-23/lsstsw/stack/Linux64/daf_persistence/2016_01.0-2-g3878625+3/python/lsst/daf/persistence/butler.py"", line 210, in queryMetadata      tuples = self.mapper.queryMetadata(datasetType, key, format, dataId)    File ""/nfs/farm/g/desc/u1/LSST_Stack_2016-02-23/lsstsw/stack/Linux64/daf_persistence/2016_01.0-2-g3878625+3/python/lsst/daf/persistence/mapper.py"", line 101, in queryMetadata      return func(key, format, self.validate(dataId))    File ""/nfs/farm/g/desc/u1/LSST_Stack_2016-02-23/lsstsw/stack/Linux64/daf_butlerUtils/2016_01.0+16/python/lsst/daf/butlerUtils/cameraMapper.py"", line 285, in queryClosure      return mapping.lookup(format, dataId)    File ""/nfs/farm/g/desc/u1/LSST_Stack_2016-02-23/lsstsw/stack/Linux64/daf_butlerUtils/2016_01.0+16/python/lsst/daf/butlerUtils/mapping.py"", line 177, in lookup      return self.registry.lookup(properties, self.tables, lookupDataId)    File ""/nfs/farm/g/desc/u1/LSST_Stack_2016-02-23/lsstsw/stack/Linux64/daf_butlerUtils/2016_01.0+16/python/lsst/daf/butlerUtils/registries.py"", line 306, in lookup      c = self.conn.execute(cmd, valueList)  sqlite3.OperationalError: no such table: raw_skyTile  bash-4.1$   {code}    I've attached the output of eups list | grep setup so that you can see my runtime configuration.    ",NULL
DM-5736,"git-lfs configuration is fragile.","The git-lfs configuration is fragile. The git-lfs client doesn't support our most common use case.",NULL
DM-5737,"Upgrade the git-lfs server protocol","The git-lfs server protocol needs to be upgrade to support the batch protocol. This is required to get support from the git-lfs core team and to keep up with the client which will deprecate v1 API in the near future.    See:    https://github.com/github/git-lfs/blob/master/docs/api/README.md?utm_source=gitlfs_site&utm_medium=api_spec_link&utm_campaign=gitlfs",NULL
DM-574,"Start replicator node servers/distributor servers/archive DMCS in arbitrary order","These servers are interdependent, and need to be started ""in order"".   This task will enable these servers to be started in any order.   Depending on the client/server relationship the lost connections will also need to be reestablished.",NULL
DM-5749,"Update welcome message in newinstall.sh","The welcome message should point a user to community.lsst.org rather than the mailing lists.",NULL
DM-575,"Archive DMCS receives information from distributors when it restarts","The Archive DMCS maintains distributor's network address and IP port information.  When the Archive DMCS goes down, this information is lost.   It must reestablish this information.  The current idea is that the Archive DMCS will send out a DM message to which all the active Distributors will respond with their information.",NULL
DM-5751,"16 bit vs 18 bit usage in LDM-141 and LCR-131","I might have misinterpreted this in the documentation I read, so if this is completely incorrect, please ignore this.    LDM-141 says the images are 16-bits per pixel.  LCR-131 says this info is now 18-bits per pixel.  The Confluence pagehttps://confluence.lsstcorp.org/display/LKB/LSST+Key+Numbers  says that the images are 16-bits per pixel.  The Key numbers page http://lsst.org/scientists/keynumbers says Dynamic Range is 18 bits.    Some of the documentation says we're handling 16-bits per pixel, and some says we're dealing with 18-bits per pixel.  Now, I believe that it's settled on 18-bits.    A lot of calculations in LDM-141 use 16-bits per pixel, which cascades throughout the rest of the spreadsheet.  (examples otherInput!H10, otherInput!H12).",NULL
DM-5752,"Add ""original cycle"" field in jira for epics","The last few days exposed that we need a new field in JIRA for epics, something like ""original cycle"". The problem is that if some epics are late (like the 29 W16 epics that are now assigned to X16 or F16): https://jira.lsstcorp.org/issues/?filter=14212    We had to manually track them down, just look at the filter:    {code}  project = DM AND   issuetype = Epic AND   key in (DM-1109, DM-1111, DM-1117, DM-1123, DM-1233, DM-1955,           DM-1956, DM-1988, DM-2089, DM-2208, DM-2586, DM-3500,           DM-3559, DM-3578, DM-3581, DM-3594, DM-3602, DM-3622,           DM-3647, DM-3832, DM-3838, DM-3841, DM-3842, DM-3875,           DM-3876, DM-3878, DM-3887, DM-3906, DM-3909)   ORDER BY cf[10202] DESC, cf[10502] DESC, Cycle, WBS, Key  {code}    So, as much as I hate to ask for another field, it will really help.    And we can always do a swipe through all epics when we are starting a new cycle and set these if they are not set, so hopefully it won't be much burden on anyone.    I don't feel strongly about the name, if someone has better suggestion than my ""original cycle"", let's call it something else!",NULL
DM-5754,"Fix install documentation","NOTE: This ticket will be done by Oualid Achbal, trainee at LPC/IN2P3, but will be merged with [~jammes] account for convenience.    Oualid has identified several error in install documentation (quick start and quick-start for developpers). He will fix it here.",NULL
DM-5755,"Please create new team for non-project contributors","Per RFC-145, T/CAMs are supposed to assign tickets to appropriate teams and, indeed, we all get mail to complain when this hasn't been done.    Some tickets do not map onto the existing team structure. Usually, that's because they're being carried out by individuals who do not work for the project; occasionally, they are also using JIRA to track work which isn't a project deliverable. A good example of both is DM-5724.    Please create an appropriate team (""Externals"", maybe) for these stories.",NULL
DM-5758,"processEimage.py raises various errors in .calibrate.measureApCorr","Running on phosim data for Twinkles, we're seeing ZeroDivisionErrors and InvalidParameterErrors for ~1% of the eimage data.    I've placed example data in /nfs/home/jchiang/Twinkles at lsst-dev.    Here is the command and traceback for the ZeroDivisionError:  {code}  [obs_lsstSim b1953] processEimage.py input --id visit=1697655 --output output --doraise --clobber-config    < skipped output>    processEimage.calibrate.measureApCorr: Measuring aperture corrections for 2 flux fields  processEimage.calibrate.measureApCorr: Aperture correction for base_PsfFlux: RMS 0.063219 from 143  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/obs_lsstSim/2016_01.0-2-ga2b3e68+2/bin/processEimage.py"", line 25, in <module>      ProcessEimageTask.parseAndRun()    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/cmdLineTask.py"", line 444, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/cmdLineTask.py"", line 199, in run      resultList = mapFunc(self, targetList)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/cmdLineTask.py"", line 318, in __call__      result = task.run(dataRef, **kwargs)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/obs_lsstSim/2016_01.0-2-ga2b3e68+2/python/lsst/obs/lsstSim/processEimage.py"", line 123, in run      result = ProcessCcdTask.run(self, sensorRef)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-11-g81670f4+2/python/lsst/pipe/tasks/processCcd.py"", line 157, in run      calib = self.calibrate.run(postIsrExposure, idFactory=idFactory, expId=expId)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-11-g81670f4+2/python/lsst/pipe/tasks/calibrate.py"", line 552, in run      apCorrMap = self.measureApCorr.run(bbox=exposure.getBBox(), catalog=sources).apCorrMap    File ""/home/lsstsw/stack/Linux64/meas_base/2016_01.0-9-g0a48b21+5/python/lsst/meas/base/measureApCorr.py"", line 198, in run      apCorrData[n] = record.get(self.refFluxKeys.flux)/record.get(keys.flux)  ZeroDivisionError: float division by zero  [obs_lsstSim b1953]   {code}    Here is the same for the InvalidParameterError:  {code}  [obs_lsstSim b1953] processEimage.py input --id visit=712850 --output output --doraise --clobber-config    <skip output>    processEimage.calibrate.measureApCorr: Aperture correction for base_PsfFlux: RMS 0.033659 from 154  /home/lsstsw/stack/Linux64/meas_base/2016_01.0-9-g0a48b21+5/python/lsst/meas/base/measureApCorr.py:213: RuntimeWarning: invalid value encountered in less_equal    keep = numpy.fabs(apCorrDiffs) <= apCorrDiffLim  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/obs_lsstSim/2016_01.0-2-ga2b3e68+2/bin/processEimage.py"", line 25, in <module>      ProcessEimageTask.parseAndRun()    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/cmdLineTask.py"", line 444, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/cmdLineTask.py"", line 199, in run      resultList = mapFunc(self, targetList)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/cmdLineTask.py"", line 318, in __call__      result = task.run(dataRef, **kwargs)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/obs_lsstSim/2016_01.0-2-ga2b3e68+2/python/lsst/obs/lsstSim/processEimage.py"", line 123, in run      result = ProcessCcdTask.run(self, sensorRef)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-11-g81670f4+2/python/lsst/pipe/tasks/processCcd.py"", line 157, in run      calib = self.calibrate.run(postIsrExposure, idFactory=idFactory, expId=expId)    File ""/home/lsstsw/stack/Linux64/pipe_base/2016_01.0-2-g1dce45a+2/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-11-g81670f4+2/python/lsst/pipe/tasks/calibrate.py"", line 552, in run      apCorrMap = self.measureApCorr.run(bbox=exposure.getBBox(), catalog=sources).apCorrMap    File ""/home/lsstsw/stack/Linux64/meas_base/2016_01.0-9-g0a48b21+5/python/lsst/meas/base/measureApCorr.py"", line 203, in run      apCorrField = ChebyshevBoundedField.fit(bbox, x, y, apCorrData, ctrl)    File ""/home/lsstsw/stack/Linux64/afw/2.2016.10-1-g690005f+1/python/lsst/afw/math/mathLib.py"", line 9089, in fit      return _mathLib.ChebyshevBoundedField_fit(*args)  lsst.pex.exceptions.wrappers.InvalidParameterError:     File ""src/math/LeastSquares.cc"", line 434, in void lsst::afw::math::LeastSquares::_factor(bool)      Number of columns of design matrix (6) must be smaller than number of data points (0) {0}  lsst::pex::exceptions::InvalidParameterError: 'Number of columns of design matrix (6) must be smaller than number of data points (0)'    [obs_lsstSim b1953]   {code}    We see these errors for tags b1936 through b1953.  Later tags fail immediately with  {code}  AttributeError: 'CalibrateConfig' object has no attribute 'repair'  {code}",NULL
DM-576,"Additional fault tolerance for AP Simulator - v6","Additional steps for fault tolerance  * Build standby Base DMCS * Build standby Archive DMCS",NULL
DM-577,"Build standby Base DMCS","If a connection failure to the OCS occurs (detected via a heartbeat/lease), the machine is powered down and a spare machine is enabled with the same IP address, being sure to arp_announce when the system is brought up to clear the arp table entries of the previous machine on machines on the subnet.",NULL
DM-5772,"Comments on LSE-68","Here are some comments on LSE-68.   These are likely my misunderstanding of what's being said, but I think some of this might need to be clarified.    Comments on LSE-68:  —  1 Software Delivery - ID: CM-DM-DAQ-ICD-0075  “… in this document shall be delivered as one or more libraries that can be linked into a C++ application, compiled against the library headers, with its main program provided by the user.  The libraries will be supplied as pre-compiled shareable in Unix “.so” format.”     “The source code for the client libraries will be maintained using common LSST source control tools”.    4.4 Crosstalk-correction algorithm portability - ID: CM-DM-DAQ-ICD-0056  “The Camera shall provide a reference implementation for Unix in C++ of the actual algorithm used in the Camera data acquisition system.”    Comments:   I presume these are all mutually exclusive.  It’s our understanding that as far as DM access to the camera buffer goes, we will never have source code delivered to us, and that we will only ever get versioned, shared libraries and library headers.  Just so I’m understanding, these are saying: 1) Shared libraries with headers will be delivered; 2) Camera will maintain it’s own repository for the source code; 3) Camera will provide DM with only the source code for the crosstalk-correction algorithm.    —  2.1 Image Identification - ID: CA-DM-DAQ-ICD-0059    Comment - From reading this, it appears that the Image ID is assigned to amps.    3.10 Selection of region of focal plane to be retrieved - ID: CA-DM-DAQ-ICD-0091    Comment - From reading this, the largest unit we expect to be able retrieve is a raft.    It would be good to spell out exactly what subregions we expect to be able to request from the Camera, and what “format” those will be delivered in.  In other words, do we expect to get a raft back as one continuous buffer, or do we expect the raft to be delivered as a sequence of CCDs that need be reassembled in order to process it?   If the image is being handed back as one raft, and the convention is that the Image ID is assigned to individual amps, what will the Image ID for a raft be?  Does it even have an Image ID?       Further, I don’t recall seeing any information anywhere of how a visitID and exposure number for the entire camera image are mapped to an image ID (or whatever) so the correct raft image can be retrieved individually.   Will the camera buffer software even understand the concept of a visitID and exposure number?    I guess what I’m asking here is that if DDS gives me a “startReadout” event for visitID “XYZZY”, exposure 0, how will we expect to get raft 4 of that exposure if the image ID hasn’t even been generated yet?  …And what happens if the image ID not assigned per raft, but only per-amp?    —  2.2 Structure metadata- ID: CA-DM-DAQ-ICD-0080    Perhaps a discussion in this section to clarify this a bit more?  —  3. “Interface for Buffered Data (“pull” interface) - ID: CA-DM-DAQ-ICD-0047    This section is a bit unclear to me.   It’s my understanding that the only notification DM gets from the OCS is that an image is starting to be dealt with at the camera when we get a “startReadout” event.  Some time later, the buffer will be ready to be retrieved, and we don’t get any notification about that.   It was my assumption that we’d either make a request to pull the data from the camera buffer and block, or that we’d be notified of the data within our own code through a call to the library which triggers a callback.  Which is it?    Also, what decides to discard the oldest available data?  ",NULL
DM-5774,"Provide more useful error message from stored config read failure","Sometimes in the course of development we change a {{Config}} class from what has been stored in a data repo.  Subsequent runs then fail to read the {{Config}} file and produce a perplexing error message.  Here's an example:  {code}  reduceBias.py '/Users/azuri/spectra/pfs/PFS' --calib '/Users/azuri/spectra/pfs/PFS/CALIB' --output '/Users/azuri/spectra/pfs/PFS/CALIB/combined' --calibId calibVersion=bias arm=m --do-exec --id field=BIAS dateObs=2015-12-22 spectrograph=2 site=S category=A filter=m --nodes=1 --procs=1  : Config override file does not exist: '/Users/azuri/stella-git/lsst/obs_subaru/config/bias.py'  : Config override file does not exist: '/Users/azuri/stella-git/lsst/obs_subaru/config/pfs/bias.py'  : input=/Users/azuri/spectra/pfs/PFS  : calib=/Users/azuri/spectra/pfs/PFS/CALIB  : output=/Users/azuri/spectra/pfs/PFS/CALIB/combined  CameraMapper: Loading registry registry from /Users/azuri/spectra/pfs/PFS/CALIB/combined/_parent/registry.sqlite3  CameraMapper: Unable to locate calibRegistry registry in root: /Users/azuri/spectra/pfs/PFS/CALIB/calibRegistry.sqlite3  CameraMapper: Unable to locate calibRegistry registry in current dir: ./calibRegistry.sqlite3  CameraMapper: Loading Posix registry from /Users/azuri/spectra/pfs/PFS/CALIB  type(config) =  <class 'lsst.afw.cameraGeom.cameraConfig.CameraConfig'>  CameraMapper: Loading registry registry from /Users/azuri/spectra/pfs/PFS/CALIB/combined/_parent/registry.sqlite3  CameraMapper: Unable to locate calibRegistry registry in root: /Users/azuri/spectra/pfs/PFS/CALIB/calibRegistry.sqlite3  CameraMapper: Unable to locate calibRegistry registry in current dir: ./calibRegistry.sqlite3  CameraMapper: Loading Posix registry from /Users/azuri/spectra/pfs/PFS/CALIB  type(config) =  <class 'lsst.afw.cameraGeom.cameraConfig.CameraConfig'>  bias FATAL: Failed in task initialization: No module named detrends  Traceback (most recent call last):    File ""/Users/azuri/lsstsw/stack/DarwinX86/pipe_base/2016_01.0-6-g7751869+1/python/lsst/pipe/base/cmdLineTask.py"", line 285, in precall      task.writeConfig(parsedCmd.butler, clobber=self.clobberConfig, doBackup=self.doBackup)    File ""/Users/azuri/lsstsw/stack/DarwinX86/pipe_base/2016_01.0-6-g7751869+1/python/lsst/pipe/base/cmdLineTask.py"", line 494, in writeConfig      oldConfig = butler.get(configName, immediate=True)    File ""/Users/azuri/lsstsw/stack/DarwinX86/daf_persistence/2016_01.0-8-geb625e2/python/lsst/daf/persistence/butler.py"", line 370, in get      return callback()    File ""/Users/azuri/lsstsw/stack/DarwinX86/daf_persistence/2016_01.0-8-geb625e2/python/lsst/daf/persistence/butler.py"", line 365, in <lambda>      callback = lambda: self._read(location)    File ""/Users/azuri/lsstsw/stack/DarwinX86/daf_persistence/2016_01.0-8-geb625e2/python/lsst/daf/persistence/butler.py"", line 455, in _read      results = location.repository.read(location)    File ""/Users/azuri/lsstsw/stack/DarwinX86/daf_persistence/2016_01.0-8-geb625e2/python/lsst/daf/persistence/repository.py"", line 346, in read      return self._access.read(butlerLocation)    File ""/Users/azuri/lsstsw/stack/DarwinX86/daf_persistence/2016_01.0-8-geb625e2/python/lsst/daf/persistence/access.py"", line 125, in read      return self.storage.read(butlerLocation=butlerLocation)    File ""/Users/azuri/lsstsw/stack/DarwinX86/daf_persistence/2016_01.0-8-geb625e2/python/lsst/daf/persistence/posixStorage.py"", line 229, in read      importType = __import__(importPackage, globals(), locals(), [importClassString], -1)  ImportError: No module named detrends  usage: reduceBias.py input [options]  reduceBias.py: error: Error in task preparation  {code}    In this case, it's because the {{detrends}} module has been renamed, but it's not obvious. We need a more helpful error message.",NULL
DM-5776,"Bad logic in data unpersisting in calibrate.py","The logic here:  https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/calibrate.py#L304  {code}          if doUnpersist:              if None in (exposure, background, icSourceCat):                  raise RuntimeError(""doUnpersist true; exposure, background and icSourceCat ""                      ""must all be None"")              exposure = dataRef.get(""icExp"", immediate=True)              background = dataRef.get(""icExpBackground"", immediate=True)              icSourceCat = dataRef.get(""icSourceCat"", immediate=True)          elif exposure is None:              raise RuntimeError(""doUnpersist false; exposure must be provided"")  {code}  does not corroborate the documentation there or here:  https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/calibrate.py#L290  {code}          @param[in] doUnpersist  unpersist data:              - if True, exposure, background and icSourceCat are read from dataRef and those three arguments                  must all be None;              - if False the exposure must be provided; background and icSourceCat are optional.              True is intended for running as a command-line task, False for running as a subtask  {code}    If the documentation is correct, then the condition should be checking wether any of {{exposure, background, icSourceCat}} is NOT None.",NULL
DM-5777,"Base Measurement task contains reference to variable that does not exist","{{BaseMeasurementTask}} contains a reference to {{self.config.plugins}} however {{BaseMeasurementConfig}} does not contain a variable called {{plugins}}. This does not get created until a child class is created. This currently does not create any issues as the proper variable is defined in all child classes. However a default plugin variable should be created in the config both for readability in the code, and so that if future children do not create the variable for some reason things will not break unexpectedly.",NULL
DM-5779,"Test max/min versions of ""important"" system packages in CI","In discussion on HipChat, we realised that we have effectively no idea what the minimum version of NumPy required to run the stack is. Our EUPS package [implies version 1.5.1 is adequate|https://github.com/lsst/numpy/blob/master/ups/eupspkg.cfg.sh#L2], but nobody seems to believe it; our [list of third party software|https://confluence.lsstcorp.org/display/DM/DM+Third+Party+Software] claims we distribute version 1.8.0 (we don't); your humble author wonders if it's safe to use {{numpy.nanmedian()}} (first included in version 1.9.0).    It would be great if the CI system ran not just with standard CentOS (or other; DM-3128) packages, but if it actually tracked versions of major system dependencies (NumPy, SciPy, Matplotlib, maybe others) that we (claim to) support, and tells us when we didn't. Rather than trying to support an N-dimensional matrix of versions of all our dependencies, [~pschella] points out that simply building with min-of-everything and then max-of-everything would be pretty awesome.",NULL
DM-578,"Build standby Archive DMCS","If a connection failure to the OCS occurs (detected via a heartbeat/lease), the machine is powered down and a spare machine is enabled with the same IP address, being sure to arp_announce when the system is brought up to clear the arp table entries of the previous machine on machines on the subnet.",NULL
DM-5783,"Please support adding builds of git branches to the lsst-dev shared stack","The {{lsst-dev}} shared stack provided in DM-5435 breaks the link between Buildbot and the shared stack. This means it's no longer possible to request a build of a particular branch from the CI system and have that declared into the shared stack for testing.    That's a useful feature: please reinstate it.",NULL
DM-5785,"showVisitSkyMap.py broken by Butler changes","Thus:  {code}  Traceback (most recent call last):    File ""./showVisitSkyMap.py"", line 135, in <module>      ccdKey=args.ccdKey, showPatch=args.showPatch, saveFile=args.saveFile)    File ""./showVisitSkyMap.py"", line 53, in main      mapper = butler.mapper  AttributeError: 'Butler' object has no attribute 'mapper'  {code}",NULL
DM-5786,"CCD ID and CCD name mixed up","I think there is a bug in  {{/Users/azuri/lsstsw/stack/DarwinX86/daf_butlerUtils/2016_01.0-4-g325729a+1/python/lsst/daf/butlerUtils/cameraMapper.py(858)_defectLookup()}}:  {code}  ccdKey, ccdVal = self._getCcdKeyVal(dataId)  {code}  which assigns the CCD name (in my case {{'r_2'}}) to {{ccdVal}}, while {{obs_subaru/bin.src/genDefectFits.py}} and {{obs_subaru/bin.src/genDefectRegistry.py}} use the ccd ID (in my case {{'5'}}) or serial number ({{5}}) to create the fits file containing the defects for each CCD. For HSC, the CCD name, ID, and serial are all the same, but it's causing trouble for PFS...",NULL
DM-5788,"Qserv OS X Xcode 7.3 compiler issue","Not entirely sure why this is triggering as I had built Qserv on my mac last week but I'm now getting a compiler error from a test:  {code}  Compiling static object build/util/testIterableFormatter.o  core/modules/util/testIterableFormatter.cc:85:32: error: implicit instantiation of undefined template 'std::__1::array<std::__1::basic_string<char>, 6>'      std::array<std::string, 6> iterable { {""1"", ""2"", ""3"", ""4"", ""5"", ""6""} };                                 ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/__tuple:116:65: note: template is declared here  template <class _Tp, size_t _Size> struct _LIBCPP_TYPE_VIS_ONLY array;                                                                  ^  1 error generated.  {code}  The fix is trivial: include {{array.h}}.",NULL
DM-5789,"Functionality requests that only impact HSC","This epic tracks feature requests and enhancements that are required for HSC. It should not be scheduled in a particular LSST cycle, since it earns LSST no value.",NULL
DM-579,"Build OCS Interface - v7","Build the OCS Interface  Send all OCS commands Respond to OCS commands",NULL
DM-5790,"Provide unified plotting abstraction layer","The stack has dozens of ""import matplotlib"" statements scattered around. This can cause unexpected behavior, slowdowns and can mess up subsequent matplot commands (e.g. if a implicit import prevents a subsequent explicit import from setting the backend).     We provide an afw.display abstraction for doing ds9-related things, so we should also have an afw.plot or similar abstraction for plotting. One could then swap in Seaborn or another plotting system more easily, and it would reduce the ""import matplotlib"" to one place, which would only occur when e.g. that class is instantiated.    A first step of this would be a simple class that just handles the import and environment setup, while not actually abstracting away any of the matplotlib calls. We could then make it a more general abstraction layer if desired.",NULL
DM-5796,"Document math support in reStructuredText Style Guide","Document the math role and directive available in Sphinx in the ReStructuredText Style Guide section of the DM Developer Guide.",NULL
DM-58,"package zookeeper, kazoo and db","Fabrice,  The new CSS needs Zookeeper (including C bindings), Kazoo and the db package.  The steps I did to get Zookeeper and Kazoo to work are described in: https://dev.lsstcorp.org/trac/wiki/db/Qserv/ZookeeperNotes  Can you package the zookeeper, kazoo, and tweak the db package as needed?  I have a working prototype, where  I installed these components by hand. If you want to test it, the code is in branch called u/jbecla/cssProto4, Once you are done with the building/installing things let me know, and I'll help you test it. ",NULL
DM-580,"Send OCS Commands","Send commands to the Archiver and the Alert Production Cluster entities on the Base DMCS.  Commands are:  * init * configure (only one configuration, hard-coded). * enable * disable * release * stop * abort * reset",NULL
DM-5801,"Audit and update SdssShape","The {{SdssShape}} contains a number of poorly-motivated numerical constants (""magic numbers"", e.g. [here|https://github.com/lsst/meas_base/blame/master/src/SdssShape.cc#L241]) and inconsistent sanity checks (compare [here| https://github.com/lsst/meas_base/blob/master/src/SdssShape.cc#L96] and [here| https://github.com/lsst/meas_base/blob/master/src/SdssShape.cc#L440]).    Please audit the code for these issues. In particular:    * Check for numerical constants and ensure their values are appropriate and properly justified.  * Ensure that checks on the shape determinant are consistent, including both those listed above and those added in DM-3935.",NULL
DM-5807,"AWS user does not have full control.","See: https://github.com/lsst-sqre/ltd-keeper/blob/master/app/route53.py#L157    This same code works with my own AWS account and credentials - using one of my own domains.    My AWS account has been in a weird state since it was created. I also don't have permissions all over the place in the management console and can't use IAM.",NULL
DM-5809,"Rename JIRA component 'SUI' to 'SUIT'","To ensure consistency with the evolving use of ""SUIT"" in place of ""SUI/T"" and ""SUI"", the name of the corresponding JIRA component should be edited.",NULL
DM-581,"Archiver entity responds to OCS commands","Archiver entity should respond and act appropriately to all commands:  * init * configure (only one configuration, hard-coded) * enable * disable * release * stop * abort * reset",NULL
DM-5811,"Combine 2D x-y and density plots with 1D histograms of the two variables","In refining the new histogram component (already looks good) and how it is packaged within the Firefly environment, it would be good to bear in mind the value of being able to stick 1D histograms onto the sides of a 2D x-y plot (scatterplot) or density plot, showing the projections of the two individual variables.    This requires the 2D and 1D plots to be able to share common sets of axes, and of course it requires one of the two 1D plots to be rotated a quarter turn.",NULL
DM-5812,"Allow for visualization of filtering ranges on 1D and 2D plots","It would be nice to have the ability to select a tool on a 1D (e.g., histogram) or 2D (e.g., x-y) plot that would allow the visualization and graphical setting of a selection region for filtering on the associated variable.    One possible realization would be as a pair of draggable vertical (or horizontal, for the y axis in a 2D plot) lines superposed on the plot.  Small triangles / arrows attached to the lines could indicate the direction of the selection and allow ""exclusive"" selections (x not between 7 and 9) to be visualized.",NULL
DM-5813,"Display of effects of filters in histograms","When a histogram is displaying the results of a filter or filters applied on variables other than the x-axis variable of the histogram, it would be very useful to be able to (optionally?) see the histogram both _before_ and _after_ the application of the filter.  (That is, to see the two histograms over-plotted.)    When doing so, there must be choices available for the vertical scaling - at a minimum, a choice between displaying both on the same scale, thus showing quantitatively the data that have been removed as well (as the space between the two histograms), or autoscaling both independently (more useful when most of the data have been removed by the filter, making the post-filter histogram uselessly shallow when displayed at the same scale).",NULL
DM-5814,"Memory corruption involving ReferenceMatchVector","I can't be very specific, unfortunately, but I'm seeing a very sneaky segfault on my El Cap machine.  In order to repeat, on my machine I run:  {{$> processCcd.py input --id run=6377 filter=u rerun=40 camcol=4 field=399 --config calibrate.detectAndMeasure.doDeblend=False --output output}}  using master of the lsst stack demo.  In my case, it segfaults in photoCal.    The best I can tell, it is caused by the ReferenceMatchVector being garbage collected prematurely, but I haven't been able to track it down further.",NULL
DM-5815,"Collect use-cases for multi-plots","There was a brief discussion today of the desirability of supporting multiple-pane displays of (small) plots (i.e., x-y plots and histograms) in the same way that we do for images, allowing the creation of grids of plots, as well as the use of some of the other concepts we have for multi-image viewing such as blinking and manual or auto-play stepping through a series of plots.    There's no doubt that this would be useful!    However, we have to think through what the use cases for this are, because this will affect the way that we allow users to create such displays and how we label them.    Converting this ticket to a story would be a request that we collect a set of use cases.",NULL
DM-5816,"Investigate behavior of Firefly and stretches for images with negative pixel values","Based on a discussion in the Tea Time HipChat room today, this is a ""note to ourselves"" to take a look at the behavior of Firefly when visualizing images with negative flux values.    This is important for difference imaging and is therefore highly relevant to both LSST and ZTF (if ZTF difference images become visible through Firefly at some point).    The behavior of the asinh stretch, in particular, should be looked at.",NULL
DM-5817,"LTD Keeper: refactor REST routes; adopt JSON API conventions","Right now resources are available on      {code}  /products/  /editions/  /builds/  {code}    routes, with editions and builds being known by their DB id number. It would be better to refer to these by their slug. To ensure uniqueness, this implies that builds and editions are sub-resources of products. I.e.    {code}  /products/<prod_slug>  /products/<prod_slug>/builds/<build_slug>  /products/<prod_slug>/editions/<edition_slug>  {code}    This ticket will complete this refactoring.",NULL
DM-5818,"LTD Keeper: refactor LTD site installation data","There is some data shared by all/many products:    - bucket name  - Fastly root domain  - LTD root domain  - Fastly service ID    These should be maintained in a Service DB model, perhaps exposed through a /services/ route. Each product would reference a row in the services table. The deployment initialization step can create the default services table row.",NULL
DM-582,"Alert Production Cluster entity responds to OCS Commands","Alert production entity should respond and act appropriately to all commands:  * init * configure (only one configuration, hard-coded) * enable * disable * release * stop * abort * reset",NULL
DM-5824,"Something wrong with InstallGaussianPsfTask with 1.1 > sigma > 1.8","Strange behaviour seen in InstallGaussianPsfTask. When installing larger than default PSFs, the measured size comes back wrong unless the ""size"" (number of pixels used to realise it) is increased. This is as expected, but the amount you can go up from the default is surprisingly small, indicating that perhaps the default size of 11pix is not enough?    The really weird behaviour though, is going to smaller than default PSFs. This is presumably either some kind of either rounding error, or is a quantisation error resulting from small PSFs being undersampled or similar?    I don't know if anyone cares about either of these, but I was told to post it anyway, so here it is.    Minimal (not-)working example below to show the issue:     {code:python}    from lsst.afw.image import ExposureF  from lsst.meas.algorithms.installGaussianPsf import InstallGaussianPsfTask, FwhmPerSigma    exposure = ExposureF(100, 100)  task = InstallGaussianPsfTask()    # run with default size  task.config.fwhm = 1.5 * FwhmPerSigma # default - is fine  # task.config.fwhm = 1.8 * FwhmPerSigma # largest you can set to without assert failing  # task.config.fwhm = 1.9 * FwhmPerSigma # 1.9 and up all fail with default size    # change the size and larger spots are fine  # task.config.width = 55 # setting the size to greater than 11 (the default) allows much larger PSFs  # task.config.fwhm =  8 * FwhmPerSigma # this is fine    # going to smaller PSFs is weird  # task.config.fwhm = 1.1 * FwhmPerSigma # smallest you can go without assert failing  # task.config.fwhm = 1.1 * FwhmPerSigma # this is not OK  # task.config.fwhm = 0.9 * FwhmPerSigma # nor is this    # task.config.width = 55 # changing the size up or down doesn't fix this (as you'd expect)  # task.config.fwhm = 0.9 * FwhmPerSigma #       task.run(exposure=exposure)  measFwhm = exposure.getPsf().computeShape().getDeterminantRadius() * FwhmPerSigma  assert abs(measFwhm - task.config.fwhm) < 1e-3  {code}  ",NULL
DM-5826,"Please include Git commit message guidelines in ""setup and best practices"" section of developer guide","We currently provide some guidelines on [""best practices"" for commit messages|http://developer.lsst.io/en/latest/processes/workflow.html#git-commit-message-best-practices] in the development workflow documentation.    However, we also provide a document on [""Git Setup and Best Practices""|http://developer.lsst.io/en/latest/tools/git_setup.html], which makes no reference to the above guidelines.    It would be helpful to ensure that all our guidelines for git usage are available from the same location.    (It would be even better if we could be clear about what's a requirement and what's a suggestion...)",NULL
DM-5828,"add an exposure astrometric match to jointcal","Some input calexp's have too poor a WCS and only a fraction of their sources  get matched to other exposures. The proposed cure is to allow for a combinatorial match  at the exposure level to the external catalog prior to matching sources from the different  calexps.",NULL
DM-5838,"3-color image label change","When generating 3-color image, the label for the image display should 'Project 3-color', i.e.   WISE 3-color, 2MASS 3-color ...  ",NULL
DM-5842,"Clearly state what Git commit message and organization workflows are requirements/recommendations","The Developer Guide’s [Workflow|http://developer.lsst.io/en/latest/processes/workflow.html] document includes appendices on how best to use Git: including advice on how best to organize commits and how to structure commit messages.    In DM-5826 [~swinbank] pointed how that these appendices can be misleading since they suggest that some advice are requirements, when in fact they may not be:    {quote}  At the moment, we simply have a statement that ""we follow"" some particular set of guidelines. My concern is simply that we don't, or at least we're not required to. Softening the language to ""we suggest"", or similar, instead would resolve the issue as far as I'm concerned.  {quote}    This ticket will    1. Determine what parts of those appendices may be requirements, if any.  2. Change the language to intentionally differentiate between suggestions and requirements (“we follow” or “we require"")",NULL
DM-5843,"Refactor afw C++ tests to use Boost.Test","Some afw C++ unit-test suites (notably the ones in {{tests/ramFitsIO.cc}}, {{tests/maskedImage1.cc}}, {{tests/convolveGPU.cc}}, & {{tests/testWarpGpu.cc}}) do not use the Boost.Test framework. This makes it harder to do useful things such as run individual tests that don't require afwdata when afwdata is not setup and enable these tests when afwdata is setup.    Please ensure that all afw C++ use Boost.Test. Use the {{utf::enabled()}} and {{utf::disabled()}} decorators to conditionally run tests dependent on whether afwdata is setup. See the DM-609 changes to  {{tests/statistics.cc}} and {{tests/background.cc}} for examples of how to use the relevant decorators.  ",NULL
DM-5846,"Migrate to Mailchimp from Mandrill for community.lsst.org","Mandrill is now part of Mailchimp. This story is to covert out Mandrill account to a Mailchimp account for continued service.    For reference, see https://mandrill.zendesk.com/hc/en-us/articles/217467117",NULL
DM-5855,"Accessing invalid attributes produces unhelpful error message","While trying to introspect afw.table, I tried ""table.deepcopy()"" and got a ""Record data is not contiguous in memory."" error. This was not helpful.    In fact, table.ANYTHING (parentheses or not) gives the same error. Looking in tableLib \_\_getattribute\_\_, there should maybe be an except clause for the getattr(columns), to properly deal with this case and produce a more helpful error message?    {code}  In [164]: xx.deepcopy()  ---------------------------------------------------------------------------  RuntimeError                              Traceback (most recent call last)  <ipython-input-164-9d4dd7ea8bd1> in <module>()  ----> 1 xx.deepcopy()    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-9-gc64e176/python/lsst/afw/table/tableLib.pyc in __getattribute__(self, name)     8142             return getattr(self.table, name)     8143         except AttributeError:  -> 8144             return getattr(self.columns, name)     8145     table = property(getTable)     8146     schema = property(getSchema)    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-9-gc64e176/python/lsst/afw/table/tableLib.pyc in __getattribute__(self, name)     8132         # We have to use __getattribute__ because SWIG overrides __getattr__.     8133         try:  -> 8134             return object.__getattribute__(self, name)     8135         except AttributeError:     8136             # self._columns is created the when self.columns is accessed -    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-9-gc64e176/python/lsst/afw/table/tableLib.pyc in __getColumns(self)     8097     def __getColumns(self):     8098         if not hasattr(self, ""_columns"") or self._columns is None:  -> 8099             self._columns = self.getColumnView()     8100         return self._columns     8101     columns = property(__getColumns, doc=""a column view of the catalog"")    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-9-gc64e176/python/lsst/afw/table/tableLib.pyc in getColumnView(self)     7970     def getColumnView(self):     7971         """"""getColumnView(_SimpleCatalogBase self) -> SimpleColumnView""""""  -> 7972         val = _tableLib._SimpleCatalogBase_getColumnView(self)     7973         self._columns = val     7974    RuntimeError:    File ""include/lsst/afw/table/BaseColumnView.h"", line 205, in static lsst::afw::table::BaseColumnView lsst::afw::table::BaseColumnView::make(const boost::shared_ptr<BaseTable> &, InputIterator, InputIterator) [InputIterator = lsst::afw::table::CatalogIterator<std::__1::__wrap_iter<const boost::shared_ptr<lsst::afw::table::SimpleRecord> *> >]      Record data is not contiguous in memory. {0}  lsst::pex::exceptions::RuntimeError: 'Record data is not contiguous in memory.'  {code}",NULL
DM-5856,"in place add operator doesn't work for Angles in a catalog","I would expect to be able to in-place add an ra/dec angle column in a table with an afw.geom.Angle object, but it throws a TypeError. See the examples below.    {code:python}  In [57]: error = geom.Angle(1e-2, geom.arcseconds)    In [58]: error  Out[58]: 4.84814e-08 rad    In [59]: cat['coord_ra'].dtype  Out[59]: dtype('float64')    In [60]: cat.schema  Out[60]:  Schema(      (Field['L'](name=""id"", doc=""unique ID""), Key<L>(offset=0, nElements=1)),      (Field['Angle'](name=""coord_ra"", doc=""position in ra/dec""), Key<Angle>(offset=8, nElements=1)),      (Field['Angle'](name=""coord_dec"", doc=""position in ra/dec""), Key<Angle>(offset=16, nElements=1)),  ...    In [61]: cat['coord_ra'] += error  ---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)  <ipython-input-59-2a953ac7fa0d> in <module>()  ----> 1 cat['coord_ra'] += error    TypeError: ufunc 'add' output (typecode 'O') could not be coerced to provided output parameter (typecode 'd') according to the casting rule ''same_kind''  {code}    Regular add also fails.    {code:python}  In [62]: cat['coord_ra'] = cat['coord_ra'] + error  ---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)  <ipython-input-65-6ca5b16b9f9e> in <module>()  ----> 1 cat['coord_ra'] = cat['coord_ra'] + error    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-9-gc64e176/python/lsst/afw/geom/geomLib.pyc in __radd__(self, lhs)     3663         return Angle_add(self, rhs)     3664     def __radd__(self, lhs):  -> 3665         return Angle_add(lhs, self)     3666     def __sub__(self, rhs):     3667         return Angle_sub(self, rhs)    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-9-gc64e176/python/lsst/afw/geom/geomLib.pyc in Angle_add(*args)     3704 def Angle_add(*args):     3705   """"""Angle_add(Angle a, Angle d) -> Angle""""""  -> 3706   return _geomLib.Angle_add(*args)     3707     3708 def Angle_sub(*args):    TypeError: in method 'Angle_add', argument 1 of type 'lsst::afw::geom::Angle const'  {code}    Element-wise math works ok for regular add,  but not in place:    {code:python}  In [67]: cat[0]['coord_ra'] = cat[0]['coord_ra'] + error    In [68]: cat[0]['coord_ra'] += error  ---------------------------------------------------------------------------  NotImplementedError                       Traceback (most recent call last)  <ipython-input-68-21c4426ca5c7> in <module>()  ----> 1 cat[0]['coord_ra'] += error    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-9-gc64e176/python/lsst/afw/geom/geomLib.pyc in __iadd__(self, *args)     3615         __iadd__(Angle self, int const & d) -> Angle     3616         """"""  -> 3617         return _geomLib.Angle___iadd__(self, *args)     3618     3619     def __isub__(self, *args):    NotImplementedError: Wrong number or type of arguments for overloaded function 'Angle___iadd__'.    Possible C/C++ prototypes are:      lsst::afw::geom::Angle::operator +=(double const &)      lsst::afw::geom::Angle::operator +=(int const &)  {code}",NULL
DM-5861,"Fix typo in obs_subaru assembleCoadd config","config.doMaksBrightObjects = True should be config.doMaskBrightObjects = True",NULL
DM-5862,"Finalize error handling in DipoleFitTask","DipoleFitTask handles several errors (fitting exceptions, edge exceptions) but it is not clear that it logs the error messages correctly, nor that it handles other rarer errors. After testing the task on real data, some of these additional errors may come to light. All of these issues need to be finalized.",NULL
DM-5865,"Update third party packaging guide","When packaging ngmix, I sought out answers to the following questions:    # Which teams need permissions on it? ""DM Externals"" sounds plausible, but I'm not sure.  # What about the version number? Upstream haven't made a versioned release. I was planning to tag it with git SHA1. Does that work, or does it break assumptions that versions are lexicographically sorted or something like that?    Answers (courtesy [~frossie] and [~tjenness]) were:    # Yes, DM Externals (& presumably also Overlords).  # SHA1 is fine, but version numbers shouldn't start with a letter or EUPS gets confused.    Please add these helpful answers to the developer docs.",NULL
DM-5867,"ngmix is a TaP package","Adding apparently innocent {{.gitignore}} and {{README.rst}} files to {{ngmix}} has broken it:  {code}  [2016-04-26T22:01:09.614069eupspkg.prep (fatal): files found in root directory; guessing this is not a TaP package.  Z] + export EUPS_PATH=/Users/jds/Projects/Astronomy/LSST/lsstsw/stack  {code}  Please fix it.",NULL
DM-5871,"RuntimeError : dictionary changed size during iteration","The following kind of error is sometimes observed in Jenkins:    {code}  [2016-04-27T00:44:00.573318Z] scons: *** [.scons/sfm-903346-6] RuntimeError : dictionary changed size during iteration  [2016-04-27T00:44:00.573449Z] Traceback (most recent call last):  [2016-04-27T00:44:00.573500Z]   File ""/home/build0/lsstsw/stack/Linux64/scons/2.3.5/lib/scons/SCons/Action.py"", line 1063, in execute  [2016-04-27T00:44:00.573582Z]     result = self.execfunction(target=target, source=rsources, env=env)  [2016-04-27T00:44:00.573626Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 136, in scons  [2016-04-27T00:44:00.573766Z]     return self.run(*args, **kwargs)  [2016-04-27T00:44:00.573810Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 117, in run  [2016-04-27T00:44:00.573829Z]     self.validateDataset(dataId, ds)  [2016-04-27T00:44:00.573873Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 90, in validateDataset  [2016-04-27T00:44:00.573988Z]     self.assertTrue(""%s readable (%s)"" % (dataset, data.__class__), data is not None)  [2016-04-27T00:44:00.574054Z]   File ""/home/build0/lsstsw/stack/Linux64/daf_persistence/2016_01.0-8-geb625e2+1/python/lsst/daf/persistence/readProxy.py"", line 41, in __getattribute__  [2016-04-27T00:44:00.574075Z]     subject = oga(self, '__subject__')  [2016-04-27T00:44:00.574153Z]   File ""/home/build0/lsstsw/stack/Linux64/daf_persistence/2016_01.0-8-geb625e2+1/python/lsst/daf/persistence/readProxy.py"", line 136, in __subject__  [2016-04-27T00:44:00.574252Z]     set_cache(self, get_callback(self)())  [2016-04-27T00:44:00.574316Z]   File ""/home/build0/lsstsw/stack/Linux64/daf_persistence/2016_01.0-8-geb625e2+1/python/lsst/daf/persistence/butler.py"", line 365, in <lambda>  [2016-04-27T00:44:00.574397Z]     callback = lambda: self._read(location)  [2016-04-27T00:44:00.574458Z]   File ""/home/build0/lsstsw/stack/Linux64/daf_persistence/2016_01.0-8-geb625e2+1/python/lsst/daf/persistence/butler.py"", line 455, in _read  [2016-04-27T00:44:00.574513Z]     results = location.repository.read(location)  [2016-04-27T00:44:00.574573Z]   File ""/home/build0/lsstsw/stack/Linux64/daf_persistence/2016_01.0-8-geb625e2+1/python/lsst/daf/persistence/repository.py"", line 346, in read  [2016-04-27T00:44:00.574720Z]     return self._access.read(butlerLocation)  [2016-04-27T00:44:00.574782Z]   File ""/home/build0/lsstsw/stack/Linux64/daf_persistence/2016_01.0-8-geb625e2+1/python/lsst/daf/persistence/access.py"", line 125, in read  [2016-04-27T00:44:00.574810Z]     return self.storage.read(butlerLocation=butlerLocation)  [2016-04-27T00:44:00.574870Z]   File ""/home/build0/lsstsw/stack/Linux64/daf_persistence/2016_01.0-8-geb625e2+1/python/lsst/daf/persistence/posixStorage.py"", line 258, in read  [2016-04-27T00:44:00.574886Z]     finalItem = pythonType()  [2016-04-27T00:44:00.574941Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0-1-g6fbf654+1/python/lsst/pex/config/config.py"", line 473, in __new__  [2016-04-27T00:44:00.575036Z]     field.__set__(instance, field.default, at=at+[field.source], label=""default"")  [2016-04-27T00:44:00.575160Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0-1-g6fbf654+1/python/lsst/pex/config/configurableField.py"", line 216, in __set__  [2016-04-27T00:44:00.575392Z]     oldValue.update(__at=at, __label=label, **value._storage)  [2016-04-27T00:44:00.575451Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0-1-g6fbf654+1/python/lsst/pex/config/config.py"", line 513, in update  [2016-04-27T00:44:00.575475Z]     field.__set__(self, value, at=at, label=label)  [2016-04-27T00:44:00.575534Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0-1-g6fbf654+1/python/lsst/pex/config/configurableField.py"", line 211, in __set__  [2016-04-27T00:44:00.575562Z]     oldValue.update(__at=at, __label=label, **value._storage)  [2016-04-27T00:44:00.575617Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0-1-g6fbf654+1/python/lsst/pex/config/config.py"", line 513, in update  [2016-04-27T00:44:00.575641Z]     field.__set__(self, value, at=at, label=label)  [2016-04-27T00:44:00.575700Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0-1-g6fbf654+1/python/lsst/pex/config/configurableField.py"", line 211, in __set__  [2016-04-27T00:44:00.575728Z]     oldValue.update(__at=at, __label=label, **value._storage)  [2016-04-27T00:44:00.575781Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0-1-g6fbf654+1/python/lsst/pex/config/config.py"", line 513, in update  [2016-04-27T00:44:00.575805Z]     field.__set__(self, value, at=at, label=label)  [2016-04-27T00:44:00.575864Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0-1-g6fbf654+1/python/lsst/pex/config/configChoiceField.py"", line 330, in __set__  [2016-04-27T00:44:00.575882Z]     for k,v  in value.iteritems():  [2016-04-27T00:44:00.575920Z]   File ""/home/build0/lsstsw/miniconda/lib/python2.7/_abcoll.py"", line 405, in iteritems  [2016-04-27T00:44:00.576530Z]     for key in self:  [2016-04-27T00:44:00.576564Z] RuntimeError: dictionary changed size during iteration  {code}",NULL
DM-5873,"Allow for dark time in addition to exposure time","Some CCD cameras can have delays between when the reset a chip and when they read it, during which time dark current accumulates. [~mfisherlevine] discovered that DECam has header keywords to describe exactly this situation:  {quote}  EXPTIME 10.0  DARK TIME 11.09515  {quote}    We need to handle this;  I'm not sure if the LSST camera will have this issue, but other cameras we process will.  One issue is where the information comes from;  it's presumably going to be in the headers, but before we can use it in the pipelines it needs to be unpacked into an object, and it isn't obvious which one (Calib isn't quite right).",NULL
DM-5876,"RFC process documentation is confusing","The [Developer Guide|http://developer.lsst.io/en/latest/processes/decision_process.html#role-of-the-assignee-in-reaching-consensus] tells us:    {quote}  When consensus is established, and a set of implementation tickets is created, the Assignee is responsible for marking the RFC as Adopted in JIRA.    Once the RFC is adopted, the Assignee should create tickets that implement the implementation plan.  {quote}    This seems to imply that we create one set of implementation tickets before hitting ""adopted"", and another set afterwards. I'm sure that isn't the intention. Please clarify.  ",NULL
DM-5882,"Jenkins hipchat messages back to default","Before we were getting hipchat notifications from Jenkins along the lines of      {code:java}  Completed #4195 branch: 11.0 3.2.0 of lsst_distrib (centos-7) for Frossie FAILURE after 3 min 5 sec (Job) (Console)  {code}      Every message had the originating user on it.     Currently there is only one message referencing the user, and that's only the initiating one.       {code:java}    stack-os-matrix #10648 Build started (Started by user Frossie) (Job) (Console)  Jenkins·11:00    stack-os-matrix/label=centos-6 #10648 Build started (Started by upstream project ""stack-os-matrix"" build number 10,648) (Job) (Console)  Jenkins·11:00    stack-os-matrix/label=centos-7 #10648 Build started (Started by upstream project ""stack-os-matrix"" build number 10,648) (Job) (Console)    Jenkins·11:49    stack-os-matrix/label=centos-7 #10648 Build successful after 49 min (Job) (Console)    {code}    Users are quite reasonably complaining that this significant reduces the usefulness of the CI-chat gateway. By the time the complete message arrives without reference to the user, it is later and it is even the case that the initial message with the originating job ID has scrolled off the screen.    At this point please estimate the effort it would take to fix this. ",NULL
DM-5883,"Include more information in DECam registry","obs_decam originally included 'object' and 'proposal' fields in the registry but these were removed at some point.  Putting them back would allow users to more easily select data.    I also suggest surveying the headers for additional useful fields.",NULL
DM-5887,"lsstswBuild.sh --print-fail flag broken","The {{--print-fail}} flag appears to broken for at least some failure modes.    https://github.com/lsst-sqre/buildbot-scripts/blob/master/lsstswBuild.sh#L145    Eg.    https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/10673//console",NULL
DM-5888,"Update named leadership in Developer Guide pages","Several pages in the Developer Guide either name leadership positions in DM (i.e., Project Engineer, etc) and/or the people who have those role. Recently DM's leadership organization changed but these documents were not updated. The following pages should be updated to reflect both the correct people, and the correct roles (e.g., the Project Engineer roles does not exist).    - http://developer.lsst.io/en/latest/processes/code_of_conduct.html  - http://developer.lsst.io/en/latest/processes/decision_process.html",NULL
DM-589,"HTCondor ClassAds Testing","Test various ClassAds-dependent scenarios in HTCondor to see how it allocates jobs to slots and whether ClassAds related to locally-cached data can be used to lower average (or even peak) filesystem access rates.",NULL
DM-5890,"Configure Fastly Logging to Panopticon","We should ingest Fastly logs (for LSST the Docs) in Panopticon to both assist with operational monitoring and to build intelligence about how the Docs are consumed.    Links:    * https://www.fastly.com/blog/level-your-log-pro-tips-streaming-logs  * https://www.fastly.com/blog/enhancing-formatting-and-analyzing-fastly-logs (custom log format in VCL)  * https://docs.fastly.com/guides/streaming-logs/",NULL
DM-5891,"LTD Keeper Logging into Panopticon","We need to log LTD Keeper to Panopticon in order to understand the service's operations and diagnose issues.    Note: this ticket may need to be refactored into two parts    1. Building the logging adapter from a Kubernetes Pod to Logstash  2. Implementation of logging inside the Keeper codebase.    Related links:    * http://www.structlog.org/en/stable/ for formatting logs in Python  * https://hynek.me/articles/taking-some-pain-out-of-python-logging/  * https://hynek.me/talks/beyond-grep/",NULL
DM-5892,"Configure Backups for the LTD Keeper DB","LTD Keeper's database resides on a Google Compute Engine Persistent disk attached to a Pod. We need to develop automated backups of this DB within a Kubernetes microservices architecture.    A possible solution is a lightweight pod that periodically runs a backup task that copies the DB to S3. A backup retention policy would also need to be considered.",NULL
DM-5895,"Implement lightweight edition and build dashboards for LSST the Docs","If a user browsers {{example.lsst.io/v/}} or {{example.lsst.io/builds/}} we should present a dashboard that dynamically lists available editions and recent builds, respectively.    This could be implemented as a React app that interacts with the public LTD Keeper API. Thanks to Fastly we could serve a single app for all documentation products, and customize the dashboard according to the URL.",NULL
DM-5896,"DS9 region not implemented","This is to collect what Firefly does not implement of DS9 region.",NULL
DM-5899,"Support LSST-produced calibs","obs_decam currently uses a {{%(path)s\[%(ccdnum)d\]}} template for its calibs (bias, dark, flat, fringe).  This is desirable in order to support calibs provided from external sources (e.g., NOAO).  However, our calib construction code does not support such a path ({{path}} cannot be inferred for new data, and we can't write MEFs from multiple processes).  We believe the best path forward is to split the {{DecamMapper}} into two mappers, one supporting calibs from external sources and one supporting calibs we construct ourselves.  The alternative is to copy the external calibs to match a template like {{BIAS/%(date)s/BIAS-%(date)s-%(ccdnum)02d.fits}}, which means breaking apart the MEFs.    I suggest also moving the defects into the obs_decam package because they're required for use, small and we don't have code to regenerate them.",NULL
DM-5902,"Get all useful dataId keys","We usually want the full range of available dataId keys to be available when given a dataId, so much so that this hack, which started in obs_subaru, is slowly finding its way to other mappers:  {code}          # Ensure each dataset type of interest knows about the full range of keys available from the registry          keys = {'field': str,                  'visit': int,                  'filter': str,                  'ccd': int,                  'dateObs': str,                  'taiObs': str,                  'expTime': float,                  'pointing': int,                  }          for name in (""raw"",                       # processCcd outputs                       ""postISRCCD"", ""calexp"", ""postISRCCD"", ""src"", ""icSrc"", ""icMatch"", ""icMatchFull"",                       ""srcMatch"", ""srcMatchFull"",                       # forcedPhot outputs                       ""forced_src"",                       # Warp                       ""coaddTempExp"",                       ):              self.mappings[name].keyDict.update(keys)  {code}    Please make this a standard feature.",NULL
DM-591,"Content of astrometry_net_data 8.0.0.0 is ancient residue","The git repository for astrometry_net_data contains a token representation of the CFHT data and the README guidance on acquiring USNOB data.  Neither of these two datasets, as-is, have had recent-past use.   Option 1: This Issue is to resolve whether they should be carried indefinitely in the git repository as they now stand; or  Option 2: Reconsider how we setup, maintain, and programatically access the various astrometry_net_data datasets and then implement the new specification.  Option 3: Perhaps this is an ancillary task to developing a new specification for how we maintain and access the different types of datasets we use: ephemerides  and test data.  The options run from Trivial importance for Option 1, Minor importance for Option 2, and Major importance for the overarching Option 3.",NULL
DM-592,"metadata returned from deepCoadds is the primary HDU, not the data HDU","The metadata returned by e.g. {code} butler.get(""deepCoadd_calexp_md"", {'filter': 'HSC-R', 'patch': '5,5', 'tract': 0}) {code} is the metadata from the first HDU rather than from the image HDU. For example, {quote} SIMPLE  =                    T / file does conform to FITS standard              BITPIX  =                    8 / number of bits per data pixel                   NAXIS   =                    0 / number of data axes                             EXTEND  =                    T / FITS dataset may contain extensions            COMMENT   FITS (Flexible Image Transport System) format is defined in 'Astronomy COMMENT   and Astrophysics', volume 376, page 359; bibcode: 2001A&A...376..359H  HIERARCH HSCPIPE_VERSION = '2.12.0b_hsc'                                        HIERARCH HSCPIPE_VERSION = '2.12.0i_hsc'                                        HOST    = 'analysis13.hsc.ipmu.jp'                                               HOST    = 'analysis09.hsc.ipmu.jp'                                               USER    = 'yasuda  '                                                             USER    = 'yasuda  '                                                             HIERARCH ROOT_PATH = '/data3b/Subaru/HSC/rerun/yasuda/SSP1_UD_sub'               HIERARCH ROOT_PATH = '/data3b/Subaru/HSC/rerun/yasuda/SSP1_UD_sub'               BGMEAN  = -0.00153754750505493                                                   BGVAR   = 9.54377967961448E-08                                                   BGMEAN2 = 1.58211691607913E-06                                                  BGVAR2  = 2.25880256778648E-12                                                   AR_HDU  =                    5 / HDU containing the archive used to store ancill HIERARCH COADD_INPUTS_ID =   1 / archive ID for coadd inputs catalogs            PSF_ID  =                   76 / archive ID for the Exposure's main Psf          WCS_ID  =                   78 / archive ID for the Exposure's main Wcs          FILTER  = 'r       '                                                             TIME-MID= '1969-12-31T23:59:51.999918210Z'                                      EXPTIME =                   0.                                                   FLUXMAG0=     63095734448.0194                                                  HIERARCH FLUXMAG0ERR =      0.                                                   END                                                                              {quote} ",NULL
DM-5944,"Improve UX of Metric Timeseries Dashboard Plots (with Git Refs)","An interesting use-case for the metric time series plots is to compare how one branch compares to another. This would be help developers understand how their code changes affect algorithmic performance before causing a regression on master. This ticket will add the ability to filter and distinguish jobs corresponding to different git refs in the metric time series plot.",NULL
DM-5950,"Automatically include git-lfs in lsst-dev (etc) shared stack","Per request on HipChat, developers would like git-lfs to be available in the shared stack on {{lsst-dev}} and other shared infrastructure.    It's the work of a few seconds to add it: put the binary in {{Linux64/git_lfs/<version>/bin}}, add a trivial table file like  {code}  envPrepend(PATH, ${PRODUCT_DIR}/bin)  {code}  and declare it like  {code}  eups declare -r Linux64/git_lfs/<version> gif_lfs <version>  eups declare -c git_lfs <version>  {code}    It would be even better, though, if this were done automatically by the shared stack maintenance scripts.",NULL
DM-5957,"Internal review of use cases","Feedback/discussions plus revisions. ",NULL
DM-5965,"Various fixes for processing Suprimecam images","[~jbkalmbach] is working with some suprimecam data for an asteroid detection project, and in attempting to run it through the stack we found numerous small bugs that blocked processing. Most of this is normal bitrot since the suprimecam side of obs_subaru (apparently) hasn't been heavily exercised lately. I've compiled a set of minimal changes required to get these data through processCcd.py. Since I only have one set of images with the old MIT detectors, I haven't attempted to generalize these fixes beyond what I could test (e.g., to the Hamamatsu-specific code/configs). ",NULL
DM-5971,"Make technote-bootstrap conda-friendly","If someone runs {{pip install -r requirements.txt}} while running conda (either the default environment, or a custom environment, conda will tell pip to install those files in {{src/}} of the current working directory. (Note: it doesn’t do this for regular {{pip install}} commands; only for {{pip install -r}} commands). The end result is that a user will unwittingly litter their technote’s directory and cause build errors.    The solution is    # Ship a conda-compatible package list file (note that it can’t use pip’s tricks to install from GitHub as I currently do) with technote bootstrap  # Add instructions to the technote bootstrap’s README to {{conda create -n technote-env —file conda-reqs.txt}} and then source activate that environment when working on a technote.",NULL
DM-5974,"Use standard smart pointers in ndarray","Most of the stack has now been converted to use {{std::shared_ptr}} and friends, but {{ndarray}} has so far been excluded because of it's extensive use of {{boost::intrusive_ptr}}.  This ticket aims to fix it.",NULL
DM-5975,"Dev Guide: Announce new features when complete","When a significant feature has been added to the code, it should be announced via community.lsst.org and documented in the release notes for the next major version.  Add this to the developer workflow documentation.",NULL
DM-5987,"Jenkins centos-6 qserv-os-matrix builds are failing","The logs contain the following message about the requests package.  It is suspected that this began happening after a slave's disk filled up, possibly leaving the checked-out package in an invalid state (but that may also be a red herring).  {code}  requests: Traceback (most recent call last):    File ""/home/build0/lsstsw/lsst_build/bin/lsst-build"", line 51, in <module>      args.func(args)    File ""/home/build0/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 758, in run      manifest = p.construct(args.products)    File ""/home/build0/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 721, in construct      self._add_product_tree(products, name)    File ""/home/build0/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 709, in _add_product_tree      dependencies.append(self._add_product_tree(products, dprod.name))    File ""/home/build0/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 709, in _add_product_tree      dependencies.append(self._add_product_tree(products, dprod.name))    File ""/home/build0/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 692, in _add_product_tree      ref, sha1 = self.product_fetcher.fetch(productName)    File ""/home/build0/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 338, in fetch      raise Exception(""None of the specified refs exist in product '%s'"" % product)  Exception: None of the specified refs exist in product 'requests'  {code}",NULL
DM-5989,"Generate PS1 3pi PV3 reference catalogs","The PS1 3pi PV3 catalog has been released to the PS1 Science Consortium, and will soon be released generally.  Convert the PS1 DVO catalog into the new LSST reference catalog format, and provide a script that can be used to convert this format into astrometry.net format.",NULL
DM-5990,"Regenerate lsst_dm_stack_demo","In the course of merging DM-4801, the timing was off and there was a change to the deblender code while the stack demo was being updated to use the deblender, so the outputs are slightly off. This ticket should regenerate the outputs with the latest deblender changes",NULL
DM-60,"Make CSS Facade configurable","Currently Store has hardcoded connection to zookeeper. Make it configurable. Also allow to use memory-based Store. ",NULL
DM-6000,"Requirements Initial Draft","Derive requirements from use cases",NULL
DM-6002,"Design Initial Draft","Initial draft of the design based on the requirements",NULL
DM-6023,"_StandardizeExposure assumes .level exists when it may not","At line 101 of mapper.py:    .level is only set if a policy contains a level, but _StandardizeExposure assumes that this _will _exists, and things then fall over if it doesn't.",NULL
DM-6024,"Calibration construction scripts should run without multithreading","The master calibration file construction scripts (construct*.py) currently can currently _only_ be run with multithreading enabled. This apparently shouldn't be the case, and should be changed to allow it to run without having to pass parallelisation arguments. It should also be possible to squash the initial spewing of the env & setup packages state.",NULL
DM-6027,"Object field missing after obs_decam ingestion","During ingestion of images in obs_decam, the necessary ingredients do not make it into the registry to allow filtering of exposures by object type etc, meaning in order to select the exposures to process, one can only specify visit numbers.    It should be possible to do, for example    processCcd.py --id object='dark'  ",NULL
DM-6035,"S17 Butler define output dataset type from user code","User code would like to be able to programmatically define new output dataset types. The thought is to add a feature (tentatively called a ""genre"") that is a starting template for a dataset type that may be modified and added to the output dataset types of a repository.    (maybe it's implicit in this epic, but we need to define, design, RFC, and implement this feature.)",NULL
DM-6044,"Add LoadReferenceObjectsTask to task documentation page","The documentation for {{LoadReferenceObjectsTask}} should show up on the task documentation page, despite its being an abstract base class.",NULL
DM-6045,"Nominal PSF required by processCcd","After the processCcd refactor, non-sky images, or, in general, images for which an initial PSF estimate cannot be made, cause {{ProcessCcdTask}} to fail if {{config.doCalibrate=True}}.    To work around this, I put a dirty hack at line 157 of my {{processCcd.py}}:  {code}  from lsst.meas.algorithms.installGaussianPsf import InstallGaussianPsfTask, FwhmPerSigma  task = InstallGaussianPsfTask()  task.config.fwhm = 1.0 * 3.53223006755  task.run(exposure=exposure)  measFwhm = exposure.getPsf().computeShape().getDeterminantRadius() * FwhmPerSigma  {code}  to ensure that there would always be kind of PSF. This should be redone in a way that someone who knows what they're doing approves of.",NULL
DM-605,"CSS support for Qserv integration tests ","Replace QMS with CSS in Qserv integration tests.",NULL
DM-6056,"Remove PTR and CONST_PTR macros","This ticket implements the solution proposed in RFC-179, replacing {{PTR}} and {{CONST_PTR}} with {{std::shared_ptr<T>}} and {{std::shared_ptr<const T>}} respectively.",NULL
DM-6058,"add numexpr and bottleneck to CI environments","The packages numexpr and bottleneck are required for pandas functionality that sims_movingObjects utilizes.    http://pandas.pydata.org/pandas-docs/stable/install.html#install-recommended-dependencies    Currently, numexpr and bottleneck are not installed on Jenkins (or, I assume, buildbot), making it impossible to incorporate sims_movingObjects into CI.  The sims team would appreciate it if they were added.",NULL
DM-6059,"Support intersphinx in LSST Docs","Intersphinx is the native Sphinx means of referencing documentation and APIs from other projects.    #. Document use of intersphinx  #. Provide a means of adding new projects to the intersphinx inventories of projects, such as technotes, the DM Developer Guide, or Pipelines documentation.",NULL
DM-606,"code glitches in sconsUtils","In looking over sconsUtils for ticket #2976 my linter found some errors in state.py. I suspect the code works anyway due to extra names being imported, but it's surprising that it works.  It is not clear that ARGUMENTS should be defined in the following: {code}     opt = ""export""     if SCons.Script.ARGUMENTS.has_key(opt):         for kv in ARGUMENTS[opt].split(','): {code}  utils is not imported; the code should use the global ""log"" {code}     utils.log.fail(""Unrecognised compiler:%s"" % env['cc']) {code}  Also, it is not obvious that the global ""log"" will always be set (in which case it will be None and all logging statements will fail). But given the fact that this code works, and the failure is obvious is log is None, I'm guessing it is not worth putting in a guard for that.",NULL
DM-6060,"proper motions handling in jointcal","jointcal has the infrastructure to fit proper motions, but no way to determine which objects are fixed. Without fixed objects, the fit is obviously degenerate. I propose to test the simplest possible way: rely on catalog flags to find stars, allow those to move and hold the remaining objects (hopefully galaxies) fixed.      The development mostly involves code in the catalog loading part, and probably no significant change in the fit itself.",NULL
DM-6061,"File descriptor leak in pex_logging","Debugging DM-5979 demonstrated that the using {{addDestination}} on the default logger leads to a file descriptor leak. If it's called once in a long running process there is obviously no issue but in a test framework where many tests are being executed within a single process and where resource leaks are tracked this can be bad. At best the file descriptor leak testing code will trigger a leak and the test must be disabled, at worst as tests continue to add new destinations the logger can be bogged down in sending messages to more and more locations.    The simplest fix for testing frameworks may be to simply add a {{removeDestination}} method that can remove the destination that was added by the test.",NULL
DM-6066,"S17 Butler S3 storage mechanism implementation","Write the serializer so butler can write python object(s) to S3 storage    there are 3 ""completeness"" options we've discussed:  1. tracer bullet - only one python type + format + storage is implemented.  2. if they have a posix serializer, leverage that with staging for a S3 serializer  3. write something custom for writing to S3 stream, and write custom serializers    it seems like #2 is the best one to do.",NULL
DM-6067,"S17 Butler DB storage mechanism implementation","There’s a small set of python object types that it makes sense to have this for. We will have to write custom serializers for these. First pass is probably to write serializers for:  * SourceCatalog  * PropertySet  * PropertyList (which is a subclass of PropertySet)",NULL
DM-6068,"S17 Butler In-memory repository","stores object references by datasetType+dataId (probably in a dict)    before sizing: need to define how and when the in-memory repo will be purged.",NULL
DM-6070,"Test that measurement plugins get aperture-corrected fluxes","[~lauren] found and fixed a problem that measurement plugins do not get aperture corrected fluxes. Unfortunately DM-4692 reintroduced the bug (to be fixed in DM-6063). We need a unit test that catches the problem, but didn't want to hold up DM-6063 for it, hence this ticket to clean up the technical debt.    This may also be related to DM-6051 and it would be nice to have the test in place for DM-5877.",NULL
DM-6080,"afw build failure with XCode 6.2 (OSX 10.9)","I'm guessing this is going to be assigned ""won't fix"" (due to XCode age), but I get the following error when attempting to build afw master on OSX 10.9, which has XCode 6.2 (most recent available version). That particular line was last changed in 2014, so I suspect the actual error is being caused elsewhere and only shows up here.    {code}  In file included from src/cameraGeom/CameraSys.cc:23:  In file included from include/lsst/afw/cameraGeom/CameraSys.h:31:  include/lsst/afw/geom/TransformMapImpl.h:60:21: error: no matching member        function for call to 'insert'          _transforms.insert(std::make_pair(nativeCoordSys,          ~~~~~~~~~~~~^~~~~~  src/cameraGeom/CameraSys.cc:54:22: note: in instantiation of member function        'lsst::afw::geom::TransformMap<lsst::afw::cameraGeom::CameraSys>::TransformMap'        requested here  template class geom::TransformMap<cameraGeom::CameraSys>;                       ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/map:1041:9: note:        candidate function not viable: no known conversion from 'pair<typename        __make_pair_return<const class CameraSys &>::type, typename        __make_pair_return<class shared_ptr<class IdentityXYTransform> >::type>'        to 'const pair<const key_type, mapped_type>' for 1st argument          insert(const value_type& __v) {return __tree_.__insert_unique(__v);}          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/map:1059:10: note:        candidate function not viable: no known conversion from 'pair<typename        __make_pair_return<const CameraSys &>::type, typename        __make_pair_return<shared_ptr<IdentityXYTransform> >::type>' to        'initializer_list<value_type>' for 1st argument      void insert(initializer_list<value_type> __il)           ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/map:1026:42: note:        candidate template ignored: disabled by 'enable_if' [with _Pp =        std::__1::pair<lsst::afw::cameraGeom::CameraSys,        std::__1::shared_ptr<lsst::afw::geom::IdentityXYTransform> >]                class = typename enable_if<is_constructible<value_type, _P...                                           ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/map:1034:18: note:        candidate function template not viable: requires 2 arguments, but 1 was        provided          iterator insert(const_iterator __pos, _Pp&& __p)                   ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/map:1050:14: note:        candidate function template not viable: requires 2 arguments, but 1 was        provided          void insert(_InputIterator __f, _InputIterator __l)               ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/map:1045:9: note:        candidate function not viable: requires 2 arguments, but 1 was provided          insert(const_iterator __p, const value_type& __v)          ^  1 error generated.  {code}    Also, for the record:    {code}  $ g++ -v  Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1  Apple LLVM version 6.0 (clang-600.0.57) (based on LLVM 3.5svn)  Target: x86_64-apple-darwin13.4.0  Thread model: posix  $ clang -v  Apple LLVM version 6.0 (clang-600.0.57) (based on LLVM 3.5svn)  Target: x86_64-apple-darwin13.4.0  Thread model: posix  {code}",NULL
DM-6081,"Missing Astropy dependency for AFW","AFW needs to depend on Astropy in its EUPS table file:    https://github.com/lsst/afw/pull/62",NULL
DM-6084,"meas_algorithms cannot be built with XCode 7.2.1","The LSST stack fails to build on OS X 10.10 ""Yosemite"" using XCode 7.2.1, the last version that is compatible with that operating system. I have attached a build log from [~wmwood-vasey].    [~smonkewitz] suggests trying the following (HipChat UW DM room, 2016-05-12 18:12 Pacific): in BinnedWcs.h change:  {code}  virtual PTR(afw::image::Wcs) clone() const {      return PTR(afw::image::Wcs)(new BinnedWcs(_parent, _xBin, _yBin, _xy0));  }  {code}  to:  {code}  virtual PTR(afw::image::Wcs) clone() const {          return PTR(afw::image::Wcs)(std::make_shared<BinnedWcs>(_parent, _xBin, _yBin, _xy0));  }  {code}    with the following explanation (HipChat SQuaRE room 2016-05-12  18:26 Pacific):    {{enable_shared_from_this}} usually holds a {{weak_ptr}} to this internally, and the first {{shared_ptr}} to manage an instance is assigned to the internal {{weak_ref}}. It looks to me like this particular version of the STL only allows {{std::shared_ptr<A>}} to be assigned to a {{weak_ptr<B>}} if {{A}} is implicitly convertible to {{B}}. In this case the first {{shared_ptr}} to manage the {{BinnedWcs}} is a {{shared_ptr<Wcs>}}, and I think a static cast is required to get from {{Wcs *}} to {{BinnedWcs *}}, so the compiler barfs.",NULL
DM-6085,"mariadb build should not complain about lack of /etc/redhat-release","On Linux systems without {{/etc/redhat-release}} (e.g. Ubuntu) the build of {{mariadb}} and {{maridabclient}} triggers a message in the log, from the {{eupspkg.cfg.sh}} script, which can confuse people when there are other build issues with the package.    {code}  ::::: [2016-05-13T01:06:39.408611Z] Setting up environment with EUPS  ::::: [2016-05-13T01:06:39.480713Z] + eupspkg PRODUCT=mariadbclient VERSION=10.1.11.lsst2 FLAVOR=generic config  ::::: [2016-05-13T01:06:39.598864Z] grep: /etc/redhat-release: No such file or directory  {code}    The warning should either be silenced by redirecting the output from the EL5 test to {{/dev/null}} or the script should check if the file exists before trying to search it.",NULL
DM-6088,"Support MathJax/LaTeX math macros in Sphinx project","User-configurable LaTeX macros are a highly requested feature and would make documents like [DMTN-015|http://dmtn-015.lsst.io] much easier to write.    [MathJax supports macro definitions|http://docs.mathjax.org/en/latest/tex.html#defining-tex-macros] (e.g., newcommand) but this does not seem to be exposed through Sphinx.    This ticket will design a Sphinx configuration scheme for user macros, and implement a pipeline to insert those macros into Javascript shipped with the Sphinx documents.",NULL
DM-6090,"Replace boost::lexical_cast with std::to_string where possible","Use {{std::to_string}} for integer to string conversions instead of {{boost::lexical_cast}}.  Also investigate how often `boost::lexical_cast` is used for the inverse operation and make appropriate replacements if possible.",NULL
DM-6091,"Replace boost::lambda with C++11 lambda","Replace {{boost::lambda}} with C++11 lambda",NULL
DM-6092,"Remove use of boost::noncopyable","Replace by deleting copy constructor.",NULL
DM-6093,"Replace boost::random with std::random where possible","The following are used:    * {{boost::random::mt19937}}  * {{boost::random::uniform_01}}  * {{boost::random::uniform_int_distribution}}    Replace these with equivalents from std if possible.",NULL
DM-6094,"Use type traits from type_traits over boost equivalents where possible","The following are used:    * {{boost::call_traits}}  * {{boost::enable_if}}  * {{boost::enable_if_c}}  * {{boost::false_type}}  * {{boost::is_arithmetic}}  * {{boost::is_complex}}  * {{boost::is_const}}  * {{boost::is_convertible}}  * {{boost::is_integral}}  * {{boost::is_same}}  * {{boost::remove_const}}  * {{boost::true_type}}    Replace these with equivalents from std where possible.",NULL
DM-6095,"Replace boost::ref","Replace boost::ref",NULL
DM-6096,"Replace boost::scoped_array with std::unique_ptr specialisation","Replace boost::scoped_array with std::unique_ptr specialisation",NULL
DM-6097,"Replace boost::bind with std::bind","Replace boost::bind with std::bind",NULL
DM-610,"The SConstruct file is not installed by ""scons install""","The SConstruct file is not installed by ""scons install"". This can be a bit of a headache for afw, since the unit tests are not built during installation (DM-609) and without an SConstruct file they cannot be built later.",NULL
DM-6103,"capture qa dashboard django logs","We ([~jhoblitt]) have been unable to get django to properly produce logs when running under {{uwsgi}}.  We have access logs via the proxy server (nginx) but these logs are important for debugging purposes.",NULL
DM-6104,"load balance multiple instances of bokeh server","As part of DM-5844, it was attempt to run multiple instances of the bokeh server using nginx as a reverse proxy/load balancer between instance.  However, this is unreliable in practice.  The qa dashboard connects to bokeh and establishes a session which is passed to the u-a.  The u-a then makes a direct websocket connection to bokeh with this connection information.  As there appears to be no state sharing mechanism between bokeh instances, this only ""works"" if both the qa dashboard and the u-a connect to the same bokeh instance.    An nginx module to add ""sticky"" cookie based http routing to nginx was tested as working:  https://github.com/jhoblitt/nginx-rpmbuild    If qa dashboard is modified to pass on a cookie from the u-a to it's connection with the bokeh server, this should ensure the dashboard and u-a are connecting to the same bokeh instance.",NULL
DM-6109,"bin/deploy should fail when curl fails to find appropriate certificate","heather999 reports [on GitHub|https://github.com/lsst/lsstsw/issues/106]:    {quote}  When using lsstsw/bin/deploy at NERSC and at Harvard, I typically lack administrative access and have trouble with the curl commands for git and eups due to the missing certificate, as mentioned here:  https://confluence.lsstcorp.org/display/LSWUG/Building+the+LSST+Stack+from+Source  It's easy enough to fix by adding a --cacert parameter to the curl command. However, it is very difficult to spot the problem given the extensive output from ./bin/deploy. It would be much better if the script could report a hard failure due to the unsuccessful curl command(s). Otherwise, it may appear to a user that everything was set up, but then problems ensue when trying to use eups or git-lfs, for example.  {quote}",NULL
DM-6110,"lsstsw should provide newer git-lfs","Apparently, git-lfs as provisioned by lsstsw does not behave as documented in our developer guide because it is too old. Specifically, heather999 reports [on GitHub|https://github.com/lsst/lsstsw/issues/107]:    {quote}  Currently, bin/deploy installs git-lfs 1.0.2. This is rather old and does not include the more recent feature modification which is referenced in the DM Doc, namely ""git lfs install"":  http://developer.lsst.io/en/latest/tools/git_lfs.html  See: https://gitlab.com/gitlab-org/gitlab-ce/merge_requests/3779   Further it seems the current DM configuration instructions do not work at all for git-lfs 1.0.2. Once I upgraded to 1.2.0 things were working in accordance to the DM documentation.    Would it be possible to upgrade the version of git-lfs that ./bin/deploy installs so others do not run into similar problems?  {quote}",NULL
DM-6114,"Remove duplicate lines","Remove some (apparently accidentally) duplicated lines.",NULL
DM-6116,"convert packer-newinstall to use a proper packer docker plugin to setup container entry","The eups distrib install based docker container build  in {{packer-newinstall}} is currently using a shell to template a trivial Dockerfile in order to set some entry parameters that are not directly supported by the packer docker provider.  There appears to now be at least one packer plugin that could replace this functionality is a more maintainable way.    https://github.com/avishai-ish-shalom/packer-post-processor-docker-dockerfile",NULL
DM-6117,"build containers using conda packages","As conda binary packages are planned to be available on a regular basis (ie, weekly) starting in the w16 cycle, we should start making container images with this packages similar to how {{packer-newinstall}} uses eups distrib install packages. ",NULL
DM-6120,"Edit on GitHub button for LTD","Low priority, but getting back the RtD ""Edit on GIthub"" button would be nice if it's not too much work. ",NULL
DM-6122,"Add clear guidelines to changed-controlled Developer Guide Pages","Many pages in the Developer Guide are change-controlled. These pages should have a clear admonition element (such as a {{note}} directive) that states    #. The page is change controlled  #. The person/organization responsible for approving changes (or another process like RFC).    This will provide great transparency over what parts of the developer guide are informational versus requirements. This will also make it easier for a DM team member to understand who to set as a potential reviewer for any changes.",NULL
DM-6129,"Dev Guide: Check license of third-party packages","Third-party packages should never be added as dependencies without an RFC, and if we are going to distribute them via {{eups}}, we need to ensure that their licenses are compatible.  Add text to the Developer Guide reflecting this.",NULL
DM-6132,"Add kwarg forwarding to AstroPy table views","Tiny change that should have been included in DM-5641.",NULL
DM-6143,"Clean up ISR task","IsrTask has the following puzzling code in its run method:  {code}          for amp in ccd:              #if ccdExposure is one amp, check for coverage to prevent performing ops multiple times              if ccdExposure.getBBox().contains(amp.getBBox()):                  ampExposure = ccdExposure.Factory(ccdExposure, amp.getBBox())                  self.updateVariance(ampExposure, amp)  {code}    This seems very inefficient if there are few amplifiers that are likely to overlap the exposure (e.g. lsstSim data, with one amp per raw exposure). Surely there is a better way?    Also, most methods take an exposure but only use the masked image. The API would be clearer if those methods a masked image.",NULL
DM-6144,"allow reading ""wrong"" type of table","Please allow a way of ignoring {{AFW_TABLE_TYPE}} when reading table files, at least for reading {{lsst.afw.table.BasicTable}}. This would be potentially useful for converting a table file from one format to another.    See https://community.lsst.org/t/how-to-update-a-saved-catalog-to-a-new-format/786 for an example of where this would help",NULL
DM-6148,"Publish LDM-151 PDF on LSST the docs","This issue will leverage LSST the Docs and Travis CI to build and publish versioned pdfs of the LDM-151 document.    This work covers    # Creating a super-minimal HTML landing page (effort will specifically not be put into making this look modern)  # Configuring the Travis build  # Provisioning the LTD product.  # Provide path instructions in the README",NULL
DM-615,"lsst-build prepare does not recognize recursive calls between two packages","At the moment, sims_catalogs_measures and sims_photUtils are dependent on each other.  The result is an infinite loop during the 'lsst-build prepare' step: {noformat}     sims_sed_library:  ok (215.8 sec). sims_catalogs_measures:  ok (3.2 sec).       sims_photUtils:  ok (2.2 sec). sims_catalogs_measures:  ok (0.2 sec).       sims_photUtils:  ok (0.9 sec). sims_catalogs_measures:  ok (0.2 sec).       sims_photUtils:  ok (0.6 sec). {noformat}  See: http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack/builds/52/steps/shell/logs/stdio  This type of error doesn't happen every day but it has happened a number of times in the past.  The build would have terminated by itself at the failsafe timeout which is currently set at 4.5 hours.  However, I would prefer that it be caught and an exception returned to buildbot.",NULL
DM-6150,"lsstsw cannot deploy numdiff","I just cloned lsstsw.  When I ran deploy, it crashed on numdiff.    I got a lot of repetitions of    tar: Damaged tar archive  tar: Retrying...    and then it crashed because numdiff did not have a configure script (or a Makefile).    I think this might be a numdiff problem.  I tried curling numdiff by hand and the same thing happened.",NULL
DM-6152,"Please allow uploading many more file types to community","lsst.community.org is very picky about the kinds of files one can attach to a posting. Would it be possible to eliminate the whitelist and perhaps use a blacklist if concerned about particular file types? (.app and .exe, perhaps?).    Or at least please allow all source code, standard image formats, yaml and json. Here is what I came up with. Some of them may already be allowed and I am probably missing others: .py, .c, .cc, .cpp, .h, .hpp, .yaml, .tex, .csv, .json, .paf, .java, .js, .fits, .jpeg, .png, .txt, .log, .text, .md, .html, .sublime-settings.",NULL
DM-6161,"Add unit test for IsrTask","ip_isr has many unit tests, but none tests IsrTask. I suggest adding such a task, likely using the data in obs_test",NULL
DM-6163,"Some validate_drp measurements are not reproducible","I ran validate_drp's {{examples/runCfhtQuickTest.sh}} several times to compare a branch to master where I expected no change. I was surprised to find a change in PA1, PA2 and PF1 (though everything else was identical) so I ran a twice more, once on the branch and once on master. Each run gave different results. For example:  {code}  PA1 : 10.10 mmag <  5.00 mmag == False  PF1 : 12.00 %    < 10.00 %    == False  vs.  PA1 : 10.35 mmag <  5.00 mmag == False  PF1 :  4.00 %    < 10.00 %    == True  {code}  for the two runs using master.",NULL
DM-6164,"Implement the API replacing XYTransform","The new transforms design (DM-3874) will involve replacing XYTransform with a more generic system that will be shared with the WCS code. This Epic is to implement the design from 3874.",NULL
DM-6182,"Add dax_imgServe support for stripe82 ","There will likely need to be changes made in how stripe82 images are located and returned by dax_imgServ when qserv is setup to provide stripe82 data.",NULL
DM-6187,"LTD Backlog Epic"," LSST-the-Docs backlog container epic for stories that have not been scheduled.    NEVER SCHEDULE THIS EPIC. Pull stories into a scheduled epic instead.     DM-5404 introduced _LSST the Docs_ as a production platform for continuous documentation delivery. This Epic covers additional improvements to the platform, such as    - Implementation of DB migration procedure  - Improvements to Fastly Varnish layer routing (support for redirects from S3 and courtesy redirects of directories, for example)  - Keeper API refactoring and enhancement.  - Notifications from LSST the Docs to users on GitHub PRs or Slack      ",NULL
DM-6200,"SQuaRE Management","Management is generally carried out as an LoE activity, however as an experiment I am storypointing some select activities here for F16 as a way of recording effort required for certain recurring tasks in order to generate realistic loading for future cycles.     I am leaving the estimated story points blank, which I think will avoid messing any PMCS assumptions for this activity [note to self: check with Kevin] [or maybe team = tcams is enough? no WBS?]      ",NULL
DM-6267,"Gracefully handle Git revision dates in technotes","The technote platform uses a metadata.yaml file to provide authoritative single-sourcing of document metadata (https://sqr-000.lsst.io/#metadata-standard). One of those is the {{last_revised}} date. The problem with using this by default is that the revision date quickly gets out of sync with the Git history while a documented is being actively drafted. Solution:    # Make it so that {{last_revised: none}} causes {{documenteer}} to obtain the date from Git history instead. When a document is being ‘released’/‘archived' (or being initially ported from another format) the {{last_revised}} field can be set to a date.  # Potentially make the Git-based date smart so that revisions to ancillary files don’t affect the {{last_revised}} date of the content.  ",NULL
DM-6268,"Clarify how content in a reST directive must be indented","A case today showed that http://developer.lsst.io/en/latest/docs/rst_styleguide.html#tables did not explicitly state that content in directives (like tables) needs to be indented; it was only implied from examples.    Explicitly state intent requirements, and perhaps clarify the formatting of directives in general.",NULL
DM-6281,"More visualization features in S17","New features for visualization in Firefly",NULL
DM-6282,"Fix wrong units in CModel schema","Replace {{seconds}} with {{second}} to pass AstroPy compliance test.",NULL
DM-6292,"Qserv tests don't run if a space is present in LSST_LIBRARY_PATH","Qserv tests fail to run on OS X if there is a space in any of the library loading paths. The fix is simply to quote the string being constructed to forward on the environment.",NULL
DM-6299,"LTD Mason: do not set Encoding-Type header","In the case of a fits.gz file, the Encoding-Type is gzip. This breaks our S3 uploads in LTD Mason because boto3 doesn’t actually let us set Encoding-Type. This is a simple removal of any attempt to set that header.",NULL
DM-6305,"Remove use of assertRaisesLsstCpp from AFW","In DM-827 use of {{utils.tests.assertRaisesLsstCpp}} was deprecated. This ticket covers the work to remove those asserts from the 5 tests in AFW that are still using it and to remove the routine from {{utils}}. The code can now use {{assertRaises}}.",NULL
DM-6318,"Update bin.src/validate_drp.py to handle missing ""requirements"" gracefully.","When I run {{examples/runDecamQuickTest.sh}} I get the following failure (using master c1ea4c0):  {code}  /Users/rowen/UW/LSST/lsstsw/miniconda/lib/python2.7/site-packages/scipy/optimize/minpack.py:690: OptimizeWarning: Covariance of the parameters could not be estimated    category=OptimizeWarning)  Traceback (most recent call last):    File ""/Users/rowen/UW/LSST/testdata/validate_drp/bin/validateDrp.py"", line 89, in <module>      if args.configFile and kwargs['requirements']:  KeyError: 'requirements'  Validation failed  {code}    [~wmwood-vasey] traced it to the following code:  {code}  if args.configFile and kwargs['requirements']:  {code}  and suggested the following change:  {code}  if args.configFile and ""requirements"" in kwargs:  {code}  ",NULL
DM-6321,"Please support ""scons"" on El Capitan","Please support plain old {{scons}} on El Capitan instead of insisting that it be {{python $(which scons)}} or equivalent. Most of our packages have been updated in this way and it's very confusing having one that has not.    (At the very least please print a useful error message).",NULL
DM-6324,"Modernize python code in Qserv admin tools","Migrate print statements to print function.",NULL
DM-6327,"Use static_assert instead of assert in meas_algorithms where possible","Came across an interesting comment in {{Interp.cc}} while hunting for boost {{static_assert}} reading: {{we'd use C++11's static_assert if available}}, so decided to fix it :)  This ticket is just to track it, work was a one liner.",NULL
DM-6328,"add hsc driver script to validate_drp","Add an equivalent of {{examples/runChftTest.sh}} to {{validate_drp}} to process {{validation_data_hsc}}.",NULL
DM-6329,"Please make ""eups distrib list"" (or equivalent) show available tags for a product","Currently, the only way to see the tags available for installation through {{eups distrib install}} is to visit e.g. https://sw.lsstcorp.org/eupspkg/tags/ and examine them directly. That's inconvenient. Please provide a command line interface.",NULL
DM-6332,"Ensure configuration framework is included in long-term planning","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  pex_config has no active developers, leading to a long backlog of tickets and a stall in big-picture improvements. Code is hard to read/maintain, and design is exhibiting serious limitations when used in large pipelines, especially in lack of support for ways to connect related options in different sections of the tree. Serialization with plugins continues to be fragile. Distinctions between what-to-run and how-to-run options it are not well captured, leading to incorrect provenance when the same pipeline is executed at different levels. We still haven't dropped pex_policy.  Provenance of configuration options is unnecessarily complex and developers are confused by --clobber-config behavior.  {quote}    My best guess is that this is 0.5 FTE ongoing.",NULL
DM-6333,"Define ownership and plan work for obs_* packages, ISR, and CameraMapper","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  Need maintainer to oversee obs_ package consistency and code reuse in defining ISR, calibration products generation, and reading raw data. Should take at least partial ownership of CameraMapper.    Locally-reasonable decisions have led to bad overall design, difficult entry for new cameras.	  {quote}    My best guess is that this is 0.25 FTE, ongoing.",NULL
DM-6334,"Ensure task and data-flow redesign is included in long-term planning","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  Data product naming consistency and relationships are a mess, and data products are poorly defined (in the code, not in the future envisioned by the DPDD). Retargetable Task interfaces are poorly specified. As parallelization increases, things will only get more complex. We need someone to think about versioning and aggregating data products beyond just providing support for them in the Butler.    Our most user-visible interfaces are poorly defined and poorly documented. Even big overhauls like ProcessCcdTask are locally reasonable but must make undesirable compromises to fit into larger system.  {quote}  The LDM-151 rewrite will include a much of the design work, making this job more about implement that refactoring (with placeholders) and ensuring it remains reasonable (and consistent with LDM-151) as algorithms change.    This work has significant overlap with the role of a ""Data Flow Scientist"" identified in a recent review.    My best guess is that this is 0.5 FTE, ongoing.  ",NULL
DM-6335,"Ensure EUPS support (or equivalent) is included in long-term planning","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  eups and eups distrib (or whatever we use instead) need an active developer who can work towards generally improving them (consider link farms, integration with pip/setuptools, faster databases) rather than just putting out fires.  The stack is too hard to install, and to hard to develop on even for experts.  {quote}  As discussed at the DMLT F2F, a major concern is giving the maintainer technical agency (including potentially moving beyond eups) within the requirements.  Most immediate task may be to ensure requirements are fully specified and distinguished from the current designs of eups and eups distrib.    My best guess is that this is 0.5 FTE, ongoing.  ",NULL
DM-6336,"Ensure release management is included in long-term planning","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  As we start having both development and long-lived stable branches, we need someone in charge of vetting what goes into a release in terms of both testing and declaring feature/code freezes. This also includes managing shared software stacks. Until package framework improvements makes this a one-button operation, we also need someone to actually shepherd that process.  Major releases are not useful for developers, and weeklies are hard to track; need feature-based releases with meaningful version numbers  {quote}    My best guess is that this is 0.25 FTE, ongoing.  ",NULL
DM-6337,"Ensure package reorganization is included in long-term planning","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  Current package organization is ~50% historical, making it hard for new users to understand. We've never settled whether we define packages as namespaces (to group similar code) or in order to sanitize dependencies, and our indecision shows. We need more sensible entry points (metapackages? tags?) for different flavors of user/developer, and someone to think hard about how to handle different classes of extension package and optional dependencies.  It's too hard for new users/developers to find the package that contains certain functionality, and too many large-scale user-visible structures are being defined an a completely ad-hoc fashion.  Progress probably needs to be incremental, not big-bang, but we need a long-term vision for where it's going.  {quote}  My best guess is that this is 0.5 FTE, ongoing.  ",NULL
DM-6338,"Ensure interactive analysis/debugging toolkit is included in long-term planning","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  There is no clear ownership for the successor to the afw.display tools. SUIT has expertise and related work already planned but no mandate and a very different timeline than what's needed here; SQuaRE and Science Pipelines perhaps have a better idea of requirements, and all three need to share low-level components.  This is not just visualization; we also need to someone to think about cleaning up lsstDebug system and figuring out how/if that integrates with logging, as well as ensure that a number of low-level afw classes (images, tables) are usable from the visualization tools and tightly integrated with them.  Currently, Science Pipelines developers individually implement their own throw-away tools. Developers are inclined to ignore scientific implications of code and algorithm changes because the overhead involved in inspecting them is too large.	  {quote}  My new best guess is that this is between 2-3 FTE, ongoing, and it became clear at the DMLT F2F that no team currently considers this part of their mission.  The best way to start may be a 2-3 day technical meeting to define requirements.  If we can't make that happen before long-term planning is complete, we need to make sure we at least guess at how much effort it will be.  ",NULL
DM-6339,"Ensure Python API improvement is included in long-term plans","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  We've identified simple and unambiguously popular ways to improve our Python interfaces, like adding properties, and there are many more not-just-Python improvements that can be made to specific classes just by looking for clunky interfaces without assuming they're set in stone.  Currently, downstream code is becoming more reliant on interfaces that we'd ultimately like to change. We're embarrassed to show code samples to outside developers, and have to warn new users that they cannot expect interface stability, discouraging adoption.  {quote}  My best guess is that this is approximately 0.5 FTE, ongoing, but it may be best done by reserving a small fraction (10%) of several developers' time.  ",NULL
DM-6340,"Ensure logging conversion is included in long-term planning","While the Data Access team has taken ownership of the new logging package, no one has taken ownership of converting old Science Pipelines code to use the new package and removing the old one.    Probably 1 FTQ at most.",NULL
DM-6341,"Ensure geometry unification is included in long-term planning","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  afw.geom and sphgeom don't interoperate with each other, WCS, or cameraGeom, and we have no overall vision for how they should. They should also have much more consistent interfaces.  afw.geom has a clunky Python interface, and Polygon doesn't really fit into the library well.  {quote}  My best guess is that this is 4 FTQ, but best split across at least two developers (from Science Pipelines and Data Access) and multiple cycles, as some work may be blocked by getting sign-off on the design from the broader team. Also,  {quote}  Astropy is just starting to think about how to integrate various geometry concepts into a single interface. If we act now and get invested in that design, we can leverage a significant amount of quality-developer time from the Astropy community. If we don't get involved now, what they produce will be unlikely to meet our needs.  {quote}  ",NULL
DM-6342,"Ensure improving threading support is included in long-term plan","From [~jbosch]'s [technical debt list|https://confluence.lsstcorp.org/display/~jbosch/DM+Technical+Debt]:  {quote}  We need to be releasing the GIL when we go from Python to C++, we need to audit our C++ code to make sure it actually is thread-safe, and we need to add OpenMP parallelization for at least trivial low-level operations (e.g. image addition), even if we don't use it most of the time.  {quote}  My best guess is that this is 2 FTQ.  ",NULL
DM-6353,"Make jointcal robust to input problems","jointcal currently crashes in exciting ways if the input isn't right in a variety of ways. It should be smarter about checking various things before attempting to process the data, and it should never just segfault without any description as to why.",NULL
DM-6354,"Warn/raise when input catalogs have no (few?) cross-matches","jointcal should either print a large warning, or raise an exception if the input catalogs (either the N FittedStar catalogs, or the RefStar catalog) don't have any (or less than some reasonable number, say 10% of the total?) associations between each other. It currently just keeps going and then segfaults later on, likely because the fitting code can't handle that situation.",NULL
DM-6355,"Raise exception when input catalog WCS is badly formed","jointcal should raise an exception if the input WCS info associated with a catalog is badly formed. It currently always attempts to determine the inverse WCS via gtransfo and can fail with: {{trying to invert a singular transformation: a (nice) crash follows}}. It should stop before that and say something more about what the actual problem was.",NULL
DM-6366,"Add selection/completeness maps and masks to DPDD","The DPDD currently barely mentions (in section 3.2) characterization of selection functions and completeness, and does not define any data products related to these quantities.  This makes it hard to flow down a design for producing them to LDM-151.  Unfortunately, I don't think anyone on the DM team really has a lot of experience working with these products; we may need to look for outside help.  ",NULL
DM-6367,"Clarify role of DM in producing/serving photometric redshifts in DPDD","The DPDD mentions storage of photometric redshifts but gives no indication of where they will come from or how they will be produced.  I realize we have perhaps intentionally punted on this issue in the past, and we expect outsiders to do the work, but we now need to have the procedures for accepting, validating (or not), and loading photometric redshifts better defined in order to adequately plan.  ",NULL
DM-6371,"Merge DRP and AP algorithmic component sections of LDM-151","Work 90% complete already from live meeting; just need at ticket number for the branch.",NULL
DM-6372,"afw display ds9 fails on numpy arrays","afw.display apparently cannot deal with pure numpy arrays. They have to be wrapped in an afw.image first. It would be very useful to be able to just stuff a numpy array through to ds9 (or whatever display system is configured).    The following command sequence fails:    {code}  import lsst.afw.display.ds9 as ds9  data = np.zeros((10,10), dtype=np.float32)  In [3]: ds9.mtv(data)  ---------------------------------------------------------------------------  RuntimeError                              Traceback (most recent call last)  <ipython-input-3-38b73304e0e0> in <module>()  ----> 1 ds9.mtv(data)    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-20-g39312e7/python/lsst/afw/display/ds9.pyc in mtv(data, frame, title, wcs, *args, **kwargs)       78       79 def mtv(data, frame=None, title="""", wcs=None, *args, **kwargs):  ---> 80     return getDisplay(frame, create=True).mtv(data, title, wcs, *args, **kwargs)       81       82 def erase(frame=None):    /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/afw/2.2016.10-20-g39312e7/python/lsst/afw/display/interface.pyc in mtv(self, data, title, wcs)      384             self._impl._mtv(data.getImage(), data.getMask(True), wcs, title)      385         else:  --> 386             raise RuntimeError, ""Unsupported type %s"" % repr(data)      387     #      388     # Graphics commands    RuntimeError: Unsupported type array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)  {code}    But this works:    {code}  img = afwImage.ImageF(data)  ds9.mtv(img)  {code}",NULL
DM-6374,"post-qa should get repo URL from repos.yaml or Git remote","https://github.com/lsst-sqre/post-qa/pull/1 Josh notes that post-qa should not assume the Git repo URL pattern for a stack package.    bq. There are a currently a few packages in repos.yaml under the lsst-dm org. This is fine for now but it would be safer to either look at the repo's remotes or lookup the url from repos.yaml.    ",NULL
DM-638,"Generate System Test Specifications from DM Subsystem Requirements","Test Specifications will be generated from the  Enterprise Architect SysML using the traceability matrix from the DMSR to the UseCase and Activity objects.  The DMSR needs to include the traceability from both the  SRD and OSS.  The resulting test specifications will be allocated to one of System Test Specifications and/or Acceptance Test Specifications based on the requirements' nature (end user requirements and/or derived performance metrics). (PMCS: DMTF-19960 DMTF-19970 DMTF-19980 DMTF 19990)  Blockers: Completion of DMSR Traceability matrix which includes OSS links ",NULL
DM-6386,"Create unit test(s) for imageDifference command-line task","There are currently no unit tests for `imageDifference.py` in `pipe_tasks`. Add some, either on fake data and/or real data from `afwdata`.",NULL
DM-639,"Generate  Integration Test Specifications from UML model","The Integration Test Specifications driven by the design model need to be updated to allow the selection methodology defined by the DM Project Scientist.  This task will implement a process which uses an upgraded version of the LSSTTools to prepare DAG node specifications of the design model, then uses a DAG renderer to create the graphical representation of the primary Integration points in the design model.  This task is blocked until the external contract to update the LSSTTools EA add-in is completed.  Deliverable is the suite of rendered DAGs  rooted at each subsystem integration point. The Requirements satisfied at every UC and Activity will be incorporated into the node rendering.",NULL
DM-6390,"Create a testing distrib package","Create a far smaller testing distrib eups package for testing deploys.",NULL
DM-6391,"afw display needs to be able to display a catalog","afw.display has to display points one at a time via:    {code}  for pt in sourceCat:      display.dot('o',pt.getX(),pt.getY())  {code}    This is silly. I should be able to say {{display.showCatalog(sourceCat, coords='radec')}} or {{display.showCatalog(sourceCat, coords='cartesian')}} (or something) and get the catalog displayed in an appropriate manner.",NULL
DM-6393,"audit EUPS products for unnecessary ups/*.cfg files","It appears that at least most of the ""metapackages"" have boiler plate {{ups/<foo>.cfg}} files in them that are unnecessary and may be removed by adding {{noCfgFile=True}} to the sconsUtils constructor.",NULL
DM-640,"Initial prototype for shapelet PSF fitting","Jim will implement a class that can fit multi-shapelet models to PSF images on the HSC branch.",NULL
DM-641,"Port shapelet PSF fitting code to LSST","Move the code on the HSC fork from DM-640 back to LSST, clean it up, and get it through code review.",NULL
DM-6413,"Our release tag overwrites external package versions","I was updating to {{b2072}} yesterday, and I noticed that our release tags are overriding the versions of release packages. For example, boost gets built as {{boost-12.0.rc1+1}} instead of {{1.60}}. I haven't checked, but I suspect the same may be the case with other external packages.    This will be rather confusing -- could we fix it before the release?  ",NULL
DM-6424,"Refactor ip_diffim image differencing","It has been noted that `ip_diffim` could use some refactoring. Possible issues include:    1. clean up unneccessary config options  2. remove (or move elsewhere) unused code such as delta-function basis function  3. simplification and possible pythonification?",NULL
DM-6442,"ModelPsfMatchTask doesn't perform as well as ImagePsfMatchTask","Naoki Yasuda has shown that the vanilla diffim code works fairly well if he uses the ImagePsfMatchTask.  When he uses ModelPsfMatchTask, things get much worse.  We need to track down why there is any difference at all.  This is important because it impacts PSF matched coadds as well.",NULL
DM-6468,"pin nginx version","The version of nginx has been free floating and this caused warnings/errors in the 1.9 -> 1.10 transition when the {{spdy}} keyword was renamed to {{http2}}.",NULL
DM-6469,"update tls certificate","The *.lsst.codes wildcard cert in usage is about to expire.",NULL
DM-6472,"rework jenkins snapshot purging to be external to jenkins","In order to restrict the jenkins aws credentials to only being able to create new snaphots, a mechanism is needed to handle purging old snapshots that is external to jenkins.",NULL
DM-6478,"Add link to AHM 2014 CAM training slides","These provide a convenient overview of EVMS principles and useful background.",NULL
DM-6479,"Clarify mechanism for charging for science time","Distinguish between ""has science time"" (20% encumbrance) and ""is a project scientist"" (charge to separate account).",NULL
DM-6480,"Variation of rates across institutions","DMTN-020 refers to nominal cost rates for scientist/developer time which are constant across institutions. There's some suggestion this is either no longer the case, or may no longer be the case in future. Figure out which, and update the document accordingly.",NULL
DM-6481,"Recording actual time spent","If SPs track actual time spent, they are a direct proxy for actual cost. But, currently, they aren't. Should they be?",NULL
DM-6482,"Tracking people working an epic","The current DMTN-020 text makes it sound as though an LCR would be required if the folks assigned to an epic change. This likely isn't the case. Clarify.",NULL
DM-6483,"Clarify CCB process","In particular, note that CCB meeting is the third Wednesday of the calendar month. Check how far in advance LCRs have to be submitted.",NULL
DM-6484,"Correct thresholds for variance narratives","Confirmed with Kevin:  {quote}  It is 10% or $100k.  {quote}",NULL
DM-6485,"Clarify LDM-PMT document number","At different places, we refer to LDM-PMT as LDM-465 and LDM-472. Which is it?",NULL
DM-6486,"Correct links to LDM-PMT","Not clear if it's actually LDM-465 or LDM-472.",NULL
DM-6489,"Every epic must have at least one story set","Per JB discussion with KL; make sure this is clear.",NULL
DM-6493,"Add note on research epics","Currently, DMTN-020 is silent on the topic of timeboxing for research: it asserts that every epic must either have a concrete deliverable, or cover emergent work. However, we also expect to have to define timeboxed research projects. Please reflect this in the document.",NULL
DM-6495,"Please add raw data and calibration data to validation_data_decam","validation_data_decam cannot be used with the default DECam ISR task because it is missing raw and calibration data. It would be very nice to have this data available in order to compare the default DECam ISR task to the null ISR task that relies on the community pipeline and be able to run both.",NULL
DM-6496,"Create obs package for UKIRT+WFCam","We have UKIRT+WFCam observations we want to combine with HSC.  To start that, we need an obs_ukirt package with a {{WfcamMapper}}.  The data presented to the observer have been processed through the equivalent of ISR, but still need photometric and astrometric calibration.",NULL
DM-6499,"Please document DM meetings report policy","Per mail from [~jbecla] to the DMLT of 2016-06-07, individuals who are funded to attend a meeting by LSST are obliged to provide a meeting report when one is requested via JIRA issue.    A list of all meetings and attendees is available at https://confluence.lsstcorp.org/display/DM/DM+Meetings.    Please ensure this policy is documented somewhere appropriate. (Does it belong in the Developer Guide alongside topics like Team Culture?)",NULL
DM-6507,"Clarify & document approval process for changes to coding guides","Per RFC-24, the System Architect is the BDFL for ""the Coding Standards document"": he is ultimately responsible for approving or rejecting all changes suggested.    But: what is ""the Coding Standards document""? There are several ""Coding Guides"" listed [part of the Developer Guide|https://developer.lsst.io/#part-coding]. [Some of them|https://developer.lsst.io/coding/cpp_style_guide.html] are marked with specific notices that changes must be approved by the System Architect; [others|https://developer.lsst.io/coding/using_cpp_templates.html] aren't. Are they all covered by the RFC-24 provisions? Should they be?    Once the above is clear, please document this policy somewhere obvious (perhaps in the [introduction|https://developer.lsst.io/coding/intro.html]?), so that folks who are proposing new pages (rather than simply editing existing pages) will understand the policy.",NULL
DM-6508,"List DECam linearizers in a calibration table","The linearization support added in DM-6356 treats the linearizers as normal data products, rather than calibration products with a database and date ranges. This ticket is to clean up that technical debt and make the ingest code part of normal calibration ingest.",NULL
DM-6509,"Make clear that there are more than just four ""resource (sic) types""","See list in LPM-81 table 5.2",NULL
DM-6511,"Produce Baseline Change Request covering FY16 DM replan","Produce baseline change request that captures the DM re-plan. This includes:   - revisiting and documenting project mgmt processes and tools   - revisiting average base salary rates for all institutions   - updating WBSes as necessary (NCSA, SQuaRE for sure)   - shifting funds between planning packages as needed   - shifting cycles / releases / milestones by 3 months",NULL
DM-6512,"Add missing pictures for your team","I am asking all TCAMs to review [The Team|https://confluence.lsstcorp.org/display/DM/The+Team] page and add missing pictures of all members of your team. Please remove your name from the reviewers' list when done. The last person should mark it ""reviewed""",NULL
DM-6515,"Please make butler read proxies work for functors","Butler read proxies raise an error message if used as callable objects. This is a headache for linearizers, which are functors. One must use {{get(..., immediate=True)}} in order to successfully retrieve a usable linearizer.    [~ktl] suggested a fix on HipChat: add {{\_\_call\_\_}} support to {{lsst.daf.persistence.ReadProxy}}    DM-6514 includes a workaround for this bug (using {{immediate=True}} in the ISR tasks in ip_isr and obs_subaru to unpersist linearizers) and it may be worth removing that workaround when fixing this issue.",NULL
DM-6517,"numpy (etc) should not use the 'nomkl' option on macs","LSST fftw and numpy 'mkl'-version have namespace collisions, on linux machines. Therefore the LSST build requires a nomkl version of numpy + friends. However, on macs the 'nomkl' option is not necessary and should not be there.    [~tjenness] suggests that lsstsw/bin/deploy should be fixed to avoid using nomkl on macs.     It would be good to fix this throughout our distribution schemes though - conda has a (self-described by [~mjuric]) ""horrible hack"" which should avoid this in future conda releases.     I'm not sure what happens with eups distrib releases, where miniconda is installed first.     But anyway - it would be good to fix this across the board.",NULL
DM-652,"Run initial set of four baseline HTCondor ClassAds scenarios","Run the initial set of four baseline HTCondor ClassAds scenarios. These will verify that the basic implementation of utilizing Rank to place jobs near data is operating as anticipated.   The main test within the scenarios is whether a CCD that is advertised to have calibrations in a given slot/node will consistency execute in that location, even when other open slots are always available (e.g., 4 CCDs, 5 total slots.)",NULL
DM-6522,"Make old TCAM guide obsolete","We should make this page: https://confluence.lsstcorp.org/pages/viewpage.action?pageId=21397653 obsolete once the DMTN-020 is done.",NULL
DM-6523,"Add example hourly rate calculates for scientist, developer","This seems to be a persistent source of confusion. The example I posted to the mailing list got agreement from Kevin, so let's add it to the doc.",NULL
DM-6525,"ANetAstrometry now broken because of recent changes of refObjLoader","While I was running processCcd.py, I encountered an error linked to 'refObjLoader' and 'anetAstrometry', which could apparently comes from recent changes in the way reference object loaders are now constructed (https://community.lsst.org/t/testing-obs-decam-and-obs-cfht/820).    You can find below the error message followed by the part of the config file related to this error. Please let me know if I can help in any ways to debug, run more tests, or in you need details on what I've used to produce the bug (but keep in mind that I have very little knowledge of the stack for now).    **Error message:**  {code}  _CameraMapper: Loading Posix registry from /home/chotard/Work/clusters/3C295/output_  _Traceback (most recent call last):_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_tasks/12.0.rc1-1-gc9b72b9+1/bin/processCcd.py"", line 25, in <module>_  _    ProcessCcdTask.parseAndRun()_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_base/12.0.rc1+8/python/lsst/pipe/base/cmdLineTask.py"", line 452, in parseAndRun_  _    resultList = taskRunner.run(parsedCmd)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_base/12.0.rc1+8/python/lsst/pipe/base/cmdLineTask.py"", line 194, in run_  _    if self.precall(parsedCmd):_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_base/12.0.rc1+8/python/lsst/pipe/base/cmdLineTask.py"", line 281, in precall_  _    task = self.makeTask(parsedCmd=parsedCmd)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_base/12.0.rc1+8/python/lsst/pipe/base/cmdLineTask.py"", line 371, in makeTask_  _    return self.TaskClass(config=self.config, log=self.log, butler=butler)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_tasks/12.0.rc1-1-gc9b72b9+1/python/lsst/pipe/tasks/processCcd.py"", line 135, in __init___  _    self.makeSubtask(""calibrate"", butler=butler, icSourceSchema=self.charImage.schema)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_base/12.0.rc1+8/python/lsst/pipe/base/task.py"", line 226, in makeSubtask_  _    subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pex_config/12.0.rc1-1-gc153103/python/lsst/pex/config/configurableField.py"", line 77, in apply_  _    return self.target(*args, config=self.value, **kw)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_tasks/12.0.rc1-1-gc9b72b9+1/python/lsst/pipe/tasks/calibrate.py"", line 279, in __init___  _    self.makeSubtask(""astrometry"", refObjLoader=self.refObjLoader, schema=self.schema)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pipe_base/12.0.rc1+8/python/lsst/pipe/base/task.py"", line 226, in makeSubtask_  _    subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/pex_config/12.0.rc1-1-gc153103/python/lsst/pex/config/configurableField.py"", line 77, in apply_  _    return self.target(*args, config=self.value, **kw)_  _  File ""/home/chotard/Work/lsstsw/stack/Linux64/meas_astrom/12.0.rc1-3-g992ad82+1/python/lsst/meas/astrom/anetAstrometry.py"", line 145, in __init___  _    pipeBase.Task.__init__(self, **kwds)_  _TypeError: __init__() got an unexpected keyword argument 'refObjLoader'_  {code}    **Config file**  {code}  _from lsst.meas.astrom.anetAstrometry import ANetAstrometryTask_  _config.calibrate.astrometry.retarget(ANetAstrometryTask)_  _config.calibrate.astrometry.solver.sipOrder=3_  {code}",NULL
DM-6526,"Is it ok to delete stories?","Check with [~klong] -- can deleting stories from JIRA upset the PMCS ingest?",NULL
DM-6530,"Add note on handling actuals for vacations","At the ProjMgmt WG telecon of 10 June, there was some question about how to account for actuals during vacations, the answer being effectively to spread them evenly across WBS. Add a brief note on this to DMTN-020.",NULL
DM-6531,"Deal with actions from past meetings","We are doing poorly with dealing with action items from past meetings.  Looking at Feb JTM:    https://confluence.lsstcorp.org/display/DM/DM+JTM+2016-02+Meeting+Notes    17 out of 21 are not done.    Looking at the latest: DMLT F2F    https://confluence.lsstcorp.org/display/DM/DM+Leadership+Team+Meeting+2016-05-16+to+19%2C+Face-to-Face    ~35 out of ~50 are not done.    Could TCAMs review these actions, follow up with your team and deal with them?    I’d suggest to turn still-open actions into stories in jira.",NULL
DM-6532,"Write LDM-PMT doc","Write LSST DM Project Management and Tools document.",NULL
DM-6534,"Add N-way matching to DLM-151","Add a requirement for an N-way matcher to LDM-151.",NULL
DM-6535,"Please mark Buildbot as deprecated","At least one developer hadn't realised that the Buildbot system at http://lsst-buildx.ncsa.illinois.edu:8010/builders/DM_stack is now deprecated and they should be using Jenkins instead. Of course, that's clear when reading the Developer Guide, but that isn't clearly linked from the Buildbot pages. Assuming we can't just remove the Buildbot pages altogether, it would be great to add a prominent note to them directing developers to look elsewhere.",NULL
DM-6536,"Review F16 milestones","We currently have 73 milestones scheduled for F16 (see the list [here|https://jira.lsstcorp.org/issues/?filter=14309]). While it is true that we will be revisiting milestones when we finalize replanning, there is no point in keeping any milestones open with due date in F16 if we already know they won't happen, so I am asking TCAMs to revisit them and update: delete these that no longer make sense, or push these that you are not planning to deliver in F16 into future cycles. Please keep only these milestones scheduled for F16 that you are going to deliver.",NULL
DM-6537,"Parallax and proper motion in level 1 DB","DRP will be able to measure proper motion and parallax better than we can in AP.  Additionally, the baseline for measuring parallax will be about the same as the initial data release.    This issue is to follow up about whether it is actually useful to calculate those quantities in AP at all.  We could potentially just use the DRP numbers.",NULL
DM-6539,"Updates to DMBP and DM projects","Update DMBP and DM projects based on decisions made through ProjMgmtWG:    DMBP  * remove planning packages  * milestones need WBS  * add field to Milestones: ""planning package key""    DM  * add to epics a field ""planning package key""  * add fields ""TCAM1"" and ""TCAM2"" to all issues  * add fields: reviewer #1, reviewer #2  * tweak workflow for epics:  ** must have reviewers assigned before epic switched to ""in progress""  ** reviewers must sign off the review before epics marked ""done""      I think it'd be also useful to have a brief description of milestone levels on the ""edit issue"" page (visible to NSF, cross-subsystem, cross-team, internal to one team)    I assume we will transfer KPMs from DLP to DMBP as-is, when is the best time to do that?  ",NULL
DM-6543,"Please add lsst_ci to the default list of packages built by Jenkins","When I do a Jenkins run I usually want to build as much as possible, so I would appreciate it if {{lsst_ci}} was built by default.",NULL
DM-6549,"Capture material on 3-month planning cadence in DMTN-020","Per discussion in DM-6532, this material is appropriate for DMTN-020:    {quote}  +It is acceptable to load the plan in 3-month chunks, e.g., the plan for   +the first 3 months of the cycle is loaded before the start of the cycle, and the remaining   +part of the plan covering the last 3 months is loaded before the 4th month starts. This   +allows for minor fine-tuning of the second half of the cycle without going through the CCB   +approval.  {quote}",NULL
DM-6550,"Capture material on epic-based long term planning in DMTN-020","Per review of DM-6532, this material should live in DMTN-020:    {quote}  +As explained above, epics are used for planning and executing work within a cycle.   +   +In addition to that, epics are also extremely valuable for longer-term planning at a fine-grain level.   +When details of work for a given planning package are known, they can and should be captured through   +epics. Such epics can be freely created and changed at any time without any approvals. They   +should, of course, fit within the scope and budget of related PP. They can be useful for   +bottom-up analysis and validation of resources needed to implement a given PP. This allows   +to do detailed planning in flexible and agile way, while ensuring the scope/cost/schedule is   +well controlled and managed.  {quote}",NULL
DM-6551,"Please make an lsst_x package that includes all the packages we support","It would be helpful to have a single package that includes basically everything we support. I realize the details could be tricky, though. Products I want include everything a DM developer might work on or at least be expected not to break, including:  - obs_x for all x that we are supposed to keep working  - testdata_x  - validate_drp  - validation_data_x    We also might want variants that exclude certain packages:  - Monster CI packages that require more data or time than can reasonably be expected to be run on a laptop.    Also, I would like this package to be built by Jenkins by default, and explicitly listed as a package that will be built. As such, I think this replaces DM-6543, at least for the long term.",NULL
DM-6553,"Need to define a description for detector defects","HSC currently has a definition file for defects consisting of  {code}  #CCD x0    y0    width height  {code}  (this is then converted to a binary file for efficient application to the data).    We need to define a similar file format for the use of all cameras.  The format can be different (yaml?), but it also needs to add some extra fields;  enums that define the type of the defect (e.g. trap, serial trap, hot pixel, ...) and the action to be taken, and some parameters (e.g.trap lifetimes).  We will also have to write code to make use of this information.    This format needs to be defined in conjunction with the camera team.",NULL
DM-656,"selecting from non-partitioned table is broken","Queries such as ""select * from Science_Ccd_Exposure limit 1"" fail. In the xrootd log I can see things like  {code} GroupSched:Adding new task: 3600 : SELECT * FROM LSST.Science_Ccd_Exposure AS QST_1_ LIMIT 1 Foreman:Started task Task: msg: session=1 chunk=3718 db=LSST entry time= frag: q=SELECT * FROM LSST.Science_Ccd_Exposure AS QST_1_ LIMIT 1, sc= rt=r_16c3894f43d445a95eddf424676\ 3daf74_3718_0 {code}  for every chunk.  In czar log, I see  {code} 20140509 15:43:22.519829 0x7f8f84008ee0 ERR Msg: 3598 -1064 Failed to merge results. {code}  Eventually, the client sees the error {code} ERROR 4120 (Proxy): Error during execution: -1064 Failed to merge results. (3360) {code}  ",NULL
DM-6562,"DoubleShapeletApproximation fails to fit Gaussian PSF","When iterating a psf in a single frame processing for ci_hsc data-product visit 903338 ccd 18, the psf determiner generates a PSF for which a shapelet approximation cannot be found. Specifically, when the PSF determination stage iterates, it converts the determined PSF from the previous step into a Gaussian with an equivalent width. The double shapelet approximation fails to fit this Gaussian.    This ticket should make the PSF fitting robust against cases like this. Additionally it should revert the changes in ci_hsc on the DM-4202 ticket branch which were put in as a temporary workaround for this problem.    Attached to this ticket is the PSF that is failing, and a script which can reproduce the problem.",NULL
DM-6564,"Make eups matplotlib package PEP440 compliant for matplotlib check","If a user has a version of matplotlib installed from a git clone, the eups dummy package fails at the matplotlib version check. The versioning scheme for this type of install is determined by pep 440. Make the configuration script handle this type of version comparison.",NULL
DM-6565,"Replace short-period coadds with coadd difference images","Instead of producing short-period (e.g. yearly) coadds for the purpose of detecting and characterizing faint, slowly-changing sources, I believe we should diff these coadds against a full-depth coadd (or perhaps each other), and detect and characterize on the difference images.    This should reduce blending issues in characterizing these objects, and it should make what I'm calling the DeepAssociate pipeline (where we merge Footprints and Peaks to define Object candidates, just before deblending) much easier, because the short-period coadds will no longer contribute redundant detections for static sources at single-year depth.    I thus far have not included this proposal in LDM-151, in order to maintain consistency with the DPDD.",NULL
DM-6567,"Participate in DM replanning process (Project Science)","Stories related to the ongoing DM replanning process which are handled outside the regular DM WBS breakdown.",NULL
DM-657,"Improve error handling (qserv initialization broken when previously registered database does not exist)","Recipe:  * start Qserv, use a database ""LSST""  * stop all Qserv services  * remove the LSST database  * start Qserv  Result: {core} Starting xrootd.SQL error: Can't list tables for db LSST because the database does not exist.  /usr/local/home/becla/qserv/1/qserv/build/dist/etc/init.d/qserv-functions: line 37: 18735 Aborted                 (core dumped) /usr/local/home/becla/qserv/1/stack/Linux64/xrootd/qs5/bin/xrootd -c /usr/local/home/becla/qserv/1/qserv/build/dist/etc/lsp.cf -l /usr/local/home/becla/qserv/1/qserv/build/dist/var/log/xrootd.log -n worker : Manager of pid-file quit without updating file.          [FAILED]                                                            [FAILED] {core}  xrootd log shows {code} xrootd: build/wpublish/MySqlExportMgr.cc:130: void<unnamed>::doDb::operator()(const std::string&): Assertion `ok' failed. {code}  It is perfectly fine for Qserv to refuse to start if the database was removed, but the errors handling should be better. ",NULL
DM-6570,"afw::table throws when extracting subschema","Given a schema which contains field names with underscores in them, it's useful to be able to strip off a prefix by making a subschema. For example:    {code}  In [1]: import lsst.afw.table as afwTable  In [2]: import lsst.meas.base as measBase  In [3]: c = measBase.NaiveCentroidControl()  In [4]: s = afwTable.SourceTable.makeMinimalSchema()  In [5]: measBase.NaiveCentroidAlgorithm(c, ""test"", s)  Out[5]: <lsst.meas.base.baseLib.NaiveCentroidAlgorithm; proxy of <Swig Object of type 'lsst::meas::base::NaiveCentroidAlgorithm *' at 0x113448210> >    In [6]: s  Out[6]:  Schema(      (Field['L'](name=""id"", doc=""unique ID""), Key<L>(offset=0, nElements=1)),      (Field['Angle'](name=""coord_ra"", doc=""position in ra/dec""), Key<Angle>(offset=8, nElements=1)),      (Field['Angle'](name=""coord_dec"", doc=""position in ra/dec""), Key<Angle>(offset=16, nElements=1)),      (Field['L'](name=""parent"", doc=""unique ID of parent source""), Key<L>(offset=24, nElements=1)),      (Field['D'](name=""test_x"", doc=""centroid from Naive Centroid algorithm"", units=""pixel""), Key<D>(offset=32, nElements=1)),      (Field['D'](name=""test_y"", doc=""centroid from Naive Centroid algorithm"", units=""pixel""), Key<D>(offset=40, nElements=1)),      (Field['Flag'](name=""test_flag"", doc=""general failure flag, set if anything went wrong""), Key['Flag'](offset=48, bit=0)),      (Field['Flag'](name=""test_flag_noCounts"", doc=""Object to be centroided has no counts""), Key['Flag'](offset=48, bit=1)),      (Field['Flag'](name=""test_flag_edge"", doc=""Object too close to edge""), Key['Flag'](offset=48, bit=2)),      'test_flag_badInitialCentroid'->'slot_Centroid_flag'  )    In [7]: s['test'].getNames()  Out[7]: ('flag', 'flag_edge', 'flag_noCounts', 'x', 'y')  {code}    However, if we attempt to strip off a longer prefix including the underscore, we throw an unhelpful exception:    {code}  In [8]: s['test_flag'].getNames()  ---------------------------------------------------------------------------  Exception                                 Traceback (most recent call last)  <ipython-input-8-a15d95e770e9> in <module>()  ----> 1 s['test_flag'].getNames()    /Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/afw/2.2016.10-23-g120d329/python/lsst/afw/table/tableLib.pyc in getNames(self, topOnly)     1401         getNames(SubSchema self) -> std::set< std::string,std::less< std::string >,std::allocator< std::string > >     1402         """"""  -> 1403         return _tableLib.SubSchema_getNames(self, topOnly)     1404     1405     def find(self, k):    Exception: basic_string  {code}    Please fix it.",NULL
DM-6571,"Remove the 'makeSourceCatalog' alias for the run method of SourceDetectionTask","[Line 288 in {{detection.py}}|https://github.com/lsst/meas_algorithms/blob/554ea72d0c0a83ccceb62fbef5f6807ffc47530b/python/lsst/meas/algorithms/detection.py#L288] defines the alias {{makeSourceCatalog}} for the run method of the {{SourceDetectionTask}} and notes that the alias is deprecated and should be removed after checking for current usage.  Remove the alias definition after checking to ensure that the alias is not used anywhere in the stack.    Example of current usage in the stack (NB not complete!):    # [pipe_tasks - {{multiBand.py}}: The {{runDetection}} method of {{DetectCoaddSources.py}} (line 280)|https://github.com/lsst/pipe_tasks/blob/a43c408a0c5653160b8646e622e4adecaeee4232/python/lsst/pipe/tasks/multiBand.py#L280].",NULL
DM-6572,"Fix documentation of the 'measure' method of the DetectAndMeasureTask","[The signature|https://github.com/lsst/pipe_tasks/blob/a43c408a0c5653160b8646e622e4adecaeee4232/python/lsst/pipe/tasks/detectAndMeasure.py#L204] of {{DetectAndMeasureTask}}'s measure method does not match the method's documentation. Specifically, while the signature indicates that the function expects a {{SourceCatalog}}, the documentation suggests that the function expects a background instead. Fix the documentation.",NULL
DM-6573,"Determine what is occurring during mmap and mlock call that speeds up queries.","Calling mmap and mlock causes queries to run significantly faster, including solo queries, which is unexpected. Further, the mlock call must complete before the query is passed to mysql and only one mlock call can be be running at a time or the speed benefit vanishes. This is not the expected behavior and it would be good to know exactly what is happening. At this point, this has only been tested on the in2p3 cluster.",NULL
DM-6574,"Convert LDM-151 references to bibtex","At some point we need to convert LDM-151's reference to use bibtex; right now I'm just adding TODO notes for any references I'd like to add instead of going through the pain of adding them manually.    I'm assigning this to [~zivezic], with the expectation that he'll assign it to someone else once he determines its priority.  ",NULL
DM-6576,"Change RADECSYS to RADESYS in our code base","The FITS keyword {{RADESYS}} is often misspelled as {{RADECSYS}}. Both appear to be accepted by our FITS reading code, but the latter is incorrect and we should not be using it. I found 31 instances across 27 files. I will attach the search results.",NULL
DM-6584,"Functionality requests that only impact PFS","No cycle; no WBS → No PMCS import; no earned value.",NULL
DM-6585,"Allow the dataId key ""ccd"" to mean ""ccdnum"" for DECam raw/calexp/src data","{{obs_decam}} uses {{ccdnum}} as the CCD dataId key, while most other obs packages use {{ccd}} and there are a number of places in the stack that the key name ""ccd"" is assumed (e.g. PropagateVisitFlags DM-5326; afw/cameraGeom/utils ButlerImage).  This has caused many troubles for using {{obs_decam}}.      {{ccd}} used to mean something different in {{obs_decam}} because there were no designated numbers for the sensors and side+ccd were used when the package was first put together.  Since the merge of this commit https://github.com/lsst/obs_decam/commit/1c4db0457cc4ff6e3bbb76b468f5da10de7a0c37 (DM-3591), side+ccd are no longer used in obs_decam. To ease maintenance need and reduce confusion, this ticket will remove the deprecated usage of the dataId ""side"" and ""ccd"".   Afterwards ""'ccd"" can be made to mean ""ccdnum"" for specific dataset types.     This provides a workaround for obs_decam for DM-5326, and CBP/CPP work.  ",NULL
DM-6586,"Maintain & recover from technical debt in camera packages","This epic represents a collection of ""technical debt"" and design issues around our camera packages (""obs_*"", plus supporting scripts elsewhere). Currently, there is no team which is scoped to resolve these issues (see DM-6333).",NULL
DM-6587,"Remove RTLD_LOCAL","[~mjuric] writes:  {quote}  	The recent convenience functions & checks you merged into base break MKL on OS X. I traced it down the other night and your code [requires a one-word change (removal of {{RTLD_LOCAL}})|https://github.com/mjuric/conda-lsst/commit/0f424cdcf699a3a44d19f66de0f44f78ba157e92#diff-66d97b24f2108ae7eceade9ef8e5e136].    Could you plug that in when you get a chance?    PS: The problem is that on OS X, apparently once you load a library with {{RTLD_LOCAL}}, it will remain local even when it’s subsequently auto-loaded as a dependency of another library (which then won’t be able to find the symbols it needs, ergo the error message). In particular, the issue here is that running:    {code}  getMklThreads = loadSymbol<Getter>(""libiomp5"", ""omp_get_max_threads"");  {code}    loads libiomp5 as {{RTLD_LOCAL}}, and then the next time when the linker needs it (when loading MKL-enabled numpy on OS X, which is linked against libmkl_core, which itself is linked against libiomp5) it doesn’t find its symbols.  {quote}    I need to check that this won't break things on Linux.",NULL
DM-659,"running individual query in the automated test suite","It would be very nice if the automated test suite would allow to run a selected query, without having to reload anything. Say that I want to debug something about query 34 in case02, and so I will be repeatedly rerunning that query, maybe with debugger. So in practice I would like to run automated test and tell it ""setup environment for query 34 from case02"" (it should create database and load data), and then I'd like to run only query 34 from case02. Untangling how to partition / load data is non trivial. Running query is easy, I can just do it by hand. ",NULL
DM-6590,"Collection of small bugs and look and feel issues","1. In the color stretch dialog:  (in chrome Version 50.0.2661.102 (64-bit) on my Mac OSX 10.9.5)      * the asinh beta and powerLaw Gamma input field do not  look like part of the input fields group. Too much white space in between.      * When clicked on the Z scale option when the asinh and powerLaw types selected, the min and max values overlapped with the option choice. ",NULL
DM-6592,"enable subMenu in the menu list","We want to have a subMenu component in the Firefly so we could group some of the actions together by putting them into subMenu. ",NULL
DM-6594,"ObjectSizeStarSelector can produce numpy warnings","{{ObjectSizeStarSelector}} can produce the following numpy warnings:    {code}  objectSizeStarSelector.py:382: RuntimeWarning: invalid value encountered in less    bad = numpy.logical_or(bad, width < self.config.widthMin)  {code}    {code}  objectSizeStarSelector.py:383: RuntimeWarning: invalid value encountered in greater    bad = numpy.logical_or(bad, width > self.config.widthMax)  {code}    I suggest that the star selector handle this situation more gracefully, e.g. by reporting an appropriate exception or handling the data in an appropriate way. If logging a message would be helpful, then please do that. (DM-5428 handled a similar warning).    One way to reproduce this is to build ci_hsc.  ",NULL
DM-6595,"starSelector can issue warnings","The following warning was issued by the starSelector when building ci_hsc  {code}  processCcd.charImage.measurePsf.starSelector WARNING: Failed to make a psfCandidate from star 775966608882401357:     File ""src/image/Image.cc"", line 91, in static lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const Extent2I&, const Extent2I&, const _view_t&) [with PixelT = float; lsst::afw::image::ImageBase<PixelT>::_view_t = boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float, boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t> > >*> > >; lsst::afw::geom::Extent2I = lsst::afw::geom::Extent<int, 2>]      Box2I(Point2I(-2,1120),Extent2I(41,41)) doesn't fit in image 2048x4176 {0}    File ""src/PsfCandidate.cc"", line 200, in std::shared_ptr<lsst::afw::image::MaskedImage<EntryT> > lsst::meas::algorithms::PsfCandidate<PixelT>::extractImage(unsigned int, unsigned int) const [with PixelT = float]      Extracting image of PSF candidate {1}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-2,1120),Extent2I(41,41)) doesn't fit in image 2048x4176 {0}; Extracting image of PSF candidate {1}'  {code}    Should this warning be issued? If it is to be expected, can we clean it up a bit? The last line of the warning looks like it's associated with DM-5680",NULL
DM-6597,"Butler arg to ProcessCcdTask ctor breaks SingleFrameDriver","It looks like DM-5798 broke pipe_driver's SingleFrameDriverTask, as it also needed to be updated to pass a butler to ProcessCcdTask.  This is not shocking, as SingleFrameDriverTask has no tests, but I think it's a sufficiently serious regression that I think we should consider putting out a bugfix release; if it were working, SingleFrameDriverTask would be one the of the first points of entry for people who want to process data that has an associated obs package (I think it supersedes ProcessCcdTask in at least most of that role).  ",NULL
DM-6599,"Support wrapping in ReStructuredText ""prompt"" environnments","Supplying a long line in a {{.. prompt:: bash}} environment in ReStructuredText results doesn't result in any wrapping; instead the line extends outside the rest of the column of text, which looks strange (and requires scrolling).  I imagine wrapping long lings with a hanging indent would be preferable, especially if we can still make it possible for selecting and copying the text to result in a single pasted line.    This is a very low-priority issue, as it's easy to workaround by splitting the long command into multiple lines manually while backslashes to indicate that it's a single commend.  If fixing it is difficult, a note in the docs suggesting the workaround would be quite acceptable, and even that isn't urgent.    ",NULL
DM-6602,"Update dev guide to capture changes introduced through LDM-PMT","Once LDM-PMT is approved, we should update the dev guide to reflect proposed changes (for example code freeze 2 weeks prior to end of cycle)",NULL
DM-6604,"add sims specific python dep support to CI","On several occasions, sims has desired to add python packages to the standard build env which are not DM approved 3rd party packages.  The result has either been to block sims development or undocumented deps creeping into DM packages.    This may be avoidable by adding a sims specific installation flag to {{lsstsw}} along with a sims env specifc jenkins job.",NULL
DM-6605,"Final optimizations of new dipole fitting algorithm and task","The newly improved dipole fitting algorithm needs to be optimized and finalized for production use. This epic is a catch-all for the tickets necessary to finalize dipole measurement.",NULL
DM-6606,"Test the new dipole fitting task on real data","This epic covers testing the speed and robustness of the new dipole measurement on realistic data (e.g. crowded fields and poor subtractions) to make sure it will work in production.",NULL
DM-6613,"Typo in afw/display/interface.py","There's a typo in interface.py;  the patch is  {code}  diff --git a/python/lsst/afw/display/interface.py b/python/lsst/afw/display/interface.py  index 8599629..d7f58ea 100644  --- a/python/lsst/afw/display/interface.py  +++ b/python/lsst/afw/display/interface.py  @@ -537,7 +537,7 @@ class Display(object):               k, x, y = ev.k, ev.x, ev.y      # for now                  try:  -                if self.callbacks[k](k, x, y):  +                if self._callbacks[k](k, x, y):                       break               except KeyError:                   print >> sys.stderr, ""No callback is registered for %s"" % k  {code}",NULL
DM-6615,"Please include meas_extensions_photometryKron in lsst_apps","I don't think that we're distributing the Kron code; please add it.  ",NULL
DM-6617,"Catalog Search Panel bugs","from pull request DM-6500:  https://github.com/Caltech-IPAC/firefly/pull/102  * 2MASS All-Sky 'Read Me!' link in Catalog Search panel points to http://localhost:8080/irsaviewer/Q'http://hades.ipac.caltech.edu/applications/Gator/GatorAid/irsa/scan.html'E * -If you search m31 2MASS scan info catalog (4th catalog in the list), then bring back catalog panel, the first catalog is highlighted, but if you search, you'll notice that you are still searching 2MASS scan info.-",NULL
DM-6618,"Migrate DMTN-023 tutorial to pipelines.lsst.io","[~jbosch] wrote an excellent tutorial on command line data processing with HSC data. This is the type of content that’s sorely needed to get new users up to speed with LSST processing.    Naturally this content belongs in the {{lsst_apps}} documentation project, whose URL is currently https://pipelines.lsst.io. I think this tutorial can serve as a nucleus for a set fo ‘First Steps’ tutorials that serve as primers for {{lsst_apps}} before a user dives deeper into the tutorials and API references associated with each package.    With that, this ticket will    1. Migrate DMTN-023’s content to the https://github.com/lsst/pipelines_docs repo and deprecate DMTN-023 (with a link to follow-up)  2. Create a new section in https://pipelines.lsst.io for introductory tutorials  3. Migrate the content  4. Improve the tutorial’s content and presentation were possible.",NULL
DM-6619,"Refactor DM System Release Notes","In the v12_0 release, release notes and a release announcement for the entire DM system (`lsst_apps`, qserv, dax, firefly and doc projects) got lumped into the documentation site for `lsst_apps` (pipelines.lsst.io).    This ticket is a reminder to re-assess this situation, figure out the business needs that motivated it, and to drive a re-factoring of documentation and release notes to the appropriate places so that we have a solid architecture for documentation in the future.    cc. [~frossie]",NULL
DM-6626,"jointcal fails MemoryTestCase","jointcal currently fails several of the tests that get run as part of lsst.utils.tests.MemoryTestCase. For example:    {code}  File open: /Users/parejkoj/lsst/lsstsw/stack/DarwinX86/obs_lsstSim/12.0.rc1-2-g80ec5c9/description/defects/defectRegistry.sqlite3  File open: /Users/parejkoj/lsst/simastrom/validation_data_jointcal/twinkles1/registry.sqlite3  F  204295 Objects leaked:  ...  47: 0x114f5aeb0 lsst::afw::table::BaseRecord  46: 0x114f5b410 lsst::afw::table::BaseRecord  45: 0x114f5b370 lsst::afw::table::BaseRecord  44: 0x114f5b2d0 lsst::afw::table::BaseRecord  43: 0x114f5b230 lsst::afw::table::BaseRecord  42: 0x114f5b190 lsst::afw::table::BaseRecord  41: 0x114f5b0f0 lsst::afw::table::BaseRecord  40: 0x114f5b050 lsst::afw::table::BaseRecord  39: 0x114f5afb0 lsst::afw::table::BaseRecord  38: 0x114f5ae10 lsst::afw::table::BaseRecord  37: 0x114f5ad70 lsst::afw::table::BaseRecord  36: 0x114f5acd0 lsst::afw::table::BaseRecord  35: 0x114f5ac70 lsst::afw::table::BaseRecord  34: 0x114f5ab50 lsst::afw::table::BaseRecord  33: 0x114f5aab0 lsst::afw::table::BaseRecord  32: 0x114f5a9d0 lsst::afw::table::BaseRecord  31: 0x114f5a910 lsst::afw::table::BaseRecord  30: 0x114f5a830 lsst::afw::table::BaseRecord  29: 0x114f058f0 lsst::afw::table::BaseTable  28: 0x114f04f28 lsst::afw::table::detail::SchemaImpl  F  ======================================================================  FAIL: testFileDescriptorLeaks (__main__.MyMemoryTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/parejkoj/lsst/lsstsw/stack/DarwinX86/utils/12.0.rc1-2-gbf6fe2f/python/lsst/utils/tests.py"", line 134, in testFileDescriptorLeaks      self.fail(""Failed to close %d file%s"" % (len(diff), ""s"" if len(diff) != 1 else """"))  AssertionError: Failed to close 2 files    ======================================================================  FAIL: testLeaks (__main__.MyMemoryTestCase)  !Check for memory leaks in the preceding tests  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/parejkoj/lsst/lsstsw/stack/DarwinX86/utils/12.0.rc1-2-gbf6fe2f/python/lsst/utils/tests.py"", line 118, in testLeaks      self.fail(""Leaked %d block%s"" % (nleak, plural))  AssertionError: Leaked 204295 blocks    ----------------------------------------------------------------------  {code}    We should track down why these tests are failing, and fix them. I have commented out MemoryTestCase for now, so that CI will run cleanly.",NULL
DM-6635,"Typo in CoaddSrcTransformTask","[~price] points out  a typo [in {{CoaddSrcTransformTask}}|https://github.com/lsst/pipe_tasks/blob/0eef0fd518098cc25c66bfff40f53ccca9058431/python/lsst/pipe/tasks/transformMeasurement.py#L263]: {{self}} should not be repeated.",NULL
DM-6636,"DMTN-020 complete","This ticket contains no work, but is blocked by all tickets which should be addressed before DMTN-020 fully reflects the work of the PMP-WG.",NULL
DM-6637,"Extra redundancy in CameraMapper","The CameraMapper is doubly redundant, claiming to load the ""registry registry"" and saying it twice:    {code}  CameraMapper: Loading registry registry from /Users/parejkoj/lsst/simastrom/validation_data_jointcal/twinkles1/registry.sqlite3  CameraMapper: Loading registry registry from /Users/parejkoj/lsst/simastrom/validation_data_jointcal/twinkles1/registry.sqlite3  {code}    I'm guessing this redundancy is not necessary. It's certainly somewhat confusing.",NULL
DM-6639,"Developer guide incorrectly calls improvements ""enhancements"".","By means of [GitHub Pull Request|https://github.com/lsst-dm/dm_dev_guide/pull/41], [~gpdf] notes that https://developer.lsst.io/processes/workflow.html incorrectly refers to ""enhancement"" when it means ""improvement"". Please fix it.",NULL
DM-6641,"Add exact variants of image comparison test utilities","Using the {{assertXxNearlyEqual}} methods for exact comparisons is a bit confusing, even if it works (if you set {{atol=0 and rtol=0}}).  On this ticket I'll add simple variants whose names don't imply an approximate comparison and just delegate to the {{NearlyEqual}} variants with zero tolerances.    Doing this now so I can use it on DM-6631.  ",NULL
DM-6648,"Refresh & unify documentation on writing Tasks","Our current documentation on writing Tasks is fragmentary: it's partially in [Doxygen|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/pipe_tasks_write_task.html] and partially on [Confluence|https://confluence.lsstcorp.org/display/DM/How+to+document+a+Task]. It's also incomplete -- or, at least, it doesn't fully and unambiguously specify all the rules, procedures and best practices which developers are expected to bear in mind (see, for example, [this discussion|https://community.lsst.org/t/task-api-changes-involving-dataids-butlers/881/5]).    Please provide a unified reference document on task construction, explicitly specifying all requirements.",NULL
DM-6649,"Please make it possible to contact CDS and dynamically create a reference catalogue","We currently use fits tables to provide local copies of astrometric/photometric catalogues.  Please make it possible to use the CDS protocols to download catalogues as needed.      This is what Emmanuel does in Scamp...  ",NULL
DM-6654,"Create test for imageDifference command-line task","There is no test for imageDifference.py. Create one.",NULL
DM-6655,"Webpage of flags produced by various stack products","SDSS has a handy webpage with descriptions of all of their bitmask flags:    http://www.sdss.org/dr12/algorithms/bitmasks/#ListofBitmasks    It would be exceptionally useful for LSST to produce a similar webpage. I could see it being auto-built from our current flags documetation, which would also help us identify places where our current docstrings are lacking (which many of them are).",NULL
DM-6658,"Access reference catalogs with butler","In several places we have talked about how to use the butler to get reference catalogs.  The general idea has been to define a dataId that can describe a bounding box or sky circle and use the standard reference loader internal to the butler to get the catalog and pass it back.    We need to also support multiple reference catalogs and catalogs from external repos in this context.",NULL
DM-6659,"`eups distrib install` of lsst_apps with tag sims fails if lsst_sims is already installed","I have been trying to install latest tagged versions of both lsst_apps and lsst_sims that will talk to each other. This is facilitated by @danielsf releasing a tag sims using :    {code:java}  rebuild -r sims_2.2.6b sims_2.2.6 12  {code}    and therefore I was trying to install both the apps, and sims packages with the sims tag on an OSX Mavericks (OSX 10.10.4). I tried to use a personal version of anaconda (also a fresh version: 4.1.2). I have confirmed that I can install both of them if I install them in the order lsst_apps, lsst_sim (so I am up and running), but this fails if I try to install lsst_sims first and lsst_apps after that (All fresh installs using the latest newinstall.sh).    When I do get this problem, the install fails on trying to install the package   {noformat}  meas_modelfit 12.0+1  {noformat}  where it seems to fail on trying to setup the package. It may be worth noting that in the order prescribed (sims, apps), this is the first package that lsst_apps tries to install (59/62) that has not already been installed by lsst_sims  (ie. lsst_sims requires at least 58 of the DM packages to build). So, it is possible that there is some initialization step that the installer usually performs well before getting to this package, but is needing to do now. Or maybe something else is happening.    The build log is attached.      ",NULL
DM-6662,"Provide specification for in-Butler catalogue joins","Per the [February 2016 JTM|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=44138708], provide the Data Access group with a specification for ""in Butler catalogue joins"".",NULL
DM-6666,"Decision on who gets access to the Prototype Dara Access Center (PDAC)","We are going to have a PDAC set up at NCSA at the end of F16. Current plan is to give access to everyone in LSST project. We would also like to have some outside users to have access to the system to give us feedback.     Who should have the access to PDAC?",NULL
DM-6703,"community page always shown as unread after split","After having some of its posts split onto a separate topic, the [DMTN-23 topic on community.lsst.org|https://community.lsst.org/t/dmtn-23-tutorial-on-command-line-drivers/870] became stuck in a partial ""unread"" state for me: when on the ""Latest"" tab, it appears in black (not gray), no matter how often or how long I look at its posts or the posts of the linked topic.  It does not appear in the ""Unread"" tab, however.    This is obviously a very minor issue, and I'm not hurting for a fix.  But it does seem to obviously be a bug (and possibly one that needs to be reported upstream).  ",NULL
DM-6704,"ImageHeader may need to refactor","While working the unit test for Projection in java and javascript, I feel that ImageHeader.java may need some work to refactor it.  It will be better it is more resemble the Header in FITS library.  The ImageHeader has many public instance variables.  However, it does not have any utilities method such as get/set etc.       Two methods were added for running unit tests.  In the long run, if we have time, it is good to refactor it since it is public now.",NULL
DM-6716,"Summary report from LSST in Europe","I would very much like to see a report on the outcomes and discussion from the LSST in Europe meeting.  I think [~mjuric] is probably the right person to delegate this.",NULL
DM-6717,"Add meas_extensions_ngmix to repos.yaml","As summary. Necessary to run meas_extensions_ngmix through Jenkins.",NULL
DM-6718,"afw table and record should have useful str() and repr()","To see the contents of an afw table in python, you have to do something like {code}catalog[0].extract(""*""){code} which is totally not discoverable and returns a dict which doesn't print well. Much more useful would be for str() on a record to produce a pretty-printed list of the contents, and str() on a table to produce some nicely formatted summary (like a numpy recarray does, only printing a few things separated ""..."" for large tables).    I'm not sure what the best repr() output would be (certainly for a Record it should be a full dump of the contents), but currently table.repr()==table.str(), which is equally unhelpful.    Maybe this will come ""for free"" when we get astropy.table views, but it makes exploring the contents of a table a pain right now.",NULL
DM-6719,"meas_extensions_ngmix has no SConstruct","meas_extensions_ngmix has no {{SConstruct}} file.    That means that running {{scons tests}} does nothing useful.    Simply dropping in a basic {{SConstruct}} breaks:    {code}  Setting up environment to build package 'meas_extensions_ngmix'.  Failed to load required dependencies: ""ngmix""  {code}",NULL
DM-672,"remove meas_multifit dependency on meas_extensions_multiShapelet","The completion of other tasks in DM-454 should allow us to replace the use of {{meas::extensions::multiShapelet::FitPsfAlgorithm}} with {{meas::multifit::PsfFitter}}.",NULL
DM-6720,"Python coding standard is contradictory","Our Python coding standard informs us that [""{{is}} and {{is not}} SHOULD be used when comparing to {{None}}""|https://developer.lsst.io/coding/python_style_guide.html#is-and-is-not-should-be-used-when-comparing-to-none]. A couple of lines later, however, it tells us that [""{{if x}} SHOULD NOT be used when you mean if {{x != None}}""|https://developer.lsst.io/coding/python_style_guide.html#if-x-should-not-be-used-when-you-mean-if-x-none].    Since I am following the first rule, I *never* mean {{if x != None}}.",NULL
DM-6721,"Please document appropriate use of ups/*.cfg files","There is no useful stack documentation I can find which covers the {{ups/*.cfg}} files used by sconsUtils. In particular:    * sconsUtils is not listed as a documented package on the [Doxygen front page|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/].  * Searching Doxygen pulls up some docstrings in sconsUtils which indicate where it's used, but give no indication of the appropriate usage patterns.  * The Developer Guide page on [""adding a new package to the build""|https://developer.lsst.io/build-ci/new_package.html] makes only a passing reference to it.  * [~ktl] points me to the [template|https://github.com/lsst/templates/blob/master/ups/TMPL.cfg], which contains some comments. Setting aside the fact that comments in templates don't seem to be the appropriate place to store our documentation, I'm left wondering (for example):  ** Are these build or run time dependencies (the answer is build, but the documentation doesn't say that);  ** Does ""scons tests"" count as part of the build? That is, do I need to list packages which compilation doesn't require, but test execution does?  ** Does ""pure-Python dependencies"" mean ""dependencies which are written purely in Python"" or ""packages which are only depended on in Python code""?    Please provide a convenient reference in the Developer Guide and/or sconsUtils documentation.",NULL
DM-6722,"ingestCalibs doesn't respect calib repo path","Ingesting calib files with e.g.    {code:java}  ingestCalibs.py /path/to/repo/ --calibType <any> /path/to/fits/file/*.fits --validity 999  {code}    will put the calib repo (i.e. calibRegistry.sqlite3) wherever the ingest command was executed, rather than adding this to the calib repo which lies in the path specified.",NULL
DM-6724,"Ship ngmix with license","As of DM-6127, we've clarified the licensing position of ngmix. Please ensure that we're actually carrying a version of ngmix which has a license attached in our repository.",NULL
DM-6767,"Add ability to reverse the current selection in a table viewer","(If this is not already there...)    It would be very nice to have a UI element that permitted the reversal of a selection on a table.  This can be used to perform certain types of selections that the current UI does not permit (because the column-oriented selections can only be ANDed).    This is a commonly available feature in comparable systems.",NULL
DM-6782,"afw table string keyword access is much slower than asKey access","Looking up keys by string,  {code}src['somefield']{code}  is about 60x slower than pre-computing the key and using that,   {code}  key=src.schema.asKey()  src.get(key)  {code}  We should be able to cache those keywords in a dict to normalize that access speed and so that users don't have to deal with pre-defining and carrying around a bunch of keys.    I'll attach a simple ipython file that demonstrates the speed difference. The output of one run of it is shown below:    {code}  $ ipython field_key_timing.py  Small schema, string lookup  1 loops, best of 3: 214 ms per loop  Small schema, asKey() lookup  100 loops, best of 3: 3.35 ms per loop    Large schema, string lookup  1 loops, best of 3: 201 ms per loop  Large schema, asKey() lookup  100 loops, best of 3: 3.18 ms per loop  {code}",NULL
DM-6786,"Write an in-place convolution method","Currently afwMath.convolve does not have an in-place operation option, thus always requires at least double the memory. Apparently SDSS had an in-place version, and we should include that capability for LSST processing.",NULL
DM-6787,"Define ownership of the build system","Our code is built using scons, with our own set of extensions in the form of the sconsUtils package. sconsUtils is not well documented and requires ongoing maintenance (see, e.g., DM-6721). It was originally written by Science Pipelines (and, indeed, still carries a README directing queries to Science Pipelines developers), but the pipelines groups are not scoped to assume continuing responsibility for it. Most work recently has been performed by Architecture and SQuaRE, but this largely consists of relatively minor fixes: it's not clear that anybody assumes responsibility for the high-level structure and direction of this package.    The above text focuses on scons & sconsUtils, but it's possible that more dramatic evolution of the build system will be necessary during construction: this also includes investigating and leading the adoption of other tools if & when necessary.    Worth noting that this relates to, but is not the same as, the work on EUPS described in DM-6335.",NULL
DM-679,"Megacam CCD defects are not handled properly in obs_cfht","Megacam CCD defects files should be selected according to a keyword present in the image fits header (currently they are selected according to a date). Independently of the selection mechanism, there are also some questions on the validity of the current defects files. Defects footprints can be generated from the original Elixir defects files. According to Pierre Astier, for filters i and z, the defects footprints should be enlarged by 1 pixel in the y direction .",NULL
DM-68,"Qserv should fail when table not registered in CSS","""Table does not exist"" message from CSS is currently ignored and query proceeds. Example, a query ""select count(*) from t"" where t does not exist will successfully return 0 rows.",NULL
DM-682,"Unassociated sources in database","We need to understand how to deal with sources (and ForcedSources etc) that are not associated with any object. This complicates Qserv design because our partitioning scheme assumes that these tables are partitioned based on position of the associated object.  ",NULL
DM-6823,"Add new boilerplate to introduction of SQR-012","SQR-012 provides details of how the new unittests should be written, but it only gives an example of the old testing boilerplate in the introduction. It would be very helpful to have the first thing a reader sees be the new, correct, boilerplate, so they can immediately drop it into a new testing file.",NULL
DM-6825,"Define policy for handling user support and bug reports","Currently, we track bugs and issues through JIRA, which is only accessible to project members.    We also have a per-repository issue tracker on GitHub. These are open to the public. According to [DMTN-020|https://dmtn-020.lsst.io/#receiving-bug-reports] (NB still not formally approved at time of this writing), T/CAMs will monitor the GitHub trackers and transfer information back and forth between them as necessary.    Currently, the rate of tickets being filed on GitHub is very low. Since the threshold to reporting issues on GitHub is minimal, though, it seems reasonable to expect that this will increase in future. We might also expect that, as community interest in the LSST stack grows, and increasing number of less technical users will attempt to get our code working, run into problems, and file GitHub tickets that do not represent ""real"" bugs, but rather misunderstandings or user errors.    We also have the [LSST Community|https://community.lsst.org] forum. Currently, queries there are handled by interested readers on a best-efforts basis. Occasionally, they uncover issues which can be fed into JIRA. As our science community expands, the total number of support queries seems likely to quickly balloon beyond what the residents experts can handle informally. This is a problem in at least two ways: it will be hard to be responsive to the community, and we risk having our expert developers distracted into providing user support rather than working on core development. Further, sorting the important bugs from the mass of general support requests may become untenable.    Please define policies and resourcing for providing appropriate end-user facing support while ensuring that key bug reports are handled appropriately.",NULL
DM-6826,"Define commitments and resourcing for handling requests for DM support from other subsystems","Over recent months, the Camera subsystem has required substantial input from the DM team in using our codebase to process data from their “Monocam” instrument. It seems likely that requests for support from other subsystems within LSST will continue throughout construction. Who is responsible for handling and resourcing these?",NULL
DM-6827,"Clarify environment setup in Quick Start documentation","See this link for background:  https://community.lsst.org/t/python-configuration-for-qserv-qserv-configure-py-is-not-set-up-correctly/897    While the instructions in the ""Quick Start Guide"" work well, it's not clear that the ""setup qserv"" step has to be run each time to set up the environment, prior to using such tools as qserv-configure.py.  I think it would be useful to mention this, especially for users installing this without a familiarity of the eups system.  It might also be good to mention it on the ""Configuration procedure"" page.",NULL
DM-686,"update PKG_CONFIG_PATH in LSST 3rd-party table files","From Dustin's mail to lsst-data: {quote} As you may know, pkg-config is the de-facto standard for finding out what command-line args are required to compile and link a library. Unfortunately, many of our third-party packages don't add themselves to PKG_CONFIG_PATH, so pkg-config doesn't work correctly for them.  For instance, the ""gsl"" package uses pkg-config, but the eups package doesn't add its path to PKG_CONFIG_PATH, so pkg-config doesn't know about the setup gsl.  As a result, *other* third-party packages can't find it. I just hit this in building astrometry_net.  I think it's a good idea in general, because then if users build other packages while LSST packages are set up, they will use the setup LSST versions. {quote}",NULL
DM-6877,"Please document the appropriate location for tasks","On DM-6631, [~rowen] asserts:  {quote}  Command-line tasks are encouraged to be in the package that makes most sense for them, so IsrTask belongs in ip_isr. pipe_tasks is intended for tasks (command-line and otherwise) that need so many packages that they don't make sense being in a lower-level package.  {quote}    I certainly don't disagree – seems perfectly reasonable – but I've not seen it written down before. Please capture this in the developer documentation.",NULL
DM-6878,"Create pybind11 coding guidelines document template","Ongoing work on the pybind11 trial (see DM-6168) should eventually lead to nice and consistent wrapping of C++ code. This includes conventions on file / module / variable (for things like class objects) naming, import conventions, etc.  In addition we need some place to keep track of pybind11 gotchas.  This ticket aims to setup a template for such a document (as a technote) which is to be continuously updated as the epic progresses.  Reviewers of epic tickets can check against conventions set in this document to help ensure consistency.",NULL
DM-688,"meas_base exception handling not consistent with meas_algorithms","The current meas_base framework does not allow algorithms to both set values in their measurement records and set the general failure flag, since on failure, the Result object is not returned to the framework code.  SdssShape in particular is written to set values in its record even when certain errors occur.  For that reason, when one of these errors orccurs, it can be written either to have all its record values set except for the general error flag, or to only have the general error flag set.   I chose for testing purposes to not set the general error flag and instead test for failure using the disjunction of the other flags, but that is not a good long term solution.",NULL
DM-6885,"Add support for Angle to pex_config","pex_config only supports a limited number of primitive types for fields, and that list does not include Angle, which we otherwise rely on to ensure consistent angle units in our code.    This issue should either include an audit of existing configuration classes (and C++ control classes) looking for fields that should be converted to use Angle, or spawn a new issue to do that.",NULL
DM-6887,"Document the semantics of measurement algorithm flags","On [clo|http://community.lsst.org/], [~jbosch] [writes|https://community.lsst.org/t/additional-summary-flags-during-single-frame-and-centroid-processing/901/2]:    {quote}  ... This is generically true of measurement algorithms: the general flag indicates a good result, not the presence of any result ...  {quote}    This is — as that thread demonstrates — both important to properly understanding the outputs and implementation of meas_base style algorithms, and not immediately obvious. Please ensure it's documented for both users and algorithm implementers.",NULL
DM-6888,"Update lsst.codes cert on LTD Keeper deployment","Update certificate in LTD Keeper’s Kubernetes deployment with new lsst.codes cert.",NULL
DM-6889,"Improve support for non-contiguous catalogs and flag fields in Python","afw catalogs have a very Pythonic interface for contiguous catalogs, but require explicit python loops otherwise. Furthermore, flag fields require special handling. Both of these are major stumbling blocks for their use. Several options have been floated for improving non-contiguous support, including SQL-like syntax. Regardless of whether SQL is ever supported in any form, I think it would be a huge win to improve basic support, as follows:  - `array = cat.get(field, deep=True)` return a deep copy  - `array = cat.get(field, deep=False)` no change: return a shallow copy if cat is contiguous, else raise an exception  - `array = cat[field]` no change: same as `array = cat.get(field, deep=False)`  - `array = cat.set(field, array_or_scalar)` works even if cat is not contiguous, and even if `field` is a flag field  - `array = cat[field] = array_or_scalar` same as `array = cat.set(field, array_or_scalar)`",NULL
DM-6891,"make jointcal less sensitive to details of selected sources","Jointcal is currently highly sensitive to the minimum S/N cut imposed on input catalogs. Slight changes in S/N cause the relative or absolute errors to go out of bounds (on the lsstSim tests in testJointcal.py). Either those errors are too stringent, or we need to figure out how to make jointcal less dependent on the details of how sources are selected for inclusion.    It may not be the S/N cut itself that causes the problem, but rather other properties (e.g. covariance of centroid) that are related to the S/N cut. But the S/N is the easiest property to tweak.",NULL
DM-6895,"Vary number of cores being used for testing","Our colleagues on HSC have reported a non-deterministic astrometric solution being generated by the stack. That's clearly a bug, and will have to be fixed (and it might be HSC specific). However, it's notable that this varies with the number of cores being used to execute the stack. That's hard for developers to spot when running tests on their own systems, but it would be easy to catch by varying the (virtualized?) hardware environment when running automatic regression tests.",NULL
DM-6896,"remove ButlerFactory and change all the packages that use it to use Butler() directly","ButlerFactory is vestigial and should be removed (per KT).   There are maybe a dozen packages that still use it and these should be changed to use the Butler initializer directly - either 'new' style Butler(outputs=...) or 'old' style Butler(root=..., mapper=...., etc)",NULL
DM-6901,"Remove PsfCandidate code from second moment star selector","The second moment star selector makes PsfCandidates merely to check if the candidate image contains NaNs. This duplicates existing code in BaseStarSelector and can be removed. See [~rhl]'s [comment|https://jira.lsstcorp.org/browse/RFC-198?focusedCommentId=50768&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-50768] in RFC-198  ",NULL
DM-6906,"Update DateTime to reflect 2017-01-01 leap second","http://maia.usno.navy.mil/ser7/tai-utc.dat has already been updated.",NULL
DM-691,"Migrate tutorial on git usage from trac/wiki to Confluence","Migrate the git tutorial on the trac/wiki (https://dev.lsstcorp.org/trac/wiki/GitDemoAndTutorial) to Confluence, as a chapter in the DM Developer Guide. Ensure that the migrated content is current and accurate; embellish where necessary, and omit historical content if it is not relevant. ",NULL
DM-6910,"Tab panel issues","There are several issues with tab titles:  - Sometimes we don't want them to be resized at all (like in Images panel since the tabs are all fixed at begining)  - When titles are resized, their size is proportional to the length of the title, use fixed-size tab length instead to make sure no tab is too long.    Please add other tab panel issues here.",NULL
DM-6911,"Ingesting tile compressed FITS files","When I try to read in FITS tile compressed calexps (created with either cfitsio's fpack or astropy.io.fits), the files appear to be read correctly, but something from the headers appears to be mangled. I have not tracked down the problem in detail, but running jointcal on a butler repo containing the tile compressed calexps results in bad fits, while running it on the uncompressed files works fine.    The problem likely lies in how we are extracting header information that may be modified as part of the tile compression process (compressed HDUs are actually binary tables, not images).    This ticket came out of a [discussion on community|https://community.lsst.org/t/fits-tile-compression-support-in-the-stack/909].",NULL
DM-6912,"Please add a way to interpret opaque sourceId in terms of visit/tract/patch/ccd/...","The mappers provide a way to pack useful information into sourceIds (e.g. visit/raft/chip or tract/patch/filter), but no way to do the unpacking.    Please provide a unified interface that all mappers realise to do this -- my trial implementations use `splitId` and I provide a couple of examples below.  If the way that the packing is done is standardised (in terms of variables of numbers of bits per field this might be done just once in some base Mapper.    As implied, you need to know a bit about where the sourceId came from (visit? patch?), and this is something that the user knows as they got the data from the butler;  this implies that this function should be findable by the same name (including butler aliases etc.)    We also need a corresponding routine to unpack the exposureId    My implementation uses a MapperInfo class, but putting it directly into the mapper would probably be preferable.  Note that `splitId` accepts numpy arrays (or afwTable columns), and optionally returns a dict;  both features have proved useful.    {code}         @staticmethod         def splitId(oid, asDict=False):             """"""Split an ObjectId into visit, raft, sensor, and objId""""""             objId = int((oid & 0xffff) - 1)     # Should be the same value as was set by apps code             oid >>= 16             raftSensorId = oid & 0x1ff             oid >>= 9             visit = int(oid)               raftId, sensorId = int(raftSensorId//10), int(raftSensorId%10)             raft = ""%d,%d"" % (raftId//5, raftId%5)             sensor = ""%d,%d"" % (sensorId//3, sensorId%3)               if asDict:                 return dict(visit=visit, raft=raft, sensor=sensor, objId=objId)             else:                 return visit, raft, sensor, objId  {code}    Here's an example that splits a coaddId from HSC (there may be some irrelevant details in here, I hope not...).  The mapper is the mapper in use (his is a static method of HscMapperInfo)  {code}          @staticmethod          def splitCoaddId(oid, asDict=True, hasFilter=True):              """"""Split an ObjectId (maybe an numpy array) into tract, patch, [filter], and objId.              See obs/subaru/python/lsst/obs/hscSim/hscMapper.py""""""              mapper = HscMapperInfo.Mapper                try:                  oid[0]              except TypeError:                  oid = [oid]                oid = np.array(oid, dtype='int64')              objId = np.bitwise_and(oid, 2**mapper._nbit_id - 1)              oid >>= mapper._nbit_id                if hasFilter:                  filterId = np.bitwise_and(oid, 2**mapper._nbit_filter - 1).astype('int32')                  oid >>= mapper._nbit_filter                    filterName = np.empty(oid.size, ""a6"")                    if filterId.size == 1:                      filterId = [int(filterId)] # as you can't iterate over a length-1 np array                    for fid in set(filterId):                      name = afwImage.Filter(int(fid)).getName()                        filterName[filterId == fid] = name              else:                  filterName = None                patchY = np.bitwise_and(oid, 2**mapper._nbit_patch - 1).astype('int32')              oid >>= mapper._nbit_patch              patchX = np.bitwise_and(oid, 2**mapper._nbit_patch - 1).astype('int32')              oid >>= mapper._nbit_patch              add = np.core.defchararray.add # why isn't this easier to find?              patch = add(add(patchX.astype(str), "",""), patchY.astype(str))              patch.shape = patchY.shape # why do I have to do this?                tract = oid.astype('int32')                if oid.size == 1:     # sqlite doesn't like numpy types                  if filterName:                      filterName = str(filterName[0])                  tract = int(tract)                  patch = str(patch[0])                  objId = int(objId)                if asDict:                  return {""filter"" : filterName, ""tract"" : tract, ""patch"" : patch, ""objId"" : objId}              else:                  return filterName, tract, patch, objId  {code}  (this is called splitCoaddId, but there's another level of indirection to call it as appropriate)",NULL
DM-6913,"Please document the semantics of object identifiers","On [clo|https://community.lsst.org], [~rhl] [mentions|https://community.lsst.org/t/presentation-of-coadded-image-data-to-users/913/6] that object identifiers in source (object, etc) tables contain useful information beyond merely being a unique id. Please capture this in the stack documentation.    It may also be appropriate to describe this in the DPDD. I leave that to those wiser than myself (i.e. [~zivezic]) to opine on.",NULL
DM-692,"Document how measurements are performed. ","Create a chapter for the LSST SWUG that describes how the various kinds of photometric measurements are performed in the LSST Stack. Describe how these measurements are represented in the output catalogs that are created in the processing pipelines. ",NULL
DM-6920,"Yellow dot higlight fails to display the right data point in XY plot ","When a user highlight a row in a table, a yellow dot display on the XY plot to indicate where is actually the point corresponding to the row selected.  The animation fails to display the right datapoint in many cases (rounding problem, out of boundary/viewport plot limit?).  Steps to reproduce:  Search for m31 in AllWISE source catalog  with 100'' radius search. This will return a table with 6 rows. Select the second row designation = 'J004252.53+411541.6', the yellow dot will go in a place where is no data point in XY plot show. Although it seems to be the right RA, DEC coordinates in XY plot, no regular dot is shown, only the yellow one.     Please investigate.    Xiuqin    In my situation, the above issue didn't happen. It is different issues:    Clicking on the dot highlight a different dot. I think maybe it was because the Dec value is negative? (try m16)",NULL
DM-6921,"Investigate support of GWCS in Firefly","We may want to look into whether/how to support [gWCS|https://github.com/spacetelescope/gwcs] within Firefly, as it appears this is the system that will be used for JWST imagery.  (I suggest this from a community-support perspective, not an LSST one; at this time it does not appear that gWCS will be used in LSST.)",NULL
DM-6926,"shared_stack.py failure due to missing mkl-service","Per [clo|https://community.lsst.org/t/using-the-shared-stack-py-utility-for-weeklies-installation/923], the {{shared_stack.py}} script is failing because it executes a {{conda remove mkl-service}} and {{conda}} complains that {{mkl-service}} isn't installed.    This seems to be a bizarre change in behaviour of Anaconda (the versions of both Miniconda and Anaconda we're working with haven't changed, but the package selection has).",NULL
DM-6927,"Butler should support flags for writing source tables","[~krughoff] discovered an interesting bug: several tasks that save source catalogs using {{butler.put}} want to specify a value for the flags argument of the FITS writer. They do this by passing {{flags=...}} to the {{butler.put...}}. However, the butler assumes that the flag argument is another data ID key:value pair. (Normally this extra item is ignored, which is why we have only now noticed it, but in some cases it can cause an exception).    We might be able to manage without passing the flag at all, but I worry that is not a good solution. I'd be much happier having some way to pass the flags argument through to the persistence code.  ",NULL
DM-6929,"Image Viewer: Support circular selection","Support circular selection by choosing a point and dragging the mouse across the image. The callbacks and functions supported for rectangular selection should be also supported for circular selection.      (Circular region selection is a part of Paul O'Connor's visualization wish list and is a long promised feature.)",NULL
DM-6930,"Define upper size limit for objects in DPDD","From a [~rhl] comment in LDM-151:    {quote}  We need to define what we'll do with objects that are too large to fit in a single patch, even with overlaps  {quote}    The simplest proposal is that we simply won't try to process them as single objects, but I think this is a question whose answer belongs in the DPDD.  If we need to process objects up to a certain scale, that either puts a requirement on our patch sizes or will require special code to process them.",NULL
DM-6931,"Clarify requirements on per-snap processing","The DPDD does not currently list any measurements that result from measurements on individual snaps or differences between snaps.  However, I have heard plenty of informal discussion about such quantities.  We need to clarify what we will measure on snaps (if anything), and codify this in the DPDD.  ",NULL
DM-6932,"Clarify rules on using previous DRs or AP results in the next DRP.","I wrote in the LDM-151 redraft that decisions on how to build a reference catalog for use in DRP will depend on DM leadership clarifying the rules for reusing previous data release catalogs when processing the next one, and [~rhl] commented:  {quote}  Is the need for this guidance captured formally anywhere?  {quote}  And hence I'm assigning this back to [~rhl], to identify uses cases where DRP would want to rely on previous data releases or AP results, and to consult with the rest of the DMLT on how to deal with any provenance concerns this would raise.    Personally, I don't think we'll need a previous data release to make good-enough reference catalogs for PSF estimation and astrometric calibration, and I any help they'd provide in photometric calibration is outweighed by the provenance problems that'd raise.  However, I do think we'll want to use the latest AP SSObject database in DRP MOPS.  And I can't think of any other cases where this comes up.",NULL
DM-694,"Document Camera Geometry for the SWUG","Develop a chapter for the LSST SWUG on camera geometry, with a high-level introduction to how the geometry is represented in software. Describe the high-level steps that are necessary to support a new camera in LSST software. Another page (probably in the DM Developer Guide) would provide a step-by-step ""how-to"" of building software for a new camera; this task is to provide the background and context necessary to understand the concept of camera geometry as used in LSST software. ",NULL
DM-6943,"in2p3 cluster crashes","The in2p3 cluster crashes after it has been running for several hours. Many, if not all, of the nodes crash including the czar. This has been seen more on nodes 125-149. ",NULL
DM-6946,"Strange results with near neighbor queries.","Queries similar to following give strange results.    * SELECT COUNT(*)  FROM Object o1, Object o2 WHERE qserv_areaspec_box(...) AND scisql_angSep(o1.ra, o1.decl, o2.ra, o2.decl) < 0.015;  returns about 76 million   * SELECT COUNT(*)  FROM Object o1, Object o2 WHERE scisql_angSep(o1.ra, o1.decl, o2.ra, o2.decl) < 0.015; returns about 52 million    The second query should return many more results than the first as  it should not be limited to a particular region of the sky, so something is not right. Also, it is important that distinct works properly with these queries.",NULL
DM-6947,"Firefly typos, broken help links and label fixes","There are a couple of misspelling label, wording and broken help links to be fixed and changed:  1- Help links:  * ""?"" icon at upper right links to the wrong page --   * Context-sensitive help in the table shows user a blank page. -- updated Atlas and Gator the help point to ""onlinehelp/irsaviewer/#id=catalogs.columnsfilters"" WMI    2- Labels/Wording    * -Strange title: ""This is a title"" when no string or null is passed to API table- [Should be fixed in [DM-6948], use showTitle:false in option 'showTable())] --DONE in Atlas and Gator WMI  * In Edit Table Options, the Reset tooltip has a misspelling: ""defauls""-->""defaults""  * In Layers window, after adding a marker, change ""drage"" --> ""drag""  * -The units on the plot are indicated with a comma, e.g. “dec, deg”, should be ""dec (deg)""- [ To be fixed in [DM-6954] ]  * There is now a ""flip"" option. Change to ""reverse""  * 'Save' icon tooltip misspell 'Fits' -> 'FITS'. Check elsewhere too.  * The tab called ""Image Meta Data"" should be changed to ""Images""  * WISE images are labeled ""WISE: 1b, B1"". Change to a more WISE correct label ""WISE: L1b, W1"" (need to be confirmed).    3- Layer dialog  -The Layers dialog has changed during migration. The title is different- MOVE to [DM-7116].   Arcsec and arcmin are still misspelled.   The ""click and drag"" statement has changed.   ""Color"" and ""Delete"" are no longer underlined. The color picker has changed. The title is different, and there is no cancel button anymore.    4- The order of the icons in the toolbar has changed.",NULL
DM-6948,"API default options changed during migration","Please restore the default options when calling the API that was available before migration from the API such as:  * The grid overlay comes up by default. Turn it off by default.   * The degree of magnification of the image is different than before.  * Units should be turned on by default  * showTitle default to false in showTable() options.    I think all could be addressed by setting the parameters in Gator call.   ",NULL
DM-6950,"Improve label on compass and grid ","Remove white background from compass and grid overlay labels and make those labels to have same color as the rest of the overlay grid or compass.",NULL
DM-6951,"New IRSAViewer doesn't look like before migration","Couple of problems are related to the new IRSAViewer and needs to be fixed:    *-Image preview button seems to not work at first. The first thing you see is the search image panel. After a while, the image does come up.-FIXED    ** -Show a spinning wheel instead.- FIXED  * -Missing image filename, search coords, epoch, zoom level at top of page-.    ** get the same info as in OPS --FIXED  * -Single Image is labeled with ""Tile View"" at top left instead of information detailed.- NOT a bug,  would be nice to show with single tile   * -From Atlas result page, the first image in external irsaviewer shows the correct ""fit-to-width"", the second, third... all shown with zoom=1, even the plotting option defined as fit-to-width- --FIXED",NULL
DM-696,"Revisit emptyChunk list","Empty chunk list is currently installed in build/dist/etc, and it is ""global"", e.g., there can be only one such list at any given time. That prevents us from talking to more than one database, plus, switching to a database with a different empty chunk list can result in very confusing messages. We should at minimum fix that.     Further thoughts from Daniel are pasted below.    Ideally, we would rebuild it upon any changes to the dirTable. There was code in indexing.py(I think?) that was a placeholder attempt of code that could generate it.    There are a couple ways of generating it:  # from the range of numbers defined by the min and max chunk number, filter out chunks determined to be non-empty by the existence of dirtable_NNN tables. This is what my scripts did, and it was a hassle to get it to work.  # Do a special all-chunks query (count(*) or similar), but don't squash on errors-- add them to the empty chunks list.  # Populate the empty chunks file when you create a database, or when a czar becomes aware that a database exists. Every time you load data, you know what chunks you are creating, so remove those chunks from the empty chunks file/list. The czar always checks the db entry in css to see if its empty chunks file is out of date. It is impossible to delete partitioned rows, so chunks never become empty after being non-empty.  # Compute a non-empty chunk list from the sec-index list: select distinct chunkId from blah (will go obsolete soon) and create a hash-table or std::map, and use it. The czar can compute and cache this the first time the db is accessed.",NULL
DM-6965,"Remove variance rescaling in coaddition","The variance rescaling we're doing on the coadds (to try to account for missing covariance) is currently causing the HSC weak lensing team a lot of problems, because they affect every uncertainty value we produce and they're hard to include in simulations that otherwise look a lot like the data.  I think we need to disable this - and probably take it out of the codebase entirely, just to be safe - before the next major HSC release, even if we don't have anything to replace it.  I think it's doing more harm than good.  That will require changing coadd detection thresholds to compensate, I think.",NULL
DM-697,"JIRA sometimes thinks Perry is Robyn","The 4th comment on DM-445 is actually from Perry, but JIRA labels it as being from Robyn.  We previously saw another instance of this, but I don't recall where.  Perry, do you have anything to add?  Was there anything different between that comment, and the one that followed it (which is correctly attributed to you)?",NULL
DM-6979,"Firefly strategy response to RFC-193","RFC-193 has been adopted.   ""we propose to replace the current World Coordinate System handling and XYTransform code in the LSST stack with Starlink AST, using a new API that is under development in DM-3874 (that API will have its own RFC). ""     How does Firefly respond to this and display images as accurately as possible?   ",NULL
DM-6981,"Add column setters for Flag types in catalogs","It should be possible to set Flag columns with bool arrays or scalar values.    This should just be a matter of adding a {{set}} overload or two around line 100 of {{specializations.i}}.",NULL
DM-6993,"Use KD Tree for matching in MatchOptimisticB","The current {{matchOptimisticB}} algorithm uses a sophisticated search to initially solve for the mapping between the source catalog and the reference catalog using only the brightest ~hundred stars, but then verifies this solution by matching the entire contents of the source and reference catalogs by a brute-force search. This search is repeated several times as the fit converges. In crowded fields these catalogs can be very large, and matching overall can take ~30 minutes per CCD.    Replacing this brute force search with a KD tree should provide a functionally identical result at greatly reduced computational cost. ",NULL
DM-6994,"Enhancements to HTM indexed reference catalog system","In the process of implementing DM-6969 and DM-6352 a few things have emerged that would be nice to have in the new HTM indexed catalog system:    1) If one ingests a reference catalog twice, one gets duplicate entries from the reference object loader. This is a side effect of allowing extending an existing catalog with additional data. However, it is not clear if we need that feature, and it may be more work to support it while preventing duplicates than it is worth.    2) If one ingests catalogs with magnitude field names such as ""rmag"", one ends up with reference catalog filters with names such as ""rmag"" instead of ""r"". It would be nice if there was a way to avoid this, e.g. by having the ingest system support a list of desired filter names, and work from there to identify columns. This is definitely an issue for the text catalog reader, and may also be an issue for the FITS catalog reader.",NULL
DM-6995,"cosmetic fixes for LDM-151","I scanned parts of ldm-151 (draft branch) and ended up making few minor fixes, see branch u/jbecla/201607-fixes, feel free to accept them or ignore :). Also, I noticed few other things that I haven't fixed:    * consider refreshing author list, eg I know Frossie spent  time contributing  * SUI --> SUIT?  * break figure 1 into two and turn 90 degrees?  * figure 2 turn 90 degrees?  * ""The data products produced by Alert production are given in table ."" -  missing table number. This is on page 21  * ""this would likely only be able to sample the selection function over a subset""  this sentence seems unfinished  * ""A catalog of Sources with measured features (as described in ??).""  Maybe you want to refer to LDM-153 (db schema)?  * OCS introduced on page 17, no need to reintroduce on page 23?  * page 24: ""Source detection to the 5σ limit of"" something is broken here",NULL
DM-6997,"Fix titles and names in Code of Conduct document","The titles and names in the code of conduct document are outdated.  Please fix.",NULL
DM-7,"Enable C++11 support","Enable C\+\+11 support on all platforms. As far as I know, we're currently blocked on knowing whether the stack builds with libc++ with clang (required for OS X support).",NULL
DM-700,"Detection Footprint's growing is hard-coded to ""slow, not very isotropic""","In detection.py (8.0.0.0) I found {code} fpSet = afwDet.FootprintSet(fpSet, nGrow, False) {code}  I think this should obey the config parameters, i.e. {code} fpSet = afwDet.FootprintSet(fpSet, nGrow, self.config.isotropicGrow) {code} ",NULL
DM-7002,"Only list needed imports in persisted configs","When a config is persisted it may include many irrelevant import statements. There are at least 3 different pieces of code that output import statements to the config, and some of them should either be disabled or moderated.    I have attached a horrid example that includes nearly 100 imports, including at least one private library, which is not safe. None of the initial imports is required (other than the first one, used for type checking).    This Jenkins log shows a failure whose exact cause is unknown, but it was triggered by the excessive imports:  https://ci.lsst.codes/job/stack-os-matrix/compiler=gcc,label=centos-7,python=py2/13307//console    Here are links from [~ktl] to various bits of code that write import statements when saving a config, and he reports that there is also a third place where imports are written. The first one is probably responsible for writing out the unwanted configs in question:  - https://github.com/lsst/pex_config/blob/master/python/lsst/pex/config/config.py#L609-L611  - https://github.com/lsst/pex_config/blob/master/python/lsst/pex/config/configurableField.py#L236-L237    I suggest that only the minimal imports be written out, perhaps only those that were seen in any config override file.",NULL
DM-701,"Add additional capabilities to buildbot CI","The new continuous integration framework should add support for :  * updating stack doxygen documentation for a master DM build. (DM-951)  * running nightly small-sized- and weekly medium-sized-  dataset end-to-end runs for TBD processes such as processCoadd (DM-875)  * supporting builds on OS other than RedHat 6  * supporting the Clang C++ compiler (in addition to GCC)     ",NULL
DM-7013,"CoverageWatcher.js improvements","Currently, whenever new (or filtered) catalog table is received, we send a request to the server to obtain central and/or corner points for each catalog row. (Search for doFetchTable call in CatalogWatcher.js) These points are then used to compute central point and maximum radius to get the coverage image. (See CatalogWatcher:computeSize.)    For doFetchTable request, we a get a few columns of the WHOLE table, which considerably slows the system when catalog table is huge.     A big improvement would be replacing doFetchTable call with a server call, which takes catalog table request and calculates central point and radius on the server.    Further optimization – if needed in future – could be adding central point and radius into table meta in prepareTableMeta of the search processor when we know it (cone search) or it is easy to calculate (polygon search).         ",NULL
DM-7023,"EUPS numpy package can't assume SCons python is relevant python","DM-6312 might imply that the python being used to run SCons is not the same as the python we want to use when running the code. Update the EUPS cfg file to use python from the PATH rather than SCons python.",NULL
DM-7027,"Region issues","There are several region issues I have found when comparing old and new API:    1. The default coordinate system when ""pure numbers"" are used should be pixel positions on the original image.     The [spec](http://ds9.si.edu/doc/ref/region.html) says the following:   ""...the default system is implicitly assumed to be PHYSICAL. In practice this means that for IMAGE and PHYSICAL systems, pure numbers are pixels.""  'If no coordinate system is specified, PHYSICAL is assumed.'    More explanation can be found [here](http://hea-www.harvard.edu/RD/funtools/regcoords.html)    2. PHYSICAL coordinate system does not mean screen pixels. The question is whether we can always use image pixels.    _If wcs is ""physical"", WCS is the pixel coordinate system of the original image, which may be different from the pixel coordinate system of the current image, if the current image is the result of an imcopy or other geometric transformation operation. In the ""physical"" coordinate system the ltv, ltm and the axis attribute parameters wtype, axtype, units, label, and format may be edited, but the FITS parameters crval, crpix, and cd cannot_. [reference](http://stsdas.stsci.edu/cgi-bin/gethelp.cgi?wcsedit)    3. When used in API, the actions are dispatched one after another. For example, dispatchCreateRegionLayer might be issued before image plot has finished loading. Can we make regionCreateLayerActionCreator wait for image with plotId to finish loading?     More details: If I create a region layer with a few regions in image coordinates right after firefly.showImage, two errors are loaded to console:   doOnAppReady: uncaught TypeError: Cannot read property 'getImageCoords' of null(…),   RegionFactory:602: Uncaught (in promise) TypeError: Cannot read property 'getImageCoords' of null(…)    4. When a region is selected, yellow dashed line appears around it. On zoom the line does not change color.    5. Can not add empty region. Line 124 of the test script.    6. Can not add or delete regions after the first one. (Load test script, click Show Regions, then Add Region, line 121 of the test script) When there is one region in the layer and you are adding another region with a different position but everything else the same, the added region is perceived to be the same.    7. It's possible to select regions only in in the first region layer added. In the other layer, you can sometimes see the selection blinking, but it does not stay.    8. If region format is wrong, for example  'image; box 40 400 72 72 # color=blue' instead of  'image; box 40 400 72 72 0 # color=blue',   the region is silently ignored, no warning or error is logged to console.    9. We need region selection action to support callbacks on region selection. (Currently the selected region is ""calculated internally based on the distance between the region and  mouse readout.)",NULL
DM-7041,"Update post-qa to submit new JSON from validate_drp measurement API","DM-6629 introduced a new measurement API. The {{post-qa}} app needs to be updated to shim and upload this JSON to the SQUASH API.    Note that this ticket may also be dependent on related updates to the SQUASH API itself.",NULL
DM-7042,"Introduce PEP8 naming and new license style to validate_drp","Change validate_drp to use new-style license information (RFC-45) and PEP-style names (RFC-97).    On the matter of RFC-97, which is still flagged, SQuaRE does have a waiver to use PEP8, though validate_drp is also properly a part of the Stack. This will need to be clarified before work can begin.",NULL
DM-7043,"Update SQuaSH database model and JSON API with concepts from validate_drp measurement API","In DM-6629, a new measurement API was introduced into validate_drp that established a JSON data model for metrics, specifications of metrics, measurements, and general blob datasets. The intent of that work is to enable rich plots in the SQUASH dashboard, with access to data behind measurements. The new data model also clarifies the subtleties of metric specifications (filter dependence, and dependence on other specifications). This ticket will incorporate validate_drp’s new data model into the SQUASH database and API.    Also related to DM-7041, which will update the post-qa tool that submits validate_drp json to the SQUASH API.",NULL
DM-7045,"Support reduction of i2 observations","This is a port of [HSC-1397|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1397] to support reduction of observations with the new HSC-I2 filter.  There is already support for reading that data, but we need a few config tweaks to allow processing it.",NULL
DM-7052,"Change ProcessResponseData to work with latest xrootd","It looks like getting things working again is simply a matter of adding XrdSsiRequest::PRD_Normal as the return value for void QueryRequest::ProcessResponseData(char *buff, int blen, bool last).    To get it working as desired will take more work. If XrdSsiRequest::PRD_Hold is returned, and then restarted later, then ProcessResponseData(char *buff, int blen, bool last) will have exactly the same information as the first time, correct?    I suspect what I will do is have QueryRequest remember if the last call triggered a large response and if 'not last' then increment a semaphore or something and return XrdSsiRequest::PRD_Hold. There might be a race condition between returning XrdSsiRequest::PRD_Hold and the call to XrdSsiRequest::RestartDataResponse(), and I'm not sure what would happen if XrdSsiRequest::RestartDataResponse() is called before XrdSsiRequest::PRD_Hold is returned. It might take some shenanigans to ensure that the semaphore is updated after the return statement.    There's also InfileMerger::_largeResultPool which will no longer serve a purpose.    xrootd specifics  http://xrootd.org/doc/dev42/ssi_reference.htm#_Toc452741870  ",NULL
DM-7058,"Generate useful plots of jointcal results","JointcalTestBase currently generates a few different plots (old vs. new WCS quiver and heat maps, relative and absolute RMS histogram) to summarize the fitting results. After showing these to people and some discussion, we came up with a number of other plots that would be very useful to help understand the fitted WCS that jointcal produces. Since figuring out how to produce some of these plots may be tricky, or may be best done after the new WCS system is in place, I'm making separate stories for them.    It also would be good to have an external plotting framework that lives outside the tests.",NULL
DM-7064,"Refactor the id parsing code and make it easier for reuse outside the CmdLineTask framework","Some scripts aren't suited to being full command line tasks but need bits of the task infrastructure, for example, the id parsing {{IdValueAction}}. Reusing it [seems to require some dirty hacks|https://github.com/lsst/skymap/commit/45734d2c4686f957e92e68765c1d2b4585938bff]. Please refactor and provide an easy to use version.     ",NULL
DM-7085,"Intermittent failure when installing through Conda","The Conda install is failing for the Sims team intermittently. There is not enough detail to follow up yet. In the future it's recommended to run 'conda info' after any failures to capture details about the problem and report it.    It is a known issue that the Conda repositories are constantly pinged during installs and will sometimes fail. But it's unknown if this is what is causing these failures.",NULL
DM-7086,"monitor full stack doc build for ""freshness""","Monitor and generate notifications if the full  stack docs haven't been updated for some reasonable time period.  Say, ~1day.",NULL
DM-7087,"stack doc deployment should not depend on buildbot account","At present, the doxygen builds are being made available https://lsst-web.ncsa.illinois.edu/doxygen/, which is mapped to {{/home/buildbot/public_html}} on lsst-dev.    This was being accomplished by rsyncing the doc builds over ssh from the {{lsstsw}} account to the {{buildbot}} account.  The {{create_xlinks.sh}} script needs to be updated to simply install into a local path.    [~bglick] has been requested to remap https://lsst-web.ncsa.illinois.edu/ to {{/home/lsstsw/public_html}}.",NULL
DM-7089,"Image plot with multiple layers differ from ops","Image plot with multiple layers incorrectly apply selection and filtering to all of the layers instead of just the active one.    Steps to recreate in Firefly:    - catalog search, m31 -> radius 100arcsec  - catalog search, m31 -> radius 200arcsec    - select a source in the outer circle of the '200' catalog, notice one is selected from the '100' catalog as well.    - -both layers are acting on the same mouse click-.    - _Sort a table first_. Then using image selection tool, select an area, then filter:  the filter *fails*. Points are outside the selected area.  ",NULL
DM-7092,"Please make weeklies weekly","Our ""weeklies"" are currently tagged on an ad-hoc basis, often several weeks apart (at time of writing, the latest — {{w_2016_28}} — is 3 weeks 6 days old). Please arrange for them to automatically be built on a predictable weekly cadence.",NULL
DM-7093,"Remove unused inPlace argument from PropertyList","In DM-1972 [~ktl] says:  {quote}  It looks like we don't actually use the inPlace argument to the PropertyList add and set methods anywhere in the stack besides the daf_base unit tests. I think it was originally put in to accommodate the addition of COMMENT and HISTORY items that might want to be placed at the end of the PropertyList instead of grouped together, but the implementation actually still groups all of these items together, just at the end of the list instead of where they were. If this is not actually useful, the argument could be removed and SWIG presumably appeased.  {quote}  This removal is no longer needed to appease SWIG but the code can usefully be removed and a test branch for this was created as part of DM-1972. This ticket will be used for the actual removal.  ",NULL
DM-7095,"Pilot Sphinx/Breath/Doxygen-generated API docs for daf_base","Since daf_base is low in the dependency tree and dominated by C++ APIs, it is an excellent candidate for exploring Sphinx-based package documenation for C++ with breathe and doxygen. We will also see how astropy’s automodsumm sphinx extension works with SWIG’d Python APIs.    This prototype will build upon the system developed in DM-7094.    Lessons from this prototype will be fed into a ""Pipelines Documentation Implementation Planning"" tech note in a follow-up ticket.",NULL
DM-7096,"Initial draft of Pipelines Documentation Implementation Plan technote","This technote will present an implementation plan for migrating the LSST Stack’s documentation from doxygen to sphinx based on experience in DM-7094 and DM-7095.    This technote will also cover initial plans for content strategy.",NULL
DM-7097,"Develop README, CONTRIBUTING templates for stack packages","In conjunction with deploying a Sphinx-based stack documentation system, we should not neglect the other types of documentation in GitHub repos. This ticket will develop templates/systems for the following, leading to an RFC:    - README  - CONTRIBUTING  - GitHub summary line  - Issue/PR templates https://github.com/blog/2111-issue-and-pull-request-templates",NULL
DM-7098,"Add two more env variables from jenkins in the JSON gerenated by post_qa","The JSON generated by post_qa should include two more fields required by DM-6992 obtained from the following env variables in Jenkins    ""ci_name""   from ""PRODUCTS""  ""ci_dataset""  from ""dataset  ""ci_label"" from ""label""    Example of env variables available from Jenkins     https://ci.lsst.codes/job/validate_drp/177/dataset=cfht,label=centos-7,python=py2/injectedEnvVars/  ",NULL
DM-7099,"Improve astrometry-related flags and output","From [this conversation on Community|https://community.lsst.org/t/additional-summary-flags-during-single-frame-and-centroid-processing/901/2], there are a number of values that would be useful for selecting astrometric sources. This epic encompasses those new calculations and source catalog outputs.",NULL
DM-7100,"Add blended/isolated flags","It would be useful to have a pair of flags that describe the ""blendedness"" of each source. As described in [this community post|https://community.lsst.org/t/additional-summary-flags-during-single-frame-and-centroid-processing/901/2], a post-deblender possibility would be:    * blended: deblend_nChild > 0  * isolated: parent == 0 and not blended    while in context where we don't run the deblender, we could instead set:    * blended: len(footprint.getPeaks()) > 1    It's not clear whether we need to check the number of peaks post-deblender, but this should be investigated.",NULL
DM-7101,"Output centroid covariance","To identify good sources for astrometry and for jointcal's internal calculations, we need to have the covariance of the centroids. This should be computed as part of the normal centroid calculations.    Once we have this, we can also use it to possibly set other shape-related flags (e.g. a ""centroid quality"" flag, incorporating the determinant of the covariance matrix).",NULL
DM-7102,"Add centroid reliability flag or improve Centroid_flag behavior","The current {{base_SdssCentroid_flag}} does not incorporate as much detail about the ""goodness"" of the centroid fit as I would like. For example, for a random HSC image, I find a non-zero list of objects with centroid_flag False and interpolated, saturated, and/or edge True:    {code}  ipdb> dataRef.dataId  DataId(initialdata={'taiObs': '2006-06-02T09:07:28.22', 'extension': 13, 'object': 'D3', 'visit': 850587, 'filter': 'r', 'state': 'p', 'runId': '06AL01', 'tract': 0, 'date': '2006-06-02', 'ccd': 12, 'expTime': 300.181}, tag=set([]))  ipdb> cent = src['slot_Centroid_flag']  ipdb> sat = src['base_PixelFlags_flag_saturated']  ipdb> interp = src['base_PixelFlags_flag_interpolated']  ipdb> edge = src['base_PixelFlags_flag_edge']   ipdb> (sat & ~cent).sum()  52  ipdb> (interp & ~cent).sum()  173  ipdb> (edge & ~cent).sum()  5  {code}    I do not have any cases where the centroid values or sigmas are non-finite and Centroid_flag is False, but those were claimed to have been found in some CFHT data in the past.    I would propose that a {{Centroid_isReliable}} (or similar name) flag be defined as follows, with fields removed if they are already incorporated into {{Centroid_flag}}.    {code}  all(  not centroidFlag,  isfinite(centroid),  isfinite(sigma),  ok covariance (somehow defined),  not saturated,  not interpolatedCenter,  not edge  )  {code}",NULL
DM-7109,"remove install_name_tool hack from boost's eups.pkg.cfg.sh","As pointed out in the discussion in SIM-1314, libboost_python.dylib no longer builds against libpython.dylib, so the install_name_tool hack can (probably) be removed from eupspkg.cfg.sh",NULL
DM-711,"Request: Upgrade astrometry.net to release 0.48","Release 0.30 is from 2010.  Specific changes that we benefit from include: * (a) fixes to eups table file to not set CFLAGS.  This was a mistake and it continues to cause trouble with builds. * (b) fixes to the build system giving more control over setting the include and library paths, making it easier to avoid conflicts between system versions and 'setup' versions. * (c) the ability to use the 'setup' gsl rather than a shipped subset of gsl. * (d) the ability to use ""multi-index"" files, with the ability to unload and reload them from memory to control resource use. * (e) the ability to redirect logging.  Of these, (a) fixes a small but annoying problem, (b) and (c) are fairly minor, (e) is nice-to-have (Astrometry.net log messages are captured as {{pex_logging}} messages), and (d) is the big win.  Currently, the {{astrometry_net_data}} files contain multiple copies of the reference stars, and these are loaded into memory and used independently.  With the multi-index changes, we can eliminate this duplication, increasing the speed and decreasing the on-disk and in-memory footprint of {{astrometry_net_data}}.  The multi-index work also moves us toward refactoring {{meas_astrom}} to separate the tasks of storing reference stars (for astrometric and photometric calibration) and solving astrometry.  Changes to the LSST Stack to accommodate release 0.48 have been made as part of [#2481|https://dev.lsstcorp.org/trac/ticket/2481].  These include a couple of minor changes adapting to changes in the Astrometry.net API, plus a larger refactor to use multi-index files and more carefully manage resources.",NULL
DM-7110,"Update python future package","Python future 0.16 is now out. We should update to that version before we begin to use it in earnest.",NULL
DM-7112,"Fix /en/latest LSST the Docs redirects on Fastly","The addition of DM-5894 (courtesy redirects of directory objects) broke the /en/latest/* -> /* redirects for old RTD links.",NULL
DM-7113,"Improve query generation performance with sqlalchemy","While doing timing of various large queries in my AP prototype script I noticed that for some of them significant time is spent in the script itself (CPU time is approximately wall clock time). Profiling shows that most of that time is in sqlalchemy ""query compiler"". With large query szie (few MBs) this is probably reasonable but I still want to avoid it if possible to get more useful time estimates. Will try to rewrite my script to not use sqlalchemy query mess and instead producing query test myself.    Another reason for this change is to speed up my prototype, right now it runs slower than real life mostly because of CPU time in sqlalchemy.",NULL
DM-7114,"Guard against assertion failure","measureCoaddSources can hit an assertion failure in GalSim:  {code}  python: src/hsm/PSFCorr.cpp:731: void galsim::hsm::find_ellipmom_1(galsim::ConstImageView<double>, double, double, double, double, double, double&, double&, double&, double&, double&, double&, double&, boost::shared_ptr<galsim::hsm::HSMParams>): Assertion `iy1 <= iy2' failed.  {code}    This is due to GalSim using asserts rather than throwing an exception.  We fixed this on the HSC side ({{7a73869}}), but it didn't come over because we're using GalSim as a dependency rather than sucking it into the package.    I thought I reported this upstream at one point...",NULL
DM-7115,"Enable pybind11 wrapped functions with container arguments to accept any sequence type","Currently pybind11 uses an explicit mapping:    * {{std::vector}}, {{std::list}} <-> {{list}}  * {{std::tuple}}(, {{std::pair}}) <-> {{tuple}}    The problem of that lies in the two-way conversion. This means that wrapped C++ functions with {{vector}} or {{list}} arguments cannot accept a Python tuple and functions with {{tuple}} (or {{pair}}) parameters cannot accept a Python list. The same holds true for any other sequence type.    This ticket aims to fix this in upstream pybind11.",NULL
DM-7116,"Drawing layers dialog title is not updated with context image mission","Before migration, layers dialog title included mission name such as for example:  ""Layers- IRIS: 25"".    Now is fixed and is different:   ""Drawing Layers"".    Make the title aware of the active target focused with the mission name.",NULL
DM-7117,"measureCoaddSources fails with ""RuntimeError: Unable to match sources""","Running measureCoaddSources.py, I'm getting a {{RuntimeError: Unable to match sources}}.  The patch I'm running on doesn't have a lot of pixels illuminated, so it's not surprising that there would be no matches, but that fact shouldn't cause the operation to fail.  The behaviour of the matcher has changed in this respect: before the refactoring of astrometry, {{AstrometryTask.useKnownWcs}} would not raise an exception, but log an error and return an empty list of matches (it successfully matched zero sources, which might be cause for concern).    I think we should fix the matcher to return an empty list (as it did previously), but perhaps a case can be made that measureCoaddSources should catch the exception and continue.  I think that's wrong because measureCoaddSources would have {{matches = None}} rather than an empty list, which communicates something different (""I have no knowledge of matches"" vs ""I tried to match and there's nothing""); and measureCoaddSources shouldn't simply set {{matches = \[\]}} because it's not its responsibility to guess a type for what its subtask returns (breaks encapsulation).",NULL
DM-7118,"Provide continuous integration for Large Scale Tests","IN2P3 teams will provide continuous integration Large-Scale-Tests based.  In this EPIC, this setup will be host on Galactica experimental OpenStack platform using CEPH storage.  In a future EPIC, goal is to host it in a production platform.",NULL
DM-7119,"revise the Firefly README file","Issues to think/discuss (in README and firefly-api-overview.md):  # mention of Firefly Tools  # mention of viewer, plot  (for image display)  # testing data location     in firefly-api-overview.md:  #  ""Firefly tools is an API that can be used from JavaScript."" ??  # Shall we always use Firefly or firefly is OK?  # ""firefly.showPlot(targetDiv, parameters) "" ??  # ""if the coverage will use the x style"" ??",NULL
DM-712,"Remove wget dependency and use curl","curl is used by newinstall.sh. That why it should replace wget in Qserv install scripts in order to remove one system dependency.",NULL
DM-7120,"Add reading of DiaSource/DiaForcedSource to prototype script","Current version of my prototype script does not read (forced)sources from database. These will be needed in AP for calculation of the DiaObject parameters. This is probably one of the most interesting features because it is time-based queries and it will potentially have a big impact on database design and may require changes to schema.",NULL
DM-7121,"dm.lsst.org has outdated installation instructions","Following the installation instructions from http://dm.lsst.org/ produces the following:    {code}  $ curl -O https://sw.lsstcorp.org/eupspkg/newinstall.sh    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                   Dload  Upload   Total   Spent    Left  Speed  100   263  100   263    0     0    740      0 --:--:-- --:--:-- --:--:--   747  $ bash newinstall.sh  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  You are following obsolete instructions!  See https://pipelines.lsst.io/install/ for new instructions  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  {code}    That's a bit sad for the public face of the project.",NULL
DM-7122,"Backup ESXi Images on S3","Backup the stable ESXi images externally so we can reproduce the hypervisor environment in the future.",NULL
DM-7123,"initial design work for composite datasets","while waiting for sign off on composite dataset requirement, begin design work based on current requirements.    ",NULL
DM-7124,"Map structure of Science Pipelines codebase and Tasks","Make an inventory of all packages, Python namespaces, and tasks. This will aid in planning the structure of the Science Pipelines documentation site.",NULL
DM-7125,"Prepare documentation tasks for 2016 Project & Community Workshop","I’ll give two talks at the 2016 project and community workshop    1. A talk for project members on tech writing best practices. I’ll also include a primer on writing numpydoc docstrings.    2. A talk for the community on documentation platforms, and how science collaborations might adopt our platforms.",NULL
DM-7127,"Understand mathematical formalism for warping in the stack","Maybe start by writing some notes which could be an introduction to a tech note describing the whole of DM-6176.",NULL
DM-7129,"Implement covariance propagation in warping","This code will exist on a ticket (or user) branch, and is not intended for merging to the stack.    It's fine to avoid the normal niceties of I/O for this purpose: ie, dump outputs directly as NumPy arrays to disk, rather than working through the Butler.",NULL
DM-713,"Request: rename meas_multifit to meas_modeling","Quoting from my email to lsst-data on this subject: {quote} We're finishing up a review of a lot of new code in meas_multifit on DM-17, and one of the issues that came up is renaming the package.  As it stands right now, meas_multifit has very little to do with ""multifit"", and that's been a constant source of confusion; I did develop an early prototype of some actual multifit code there, but it's not where we ultimately plan to put the eventual pluggable multifit measurement framework (that will be some combination of meas_base and pipe_tasks, I think).  And when I tell people about the e.g. galaxy photometry algorithms in meas_multifit, it's very hard to explain that they're not being run in multifit mode.  At present, meas_multifit contains code for fitting PSF-convolved models of sources to image data and otherwise exploring those likelihoods (with e.g. Monte Carlo samplers).  Most (but not all) of that code relies on multishapelet models, and it's currently concerned with fitting galaxies rather than stars (but it may not always be). {quote}  Subsequent emails have proposed other viable names for the package, but no one seemed to have a strong preference about those or the ones I'd proposed, so here I'm requesting that we go with my original first choice: ""meas_modeling"".",NULL
DM-7130,"Use covariance propagation code to study covariance properties","This means using the code developed in DM-7129 to study the results achieved on some real data. Likely start by using ci_hsc as a test dataset, and find other data as & when necessary.    End goal is a set scatter plots showing covariance as a function of distance and a parametric fit to that. The easy case is for the ""middle"" of a coadd, ie with a constant number of inputs: harder, but still necessary, is to see how boundaries where the number of inputs change affect the output.",NULL
DM-7131,"Write up results in DM technical note","Summarize the results from DM-6176 in a technical note.",NULL
DM-7132,"Update pykg_config","pykg_config 1.2 does not work on Python 3. v1.3 does work.",NULL
DM-7133,"Switch stack from mysqlpython to mysqlclient python package","MySQL-python does not work on Python 3. There is a forked package at https://pypi.python.org/pypi/mysqlclient which does work on python 3. We need to switch our code to use the forked version. This requires:    # Create a new EUPS package for mysqlclient (probably called python_mysqlclient to avoid name classes. Alternatively we just reuse mysqlpython eups package and update the tar file to the fork.  # Make any necessary code changes. Has the namespace changed?  ",NULL
DM-7134,"singleFrameDriver is only running with a single process","singleFrameDriver.py is only using a single process.  The problem appears to be the change to use a {{ButlerInitializedTaskRunner}}, which doesn't inherit from {{BatchTaskRunner}}.",NULL
DM-7135,"failures in rebuild under lsstswBuild.sh may cause bogus log printing","As reported by [~Parejkoj] yesterday, if a CI run fails in {{rebuild}} before {{lsst-build prepare}} has completed, bogus log files may be printed to the jenkins console obscuring the true cause  of the build failure.  This appears to happen because log files can be left over from a previous build until {{lsst-build prepare}} has sanitized all product repos that are part of the current build.  ",NULL
DM-7136,"Remove BasicTable and add client-side filtering.","BasicTable is no longer needed.  All of its functionalities are covered by TablePanel.  Convert all usage of BasicTable to TablePanel and then remove it.  Add client-side filtering.  It is not converted, yet. ",NULL
DM-7137,"utils raDecStr delimiters are untested and don't work","In working on DM-6320 I noticed that there do not seem to be any tests for the {{decStrToDeg}} and related functions. When I added the following test:  {code:python}     def testDecStrToDegDelim(self):          deg = lsstutils.decStrToDeg(""+15_00_00.00"", ""_"")          self.assertAlmostEqual(deg, 15.0, 6)  {code}  it failed with:  {code}  Failed to parse +15_00_00.00 as a declination with delimiter '_' and regex '([\d]+)_(\d+)_([\d\.]+)'  {code}  (I added some extra information to the exception).    Furthermore, {{decStrToRad}} (and the equivalent RA version) do not pass through the delimiter at all:  {code:C++}  double ut::decStrToRad(std::string decStr, std::string delimiter) {      return degToRad( decStrToDeg(decStr) );  }  {code}  Please add some tests. Alternatively, we could remove the delimiter option completely.    It might also be worth removing the code duplication in the RA and Dec variants.",NULL
DM-7138,"Port ndarray to Python 3","I just tried to build LSST ndarray on Python 3 and it does not work. It's all going wrong as {{include/ndarray/swig}} seems to not be compiling.  {code}  In file included from tests/ndarray-python-mod.cc:25:  In file included from include/ndarray/swig.h:46:  In file included from include/ndarray/swig/numpy.h:33:  include/ndarray/swig/PyConverter.h:175:25: error: use of undeclared identifier 'PyString_AsString'              char * cs = PyString_AsString(s.get());                          ^  {code}  That API changed in Python 3 and it probably should be {{PyBytes_AsString}}.    http://stackoverflow.com/questions/22487780/what-do-i-use-instead-of-pystring-asstring-when-loading-a-python-module-in-3-3#22491037    ",NULL
DM-7139,"Move to Docker 1.12","Docker daemon and swarm API has changed in Docker 1.12. This brokes current setup, which use latest Docker version.    API fix will be performed and Docker version launched on Openstack node will be fixed.",NULL
DM-72,"Install and set up lsst-build tools on lsst-dev as lsstsw user","Build the stack into a common area on lsst-dev",NULL
DM-721,"Collect FY15 development/integration hardware data","DM has FY15 construction budget available for hardware for development and integration purposes.  We need to do a needs analysis to determine what this money should be spent on.",NULL
DM-735,"reenable 2 disabled tests in Interp.py","Long long ago RHL disabled two tests in meas_algorithms {code} -    def XXXtestDetection(self): +    def testDetection(self):   -    def XXXtest818(self): +    def test818(self): {code}  The changeset was: {quote} commit bc71e63affbee923092ce051d745dc0831769547 Author: rhl <rhl@git.lsstcorp.org> Date:   Thu May 27 02:04:48 2010 +0000      Wide followed by narrow defects caused some bad pixels to be missed; #1295 {quote} ",NULL
DM-738,"Fix any AFW Table version 0 dependencies","Remove any table version 0 dependencies.  This should include processImage.py clients at the very least.    We should also figure out what else might break when we remove version 0 tables by emailing the list.",NULL
DM-739,"Correct Fluxes","Correct Fluxes was not ported during Sprint 1.  Jim has said that it needs to be rewritten.  Details yet to be decided.",NULL
DM-74,"Complete Split DRP report","Portions of the report about the Summer '13 Split DRP were written by [~spietrowicz], [~gdaues], and Dominique Boutigny.  Pull them all into a single coherent report.",NULL
DM-743,"Checking for compliance to DM Standards should occur in a pre-commit hook and also be available on demand.","Standards checking should be possible  at multiple stages in the development cycle:  1) in the user's local directory without the immediate intent of git-commit;  2) in a pre-commit hook for all branches or, possibly, just the master branch.  When standards checking is invoked, a report on the submitted code will be created and provided to a named file and/or email addresses.  For the pre-commit hook, if the SAT determines that some Rules are serious enough to inhibit a commit, that constraint needs to be incorporated into the git-hook. (The current  DM Coding Standards were developed primarily as advisory rules.)  There are nuances which need to be considered for the git-hook:  * should this be done on all branch commits or only the master branch;  * are there any instances where the commit should be aborted. ",NULL
DM-747,"Integrate Serge explanation in Qserv install procedure","Next instructions should be integrated as comment or inside documentation :   {code}  let me try to walk you through what’s happening in README-devel.txt it in more detail.  From a clean shell:  1. cd /my/stack/dir 2. source loadLSST.sh     Makes eups and basic things like python and scone available in your environment.  3. cd to the top-level directory of your clone 4. Run `setup -r .`      The main thing this does is look for a .table file in a subdirectory of the current one named ups/. It finds qserv.table, from which it infers an eups product name of QSERV. It sets QSERV_DIR to the directory you are in. It also reads the table file and setups up all the qserv dependencies it finds, which is why you need to do this for the build to succeed.  5. eupspkg -er build     This builds your clone. It uses build/ as a build directory, just as running scons would.  6. eupspkg -er install     This installs from your git-clone to your stack. The install is to a directory named /my/stack/dir/Linux64/qserv/<branch name>-g<commit hash>(-dirty), where -dirty is only added to the version name if you have uncommitted/unstashed changes in your clone.  7. eupspkg -er declare     This basically runs the following command:          eups declare -r /my/stack/dir/Linux64/qserv/<branch name>-g<commit hash> qserv <branch name>-g<commit hash>     which says: “Hey eups, directory /my/stack/dir/Linux64/qserv/<branch name>-g<commit hash> contains version <branch name>-g<commit hash> of a product called qserv. Please check that there’s a ups/qserv.table file in there, and remember all of that for me so I can come back to it later!”.  Now here is where I think you missed a step:  8. setup qserv <branch name>-g<commit hash>  (in your case, `setup qserv master-ga7082f1e02`). This says, OK eups, now I actually want to use version <branch name>-g<commit hash> of qserv (which was built and installed from your clone and is _not_ the pre-packaged version of qserv you get when installing the stack via new install.sh). This sets QSERV_DIR to /my/stack/dir/Linux64/qserv/<branch name>-g<commit hash>.  Now:  > So > eups list qserv >   LOCAL:/usr/local/home/becla/qserv/1/src/qserv 	setup >   master-ga7082f1e02 >   u.fjammes.DM-699-g4ec6034b0e 	current b69 > > is what I want. And that does not have bin…  At this stage you have not yet run `setup qserv master-ga7082f1e02`. If you had, the line for master-ga7082f1e02 would contain the word “setup” (instead of  LOCAL:/usr/local/home/becla/qserv/1/src/qserv).  > ls $QSERV_DIR > admin  config.log  css                lib               README.txt site_scons  TODO > build  core        example.custom.py  README-devel.txt  SConstruct tests       ups  That means that here, you are doing an ls of the top-level directory of your git clone. Note the config.log, which is a by-product of building, and wouldn’t be present in an installation directory.   So Fabrice’s eupspkg commands are basically hiding the details of running scons with the appropriate prefix argument, eups table file expansion, and so on. {code} ",NULL
DM-748,"Automate Jacek install procedure from git repo","Next procedure have to be automated  {code} I am playing with switching branches in my local qserv. It is actually quite a process! Here is what I found so far:   # I won't have access to the PIDs for already # running services, so better to stop them now. # That is because things are run from a place that # looks like: # ~/stack/Linux64/qserv/master-ga7082f1e02/, # and we will switch to something like: # ~/stack/Linux64/qserv/tickets.DM-630-gb26efd78da qserv-stop.sh  # switch branch cd ~/repo/qserv git checkout tickets/DM-630  # build, install eupspkg -er build eupspkg -er install  # this is only needed if it is the very first time # I work with a given version of the branch eupspkg -er decl  setup qserv <the new version>  # this basically wipes out everything: zookeeper data, mysql data... # but if I don't do that, I won't have scripts from admin/bin # for starting/stopping qserv, so I can't skip it. This really # needs to be fixed... cd $QSERV_DIR/admin scons  # if I want to run integration tests, it looks like this is needed too: setup qserv_testdata cd $QSERV_DIR/admin scons client  # and then I need to start services qserv-start.sh  then qserv-testdata.py should work  This is quite complex and error prone and easy to get wrong. Could we possibly simplify it?  Jacek  {code}",NULL
DM-749,"Ask guiding questions during install procedure","In admin/tools/qserv-install.sh, ask the user to :  - start with a clean shell environment (no loadLSST.sh done previously) - remove ~/.eups if it exists, - install test data - configure Qserv - run integration tests  Same type of questions for : admin/tools/qserv-git-install.sh",NULL
DM-752,"Remove scons cache when replacing system lib with eups libs","  While building Qserv in a git repo, switching to eups boost library, instead of previous system boost library, may lead to a build error.  Indeed some boost lib may have been detected with a prefix (e.g. -mt or -gcc41-mt) and this libs have no prefixes in eups package  That's why next command has to be performed each time a C++ system lib is replaced with a eups one :  This link explains what should be done : http://dcreager.net/2009/12/18/make-distclean-in-scons/",NULL
DM-76,"Modify version computation in pkgautoversion","Modify pkgautoversion to generate versions with the following algorithm:  * If a tag exist on a commit, use <tag> as the version. * If there's no tag, use branchname-gSHA1ABBREV, where any illegal characters in branchname get turned into dots.  Example versions: * 7.10.2.1 * master-gdeadbeef * feature.dm-1234-gdeadbeef ",NULL
DM-77,"First release of lsst-build future buildbot backend, with +N dependencies","Re +N versions:  If the package has dependencies, and a build of this package with different dependencies already exists, append a +N to the end.  Keep the mapping of +N -> (dependency name, sha1s) in a special git repository. Given the source code and this git repo, two causally disconnected buildbots must generate the same set of versions. ",NULL
DM-777,"consider using lightweight containers","We should consider using lightweight VM-like containers (e.g. https://www.docker.io/). This is beyond just Qserv.",NULL
DM-784,"Should SAT be responsible for new package names?","Since the SAT has decided that it should control package renamings, it is natural to ask whether it should approve new package names in the first place.  There are arguments on both sides.  If you think an important part of the point of controlling name changes is to ensure that the new name is appropriate, then you should think the same oversight is reasonable for all new names.  If you think that the main point of SAT approval of renamings is to ensure that the disruption caused by a renaming is justified and that there is a plan to handle it well, then you may not think pre-approval of new names is required.",NULL
DM-785,"Sprint/Epic policy definition","* Should sprints ever be extended beyond their defined end dates? * How should sprints and epics be named to simplify automated export to PMCS?",NULL
DM-79,"Improve test coverage of pipe_tasks","In refactoring cameraGeom, it has become very clear that the coverage of pipe_tasks is very poor.  I attribute this to the difficulty of creating an appropriate butler and data repository to feed to the tasks.  I propose that we generate a data repository in obs_test that contains: * An example camera ** This will include example code to generate the test camera ** An initial try at this exists in cameraGeom.testUtils and could be moved over * A set of example data including defects and calibrations ** I imagine this would be a set of a few simulated images * Astrometry.net indexes and any other ancillary data needed for reduction.  If this is too big to go in a git repository, we could use the [git-fat|https://github.com/jedbrown/git-fat] extension to git.",NULL
DM-795," information using local qserv_testdata ","Add also  infos about running ""scons client"" to setup local qserv_testdata in config file.  {code:bash} source ~/stack/loadLSST.sh  git clone ssh://git@dev.lsstcorp.org/LSST/DMS/testdata/qserv_testdata.git cd qserv_testdata setup -r. # line above is useless as no build procedure is required : # eupspkg -er build eupspkg -er install eupspkg -er decl setup qserv_testdata 2014_05.0  setup qserv cd $QSERV_DIR/admin; scons # or at least scons client qserv-start.sh qserv-testdata.py  For your information, tests outputs are, for now, in : # for test case 01 : $QSERV_DIR/qservTest_case01/outputs/ {code} ",NULL
DM-799,"Define filter and export process to get data for PMCS","Using the JIRA Search Issues, select issues associated with a given release and export the issues and epics, including fields needed to import into PMCS.  Test 2 approaches: 1. Stories, Bugs, and Improvements mapping to PMCS activities, and Technical Tasks mapping to Steps. 2.  Epics mapping to PMCS activities and with Stories, Bugs, and Improvements mapping to Steps.",NULL
DM-80,"Refactor pipeQA for new cameraGeom","The Winter2014 cycle did not leave enough time to refactor pipeQA to use the new cameraGeom.  This will be taken as a task for Summer2014. ",NULL
DM-805,"Develop Rank scheme for preference of empty/extra slots ","We develop a Rank scheme to express a preference of empty/extra HTCondor slots.  The context being considered is, for example, the ""single slow worker"" scenario, where  a second job for a ccd is submitted in the cirucumstance where a preferred location is not available. In this setting, it is preferred that the second job falls in an ""extra"" slot that has no cached data, as opposed to executing in a location that has data for other ccds cached (as  this would supplant these ccds and cause additional copies to caches.) ",NULL
DM-806,"Run ""Single HTCondor slot drops"" Classads Scenario","We investigate the scenario where an HTCondor is removed during execution. Example initial setup is [ 4 CCDs, 5 slots, Job length 90 sec, job submit interval 2 min, cache size 1].  When the HTCondor slot is suddenly removed, we test that our Rank expressions admit a path to stable execution in this scenario. ",NULL
DM-808,"Run ""Single HTCondor job failure/restart"" Classads Scenario","As a perturbation on the baseline HTCondor Classads Scenario, run a case where an HTCondor job fails due to application error, returning a nonzero exit code.   Investigate the mechanism by which the job would be automatically resubmiitted, possibly through use of holding/releasing a job, or DAGMan, rescue DAGs, etc.",NULL
DM-809,"Run ""make check"" (i.e. unit tests) in log4cxx eups install procedure ","Next issue : http://markmail.org/message/qv6rbyczcxc4wddu has been solved in DM-772. Nevertheless log4cxx unit tests doesn't still work very well, cf. : - https://issues.apache.org/jira/browse/LOGCXX-260 - optionconvertertestcase also fails intermittently",NULL
DM-810,"Test camera for afw.cameraGeom has overlapping chips","The test camera described int $AFW_DIR/tests/testCameraDetectors.dat describe a camera that has overlapping chips.  This is actually o.k. in terms of the cameraGeom, but makes it less realistic to use in tests of other packages using cameraGeom.  It is very unlikely that we will deal with cameras that overlap in focal plane coordinates, so I suggest altering the test camera to eliminate this.",NULL
DM-811,"I suggest changing the Python coding standard: == and != SHOULD be used when comparing to None","Our current Python coding standard says ""== and != SHOULD be used when comparing to None"". I recommend either deleting this rule or else changing this to read ""is and is not SHOULD be used when comparing to None"". There are two reasons: - ""is None"" works with numpy arrays, whereas ""== None"" does not - ""is None"" is idiomatic  The justification for the original wording (which I wrote) was to avoid leading naive coders astray; ""is"" must be used very cautiously because it compares object identity instead of value. But over time I have come to realize this is not a common source of errors, and I think the two points above justify deleting or changing the rule.",NULL
DM-812,"Installation of boost (with new build system) fails if user has Enthought Canopy ","Alex Kim reported a failure of boost to build when attempting to install lsst_sims (via eups distrib install lsst_sims -t sims).  This is on Mavericks, with clang 5.1.  I think the problem is the same as described here:  https://github.com/GalSim-developers/GalSim/issues/506 since after applying the workaround, he reports that boost builds.  On the issue page linked above, there is a description of how to edit the build files so that scons can find the python files appropriately. Is this something that can or should be folded back into the LSST installer? ",NULL
DM-818,"sip/cleanBadPoints can generate numpy warnings","Running meas_astrom can generate numpy errors due to requesting operations on empty arrays.  Please fix this.  {quote} /u/lsst/products/DarwinX86/anaconda/1.8.0/lib/python2.7/site-packages/numpy/core/_methods.py:57: RuntimeWarning: invalid value encountered in double_scalars   ret = ret / float(rcount) /u/lsst/products/DarwinX86/anaconda/1.8.0/lib/python2.7/site-packages/numpy/core/_methods.py:72: RuntimeWarning: invalid value encountered in true_divide   out=arrmean, casting='unsafe', subok=False) /u/lsst/products/DarwinX86/anaconda/1.8.0/lib/python2.7/site-packages/numpy/core/_methods.py:96: RuntimeWarning: invalid value encountered in double_scalars {quote} ",NULL
DM-819,"Improvements/fixes to afwTable::Schema::contains","The method contains in the afwTable::Schema object can have unexpected behaviour, and doesn't support operations that I'd expect to work. {code} >>> schema['centroid'] in schema False >>> schema.contains(schema['centroid']) *** NotImplementedError: Wrong number or type of arguments for overloaded function 'Schema_contains'.   Possible C/C++ prototypes are:     lsst::afw::table::Schema::contains(lsst::afw::table::Schema const &,int) const     lsst::afw::table::Schema::contains(lsst::afw::table::Schema const &) const >>> schema.contains(schema.find('centroid')) *** NotImplementedError: Wrong number or type of arguments for overloaded function 'Schema_contains'.   Possible C/C++ prototypes are:     lsst::afw::table::Schema::contains(lsst::afw::table::Schema const &,int) const     lsst::afw::table::Schema::contains(lsst::afw::table::Schema const &) const >>> key = schema.find(""id"").getKey() >>> schema.contains(key) *** NotImplementedError: Wrong number or type of arguments for overloaded function 'Schema_contains'.   Possible C/C++ prototypes are:     lsst::afw::table::Schema::contains(lsst::afw::table::Schema const &,int) const     lsst::afw::table::Schema::contains(lsst::afw::table::Schema const &) const # So I need to use schema.find(key) in a try block  >>> schema.contains(afwTable.SimpleTable.makeMinimalSchema()) 1 >>> afwTable.SimpleTable.makeMinimalSchema() in schema           False {code} ",NULL
DM-82,"Verify 'eups distrib install lsst_distrib' works RHEL6, OS X 10.8, OS X 10.9","Try: {code:bash} mkdir stackdir && cd stackdir curl -O http://sw.lsstcorp.org/eupspkg/newinstall.sh bash newinstall.sh {code}  and report if it worked for you (include the OS and compiler version in your comment).",NULL
DM-821,"AstrometryTask shouldn't add distorted coordinates columns to the schema","The current implementation of the AstrometryTask adds columns to the schema to hold the distorted coordinates.  This means that it cannot be used to match a pre-existing afwTable even if there is no distortion.  I don't think that this is polite behaviour.  At the very least the new columns should only be added if distortion is present (presumably controlled via a config parameter as we don't know what the data looks like when the task is created).  A better long-term solution is to change the way that the book-keeping is done.",NULL
DM-822,"AstrometryTask must check that all columns present in the schema are in the table","When you create an AstrometryTask (currently in pipe_tasks but should move to meas_astrom) it adds columns to the schema, which it then blithely uses in its run method even if they are not actually present in the input tables.  Please add a check that the columns are really there. ",NULL
DM-823,"Check that all the columns used by PhotoCalTask are actually present","When you create a PhotoCalTask it sometimes adds a column to the schema, which it then blithely uses in its run method even if it is not actually present in the input tables.  Please add a check that the column is really there. ",NULL
DM-824,"PhotoCalTask ignores all input errors if any are NaN","If any of the input flux errors are NaN, the photoCalTask ignores all of them (and uses sqrt(flux) instead) ",NULL
DM-825,"Create instructions for installing PhoSim","Create a Confluence page with instructions for installing PhoSim for users of the LSST Stack. Note: there are two libraries (cfitsio and fftw3) in common with the Stack, which needs to be specified when installing PhoSim. ",NULL
DM-826,"Configure subjects of email from jira","Mail from Jira can get lost in hyperactive mailboxes.  Can we configure Jira to set more helpful subjects (e.g. ""Review requested on DM-735"") ",NULL
DM-830,"SAT Request: delimiter for afw::table field names","As part of the ongoing measurement overhaul, I proposed a broad change to the naming conventions for afw::table fields.  This was approved in a design review a while back, but one aspect of that has remained contentious and I'd like to reopen it and have it settled more officially by the SAT before making the change.  The question is whether to use ""."" or ""_"" as a namespace delimiter in field names.  I really do mean a *namespace* delimiter; we're already using CamelCase as a *word* delimiter, so in this case we're talking about the delimiter used to separate the abbreviated-package (""{{base}}""), algorithm name (""{{SdssShape}}""), and measurement quantity (""{{xx}}"") of a name like ""{{base_SdssShape_xx}}""/""{{base.SdssShape.xx}}"".  Here are the main arguments, as I see them:   - Periods are what we have used so far in afw::table, and they're a far more natural-looking namespace separator, especially in Python.  Underscores are more naturally a word delimiter than a namespace delimiter, and using them as a namespace delimiter looks awkward in Python and C\+\+.   -  SQL databases don't support periods in field names, so we'd have to use underscores there regardless.  So if we use periods in afw::table, we'll the C\+\+/Python names for fields will be different from those used in SQL, while if we use underscores in afw::table they can be the same.  I'm personally ambivalent about this choice, but I'd like it to be official so I don't take the heat from whoever doesn't get their way :-)",NULL
DM-831,"Standards: should not shadow python built-in functions","In March 2014, Yusra posted the following as a Trac Ticket (#3134); it is now being moved into JIRA and moved over to SAT for review.  During a recent code review, we noticed that our python standards wiki page doesn't mention shadowing of built-in functions ​Python Built-ins. There was some disagreement on whether this issue is style or standard. (The context in the review was that id was used as a variable name in a class method.) This ticket proposes adding it to the standards. ",NULL
DM-835,"better resolution for duplicate issues","We should be able to resolve issues as duplicates, not just link them.  Should probably have similar resolutions for ""invalid"", ""wontfix"", ""obsoleted [by]"", maybe others.",NULL
DM-84,"Diagnostic support and error-handling in meas_base","This Epic captures issues that improve meas_base's support for investigating what went wrong with algorithms.  This includes:  - Issues to improve debug-mode functionality in various ways, including making it easier to reproduce algorithm failures without needing to re-run the entire measurement framework.  - Issues to improve the relationship between Exceptions and SourceRecord flag bits; we want all caught exceptions to result in both one or more standard flags (for slot use) and specific, more descriptive flags that document exactly what happened.  We should also have similar flag support for non-fatal errors.  Some further ideas that haven't yet been spun off into issues:  - We should have measurement framework drivers that use the same plugin interface, and rerun a single plugin on a single object after the full measurement has been done.  This would have to use the NoiseReplacer repeatability stuff (and would probably drive the interface to the NoiseReplacer repeatability stuff).  - I think we need a way to allow individual measurement plugins to save additional diagnostic outputs when so configured.  If SourceRecord could store blobs, I think we'd just use that, but it doesn't, so it may be best to just pass a DataRef in for now and define some new mapper entries.",NULL
DM-850,"Prototype Input and Output to Slots and Standard SourceRecord Columns`","This is to prototype simplifying the access of Algorithms to both the measCat (for previously measured columns) and refCat (for fields in a reference catalog).  This will be patterned on the current Input design, but will not require the Algorithm implementer to write new Input classes.  Similarly, the implementer will be able to write to the SourceCatalog over previously writtne columns.",NULL
DM-851,"Prototype and design idea for removing ResultMappers","Complete the design prototype which was started during Sprint planning to replace the ResultMappers and Results with Python code.  The C++ code will declare the field mappings as a set of structures.  As proof of concept, implement SdssShape ",NULL
DM-852,"Multi-master support, fail-over","Qserv needs to work in multi-master configuration.   If a master dies, all traffic should be seamlessly redirected to a different master. and queries with assigned queryIds should be redirected and handled by other master. ",NULL
DM-853,"Implement multi-czar, qserv-wide global queryId","Queries that are not instantaneous, including long running queries that should survive master failures need a global queryId. Need to implement the system generates such queryIds, and assigns them to appropriete queries",NULL
DM-86,"Buildbot repository locations and permissions","Move buildbot repos to final locations in LSST/DMS, and give permissions to lsstsw user to write versiondb.",NULL
DM-864,"XXX Replace mysql-proxy with czar internal handling of mysql protocol","Currently mysql connection from clients is handled by mysql-proxy which passes queries to czar and returns query results back to user. This proxy-czar combination has some significant issues which limit what we can do with it: - very limited possibility to generate data on proxy side (it has to result from SQL query of some sort) - proxy has very little information about result data and cannot do transformations on that  It looks like we can achieve better result if we can implement our own proxy which can work at the mysql wire-level protocol and integrate that proxy directly into czar. This would eliminate one server process from our current setup and should help both performance and stability. ",NULL
DM-866,"Prepare guidelines for package naming","As part of the resolution of DM-784, a set of guidelines for how to name packages needs to be created.",NULL
DM-867,"Prepare configurations for common editors to check (syntactic) coding standards","As part of the resolution of DM-831, it was suggested that modern editors can be configured to automatically check syntactic aspects of the code and highlight violations of our coding standards.  Prepare such configurations for common editors.",NULL
DM-868,"Audit ups table files to replace envAppend with envPrepend","Tim Jenness reports problems running v8 on OSX Mavericks due to the build being made against an LSST-provided cfitsio, but attempting to run against a system-installed cfitiso.  This appears to be due to some LSST packages being appended to his {{DYLD_LIBRARY_PATH}} instead of prepended:  {code:sh} $ echo $DYLD_LIBRARY_PATH  /Users/timj/work/lsst/DarwinX86/shapelet/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/meas_extensions_multiShapelet/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/skymap/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/coadd_chisquared/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/coadd_utils/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/ip_diffim/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/ip_isr/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/astrometry_net/0.30+3/lib:/Users/timj/work/lsst/DarwinX86/meas_astrom/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/meas_algorithms/8.0.0.0+3/lib:/Users/timj/work/lsst/DarwinX86/minuit2/5.22.00+2/lib:/Users/timj/work/lsst/DarwinX86/wcslib/4.14+3/lib:/Users/timj/work/lsst/DarwinX86/fftw/3.3.2+2/lib:/Users/timj/work/lsst/DarwinX86/pex_config/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/afw/8.0.0.3/lib:/Users/timj/work/lsst/DarwinX86/base/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/pex_policy/8.0.0.0+1/lib:/star/lib:/star/starjava/lib/i386:/star/starjava/lib/x86_64:/Users/timj/work/lsst/DarwinX86/daf_base/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/utils/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/boost/1.55.0.1/lib:/Users/timj/work/lsst/DarwinX86/pex_exceptions/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/pex_logging/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/daf_persistence/8.0.0.0+1/lib:/Users/timj/work/lsst/DarwinX86/mysqlclient/5.1.65+3/lib:/Users/timj/work/lsst/DarwinX86/cfitsio/3310+2/lib:/Users/timj/work/lsst/DarwinX86/xpa/2.1.14+2/lib:/Users/timj/work/lsst/DarwinX86/gsl/1.15+2/lib:/Users/timj/work/lsst/DarwinX86/meas_deblender/8.0.0.0+3/lib {code}  This suggests we need a thorough audit of ups table files, removing {{envAppend}} in favour of {{envPrepend}}.",NULL
DM-87,"Add a bootstrap script to lsstsw_home","As an I&T person, I want to be able to git-clone lsstsw_home, run a bootstrap script, and have everything ready to run stack builds/publishings on my machine.  This script should: * Download and set up Python (Anaconda) * Download and build git, git manpages * Download and build EUPS * symlink manifest.remap from etc to stack/site * git-clone lsst_build tools * git-clone versiondb ",NULL
DM-871,"Need updated benchmark file for demo2012(processCCD) for v8.0.0.0","The benchmark data used to confirm correct results for demo2012's processCCD execution needs to be  updated to reflect the v8.0.0 DM stack.",NULL
DM-872,"Provide documentation for all Tasks","The unit of reuse of LSST software is the task, so the docs need to be complete enough for someone to reuse a task without reading its source code.  See https://confluence.lsstcorp.org/display/DM/How+to+document+a+Task",NULL
DM-877,"pex_logging build warnings: lookup of 'set' in member access expression is ambiguous","When I build pex_exceptions 8.0.0.0 I get three similar warnings:  {code} src/Log.cc:172:16: warning: lookup of 'set' in member access expression is ambiguous; using member of 'lsst::daf::base::PropertySet' [-Wambiguous-member-template]     _preamble->set<string>(""LOG"", _name);                ^ /Users/rowen/LSST/lsst_home2/DarwinX86/daf_base/8.0.0.0+1/include/lsst/daf/base/PropertySet.h:139:32: note: lookup in the object type 'lsst::daf::base::PropertySet' refers here     template <typename T> void set(std::string const& name, T const& value);                                ^ /usr/include/c++/4.2.1/bits/stl_set.h:93:11: note: lookup from the current scope refers here     class set           ^ … tests/testLogRecord.cc:95:14: warning: lookup of 'set' in member access expression is ambiguous; using member of 'lsst::daf::base::PropertySet' [-Wambiguous-member-template]     preamble.set<int>(""dpint"", 2);              ^ /Users/rowen/LSST/lsst_home2/DarwinX86/daf_base/8.0.0.0+1/include/lsst/daf/base/PropertySet.h:139:32: note: lookup in the object type 'lsst::daf::base::PropertySet' refers here     template <typename T> void set(std::string const& name, T const& value);                                ^ /usr/include/c++/4.2.1/bits/stl_set.h:93:11: note: lookup from the current scope refers here     class set           ^ tests/testLogRecord.cc:96:14: warning: lookup of 'set' in member access expression is ambiguous; using member of 'lsst::daf::base::PropertySet' [-Wambiguous-member-template]     preamble.set<float>(""dpfloat"", 2.5);              ^ /Users/rowen/LSST/lsst_home2/DarwinX86/daf_base/8.0.0.0+1/include/lsst/daf/base/PropertySet.h:139:32: note: lookup in the object type 'lsst::daf::base::PropertySet' refers here     template <typename T> void set(std::string const& name, T const& value);                                ^ /usr/include/c++/4.2.1/bits/stl_set.h:93:11: note: lookup from the current scope refers here     class set           ^ tests/testLogRecord.cc:97:14: warning: lookup of 'set' in member access expression is ambiguous; using member of 'lsst::daf::base::PropertySet' [-Wambiguous-member-template]     preamble.set<long>(""dplong"", 5l);              ^ /Users/rowen/LSST/lsst_home2/DarwinX86/daf_base/8.0.0.0+1/include/lsst/daf/base/PropertySet.h:139:32: note: lookup in the object type 'lsst::daf::base::PropertySet' refers here     template <typename T> void set(std::string const& name, T const& value);                                ^ /usr/include/c++/4.2.1/bits/stl_set.h:93:11: note: lookup from the current scope refers here     class set           ^ {code}",NULL
DM-878,"XrdCl seems locked while trying to cancel.","While a single request in a single session seems to work fine, multiple sessions with their own in-flight requests seem broken.  In the stack trace, you can see that a cancelling call is in-flight (XrdSsiRequest::Finished(true) but blocked by a semaphore. An Unprovision call is also in-flight, but also blocked.  It is possible that there is a memory new/delete problem. I don't know yet. The memory management is a bit foggy for me because it's not always clear who should own these *Session or *Request objects at what points in the code.  This is going to be hard to reproduce, because I'm constantly changing the code. But it's  commit 7758b32c467 on u/danielw/kirkwood .  I am including a stack trace in the hopes that it provides enough clues.",NULL
DM-879,"One unit test fails (barely) on MacOS","tests/subtractExposures.py always fails on MacOS X: a value is slightly wrong.  {code} FAIL: testXY0 (__main__.DiffimTestCases) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/subtractExposures.py"", line 182, in testXY0     self.runXY0('polynomial')   File ""tests/subtractExposures.py"", line 263, in runXY0     self.assertAlmostEqual(skp1[nk][np], skp2[nk][np], 4) AssertionError: -0.0004418029210029317 != -0.00039009266307754586 within 4 places {code} I have attached the full output as a file.",NULL
DM-88,"Ensure that the blas library is included in the Stack installation ","A user reported that, upon an apparently successful binary installation of the v7.3 Stack on a RHEL 6 system, a subsequent attempt to run the Demo script failed with the non-obvious error: ImportError: numpy.core.multiarray failed to import. The problem was that the blas library (a required prerequisite) was not installed on his system.   There are two elements to the fix: one is to ensure that the installation script checks that the blas library is available (complain helpfully if not), and the second is to update the Software User Guide (Prerequisites) to add blas to the list of required software. ",NULL
DM-880,"mocks/makeSimpleCamera returns an invalid transformMap","mocks/simpleMapper/makeSimpleCamera returns a camera whose transformMap is a dict instead of a CameraTransformMap.  I propose to fix this as part of DM-764 so I can commit that with a full suite of working unit tests. I'd do it on trunk, but I'm not sure what versions of packages are required to make the current trunk work. In any case, the fix is very simple.",NULL
DM-881,"tests/testTruncatedGaussian.py fails due to no scipy.integrate.Inf","Line 111 of tests/testTruncatedGaussian.py fails on my Mac because my scipy (version 0.13.0) has no scipy.integrate.Inf: {code} return scipy.integrate.quad(func, 0.0, scipy.integrate.Inf) {code} scipy.integrate.Inf appears three times in that file. The symbol is also missing from scipy 0.10.1 on lsst11  This is with the current master b6e7588 (the Affected Version/s field doesn't accept this). ",NULL
DM-883,"Update obs_file","obs_file is using the old camera geom.  I'm using: https://github.com/SimonKrughoff/obs_file to work on this.  ",NULL
DM-884,"ds9.dot bug making symbols with colors other than default not show up","I introduced a bug when implementing the angle option to ds9.dot.  If a ctype other than default is passed, the symbols are not drawn.  The failure is silent.",NULL
DM-885,"Consolidate interpolation codes","pipe_tasks contains two interpolation codes: one in {{RepairTask.interpolate}} and one in {{InterpImageTask.interpolateOnePlane}}.  These should be consolidated.",NULL
DM-888,"Coding Standards: No unnecessary parens","From: Robert Lupton the Good <rhl@astro.princeton.edu> On: May 19 To: LSST  I continue to see code like:  > nGrow = int((self.config.nSigmaToGrow * sigma) + 0.5)  This should be         nGrow = int(self.config.nSigmaToGrow*sigma + 0.5)  That is:         No unnecessary parens -- they require the reader to think, ""What do those parens mean --- Oh, nothing""         ** * / bind harder than + - and should have no whitespace (+- do need spaces).  Just the way that you write mathematics by hand:                 ""1 + 2x"" not ""1+2 x"" or ""1 + 2 x""  Maybe we should use         nGrow = round(self.config.nSigmaToGrow*sigma) or         nGrow = np.rint(self.config.nSigmaToGrow*sigma) but I'm not excited (especially as round(0.5) changes from 1 to 0 in python3)  ",NULL
DM-889,"Stories relating to Developer Support, Infrastructure and Tools","This Epic aggregates Stories supporting the process of software development. Generally these stories deal with continuous integration and test,  automated coding standards review, software distribution and management, and establishing frameworks assisting collaborative software development.",NULL
DM-89,"Making very large images seg faults","I noticed this when trying to make camera sized images for LSST.  If I give the Image* constructor 30Kx30K, it works.  If I give 60Kx60K, it seg faults.  How to repeat.  From the interpreter on lsst-dev: {code} >>> import lsst.afw.image as afwImage >>> afwImage.ImageF(30000, 30000) <lsst.afw.image.imageLib.ImageF; proxy of <Swig Object of type 'boost::shared_ptr< lsst::afw::image::Image< float > > *' at 0x1a9f900> > >>> afwImage.ImageF(60000, 60000) Segmentation fault {code}",NULL
DM-890,"Create new htcondor pools for replicators and workers","I've asked the admins create some new VMs so I can transition the work to HTCondor pools I can completely control without affecting anyone else.   There'll be an HTCondor pool available for the replicators and a HTCondor pool available for the workers.  Initially the size of the pool for the workers will be only about 55 slots, and I'll get that increased as soon as I have things configured properly.",NULL
DM-891,"Transition software to /nfs/workflow","The software running on the VMs is currently running out of my home directory.   The admins would like all VMs to not have access to the /lsst* directories were possible.   While they're building the VMs for the transition in sub-task DM-890, I'll be moving the software over to that directory, and making sure that it works.",NULL
DM-892,"Test HTCondor team's solutions to autoclustering bug discovered by LSST","The original scenarios utilizing job Rank in DM-653  were revisited upon the following discovery. Original results where jobs executed in ""appropriate"" slots were discovered to be an artifact of job submission order. A more scrambled job submission order in a base scenario with static class ads for slots showed jobs to not execute in the locations (host/slot) expected.     A base scenario demonstrating job Rank not delivering expected results was presented to the HTCondor team.  They were able to reproduce the results and traced the problem to a long existing auto clustering bug in HTCondor. They created ticket #4403  https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=4403 for HTCondor, and subsequently provided a workaround for current releases, and implemented a solution in the codes base for 8.2.1+.  In this issue we test these workarounds/solutions.",NULL
DM-893,"getX fails in astrometry matches list ds9 display","Dear DM:  * Summary: Visualizing matches fails to get the X attribute of the matches in tag 8.0.0.3 (release/Winter2014): i.e., .getX() fails from an element of the matches list.  {code}   File ""/Users/wmwv/stackdir/DarwinX86/pipe_tasks/8.0.0.1+3/python/lsst/pipe/tasks/astrometry.py"", line 209, in astrometry     self.display('astrometry', exposure=exposure, sources=sources, matches=matches)   File ""/Users/wmwv/stackdir/DarwinX86/pipe_base/8.0.0.0+3/python/lsst/pipe/base/task.py"", line 306, in display     x1, y1 = first.getX() - x0, first.getY() - y0   File ""/Users/wmwv/stackdir/DarwinX86/afw/8.0.0.3/python/lsst/afw/table/tableLib.py"", line 6891, in <lambda>     __getattr__ = lambda self, name: _swig_getattr(self, SimpleRecord, name)   File ""/Users/wmwv/stackdir/DarwinX86/afw/8.0.0.3/python/lsst/afw/table/tableLib.py"", line 63, in _swig_getattr     raise AttributeError(name) AttributeError: getX {code}  * Context and Details:  I'm working on running some images from WIYN+WHIRC.  They're already ISRed and combined.  I'm just trying out measurement of sources and astrometry.  Simon has been most helpful in getting obs_file updated and sharing with me LSSTish astrometry.net indexes for 2MASS.  I'm finding that the astrometry solution calculated by the astrometry step is wrong.  I'd like to use the built in lsst.pipe.base.tasks.Task.display('astrometry',exposure=exposure,sources=sources,matches=matches) to investigate the matches.  But when I do this it fails at the step of plotting up the matches with the error message below.  As the individual matches are swigged objects I don't really know how to easily inspect them in Python.  Have the parameter names or the way of accessing them been changed since Task.display was last updated?  Here's the relevant snippet of code where the error above comes from:  {code}         if matches:             with ds9.Buffering():                 for first, second, d in matches:                     i = len(sources)    # counter for ptypes/ctypes                      x1, y1 = first.getX() - x0, first.getY() - y0                      ctype = ctypes[i%len(ctypes)]                     ptype = ptypes[i%len(ptypes)]                     size  = 2*sizes[i%len(sizes)]                     ds9.dot(ptype, x1, y1, size=8, frame=frame, ctype=ctype)                     i += 1                      ctype = ctypes[i%len(ctypes)]                     ptype = ptypes[i%len(ptypes)]                     size  = 2*sizes[i%len(sizes)]                     x2, y2 = second.getX() - x0, second.getY() - y0                     ds9.dot(ptype, x2, y2, size=8, frame=frame, ctype=ctype)                     i += 1 {code}  {code} export ASTROMETRY_NET_DATA_DIR=/Users/wmwv/stackdir/DarwinX86/astrometry_net_data/8.0.0.0/2MASS  processFile.py 20131025/06.stack/iPTF13dge___H_OPEN.lsst.fits  \   --output output \   --clobber-output --clobber-config \   -c gain=3.4 \   isBackgroundSubtracted=True  {code}  I added to \file{obs_file/python/lsst/obs/file/processFile.py the import of debug.py, and my local debug.py is just  {code} import lsstDebug lsstDebug.Info(""lsst.pipe.tasks.astrometry"").display = {'astrometry': True} {code}  (which took me a couple of hours to figure out, but that's a different ticket -- DM-894).   ",NULL
DM-894,"Request for example debug.py enumerating all of the debug options.","Would it be possible to provide an example debug.py file with all of the potential options for debugging given (and set to False).    I'm of course new and an outsider to main DM developement, but as an example it took me an hour to figure out that I needed to pass a dict instead of just saying {{True}} for setting {{display}} for {{lsst.pipe.tasks.astrometry}}.  {code}  lsstDebug.Info(""lsst.pipe.tasks.astrometry"").display = {'astrometry': True}  {code}    ",NULL
DM-896,"use-case : use several Qserv instance with several run-dir on a same account","Is this use-case usefull ? Maybe in the long-term in order to compare several Qserv version on different data sets ?",NULL
DM-899,"Request for persisted measured sources even if calibrate fails","Add a debug option for calibrate to persist its measurements to date even if it fails to astrometrically or photometrically calibrate those measurements.  Calibrate conducts a first round image characterization, which includes draft identification and measurement of sources.  It then performs astrometric and photometric calibration.  However if either of these last two steps fails, calibrate just raise an exception and we exit.  It would be very useful to be able to debug by having calibrate persists its measurements to date.",NULL
DM-901,"testCameraGeom has a shadowed unit test","testCameraGeom.py has two tests with the same name: testCameraGeomUtils. Only one will run. It also has a few unused imports.",NULL
DM-902,"Camera.transform cannot transform from a detector-based CameraSys","Camera.transform is supposed to be able to transform from any valid CameraPoint to any supported CameraSys or CameraSysPrefix (in which case it looks for a suitable detector). However, if the CameraPoint is on a known detector, the transformation fails. Here is an example:  Also, I don't see any unit tests that exercise Camera.transform from or to a detector-based coordinate system. The following code shows the problem and might be the basis of some additional tests.  {code} #!/usr/bin/env python2 import lsst.afw.cameraGeom.testUtils as testUtils lsstCamWrapper = testUtils.CameraWrapper(isLsstLike=True) camera = lsstCamWrapper.camera  import lsst.afw.cameraGeom as cameraGeom import lsst.afw.geom as afwGeom  # get a detector by name det = camera[""R:1,0 S:1,1""] # convert from PIXELS to FOCAL_PLANE and PUPIL coordinates detPoint = det.makeCameraPoint(afwGeom.Point2D(25, 43.2), cameraGeom.PIXELS) # position on detector in pixels fpPoint = det.transform(detPoint, cameraGeom.FOCAL_PLANE) # position in focal plane in mm pupilPoint = camera.transform(detPoint, cameraGeom.PUPIL) # position in pupil, in radians  # convert PUPIL to PIXELS. The target system (PIXELS) is detector-based, so you may specify a detector # or let the Camera find a detector. # * To specify a particular detector, specify the target coordinate system as a CameraSys; #   this is faster than finding a detector, and the resulting point is allowed to be off the detector. # * To have the Camera find a detector, specify the target coordinate system as a CameraSysPrefix #   (e.g. cameraGeom.PIXELS). It will search for a detector that overlaps the point; #   if it finds 0 or more than 1 then it will raise an exception. detPixelsSys = det.makeCameraSys(cameraGeom.PIXELS) # detPointOnSpecifiedDetector = camera.transform(pupilPoint, detPixelsSys) detPointOnFoundDetector = camera.transform(pupilPoint, cameraGeom.PIXELS) assert detPointOnFoundDetector.getCameraSys() == detPixelsSys # same detector # find a detector given a camera point (in this case find the detector we already have) detList = camera.findDetectors(fpPoint) assert len(detList) == 1 assert detList[0].getName() == det.getName() {code}",NULL
DM-905,"Improve JOIN syntax support","At the moment, the Qserv SQL parser throws an exception when it encounters AST nodes for the ""subquery"" production (in {{parser/FromFactory}}).  Note that in our grammar the table_ref production (which appears in FROM clauses) can be expanded as follows:  {code} table_ref -> table_ref_aux (qualified_join | cross_join)* -> table_subquery -> subquery -> ""("" query_exp "")"" -> ""("" query_term "")"" -> ""("" query_primary "")"" -> ""("" table_ref "")"" {code}  In other words, by disallowing subquery nodes in our from clauses, we've also thrown out the ability to parenthesize the constituent joins (running a query with parentheses in the from clause results in: ""{{Qserv error: 'Unknown ANTLR error}}'"").  Also, the IR ({{query/TableRef}}) cannot fully represent the structure of from clauses with parenthesized components.  We may want to fix this at some point, which would involve tweaking the IR, and being more discerning about subquery AST nodes. In particular, we would have to let through a subquery that reduces down to just a table_ref. ",NULL
DM-906,"flux.aperture fails to set nProfile","The flux.aperture algorithm fails to set the number of successful apertures.    N.b. on the HSC side the code uses Sinc apertures and handles this via a try block. ",NULL
DM-907,"flux.aperture.elliptical doesn't set nProfile correctly","The elliptical aperture inner loop looks like this.  If calculateSincApertureFlux throws an exception it looks as if nProfile isn't set (although some profiles may have been measured);  the simplest fix is to set it to i at the bottom of the loop.  In fact I don't think that calculateSincApertureFlux ever throws, so we don't know how many valid measurements were made.  {code}     for (int i = 0; i != nradii; ++i) {         afw::geom::ellipses::Axes outer(shape);         outer.scale(fac*radii[i]);          std::pair<double, double> flux =             algorithms::photometry::calculateSincApertureFlux(mimage,                                                               afw::geom::ellipses::Ellipse(outer, center),                                                               oradius/outer.getA());         oradius = outer.getA();          source.set(_fluxKey[i], flux.first);         source.set(_errKey[i],  flux.second);     }     source.set(_nProfileKey, nradii); {code}",NULL
DM-908,"ds9.dot(str, ... textAngle=XXX) broke ctype for all other types","The changes to support textAngle in ds9.dot unfortunately broke colour processing for all other symbol types. ",NULL
DM-91,"The snapCombineTask should sum entries in the calib object","The CameraMapper strips out EXPTIME and put it in the exposure calib object.  The SnapCombineTask does not have a mechanism for summing exposure times stored in calib objects.  A mechanism to do this should be added to replace the old mechanism of summing metadata (fixMetadata).",NULL
DM-910,"Move camera description raw files and conversion scripts","For obs_lsstSim, the camera description is generated by a few data files and scripts. Presently the script(s) are in bin/ and the description files are at the top level of description. This makes the scripts too easy to run, and confuses users. Please put them somewhere out of the way, with a text file explaining what they are, where the raw data came from, and how to process the raw data.",NULL
DM-917,"Provide Task documentation for FringeTask","See Summary",NULL
DM-918,"Provide Task documentation for SourceDetectionTask","See Summary",NULL
DM-919,"Provide Task documentation for SourceMeasurementTask","See Summary",NULL
DM-920,"Provide Task documentation for PhotoCalTask","See Summary",NULL
DM-921,"Provide Task documentation for AstrometryTask","See Summary",NULL
DM-922,"Provide Task documentation for CalibrateTask","See Summary",NULL
DM-923,"Provide Task documentation for InterpImageTask","See Summary",NULL
DM-924,"Provide Task documentation for MeasurePsfTask","See Summary",NULL
DM-925,"Provide Task documentation for ReplaceWithNoiseTask","See Summary",NULL
DM-93,"pcaPsfDeterminer has some undesirable code to compute kernel size","pcaPsfDeterminer contains this bit of code: {code:py}         if self.config.kernelSize >= 15:             self.debugLog.debug(1, \                 ""WARNING: NOT scaling kernelSize by stellar quadrupole moment "" +                 ""because config.kernelSize=%s >= 15; using config.kernelSize as as the width, instead"" \                 % (self.config.kernelSize,)             )             actualKernelSize = int(self.config.kernelSize)         else:             # compute size using self.config.kernelSize as a multiplier {code}  Paul says the ""if"" condition is a leftover from changing kernelSize from an absolute value to a multiplier, and he kindly offered to clean it up.",NULL
DM-931,"The distortion code in AstrometryTask assumes that the exposure has a Wcs even if there's no distortion","The distortionContext code in astrometry.py adjusts the input WCS for the distortion even if there is no input WCS and the adjustment would be by (0, 0) anyway. ",NULL
DM-932,"Attempting to assign a Task to a ConfigurableField raises a confusing exception","An exception is raised when you attempt to assign a task to a ConfigurableField of a Config, but the exception is confusing.  It would be a huge help if it explicitly said ""use retarget"", e.g.: ""cannot assign a task to a ConfigurableField; use retarget(NewTask)"".",NULL
DM-934,"Mandate use of super in python","As posted to lsst-data:  I see this pattern in our code:  {code:py} class FooTask(Task):    def __init__(self, whatever):        Task.__init__(self, whatever)        self.metadata.add(""task"", ""foo"") {code}  (and similarly for other methods, and other classes such as {{Config}}).  I would like to encourage people not to write {{Task.\_\_init\_\_}}, but write instead {{super(FooTask, self).\_\_init\_\_}} (and similarly for other methods, and other classes).  I understand that this is longer and the repetition of the class name opens possible maintenance problems (e.g., changing the name of the class).  The reason for using {{super}} is that it makes multiple inheritance easier and, in some cases it is required to make it operate properly.  For example, consider that I now want to make a version of {{FooTask}} that has some extra function (inherits from {{ExtraFunctionTask}}):  {code:py} class ExtraFunctionFooTask(ExtraFunctionTask, FooTask):    def __init__(self, whatever):        # ??? {code}  The question is, what should {{ExtraFunctionFooTask.\_\_init\_\_}} call?  Well, if it explicitly calls the {{\_\_init\_\_}} methods of the two parent classes, then it ends up calling {{Task.\_\_init\_\_}} twice, which can lead to trouble if, e.g., one of the parent classes modifies a value initialised by {{Task.\_\_init\_\_}}:  {code:py} class ExtraFunctionTask(Task):    def __init__(self, whatever):        Task.__init__(self, whatever)        self.metadata.add(""task"", ""extraFunction"") {code}  so calling {{ExtraFunctionTask.\_\_init\_\_}} followed by {{FooTask.\_\_init\_\_}} (or the other way around) would result in a {{self.metadata}} containing only one element (because {{Task.\_\_init\_\_}} initialises {{self.metadata}}).  This is a simple, contrived example, but it serves to illustrate the general problem.  Python provides super for this reason --- calls to {{super()}} march along the ""Method Resolution Order"", so that each parent class' method is only called once (see https://www.python.org/download/releases/2.3/mro/).  The problem is, if you're going to use super at all, then all parent classes need to use super (otherwise the chain gets interrupted).  I suggest that this is the Pythonic way of referring to the super class, and failure to use it may unnecessarily limit the use of our code.  Common objections:  * I've heard that {{super}} should be considered harmful (https://fuhm.net/super-harmful/) ==> The objections raised here basically boil down to ""use {{super}} always or not at all"", and ""you need to be careful/disciplined"".  * It's harder to read / it's not clear ==> Yes, it's annoying that you have to repeat the name of your class (which can also lead to maintenance problems if you change the name of the class), but that's an unfortunate limitation of our language of choice (python 2; it's fixed in python 3).  Nevertheless, it's the pattern provided by the language, so it's arguably idiomatic, and therefore should be readily recognisable by all python programmers.  I suggest that it will become clearer and easier to read as we become more practised using it.  * There are some weird corner-case issues with it ==> {{super}} doesn't always resolve to the super-class of the class that's calling {{super()}}.  In fact, this is a feature (in the example above, it's why {{Task.\_\_init\_\_}} doesn't get called twice).  This should only be a problem when debugging objects with multiple inheritance, and it is solved by understanding the MRO. You can always look at the {{\_\_mro\_\_}} attribute of your object.  * It only matters with multiple inheritance ==> Kinda.  It only matters if someone wants to use multiple inheritance using something we've written.  But as I've demonstrated above, choices made low in the hierarchy level (e.g., how {{FooTask}} calls the super class) affects the ability of higher-level components to effectively use multiple inheritance.  If someone wants to use our classes in a multiple inheritance (and I do) and we haven't used super, then we're preventing them from making use of our code.  For the sake of users of our code who want to use multiple inheritance, please consider mandating the use of super.",NULL
DM-935,"Make sure the install and the lsst package use https:// in EUPS_PKGROOT","For security reasons, we should switch to using the https:// versions of the URLs (once appropriate certificates are installed to sw.lsstcorp.org).",NULL
DM-938,"testPipeQaTask.py fails due to lack of obs_subaru","tests/testPipeQaTask.py master fails in testing_pipeQA due to obs_subaru not being setup. This is making buildbot unhappy.  So…do we add obs_subaru to the stack or change the unit test to not require it, or a compromise: run some tests but not others if obs_subaru is not available.  In case you want to see the error log, it is on the lsst machines here: /lsst/home/lsstsw/build/FailedLogs/6/testing_pipeQA/testPipeQaTask.py.failed",NULL
DM-939,"Please add --show glob=PATTERN to cmdLineTask","I almost always use --show config as e.g. {code} processCcd.py ...  --show config | grep threshold {code}  Please support {code} --show glob=*threshold* {code} (N.b.  any csh users out there will need to quote the *s)  (I already have an implementation for my lightweight process-this-fits-file-for-me script) ",NULL
DM-940,"sims_catUtils's testAllObjects.py test is failing with ""no route to host""","sims_catUtils's testAllObjects.py test is failing with a complain ""No route to host"". This is making buildbot unhappy. Any ideas on how to fix this? Is this the issue that the UW database has been closed to outsiders? Is there a reasonable way to rewrite the tests to not use that database?",NULL
DM-941,"tests/wrap.py fails on master","pex_config's unit test tests/wrap.py is failing with a long error message (see below) when built master against master.  {code} lsst11$ cat pex_config/wrap.py.failed  tests/wrap.py  ...EE.E. ====================================================================== ERROR: testDefaults (__main__.NestedWrapTest) Test that C++ Control object defaults are correctly used as defaults for Config objects. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 89, in testDefaults     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/nfs/lsst/home/lsstsw/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testInt64 (__main__.NestedWrapTest) Test that we can wrap C++ Control objects with int64 members. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 95, in testInt64     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/nfs/lsst/home/lsstsw/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testReadControl (__main__.NestedWrapTest) Test reading the values from a C++ Control object into a Config object. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 82, in testReadControl     config.readControl(control)   File ""/nfs/lsst/home/lsstsw/build/pex_config/python/lsst/pex/config/wrap.py"", line 212, in readControl     __at=__at, __label=__label, __reset=__reset)   File ""/nfs/lsst/home/lsstsw/build/pex_config/python/lsst/pex/config/wrap.py"", line 217, in readControl     self.update(__at=__at, __label=__label, **values)   File ""/nfs/lsst/home/lsstsw/build/pex_config/python/lsst/pex/config/config.py"", line 515, in update     field.__set__(self, value, at=at, label=label)   File ""/nfs/lsst/home/lsstsw/build/pex_config/python/lsst/pex/config/config.py"", line 310, in __set__     raise FieldValidationError(self, instance, e.message) FieldValidationError: Field 'a.q' failed validation: Value 4 is of incorrect type long. Expected type int For more information read the Field definition at:   File ""/nfs/lsst/home/lsstsw/build/pex_config/python/lsst/pex/config/wrap.py"", line 184, in makeConfigClass     fields[k] = FieldCls(doc=doc, dtype=dtype, optional=True) And the Config definition at:   File ""/nfs/lsst/home/lsstsw/build/pex_config/python/lsst/pex/config/wrap.py"", line 131, in makeConfigClass     cls = type(name, (base,), {""__doc__"":doc})   ---------------------------------------------------------------------- Ran 8 tests in 0.012s  FAILED (errors=3) lsst11$  {code}",NULL
DM-942,"ap unit test sourceClusterTable is failing","ap unit test sourceClusterTable is failing (on master built against master of other packages):  {code} tests/sourceClusterTable s Running 1 test case... tests/sourceClusterTable.cc(239): error in ""testSourceClusterTable"": check incat.getTable()->getMetadata()->nameCount() == outcat.getTable()->getMetadata()->nameCount() failed [2 != 1]  *** 1 failure detected in test suite ""SourceClusterTable"" {code}",NULL
DM-943,"Workers contact Archive DMCS for Distributor info","On startup, workers contact the archive DMCS for info on which distributor to contact.",NULL
DM-947,"Provide eups tag for build onbuilbot status email","Provide the eups-tag assigned to a build within the email response.  Currently it needs to be deduced from the BB stdio log.",NULL
DM-948,"Many examples mis-test for afwdata being setup and reference afwdata files that have moved","Many examples in afw mis-test if afwdata has been setup, and crash rather than printing the expected message. Also, many examples reference files in afwdata that have been moved (or, in a few cases, deleted).  Also, several unittests have the same inicorrect test for afwdata being setup. At present no afw tests run (or are even built) if afwdata is not setup, but in the interest of future improvements, I propose to fix those tests, as well.",NULL
DM-949,"setup afw does not setup afwdata","When I setup afw it does not setup afwdata, even though I have a version  declared current. I see this on both lsst11 and my Mac, and using both the 8.0.0 version and the current master.  I suspect this must be a new problem else others would have complained long before now. Another mystery is that ups/afw.table file contains setupOptional(afwdata) and setupOptional(pyfits) and pyfits is setup but afwdata is not.  Here is a log setting up afw 7.3.1.1+1 on lsst11 (where is afw 8.0.0?):  {code} localhost$ ssh lsst11 Last login: Mon Jul  7 19:04:39 2014 from d-128-208-13-159.dhcp4.washington.edu lsst11$ lsst lsst11$ setup afw -v Setting up: afw                             Flavor: Linux64    Version: 7.3.1.1+1 Setting up: |daf_base                       Flavor: Linux64    Version: 7.3.1.1+1 Setting up: |utils                          Flavor: Linux64    Version: 7.1.2.0+3 Setting up: |base                           Flavor: Linux64    Version: 7.3.1.1+1 Setting up: |boost                          Flavor: Linux64    Version: 1.51.0+4 Setting up: |python                         Flavor: Linux64    Version: 2.7.6+1 Setting up: |tcltk                          Flavor: Linux64    Version: 8.5.12+1 Setting up: |sqlite                         Flavor: Linux64    Version: 3.7.14+1 Setting up: |sconsUtils                     Flavor: Linux64    Version: 6.2.0.0+3              sconsUtils 6.2.0.0+1 is currently setup; overriding with 6.2.0.0+3 Setting up: |scons                          Flavor: Linux64    Version: 2.1.0+4              scons 2.1.0+5 is currently setup; overriding with 2.1.0+4 Setting up: |doxygen                        Flavor: Linux64    Version: 1.8.2+1 Setting up: |swig                           Flavor: Linux64    Version: 2.0.4+3 Setting up: |pex_exceptions                 Flavor: Linux64    Version: 6.2.0.0+5 Setting up: |numpy                          Flavor: Linux64    Version: 1.8.0+1 Setting up: |daf_persistence                Flavor: Linux64    Version: 7.3.1.0+1 Setting up: |mysqlclient                    Flavor: Linux64    Version: 5.1.65+2 Setting up: |pex_logging                    Flavor: Linux64    Version: 7.3.1.0+1 Setting up: |pex_policy                     Flavor: Linux64    Version: 6.2.0.0+6 Setting up: |pex_config                     Flavor: Linux64    Version: 7.3.1.0+1 Setting up: |ndarray                        Flavor: Linux64    Version: 7.3.1.0+1 Setting up: |eigen                          Flavor: Linux64    Version: 3.1.1+1 Setting up: |fftw                           Flavor: Linux64    Version: 3.3.2+1 Setting up: |cfitsio                        Flavor: Linux64    Version: 3310+1 Setting up: |wcslib                         Flavor: Linux64    Version: 4.14+2 Setting up: |xpa                            Flavor: Linux64    Version: 2.1.14+1 Setting up: |minuit2                        Flavor: Linux64    Version: 5.22.00+1 Setting up: |gsl                            Flavor: Linux64    Version: 1.15+1 Setting up: |pyfits                         Flavor: Linux64    Version: 3.1.2+1 lsst11$ eups list afwdata    6.2.0.0        6.3.0.0    	current lsst11$ eups --version EUPS Version: 1.2.32 {code}  I have the same problem when I setup afw 8.0.0.3 on my Mac. Maybe it's some kind of configuration error, but what? ",NULL
DM-950,"""eups distrib install --nodepend"" seems to have a bug","For next case :  ||product||version|| |PROD | 1| |DEP1 | 1| |DEP2 | 2|  It seems the algorithms will install PROD-v1 but also DEP1-v1 because i has the same version number.  This pull request propose a trivial fix. Please note that no real tests of this bug have been performs for time constraints.  Code is available here : https://github.com/RobertLuptonTheGood/eups/pull/20",NULL
DM-952,"Refactor ""Packages & Tasks"" chapter in the S/W User Guide","Update and refactor the ""Packages & Tasks"" chapter of the SWUG to give a better overview of the Stack, and to incorporate a recent presentation by K-T to the DES-LSST workshop (see attached). Also update the Package Index, which should also include links to the task-level documentation, which is now being written. ",NULL
DM-955,"Buildbot's on-repo-change cron job needs to check the stash repositories","Currently the on-source-change cron trigger of buildbot processing only checks the gitolite-fronted repositories. It also needs to check the stash-fronted repositories.  This becomes urgent when DM's git repos  moves over to stash management.",NULL
DM-959,"In some circumstances lsst_build is building an incoherent / malfunctioning stack","In several occasion I got a malfunctioning stack. A careful analysis of the last instance of the problem showed that the afw library was wrong, resulting in a very inefficient {{afwMath.convolve}} function. I haven't been able to track the exact origin of the problem, nevertheless I think that it could be due to lsst_build running in an unclean environment mixing different stack version. The other possibility is a bug in lsst_build.",NULL
DM-960,"Remove test of non-existant source table object","An AP unittest failure was not caught and reported when it first occurred. Now that the unittest failures are correctly being caught, the packages with such failures are now being tracked down and repaired.  Thies failure is one of them.  Regarding this failure, Perry said: ""... the one in AP might have been caused by a change Jim and I made 2 mos. ago.  Has it been a long time since the tests on ap was examined?  I don't think anything that I did recently could have caused this.  I am going to suggest that we disable this assert, since the test runs fine without the one assert on lines 238-239 of $AP_DIR/tests/sourceClusterTable.cc.  I just commented out this assert, and the test is otherwise OK. """,NULL
DM-961,"testing_pipeQA attempts to access 'cast_Raft()' which is no longer defined.","Testing_pipeQA attempts to access 'cast_Raft()' which is no longer defined.  It is used in two packages:  testing_pipeQA    e.g.   ./testing_pipeQA/python/lsst/testing/pipeQA/QaFigures.py:            raft   = cameraGeom.cast_Raft(r)   ./testing_pipeQA/python/lsst/testing/pipeQA/figures/FpaQaFigure.py:            raft   = cameraGeom.cast_Raft(r)   ./testing_pipeQA/python/lsst/testing/pipeQA/figures/QaFigureUtils.py:        raft = cameraGeom.cast_Raft(r) ./testing_pipeQA/python/lsst/testing/pipeQA/CameraInfo.py:            raft = cameraGeom.cast_Raft(r)  And  in afw/examples (a good reason why they should be periodically tested): e.g.   ./afw/examples/showCamera.py:        raft = cameraGeom.cast_Raft(raft) ",NULL
DM-962,"Move sims packages to a separate buildbot","There should be two separate buildbots for the LSST Stack and the Sims distribution.  The users of the two products do not necessarily overlap, so combining them into one distribution is inappropriate.  Their development schedules also do not overlap, and we should endeavor to limit the impact of changes in one on the other.  Having them both built in the same process, as is done now, is arguably a bug.",NULL
DM-965,"Create a ""tutorials"" package in /contrib","Create a ""tutorials"" package with helper scripts, configurations, and data files, and place under version control (git). Upload the package to the LSST cgit repository (in /contrib). Update the tutorials to instruct users how to download and use this package, and remove references to scripts attached directly to the tutorials pages. ",NULL
DM-968,"Add wavefront job and wavefront sensor jobs","Add the wavefront job (the extra rep/dist pair) and the wavefront sensor jobs (workers).",NULL
DM-969,"Add profiling command-line option","This is a pull request for adding a command-line option to enable and dump the python profile of a Task.",NULL
DM-97,"Check the right way to programmatically define an EUPS tag","Right now, lsst.ci.build has a function:  {code} def declareEupsTag(tag, eupsObj):     """""" Declare a new EUPS tag         FIXME: Not sure if this is the right way to programmatically                define and persist a new tag. Ask RHL.     """"""     tags = eupsObj.tags     if tag not in tags.getTagNames():         tags.registerTag(tag)         tags.saveGlobalTags(eupsObj.path[0]) {code}  Is this the right way to define a tag?",NULL
DM-970,"Create Demo Video","Create a screen capture video demonstrating all this working together.",NULL
DM-971,"Task initialisation should validate the config","Config validation and freezing is currently carried out by the argument parser.   This ticket requests that the config validation (also?) be carried out by the task constructors.  This should be an essentially free operation, which may require caching the status in the Config object. ",NULL
DM-972,"Task initialisation should freeze the config","Config validation and freezing is currently carried out by the argument parser.   This ticket requests that the config freezing (also?) be carried out by the task constructors.  This should be an essentially free operation, which may require caching the status in the Config object.  N.b.  It is not clear that we are quite ready to implement this ticket as the per-dataset behaviour of at least some tasks can only be modified by changing the config.  This is not desirable (and Jim thinks that it may not always work), but it may require changing the run API before we work on this issue. ",NULL
DM-974,"Remove ""references"" and ""referenced by"" sections of Doxygen documentation","Our Doxygen documentation includes sections labelled ""References"" and ""Referenced By"" which are unfortunately nearly useless. They include references to and from instance variables, which often results in long lists of uninteresting links. Often these sections are so long that they obscure the more useful text around them.  I issued an RFC to get rid of them which produced several supporting comments and not dissention. The relevant doxygen config parameters are:  {code} REFERENCED_BY_RELATION If the REFERENCED_BY_RELATION tag is set to YES then for each documented function all documented functions referencing it will be listed.  REFERENCES_RELATION If the REFERENCES_RELATION tag is set to YES then for each documented function all documented entities called/used by that function will be listed. {code}  This should be trivial to change in all packages because there is one file that is used for the default configuration of all our Doxygen: base/doc/base.inc (thanks to RHL for the pointer).",NULL
DM-985,"Implementation of Base DMCS submission of DAGs","Base DMCS currently submits jobs individually.  Submitting them via a DAG buys us a lot, including automatic restart/resubmit of failed jobs and a resubmit file of all jobs that failed (if all retries fail).",NULL
DM-986,"Add Task method decorators","This is a pull request for method decorators that I've found helpful on the HSC side.",NULL
DM-987,"Stories that cleanup the codebase in non-intrusive ways","A home for cleanup that needs to be done to the codebase (e.g. removing unused functions in anonymous namespaces).",NULL
DM-988,"Remove unused overload of doMeasureCentroidImpl in SdssCentroid.cc","There is an unused instantiation of doMeasureCentroidImpl in an anon namespace in SdssCentroid.cc.  Please remove it. ",NULL
DM-990,"Possible stuck WaitForResponse in XrdCl","Not sure this is a problem. Can we determine anything from the stack trace? {code} Thread 1 (Thread 0x7fe5d332e700 (LWP 27021)): #0  0x00007fe5d223c3b9 in syscall () from /lib64/libc.so.6 #1  0x00007fe5c7647a7b in XrdSys::LinuxSemaphore::Wait() () from /u1/lsst/xrdNew-inst/lib64/libXrdCl.so.2 #2  0x00007fe5c7666608 in XrdCl::SyncResponseHandler::WaitForResponse() () from /u1/lsst/xrdNew-inst/lib64/libXrdCl.so.2 #3  0x00007fe5c76666bc in XrdCl::MessageUtils::WaitForStatus(XrdCl::SyncResponseHandler*) () from /u1/lsst/xrdNew-inst/lib64/libXrdCl.so.2 #4  0x00007fe5c767aaa6 in XrdCl::File::Truncate(unsigned long, unsigned short) () from /u1/lsst/xrdNew-inst/lib64/libXrdCl.so.2 #5  0x00007fe5c9bb99a6 in XrdSsiTaskReal::Kill() () from /u1/lsst/xrdNew-inst/lib64/libXrdSsi.so.1 {code} The process trapped a Ctrl-C, and issued XrdSsiRequest::Finished(true), but the XrdCl code seems to be waiting a message (from the server?). In this case, the server process has already been killed, so no message will be received. It seems strange that the client could be waiting for a message when the connection is already dead. Perhaps the condition variable missed waking up the appropriate waiters?  I will attach the full pstack. Sorry, it wasn't a debug build(?) so there are no line numbers.  Would you have a look at the code around the stack trace? If you can't determine anything, that's okay. We can close/defer this until we get more information.",NULL
DM-992,"Research whether bittorrents could be useful for our data distribution","It is somewhat wild idea, and not immediately obvious because bittorrents were built for completely different reasons and for different communities, but perhaps we could use bitttorents for distributing / replicating our database files between different nodes within a cluster, and/or between different data centers. (Thanks to Daniel for suggesting)",NULL
DM-996,"A Release should also capture the doxygen document under the new Release's Name","When a new Release is generated, the associated doxygen documentation should also be tagged within the web-portal's doxygen document directory and, possibly, also archived in a more permanent location.",NULL
DM-997,"Test workaround in meas_base","Testing workaround in meas_base which may be the cause for asemi-random failure in one of the unit tests.  Note: I wanted to test in a private u/rallsman/<xx> space but was unable to ""git push origin u/rallsman/<xx>""  due to hook reject.  Note: I wanted to attach this Issue to the upcoming version 9.0 but was rejected since it wasn't pre-defined.",NULL
RFC-10,"Enable Markdown processing of Doxygen comment blocks","Since version 1.8.0, Doxygen has supported pre-processing comment blocks with [Markdown|http://www.stack.nl/~dimitri/doxygen/manual/markdown.html]. This enables the author of the documentation to include constructs such as lists, verbatim text blocks, and so on, which will then be appropriately formatted in the output.  By default, Doxygen ships with Markdown support enabled and notes that it should be disabled ""only in case of backward compatibilities issues"". However, it is explicitly disabled across the LSST stack by [line 281 of {{doc/base.inc}} in the {{base}} package|https://github.com/LSST/base/blob/master/doc/base.inc#L281].  Unfortunately, processing through Markdown does not leave text unmodified. For example, the string {code} \_\_init\_\_ {code} becomes {code} __init__ {code} when processed. Therefore, text which is not processed through Markdown but which was written on the assumption that it would be, or vice versa, may not be appropriately handled. In particular, this is the reason for the missing examples in the [{{pipe_tasks}} documentation|http://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_01_08_11.01.27/pipe_tasks_write_task.html#pipeTasks_writeTask_initMethod].  Given the above, we should have a policy on whether Markdown will be enabled when producing documentation for the LSST stack. Given that # it is enabled by default in Doxygen, and # the formatting it enables can help produce better structured and more readable documentation,  my preference is to enable it by default across the LSST stack. If this RFC is accepted, I will do that by changing the setting in the {{base}} package and adding a note of this decision to the [documentation standards|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards]. If this RFC is rejected, we should still update those standards to note that Markdown will _not_ be run.",NULL
RFC-100,"Switch from boost::shared_ptr to std::shared_ptr","I would like to switch our code from boost::shared_ptr to std::shared_ptr. If allowed, I would extend this to include all other boost:: classes that now have standard equivalents, such as boost::unordered_map.    boost works, so why bother? I see several advantages:  * It makes our code more standard, so less likely to confuse new developers and outsiders.  * Code that uses a mix of unique_ptr and shared_ptr would be less confusing.  * It would also give us a more accurate picture of what parts of boost we truly rely on, in hopes of eventually ending our use of boost entirely.    Acceptance of this RFC does not imply that it is a high priority. It simply gives us a go-ahead to write a ticket and implement the ticket when we have time. I have not looked deeply at the code to estimate how much effort is required, but I don't expect it to be very much. We have to change the definition of PTR and CONST_PTR and then clean up some additional code.",NULL
RFC-101,"generalize risk 54 from security related disruptions to general IT continuity due to link failure ","Risk 54 is specific to failure of the network between Chile and NCSA due to security incidents with the link.    My judgments is that this topic would not merit a specific risk.  I'd propose generalizing the risk to the risk to IT continuity due to any sort of link failure. ",NULL
RFC-102,"Deprecate Image <<= and stop using it in our stack","The Image operator {{<<=}} that copies pixels is surprising to most users because it is not a common operator in C++ and is virtually unknown in Python. As [~rhl] points out {{image\[:] = other}} already works so we don't actually need {{image <<= other}}.    For the sake of backwards compatibility I propose that we remove all use of {{<<=}} in our own code and deprecate {{<<=}} in the release notes and afw documentation. Then remove {{<<=}} at some future date.",NULL
RFC-103,"Add pydl as an external package dependency","obs_sdss includes a copy of what looks like the yanny reader from pydl. We should just have pydl as a third party external dependency. It might have a few other things we may want to use, though probably only for using sdss data.    Worth considering is whether we might want to use anything else provided by pydl, or if we just want the yanny reader. If the latter, it would just be a dependency for obs_dss.    https://github.com/weaverba137/pydl",NULL
RFC-104,"GitLFS as a solution to storing data files in repos","SQuaRE proposes to use the open source [GitLFS|https://git-lfs.github.com/] protocol as a solution to storing large files in repos in general, and binary-heavy repos like awfdata in particular. We are proposing this solution after evaluation because it has a workflow very close to what people are used to already, and it allows almost normal interaction with GitHub, while storing the files on an in-house backend server so that we do not have to pay hosting fees for the data. We are happy that this is a maintainable solution and friendly to non-project contributors; while GitLFS is still a young project barely out of beta, we have noted a good rate of bugfixing and have had good interactions with the development team. It also has had good adoption beyond GitHub, from both Microsoft and Atlassian.     As part of the evaluation, [~jmatt] deployed a GitLFS cloud-based server on the NCSA Nebula OpenStack cluster which we now invite you to try out. We have set up a test repository in the lsst organisation that is seeded with afwdata:    https://github.com/lsst/afwdata-cowboy    Feel free to do whatever you want with this, it will be deleted once the RFC is over. If you are in the Data Management team, you should be able to push to the repo.     Instructions on how to interact with the repository are in the README:    https://github.com/lsst/afwdata-cowboy/blob/master/README.md    Note that this is a LARGE repository (intentionally) so expect some initial setup time and make sure you're on good bandwidth.     Feel free to improve the README for particular platforms, etc. We will save it for inclusion in the real afwdata if the RFC is adopted.  ",NULL
RFC-105,"Update cfitsio to version that adds bz2 support","Our current version of cfitsio is 3.36 (6 December 2013), while 3.37 (3 June 2014) adds support for bz2 files, which would make my #3033 life much easier. We should upgrade.    Here's the changelog, for reference:    http://heasarc.gsfc.nasa.gov/FTP/software/fitsio/c/docs/changes.txt    NOTE: it appears the above statement is not true: 3.37 does not actually support bz2. This is being dealt with upstream. I've modified the title to better represent what this RFC actually is requesting: updating to whatever version actually does properly support bz2-compressed files (likely 3.38).",NULL
RFC-106,"Wrapping docstring paragraphs at 79 characters","Our Python (and C++) coding standard (https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard) states that    ""Line Length MUST be less than or equal to 110 columns""    (and I also anticipate that any future RFCs to bring our coding style into conformance with PEP-8 community standards will always be tempered with the allowance to use 110-character wide code lines).    A consequence of this is that text paragraphs in docstrings will also be hard wrapped at 110 characters. Having blocks of text that are formatted as 110-character wide paragraphs has two problems:    # These paragraphs are 150% wider than any typographer would dare typeset. Paragraphs this wide are hard to read, especially with the small leading (vertical line-to-line space) that most of us use in our code editors. 110-character wide doc paragraphs are bad for developers.  # Terminal-based Python help utilities do not reformat paragraphs, and instead show the docstring verbatim. Try {{help(numpy.array)}} in a Python terminal or {{numpy.array?}} in a Jupyter terminal/notebook. This means that all of our users will need to have terminals that are at least 110 characters wide. In the era of 13"" laptop screens with tmux panes divided vertically three times, this isn't a reasonable thing to ask. 110-character wide doc paragraphs are bad for users.    A solution is to adopt the Python community standard where docstrings are wrapped at 79 characters. This convention seems to work well, and is what Python users expect.    Since C++ docstrings are also being propagated into Python {{\_\_doc\_\_}} objects, the docstrings we write in C++ and SWIG .i files should also be wrapped at 79 characters.    Therefore I request that we change our coding standard to have docstrings be formatted at 79 characters.    -**Amendment:** On the grounds of making prose readable, I also propose that code comments also be wrapped at 79 characters.-",NULL
RFC-107,"Wrap python docstring text after 79 characters","(This RFC is a re-scope of RFC-106, which has been withdrawn.)    Our Python coding standard (https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard) states that    bq. “Line Length MUST be less than or equal to 110 columns""    A consequence of this is that text in docstrings will also be wrapped at 110 characters.    *I propose that we change our Python coding standard to state that any text extracted as user-facing documentation (such as docstrings) have a maximum line length of 79 characters.*    This proposal is motivated by our goal to create an open source Python astronomy package with legacy value and broad adoption. Documentation is a key product that we deliver to users. Since our users are necessarily part of the Python astronomy community, we must appreciate the expectations that the Python community has for documentation. Users of open source Python expect <=79 character line lengths for docstrings when they ask for help in a Python terminal.    This norm, backed universally in the style guides of scientific python packages, originated for two reasons:    # These paragraphs are 150% wider than any typographer would dare typeset. Paragraphs this wide are hard to read, especially with the small leading (vertical line-to-line space) that most of us use in our code editors. 110-character wide doc paragraphs are bad for developers.  # Terminal-based Python help utilities do not reformat paragraphs, and instead show the docstring verbatim. Try {{help(numpy.array)}} in a Python terminal or {{numpy.array?}} in a Jupyter terminal/notebook. This means that all of our users will need to have terminals that are at least 110 characters wide. In the era of 13"" laptop screens with tmux panes divided vertically three times, this isn't a reasonable thing to ask. 110-character wide doc paragraphs are bad for users.    The second reason is the one that our users, and there for I, care most deeply about. This RFC proposal strives to present our package as consistently as possible with other Python packages in the ecosystem.    Note that this RFC says nothing about the lengths of non-documentation lines in source. Nor does it say anything about the lengths of documentation lines in other languages such as C++ even if they may eventually become docstrings in Python.",NULL
RFC-108,"Make --show config=glob case insensitive","The pipe_base parser supports  {code}  --show config=PATTERN  {code}  where PATTERN is a glob for the parameter you want to find.  With the advent of documentation strings with the parameters this is no longer equivalent to piping the output to grep, so it's likely to see more use.    One annoying feature is that glob matches are case sensitive, and you often want to look for a parameter without knowing it's case (Is it \*Background\*"" or \*background""?).  There's a workaround (\*\[Bb\]ackground*) but this is inconvenient.    This RFC proposes making the match case insensitive.  It's easy to do using fnmatch.translate, and I'm willing to do the work (i.e. push the patch and ask for a review)",NULL
RFC-109,"In C++, pass modifiable objects by pointer not reference","The coding standards are unambiguous that immutable large objects should be passed by const reference (small objects such as STL iterators should probably be passed by value), but they are not clear about objects that can be modified.    This RFC proposes that we pass such objects by pointer, not reference, as it is then immediately clear to the reader of the calling function that the object in question may be changed.    The objection is usually that it makes the called function harder to read, but I find this unconvincing.  We've successfully used C for many many years writing `*ptr`, and there's been syntactic sugar for `(*ptr).` since the earliest days of the language.  In fact, I think it's an advantage to have to write the explicit dereferences -- it reminds you that you're modifying an object with a life out of your function's scope.    (P.S. this came up in a code review, and I was asked to file an RFC; so I am)",NULL
RFC-11,"""suspect"" flag for measurement outputs","This is a rerun of an previous lsst-data RFC email thread (subject: 'RFC: ""suspect"" flags for slots') that never converged.  I'm making essentially the same proposal, but I'll try to provide more detail and motivation.  h4. Current Status  All measurement algorithms record their state in a set of flag fields, which are typically set to indicate a particular failure mode (though some may not suggest a problem with the results).  In addition, each algorithm also sets a ""general failure"" flag, often to the OR of all flags that indicate specific failures.  This general flag is also set when an unexpected exception is thrown (this also results in a warning message in the logs).  When measurement outputs are accessed via the slot interface, however, they must conform to a consistent interface, and hence only the general failure flag can be accessed via a getter in the SourceRecord class.  Other flags can still be accessed using the alias that defines the slot (e.g. if the ""Shape"" slot is set to ""base_SdssShape"", then ""slot_Shape_flag_unweighted"" resolves to ""base_SdssShape_flag_unweighted"").  But because these flags are different for different algorithms, this is only useful for human consumers of the slots, not code that needs to determine generically whether a slot measurement is usable.  This is particularly important because the slots are the primary way earlier algorithms are used to feed later algorithms: flux algorithms that need a centroid or shape, for instance, use the Centroid and Shape slots.  These flags do not provide a way for an algorithm to indicate a partial success that resulted in a crude estimate that may be usable for downstream algorithms but should not be considered fully trustworthy.  Frequently - but not uniformly - algorithms indicate this state by setting the general failure flag while still providing an output value.  This forces downstream code to check not just the state of the flag, but also whether the measurement values are NaN.  h4. Proposal  I propose we add general ""suspect"" flag to all algorithms, and to the slots, which would be set instead of the failure flag when a reasonable but crude result can be obtained.  A full failure would be indicated by setting the current failure flag, and would generally not be accompanied by non-NaN outputs (or, if non-NaN outputs are recorded, they are considered so untrustworthy that they are only useful for debugging purposes).  Choosing whether to set ""suspect"" or ""failure"" is clearly a subjective, algorithm-dependent choice, and a science-quality, human-directed data analysis should always involve looking at the specific algorithm's detailed flags.  The ""suspect"" and ""failure"" flags will be intended more for quick, algorithm-independent QA analysis and, most importantly, other dependent measurement algorithms.  As such, the primary consideration in choosing whether to set ""suspect"" or ""failure"" should be whether the result is likely to be good enough to feed downstream algorithms.  In most cases, a dependent algorithm that receives a ""suspect"" input should mark its own output as ""suspect"" as well, but this may not always be the case.  For instance, a model-fitting algorithm may use the centroid as an input, but allow the centroid to vary as a free parameter as well, which could allow it to recover completely from a suspect centroid input.  If a dependent algorithm receives a ""failure"" as input, it will almost always just bail out early.  The last time this proposal was circulated, the discussion mostly centered on whether a single additional flag would be enough, and whether some other generic quality metric would be useful.  My opinion is that it would not be; we really want something that tells a downstream algorithm whether it should give up in advance (because its dependency failed) or proceed with caution (because its dependency is suspect), and I think it's best to leave that binary decision to the dependency, not the dependent.  h4. Examples   - Centroids should almost never fail complete, as they will start with the Peak position as a starting point, and just using that as an output is good enough to be called ""suspect"" instead.  - Least-squares fitting algorithms that reject a large fraction of pixels due to mask values or image boundaries will typically set ""suspect"" (with the threshold likely configurable), and set ""failure"" only if a higher threshold is set, or if the algorithm fails to converge.  - Weighted adaptive moments codes that fall back to unweighted moments (such as SdssShape) will set ""suspect"" for unweighted moments. ",NULL
RFC-110,"Consistent location for GitHub-based documents","The source for a many DM design documents is now hosted on GitHub. However, the location and naming of the repositories is haphazard: some exist in the {{lsst}} organization under repositories named according to the document ""handle"" (e.g. [LDM-152|https://github.com/lsst/LDM-152]); some under {{lsst}} but under more free-form descriptive names (e.g. [data_products|https://github.com/lsst/data_products], which is actually LSE-163); and some under {{lsst-dm}} (e.g. [dm_applications_design|https://github.com/lsst-dm/dm_applications_design], AKA LDM-151).    Following the model suggested by [~jsick], I propose that all documents should be contained in the {{lsst}} organization in a repository named for the document handle (e.g. ""LSE-163""). The description of the repository should provide the human-readable name (e.g. ""LSE-163: Data Products Definition Document"").    Documents which do not have project-asigned handles are not covered by this RFC.    If & when this RFC is adopted, I will rename the two repositories mentioned above to be consistent with the new scheme and audit the {{lsst}} and {{lsst-dm}} organizations for other documents, renaming them similarly. ",NULL
RFC-111,"Cleaner git status reports in packages (particularly external ones)","For performing automated repo analysis (e.g. answering the question ""have I made any uncommitted changes to this repo?"", or the bigger question ""which repos have I changed?""), it would be very useful to not have the ""git status"" log filled up with files that appeared during the build process. This is particularly bad for external packages, where all of the untarred files get dumped right into the build dir, and thus all come out as ""untracked files"" in git status.    One solution to this would be for the untarring and building of external packages to all happen in a subdir (say, build_ext) that is added to the .gitignore for each external package. This would probably require changes to eups_pkg, though I'd suspect they would be relatively simple changes.    For non-external packages, would it be enough to add _build.* to the .gitignores?",NULL
RFC-112,"Discussion of differential chromatic refraction (DCR)","[~sullivan] and [~reiss] have been doing a literature search on DCR.  There has been some discussion of the impact of DCR on difference imaging, in particular relating to template generation.  This RFD is to ask for interested parties to gather during the RFD slot (12:30-2:00PM Pacific/3:30-5:00PM Eastern) on Dec. 1 to discuss mitigation techniques for DCR.    A report on the current literature will be forthcoming.",NULL
RFC-113,"Changing config defaults in tasks","In doing some work with coadds, it's come up that some of the default values are not appropriate for the most generic use case: DM-4374.  Changing the config defaults is easy, but could lead to breakage of existing documentation.      My proposal that, in addition to the code change, it is enough to search confluence and c.l.o for any appearance of either the task name or the config parameter.  The documentation will be brought in line with the current master configuration unless the document in question specifically refers to a version of the stack predating the change.",NULL
RFC-114,"MariaDB and third-party package","Qserv is now switching from MySQL to MariaDB ([https://github.com/lsst/mariadb]), MariaDB build requires next system dependencies:  -  cmake zlib-devel xz-devel ncurses-devel    all of those packages are part of the CI system packages set except for xz-devel.    This RFC aims at adding MariaDB to eups packages and xz-devel to system packages (and document the add of the latter one in the install documentation).    Please note that xz-devel is named liblzma-dev on Debian-based systems.",NULL
RFC-115,"Discussion about the best way to automatically estimate the initial PSF","Currently, one of the configuration parameters is an initial estimate of the PSF FWHM (calibrate.initialPsf.fwhm).  processCcd can fail if this is not set properly.  This is problematic for two reasons:  1) The LSST science pipeline needs to be completely automated and can't depend on exposure-specific user input.  2) The LSST config philosophy is that there is a one set configuration setup for a full (re)run of a dataset.  If the initial PSF FWHM config parameter needs to change from visit to visit (because the seeing does change throughout the night), then this breaks our ""one config only"" philosophy.  Therefore, we need code that will automatically estimate the PSF FWHM.    A few options on how to proceed have already been mentioned but no clear path has emerged.  Therefore, we will have an RFD discussion on this topic.  Note that this will impact how single frame processing (e.g., processCcd) works.    A couple of issues/ideas to keep in mind:  -can we use an iterative procedure starting with a delta function and looping through most of the single frame processing steps a couple of times until we converge on a good PSF  -the single-visit CR rejection algorithm requires an initial PSF estimate  -how to deal with contamination from galaxies and ""junk"" which will mess up the PSF estimate  -if we use an external catalog to select the stars then this will require more steps (i.e. astrometry, matching, etc.)    Product of the discussion:  -We want to converge on the path forward on how to implement an automated initial PSF estimation code  -Who will do the work, or which team  -What is the priority  -On what timescale should this work be completed    Coordinates:  - RFD slot on Tuesday Dec. 15, 12:30pm PST   - bluejeans: ls.st/jmc",NULL
RFC-116,"Add methods to Wcs so one can determine if sky systems of two WCS match","As part of implementing DM-4162 I realized that it is helpful to know if two WCS have the same sky system (by which I mean coordinate system and equinox are equivalent). This allows a speedup of warping code. The following code for computing source position from destination position is slow but safe, since skyCoord will be converted to the srcWcs's coordinate system and equinox, if necessary:  {code}  skyCoord = destWcs.pixelToSky(destPos))  srcPos = srcWcs.skyToPixel(skyCoord)  {code}  The following code, which is what we use now for warping, runs nearly twice as fast. However, it only gives correct answers if the sky systems are the same for the two WCSs (a bug I am fixing as part of DM-4162).  {code}  Angle sky0, sky1  detWcs.pixelToSky(destPos[0], destPos[1], sky0, sky1)  srcPos = srcWcs.skyToPixel(sky0, sky1)  {code}    We should use the latter code when we can, for speed. But how to tell when we can?    I propose to add the following three methods to Wcs:  - getCoordSystem() -> coordinate system as an afw::coord::CoordSystem enum  - getEquinox() -> date of equinox as a double  - isSameSkySystem(Wcs const &wcs) -> true if the two WCSs have equivelent on-sky systems, meaning any one of:     • both have the same coordSystem and equinox     • both are ICRS, in which case equinox is ignored     • one is FK5 J2000 and the other is ICRS (not exact, but close enough)    An implementation can be found in afw tickets/DM-4162.",NULL
RFC-117,"Add sphgeom as an LSST package","The sphgeom C++ package has been converted to standard LSST stack form in DM-2262, and will be required for spatial support in the butler. I'd like to add it to etc/repos.yaml in lsstsw and make it part of CI.",NULL
RFC-118,"Add getCoordSystem to lsst::afw::Coord","I propose adding method {{Coord.getCoordSystem() -> CoordSystem enum}}    It would often be helpful to be able to find out what coord system is used for a particular {{lsst::afw::Coord}}, especially since many methods return {{PTR(Coord)}}, leaving you no idea what the data you can access (position and epoch) means. Other use cases include:  - Some {{Wcs}} constructors allow one to provide a {{Coord}} to define {{CRVAL}} and {{EQUINOX}}. Logically the coordinate system of the WCS should also be taken from this coord (e.g. {{CTYPE...}} and {{RADESYS}}), but there is no way to do that at present.  - Unit testing  - It might be useful to short-circuit conversion (though if conversion code is written correctly, it should not help enough to justify the extra test and code complexity).    I also propose to add a few more minor cleanups at the same time. In my opinion these are too minor to justify their own RFC, but are worth making public and discussing here:    1) Add a new {{CoordinateSystem}} enum {{UNKNOWN = -1}} (leaving the numeric values of the existing enums unchanged). Two uses cases:  - Initialize {{Wcs._coordSystem}}. This is presently this is done by casting -1, which is needlessly obscure.  - As a value when the coordinate system is truly unknown or invalid (I propose conflating these two meanings with the one enum because this is rare).    2) Remove the method {{getClassName}} which is primarily used to support {{Coord.__repr__}} in Python, and that can be done in other ways.    I would also like to make the base class {{Coord}} pure virtual, but that will be a much more disruptive change, and justifies having its own RFC. Meanwhile I propose retaining the current strange behavior for conversions involving {{Coord}}, which is to assume that the Coord has whatever coordinate system you are converting to (hence the ""conversion"" will be at most a precession).",NULL
RFC-119,"Change warpImage and warpExposure to take shared_ptr<Wcs> instead of Wcs","Due to recent changes (DM-4162) the {{warpImage}} and {{warpExposure}} functions now clone both WCS arguments (in order to make an xy transform). In reviewing the ticket [~jbosch] suggested changing the API to take {{shared_ptr<Wcs>}}, arguing that:  - Wcs are complex enough that we don't want to copy them needlessly. The slowdown may be significant when warping postage stamps.  - We essentially always have shared pointer to a WCS, because that's what you get out of an Exposure.    Please see his comment 2015-12-14 comment on DM-4162 for a more eloquent explanation.    [~jbosch] please feel free to edit this text as you see fit.    Alternative solutions are:  - Live with it. Accept the slowdown when warping small images.  - Solve the problem internally, e.g. make a variant xy transform that holds its WCSs by pointer. I have not thought of a clean solution and am very reluctant to create a duplicate xy transform that stores WCS by bare pointer, since it would be unsafe and requires code duplication.",NULL
RFC-12,"Measurement transformation system","DM-1074 describes the need for a system by which ""raw"" measurement quantities (e.g. fluxes, positions in pixels) can be transformed to global units (e.g. magnitudes, celestial coordinates).  In order to address this issue, I have prototyped a transformation system which is described in detail, including its motivation, requirements, and design, on [Confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=25690539]. An initial implementation is available on the {{u/swinbank/DM-1598}} branches of {{meas_base}} and {{pipe_tasks}}.  I request your comments on the overall design of the proposed system. (Detailed comments on the implementation can be addressed during code review.)",NULL
RFC-120,"Add pandas to third-party packages ","Sims needs to start using pandas (see e.g. https://github.com/lsst/sims_maf/tree/feature/MafSSO which is languishing in a branch because it uses pandas - we can't merge it without breaking master).     How can we get pandas added to the third party packages available to jenkins? Also, how do we make this available to users? Most of our sims users are using anaconda and have pandas installed as part of anaconda, but I understand that not all users are using anaconda. This aspect ties into RFC-50 (https://jira.lsstcorp.org/browse/RFC-50). ",NULL
RFC-121,"Prevent force pushing to master","It is currently possible to force push to master branch in git hub, and it is quite easy to mistakenly force push to master instead of a branch, or have someone new/inexperienced do it without realizing potential consequences. I am proposing we install a pre-hook for packages under https://github.com/lsst that prevents that, or at least prints a clear warning.",NULL
RFC-122,"Discussion of DM simulations needs","We need to provide Andy Connolly with a more detailed view of what simulations (cadence, catalog, and image) -- regardless of specific tools -- and at what quality levels DM  will need to develop, test, and verify its algorithms.  The DM group leaders should come up with their own list of simulation needs and we will then meet to discuss these to produce a final list.    Proposed slot: 2016-01-19, 12:30-2:00 Pacific.",NULL
RFC-123,"technical discussion of simultaneous astrometry integration","There has been a lot of work on simultaneous astrometry. Jim and Dominique have discussed how to bring it in line with how other measurements operate. I would like to have a technical discussion about how to get this done.    In preparation, those mentioned as watchers should take a look at the current interface, which lives in [meas_simastrom|https://github.com/lsst-france/meas_simastrom], and also take a look at the open tickets against [that product|https://jira.lsstcorp.org/browse/DM/component/12893].    Proposed time and date: 12:30 Pacific/3:30 Eastern on 26/Jan/16.  ",NULL
RFC-124,"Discuss plan for investigating astropy integration","I have been tasked with investigating the options for integrating Astropy into the LSST data management software. I would like to have a kick-off meeting for this with anyone interested in the topic where we can discuss the avenues for investigation and agree on tasks.    Proposed time and date: 12:30 Pacific/3:30 Eastern on February 2nd 2016.",NULL
RFC-125,"Add pyyaml package to build","{{daf_persistence}} uses YAML format policy files. In order to read YAML files we need a Python YAML reader. This RFC proposes (retroactively) that we adopt the PyYAML package and create a corresponding EUPS package.",NULL
RFC-126,"Add ""usesMatches"" method to star selectors","We have a standard API for star selectors, including a {{selectStars}} method that optionally takes {{matches}}, a list of source/reference object matches. Some star selectors require this argument, whereas others ignore it (and we may eventually have selectors that use it if provided), but there is no way to tell if matches are used by querying the star selector.    In some cases the user may have to make an extra call to obtain matches, but there is no easy way to tell if this call would be useful. for instance a new {{DetectAndMeasure}} task runs a star selector if computing aperture corrections and it will do its own matching if required.    I propose adding a new method {{usesMatches()}} to all star selectors. It returns True if {{selectStars}} can use the matches argument and False otherwise.    Variants I considered, but am mildly against:    - Adding a second {{requiresMatches()}} method. I don't think it is needed, though it would allow slightly earlier error reporting (e.g. if matches are required but no matcher is provided).  - Making {{usesMatches}} a property instead of a method. Some star selectors are C++, though admittedly they need wrapping, so a property is possible; it's just more work.",NULL
RFC-127,"Add clone method to BackgroundList","I propose adding a {{clone()}} method to {{lsst.afw.math.BackgroundList}} that returns a shallow copy. This means that if a user appends additional backgrounds to one copy (a very common operation), the other will not see that, but if a user modifies an existing background in one copy (a rare operation), that change will show up in all copies.    The use case is that I want to be able to copy a background model without caring about the implementation detail that it is a list.    This is a trivial addition. It would take more work to make clone return a deep copy (I don't see the necessary copy constructor for {{Background}}), but that may be worth doing.",NULL
RFC-128,"Add scipy as a stack dependency","We have been slow to adopt scipy as a dependency in the stack for various reasons.  I think one of the major concerns has always been that it's hard to install.  That has gotten much easier with various binary versions.  I think it's more work than we want to go through to build scipy from source.    My proposal:  Install it for users using the distributed Anaconda and ask other users to install it in their local installation.  In addition, this will involve porting the current policy, https://dev.lsstcorp.org/trac/wiki/TCT/DmScipy, to current documentation and updating it.    An aside: not to derail the conversation by referring to RFC-50, but why don't we just make a requirements.txt available?",NULL
RFC-129,"Base ISR Effects","Edit: I'm trying to make it more clear what I consider in scope for the ""base ISR.""    A milestone for this cycle is to deliver ISR to correct for all expected effects excluding those that involve pixel boundary distortions: brighter fatter, tree rings, edge rolloff, etc.  The rationale is that we are still learning about the more subtle effects, but we need a basic ISR that will allow processing all data up to current levels.  Below I outline three categories of effects.   1. Effects in scope for ""Base ISR""  -- I have put the things in bold that I think we need to spend some effort on.   2. Effects that part of ISR, but out of scope for ""Base ISR""   3. Effects that are not part of ISR    *Effects in scope for ""Base ISR""*   * *Non-linearity correction*   * *Intra-chip cross-talk correction*   * Basic defect and saturation interpolation   * Dark current correction   * Full frame bias correction   * Overscan correction   * Flatfielding   * *Illumination correction*   * Amp assembly   * *Fringe correction*    *Effects in ISR, but out of scope for ""Base ISR""*   * Brighter/Fatter   * Treerings   * Edge and midline rolloff   * Pixel response non-uniformity   * Hysteresis effects from bright stars   * Charge transfer efficiency corrections   * Inter-chip crosstalk   * Advance defect and saturation interpolation (Gaussian processes?)    *Aspects that are out of scope for ISR*   * Cosmic ray mitigation   * Persistence -- My hope is this is handled by a higher level logging/orchestration framework   * QA -- We will need to instrument our tasks, but I think that's a different effort",NULL
RFC-13,"Add dependency on flask, needed by Data Access Services","We want to support RESTful access to LSST Data Access Services such as image cutout, metadata and databases. (A v0 of the API we are planning to support is captured on [Data Access APIs|https://confluence.lsstcorp.org/display/DM/API] page.)  Python based [flask|http://flask.pocoo.org/] looks like a good match for what we need. It is very lightweight, simple to use, and as such ideal for quick prototyping. We put together a quick proof of concept prototype, see [here| https://dev.lsstcorp.org/cgit/contrib/webserv.git/tree/server.py], and packaged it , see [DM-1797|https://jira.lsstcorp.org/browse/DM-1797].  So, I am requesting comments whether anyone has objections to add flask to the list of external dependencies. Note that this will only affect a few selected modules: imageserv, metaserv, qserv, plus a dbserv module we are planning to create for non-qserv databases.   I already discussed it with the DB and IPAC teams, as well as with K-T, so I am hoping this won't be very controversial. If you have objections or  any suggestions or comments, please reply by Monday.  Thanks, Jacek",NULL
RFC-130,"Cost updates to Site Specific Infrastructure Estimation (LDM-144) and Explanation (LDM-143)","This RFC is to solicit feedback and approval for changes to LDM-143 and LDM-144 such that these latest versions can become preferred versions. This is part of a 6-month, reoccurring update to these documents.    [http://ls.st/ldm-144*] (latest version, currently docushare version 24)",NULL
RFC-131,"Make afw::Coord pure virtual and add SphPoint","There are two issues in afw::coord that I find confusing:    1) A plain {{Coord}} can be instantiated and converted to any other coordinate system, with the assumption being made that the plain {{Coord}} has the same coordinate system as the target system (hence the conversion will, at most, involve a precession).    2) {{Coord}} is being used as a generic spherical point in some places. For example {{Coord.offset}} and {{Coord.rotate}} both take a coord of any type, but the coordinate system and date are ignored. The implementation of afw::coord also contains a lot of usage of {{Coord}} as a spherical point, and the code would be cleaner we had a dedicated spherical point class.    My proposed solution is to make {{Coord}} pure virtual (so it cannot be instantiated) and to add a new class {{afw::geom::SphPoint}} that represents a spherical point.    I further propose that {{Coord}} contain a {{SphPoint}} rather than inherit from it. This allows {{Coord.offset}} and {{Coord.rotate}} to take a {{SphPoint}}, without the confusion of allowing the user to provide a {{Coord}}.    One question is whether {{SphPoint}} should be a unit vector or be a spatial vector with encoded distance (e.g. in au, with an upper limit on distance for objects at infinity). Supporting distance permits simple handling of parallax, proper motion and radial velocity. On the other hand, unit vectors are simple and suffice for our needs, and we should probably be using 3rd party packages for such fancy coordinate conversions. Thus I lean towards unit vectors, though I am happy to support distance. If we do support distance then may wish to make {{SphPoint}} a subclass of {{Point<double, 3>}}, to inherit useful methods.  ",NULL
RFC-132,"Cross Data Release Database Joins","This question is for science experts... I wanted to bring to your attention the fact that architecting database to support joins between different data releases *through a single query* will be (mildly speaking) non-trivial, so we would like to understand the demand really well before even considering implementing it. There are several challenges here:   * we'd like to have flexibility to change partitioning scheme between data releases. It is very unlikely that partitioning that works for DR1 will still be optimal for DR 3 or 7. And in order to join data between data releases, partitioning must be the same. To alleviate that, we could repartition older data releases when doing a new release. Note that we would also have to repartition all L3 databases derived from these releases, and that gets messy very quickly.  * Further, in the current design, Qserv requires corresponding partitions to be placed on the same node. Enforcing that across DRs would force us to go through a major redistribution of older data releases, which is doable, but again, complicates our data distribution tools, data loading tools, and operations in general, not to mention lots of extra network and disk IO.    So, we are currently going with the assumption that it'll be sufficient if we support two-step join, where user runs a query on DRx, produces a list of object ids, and then joins that with DRy.    Comments? Reactions?  ",NULL
RFC-133,"No full table scans on Level 1 catalogs","Following today's discussion at DataAccess meeting and email exchange with [~gpdf], I wanted to bring to your attention the fact that we are NOT planning to support heavy full table scans on Level 1 database. This has been documented for a long time and has been in the baseline for many years, see LDM-135 (I pasted relevant part below). If that is going to hamper science analysis, let's revisit this. It impacts our L1 database design, so if we have to support full table scans on L1, it'd be good to know it soon.    From LDM-135:  ""Based on the science requirements, only short-running, relatively simple user queries will be needed on the Level 1 catalogs. The most complex queries, such as large-area near neighbor queries, will not be needed. Instead, user queries will consist mainly of small-area cone searches, light curve lookups, and historical versions of the same. Since the catalogs are sorted spatially, we expect to be able to quickly answer spatial queries using indexed HTM ID columns and the scisql UDFs, an approach that has worked well in data-challenges to date. Furthermore, note that the positions of diaSources/diaForcedSources associated with the same diaObject will be very close together, so that sorting to obtain good spatial locality also ends up placing sources belonging to the same light curve close together. In other words, the data organization used to provide fast pipeline query response is also advantageous for user queries.”",NULL
RFC-134,"Table scans for older releases","The current baseline is modeled support the following shared scans:   * Object *for the latest DR only*   * Object_extra *for the latest and latest-1 DRs*   * Source *for the latest DR only*    * ForcedSource *for the latest DR only*    Note that with the exception of Object_extra, we are planning to support efficient full-table-type access to the latest data release only. Short non-scan queries will be supported for any data release available on disk (per baseline, that'd be latest and latest -1).    I'm soliciting comments and reactions. Changing the model and providing access to older data release is not a technical problem, it is only a question of $ (how much hardware to purchase).",NULL
RFC-135,"Discussion of Level 1 re-processing in DRP","I just read section 4.3.5 of the DPDD on Level 1 reprocessing during DRPs, and that left me with a lot of question and concerns.  The discussion there seems to assume processing essentially identical to what's run nightly, but I'd like the Level 1 processing at this stage to be more integrated with the rest of the data release processing, and to take advantage of improved calibrations and intermediate data products that might be unavailable in nightly processing.  I'm also very concerned about the requirement that Level 1 DRP processing include all observations up to when the data release is completed, instead of when it's started.  And finally, I'm worried that the planning and implementation of this part of the pipeline could fall between the cracks, as it seems to be partially owned by both the AP and DRP teams with a rather fuzzy boundary.    I think it's a requirement that we get at least [~mjuric], [~rhl]l, [~krughoff], and [~swinbank] together to have a productive discussion, and given that [~mjuric] does not have the usual RFD slot free, I propose we meet an hour earlier: 11:30am Pacific (just after the stand-up), on Tuesday, February 16.    Please reply below if you're interested, and if so, whether you're able or unable to attend.",NULL
RFC-136,"Remove ProcessCoaddTask","`ProcessCoaddTask` is an outdated technique for measuring sources on coadds. The preferred technique is multi-band processing. The continued existence of `ProcessCoaddTask` is confusing.    Furthermore, if we keep `ProcessCoaddTask` then it have to be rewritten as part of DM-4692. Rewriting it is not terribly difficult, but it will also need to be tested and config override files updated. So I would prefer to delete the task now, if possible.    See also the discussion on [What is ProcessCoaddTask supposed to do?|https://community.lsst.org/t/what-is-processcoaddtask-supposed-to-do/511/4] on community.    I made the discussion period short in the expectation that this would not be controversial, and to avoid hanging up DM-4692. I am happy to extend the end date if wanted.",NULL
RFC-137,"Disable/remove background matching from default coadd assembly","The background matching introduced into pipe_tasks a couple of years ago works only on drift-scan observations and is not applicable to pointed observations (e.g., DECam, HSC, LSST).  Making background matching work with pointed observations will take a lot more care and coordinated processing across patches due to the smaller scales involved (arcminutes for CCDs vs many tens of degrees for SDSS runs).  However, background matching is currently enabled by default in the stack, which necessitates overrides (which are often neglected, at least initially, by the new user) for pointed observations, which is the vast majority of current use cases.    Given that the current background matching code is only applicable to a single camera supported in our stack, I propose moving it into obs_sdss.  [~yusra], the original author of the code, has indicated that she ""wouldn't object"" to this proposal.  However, if this is deemed undesirable, then I propose disabling the background matching by default.    A related option to the background matching is the background restoration in {{makeCoaddTempExp.py}} ({{MakeCoaddTempExpConfig.bgSubtracted}}), which is currently enabled by default (required for background matching, not desired if not background matching).  I propose to leave this functionality in its present place (because it will be desired when we have background matching working on pointed observations), but disable it by default.",NULL
RFC-138,"Moving DM 6-month Cycle","Our original MREFC plans placed the end dates of the 6-month cycles at end of February (Winter cycle) and end of August (Summer cycle).  This was done intentionally to coincide with the LSST all hands meetings (Joint Technical Meetings in February and Project Community Workshops in August).  The intent was to have an opportunity to describe what is being delivered in that cycle.    However, since we have started construction, and now through S14, W15, S15, and (nearly) W16, we have observed some issues with this:    1.  Our productivity is lower in the first half of each cycle and improves in the second half of the cycle.  As things now stand, preparing for the upcoming all hands meeting is in conflict with the highest productivity period of each cycle.  2. Particularly for SQuaRE, the end of cycle is a very busy time, and many activities are going on in parallel in this time frame in order to build and test the release, take QA measurements, document the release, etc.    So, a suggestion has been made to shift the cycles to end in between the all hands meetings, nominally ending in May and November.  This coincides more with our DMLT meetings, and those could be leveraged to do end of release coordination and next release planning.  Alternatively, the cycle end dates could be scheduled a month before or a month after the DMLT meetings.    In order to move off the current schedule, we could either do a very short cycle of 3 months, or an extended cycle of 9 months.     So, the discussion questions are:    Should we move the cycle end dates at all?  If so, to what months?  If we move, do we do a short cycle or an extended cycle to change from the current to the new schedule?    ",NULL
RFC-139,"2016 Renewal of Information Security Program","The LSST cybersecurity program consists of four top­ level policies, a subsystem ­specific plan for each technical subsystem, and cybersecurity risk spreadsheets for each subsystem.  The program is to be reviewed annually and amended appropriately (see Baseline Project Information Security Program).  For the renewal, each subsystem manager must review their respective subsystem plan and risk assessment spreadsheet using Docushare.    We are in the process of doing the 2016 update to the Data Management Information Security Plan (LDM-324) and Risk Sheet (LDM-325).  A confluence page has been set up with links to the appropriate documents:    https://confluence.lsstcorp.org/display/SPR/Security+Plan+Renewal    Please review the LDM documents and suggest edits/updates as necessary.  If you wish, edit a copy of the word and excel files with revision tracking on and send to [~jkantor]",NULL
RFC-14,"Be specific about Python versions supported","Over time, the Python interpreter has evolved to support features which make code simpler and more readable. We should use these features where we can.  Unfortunately, using new features means the code will not run on older versions of Python. We should therefore be specific about which versions the stack supports, so that developers know which features are available to them, and end users know which version of Python they require.  Currently, our [coding standards|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard] specify that ""Python 2.5 SHOULD be used"". However, in practice our installation & build procedure checks for at least Python 2.7, and, [according to the TCT via Confluence|https://confluence.lsstcorp.org/display/DM/DM+Third+Party+Software], we distribute Python 2.7.6 as part of the stack (but note that we do not force users to use this version of Python, so that does not guarantee that it will be available when the stack is run).  I propose modifying the existing text in the coding standard so that it reads:  {quote} Idiomatic modern Python SHOULD be used  The Python language has evolved with time. Learn the new features of Python and use them where applicable to make your code simpler and more readable. For example:  * Use iterators, generators (classes that act like iterators) and generator expressions (expressions that act like iterators) to iterate over large data sets efficiently. (New in Python 2.2, except generator expressions were added in 2.4 and generators were slightly enhanced in Python 2.5.)  * Use the with statement to simplify resource allocation. (New in Python 2.5.) For example to be sure a file will be closed when you are done with it:   {code} with open('/etc/passwd', 'r') as f:     for line in f:         ... {code}  The LSST stack mandates that a certain minimum version of Python will be available to run your code. This version will be periodically updated; refer to [the external/python repository|https://dev.lsstcorp.org/cgit/LSST/external/python.git/tree/scripts/check_python] for the current requirement. Do not use features that are not available in the required version. {quote}",NULL
RFC-140,"Replacing Hipchat with Slack as chat solution","This RFC covers the proposal of adopting Slack for our chat platform (in lieu of hipchat).    The current user arguments in favour are:    - User preference (as expressed frequently to me) due to familiarity    with other projects using Slack    - More finely tuned notifications, client read message synchronisation    (mobile/desktop) and similar superior usability features    - Perceived poor performance of Hipchat (in terms of service    availability, connection management, buggy clients)    The community support arguments are:    - Interface with standard protocols (IRC, XMPP) and fee structure make    it more approachable to eg. students needing urgent help rather than    the more ""gatekeeper"" model required from hipchat    - The adoption trend seems to favour Slack, so likely to be more    popular as we onboard more external collaborators in the coming yers    The SQuaRE arguments in favour are:    - More actively supported integrations platform    - An opportunity to normalise chat usernames to match Github    usernames, which would simplify implementation of a much requested    feature to @ users for their build failure notifications    - No need to cull inactive usernames    The arguments against are:    - Disruption of change    - No 1:1 video facility    RFC Process:    - There is a separate question of whether to join Slack with the free    (but limited history search) plan, or the per-user plan with    unlimited history plan (which would still need some    gatekeeping). The argument is that the successful introduction of    Discoursed-base c.l.o should lead us to treating chat as a more    transient platform and flush substantive conversations to    Discourse.    - In the event this RFC passes, implementation needs to lag until    SQuaRE can move the current integrations (eg. Jenkins) over.     - Despite a SQuaRE preference for Slack for the reasons stated, we    feel like a minor stakeholder in this questions compared to user    preference. Also, this RFC affects more users than typical. For this    reasons, I encourage you to fill this poll unless you really want to    add something to the arguments.     https://www.surveymonkey.com/r/R5TKNBB    Poll Results as of 2016-02-12 (see [^Data_Q1_160212.pdf] attachment):    ""I actively prefer Slack"",""36.4 %""  ""I actively prefer Hipchat"",""4.5 %""  ""I don't mind, let the people who care choose"",""50.0 %""  ""I don't have a preference but don't want to have to change."",""9.1 %""    Answers: 22      ",NULL
RFC-141,"Astropy continued discussion","Continue the discussion on Astropy.    Proposed time Tuesday 16th February 1pm Pacific.",NULL
RFC-142,"Disable OpenBLAS threading unless explicitly enabled","DM-4714 outlines problems encountered while running a {{CmdLineTask}} under python multiprocessing (i.e., with the {{-j}} command-line flag): OpenBLAS assumes by default that it should use as many threads as the machine has cores, and so the parallelism is compounded as each python process uses all the cores, and the result is a surprising net _increase_ in execution time.  OpenBLAS have indicated that they consider their assumption reasonable for most of their users, but it is apparent that it is not reasonable for us because we have our own parallelism scheme, and we therefore need to control the OpenBLAS parallelism.  It is possible to do this by setting environment variables, however we cannot rely on our users to have set these, but nor do we want to set them programmatically because the user may want to do something different.    This RFC proposes that when we are running multiprocessing (and only then), we should intervene on behalf of the user and disable threading in our dependencies unless the user has explicitly requested it (principle of least surprise).  A rough example of this is shown on the discussion of DM-4714, but the principal elements of this strategy are:  1. The intervention only fires if it appears that the user is about to get into trouble (e.g., using OpenBLAS, the number of threads is not specified via environment variables, and we're using multiprocessing);  2. There's a way of disabling the intervention; and  3. The user is warned about the intervention and advised how it can be disabled.    We do not yet have a demonstration of strategy for libraries other than OpenBLAS (e.g., Accelerate Framework on Macs), but propose handling those in the same way.    This RFC solicits arguments against this proposed strategy.  Comments about the exact means of executing this strategy should be made on DM-4714.",NULL
RFC-143,"refactor aperture corrections and measurement afterburners","Currently, applying aperture corrections is the responsibility of the measurement tasks ({{SingleFrameMeasurementTask}} and {{ForcedMeasurementTask}}).  Unfortunately, they're unable to do this with a single call to {{task.run()}}, because the right order of operations is:   - measure source properties from pixels   - (optional) measure aperture corrections from source properties   - apply aperture corrections to source properties   - derived new source properties (e.g. classification) from corrected source properties    The measurement tasks currently treat algorithms that derive new source properties from corrected measurements the same as those that measure quantities from pixels, so the actual measurement has to be split up when aperture corrections are applied.  The result is that doing all of the above involves a torturted mess of multiple calls to the exact same {{run()}} method, with different arguments:   - if measuring aperture corrections: https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/calibrate.py#L548   - if not: https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/processImage.py#L180    This complexity - and the fact that just calling {{run()}} with the required arguments silently does something incorrect almost all the time - has led to a lot of confusion and incorrect behavior as reported on DM-4836 and DM-5109, and some extra work on DM-4692.    Instead, I think this sequence should look something like this:  {code:hide-linenum}  self.measurement.run(catalog, exposure)  apCorrMap = self.measureApCorr(exposure.getBBox(), catalog)  self.applyApCorr(apCorrMap, catalog)  self.afterburners.run(catalog)  {code}    To make that happen, we'd make the following changes:   - Move ApplyApCorrTask from being a child of measurement tasks to a sibling (a subtask of the same parent as the measurement task).   - Define a new plugin system and subtask for running afterburners (e.g. classifiers).  At least at present, these don't actually need the pixel data, so we could use the same task for both single-frame and forced plugins (if we even want to run these in forced mode - we traditionally have, but there really isn't a good reason to).    I'd slightly prefer to make these changes before DM-4692 and before I do further work on the HSC merge (I'm curious about [~rowen]'s, [~swinbank]'s, and [~lauren]'s opinions on this), as I think these changes will simplify that work and I don't think it will take me more than a day of coding (I'm proposing that I make these changes both on master and a branch of DM-4692 that could be reintegrated with it before merging).  Note that deferring this change will require workarounds to be added to coadd processing and forced photometry in the meantime, because we're currently just calling {{run()}} once there, and hence doing things incorrectly.  ",NULL
RFC-144,"Add OpenOrb as a third party package for SIMS","We want to add code to generate moving object ephemerides, needed to support the sims catalogs.   We have been using OpenOrb to do this, Mario has provided some eups packaging for it, and we'd like to add it as a third party package.   It does require gfortran to build.  We'd like to this to be available on jenkins as well as to users.   Here's the repo: https://github.com/EUPSForge/oorb",NULL
RFC-145,"Always set Team field in jira","It'd be very useful to have the ""team"" field set for every epic/story that we maintain in JIRA. Doing so has several benefits:   * it'd be easier for T/CAMs to see only activities they manage   * it'd be easier for Project Mgr to run statistics for the entire DM project   * it'd simplify filters we use for per-team agile boards    As of now, 75% of all our stories have the team field set (3568 out of 4801).     Note that (as discussed below), it might be difficult for a story creator to pick the right team, so the proposal is to allow new stories without Team field set, but TCAMs should quickly deal with such cases and add missing story->team assignment.",NULL
RFC-146,"Add ExposureIdInfo class to daf_butlerUtils","Presently every task that adds sources to a catalog uses a butler to determine several related pieces of information: the exposure ID and the number of bits used for an exposure ID. Using a butler for this is clearly overkill (in many cases the task has no other need for a butler at all) and passing this information as two separate items seems overkill when they are deeply related for the purposes of creating source IDs.    On DM-4692 I have added a class ExposureIdInfo to daf_butlerUtils that in my opinion neatly solves this problem. The object need only be constructed once by a high level task (one that already has a butler) and then passed along as required. The information is immutable and provides just what is required to construct data IDs.    Eventually we may want to expand this object to be able to compute data IDs or convert data IDs to visit and CCD. However, I feel that is out of scope for this RFC. At the moment all I want is permission to add this simple class.    For usage see tickets/DM-4692 of pipe_tasks. It is constructed in CalibrateTask and CharacterizeImageTask and passed down into DetectAndMeasureTask (which has no butler).    I provide two ways to construct the object: one from a data reference (which is how tasks use it) and the other directly (which is how examples and tests that don't have a butler use it). Both are useful and I would be loath to give either up.",NULL
RFC-147,"Best practices to report an issue with DM system","Recent discussions in hip chat and in JIRA exposed it'd be good to have a well defined policy describing best practices related to reporting issues by users. Below I typed proposed narrative, comments / improvements are welcome.    This RFC should be implemented by incorporating this text into one or both of pipelines.lsst.io and developer.lsst.io.    Recommendations for filing a new issue:  * Project: DM  * Issue Type: use the ""bug"" or ""improvement"" issue type. The former should be used to report flaws / failures, the latter should be used to request an improvement or a new feature  * Summary: short description of the issue  * Priority: make a guess, or leave default. Use blocker or critical for truly blocking issues.  * Component: set it if it is obvious. Do not create new components (request needed components from a JIRA admin)  * Fix Versions: leave empty  * Assignee: set to ""Unassigned""  * Watchers: leave empty, or add developers that you'd like to be alerted about your issue  * Description: type the description, paste relevant error messages, snippets from log files and anything else that you think may help us understand the issue. This might include things like operating system (and browser, if web-based); eups setups (eups list -s); input data; the command that's producing the issue; the actual output from that command; the expected output from the command etc.  * Story Points: do not set it, leave it for the domain expert to make the assessment  * Linked Issues / Issue: leave blank  * Epic Link: leave blank  * Sprint: leave blank  * Team: feel free to set it if you know which team it falls into",NULL
RFC-148,"Cleanup ""versions"" in JIRA DM project","We started using versions in JIRA DM at some point in the past, but that never really took off, see https://jira.lsstcorp.org/projects/DM?selectedItem=com.atlassian.jira.jira-projects-plugin:release-page.    I propose we scratch it: delete currently defined versions, and hide the version related fields from the ""create issue"" page.    Comments? :)",NULL
RFC-149,"Use dueDate field in JIRA sparingly","I am not sure if we ever formalized how to use due date in JIRA, so perhaps let's try?    We have 543 with due date set, and 66 out of them are currently past due date (here is the filter: https://jira.lsstcorp.org/issues/?filter=14023).    I think there IS a value in having due date in some selected cases, where the story creator, or perhaps a TCAM, or System Architect etc needs the story completed by a certain date. So my proposal is:    1. Use due date sparingly when you really mean it, and ideally, add a comment why you are setting it  2. Let's monitor due dates, and react when a story is not completed and it is past due date.    When the exact due date is not essential, we can simply rely on cycle field to drive the time story should be done.     Of course we should also revisit the stories currently past due.    Anyway, I'd like to see reactions and comments.",NULL
RFC-15,"Allow compiler-supported C++11 in headers if not SWIGged","Some C\+\+11 features (like smart pointers) are most useful if they appear in interfaces.  We have disallowed any C\+\+11 features in header files for fear of confusing SWIG.  But this may be too strict for C\+\+-only interfaces.  Proposal: Change the coding standards to permit C\+\+11 features (that are supported by our official compilers) in headers as long as SWIG does not see them (either because they are not in method signatures or because they have been explicitly excluded with {{#ifndef SWIG}}).",NULL
RFC-150,"Set JIRA component lead to the component best expert","I propose we ensure every component in JIRA has a component lead, and we set it to the best expert of that component.    We considered assigning components to TCAMs in the past (see [RFC-51|https://jira.lsstcorp.org/browse/RFC-51]), primarily because TCAMs could then catch issues with story points / PMCS etc, but I'd argue that TCAMs should be actively monitoring all activities that are happening in their area (RSS feed etc), so assigning components to TCAMs does not bring any extra value here.    The component list and their leads are here: https://jira.lsstcorp.org/plugins/servlet/project-config/DM/components    Based on discussion in the comments, this proposal now includes keeping the list OUTSIDE of JIRA.  ----    Thanks to [~gpdf] for reminding me about BaBar package coordinators and making me open this RFC. Gregory said:    {quote}BaBar ""package coordinators"" were meant to be, in general, the most knowledgable developer associated with a package - someone who could answer questions and make technical decisions about a package.  I thought that was a good concept.{quote}",NULL
RFC-151,"Turn OFF automatic story assignment in JIRA","Assuming we set Team field for every story (see [RFC-145|https://jira.lsstcorp.org/browse/RFC-145)]), I propose by default new stories are left Unassigned.    Why not assign it to the lead of a given component if the component is set? Because it would ""led to a few of our most experienced developers being deluged with automatic assignment notices"" as John pointed out in [RFC-150|https://jira.lsstcorp.org/browse/RFC-150].    Why not assign it to a TCAM? It's cleaner to limit the list of assigned stories to minimum (that is my humble opinion anyway). TCAMs can always find the stories they are ultimately responsible for through Team field.",NULL
RFC-152,"DRP high-level data flow and parallelization","I'd like to schedule some time to discuss the high-level data flow and parallelization questions we have to make for DRP, and if time permits go through my proposed answers on DM-4839.    I think we need at least [~rhl], [~zivezic], and [~ktl] to have a quorum, but there are many  others whose opinions it would be valuable.    -I'm proposing 2:30pm Eastern on Tuesday, March 15 - that's an hour earlier than the usual RFD slot, both to give us more time for what might be a very long discussion and to increase the chances that [~mjuric] could attend.-    Please RSVP here if you're interested in participating and either available or unavailable at the proposed time.",NULL
RFC-153,"Add SuiteSparse external package (has LAPACK/BLAS dependency)","jointcal currently depends on a subset of SuiteSparse vi via the micro_cholmod meta-package. From the results of RFC-123, [~jbosch] suggested that we [take the whole SuiteSparse package as-is|https://community.lsst.org/t/simultaneous-astrometry-rfd-notes/503/9], since we may want to use it and/or LAPACK/BLAS in the future.    SuiteSparse depends on LAPACK/BLAS, so this may be opening up a can of worms. I was able to build SuiteSparse 4.5.1 on OSX without trouble, but it failed on a random Linux machine that has a fairly minimal set of libraries.    Whether we can legally pull the specific parts that micro_cholmod used into jointcal is unclear. The README.txt says:     {quote}Some Modules of CHOLMOD are copyrighted by the University of Florida (the  Core and Partition Modules).  The rest are copyrighted by the authors:  Timothy A. Davis (all of them), and William W. Hager (the Modify Module).  {quote}    which suggests that we can't just extract it and distribut it ourselves. In fact, its not clear that we would be allowed to distribute SuiteSparse as part of the stack anyway: the base-level README.txt says:    {quote}  Refer to each package for license, copyright, and author information.  All  codes are authored or co-authored by Timothy A. Davis.  {quote}",NULL
RFC-154,"Change star selectors to return stars instead of PSF candidates","Our current star selectors return a list of {{PsfCandidate}} s instead of source records. This makes them clumsy for many operations.    I propose the following changes:  - Add a base class for star selectors. Have this base class provide a method that turns a source catalog into a list of {{PsfCandidate}}. The default implementation will probably suffice in all cases, but if not, this offers natural way to override it.  - Change the {{selectStars}} method to return an {{lsst.pipe.base.Struct}} that contains a catalog of sources that are believed to be stars.  - Make star selectors into tasks. They are already configurable objects. This has several advantages, including adding a log object and eliminating the need for a registry. I suggest that the primary method retain the name {{selectStars}}, but if people prefer to rename it to {{run}} I don't mind.  - Eliminate the star selector registry, unless it is still wanted.    Note that there are alternatives to returning a source catalog that might do just as well, though I think a catalog is more natural:  - Have the task flag stars. This is somewhat clumsy because it requires that the star selector add a field (whose name will presumably be a config parameter) and users must then scan the catalog to find stars. It does have the advantage of saving space, and users can easily find stars or non-stars equally easily. Which brings up a point discussed below.  - Return a list of source IDs. This is somewhat clumsy for users but saves space.    Will users ever want an easy want to identify non stars? Neither the current interface nor my proposed changes support. Obvious options include:  - Set a flag in the catalog instead of returning a catalog, as mentioned above.  - Return two two catalogs: stars and non-stars.  - Have a flag that controls whether the output is stars or non-stars.    Note that {{lsst.ip.diffim.DiaCatalogSourceSelector}} already returns a list of sources (an ordinary python list) rather than a list of {{PsfCandidate}}. It should be made the same as the others, and probably renamed from {{...SourceSelector}} to {{...StarSelector}}.",NULL
RFC-155,"Change estimateBackground to not deep copy the exposure and make it a task","{{estimateBackground}} deep copies the exposure if {{subtract=True}} and returns the copy. This seems a waste of memory and time with no real benefit for the user. In my opinion it should simply modify the exposure in place. In addition, it returns the exposure or None if background not subtracted; this is clumsy and seems unnecessary if it modifies the exposure in place. Also it has a ConfigClass attribute so I suggest that it be made a task.    My full proposal is:  - Modify the exposure in place if subtracting the background  - Accept the current background model, make a shallow copy and return the new background. That way if background is not to be subtracted the input background model is still valid for the exposure.  - Make {{estimateBackground}} a task  ",NULL
RFC-156,"Providing a shared stack on LSST development machines","An important part of the workflow for (some) Science Pipelines (and perhaps other?) developers is the ""shared stack"" available on the {{lsst-dev}} system. For details on use, see [the Developer Guide|http://developer.lsst.io/en/latest/services/lsst-dev.html#load-the-lsst-environment] and [this post on clo|https://community.lsst.org/t/using-the-stack-on-lsst-dev/221].    As it stands now, the stack is generated by the nightly [Buildbot|http://developer.lsst.io/en/latest/build-ci/ci_overview.html#buildbot] run and tagged to match ({{bXXXX}}). Over time, it has accumulated a large number of tags, which make it extremely slow to use. (This is likely an artefact of EUPS implementation, but fixing it is outside the scope of this RFC.) Further, since the recent [switch to Miniconda|https://github.com/lsst/lsstsw/commit/5a586c6429e72a50db5cd368fecc986b114d2acf], it doesn't provide a full Anaconda installation and hence is lacking tools that developers expect (notably IPython). This second problem is not insurmountable, but it adds overhead and confusion, particularly to new users (see repeated discussions in the HipChat Data Management room over the last several weeks). Finally, because of the way tags are applied to the Buildbot products, although the regular {{bXXXX}} tags are applied to this stack, the (arguably) more significant tags identifying weekly releases ({{w_YYYY_WW}}) are not.    The clo post linked above suggests a couple of alternative ways an end user might set up a stack which avoid some of the slowdowns inherent in the above. However, the change to Miniconda broke those options (DM-5021): it's possible to workaround by editing some scripts, but again, this is not the experience we want to be providing for developers.    Following [discussions at the JTM|https://confluence.lsstcorp.org/display/DM/DM+JTM+2016-02+Meeting+Notes#DMJTM2016-02MeetingNotes-LimiteddevelopersupportinSpring16andlsst-devsharedstack], it was agreed that SQuaRE will not be supporting the shared stack on {{lsst-dev}}, at least through the short X16 cycle. This will be taken on by Science Pipelines with a shoestring time allocation.    The aim of this RFC is to establish what the Science Pipelines team can do to make developers' time as productive and stress-free as possible in the near term. You are invited to supply your suggestions and requirements.    Here's my zeroth-order proposal to give you something to shoot at. Assuming:    * We can break the degeneracy between the Buildbot stack and that provided for use by end users, on the basis that changes as required by Buildbot or the release process should not impact on the stack being used for development.  * Except on very rare occasions, developers rarely have to resort to arbitrary Buildbot tags to diagnose problems. Therefore, providing all of these outside the {{lsstsw}} stack is unnecessary: just making weeklies and other special tags available should be adequate.    I suggest using {{eups distrib}} to install a new version of the stack which is rooted outside the {{~lsstsw}} tree and telling developers to use this. This stack will include enough Anaconda (or Miniconda) packages to ensure end users can run IPython or any other services they need. A simple script run from cron, or equivalent, will regularly check the {{sw.lsstcorp.org}} server and install any new weeklies into this shared stack. Experts can fall back to the {{~lsstsw}} stack when required for debugging purposes; others can forget it exists.    Considerations:    * Which systems other than {{lsst-dev}} is the shared stack accessed from? Is there a downside to having it {{eups distrib install}} ed on each of those systems independently, or do they need to share some common networked filesystem?  * Is there are requirement for end users to not simply be able to use the stack but be able to write to it (to add new packages, tags, etc)?  * Is a shared stack of this form actually necessary and useful to developers at a level that makes it worth dedicating resources?  * ... others?",NULL
RFC-157,"Remove doRenorm option from AssembleCcdTask","AssembleCcdTask's config defaults to doRenorm=True. The option is described as ""renormalize to a gain of 1? (ignored if setGain false)"". HSC has chosen to change the default to False with the comment ""...only the scaling of the flats should determine the relative normalisations of the CCDs"". I propose we get rid of this option and never renormalize.",NULL
RFC-158,"Add cross OSX/GNU portable floating point exception management to lsst utils","While dealing with DM-5275, I found a portable implementation of the GNU floating-point exception management functions (e.g. feenableexcept):    http://www-personal.umich.edu/~williams/archive/computation/fe-handling-example.c    It's public domain, and seems like it would be useful for others elsewhere in the stack. I  propose adding it to utils/include/ with the main() commented out and name it ""fenv-portable.h"" since the GNU file is fenv.h. There may be a better place to put it, so suggestions are welcome.    Here's a description of what feenableexcept can do:    http://jayconrod.com/posts/33/trapping-floating-point-exceptions-in-linux",NULL
RFC-159,"Astropy integration pre-Workshop meeting","I would like to use next week's RFD slot to finalize our plans and requirements for the upcoming Astropy/LSST workshop. In particular if we would like the Astropy community to make any specific preparations for us.  ",NULL
RFC-16,"Repository renaming","[Yes I should have done this earlier]  This is the state of my notes on the repository renaming as part of the github move. As a reminder, the idea is to group repos under a prefix that are both representative in architectural terms, but also allow a cleaner namespace and facilitating navigation by grouping together related repos.  I am happy to take comments either at Jim's Monday repo re-org session (if he feels it relevant), or my github migration session (Tue). I am sure there are mistakes in it so don't be shy.   Current draft attached, ""live"" version with any changes at:  https://gist.github.com/frossie/680b27d50d19b6cf3ba5/    ",NULL
RFC-160,"Revise naming scheme for coadd images","We need to have two dataset names for coadd images: one for the raw coadd produced by assembleCoadd.py, and one for the background-subtracted coadd with detections identified produced by detectCoaddSources.py.  The first is {{deepCoadd}} in both LSST and HSC, and is not in contention.  The second is {{deepCoadd_calexp_det}} in LSST and {{deepCoadd_calexp}} in HSC.  I think the LSST name originates from before HSC transitioned to a naming scheme that I think makes sense, and hence I think it's a bit confused, and unnecessarily long and clumsy.  I would like to make the LSST naming scheme consistent with that of HSC because I think it's simpler, clearer, and more people have more experience with it.  {{deepCoadd_calexp}} currently appears to be unused on the LSST side.    Now seems to be a good time to do this, before the multiband products become more widely used on the LSST side.  The HSC port ticket DM-5288 will do this work, including updating dataset names in the obs_* packages.",NULL
RFC-161,"Make the background model part of the exposure","At present the model of the background that has been subtracted from an exposure is a separate entity from that exposure. It can be difficult to keep the two objects synchronized as backgrounds are estimated and subtracted, and persistence is a nuisance because it requires two separately managed dataset types.    I propose that the model of the background that has been subtracted be kept with the Exposure and persisted with the exposure.    This RFC is primarily intended to get permission to make this change. I am not sure we can implement this proposal yet. Everything else in an Exposure is a C++ object, and background models are Python, but my understanding is that we will be able to solve this eventually.",NULL
RFC-162,"LSST python coding style delta to PEP8","The LSST project will adopt PEP8 as the python coding standard, with the following modifications. Our [current python coding standard|http://developer.lsst.io/en/latest/coding/python_style_guide.html] will be reduced to a link to PEP8 https://www.python.org/dev/peps/pep-0008/  , a list of the items where we differ from PEP8, and the remaining items in our current standard that PEP8 is silent on (some of which-e.g. use of super()-will be revisited in a future RFC).    pep8+flake8+pep8-naming modifications and ignore list:    {quote}  max-line-length = 110  E133: closing bracket is missing indentation  E226: missing whitespace around arithmetic operator  E228: missing whitespace around bitwise or shift operator  E251: unexpected spaces around keyword / parameter equals  N802: function name should be lowercase  N803: argument name should be lowercase  W391: blank line at end of file  {quote}    Example flake8 configuration file:  {code}  [flake8]  max-line-length = 110  ignore = E133, E226, E228, E251, N802, N803, W391  {code}    Example SublimeLinter-flakes configuration for SublimeText:    {code}              ""flake8"": {                  ""ignore"": ""E133,E226,E228,E251,N802,N803,W391"",                  ""jobs"": ""1"",                  ""max-complexity"": -1,                  ""max-line-length"": 110,                  ""show-code"": false              },  {code}    SQuaRE has agreed to support this transition by supplying a flake8 linter configuration (example above), documentation updates, and CI support (further in the future).    For the curious, we took [notes in this Google doc|https://docs.google.com/document/d/1Zmw1NfB_ViBVnVE6h9KbTZs2hffk4KD0CVxdzwERMdU/edit] to identify the similarities and differences between PEP8 and the LSST python coding standards.",NULL
RFC-163,"What goes in a VOEvent?","The [DPDD|http://ls.st/dpdd] states in section 4.5.1 the expected content of each alert.  The DPDD explicitly states that alerts will be packaged using the {{VOEvent}} standard.    In the context of {{VOEvent}} as the assumed standard, I would like to discuss whether the DPDD is still an accurate representation of the general thinking in the project on what should go in an alert.    Given the discussion on alert contents, I would like to examine what modifications to the current standards will be needed to support the LSST use case.    Suggested time: 12:30 Pacific on Tuesday April 5    For convenience, following are the current contents from the DPDD:  * _alertID_: An ID uniquely identifying this alert. It can also be used to execute a query against the Level 1 database as it existed when this alert was issued  * _Level 1 database ID_: For example, {{DR5L1}}  * _Science Data_:  ** The {{DIASource}} record that triggered the alert  ** The entire {{DIAObject}} (or {{SSObject}}) record  ** All previous {{DIASource}} records  * Cut-out of the difference image centered on the {{DIASource}} (10bytes/pixel, FITS MEF)  * Cut-out of the template image centered on the {{DIASource}} (10bytes/pixel, FITS MEF)",NULL
RFC-164,"Unifying non-linearity correction in the ISR task","As we start supporting non-linearity correction for more cameras I would like to propose some rules, to keep things simple and allow a default version of non-linaerity correction in {{IsrTask}}.    We need to support correction based on lookup tables and have existing code that uses polynomials. We may want to support variations of these or other techniques. Thus I propose to make non-linearity correction a subtask of IsrTask that can be retargeted. I further propose that default task support polynomial correction, because it is simple and all the necessary data exists in the {{AmpInfo}} table.    For the polynomial-based correction there are a few issues:    - We have four coefficients available. This can easily be increased if needed, but there is no sign of that, rather the opposite...  - Unused coefficients are often set to {{nan}} (e.g. in obs_subaru and obs_test)  - The settings for various cameras have some quirks:    - {{obs_subaru}}, which has working non-linearity correction code, sets {{linearityType= ""PROPORTIONAL""}} (or {{""NONE""}} if no correction is wanted) in the {{AmpInfo}} table, and the associated {{linearityCoeffs}} are as follows. This seems a very unnatural order, and I dislike forcing a value to 0 and raising an exception if it is not:      - coeff[0] sqareCoeff: corrIm = uncorrIm + sqareCoeff*uncorrIm^2      - coeff[1] threshold: this must be 0 else an exception is raised      - coeff[2] maxUncorr: if maxUncorr > 0 then flag pixels as SUSPECT where uncorrIm > maxUncorr    - lsstSim sets {{linearityType=""polynomial""}} and {{linearityCoeffs=(0, 1, 0, 0)}}, presumably the first four polynomial coefficients. This data is ignored.  - The concept of {{maxUncorr}} from {{obs_subaru}} seems useful, but it is not a natural fit to a lookup table.    I propose the following:    - Add field {{suspectLevel}} to the {{AmpInfo}} table and set the {{SUSPECT}} flag using specific code for it, rather than relying on non-linearity correction code to do it. This makes the field more obvious than tucking it away as an element of {{linearityCoeff}} and also simplifies writing variant tasks to correct non-linearity, since they can focus on doing one thing well.  - Offer constants for {{linearityType}} names and use those in code. This catches typos earlier. Suggested names include {{NONE}}, {{POLYNOMIAL}} and {{LOOKUP_TABLE}}. These can be enums in python 3.  - The standard non-linearity correction tasks must accept {{NONE}} and do nothing for such detectors. Mosaic cameras may have detectors for which we don't want this correction.  - The standard non-linearity correction tasks will only support one {{linearityType}} or {{NONE}}. More heterogeneous cameras will require specialized tasks.  - Encode the number of coefficients by adding an extra field, e.g. {{numLinearityCoeffs}}. The specified number of fields are required to be finite. Yes we could (and perhaps should) also force the camera to make all unused fields 0, but this is an extra step and is clearly easy to forget. In any case, it is useful information; for instance it can save time to know the order of a polynomial in advance.  - For the standard polynomial non-linear correction task: the coefficient array starts with the x^2 coefficient and goes up from there. Constant and scale correction are explicitly omitted because they are not useful.",NULL
RFC-165,"Tech Talk: How DES uses easyaccess and experiments with Jupyterhub","Last week when [~jbecla] and I were at NCSA, [~mcarras2] showed some neat work that they've been doing on the DES side of the shop. One is called [easyaccess | https://github.com/mgckind/easyaccess] -- an enhanced ""SQL database client for astronomers"" -- and the other is the capability to run Jupyterhub running on Nebula (with all analysis software required by DES already preinstalled).    Matias kindly agreed to give a talk about it to the broader project, so I'd want to use this RFD to poll us for a good time. Right now, it looks like the next slot -- March 22nd @ 12:30 PDT -- is open. Let me know if that would work for you, to see if we'd have sufficient quorum to make it worthwhile.",NULL
RFC-166,"Port simple shape measurement plugin to a repository with corresponding name","On HSC there exists a measurement plugin for doing simple shape measurements which lives in the package meas_extensions_simpleShape. I am proposing to create a new lsst package with the same name. This not only provides a method to get shape estimates early on in the processing pipeline, but serves as an example for anyone looking to implement a measurement extension plugin package.",NULL
RFC-167,"Temporarily add esutil to the stack","For the implementation of indexed reference files, I needed python bindings to an indexing scheme.  Because it is fairly easy to use, I settled on HTM.  Unfortunately, there are no bindings to HTM functionality in the stack.  It appears that this functionality is likely to materialize relatively soon, however I am ready to merge the indexing code now.    My proposal is to add esutil to the stack until we have our own bindings.  At that point, I will remove esutil from the stack.  Implementation will be similar to that for other non-conda installable packages: e.g. healpy.  Specifically, we'll get a release tarball and use the standard prep, build and install to install it in the stack {{PYTHONPATH}}.",NULL
RFC-168,"Add python package lmfit","It would be nice to be able to optimize functions written in {{python}}/{{numpy}}. {{scipy.optimize}} contains interfaces to a variety of minimization methods, including {{leastsq}}, a basic wrapper around MINPACK’s {{lmdif}} and {{lmder}} algorithms for the widely-used Levenberg-Marquardt nonlinear least-squares minimization method.    The {{lmfit}} package (https://pypi.python.org/pypi/lmfit) adds a layer on top of {{scipy.optimize}} that adds capabilities for bounding or constraining parameters (using transformations similar to those used by {{minuit}}). It also includes methods for estimating parameter confidence intervals and other useful quality-of-fit metrics. {{lmfit}} was tested and eventually used for the improvements to the {{dipoleMeasurement}} algorithms (DM-3880), and was found to be at least as efficient as the existing C++ algorithm (which uses {{minuit2}}) and more efficient than using other options that I have tested, such as {{iminuit}}. I also found that the capability to speciry parameter constraints was useful in enhancing optimization efficiency and robustness.    Now that it has been accepted that {{scipy}} is to be integrated into the stack (RFC-128; DM-5446), I’d like to propose adding {{lmfit}} as a dependency so that we can merge the updated {{dipoleMeasurement}} task (DM-4999). {{lmfit}} is pure python, so does not have any issues with building (other than the {{scipy}} dependency, and is simple to install via {{conda}} or {{pip}}. We can implement it in an identical way to the {{scipy}} integration.",NULL
RFC-169,"Implement HSC-like stack provenance","We need a means of recording the version of the stack used in processing data, and of verifying that a consistent version is being used.  HSC has some code used to do this in {{CmdLineTask}} in the same kind of way that we currently do with configurations, and this provenance recording is an important part of our production runs.    Following [a discussion on CLO|https://community.lsst.org/t/stack-provenance-in-tasks/605], it seems the best way forward is to port the HSC code and adapting it to pull the versions from the code itself rather than EUPS.  The intention is that this code will eventually move to the orchestration or execution packages, but this plan provides a near-term solution and the foundation for the longer-term solution.  Barring objections, I propose to implement this plan in DM-3372.    For details on the HSC implementation, see [the discussion on CLO|https://community.lsst.org/t/stack-provenance-in-tasks/605], and the HSC code in [pipe_base|https://github.com/HyperSuprime-Cam/pipe_base/blob/master/python/lsst/pipe/base/cmdLineTask.py#L397-423] and [daf_persistence|https://github.com/HyperSuprime-Cam/daf_persistence/blob/master/python/lsst/daf/persistence/eupsVersions.py] (though that code in daf_persistence should probably live next to the implementation in pipe_base; but let's not worry about such details here).",NULL
RFC-17,"tools/format for long-term planning (and LDM-240)","In our effort to rewrite our pieces of LDM-240, [~krughoff] and I have been using [Trello|https://trello.com/b/kkHmBjU2/s15-planning], just as a free-form,  electronic post-it note system that allows us to quickly reorganize information.  For the purpose of making major changes to the plan, this is much better than Excel, but I think it may be a worse way of looking at the finished result, and I think we really need something that does more than either Trello or Excel.  In particular:   - I think we need a format that captures tasks, milestones, and dependencies more naturally (and distinguishes tasks and milestones).  - We *might* need to be able to export something that looks like just what's in the main LDM-240 worksheet right now (i.e. just milestones).  - We need to be able to reorder things easily (i.e. dragging them around, not modifying fields on an entry).  - I think we want to do this at a 6-month rather than a 1-year granularity.  I do *not* think we need this to be at all connected to JIRA; this is about the relationship between epics and the meta-epics that hold them (and the WBS that hold those), not about the relationship between stories and epics.  Moreover, I think we *want* to explicitly create real JIRA epics from this plan manually every six months, not just export them blindly.  Some possibilities I've heard floated as good tools to organize this information are:  - MS Project (apparently [~mjuric] told [~krughoff] it isn't half bad?)  - http://leankit.com/ (something [~jhoblitt] suggested on HipChat; $$)  - New, contorted JIRA project (stories=epics, epics=wbs, statuses=years)  I don't have a concrete proposal for this, but I'd like to hear any other requirements people might have or suggestions for tools.",NULL
RFC-170,"Add a 3rd Party package for Erin Sheldon's ngmix github repository","We would like to test this code for creating gaussian mixtures, and possibly add a meas_base style plugin to facilitate its testing within the DM stack.",NULL
RFC-171,"Add pipe_drivers to lsst_distrib","pipe_drivers provides some useful scripts for processing data on a cluster using ctrl_pool.  I propose it be bundled with lsst_distrib so users can use it conveniently.    pipe_drivers will also pull in ctrl_pool and therefore mpich and mpi4py.  mpi4py had some [trouble building on OSX|DM-5409] recently, but this has been resolved.  Based on recent lsstsw runs, pipe_drivers builds fine and should not break lsst_distrib when included.",NULL
RFC-172,"Add meas_extensions_shapeHSM and dependencies to lsst_distrib","meas_extensions_shapeHSM wraps the HSM shape measurement algorithms in GalSim for use by our measurement framework.  It is probably the premier shape measurement package working with our stack at the moment.  We are using it in HSC as the principal source of our shape measurements for early science.  I propose including it in lsst_distrib so it may easily be built, used and CI-ed.    meas_extensions_shapeHSM depends on GalSim, which in turn depends on TMV.  Both of these have already been packaged for use in LSST.    Implementation will be on DM-2141.",NULL
RFC-173,"Pass a butler to the constructor of all command-line tasks","Based on this discussion https://community.lsst.org/t/how-to-get-a-butler-to-the-new-reference-object-loading-code/719/21 it is useful to pass a butler to the constructor of some command-line tasks. Few tasks need it, but enough do that I am proposing we always pass the argument.    The implications of this are small. All task constructors accept a keyword argument dict and pass it along to construct other tasks, so no rewriting of existing tasks is necessary except those that already accept a butler (redoing how arguments are handled and modifying or eliminating custom task runners). Most of the work would be done in pipe_base.    One wrinkle is that @rhl has demanded that we not pass a butler to the constructor of non-command-line tasks (or perhaps it is cleaner to say tasks that do no I/O). This means that pipe_base {{Task}} must determine if a task is a subclass of {{CmdLineTask}} or {{Task}} before deciding to pass the butler argument. This is an easy bit of code to write. One other implication of this restriction is that if we have the following subclass hierarchy: A -> B -> C where A and C are command-line tasks and B is not, then C will not receive a butler. I think we can live with this.    One problem with this proposal is that it introduces more than one way to do things. Command-line tasks that use the butler passed to their constructor potentially have two ways to access a butler: via a butler saved by the constructor or via the butler or data reference passed into the {{run}} method or other methods that performs I/O. What's worse is that the butler almost certainly has to be saved as an instance variable for command-line tasks in order for {{makeSubtask}} to have it available to pass it along. However, I suggest we simply mandate that any method that performs I/O have a butler passed to it, and ignore the saved butler, this makes the fact that the method performs I/O more obvious.    An alternative we rejected is to eliminate the duplication by only passing the butler to the constructor. This entirely eliminates the ambiguity but has other issues, including it makes it harder to see which methods perform I/O, it would not work well with existing command-line tasks (which receive data references containing a butler) and apparently it would not work well with new super task. Hence we are not proposing it.",NULL
RFC-174,"Suppress gcc warnings about ""unused local typedefs""","I propose adding {{\-Wno\-unused\-local\-typedefs}} to our gcc options.  This cleans up the build significantly, because there's a flood of warnings of this type coming from boost.  If we suppress those, it might become possible to notice warnings that we care about.    This option is supported by gcc >= 4.8, so I think it's safe because we already require that because of our use of C++11.    As an example, I built meas_extensions_shapeHSM with and without this flag.  Currently, the build outputs 369 lines.  With this flag, the build outputs 57 lines.",NULL
RFC-175,"Butler support for multiple repositories, intermediate outputs, and parents.","Insufficiencies in the current private (not-yet-publicized) architecture for multiple repository support in Butler have been identified and discussed with KT & Gregory. KT and I have been working from there to create a more appropriate design for multiple repositories in Butler. This new design should provide needed support for saving intermediate outputs for reruns, and for writing butler.put for different datasets to the correct repository/repositories.     I’d like to discuss it to see if it’s ok to proceed with implementing this, or identify issues that need to be addressed first. Specifically I want to be sure this solves the problems KT and Gregory and I discussed a couple weeks ago.    Please see the linked google doc to review the design.  https://docs.google.com/document/d/1IM29jAfVzk74m51dh0EkF4po7VQtw2HzkIX9rkBNAhM/edit?usp=sharing    The topics I'd like to cover are:  # New Butler API  ## Changing the API in existing code would be a bit of work. We can manage it so that existing code does not have to change (until it needs to take advantage of new butler features): I’d propose using keyword arguments, and not modifying the default signature, except to add default value root=None. This allows the old API to be used by default, and the new API to be called explicitly.  # Repository Stacks  ## Does this satisfy Gregory’s requirements?  ## Is the opaque stateful behavior of the RepositoryStack ok, or do we need to change this? (how?)  # Inferring output cfg parameters from input cfgs  ## This point is raised to emphasize this is how we will proceed with determining outputs from inputs as best as possible. (It should not be a lot different than existing butler behavior.)  # Are there any issues that need to be addressed with implementing multiple repository behavior this way?",NULL
RFC-176,"Add psutil to stack","As part DM-5561 I have written an additional test case in {{lsst.utils.tests}} to check for leaks of file descriptors. To do this I use the {{psutil}} package from PyPI.  It would be really useful if I could add psutil as a stack dependency. psutil seems to be very portable (python 2.6+, windows/solaris/linux/bsd/OSX) and has no dependencies.    The two alternatives I have are:  * Copy the code from {{meas_astrom/tests/openFiles.py}} which uses {{lsof}} and is not overly portable.  * Only run the test if psutil is installed.",NULL
RFC-177,"Enforce Astropy-compliant strings for units in afw.table","With DM-5641, we'll soon be able to get {{astropy.table}} views into {{afw.table}} objects. That will be more useful if astropy can understand the unit strings we give it, and since we currently don't use those strings as anything more than textual information for humans, we might as well standardize on the terms they've already selected (see http://docs.astropy.org/en/stable/units/). This standardisation is covered by DM-5642.    *I propose that enforcement of compatible units will be turned on by default.*    This will have three related benefits:    # Minimises potential parsing problems by defining and enforcing a standard format for units.  # Will allow for easier interoperability with astropy (e.g. with quantities) in the future.  # Prevents misuse of unit field for storage of non-unit related information.",NULL
RFC-178,"Add Astropy to stack","Following the Astropy/LSST summit in March and the completion of the subsequent SPIE paper (DM-5444) I propose that we can now start using Astropy in LSST DM code. I am proposing to do this via a stub EUPS package as for {{numpy}} and {{scipy}}. No change is required to Jenkins or {{newinstall.sh}} as Astropy is already included because of a Sims dependency.    I propose we start with version 1.1 as the minimum acceptable version (this is the version that includes pandas support).    Astropy affiliated packages are not part of this RFC.",NULL
RFC-179,"Remove PTR and CONST_PTR macros","With the adoption of RFC-100 and the completion of DM-5879 we now use {{std::shared_ptr}} and friends throughout the stack.  We are unlikely to ever change smart pointer types again (since we are now using the standard). The pointer macros are already not consistently used. Moreover, given that C++11 no longer requires spaces between nested templates, the macros are no longer needed.  Therefore I propose to remove them.    In my opinion this has three benefits:    0. It improves consistency;  1. It improves code readability by not having to remember what the macros do;  2. It removes undue bias towards {{std::shared_ptr}} over the often better choice {{std::unique_ptr}}.",NULL
RFC-18,"LOWERCASE SPELLING OF LSST GITHUB ORGANIZATION","THE ""NORM"" IS FOR GITHUB ORGANIZATIONS TO NOT USE CAPITAL LETTERS.  IT IS UNUSUAL TO USE UPPERCASE CHARACTERS IN URLS DUE TO THE HOST PORTION OF THE SCHEMA BEING INHERENTLY CASE INSENSITIVE AND THE ORG NAME IS PART OF THE CANONICAL REPOSITORY URL.  FURTHER, IT IS INCONVENIENT TO BOTH READ AND TYPE LOTS OF CAPITALS LETTERS (KT HELPFULLY POINTED OUT THERE IS A MAGIC KEY THAT WILL CAPITALIZE ALL CHARACTERS FOR YOU, PERHAPS THE POINT ABOUT TYPING THEM IS MOOT).  PROPOSAL:  CHANGE THE SPELLING OF ""LSST"" GITHUB ORGANIZATION TO ""lsst"".  IMPACT:  * COSMETIC ONLY CHANGE * HAPPINESS OF THOSE THAT PROGRAM WITH UNDERSCORES: +1 * CAMEL-CASE JOCKEYS: ALIGNMENT UNAFFECTED * CLONE REMOTES: UNAFFECTED; GITHUB SSH+GIT AND HTTPS REMOTES ARE TRANSPARENTLY CASE INSENSITIVE (NOTE THAT HTTPS REMOTES ISSUE A REDIRECT TO THE CANONICAL CASE URL) * BROWSER HISTORY: UNAFFECTED; UNLIKE FOR REPO REMOTES, GITHUB DOES NOT ISSUE A REDIRECT. E.G. HTTPS://GITHUB.COM/lsst/AFW  ",NULL
RFC-180,"Pass background to NoiseReplacerTask","`NoiseReplacerTask` wants some statistics about the background that was subtracted from the exposure, but it gets these in a fragile and roundabout fashion: it expects the code that measures the background to put the mean and variance into the exposure's metadata, using special keys. It is difficult to enforce correctness because background is measured several times while processing an exposure.    I propose that the background be passed directly to `NoiseReplacerTask`. This will require passing the background through the various measurement tasks, which will require small changes to code that calls the measurement tasks.    I further propose to remove computation of background statistics from the background fitting code (presently `lsst.meas.algorithms.estimateBackground`, but that will change when DM-5323 is merged).    In the long run, the cleanest solution is be to save the background model as part of the exposure (e.g. by adding it only in python and tweaking the wrapper code for persisting exposures to handle the background separately from the underlying implementation). However, I don't see that happening soon enough, hence this proposal.  ",NULL
RFC-181,"Add meas_extensions_photometryKron to lsst_distrib","meas_extensions_photometryKron provides a measurement algorithm which is returns the flux in an elliptical aperture as defined by [Kron (1980)|http://adsabs.harvard.edu/abs/1980ApJS...43..305K]: the Kron radius is measured as the moment within an (elliptical) aperture of six times the (adaptive) moment, and the Kron flux is measured within an (elliptical) aperture of twice the Kron radius.    This package is currently available, and can be built using lsstsw. I propose including it in lsst_distrib, so that it can be installed more easily and will automatically be included in CI.    The package has no dependencies which are not currently satisfied within lsst_distrib.",NULL
RFC-182,"Trial conversion from Swig to Pybind11","[~pschella] recently spent some time exploring a few alternatives to Swig, by building wrappers for some example C++ interfaces I put together.  The results for [Cython|http://cython.org/] and [Pybind11|https://github.com/pybind/pybind11] have been written up in technical reports ([DMTN-13|https://dmtn-013.lsst.io] and [DMTN-14|http://dmtn-014.lsst.io/en/latest/], respectively), and [CFFI|http://cffi.readthedocs.io/en/latest/] was rejected early due to the need to write pure C wrappers for every C++ interface.    I encourage everyone to read those notes and form their own opinions, but I think they make a very strong case for switching to Pybind11 and essentially reject Cython.  In particular:  * Pybind11 is essentially a full rewrite of Boost.Python, but as a dependency-free header-only  library.  It's got all the nice support for edge cases and careful memory management that Boost.Python has, better support for the C++ standard library, and the extensibility that comes from being able to just write customization code in C++ without going through a code generator.  I don't have a good sense for how widely adopted it is (the fact that it's been around less than a year puts a pretty strong upper bound on that), but the main developer is very active and responsive, and it has excellent documentation.  * Cython has a ton of market share (mostly, I think, because it's very good at adding a small amount of compiled code to Python), but its C++ support is immature and they've made some architectural decisions that make me doubt it will ever really be any good. I'd put their ceiling (for wrapping C\+\+) as perhaps only slightly better than Swig, and it's really not anywhere close to Swig now.  This is a disappointment - Cython is what Astropy uses, as well as a significant fraction of the scientific community.  But I'd rather use Swig or even the raw Python C API to wrap C\+\+ at this point.    With that in mind, I think our choices come down to switching to Pybind11 or rewriting much of our Swig to improve our dependency handling.  Given that even the latter would still be a significant amount of work, and I think Pybind11 is the better choice from a (preliminary) technical standpoint, I'd like to propose that we have [~pschella] spend some fraction of his time over the next few months actually converting Science Pipelines code (from the bottom up) to use Pybind11 on a branch.   The intent is that this would inform a *later* decision around the time of the AHM on whether to convert the rest of the stack or throw away the branch.    All I'm proposing right now is that we devote some of Pim's time to this project; I'd like to allocate enough effort that we have a reasonable shot at getting through much or all of afw, but his actual pace will tell us quite a bit about the cost of a more complete conversion.    One reason I'm attracted to Pybind11 is that we do want to spend more effort defining Pythonic interfaces - this is easier to do in Pybind11, I think, and wanting custom-crafted interfaces negates much of the automatic-interface-generation advantages of Swig.  But I'm *not* proposing that we make any such changes while converting to Pybind11; I think it's much easier if we try to maintain the same Python interfaces whenever possible at this point, and deal with making them more Pythonic in the future.",NULL
RFC-183,"Variant tasks should have a registry and an abstract base class","When there are different versions of a given task, I propose that we require those tasks to have an abstract base class that defines the interface.    I further propose that the same file that defines the abstract base class also define a registry, and all implementations register themselves. This has several advantages:  - Finding variant subtasks is easier  - Switching between variant subtasks can be done from the command line (by comparison, retargeting requires a config override file)  - Switching between variant subtasks is safer because they all have the same API  - Config override files or users can configure more than one variant, again improving the ease and safety of using variants    Details of the proposal:  - Abstract base classes should use the `abc` package  - A task should have an abstract base class if it has multiple implementations or is likely to have them. Otherwise I suggest not bothering, in order to avoid clutter.  - Use `lsst.pex.config.RegistryField` for the registry. It is simple and easy to use. Note that `lsst.pipe.base.Task.makeSubtask` will need minor work to support this.    Once this is implemented I anticipate essentially all retargeting of subtasks to be replaced by use of the registry.",NULL
RFC-184,"Proposed changes to Butler (and its API) for multiple repository support","h1. Description  We need to make some changes to Butler to better support using multiple repositories. We discussed with KT & Gregory, and in RFD. The new design should allow:   * multiple input and output repositories to be managed by Butler  * control over which input repositories are used when calling {{Butler.get}} (see ""tagging"" below)  * provide needed support for saving intermediate outputs for reruns (this is not discussed any more in this RFC)  * {{Butler.put}} should write the object to the correct repository/repositories (not to all repositories) according to the dataset type.      We decided :  * Butler will manage its own input and output repositories, and it may have any number of each (including zero).   * The api to instantiate a butler and its repositories will be {{Butler(inputs=..., outputs=...)}}, where inputs and outputs are repository configurations or a sequence type of repository configurations.  ** Changing the API in existing code would be a bit of work. We can manage it so that existing code does not have to change (until it needs to take advantage of new butler features): we will use keyword arguments {{inputs}} and {{outputs}}, and not modify the default signature, except to add default value root=None. This allows the old API to be used by default, and the new API to be called explicitly.   * Butler will only read from input repositories and will treat outputs as read-write (repositories may enforce write-only rules internally).   * Repositories will keep track of their parents.  * All the input repositories will become parents of each output repository.    Repository configurations can be stored inside a repository and also can be stored seperately from the repository (there are implications to each, more info below).     h1. Inputs and Outputs  Butler will only perform read actions on input repositories and will perform read and write actions on output repositories. Repositories may also have an internal mode that can be one of:  * read  * write  * read-write     Repository r/w mode must be enforced inside the repository. Attempting to pass a read-only repo as a butler output or a write-only repo as a butler input will raise an exception.    Input repository configurations must specify certain parameters. In some cases output configurations may be more sparsely populated and derive parameter values from input configs, but the inputs must be uniform. See the next section.    h1. Output configuration derived from inputs  Some settings for output configurations can be derived from input configurations. For example, if an output configuration does not specify a mapper, the input mapper may possibly be assumed (this will work as long as all the input repositories use the same type of mapper; if the inputs use different types of mapper then a single type mapper can not be inferred to use for the output repositories). When possible the butler will use settings from input configurations to complete output configurations.     h1. Search Order  The order of repositories passed to inputs and outputs is meaningful; search is depth-first and in order (left to right). See the attached diagram.    h1. Tagging     Input repositories can be “tagged” with a temporary id that gets used when reading from a  repository. {{RepositoryCfg}} will have a method to ‘tag’ a repository with a value or object. A repository  can be tagged with more than one tag by passing in a container of tags. The tag is not persisted with the  repository.    Originally we were planning to extend the Butler API so that Butler functions that perform read operations on repositories would take an optional tag argument, e.g.  {{def get(self, datasetType, dataId={}, immediate=False, tag=None, **rest)}}. After conversations with  [~jbosch] it became clear that the dataId and tag should be thought of more as a unit. This allows the dataId to encapsulate the fact that it is intended for a particular repository (or repositories) (and this implicitly means that the dataId keys conform to the semantics of the mapper + policy in that repository).     A new class will be created that contains the dataId dictionary and the tag.    {code:python}  class DataId(collections.UserDict)      def __init__(self, id={}, tag=None):          ...  {code}    * {{tag}} may be a string or other type, including container types. When searching repositories, if the tag  argument is not None, then repositories will only be searched if their tag equals the value of tag (or if a  match is found in either container of tags).  * When searching, if an input repository is tagged, all of its parents will be searched (even if they do not  have a tag).  * The Butler API will remain backwards compatible: if a dict is passed for dataId, as needed it will be  constructed into a DataId object internally. For example, both of the following will work (but only the latter will limit the search according to tagged repositories)  {code:python}  # dataId-as-dict  butler.get(""calexp"", {'visit':1, 'ccd':1})  # dataId-as-DataId  butler.get(""calexp"", DataId(id={'visit':1, 'ccd':1}, tag='CFHT'))  {code}  * DataId will be a type of dict, the map functions will access the contained id (the key-value part of the dataId); existing calls on the  dataId by e.g. mapper subclasses will continue to work.      h1. Repository Configuration  Repository Configurations can be in different states. The state dictates which parameters must be populated.  * Getting ready to butler.put() a configuration to a repo-of-repos  ** Mapper defined.  ** Storage & Root might be defined (it often would be left undefined, and then it will be defined by metarepo. This is useful when the repos exist ‘in place’, directly within the metarepo).  ** Parents may be defined.  ** Mode must be defined.  * Ready to use as Butler input:  ** Points to a location of an existing repository but no other info is known.  *** Mapper not defined.  *** Storage & Root must be defined.  *** Parents not defined.  *** Mode not defined.  ** Info is known (already deserialized cfg or otherwise not deserializing)  *** Mapper might be defined (in Butler it could be inferred from input repository/repositories).  *** Storage & Root must be defined.  *** Parents must be defined (if any).  *** Mode must be defined.  * Ready to use as Butler output  ** Mapper may be defined. (If it is not and Butler’s inputs all have the same mapper then that mapper will be added to the cfg. If Butler’s inputs have different mapper types then Butler will throw instead of assigning the mapper).  ** Storage & Root must be defined.  ** Parents must not be defined. (The Butler’s inputs will be added as parents to the cfg).  ** Mode must be defined (one of ‘w’ or ‘rw’. ‘r’ (read-only) will throw).  * Serialized in place (cfg resides in root of repository)  ** Mapper must be defined.  ** Storage & Root may be defined (at least in some cases it can be inferred from the physical location of the repository.  ** Parents must be defined.  ** Mode must be defined.   * Serialized elsewhere (cfg does NOT reside in repository)  * If repo also has a cfg in place:  ** Same as “points to a location of an existing repository…”.  ** If repo does not have a cfg in place or if other settings are desired:  *** Mapper must be defined.  *** Storage & Root must be defined.  *** Parents must be defined.  *** Mode must be defined.    h1. Getting a Configuration  A configuration can be retrieved simply by specifying root. On a posix system, this will be a string (a path). In other storages this may need to be a more complex object.I propose to add a static getter:   {code:python}  class RepositoryCfg:     @staticmethod     def getCfg(root):        """"""Returns a RepositoryCfg from the cfg stored at root""""""  {code}    h1. Examples  h3. Creating, getting, and using repository cfg  _“I got this input repo config from some location (the location in this case does not matter) and I want to instantiate a butler with it as my input repo, and with an output at <here>._  _For a single output, say <here> is at ""foo/bar""_  _For multiple output, say <here> is at ""foo/bar"" and ""foo/baz""_    {code:python}  # single input & output  import lsst.daf.persistence as dp  inputCfg = foo.getCfg()  outputCfg = dp.RepositoryCfg()  outputCfg.setRoot(""foo/bar"")  butler = dp.Butler(inputs=inputCfg, outputs=outputCfg)    # multiple inputs & outputs  import lsst.daf.persistence as dp  inputCfgs = foo.getCfgs()  outputCfgs = []  for root in (""foo/bar"", ""foo/baz""):  	cfg = dp.RepositoryCfg()  	cfg.setRoot(root)  	# I think in actual use there would be some other difference between the repositories  	# that would be the reason to have 2 output repositories. Maybe they exist in different  	# storages (for example, Posix and S3), or have different mappers.  	outputCfgs.append(cfg)  butler = dp.Butler(inputs=inputCfgs, outputs=outputCfgs)  {code}    _“I got this input repo config AND an output repo config from some location (that does not matter) and I want to set up a butler with it as my input repo, and with that output repo”_  {code:python}  # single input & output  import lsst.daf.persistence as dp  inputCfg = foo.getInputCfg()  outputCfg = foo.getOutputCfg()  butler = dp.Butler(inputs=inputCfg, outputs=outputCfg)    # multiple inputs & outputs  import lsst.daf.persistence as dp  inputCfgs = foo.getInputCfgs()  outputCfgs = foo.getOutputCfgs()  butler = dp.Butler(inputs=inputCfgs, outputs=outputCfgs)  {code}    _“I’m creating a new output repository (with no parents) and want it to go <here>, with this mapper”._  _For a single output, say <here> is at ""foo/bar""_  _For multiple output, say <here> is at ""foo/bar"" and ""foo/baz""_    {code:python}  ## single output  import lsst.daf.persistence as dp  outputCfg = dp.RepositoryCfg()  outputCfg.setRoot(""foo/bar"")    # any of these should be supported as iarg to setMapper:  if option1:      outputCfg.setMapper(dp.CameraMapper)  elif option2:      outputCfg.setMapper('lsst.daf.butlerUtils.CameraMapper')  else: # option3      mapper = MyMapper()      outputCfg.setMapper(mapper)  butler = dp.Butler(outputs=outputCfg)    ## multiple outputs  import lsst.daf.persistence as dp  outputCfgs = []  for root in (""foo/bar"", ""foo/baz""):      outputCfg = dp.RepositoryCfg()      outputCfg.setRoot(""foo/bar"")      # any of these should be supported as iarg to setMapper:      if option1:           outputCfg.setMapper(dp.CameraMapper)      elif option2:   	outputCfg.setMapper('lsst.daf.butlerUtils.CameraMapper')      else: #option3          mapper = MyMapper()          outputCfg.setMapper(mapper)      outputCfgs.append(outputCfg)  butler = dp.Butler(outputs=outputCfgs)  {code}    h3. Repository Tagging  _”I have 2 repositories that contain datasets with identical dataset type and dataId, of different versions and I want to compare datasets from those repositories”_    {code:python}  import lsst.daf.persistence as dp  oldCfg = getCfg(version=1)  newCfg = getCfg(version=2)    oldCfg.setTag(‘old’)  newCfg.setTag(‘new’)    butler = dp.Butler(inputs=(oldCfg, newCfg))  oldData = butler.get(datasetType=‘data’, dataId={someId:123}, tag=‘old’)  newData = butler.get(datasetType=‘data’, dataId={someId:123}, tag=‘new’)    # and then, for example:  results = compare(oldData, newData)  ...  {code}    h1. Diagram  !example_RFC-184.png|thumbnail!",NULL
RFC-185,"Update ""Using Boost"" section in DM Developer Guide to prefer standard library by default","With DM-5880, DM-4035, DM-4036, DM-4014 and DM-5879 usage of Boost within the stack has decreased. This will require modifying the ""Using Boost"" section in the DM Developer Guide.  Rather then specifying a list of Boost libraries / functions that are not to be used anymore, I propose to add a new default rule to the effect that ""The C++11 standard library should be preferred over Boost equivalents wherever possible."" Possibly followed by a list of exceptions.  This puts the responsibility with the developer (and the reviewer) to check if a standard library equivalent is available whenever a Boost function is used.  This change will not affect the list of libraries *not* allowed in the stack (or only allowed after review).",NULL
RFC-186,"Update scons to v2.5.0","{{Scons}} is a core dependency of the LSST software stack. Before we can add Python3 support scons must be updated to support Python 3 itself. Given that that will be a major change to scons I would like to upgrade our scons from 2.3.5 to the current stable version, 2.5.0, so that any issues we have when going to v3.x are known to be caused just by that version and not something that was broken in an earlier release.    I have tested v2.5.0 in ticket DM-5756 and it now builds ok. There was one bug discovered in {{scons}} itself that was breaking our SWIG builds and one change in behavior that triggered a modification of a file in the {{partition}} package. With those fixes in place builds work fine on Linux and OS X.    I propose that we upgrade this after v12.0 of the stack is released.",NULL
RFC-187,"Remove DetectAndMeasureTask","Work on refactoring the aperture correction logic on DM-5877 (see RFC-143), [~nlust] and I have come to the conclusion that trying to keep {{DetectAndMeasureTask}} alive after this change will probably do more harm than good.  Right now, this task aggregates a few subtasks that are frequently used together.    After the DM-5877 refactor, {{DetectAndMeasureTask}} would have to include the following steps, each of which corresponds to a call to a direct subtask's run method (possibly guarded by a ""{{if self.config.do\[Step\]}}""):  # {{SourceDetectionTask}} (as before)  # {{SourceDeblendTask}} (as before)  # {{SingleFrameMeasurementTask}} (as before)  # {{MeasureApCorrTask}} (as before)  # {{ApplyApCorrTask}} (moved out of {{SingleFrameMeasurementTask}} as per RFC-143)  # {{AfterburnerTask}} (moved out of {{SingleFrameMeasurementTask}} as per RFC-143)    However, it would be called with the following qualifications:   - In {{CharacterizeImageTask}}'s first loop to estimate the PSF, only steps 1-3 are run.   - In {{CharacterizeImageTask}}'s second call, only steps 3-6 are run.   - In {{CalibrateTask}}, steps 1-3 and 5-6 are run, but not step 4.    Given the differences in each of these calls (and the simplicity of each step), I think it will be much better to just remove {{DetectAndMeasure}} and put each of these steps (and the corresponding subtasks) directly in {{CharacterizeImageTask}} and {{CalibrateTask}}.  This will involve a tiny amount of code duplication, but it will make the code vastly easier to read, and it has the added bonus of making some of our most frequently used configuration settings more concise and more similar to their coadd-measurement equivalents:  {code}  config.calibrate.measurement.plugins.names.add(""base_SdssShape"")  {code}  instead of  {code}  config.calibrate.detectAndMeasure.measurement.plugins.names.add(""base_SdssShape"")  {code}    Please note that I am giving this RFC less than 24 hours, as we'd like to get these changes in before the release feature freeze, I think a very small number of people will actually care, and I think these changes are virtually required by RFC-143 (otherwise we'd just be moving the confusing aperture correction logic from {{SingleFrameMeasurementTask}} into {{DetectAndMeasureTask}}).",NULL
RFC-188,"Committing to supported DM stack releases","This is a proposal for DM to commit to supporting (perhaps selected) DM stack releases with bugfixes and occasional backports of functionality. The proposal was triggered by a discussion at the Project Science Team meeting last week (incl. [~cstubbs], [~cclaver], [~zivezic] and [~rhl]), but I think the problem's been recognized for a while now. After a little discussion with [~ktl] and [~rhl], I thought that an RFC would be in order.    The issue seems to be that our releases de-facto become outdated very quickly, as bugfixes and new functionality tend to land on master. This makes our downstream users gravitate towards using master (e.g., the sims team), but at a cost of issues with API breakage unrelated to their work and additional instability. This instability propagates far out, as sims codes are becoming used broadly within the LSST community (especially within DESC, and MAF is being used across all collaborations). It also makes the discoverable docs (which may refer to the APIs as they were at the time of the release) inconsistent with what people are actually using (whatever is on master). Finally, it makes it difficult to simultaneously use the DM release and the most recent sims release.    The concrete straw-man proposal (just to begin the discussion) is for us (DM) to commit to:  * Backporting bugfixes to some number of stable releases (likely only the most recent one?)  * Occasional backports of new functionality that may be required by downstream users  * Regularly (at least nightly) CI-ing the backports above  * Making regular (~monthly?) point releases with the backports accumulated above (including binaries, when we get to releasing those)  * Never breaking backwards compatibility in point releases.    If we do this, it should enable our downstream clients (e.g., the sims team) always work off of a stable release, rather than a master. That will make it possible to have DM and sims releases setup-ed (in the EUPS (or conda) sense) at the same time. It will also make support and documentation easier -- we can point everyone to the docs related to the release (which we then maintain), rather than have downstream people follow our development process to understand what's going on on master. Finally, if we commit to supporting stable versions it will be easier for other subsystems to use it (e.g., the camera team).    This proposal will obviously require resources (K-T mentioned maintenance engineers), but I'd like to focus the discussion here on what would be a good process to establish here, both from DM's point of view as well as our downstream users (e.g., sims -- [~ljones] or [~cwalter] or others, feel free to comment). And given the release of X16 in a few weeks, this discussion is timely!",NULL
RFC-189,"Modify star selector and aperture correction interfaces","On DM-6078, I tracked down an aperture correction bug in the SDSS-based lsst_dm_stack_demo to a fundamental structural limitation: measuring aperture corrections require a star selector to be run, but we only run star selectors when we're modeling the PSF, and we don't (usually) model the PSF when processing SDSS data (because we can load the SDSS PSF models, and doing as well at PSF modeling ourselves would require specialization for drift-scan processing).    Fixing this without reintroducing a custom version of ProcessCcdTask will require some API changes; here's what I'm proposing (and have in fact implemented on DM-6078):   - Give {{MeasureApCorrTask}} its own star selector subtask.  This requires moving {{MeasureApCorrTask}} from meas_base to meas_algorithms, because the latter is where the star selector interface is defined.  It also requires {{MeasureApCorrTask}} to take an {{Exposure}} argument instead of just the {{Exposure}}'s bounding box, since some star selectors require access to the Exposure.   - Implement a new trivial star selector, {{UsePreviousStarSelector}}, that just looks at an existing flag field, to serve as the default for {{MeasureApCorrTask}}.  This preserves the current behavior of {{MeasureApCorrTask}} for other cameras (but changes a small part of their configuration tree).   - Modify the star selector constructor interface to take a {{Schema}} argument, as all other {{Task}}s that may wish to extract keys do.  (The new star selector could have been implemented (less efficiently) without this, but now seemed a good time to fix it).    In addition, I'm proposing that we modify the behavior of aperture correction measurement to raise an exception instead of warn when it fails.  This bug - which is now a release blocker - has been present in a pipe_tasks unit test (as well as the demo) for many weeks now, and has gone unnoticed because it's just a warning.  ",NULL
RFC-19,"Not Renaming afw","In the Legendarily Horrible Renaming Session of February 9, we tentatively agreed to rename ""afw"" to ""toolkit"", but that we'd sleep on it first.  Since then, I've heard much more grumbling than glee, so I propose that we do *not* rename afw, and instead merely re-acronym it, because most (all?) of the objections to the name ""afw"" fell into one of these categories:  - ""afw"" is cryptic and not descriptive (but so is ""toolkit"")  - it's really not an ""applications framework"".  So I'd like to address the latter by getting *at least* the word ""applications"" out of the acronym.  ""Astronomy"" is a natural substitute, so I hereby propose that we redefine ""afw"" to mean ""astronomy framework"".  Other suggestions welcome.  I'm also open to replacing ""framework"" with something else, if anyone has a suggestion, but I'm not nearly as bothered by us not using that term in precisely the right way (as I think most of our users won't notice our imprecision), and I'm not quite ready to call it ""Astronomy For the Win"".  I know everyone just wants this discussion to be done with, but I'm hoping not renaming afw will give us more energy for other refactoring/renaming, where I think there's both a lot more room for improvement and a lot less pain involved in the change.",NULL
RFC-190,"Use nan for saturation level to prevent masking","The saturation level for each amplifier is specified in the {{AmpInfoTable}} and is of type {{int}}. However, the ISR code that masks saturated pixels checks if the saturation level is {{nan}} and avoids masking if so. An {{int}} can never be {{nan}} so saturation is always flagged. I am guessing we never noticed because all cameras set a reasonable saturation value.    I want to fix this because I am adding an analogous level for setting the {{SUSPECT}} mask plane. It is used in the same way as the saturation level and set at the same point in the ISR code, but few cameras are likely to use it (at least initially), so a workable value for ""do not mask"" is important.    I see two good solutions:  - Keep saturation level an {{int}} but use a value of <= 0 to indicate ""do not mask""  - Make saturation level a {{float}} and continue to use {{nan}} for ""do not mask""    Is there a strong technical argument for one or the other? I can't think of one. {{float}} and {{int}} both have adequate dynamic range. I plan to add a field to  {{AmpInfoTable}}, so the schema will change in any case. The ISR task detects saturation on a {{float32}} image, but any casting is done once for many pixels, so the choice will not have a measurable effect on speed.    My main desire is to pick something quickly and move on.    Solutions I rejected:  - Add a separate flag to the {{AmpInfoTable}} for ""the saturation level is invalid"". I want to avoid the clutter of an additional field and having two closely related values when one would suffice. And such a solution does not answer the question of which type to use for the saturation and suspect levels.  -  Specify a huge value to avoid masking. This works, but is hard to read. Also, we either have to test if the value is larger than the camera can output (which is non-trivial) or loop over all pixels. I prefer an obvious value such as {{nan}} or {{0}}.",NULL
RFC-191,"Can we add some unsupported functions to Eigen?","I am using the Spline fitting and the Levenberg-Marquardt algorithms for the PFS pipeline. Both are part of the Eigen unsupported modules. Unsupported modules are currently not part of our eigen version. Are there any objections to adding them to our own eigen version?",NULL
RFC-192,"Single source for camera data","Camera geometry used to be defined using PAF (policy) files, which are now deprecated.  As part of the transition to the refactored camera geometry scheme, scripts were introduced to convert from the PAF files to the new camera geometry configuration scheme which uses FITS files and a python file to describe the camera.  These scripts (""genCameraRepository.py""  in obs_cfht, ""makeLsstCameraRepository.py"" in obs_lsstSim, ""makeDecamCameraRepository.py"" in obs_decam) are still part of the obs_* packages, and some people rely on them for making changes to the camera description.  On the other hand, the generated FITS files and python file are also first-class members of the obs_* packages.  This means that we have two sources of the same information, which is dangerous.  What should be viewed as the primary source?  The options appear to be:    1. The generated FITS files and python files are primary, and the conversion scripts were temporary, intended for the transition only.  This is the approach we have taken in obs_subaru, where we have deleted the conversion script as a relic of the past and track only the FITS tables because that's what the {{CameraMapper}} uses; the disadvantage is that changes to the camera geometry require editing the binary FITS tables.  If this option is accepted, we should delete the conversion scripts in all obs_* packages.    2. The conversion scripts are primary, and the generated FITS files and python files are just a cache for the {{CameraMapper}} to read.  This provides a readable and easily editable source, but requires additional steps to generate what the {{CameraMapper}} will use.  If this option is accepted, we should delete the generated FITS files and python files from the obs_* packages and generate them at build time. We should also update the conversion scripts to use a different source than the deprecated PAF files (e.g., YAML).    3. Some other source is primary.  Maybe the FITS files weren't such a great idea because they're hard to edit, and we should specify our camera geometry purely in python.  This is [the approach taken in obs_monocam|https://github.com/lsst/obs_monocam/blob/master/python/lsst/obs/monocam/monocam.py].  However, Monocam has only one CCD, and maybe this wouldn't be feasible for cameras with a hundred CCDs.    It may be that this choice could be different for different obs_* packages depending on the preferences of the maintainer, but a choice needs to be made for each.",NULL
RFC-193,"Adopt AST for WCS and transforms","In order to achieve our science requirements, LSST will need to correct for a variety of complex image distortions (optical focal plane, CCD tree rings, etc.). The current software in the stack (afw.image.Wcs), built on top of FITS WCSLIB, is not capable of handling these distortions.    Based on the research performed in DM-3873 (summarized in [DMTN-010|http://dmtn-010.lsst.io]), we propose to replace the current World Coordinate System handling and XYTransform code in the LSST stack with [Starlink AST|http://starlink.eao.hawaii.edu/starlink/AST], using a new API that is under development in DM-3874 (that API will have its own RFC). AST appears to perform well enough for our needs, meets our current functionality requirements, and allows us to continue to make Wcs and transform calls in the C++ layer. As part of this, we will work with AstroPy developers to ensure that we share a serialization format with AstroPy's [GWCS|http://gwcs.readthedocs.io] library.    In addition, we propose to contract out David Berry (the AST developer) to develop a new, modern AST written in C++, taking lessons learned from AST, GWCS, and the transform API that we are currently developing. This would allow us to take the new AST under the LSST umbrella, so that we do not have a long-term dependency on AST's idiosyncratic ""object oriented C"".    We explored the possibility of using GWCS, but believe there are too many performance questions for it to be a viable choice, particularly for the multifit and image warping use cases.    For details of our requirements, the available options and the decision process, see [DMTN-010: WCS and Distortion Requirements and Existing Options|http://dmtn-010.lsst.io].",NULL
RFC-194,"Names of rotation columns in Exposure and Visit tables in baseline schema","DM-1785 is supposed to add new column to Visit and Exposure tables in baseline schema, this column will contain sky rotation angle. While working on it I noticed that schema contains {{rotation}} column which is camera rotation angle (in alt/az frame). To avoid possible confusion I propose to rename existing column to make it more explicit. Additional concern for existing column is that documentation does not specify whether camera angle (and altitude/azimuth) is determined from hardware, either expectation or measure, or does it come from later analysis, so we are also seeking input on what documentation for those three columns should say.    Current column definition (see also [here|https://github.com/lsst/cat/blob/master/sql/baselineSchema.sql#L2794]):  {noformat}      rotation DOUBLE NOT NULL,          -- <descr>Rotation of the camera.</descr>          -- <unit>deg</unit>  {noformat}    With this proposal it is going to change to (description for altitude/azimuth will be updated as well):  {noformat}      cameraRotation DOUBLE NOT NULL,          -- <descr>Rotation angle of the camera in alt-azimuthal frame as determined from <source>?.</descr>          -- <unit>deg</unit>  {noformat}    The new column for sky rotation angle will be defined as:  {noformat}      skyRotation DOUBLE NOT NULL,          -- <descr>Sky rotation angle.</descr>          -- <unit>deg</unit>  {noformat}    Please comment on the names of the columns and preferred source of the camera rotation angle. I'll be on vacation for a big part of the next seven days but will try to follow up.   ",NULL
RFC-195,"Split DecamMapper","The current obs_decam attempts to simultaneously satisfy users bringing Community Pipeline (CP) data and users bringing raw data.  It has become clear that this is no longer feasible.  The main reason for this is the calibrations: CP calibs are MEF files containing all CCDs, while calibs produced by the LSST pipeline from raw data are a FITS file for each CCD; these two cannot be reconciled in the mapper.    We (Hsin-Fang, Merlin and I) propose to split the {{DecamMapper}} into two: {{DecamMapper}} to support raw DECam data and {{CpDecamMapper}} to support CP DECam data.  These will likely share a base class for the things that are common between them.    I also wonder if we can simply dump support for the Community Pipeline, which would make this work and continued maintenance easier for this package which doesn't have a real owner.",NULL
RFC-196,"Add pybind11 as an external package","Per RFC-182 a trial conversion from Swig to pybind11 has started. This requires adding pybind11 to the lsst repository as an external package (primarily for CI runs). This RFC proposes to do just that.    Note that pybind11 will not be added to {{lsst_distrib}} (yet) and nobody else should depend on it for now. The package will be removed again if, after the trial, it is decided to stay with Swig.    Further note that pybind11 is a header only C++ library released under a BSD style license. For more information see https://github.com/pybind/pybind11 or DMTN-014.",NULL
RFC-197,"Add updateSourceCoords and updateRefCentroids to afw.table","{{updateSourceCoords}} and {{updateRefCentroids}} are presently static methods on {{lsst.meas.astrom.FitTanSipWcsTask}}. Given a WCS and a catalog, they update the appropriate field. They are generally useful and I propose to make them free functions that live in {{lsst.afw.table}}.    They are simple, but not completely trivial. By making them readily accessible we increase the odds that source catalogs will have correct coord fields and reference catalogs will have correct x,y centroids, e.g. after a WCS is fit. Furthermore, if it we should ever require extra speed we could code these functions in C++ and all users would benefit.",NULL
RFC-198,"Cleanup and unify star selector call signatures","objectSizeStarSelector and secondMomentStarSelector (and their base class) currently take an exposure, from which they extract the detector (could be None) and the maskedImage (if making debugging plots). This suggests that what they really should take are two kwargs: detector and maskedImage, both defaulting to None.    This would simplify testing, as one wouldn't have to have or construct a full Exposure object when it's not really necessary. It also would give us good reason to clean up the selectStars methods to not have all the plotting code wedged inside.    One objection to this might be that secondMomentStarSelector calls {{algorithmsLib.makePsfCandidate}} which also takes an exposure. But PsfCandidate only uses getMaskedImage() and getXY0() (which it could get from the maskedImage).    More broadly speaking: is the intention that the star selectors work on source catalogs, or on images?",NULL
RFC-199,"Add new metadata to ExposureInfo","DM-5502 details metadata that should be added to {{ExposureInfo}}. This RFC gives the details. I am using a short duration because DM-5502 already is basically an RFC.    - Add {{set/getBBox}} methods for the bounding box of the exposure, an {{afw::geom::Box2I}} (not a shared pointer, thus matching exposure.getBBox()). This will be automatically updated when one retrieves a subexposure or calls exposure.setXY0 but will not be safe against changing the xy0 of the underlying masked image (just as one can, but should not, manually change the xy0 of the variance plane or mask in a masked image).  - Add {{set/getField}} methods for data describing the center of the field of view; a {{shared_ptr<Field>}}, where {{Field}} is a new struct with attributes:    - {{exposureId}}: exposure ID, a {{long int}}; this needs a demangler to turn it into a visit ID (see DM-6912)    - {{sky}}: coordinates of the center of the field of view; an {{afw::coord::IcrsCoord}}    - {{observed}}: refracted, topocentric Az/Alt; an {{afw::coord::TopocentricCoord}}    - {{airmass}}, a {{double}}    - {{midDate}}, a {{daf::base::DateTime}}    - {{last}} local apparent sidereal time, an {{afw::geom::Angle}}    - {{positionAngle}}, angle from celestial north to x axis of focal plane, an {{afw::geom::Angle}}    - {{airTemperature}} in C, a {{double}}    - {{airPressure}} in Pascals, a {{double}}    - {{humidity}} as a fraction, a {{double}}    - {{observatory}} longitude, latitude and elevation of telescope; a {{afw::coord::Observatory}}    - note that exposure time will remain in {{Calibrate}}, but {{Field}} documentation will say so  - Add {{getAirmass(pixPos)}} and {{getObserved(pixPos)}} functions that use {{Field}}, {{Wcs}} and {{Detector}}; {{pixPos}} will default to the center of the detector.    One simplified alternative is to store the airmass, position angle and observed position for the center of the CCD, rather than the center of the field, and ditch the functions that compute these values at other points on the CCD. If we do that, the name of that object will need some thought, since it must be made clear that the data is for the center of the CCD, not the center of the field of view. My proposal matches my expectation that most observatories will provide the data for the center of the field of view, and that most users will be content with that, but some algorithms, such as DCR correction, may require those values at the center of the CCD or even at different points across the CCD.    In any case we have to be careful how {{IsrTask}} loads or computes this data, since one can imagine that an observatory might provide airmass, etc. at the center of each CCD instead of at the center of the field.",NULL
RFC-2,"Test RFC ticket - feel free to use to play with",bar,NULL
RFC-20,"C++ naming convention for factory functions and methods that return by value","In porting/rewriting the spherical geometry primitives from Python to C++, I have run into two situations that I think merit a change and an addition to our current coding standard.  The first concerns static factory methods, which the coding standard does not address. I find that using these to create objects is often clearer than using a constructor because they have names. Tracking which combination of arguments means what for a class with many constructors can be quite difficult. Sometimes you also want two constructors that do different things but which have the same argument signature. I propose that we use `from` as a prefix for the latter:  {code}     Angle::fromDegrees(1.0);     Angle::fromRadians(1.0); {code}  For the former, I propose we choose names that describe the special properties of the objects created by that factory method, e.g.:  {code}     Vector3d::orthogonalTo(vector1, vector2);     Vector3d::northFrom(vector); {code}  Note that sometime it makes sense to use a factory method that takes no arguments instead of static const member variables. I think the same principle should apply, e.g.:  {code}      static Vector3d X() { return Vector3d(1, 0, 0); }      static Circle empty() { return Circle(...); } {code}  rather than  {code}      static const Vector3d X;      static const Circle EMPTY; {code}  The reasoning behind this is that the function can be inlined and thus the optimizer may have the opportunity to perform constant folding and other related optimizations when using the return value.  The second issue is that the standard says that method names should describe the action being performed. I think this is OK for accessors and mutators:  {code}      Box b;      LonLat c = b.getCenter();      b.dilateBy(Angle(0.1)); {code}  However, in an attempt to at least make it possible to make my classes immutable, I often found myself pairing a mutator with a corresponding const method that would return a mutated copy of the callee. In that case, the existing rule leads to names that feel verbose and clunky to me:  {code}     b.getCopyDilatedBy(Angle(0.1)) {code}  I propose we allow the use of past tense verbs in the naming of such methods:  {code}     Box b;     b.dilateBy(a); // modifies b     Box c = b.dilatedBy(a); // returns a modified copy of b {code}  as I find it leads to more concise and readable method invocations. Consider:  {code}     Vector3d v = c.expandedTo(v).getCenter().rotatedAround(axis, angle); {code}  versus something like:  {code}     Vector3d v = c.makeNewExpandedTo(v).getCenter().makeNewRotatedAround(axis, angle); {code} ",NULL
RFC-200,"Make TAN_PIXELS cameraGeom coordinate be with respect to the center of the focal plane","The TAN_PIXELS cameraGeom coordinate system is presently defined to match PIXELS at the center of each CCD. This was a mistake on my part and I'd like to correct it. TAN_PIXELS was intended to model the effects of optical distortion, and to do so in a straightforward way TAN_PIXELS should match PIXELS at the optical axis (origin of the PUPIL frame).    Furthermore, TAN_PIXELS does not presently support rectangular pixels correctly (technically this is a misfeature, not a bug; it uses a mean pixel scale, and is documented as such). I propose to fix that, as well. With the proposed new definition of TAN_PIXELS the fix is trivial. Note that we do not yet have any cameras with rectangular pixels, so fixing this will not make any difference to current cameras.    One of the primary uses I intended for TAN_PIXELS was to provide a proper WCS given an initial TAN WCS. However, this is presently broken (DM-6529) because of the incorrect definition of TAN_PIXELS. There are other ways to fix DM-6529, but changing the definition of TAN_PIXELS as proposed is a clean, simple solution, and the results will be easier to understand.    An implementation is available on afw tickets/DM-2800 in case you want to try it out to see what difference it makes.    ",NULL
RFC-201,"LDM-PMT needs TCT approval","Dear TCT Members,    The ProjMgmt&Tools-WG would like you to review and approve the LDM-PMT document. The latest version can be found at:    https://github.com/lsst/LDM-PMT/blob/integration/index.rst    This is also rendered at https://ldm-472.lsst.io/v/integration/index.html    Note that we have a related tech note https://dmtn-020.lsst.io/    My understanding is that we will keep LDM-PMT under TCT change control, and the tech note will stay non-controlled.     (If you can do that before JDR review, I could say at the review that it is approved)",NULL
RFC-202,"Builds are optimised by default","By default, our builds are not optimised ({{-O0}}), which requires everyone who doesn't want to wait until the heat death of the universe to set {{SCONSFLAGS=""opt=3""}}, but other packages that are built with scons may not recognise this.  This default is also contrary to the standard practise for open-source software, which is that by default builds are optimised.  I propose to change the default optimisation level to {{opt=3}} from the current {{opt=0}}.  It's a very simple change in sconsUtils:    {code}  --- a/python/lsst/sconsUtils/state.py  +++ b/python/lsst/sconsUtils/state.py  @@ -98,7 +98,7 @@ def _initVariables():           SCons.Script.BoolVariable('force', 'Set to force possibly dangerous behaviours', False),           ('optfile', 'Specify a file to read default options from', None),           ('prefix', 'Specify the install destination', None),  -        SCons.Script.EnumVariable('opt', 'Set the optimisation level', 0,  +        SCons.Script.EnumVariable('opt', 'Set the optimisation level', 3,                                     allowed_values=('0', '1', '2', '3')),           SCons.Script.EnumVariable('profile', 'Compile/link for profiler', 0,                                     allowed_values=('0', '1', 'pg', 'gcov')),  {code}",NULL
RFC-203,"Use lsst::log in pipeline tasks and deprecate pex_logging","This is a continuation of RFC-29, and narrower in scope.     I would like to switch from using {{pex.logging}} to {{lsst.log}} in {{pipe_base}}. After that, all pipeline tasks ({{task}} or {{cmdLineTask}}) can switch to use {{lsst.log}}.  With lsst::log (via log4cxx implementation) there are 6 logging levels (trace, debug, info, warn, error, fatal).  Arbitrary integers will no longer be accepted as levels. In this framework it’s encouraged to use loggers with distinct, hierarchical names instead of custom levels.     In Python, I plan to use the recently added lsst.log Python interface, not through Python logging and LogHandler. For example:  {code:java}    logger = lsst.log.Log.getLogger(“a.named.logger”)    logger.setLevel(Log.INFO)    logger.trace(""This is TRACE"")    logger.debug(""This is DEBUG"")    logger.info(""This is INFO"")    logger.warn(""This is WARN"")    logger.error(""This is ERROR"")    logger.fatal(""Format %d %g %s"", 3, 2.71828, ""foo"")  {code}    For most tasks, little modifications are needed in Python, as long as the task’s own {{log}} attribute is used for logging. The “logdebug” method in pex_logging will be replaced by “debug”, and most will be format changes.     For CmdLineTasks, the namespace logging level control from the command line {{-- loglevel}} is preserved.  The command line option {{-- logdest}} will be removed; the file destination is instead set through log4cxx configuration. As I understand, many people use this option for its default formatter with dataId attached with each log record. After switching to {{lsst::log}}, dataId can still be included in the logs. I’m thinking two possibilities: an command line option to include the dataId in log records, or an command line option to more easily supply user’s own log4cxx configuration file in which format can be set.       In C++, macros are used, e.g.   {code:java}      LOG_LOGGER logger = LOG_GET(“a.named.logger“);  {code}    For logging one can use either one of the two macro families, the sprintf-based interface:   {code:java}      LOGL_INFO(logger, ""This is INFO"");      LOGL_WARN(“a.named.logger“, ""This is WARN"");      LOGL_FATAL(logger, ""This is FATAL %d %.4f %s"", 65, 42.123, ""logging"");  {code}    and the iostream-based interface:  {code:java}      LOGLS_INFO(logger, ""Format "" << 3 << "" "" << 2.71828 << "" foo c++"");      LOGLS_WARN(“a.named.logger“, ""This warning is” << “scary”);  {code}      -As I understand pex.logging {{Trace}}/{{Debug}} classes have been deprecated (RFC-86), despite still used in the codebase.  Can I use log levels combined with named loggers to replace {{Trace}} with integer 1-11 (TRACE1-TRACE11)? What mapping between Trace and log level may be accepted?  How about <3: INFO, 4-7: DEBUG, >8: TRACE? Or do all of them map to DEBUG or TRACE?-   The fine-level verbosity feature of pex.logging {{Trace}}/{{Debug}} TRACEn is supported.     The transition plan is to do this in stages: (1) individual logging not chained to CmdLineTask (e.g. daf_butlerUtils, or hard-coded and non-configurable logging), as they are not configurable through the command line task interface anyway.  (2) Python-side logging for tasks/CmdLineTasks plus log in C ++.   (3) Debug/Trace in C ++. They are typically run with unit tests and each package can be handled separately.  ",NULL
RFC-204,"Move all config and tract/patch dataset definitions to base CameraMapper","We [now|https://community.lsst.org/t/centrally-defined-butler-datasets/841] have the option to define mapper datasets that are the same for all cameras in a common location (the config files for the base class CameraMapper).  I propose that we go ahead and do this for all common datasets - essentially anything that doesn't depend on the keys of the raw data ID - now.  This will massively reduce code duplication in the mapper files, and reduce our dependence on paf files (since we'll be moving to the new YAML-ish format).    There are two possible hangups:   - We probably need to resolve DM-6858 first (we currently compare the list of centrally-defined datasets against a list hard-coded into the test).   - *This will change the location of some data products for at least some cameras.*  I think this is a good thing in the long run, as there's no reason for different cameras to put common datasets in different locations, but this opinion may not be unanimous, and it may cause some backwards compatibility problems.  If these are important, I suggest we define additional backwards compability mappers that do not move these datasets for a transitional period.  The good news is that a _preliminary_ look suggests that most of our mappers already define things consistently; obs_sdss may be the only one that uses different conventions for coadd datasets.    I do not have anyone already signed up to do this work (I'm hoping to get by on the kindness of TCAMs), but this really shouldn't be more than a couple of days - there are a lot of lines to move and reformat, but it's dead simple.    I do believe doing this sooner rather than later (and in particular not waiting on the dynamic dataset definition feature) will make Science Pipelines developers' lives easier, and I think it may be helpful to [~npease] in adding support for compound datasets as well.",NULL
RFC-205,"Disallow ""if False:"" for ""block comments"" in python","There seems to be an unwritten convention in LSST to use {{if False:}} to ""comment out"" blocks of python code. I had a community post about why this is bad practice in [December 2015|https://community.lsst.org/t/use-of-if-true-if-false-in-python-and-if-0-in-c/455], but didn't get much discussion at the time. Since it's come up again in some reviews, I'd like to address it head on here. (note: The same arguments apply to {{if True:}}.)    The argument I've been given about this is that it's a ""pythonification of a C++ standard,"" referencing [this C++ standards page|https://dev.lsstcorp.org/trac/wiki/C%2B%2BStandard/Statements#a5-38.NocodeSHOULDbecommentedoutuseapreprocessordirectivetoincludeorinhibitcodeuse]. Using {{#if 0}} (or equivalent) in C++ makes _some_ sense in light of the C pre-processor. In python, {{if debugFlag is True:}} is useful for code one wants to turn on/off with a high level flag. However, {{if False:}} it is a very poor choice for python for the following reasons:    # the code is still run by the interpreter.  # it doesn't look like a comment: editors show it as valid code.  # when reading the code, you have to visually parse the {{if}} block to realize it is unused.  # the code is now at a different indentation level, potentially causing problems if the code is restored, and potentially messing up auto-indent of subsequent {{else:}} blocks, if there are {{if}} statements inside the {{if False}}.  # It's no easier than commenting with {{#}}: editors can trivially prepend {{# }} to a region of code with a single keystroke.  # it's not pythonic: it is rarely used by python developers who were not first C/C++ developers, and it is not a generally accepted community ""block comment"" method (pre-pending # or triple-quotes are the standard).    Our [python coding standards|https://developer.lsst.io/coding/python_style_guide.html#comments] say nothing about disallowing commented-out code blocks in production code. I generally agree that that is good practice-I would suggest we discourage but not disallow it-but using {{if False}} is much worse than commenting out the code blocks with #, for the above reasons.    Our python coding standards say nothing about any of this. I believe our standards should explicitly disallow {{if True:/if False:}} in all cases, and discourage commented-out code, with exceptions allowed for if the code includes a comment with a clear timeline for when that block would be either removed or integrated back (e.g. Jira ticket number).",NULL
RFC-206,"Reconstitute TCT","DM management proposes that the TCT consist of the senior DM strategic staff reporting to the Subsystem Lead:  * System Architect (chair) - [~ktl]  * Project Manager - [~jbecla]  * Project Scientist - [~zivezic]  * Data Processing System Lead - [~petravick]  * Pipelines Scientist - [~rhl]    The TCT will make recommendations to the Subsystem Lead about change-controlled documents; it will not make final decisions about those documents.  The TCT will also handle appeals of RFCs as well as ""languishing"" RFCs.    [LDM-294|http://ls.st/LDM-294], [Confluence|https://confluence.lsstcorp.org/display/DM/Technical+Control+Team], and the [Discussion and Decision-Making Process|https://developer.lsst.io/processes/decision_process.html] need to be updated to reflect this.    Proposed wording for the Membership section in the Confluence page is:  {quote}  Chaired by the DM System Architect  Members include the DM Project Manager, DM Project Scientist, DM Data Processing System Lead, and DM Pipelines Scientist  {quote}  (note: the statement about quorum will be removed)    Proposed wording for LDM-294 (which is identical with the Confluence page) is the same.  Other changes to bring LDM-294 up to date are outside the scope of this RFC.    Proposed wording for the process document is in https://github.com/lsst-dm/dm_dev_guide/pull/46",NULL
RFC-207,"Rename Afterburner plugins to CatalogCalculation plugins","A new plugin system type was introduced to handle plugins which only operated on a catalog, after all ""measurement"" plugins had been successfully run. At the time the term afterburner was used as a way to describe a system that was optionally run after other measurements. However in the processing pipeline there may be many other steps after a catalog has been created and measurements have been made. As such I propose we rename the catalog level afterburner plugin system to CatalogCalculation plugin system to better clarify its purpose and to leave the term 'afterburner' free to be used in the context of something that may be run outside the context of normal processing flow.",NULL
RFC-208,"Place the PDAC Data Access Policy under change control","We are proposing to put under change control the PDAC Data Access Policy. The policy is currently available at:     https://confluence.lsstcorp.org/display/DM/Data+Access+Policy+for+the+Data+Management+Prototype+DAC    Sr. LSST Management has already commented, reviewed, and agreed with the general direction of the document. Any final comments on the PDAC data access policy are requested here, prior to final adoption by DM.",NULL
RFC-209,"Explicitly default or delete automatically-generated members in C++","C++11 raises a number of stylistic issues our standards currently don't address, and I think I've just made up my mind about the first one I think we should formalize.  I hereby propose adding the following to our coding standards:  {quote}  All new C++ classes SHOULD explicitly default or delete all copy and move constructors and assignment operators that may be synthesized by the compiler.  In practice, that means the following four signatures (shown here defaulted):  {code:c++}  class Foo {  public:      ...      Foo(Foo const &) = default;      Foo(Foo &&) = default;      Foo & operator=(Foo const &) = default;      Foo & operator=(Foo &&) = default;      ...  };  {code}  Defining these constructors forces us to consider their behavior and judge whether the default behavior is in fact desired when implementing a new class.  It also makes compilation errors appear sooner (rather than delaying them until a synthesized member is first needed).    When Python bindings are generated with Swig, this should also ensure that a wrapper for the copy constructor is generated, and it serves as a reminder that one should probably be included in wrappers generated with pybind11.    When these members cannot be defaulted or deleted, a comment explaining why SHOULD accompany the implementation.    Extremely small classes (e.g. dumb structs) or private/anonymous classes used only by small amount of code are expected to be the only exceptions to this rule.  {quote}    At a much lower priority, I also think we should apply this rule to our most ubiquitous existing classes.",NULL
RFC-21,"Define DM branch policy on GitHub","There is a [DM Branching Policy page|https://confluence.lsstcorp.org/display/LDMDG/DM+Branching+Policy] on Confluence, but it says that it is ancient.  Some policies around git branches are listed on the [Using Git for LSST Development page|https://confluence.lsstcorp.org/display/LDMDG/Using+Git+for+LSST+Development], but they're not very specific.  A _de facto_ policy was created by gitolite push restrictions that limited branch names and, to some extent, usage.  These restrictions no longer apply on GitHub.  Some developers have felt a tension between maintaining their time-ordered development history and a cleaned-up, reviewable, one-commit-per-feature history within a single branch.  I propose the following to replace the appropriate sections of the above Confluence pages: * *master* remains the main integration branch and must always be deployable.  No development occurs directly on master in any _lsst_ (DM team) or _lsst-dm_ repository, with the exceptions of documentation and very small fixes. * The *next* branch and release branches are created as necessary for integration. * *tickets/DM-NNNN* branches are the ""official"" history of development.  All reviews happen on these branches, and all merges to master, next, or a release branch happen from them.  These branches should be rebased with cleaned-up (squashed/separated, correctly-commented) history. * *u/_username_/_anything_* branches are available for developers to do whatever they want.  They can start investigative work on a feature without filing a JIRA ticket.  They can keep as-developed history separate from the official history.  Merges from such branches should not occur, but a tickets/DM-NNNN branch can be created at the tip of such a branch in order to merge it. * Pull requests from other repos including personal repos are the equivalent of ticket branches.  They need to have a DM-NNNN JIRA issue identifier and follow all the policies of in-repo ticket branches. ",NULL
RFC-210,"Cost updates to Site Specific Infrastructure Estimation (LDM-144) and Explanation (LDM-143)","This RFC is to solicit feedback and approval for changes to LDM-143 and LDM-144 such that these latest versions can become preferred versions. This is part of a 6-month, reoccurring update to these documents.    http://ls.st/ldm-144*   http://ls.st/ldm-143*    General   - Pricing documents are now versioned instead of adding new documents.    - Pricing document versions are indicated on the spreadsheet    InputTechPredictions   - (Q7) Cost per CPU updated (18091)   - (R7) Reverted Compute Efficiency to 25%. Expectation is that software stack will take advantage of AVX-like technologies   - (AG) Updated per-module memory price   - (AO,AR) Updated consumer sata with choice that has best WBS grand total   - (AT,AW) Updated enterprise sata with choice that has best WBS grand total   - (BC) Updated Disk Controller Pricing (19215)   - (BH,BI) Updated capacity/NSD to reflect recent GSS purchase      InputOther    - (O7,P7) Updated UofI power and cooling costs (18084)            LDM-143   - 10.11.1 - Changed database nodes per rack to 16   - Fixed typos throughout   ",NULL
RFC-22,"remove old DB ingest capabilities prior to completion of new ones","We're approaching the end of the conversion to the new measurement framework, and the last feature that needs to be implemented is database ingest.  We're already working on that, but we have other work that's being blocked by the fact that we can't remove the old measurement framework until it's done.  So I'm proposing here that we go ahead and remove the old measurement framework (and its associated old ingest scripts) now, resulting in a brief period in which we wouldn't have any ingest scripts at all.  My understanding is that currently no one is using the old ingest scripts, unless they're doing so through an older tagged release, which wouldn't be affected, but I'd like to have that confirmed here, especially by the [~jbecla] and [~boutigny].  If this RFC is approved, we'd then proceed immediately with removing the old measurement framework in {{meas_algorithms}}, and removing both {{ap}} and {{datarel}} from the stack entirely, as currently I don't believe they provide any functionality besides ingest.  At this point, I'm just proposing we remove them from buildbot and that the repos will be migrated to wherever completely dead repos go; the discussion on where exactly that should be should happen elsewhere.",NULL
RFC-23,"Allow DESC to join the DM Hipchat","At the SLAC DESC meeting there was interest in allowing DESC members to join DM's hipchat.  This was initiated by Dominique Boutigny and Chris ""Unbaptised"" Walter, who explained how useful it has been.  There was some push back from DMers who expected to be swamped by DESCies, but I don't think this is likely (e.g. I think that only about 10 have signed up to join the DESC-only Hipchat).  There was also concern that it would lock us into hipchat, which isn't a great product.  Atlassian claims that: {quote} The ability to be in multiple HipChat teams is now officially coming.  https://blog.hipchat.com/2015/02/18/more-updates-more-teams-and-more-on-the-way/ {quote} but who knows when (or if) it'll work.  I'd like to allow the DESC in now because:  1. I don't think it'll cause any problems  2. It's what DESC wants.  The fact that it may not actually be the right choice is neither here nor there;  Mario and I think that it's very important to integrate them with DM (we're haunted by the them-and-us aspects of PanSTARRS)  3. We'll reserve the right to move to a better technical solution if we find one",NULL
RFC-24,"Make the System Architect the BDFL for Coding Standards","This RFC proposes to make the System Architect responsible for the Coding Standards document, rather than the entire TCT committee. If adopted, this RFC will designate the System Architect as the owner of the coding standards document, with the responsibility to decide on acceptance or rejection of changes to coding standards documents.  Rationale: * Coding standards are naturally in the purview of the system architect, as the person responsible for the technical integrity of the system. * It's important for coding standards to reflect a coherent set of rules exposing robust and consistent underlying logic. This is best achieved by having one person responsible for them. * It removes unnecessary micromanagement and bureaucracy (for example, it's unlikely that the Project Manager or Project Scientist -- both members of the TCT -- will (or should) track development at this level of detail; therefore, decisions shouldn't be blocked on them).",NULL
RFC-25,"Increase MaskPixel size from 16 to 32 bits","We've found on HSC that we do need more than 16 mask planes, and hence it's almost certain that LSST will as well.  We might as well make that change sooner than later.  This will also make it a bit more important that we get FITS compression working (there is code to do this, but we don't know how recently we've used it). ",NULL
RFC-26,"API Change for IsrTask (Backwards-compatible)","During the S14 efforts to document the important {{PipeTasks}}, it was pointed out that the only way to use some of the low level tasks is to supply a {{ButlerDataRef}}. This discourages reuse and requires code examples/tests to create a fake {{ButlerDataRef}}. This RFC proposes to add a non-blob interface to {{IsrTask}} specficially (epic: DM-1113). Currently, the {{commandlineTask}} {{processCcd}} constructs a DataRef from the commandline arguments. {{processCcd.run()}} calls {{isrTask.run(dataRef)}} which retrieves the calibration data and removes the instrument signature.    Proposal: Keep the responsibility of retrieving the calibration data in {{IsrTask}} (and more generally always give lower-level tasks (e.g. {{FringeTask}}) responsibility of reading necessary data from the butler).  Split {{run(sensorRef)}} into 3 methods:   * {{run(sensorRef)}} which wraps a two new methods   * {{readDetrendData(dataRef)}}} and   * {{doIsr(ccdExposure, biasExposure, darkExposure, flatExposure, defects, fringes)}}.    See {{ip_isr}} branch {{/u/yusra/DM-1299}} for a prototype or pseudocode example below.    A benefit to having a separate method to read the data is the hierarchical symmetry with lower-level subtasks of {{IsrTask}} such as {{FringeTask}}. This proposal will also require an analogous refactoring of {{FringeTask}}.    A complication is that {{IsrTask}} is the most camera-specific task in the stack and requires a lot of input. {{readDetrendData}} and {{doIsr}} (if not {{run}} too) will likely need to be overridden in {{obs_*}}.  In {{obs_lsstSim}} for example, {{run}} could also be responsible for combining snaps.    This change should not break any client code, but will require an analogous refactoring of the obs_* IsrTasks, and a clean up of examples and unit tests that create fake dataRefs. Also note the choice of method names {{run}}/{{doIsr}} instead of {{runDataRef}}/{{run}}. Rational was that {{run}} will still be the primary method called and this maintains backwards compatibility.    *Pseudocode example*  {code}  class IsrTask:      ...      def readDetrendData(self, sensorRef):          biasExposure = self.getDetrend(sensorRef, ""bias"")          ...          if self.config.doFringe:              fringes = self.fringe.readFringes(sensorRef, assembler=self.assembleCcd)          ...          return dict of calibration products {biasExposure, ... fringes}        def doIsr(self, ccdExposure, biasExposure=None, darkExposure=None,  flatExposure=None,                defects=None, fringes=None):          #do processing including subtasks, e.g.          if self.config.doBias:              self.biasCorrection(exp, biasExp)          ...          if self.config.doFringe:              self.fringe.removeFringe(ccdExposure, fringeData, assembler=None)          ...        def run(self, sensorRef):          ccdExposure = sensorRef.get('raw')          detrendData = self.readDetrendData(sensorRef)          ccdExposure = self.doIsr(ccdExposure, **detrendData).exposure          ...    class FringeTask:      def readFringes(self, dataRef, assembler=None):          ...      def removeFringe(self, exposure, fringes, assembler=None):          ...      def run(self, exposure, dataRef, assembler=None):          ...          fringes = self.readFringes(dataRef, assembler=assembler)          self.removeFringe(exposure, fringes, assembler=assembler)  {code}  ",NULL
RFC-27,"Declassify key DM (and LSST) documents","Prompted by Andy Becker's request for the paper on background matching he's trying to finish with Robert & co., I ask that we make publicly accessible from Docushare (and link from a web page) all the project requirements documents listed in the attached PDF.  These include the SRD, LSR, OSS, DMSR, DPDD, the DM architecture documents (entire system, middleware, infrastructure and apps), and the DM sizing model and related documents.  Note: I just noticed that the DPDD is not mentioned by name on that slide (LSE-163). It should be made public as well.",NULL
RFC-28,"Add ""nearly equal"" comparison functions afw classes Angle, Coord, Point, WCS, etc.","I propose to add a set of functions to afw that test for approximate equality for a number of our classes, specifically: Angle, Coord, Extent, Point, Image and its kin and WCS. The primary intent is to support unit testing, but I propose to write the functions to be more general, specifically: - to return a boolean instead of raise an exception or a descriptive string (even though unit tests benefit from an exception containing a descriptive string for more complex objects) - put them in the same namespace as the object being tested  My main justification is that in some cases it is easy to get the test wrong. For instance in comparing angles people may forget about wrap. In comparing Coord it is common to compare the angles separately (even though the equatorial angle should be scaled by cos(polar angle), whereas angular separation is usually what is wanted and is robust at the poles. In other cases (comparing WCS or images) the code is sufficiently complex that users may choose to forgo the comparison or use an abbreviated and less satisfactory comparison.  Specific proposals: - wcsNearlyEqual(wcs0, wcs1, bbox, maxPixelErr=0.001, maxSkyErr=0.001 arcsec, nx=10, ny=10):   compare two WCS pixelToSky and skyToPixel over a grid in pixel space   ignore type (e.g. TAN-SIP WCS and DistortedTanWcs may be almost equal) - coordsNearlyEqual(coord0, coord1, maxSkyErr=0.001 arcsec)   compare two coords by converting one to the type of the other and then comparing the angular separation - extentsNearlyEqual(extent1, extent2, maxDiff=0.001)   compare two extents by measuring the hypotenuse of the difference, ignore type but require both have the same number of components - pointsNearlyEqual: same as extentNearlyEqual - anglesNearlyEqual(angle0, angle1, maxDiff=0.001 arcsec)   subtract, wrap the result into -pi, pi, take absolute value and compare - imagesNearlyEqual(image0, image1, maxDiff=0.001) - maskedImagesNearlyEqual(mi0, mi1, maxImageDiff=0.001, maxVarDiff=0.001, mask=0xff, ignoreMask=0)   compare a masked image at every pixel whose mask does not contain bits set in ignoreMask. For comparing the mask plane only pay attention to bits in mask. Note that ignoreMask trumps mask.  An attractive alternative is to implement this as an overloaded function ""nearlyEqual"". This requires writing the functions in C++, which is is not necessary in the main proposal but might be useful and would be easy. My main concern is that the arguments are somewhat different for the different versions (especially WCS and MaskedImage), which might confuse users and make documentation more difficult. On the other hand it makes the function name more predictable (and avoids the issue of not having a sensible plural for Wcs).  I considered but rejected these alternatives: - Implement as AClass.nearlyEqual(other). This makes the names easier to predict but I feel it clutters the API with methods that don't need internal state and whose signature and form are not self-evident (e.g. we may find we want variants). - Design explicitly for unit tests by making these asserts. That allows useful text in the returned result, but makes the functions less general. - Return a descriptive string instead of a boolean. This is what I did for the current image comparison functions in lsst.afw.image.testUtils, but it is not very general.  I propose to do the work myself. I have a WCS comparison function in review right now, but am delaying finishing that until we have a more general design approved.",NULL
RFC-29,"Standardize logging and remove pex_logging","The Data Access team has concerns about how we're doing logging.  They would like to use Python's built-in {{logging}} library for most standalone, pure-Python utilities.  In addition, we're currently using {{pex_logging}} within C++ and Python code throughout much of the LSST Stack.  This custom code should be replaced.  {{lsst::log}} has been developed as a thin layer on top of {{log4cxx}} that will be more flexible and maintainable as it is based on a standard external package.  We need to transition the Stack to this interface.  Among the Data Access team's concerns that point in favor of using Python logging: * Desire to minimize dependencies on LSST Stack and {{log4cxx}} * Simplicity of usage and default configuration * Use with third-party packages (that typically use Python logging) * Familiarity  I propose to decree the following policy, although I'm not going to do much of the actual coding work: * C++ code throughout the Stack and Qserv uses {{lsst::log}}.  (If anything is missing or poorly performing in that package, it should be fixed.) * All Python code developed by LSST DM uses Python {{logging}}. * Python apps (like {{CmdLineTask}}) that use packages that contain C++ code explicitly direct Python {{logging}} to use {{lsst.log}} in code (not via a configuration file).  (I'm still worried that this will require configuration of individual logging components in multiple places depending on whether they're C++ or Python.) * A standard default Python {{logging}} message format and a standard default {{lsst::log}} message format that are compatible with each other will be defined in a separate RFC.  This includes things like always logging timestamps in UTC.  (Note for pedants: I think these would be TAI if and only if we take the step of shifting all production machines' system times to TAI.  Even then, I would expect that non-production machines like developer's laptops will never produce TAI.) * All current code must be ported from {{pex_logging}} to the above by the end of Winter 2016 (Summer 2015?). * The Python {{logging}} configuration file that is currently installed and used by default by Qserv will be removed.",NULL
RFC-3,"RFCs should be disseminated widely","RFCs posted as issues in JIRA should be automatically mailed to the dm-devel mailing list with a Subject that makes them easy to filter and locate and also should be posted in a HipChat room.  Frossie will likely have to do much of this work.  Discuss in Software Development room.",NULL
RFC-30,"convert Footprint PeakList to Catalog","As we move towards a multi-band deblender, we've found that we need to store more information in Peaks than the current objects support (in particular, we need flags that indicate which band(s) a Peak originated in).  Rather than add these new fields to the existing Peak struct, we'd like to convert Peak to a custom afw::table Record class, allowing the fields contained in a Peak to be something configurable at runtime.  We've already made this change on the HSC side, and encountered no major problems with it, and if the RFc is approved as-is we will implement the change just by cherry-picking those commits.  This will break downstream code, but in a way that is trivial to correct, and we will make the necessary changes to all affected packages currently included in buildbot.  Aside from that disruption, the main adverse affect will be some unit test code that will get a bit more verbose, because it will become more difficult to create and manipulate Peaks outside of a Footprint.  It's also worth noting that a much larger scale rewrite of the Footprint class and its friends will be taking place later in S15 (epic DM-1769), and this can be viewed as a small piece of that.  While the long-term interface to Peaks after DM-1769 may not be identical to what we have immediately after this RFC is implemented (on DM-1943), it will be closer to that than what we have now.",NULL
RFC-31,"A standard header file for physical constants","I propose that we add a standard SWIG-wrapped C++ header file that contains physical constants to our stack, in the utils package. I propose the following as the initial contents:  double const FWHM_PER_SIGMA = 2.0*std::sqrt(2.0*std::log(2.0));  // for a Gaussian distribution double const PI = std::atan(1.0)*4.0; double const HRS_PER_DEG = 24.0 / 360.0;    // hours per degree double const RAD_PER_DEG = PI / 180.0;        // radians per degree double const KM_PER_AU = 149597871.0;         // kilometers per astr. unit (Astr. Almanac) double const AU_PER_PARSEC = 206264.8062470964;   // astronomical units per parsec double const ARCSEC_PER_DEG = 3600.0;         // arcseconds per degree double const SEC_PER_DAY = 24.0 * 3600.0;     // seconds per day double const DAY_PER_YEAR = 365.25;          // days per TT (or TAI...)  year double const DEGK_DEGC = 273.15;            // deg. Kelvin - deg. C (Allen, exact) double const MJDJ2000 = 51544.5;            // Modified Julian Date at epoch J2000.0 noon (days) double const JY_PER_MABFLUX = 3631.0;     // Mab = -2.5 log10 (fluxJy/JanskysPerABFlux) ",NULL
RFC-32,"Retire afw_extensions_rgb and integrate functionality into afw","There's code in afw_extensions_rgb to make true-colour images (using the Lupton et al. algorithm (http://arxiv.org/abs/astro-ph/0312483) by default, but easily extensible to do it the wrong way too).   It's in an extension module as it depends on tiff/png/jpeg libraries, and they are painful dependencies.  I recently realised that we can do all of this in numpy (except possibly the saturated star processing, but that's pure image processing with no RGB library dependencies).   I therefore propose moving the RGB code to afw. ",NULL
RFC-33,"Policy on moving files between git repos","AFAIK, we currently have no policy for how we should handle git history when we move files between repositories.  I propose the following:  - Any files should be simply moved without any attempt to transfer the commits that built them.  - Files should be added to the destination repository on a single commit (multiple files may be added on one commit), without being modified in any way from their state in the last commit in the original repository - modifications to work with the new repository should take place on subsequent commits, even if this means the destination package is not buildable or fails tests in the interim.  The transfer commit message should have the form ""Transfer from <orig-pkg> at <orig-sha1>"", indicating the last commit in the original repository where the files were present.  - Files should be removed from the original repository on a single commit that removes the exact same files that were added in the corresponding commit in the destination repository.  This should have a commit message of the form ""Transfer to <dest-pkg> at <dest-sha1>"", referencing the commit where the code is added to the destination repository.  - These commit message SHA1s must be updated if the destination is a ticket branch that is rebased before being merged to master.  - Additional commit message content may be present (and usually should) after the ""Transfer..."" lines.  The main motivation for this proposal is that I think the obvious alternative - using {{git filter-branch}} to prune the history of the original repo down to just those commits that reference the files to be moved, then merging those - is too difficult to be worth our time and produces results that are problematic anyway.  [~pgee] and I (but especially Perry) recently put a lot of effort into investigating these procedures for DM-420, and here's what we found:  - {{git filter-branch}} is very tricky to use, and it runs slowly on large repos.  While just copying the files takes minutes, using {{filter-branch}} to handle even a fairly simple transfer will typically take an expert hours, and a newbie days.  (StackOverflow is helpful as usual here, but it's worth noting that there are very different answers for only slightly different situations, and that this subject frequently produces highly-ranked questions with only poorly-ranked answers).  - {{git filter-branch}} does not remove empty merge commits, meaning the ""pruned"" history is actually anything but - it's something like 95% empty merge commits and 5% actual changes.  So far, the best solution we've come up with for pruning those out is to do an interactive rebase on the output of {{filter-branch}}, which requires manually re-resolving any conflicts that occurred on merges anywhere in the history.  Of course, if we remove the merge commits, we also remove the links to JIRA issue numbers we've otherwise been careful to preserve.  - Our git commit discipline was so poor in even the recent past that most of those old commits aren't worth the effort, and it only gets worse as the history extends back to the svn days.  - By its very nature, {{filter-branch}} does quite a bit of violence to the original commits.  That makes them hard to interpret at best, and at worst destroys the very history we're trying to preserve.  I'm all for having users rewrite their own recent git history before merging to master, but automatically rewriting ancient history seems to defeat the purpose of preserving that history.  Instead, my proposal for commit discipline in transfers of code should provide a reliable workaround for the biggest drawback of *not* transferring the history: the fact that {{git blame}} will not longer give sensible results directly when applied to a piece of code that has been transferred.  When a line of code we're interested is blamed on a transfer commit, we'll have to clone the original repo, check out the commit mentioned in the transfer commit message, and re-run {{git blame}}.  That's unfortunate, but I think it's better than any alternative I could think of.",NULL
RFC-34,"Standardization of Namespace Aliases","We've long had a problem with using namespace aliases inconsistently, and a recent discussion on the review for DM-2305 emphasized the need to actually resolve this rather than just writing different files differently.  So, as the loudest member of the ""no aliases"" camp, here's a proposal that still involves some aliases that I could get behind.  The tl;dr version is:  - No file-scope aliases in C++.  - File-scope aliases in Python like ""afw_geom"" rather than ""afwGeom"".  - Tiny aliases permitted in limited scopes.  - Use in new files immediately; old files will be migrated when we do the package reorg of DM-2003 (later in S15).  Here's the detailed proposal:  h3. C++ Namespace Aliases  Namespace aliases shall not be used at file or namespace scope in C++.  Instead, as many namespace components as needed to disambiguate a name should be used, within the appropriate namespace scope for the current package.  More namespace components can be used than are strictly necessary to satisfy the compiler; the goal should be to provide clarity to human readers.  The {{lsst::}} namespace component should not be included except in the rare case where the next namespace component is potentially ambiguous (i.e. both {{::lsst::foo}} and {{::foo}} are present), or in a header where it is needed to unconfuse Swig.  For example, here is how we could refer to various classes from within meas_algorithms: {code:c++|hide-linenum} namespace lsst { namespace meas { namespace algorithms {  // start with afw because it's necessary, skip lsst:: afw::geom::Point2D p;    // no need to say ""meas::"", because there's only one deblender, but // ""meas::deblender"" would also be fine. deblender::BaselineUtils b;    // using ""meas::"" is recommended here because there are other packages // ending in ""base"", even though it's clear to the compiler without it meas::base::SdssShapeAlgorithm s;   }}} {code}  In addition, in contexts where a namespace component conflicts with a frequently-used variable name, additional prefix components should be used (i.e. prefer ""afw::image"" to ""image"", because the latter is a very common variable name).  In function or class scopes, any namespace alias may be used (even much more abbreviated aliases), but these should be limited to cases where the alias is used many times within the function, and usually a typedef or full using declaration is preferred in these contexts.  h3. Python Module Aliases and Use of {{import}}  Python module aliases used at module scope must be formed from the last N components of their fully-qualified name, joined by underscores instead of periods, with no change in capitalization.  For example, {{lsst.afw.geom.ellipses}} could be aliased to {{afw_geom_ellipses}}, {{geom_ellipses}}, or just {{ellipses}}, and which of these is chosen is left to the judgement of individual developers.  However, the same guidelines discussed for use of C++ namespace components should be applied when determining which namespace components to use in Python aliases: use as many components as needed to avoid reader ambiguity.  When only the last component is used, the {{import <package>.<module> as <module>}} form should be used, rather than {{from <package> import <module>}}, to improve readability next to other nearby import statements.  The {{from <package>.<module> import <names>}} form is also permitted, but the {{from <package>.<module> import *}} form is not, except in cases where names are explicitly being lifted to package or subpackage scope in an {{\_\_init\_\_.py}}.  h3. Motivation  While this proposal still leaves some room for developer preference, it's much less room than the current situation, and in particular I believe it's enough to be able to reliably search for all possible forms of a namespace with a complex regex, and (perhaps more importantly) search for *nearly* all possible forms of a namespace with a very simple search.  Ideally, we'd be able to say {{from lsst import afw.geom}} instead of {{import lsst.afw.geom as afw_geom}}, but alas, we cannot - so I think the latter is the best we can hope for, and *much* better than the current {{import lsst.afw.geom as afwGeom}}  That's because eliminates having the same name capitalized differently in different contexts, and it minimizes having the same name punctuated differently in different contexts, both of which complicate searching - and, to the LSST-uninitiated, code reading - unnecessarily.  It also eliminates capital letters from our module aliases, bringing them in line with the PEP 8 guidelines for module and package names (I think a name that's used to refer to a module should look like a module name as much as possible, and our current ones don't).  h3. Migration  If this proposal is adopted, I propose we start using it in new files immediately, but leave old files in their current form unless more than ~80% of the file is being rewritten.  Old files will be migrated to the new form as part of DM-2003 later in S15, where the namespace themselves will be changing for most packages, and hence we'll need to touch all of the same code anyway.  Delaying the migration also leaves us time to synchronize with the HSC fork without complicating those cherry-picks with nomenclature-only changes. ",NULL
RFC-35,"Recommend rebase instead of merge-from-master in DM Developer Guide","While there's a larger, slower conversation happening on RFC-21 about our git workflow, I'd like to propose a smaller and hopefully less controversial change now:  The [DM Developer Guide|https://confluence.lsstcorp.org/display/LDMDG/Using+Git+for+LSST+Development] section on Git Workflow currently states that merges from master to a ticket branch should be avoided if possible, but it implies that these are sometimes necessary, and does not mention rebase as an alternative.  Instead, I propose that the guide state that a branch should be rebased on top of master when any such synchronization is necessary, and hence merges from master to a ticket branch should never occur.  The question of whether it should be rebased if synchronization is not necessary is one I'll leave to RFC-21.  I believe this proposal has been the recommendation of most DM developers for a while already (including, I think, [~ktl]), and I think we should bring our documents in-line with that ASAP to avoid pollution of our git history while we wait for a resolution to RFC-21.",NULL
RFC-36,"Retire obs_file and promote processFile from contrib","I wrote obs_file as an experiment; it failed.    git@git.lsstcorp.org:contrib/processFile is a lot more useful, and should be made the project ""just reduce this file"" option.  N.b. This does not mean that we shouldn't cleanup processFile to increase reuse. ",NULL
RFC-37,"New Interface for Footprint & Friends","One of the work packages for the DRP team in S15 is an overhaul of Footprint and associated classes.  A design proposal is ready for review on this PR:  https://github.com/lsst/afw/pull/17  You can find more information there (which is also where comments should go).  There is a lot of content to absorb, so I'm proposing a 2-week review period to ensure everyone who is interested has a chance to look at it.",NULL
RFC-38,"Changes to support typemaps for NumPy scalars","By default, Swig doesn't support passing some NumPy scalars to C++ functions that expect the corresponding type.  For instance, if we wrap a C++ function like this: {code:hide-linenum} void func(float v); {code} Python code like this won't work: {code:hide-linenum} array = numpy.zeros(5, dtype=numpy.float32) func(array[0]) {code}  The root cause of this is that some of NumPy's scalar types don't inherit from the corresponding Python built-in type (e.g. {{numpy.float32}} doesn't inherit from {{\_\_builtin\_\_.float}}).  To fix this, I have written new Swig typemaps for all built-in numeric types, on the tickets/DM-2201 branch of utils (and you can see them in use both there and in daf_base, same branch - I'll convert more packages once this RFC is through).  Unfortunately, due to a deficiency somewhere in NumPy or maybe Python itself, it's impossible to recognize those NumPy scalars without access to the NumPy C API.  That means any package that wants NumPy array scalars to work with its C++ functions needs to depend on NumPy, and that's why this is an RFC, not just a regular uncontroversial ticket.  I think that dependency is worth adding to the {{utils}} package, and everything above it.  In addition to just adding NumPy to their table and .cfg files (or leaving those implicit, since they'll also pull those requirements in from {{utils}}), these packages would need to add the following to their main {{.i}} files (example shown for daf_base): {code:hide-linenum} %include ""lsst/lsstNumPy.i"" %initializeNumPy(daf_base) {code}  For packages that already use ndarray's typemaps to convert ndarray and Eigen objects to NumPy, this will allow a lot of the old code to be removed, because it's now in the {{%%initializeNumPy}} macro.  It will contain the equivalent of all of the following: {code:hide-linenum} %{ #define PY_ARRAY_UNIQUE_SYMBOL LSST_AFW_IMAGE_NUMPY_ARRAY_API #include ""numpy/arrayobject.h"" %} %init %{     import_array(); %} {code} Of course, there's nothing stopping me from cleaning up this code in a similar way, even if we don't adopt the new typemaps or don't put them in utils.  So, aside from a new {{utils}} dependency on numpy (actually, just an upgrade from optional to required), what's up for debate here?  - We're *replacing*, not augmenting, Swig's own typemaps for built-in numeric types.  That will affect a ton of code, and these new typemaps haven't been as well-tested as Swigs have been.  (I don't think it's possible to fall back to Swig's old typemaps for Python objects that aren't NumPy scalars).  - We could put these typemaps in ndarray instead of utils, where our typemaps for full arrays live.  But I think we want to use them on any routines Python science-side users might use, and that includes stuff in both daf_base and utils, and adding dependency on NumPy to those is better than adding dependencies on ndarray.  (For everything above afw, this doesn't matter, those already depend on ndarray). ",NULL
RFC-39,"Rename the 1D version of makeStatistics to makeStatistics1D and reorder arguments","I would like to rename the 1D version of makeStatistics to makeStatistics1D (at least approximately) and reorder the arguments to (psuedocode): makeStatistics(array, flags, sctrl=StatisticsControl(), weights=emptyArray)  The primary reason for this change is to allow a SWIG wrapper that accepts numpy arrays without copying the data while ALSO continuing to allow lists and other sequences of numbers (but with copying).  Supporting both kinds of arrays is crucial, in my opinion. Supporting lists and other sequences is something people expect, but supporting numpy arrays without copying is useful when handling large arrays.  This turned out to be a hard problem to solve. I originally tried to solve it by offering both a std::vector version of makeStatistics and an ndarray version. The former supports lists and such with copying, while the latter supports numpy arrays without copying. Unfortunately SWIG insists on preferring the std::vector version, so numpy arrays of certain types (float, float64, int) get copied anyway. After many failed attempts to convince SWIG to reverse the ordering I decided the best solution was to shadow the C++ functions with a python function that cast lists and other non-numpy arrays into numpy arrays. To do this well requires that the python easily be able to discern which argument is an array, which in turn strongly suggests not having many overloaded functions with wildly different argument order. The simplest solution I've come up with is the one listed above. There will be only one makeStatistics1D that supports weights as an optional argument and only takes ndarray arrays. This is easy to wrap in Python and add the benefit of support for keyword arguments and a standard documentation string.  The main problem that I see is the need to change all existing code that calls makeStatistics with a 1D array.",NULL
RFC-4,"tracking metadata for data sets","Motivation ---------- We have a large (and growing) number of data sets at NCSA, and keeping track of them is becoming mundane.   Proposal -------- Develop a system for tracking metadata for all LSST data sets.   Implementation -------------- 1) the db group implements a web form with mysql backend.  2) Owners of existing data sets and data sets that will be created in the future submit the information about their data sets through the web form. (first version will cover directories containing FITS files, second version config files, third version metadata about databases)  3) Information is stored in MySQL (on lsst10), and each data set registered through the web form is automatically crawled, and additional information (such as that embedded inside fits headers or config files) is extracted, cataloged and indexed in the Metadata Store.  4) IPAC SUI team puts their magic on top to make the information captured in the Metadata Store accessible through nice web interfaces.  5) Data sets not documented by a date picked by DMLT will be trashed. Basically, all data sets we care about will need to be documented.  6) In the longer term, metadata for newly created data sets will be captured automatically through to-be-developed APIs. The web form might turn into L3 external data set submission.   Further details ----------------  * fields to be captured through the web form:     ** data set location (url)     ** type (fits, configuration, etc)     ** purpose     ** responsible user     ** level of priority (scratch, keep short term, keep long term keep)     ** backup (yes/no)     ** [please suggest what else...]  * this has been discussed at Data Access Hangout 12/1/2014  * This falls under the umbrella of Data Access Team, so we are willing to implement and maintain it, and  we think nobody will seriously object, but I wanted to run it by larger audience and get your feedback, comments, reactions, etc...   * If the reaction is positive, I'd like to shoot for having this working by the end of Summer15. ",NULL
RFC-40,"Improve 02C.06 (DataAccess&DB) WBS level 4","I'm proposing improvements to the 02C.06 wbs at the 4th level. In short summary, I'd like to remove ""data definition client services"", add ""web services"" and ""catalog services"", and improve narrative. The changes and the proposed narrative can be found in the attached documents.",NULL
RFC-41,"Mixed-Type Operations for Point and Extent","DM-1197, a request to add type-promoting operators for the {{Point}} and {{Extent}} classes, generated a [long HipChat discussion yesterday|https://lsst.hipchat.com/history/room/520390/2015/04/02?q=quick+poll&t=rid-520390#10:41:37] on an edge case, and [~rhl] requested an RFC to go over the proposed changes in detail.  Since then, I've discovered several more edge cases that may be a little controversial, largely in addressing how to reconcile C++'s and Python's differing views of integer division.  For the most part, these are problems for which there is no good solution, but I'm also pretty confident there aren't any much better than what I've proposed here.  Here's a table of the complete set of operations I propose supporting, as well as some operations explicitly not supported (indicated with ""TypeError"").  More extensive notes are below the table.  ||LHS||RHS||Operator||Result    ||Notes|| | {{PointD}} | {{PointD}} | +, +=       | {{TypeError}} | (1) | | {{PointD}} | {{PointI}} | +, +=       | {{TypeError}} | (1) | | {{PointD}} | {{PointD}} | \-       | {{ExtentD}}        | | | {{PointD}} | {{PointD}} | -=      | {{TypeError}} | (2) | | {{PointD}} | {{PointI}} | \-       | {{ExtentD}}        | | | {{PointD}} | {{PointI}} | -=      | {{TypeError}} | (2) | | {{PointD}} | {{ExtentD}} | +, +=, -, -= | {{PointD}}        | | | {{PointD}} | {{ExtentI}} | +, +=, -, -= | {{PointD}}        | | | {{PointI}} | {{PointD}} | +, +=    | {{TypeError}} | (1) | | {{PointI}} | {{PointI}} | +, +=    | {{TypeError}} | (1) | | {{PointI}} | {{ExtentD}} | +       | {{PointD}}        | | | {{PointI}} | {{ExtentD}} | +=      | {{TypeError}} | (2) | | {{PointI}} | {{ExtentI}} | +, +=       | {{PointI}}        | | | {{PointI}} | {{PointD}} | \-      | {{ExtentD}}        | | | {{PointI}} | {{PointD}} | -=      | {{TypeError}} | (2) | | | {{PointI}} | {{PointI}} | \-      | {{ExtentI}}        | | {{PointI}} | {{PointI}} | -=      | {{TypeError}} | (2) | | {{PointI}} | {{ExtentD}} | \-      | {{PointD}}        | | | {{PointI}} | {{ExtentD}} | -=      | {{TypeError}} | (2) | | {{PointI}} | {{ExtentI}} | \-, -=   | {{PointI}}        | | | {{ExtentD}} | {{PointD}} | +       | {{PointD}}        | | | {{ExtentD}} | {{PointD}} | +=      | {{TypeError}} | (2) | | {{ExtentD}} | {{PointD}} | \-, -=   | {{TypeError}} | (1) | | {{ExtentD}} | {{PointI}} | +       | {{PointD}}        | | | {{ExtentD}} | {{PointI}} | +=      | {{TypeError}} | (2) | | {{ExtentD}} | {{PointI}} | \-, -=   | {{TypeError}} | (1) | | {{ExtentD}} | {{ExtentD}} | +, +=, -, -= | {{ExtentD}}        | | | {{ExtentD}} | {{ExtentI}} | +, +=, -, -= | {{ExtentD}}        | | | {{ExtentI}} | {{PointD}} | +       | {{PointD}}        | | | {{ExtentI}} | {{PointD}} | +=      | {{TypeError}} | (2) | | {{ExtentI}} | {{PointD}} | \-, -=   | {{TypeError}} | (1) | | {{ExtentI}} | {{PointI}} | +       | {{PointI}}        | | | {{ExtentI}} | {{PointI}} | +=      | {{TypeError}} | (2) | | {{ExtentI}} | {{PointI}} | \-, -=   | {{TypeError}} | (1) | | {{ExtentI}} | {{ExtentD}} | +, -     | {{ExtentD}}        | | | {{ExtentI}} | {{ExtentD}} | +=, -=   | {{TypeError}} | (2) | | {{ExtentI}} | {{ExtentI}} | +, -, +=, -= | {{ExtentI}}        | | | {{ExtentD}} | {{double}} | \*, *=, /, /=  | {{ExtentD}}        | | | {{ExtentD}} | {{double}} | //, //=  | {{TypeError}} | (7) | | {{ExtentD}} | {{int}}    | \*, *=, /, /=   | {{ExtentD}}        | (3) | | {{ExtentD}} | {{int}}    | //, //=  | {{TypeError}} | (7) | | {{ExtentI}} | {{double}} | \*   | {{ExtentD}} -(Python), Truncated {{ExtentI}} (C++)- | (4) | | {{ExtentI}} | {{double}} | *=  | {{TypeError}} (Python), {{static_assert}} failure (C++) -Truncated {{ExtentI}} (C++)- | (2), (4) | | {{ExtentI}} | {{double}} | /   | {{ExtentD}} -(Python), Truncated {{ExtentI}} (C++)- | (4), (5), (6) | | {{ExtentI}} | {{double}} | /=  | {{TypeError}} (Python), {{static_assert}} failure (C++) -Truncated {{ExtentI}} (C++)- | (2), (4), (5), (6) | | {{ExtentI}} | {{double}} | //, //=  | {{TypeError}} | (7) | | {{ExtentI}} | {{int}}    | \*, *=   | {{ExtentI}}        | | | {{ExtentI}} | {{int}}    | /   | {{ExtentD}}        | (5), (6) | | {{ExtentI}} | {{int}}    | /=  | {{TypeError}} (Python), {{ExtentI}} (C++) | (2), (5), (6) | | {{ExtentI}} | {{int}}    | //, //=  | {{ExtentI}}        | (8) | | {{double}} | {{ExtentD}} | \*   | {{ExtentD}}        | | | {{double}} | {{ExtentI}} | \*   | {{ExtentD}} -(Python), Truncated {{ExtentI}} (C++)- | (4) | | {{int}}    | {{ExtentD}} | \*   | {{ExtentD}}        | (3) | | {{int}}    | {{ExtentI}} | \*   | {{ExtentI}}        | |  *Notes*  1. Operation is not geometrically meaningful.  Back when we decided to have both Point and Extent classes, the entire purpose was to disallow geometrically meaningless operations like this.  If you'd like to revisit that decision, please open a new RFC; that's out of scope for this one.  2. This operator is impossible to implement directly in C++, as it's an in-place operator that would require the LHS type to change.  That's actually possible to implement in Python, because Python's in-place operators don't actually have to operate in-place.  In fact, if you add a {{float}} to an {{int}} in-place, you'll change the LHS operand type: {code:python|hide-linenum} >>> a = 4 >>> a += 3.2 >>> type(a) <type 'float'> {code} However, that *doesn't* work with tuples and lists: {code:python|hide-linenum} >>> b = (3, 4) >>> b += [1, 2] Traceback (most recent call last):   File ""<stdin>"", line 1, in <module> TypeError: can only concatenate tuple (not ""list"") to tuple {code} Moreover, in NumPy, the operation is allowed but truncates: {code:python|hide-linenum} >>> c = numpy.array([2], dtype=int) >>> c += numpy.array([3.2], dtype=float) >>> c array([5]) {code} Given the mixed signals from Python, and the potential for unexpected errors (this is not a well-known feature in Python), the proposal here is to not allow in-place operations that can change the type of the LHS, and instead raise {{TypeError}}.  3. This operator is not implemented directly in either C++ or Python, but is largely supported by the fact that an overload that takes {{double}} will also accept {{int}} (but may yield different answers for extremely large integers that cannot be represented exactly as {{double}}s).  4. (Removed - my previous statement about C++ ambiguous overloads was incorrect; see discussion below).  5. All ""/"" and ""/="" operations here assume {{from __future__ import division}}.  If this is not enabled, the behavior of the ""/"" operator will be that of ""//"", and likewise for ""/="" and ""//="", for all operations with {{ExtentI}} on the LHS.  6. That this now returns {{ExtentD}} represents an API change in Python, because division of {{ExtentI}} by {{int}} previously just delegated to the truncating C++ implementation that returns an {{ExtentI}}.  7. The // operator applies only to integer types.  8. This Python-only operation does not always produce the same result as regular division of integers in C\+\+, because Python specifies that {{a // b}} is equivalent to {{floor(a / b)}}, while C\+\+ specifies that it should be equivalent to {{int(a / b)}}.  Note that {{floor}} rounds negative numbers down and {{int}} rounds them up. ",NULL
RFC-42,"Refactor the image display code to remove explicit references to ds9","The current stack image display code assumes that you're talking to ds9: {code} import lsst.afw.display.ds9 as ds9  ds9.erase() {code} As the IPAC team thinks about supporting firefly, and ginga gains traction in the scipy world, we need replace this interface with something more general.  In the longer run we need to decide what APIs we need to support, but this is hard to do until we have more than one backend.  This RFC proposes that we change the interface to something like: {code} import lsst.afw.display.imdisplay as imdisplay imdisplay.setBackend('ds9')  imdisplay.erase() {code}  It would be possible to hoist these methods to lsst.afw.display itself: {code} import lsst.afw.display as afwDisplay afwDisplay.setBackend('ds9')  afwDisplay.erase() {code} A discussion of the pros and cons is in scope for the RFC.  The choice of backend (in this case ds9) should be configurable either explicitly, as illustrated here, or via some deus ex machina technique (a dot file, an environment variable, ...).  I put the backend choice explicitly in the examples, but I would not expect this to appear in utility code.",NULL
RFC-43,"Semantics of JIRA issue types","At [T/CAM training|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=29131350] on 9 April 2015 we discussed the various types of issue -- epic, story, bug, improvement -- available in the DM JIRA project. Our aims are twofold: to ensure that JIRA provides a convenient and low-overhead tool for use by developers, and to facilitate its use in the reporting of earned value.  Currently, we report earned value based on the completion of stories and bugs, but not improvements. This has led to the rumour that developers are ""not supposed"" to use the improvement issue type.  I submit that an ""unusable"" issue type is unhelpful and confusing: either improvements should not be available for use at all (ie, the type should be removed from the DM project), or it should be available to earn value in the same way as bugs and stories.  I further submit that the distinction between stories, bugs and improvements is semantically meaningful. To wit:  * A story is the result of breaking down a large task derived from the software development roadmap (LDM-240) -- an epic, in other words -- into workable units; * A bug describes a fault or error in code which has already been accepted to master; * An improvement describes a feature request or enhancement which has not been derived by breaking LDM-240 down into epics.  For example, DM-882 describes a segfault in running code: this is clearly a bug. On the other hand, DM-609 requests an enhancement in the way tests are run -- there's no suggestion that existing code is broken or incorrect, so it's not a bug, and it's not the result of decomposing an epic into stories.  I propose that we regard bugs, stories and improvements as equivalent from an earned value standpoint, and encourage developers and their managers to use them to plan and report on their work using the definitions above.  If the above proposition is not accepted, I propose that the improvement issue type being expunged from the DM JIRA project to avoid any possible future confusion.",NULL
RFC-44,"Remove build system dependencies from tests","The discussion in DM-2527 demonstrated that test code uses EUPS to determine where things are located. This has two problems:  # Calling EUPS in a loop resulted in slow downs of the tests as the EUPS database was repeatedly read. # This makes it impossible to run the tests standalone and burns-in a particular packaging implementation that is not relevant for the tests.  This RFC proposes that EUPS be removed from tests and an alternative scheme be used for locating supporting infrastructure. Some tests still need to locate other packages (in particular {{afwdata}}) so any replacement scheme must support some form of package location.  Some options for locating {{afwdata}}:  # Provide a test utility function for locating a package which can use an environment variable to find {{afwdata}}. # Add {{afwdata}} as a git submodule in the test directory. This could result in multiple checkouts of the repository.  This RFC invites further discussion on whether EUPS should be removed from the tests. Comments on replacement functionality are welcomed to demonstrate feasibility.",NULL
RFC-45,"Process for maintaining Copyright information in DM source code"," Purpose: Adopt the most *lightweight* (from the point of maintenance) copyright process for software written in construction that is still compatible with our contractual obligations and open source principles.  Proposal for default practice:  1. Each file has a header that says “See COPYRIGHT file at the top of the source tree”.   2. The COPYRIGHT file is considered a template file, with sections of it replaceable by robots.   3. The copyright file has a line per institution that contributed to the code, in a date range eg.  Copyright University of Waterloo (2012-2015)  4. If people from two institutions are making substantial contributions to that code, they add their institution to the copyright line.  Copyright University of Waterloo and AURA/LSST (2012-2015)  5. Additional boilerplate will be included in the COPYRIGHT file reflecting the AURA/LSST-institution contractual arrangements (specifically perpetual license to AURA/LSST to modify and redistribute)  6. Requirement of developers: Use your institutional email address for commits   7. Requirement on SQuaRE: Insert template into repos. Periodically update the end date of the notice and run a simple check to make sure the list of intitutions is consistent (eg a file has a UW line if all the commits are from people with UW addresses). Scan for non-institutional emails in the commits (eg. people pushing with their gmail address).   8. This is in the default process for ""normal"" work. If someone is developing code that they are worried is of commercial value or other concerns that require a more defensive process, they are free to engage in more heavyweight processes such as including copyright statements in every file. They would undertake to maintain that non-default process.  Note that the construction contracts do not require copyright assignment to AURA/LSST and we will not require copyright assignment from open source contributors.  #6 is the most significant as VCS information can legally be used to resolve disputed claims.  Background (skip if you’re fine with the above):  - Many conventions on copyright in open source come from FSF guidance for GPL-license source but that has been drawn for specific situations that are not a particular worry to us (e.g. commercial parties subverting open source code).  - Once again we are guided by the Software Freedom Law Center  https://www.softwarefreedom.org/resources/2012/ManagingCopyrightInformation.html  Summary:  Copyright is implicit (it does not need to be asserted). A central Copyright file notice is therefore sufficient in cases where it is unlikely that a file can be separated form its source tree. The Version Control system is considered adequate proof of individual contributions. ",NULL
RFC-46,"New Multi-Band Processing Scheme for Coadds","I've just created  https://confluence.lsstcorp.org/display/DM/S15+Multi-Band+Coadd+Processing+Prototype  to describe the new approach to processing coadds that we'd like to backport from HSC.  This represents a major departure from processCoadd.py, and while processCoadd.py won't be removed immediately, now is the time to review the design of the new task (though we'd like to defer major changes until after sychronization with HSC is complete).",NULL
RFC-47,"Remove unused utils functions and change eups::productDir to getPackageDir","As part of DM-2635 I propose to make the following changes to the C++ API of the ""utils"" package:  Remove two functions we are not using anywhere: - guessSvnVersion: this is clearly useless now that we use git - stringToAny: we aren't using it, and it has no documentation  Rename lsst::utils::eups::productDir to lsst::utils::getPackageDir and remove the version argument. Note that the version argument has only one valid value, so it's never been useful. This would be the implementation of DM-2635. ",NULL
RFC-48,"Add Python requests package as external dependency","In database team we are developing services, some of the internal, which use HTTP for communication protocol and are typically based on RESTful architecture. To implement client side bindings for these services in Python we need reasonable HTTP client library which satisfies small set of requirements (copied from DM-2593): - support for arbitrary methods (GET, POST, PUT, DELETE) - basic and digest authentication -  support for HTTPS (optional) - multi-part POST - POST data streaming  Standard Python modules for HTTP are lacking in few areas. Out of several existing external packages [requests|http://docs.python-requests.org/en/latest/] looks most complete and widely used. Python library documentation in fact proposes to use {{requests}} instead of standard modules for high-level interface.  We are proposing to add requests package as an external dependency to DM stack. Currently only qserv is going to depend on it but it's likely that over time there will be other users for that package. The requests package itself does not have other required dependencies and includes its own copy of urllib3 package but optionally (if HTTPS support is needed) may use pyasn1, ndg-httpsclient, and pyOpenSSL.",NULL
RFC-49,"Using ws4py in Python client  for communication between client and Firefly server","We are using Websocket ( RFC 6455 http://tools.ietf.org/html/rfc6455 ) for bi-direction communication between browser and web server. It is well supported in modern browsers (see http://caniuse.com/#search=websocket for details) and Apache Tomcat 7 and above (JSR-356 http://www.jcp.org/en/jsr/detail?id=356).   For Firefly Python APIs, we want to use the Python implementation ws4py (GitHub https://github.com/Lawouach/WebSocket-for-Python). It is a simple implementation and has very good documents and sample codes.   There is another websocket implementation listed in Anaconda gevent-websocket (https://bitbucket.org/Jeffrey/gevent-websocket) which is for gevent library.     After some quick reading and comparison, we propose to adopt ws4py implementation.  ",NULL
RFC-5,"JIRA Renaming of ""Review Complete"" status to ""Reviewed"" - BREAKS filters","What: I propose to rename the workflow status ""Review Complete"" to ""Reviewed"" in order to be able to make requested dashboard gadgets more compact.   Affected: This affects the DM JIRA project only.  Consquences: Anyone who has a filter that searches for Review Complete will get an error and will have to edit their filter to use the new status name.   Timeline: Short timeline because it can be reversed in ~ 5 minute timescale. ",NULL
RFC-50,"Simplify distribution of common python modules","It'd be useful to have a lightweight way of packaging up dependencies that happen to be common python packages. Examples include requests, flask, kazoo, etc. Packaging it through eups is not very heavy, but it feels like a unnecessary burden.  Proposal: package up all such dependencies in a simple text file and use ""pip install blah"" to install them each in turn.  This got triggered through DM-2727.",NULL
RFC-51,"JIRA Automatic Assignment to default to system T/CAM","  This RFC is filed on behalf of Simon, John, Xiu Qin and myself following verbal discussion this morning.     Currently JIRA ""Assign automatically"" is assigning based on component 'owner'. This is resulting in items being assigned to notional owners of code, ie individual developers.     In fact due to EVM, assigning a ticket potentially costs ""points"" (SPs) and the holders and spenders of those points are T/CAMs. Therefore in cases where the assignee is not understood well enough (by both parties) to be a specific developer, we suggest the automatic assignee should be the T/CAM of the system.     Clarifications:    1. This is not intended to micomanage developers, so someone saying ""Assign a ticket to me"" on hipchat still should get their ticket    2. Developers are empowered to ""Assign the ticket to me"" to take a ticket - it's between them and their T/CAM if that causes an sprinting issue.    => This is purely for situations that are unexpected and/or of unclear domain that lead to ""Assign Automatically"" being used by the reporter. ",NULL
RFC-52,"Packages should list all direct dependencies in their ups table files","We have been allowing developers to list as many or as few direct dependencies as they wish in the ups file of a package, as long as the chain of dependencies includes everything needed. I propose that we recommend that every package list all direct dependencies in the ups file because:  - It is more robust against changes in dependent packages. If package A depends on B and C, and B depends on C, then it is sufficient for A to only list B as a dependency. But if B is later changed to not rely on C then this breaks A in a way that is surprising.  - It makes the dependencies of the package explicit to readers of the code.  - It reduces the need to understand the dependency tree of dependent packages.    This is clearly a contentious issue so I am directly assigning it to K-T",NULL
RFC-53,"RFD and design discussion time slot","At the DMLT meeting, a desire for more in-depth technical discussions was expressed.  These might be detailed design discussions for a component of the system or its interfaces; design reviews for new code or refactorings of old code; brainstorming about methods to solve difficult problems; or ""brain dump"" explanations of a design.    I propose the following:  * We choose a standing weekly time slot for such discussions.  http://doodle.com/7vtsbf8y9xa8xi8a is a Doodle poll for a suitable slot.  The Project Scientist, Project Engineer/System Architect, Deputy System Architect, and Interface Scientist will endeavor to keep this slot free.  * A person or team may claim the slot for a given week by sending an ""RFD"" (request for discussion) mail to the dm-devel mailing list with the meeting date, meeting topic, the desired result of the meeting, and all relevant background information or documents.  * The meeting requester is responsible for making sure that any required attendees are available for the meeting.  * If there are conflicting claims, the Project Engineer will arbitrate.  * If there are no requests 24 hours before a given time slot, the meeting will be cancelled and the time freed up for other activities (such as writing up papers).",NULL
RFC-54,"update anaconda version to 2.2.0","The version of anaconda used by lsstsw is the defacto reference version of python for building the stack.  At present, the version is pinned to 2.1.0 with the system package manually upgraded to work around DM-1801.  It was a conscious decision not to upgrade to anaconda 2.2.0 due to concerns about ipython compatibility.    [~danielsf] has reported that the package upgrade logic does not work for him on OSX (https://github.com/lsst/lsstsw/pull/25).  It is also my understanding that the concerns around ipython are no longer relevant.    Instead of maintaining installation logic around anaconda 2.1.0, I propose that the lsstsw and eups package (used by newinstalls.sh) versions be upgraded to 2.2.0.",NULL
RFC-55,"retire lsst_libs eups metapackage","*partially adopted; see comment [https://jira.lsstcorp.org/browse/RFC-55?focusedCommentId=35649&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-35649]*    The only package that currently depends on lsst_libs is the lsst_apps metapackage.  lsst_libs currently consists of: lsst_thirdparty, daf_persistence, pex_config, ndarray.  lsst_thirdparty is itself a meta package and there is a chain four metapackages deep in the build graph that provides no obvious benefit.    lsst_distrib -> lsst_apps -> lsst_libs -> lsst_thirdparty    The other three component packages of lsst_libs are already required by direct dependencies.",NULL
RFC-56,"Upgrade minimum required gcc version to 4.8","Our adoption of C\+\+11 (and C\+\+14 features) is held back by the very minimal (and occasionally incorrect) support for C\+\+11 in gcc 4.4.    Major long-term-support Linux distributions that ship with gcc 4.8 have now been out for about a year now (including RHEL/CentOS 7 and Ubuntu 14.04), so it may now be reasonable to expect users currently relying on gcc 4.4 to either upgrade their OS or bear the burden of installing a newer compiler.    We could also consider providing eups distrib packages with newer compiler versions for those who need them (we do this on the HSC side).    In any case, I don't think we can simply wait for OSes that ship gcc 4.4 to go away on their own; the lifespans of those distributions are simply too long.  But if certain important users are already scheduled to upgrade in the near future anyway, it may be prudent to just wait for that ourselves.",NULL
RFC-57,"retire datarel, ap, and testing_endToEnd packages","I don't think there's anything in any of these packages that is currently in use or planned to be in use in the future (functionality previously provided by these packages will be rewritten from the ground up).    My main goal here is to retire {{ap}}, which is a dependency of the others, so I can make modifications to afw::table (such as on DM-1766) without having to worry about breaking it.  If we can find a way to drop {{ap}} without dropping the others, I'd be satisfied, but I don't currently know of any reason to keep the others around.    Note that the deadline for this RFC is early next week, and I'd like to stick to that, as I hope to merge DM-1766 soon, whether that means I have to include a patch to {{ap}} or not.",NULL
RFC-58,"Refactor afw.math.Approximate, .BackgroundMI, .GaussianProcess to share interface","The LSST Stack currently models 2D surfaces in the background-matching and background-estimation components.  In the near future, code that uses 2D modeled surfaces will grow to include aperture corrections and PSF/zero-point interpolation over a chip or focal plane.  We have three classes which can model/interpolate 2D surfaces:  * {{lsst.afw.math.Approximate}} (Chebyshev polynomial fit)  * {{lsst.afw.math.BackgroundMI}} (Spline interpolation - CONSTANT, LINEAR, NATURAL_SPLINE, CUBIC_SPLINE, CUBIC_SPLINE_PERIODIC, AKIMA_SPLINE, AKIMA_SPLINE_PERIODIC)  * {{lsst.afw.math.GaussianProcess}} (Gaussian process interpolation)    This current state of affairs is lacking in that:   * The APIs for the three classes differ, which makes it difficult to swap one object in for the other. We end up with repetitive control structures the client code. For example, {{MatchBackgroundsTask}} returns either a {{lsst.afw.Approximate}} object or a {{lsst.afw.BackgroundMI}} object depending on whether it is configured to use polynomials or splines.  The result is bad code that looks like: {code} backgroundImage = backgroundModel.getImage() if \                      self.matchBackgrounds.config.usePolynomial else \                      backgroundModel.getImageF(){code}   * No persistence for Gaussian Process and Approximate.      Whereas the interpolated/fitted 2D models differ in their internal representation (e.g. coefficients vs. points to interpolate between, fitting algorithms and evaluation algorithms) they are similar in the following ways. They are have an XY domain. They are created from points (either gridded/image or non-gridded/scattered). Client code uses them primarily to evaluate at (x, y) and getImage().       Users should be able to easily swap one class for another, and storing the 2D models is necessary for aperture corrections.  These requirements call for an abstract base class that contains an interface for accessing the interpolated/approximated function while making no assumptions about its functional form. [~jbosch]  implemented this (called {{BoundedField}}) on the HSC side with support for gridded and non-gridded/scattered Chebyshev polynomials.  https://github.com/HyperSuprime-Cam/afw/compare/releases/S14A_0...tickets/DM-796 has been pulled over to the LSST side unchanged in afw: u/yusra/DM-740.    As I  adapt {{BoundedField}} for the LSST stack (DM-1191), I'm requesting comments on the API in the following four categories:    1) *Name for abstract base class*   Proposal: {{Model2D}} (with subclasses named {{ChebyshevModel2D}}, {{GaussianProcessModel2D}}, etc...)  The name should convey that it is:  * Two-dimensional: ""Surface"", ""2D"", ""Field""  * Has an XY Domain: ""Bounded""  * It is an inexact representation: ""Model""    2) Proposed API for *creating/building* a {{Model2D}}.   * Assumption: Users will not be writing their own {{Model2D}} classes. => A simple static factory (as opposed to a dynamic plug-in system such as the measurement framework) should be sufficient. Like our current {{Background}} class's {{interp_style}} ENUM, we would maintain an ENUM of available subclasses of Model2D.  * Example usage  - where ""fit"" is the name of our subclassed ""create (from data)"" method:  {code}  ctrl = lsst.afw.math.Model2DControl('CHEBYSHEV', configStruct)  chebyshevModel2D = lsst.afw.math.Model2D.fit(x, y, z, ctrl)    ctrl = lsst.afw.math.Model2DControl('AKIMA_SPLINE')  interpModel2D = lsst.afw.math.Model2D.fit(x, y, z, ctrl)  interpModel2D.getImage()	  {code}  		Note: The current Background API specifies the interpolation type in {{getImage()}} not {{BackgroundControl}} since the work is done in the evaluation stage instead of the creation stage.  This distinction is at the crux of our original Approximate vs. Interpolate dichotomy, but is an implementation detail. All client code can use both Approximate/Interpolate interchangeably.  In the the proposed API, this could be handled: {{ctrl = lsst.afw.math.Model2DControl('INTERPOLATE')}} where the ""INTERPOLATE"" enum would signal that an an {{interpolationModel2D}} be instantiated, which would take an extra argument in {{getImage('afwMath.Interpolate.AKIMA_SPLINE')}}.  It would be cleaner, however, to require the interpolation style in the object creation stage, even though its not used until {{evaluate()}} or {{getImage()}}, since we gain a lot by having an identical interface for *using* {{Model2D}} objects. This of course begs for efficient methods to turning one type of {{Model2D}} into another.    3) *Operating on a {{Model2D}}* We'd like to do basic operations on these objects, such as adding and multiplying by scalars and performing transformations on the x/y domain.  [~jbosch] recommended {{Model2D}} objects remain immutable (original HSC {{BoundedFields}} are). Operations would create new objects, rather than alter them. For example:  * {{chebyshevModel2D  = 2 * halfchebyshevModel2D}}  * {{akimaModel2D = lsst.afw.math.Model2D.fit(linearInterpModel2D, ctrl)  #see note in section 3}}      4) *using* a {{Model2D}}:  * {{model2D.evaluate(x,y)}}  * {{model2D.addToImage(image); model2D.multiplyImage(image)}}  * {{model2D.fillImage(image)}}  * {{model2D.getImage()}} Note: This requires that a Model2D be aware of its bounding box. Currently {{Approximate}} and {{Background}} have {{getImage()}} methods.  When adapting the client code, I will also do a cost/benefit analysis of maintaining this.    Epic link: DM-1991  Implementation Issue: DM-740  Some initial thoughts can be found on the Requirements page: https://confluence.lsstcorp.org/pages/viewpage.action?pageId=33161587    This RFC will be open until EOB Friday, I will take comments into account for the final design.",NULL
RFC-59,"Add gradle, node.js, and Tomcat to external packages list","This RFC is to request the addition to our repositories of packaging of 3rd party products needed to build and run IPAC Firefly: gradle, node.js, and tomcat. Once these are added, and DM-2731 is merged, it should be possible to build and run Firefly the same way we build the science pipelines or qserv (including in CI).    The git repositories and eupspkg build files for these are already at https://github.com/EUPSForge, so it should be a matter of cloning the repositories.    Note: This RFC does not attempt to prejudge the answer to the much more complex question of whether we want to continue packaging external products in this way; it's aim is solely to enable easy use of Firefly within the current framework.",NULL
RFC-6,"JIRA DM Teams changed to map to Technical Managers","Proposal: In order to facilitate Dashboards and other reporting views that map to technical managers, I propose to replace the current JIRA DM teams with these teams mapped to the DM technical managers.   Alert Production - SImon Data Access Middleware - Jacek [new plan: Data Access and Database] Data Release Production - John  [new plan: Management and Architecture - Jeff] Process Middleware - Margaret-for-now Site Infrastructure - Margaret SQuaRE - Frossie [new plan: Science Quality and Reliability Engineering] SUI - Xiu Qin [new plan: Science User Interface]  The names come from the WBS structure and Jeff very much has an opinion which I have already solicited.   The RFC here is to allow for objections to the removal of the current Teams.   Giving a week as this is neither urgent and may need thinking about.  ",NULL
RFC-60,"Support python 3 and python 2.7 simultaneously","Currently the stack targets python version 2.7 exclusively. In the current timeline python 2.7 will no longer receive anything other than critical fixes and support for that will be ended in 2020. It will not be receiving any of the new features being added to python 3 (such as type hinting or matrix operators).   Python 3 is being adopted in the astronomy community and over the lifetime of the DM stack development it is assumed that python 3 will begin to dominate (there was even a recent discussion on the astropy mailing list where the idea of dropping support for 2.7 was actively discussed during the LTS cycle). We can not afford to defer the adoption of python 3 to the end of the development process. This RFC proposes that we start to modify the stack code such that it can run on python 3 and python 2.7. This is not a proposal to drop python 2.7 support.    Migrating to python 3 will be an incremental process rather than a big bang and will depend on external dependencies migrating. I advocate that we start the process of migrating before all dependencies are themselves ported. There is a gain in writing new code in a portable manner now rather than attempting to port it later.    If we accept the principle that striving for python 3 is a good goal then we can investigate using the {{future}} portability package (and associated {{futurize}} script) [http://python-future.org/quickstart.html]. We should also setup a single CI process to run with python3 that will initially break all the time but will also tell us the porting priorities.      ",NULL
RFC-61,"Upgrade Scons to version 2.3.4","As part of the discussion behind DM-2839 it was suggested that the first step would be to update Scons to the current version. There have been four minor releases since the version we currently use (2.3.0). Does anyone have any objections to the update?    Scons is listed on https://confluence.lsstcorp.org/display/DM/DM+Third+Party+Software as not being updatable without requesting permission. An outcome of this RFC may be to change the state of Scons to be updatable.",NULL
RFC-62,"Make config files and command line specifications of configurations consistent","There is an irritating inconsistency between how you specify configuration parameters on the command line ({{config.foo.bar}}) and in a configuration file ({{root.foo.bar}}).  This is especially annoying when you use {{--show config}} to obtain the name, cut and paste the answer into a file, and are rewarded with:  {quote}   error: cannot load config file 'ptfConfig.py': name 'config' is not defined  {quote}    This RFC proposes that we adopt a consist naming convention.  I prefer {{config}} everywhere, but (as [~ktl] points out) that's a slightly more complex change than adopting {{root}} everywhere.  On the other hand, I think that adopting {{config}} is much clearer, and wouldn't tempt us to change the the pipe_base's {{-show config\[=glob]}} option.",NULL
RFC-63,"Change our c++ standard for named constants (ALL_CAPS --> mixedCase) ","Per our [C++ Naming Convensions|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685] we should be using ALL_CAPS for all C++ named constants. I don't think we are applying this rule in practice, .e.g., in Qserv in almost all places we use mixCase. A quick peek at afw reveals the same (""find afw/ -name ""*.cc""|xargs egrep ""const int|const double""|grep ';'"" gives me 92 hits, and only 1 out of them is ALL_CAPS). Clearly we seem to prefer mixCase, so I am proposing we change the 3.3 rule. If people are against changing it, then let's enforce ALL_CAPS.",NULL
RFC-64,"Add SQLAlchemy to external package list","[SQLAlchemy|http://www.sqlalchemy.org/] is a commonly used Python SQL toolkit. There are many appealing reasons to use it, the most important being:  * it provides a nice abstraction that can isolate us from specific RDBMS. For example, toggling between MySQL and SQLite or PostgreSQL should be much easier if we have SQLAlchemy in between our software and RDBMS,  * it provides numerous useful features, for example connection pool, which we definitely need both for Qserv and for all Web Services that will talk to the database (MetaServ, DbServ).    The version 1.0 just came out. We asked on their mailing lists about long term support and we were told:   * Red Hat itself does provide some enterprise support for the versions that are supported on RHEL releases, so they will backport fixes when necessary.   * Each version of SQLAlchemy is pretty stable.   * Upgrading to the newest version of SQLAlchemy in general shouldn't break things. They try to keep backwards compatibility as much as possible. You may have problems when you don't use SQLAlchemy idiomatically.   * They document breaking changes and propose fixes for those cases, should they happen.   * OpenStack relies on SQLAlchemy, they've had very little issue going through three upgrade cycles.    So, we, the Data Access and Db team are proposing to adopt SQLAlchemy and add it to our dependency list.",NULL
RFC-65,"Use #pragma once instead of include guards","Our current header files use #define-based include guards. I propose we use #pragma once, instead, because it is safer and simpler.    I acknowledge that pragma once is not part of the C++ standard, but every compiler we have considered using supports it. See http://en.wikipedia.org/wiki/Pragma_once#Portability for a list.",NULL
RFC-66,"Introduction to TOWG","[~ktl] proposes that he (and optionally [~gpdf] and [~tjenness] and [~petravick] and [~gruendl]) walk anyone interested through the Technical Operations Working Group model, including an overview of the DM-related use cases, a brief discussion of the line between technical work and policy work, an indication of the work yet to be done, and disclosure of ways to provide input.    The desired result is just information transfer from the TOWG representatives to interested parties in DM.  No decisions are expected.    The relevant documents are currently in flux, but a draft will be posted before the meeting.    (Hat tip to [~mjuric] for suggesting the topic.)",NULL
RFC-67,"Proposal to create repository and dependency for PSFex","PSF extractor is an external library which is used to determine the PSF in an image. I am proposing creating a new repository for this external library which will become a dependency. This functionality currently exists within the HSC pipeline, and will be ported over according to the LSST practices for external libraries.    As part of the port, I am also proposing to create another repository called meas_extensions_psfex . This repository will contain all the necessary interfaces between the LSST stack and the external library. This repository also currently exists in the HSC software stack, and will mostly be a direct port.",NULL
RFC-68,"Backport HSC parallelization framework","As part of porting pipeline code from the HSC fork of the stack, we'll need more parallelization features than are currently available in the LSST codebase.  Our requirements and the HSC solutions are described here:    https://confluence.lsstcorp.org/display/DM/S15+Short-Term+Parallelization+Middleware+Requirements    I propose that we simply bring the HSC framework over with very little modification - essentially nomenclature and code-cleanup changes only, into a new LSST package.  This new package would depend on [{{mpi4py}}|http://mpi4py.scipy.org/], which would be a new third-party package included in the stack.  I propose we call the new package {{ctrl_pool}}, but I'd be very happy to hear other suggestions.    New parallel driver scripts that rely on {{ctrl_pool}} would not go in {{pipe_tasks}}; we'd add a new package for these as well ({{pipe_drivers}}?).  For the most part, these would delegate their work to conventional {{CmdLineTasks}} in {{pipe_tasks}} that could be run manually on smaller scales, but this may not be entirely possible for all pipelines.    We do not anticipate this being the long-term solution for our parallel execution framework, but we believe the concepts are general enough and the interface abstract enough that it should be fairly easy to modify pipeline code to adapt to a new framework in the future.",NULL
RFC-69,"Refactor sconsUtils to enhance usability in CI","sconsUtils provides fundamental functionality for building LSST packages but has a couple of issues that, if fixed, would significantly help with advanced continuous integration builds and metrics gathering. In this RFC I propose we make the following changes to sconsUtils:    * Enable EUPS-less builds. Allowing the build to run without requiring EUPS be available simplifies decomposition of the build into individual entities. This is an extension of RFC-44. There is a bug report in DM-2769.  * Run tests with py.test or nose. The tests currently write a .failed file with the output of the test. This is not parseable by standard test reporting environments. We propose to modify the test execution to use py.test or equivalent capable of writing test results in a standard form that can be understand by many tools. This will improve the readability and metrics analysis of tests (at the expense of one more python dependency). This may require some tweaks to how tests are laid out currently and account will be made of the memory test case (including running tests with valgrind enabled).  * Replace the {{ups/*.cfg}} file with a file in standard pkg_config format. This will allow for the installed packages to be queried for dependencies using standard tools and support non-EUPS thirdparty applications linking against the stack C++.",NULL
RFC-7,"Relax section 6-6 of C++ standards","I propose we remove the ""historical / SHOULD NOT"" language from the final block of standard 6-6 of the [C++ coding standards|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908737], which currently states: {quote} For historical reasons, the format: {code:hide:linenum} void someMethod(type arg) {     ... } {code} is permitted but not encouraged, and SHOULD NOT be used in new code. {quote} This is the way many of our functions are currently defined (essentially, all of those for which this fits on one line), and I think that's an entirely readable and appropriate form that should be just as allowable as the others.  (In fact, I prefer the latter two forms over the first one, but I'm not going to fight that battle unless there's unanimous agreement).",NULL
RFC-70,"Tech Talks on Coaddition/Detection and Image Differencing algorithms","[~jbosch] and [~krughoff] hereby request the RFD slot on July 21 and August 11 for presentations describing algorithms for coaddition/detection and image differencing (respectively).    These talks will be at an introductory level, intended for typical DM developers (including those who do not have an astronomy background).  We'll send some announcements later suggesting background reading for those who may not be familiar with all of the prior knowledge we'll assume.",NULL
RFC-71,"Allow scons to be upgraded without RFC","Currently on [https://confluence.lsstcorp.org/display/DM/DM+Third+Party+Software] {{scons}} is listed without a ""u"" indicating that an RFC is required before it can be updated. In RFC-61 {{scons}} was approved for updating to v2.3.4 and now v2.3.5 is available.    Given that we are working on {{scons}} infrastructure as part of RFC-69 it would be good if we could be more flexible in tracking upstream {{scons}}.    Can we compromise and say that {{scons}} updates of minor versions can be done any time but major versions require an RFC?",NULL
RFC-72,"Specify numerical tolerances for CI integration test","After Buildbot (or Jenkins) builds the LSST stack, it checks for successful execution of the {{lsst_dm_stack_demo}}. This performs a source detection and measurement procedure, saves the results to a text file, and then compares the contents of that file against a pre-computed set of expected results.    In issue DM-1086, it was established that the results of this procedure can vary slightly from platform to platform due to differences in the relevant numerical libraries. Thus, results computed on a Mac will not be a perfect match for those computed on Linux, for example.    As part of the CI procedure, the [{{numdiff}}|http://www.nongnu.org/numdiff/] tool is used to compare numerical values. It is invoked using the following command line:  {code}  /home/lsstsw/lfs/bin/numdiff -# 11 detected-sources_small.txt.expected detected-sources_small.txt  {code}  In this mode, {{numdiff}} requires that at least one of the absolute ({{n1-n2}}) or relative ({{(n1-n2)/min(n1, n2)}}) difference between each numerical value is calculated to be zero.    However, note that we are passing a {{-#}} argument to {{numdiff}}. To quote the manual:  {quote}  By means of the option -# the user can set the number of digits in the significands used in multiple precision arithmetic. The default value is 35, the largest admissible value is 180. If numdiff has been linked against the GNU Multiple Precision Arithmetic Library (also called GNU MP), then the precision it uses is typically higher than the specified one.  {quote}   In other words: the fidelity with which the numerical differences are assessed is platform dependent. And, indeed, this makes a difference — compare:  {code}  $ uname -a  Linux lsst-dev.ncsa.illinois.edu 2.6.32-504.30.3.el6.x86_64 #1 SMP Wed Jul 15 10:13:09 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux  $ cat file1  0.0692063502115  $ cat file2  0.0692063502114  $ /home/lsstsw/lfs/bin/numdiff -#11 file1 file2    +++  Files ""file1"" and ""file2"" are equal  {code}  with:  {code}  $ sw_vers  ProductName:    Mac OS X  ProductVersion: 10.10.4  BuildVersion:   14E46  $ cat file1  0.0692063502115  $ cat file2  0.0692063502114  $ numdiff -#11 file1 file2  ----------------  ##1       #:1   <== 0.0692063502115  ##1       #:1   ==> 0.0692063502114  @ Absolute error = 1.0000000000e-13, Relative error = 1.4449541075e-12    +++  File ""file1"" differs from file ""file2""  {code}    I suggest that having not just the demo output but *the means of comparing the demo output* vary between platforms is actively unhelpful. Although it seems impossible (based on the description above) to guarantee that {{numdiff}} output will be constant when changing platforms, in every case I have tested so far simply removing the {{\-#}} argument does the trick. So far as I can see, the origin of the {{\-# 11}} argument in our invocation of Buildbot is not recorded (at least, not in version control) — it simply exists in the [versions of the scripts imported to GitHub|https://github.com/lsst-sqre/buildbot-scripts/commit/62fab156502f0453bca1dd4fccf4fe0dc8a208da]. I therefore advocate that we drop this argument to {{numdiff}} entirely.    We should also address the issue of demo output varying between platforms. First, note that this has been discussed in a couple of places already:  * DM-1379 discusses “resetting the comparison metric to EQUALS” for this test, rather than allowing any deviation from the precomputed results. I suggest that this is based on a misconception about the current situation: as it stands, numdiff is not configured to allow some predetermined deviation — it does check for a difference of zero, it’s just not very accurate about how it calculates that difference.   * DM-2877 both decries the existence of two separate numerical comparison tools ({{numdiff}} and the {{bin/compare}} script in the {{lsst_dm_stack_demo}} repository) but also suggests that it’s actually necessary to perform two different types of comparison: one for cross-platform and regression testing, and one for “scientifically meaningful” comparisons to assist developers.    I see two possible ways forward here:  # We maintain per-platform lists of expected outputs (at the moment, I believe only Mac and Linux outputs would be required, but this may change in future) and check for exact equality with the expected results for the appropriate platform;  # We relax the numerical tolerance required in our invocation of {{numdiff}} so that it accounts for inter-platform variation. The discussion surrounding DM-1086 suggests that a relative tolerance of {{4e-3}} would be appropriate for this.    Given that we do not have ground truth for the demo outputs and have no rigorous way to determine whether differences introduced by changes to the pipeline code are “scientifically meaningful”, I suggest that the latter approach is more straightforward, and therefore propose changing the {{numdiff}} invocation used by CI to:  {code}  /home/lsstsw/lfs/bin/numdiff -r4e-3 detected-sources_small.txt detected-sources_small.txt.expected  {code}  At the same time, we should more seriously begin considering what it would take to develop a means of testing demo outputs for scientific validity.    That said, my preference here is not strong: I would be equally happy if the consensus is that we should maintain per-platform demo outputs, or if another, better, proposal arises.",NULL
RFC-73,"How to handle missing aperture correction data?","DM-3182 adds the ability to apply aperture correction to the measurement task. However, I have run into a question of how to enable it and how to handle the situation if aperture correction data (an ApCorrMap) is not available.    When I originally wrote the code I made the task apply aperture correction by default (set doApplyApCorr true) but I only logged a warning if the necessary data was not available (the exposure did not have an ApCorrMap).    Jim rightly pointed out that this is dangerous: if you expect aperture correction to be applied and the data is not available, the task should fail. So I changed the code to raise an exception if aperture correction is enabled and the dat is not available.    Unfortunately I found that this broke a lot of code, including two tasks (ProcessImageForcedTask in meas_base and ImagePsfMatchTask in ip_diffim), many unit tests and some examples in at least 5 packages. I have fixed most of that code by explicitly setting doApplyApCorr False in the appropriate places. However, the need to change so much code makes me skeptical that this is the right approach.    One obvious alternative is to change default for doAppyApCorr to False in measurement task, then change that default value in the config of tasks that call the measurement task. That would eliminate the widespread breakage, but introduces the danger that users and code authors may be surprised when aperture correction is not applied.    My suggested solution to that is to log a warning if aperture correction is disabled and yet aperture correction data is available. That warning works on the theory that False is a passive default. If the user intentionally disabled aperture correction then the user might be rightfully annoyed to get the warning.    A minor variant of that is to add a third value to doApplyApCorr: None means do NOT apply it (like False) but DO log a warning if you *could*  have applied it (ap corr data was available). That would then be the default.    Basically it comes down to the question of which is worse for users writing new tasks and unit tests:  - Having to explicitly set doApCorr False to avoid an exception  - Having to explicitly set doApCorr True to apply ap corr and possibly being surprised if they forget    Barring strong objections or a better solution I plan to change the default to False and not offer a warning.  ",NULL
RFC-74,"Move obs_decam under LSST umbrella","I propose that we move obs_decam under the lsst umbrella like obs_sdss. This entails:  	* Moving it to: github.com/lsst/obs_decam  	* Adding it to buildbot's build list  	* Pipe dream: Use a decam image in an integration test    Our current process is not efficient: one or two people keep up with every change to the stack and learn how to incorporate those changes into obs_decam  (or they fall behind and spend substantial time updating it).  In contrast, it is typically straightforward for the author of the original change to duplicate the update that they make on obs_sdss.    The repository is not ready to move yet. If this proposal is adopted, we would fix up the unit test that depends on the lost testdata_decam and add some more unit tests. (The most up to date copy is under github.com/yalsayyad/obs_decam)",NULL
RFC-75,"self-merge policy for lsstsw/etc/repos.yaml","I propose that a special-snowflake self-merge policy be adopted for https://github.com/lsst/lsstsw/blob/master/etc/repos.yaml on that basis that:    * this file is CI configuration data in a simple format; there is no logic for a reviewer to evaluate  * modifications being merged into this file are in the critical path for many development activities  * a lint script (DM-3151) is run automatically by travis CI on all pushes & PRs to this repository    Proposed policy:    A developer may merge change(s) to the repos.yaml file in the lsstsw repo without formal code review if a github PR has been opened for the change(s) and travis CI has returned a success status for that PR.    (see attached images of github's CI status flags)",NULL
RFC-76,"update activemq-cpp/apr/lua/mysq packages to el6/epel-6 versions","The following list of thirdparty packages are both present in el6 core repos or epel-6 and have a version lower than what is available from those sources. As el6 is currently set as the ""low bar"", I would like to update all of these eups packages to the el6 level.    {code:java}  activemq-cpp-devel 3.5.0 -> 3.8.2  apr 1.3.3 -> 1.3.9  apr-util 1.3.4 -> 1.3.9  lua-expat 1.1 -> 1.3.0  lua-socket 2.0.2 -> 3.0  mysql 5.1.72 -> 5.1.73  mysql-proxy 0.8.2 -> 0.8.5  {code}  ",NULL
RFC-77,"Upgrade the Java version on lsst-dev system from 1.6 to 1.8","8The Java version on lsst-dev is 1.6.0_35. We need to Java1.8 for Firefly compilation purpose. I would like to propose the Java upgrade to 1.8.     ",NULL
RFC-78,"Add API error-checking to coding standards","I suggest that we add:    bq. When an API is used that provides a means for returning an error indication, the invoking code shall perform a check for errors.      bq. In particular, when invoking POSIX library functions or system calls, the return value shall always be checked for an error condition, and if an error condition is found, the value of {{errno}} should generally be preserved or reported in some form, unless a specific value of {{errno}} is expected as part of the normal operation of an algorithm.    bq. *Rationale:* In general it is clear that if you invoke a function, you want to know whether it claims to have done what you asked.  It is true that in casual coding there are some commonly used library functions that will very rarely return an error, and there are traditions in various communities of not checking in these cases.  However, in a large production project it is preferable to make error-checking a blanket rule because even extremely unlikely error conditions have a way of happening at some point, and the error check may provide the only clue for the cause of a rare failure.    to the programming standards.  I am thinking primarily of C\+\+ but the principle applies _mutatis mutandi_ to Python.    ",NULL
RFC-79,"Third-party Packages for MPI","The port of the HSC parallelization framework (RFC-68, DM-2983) depends on mpi4py, which in turn depends on an MPI implementation.  To satisfy those dependencies, I propose the following:     - Provide {{mpich2}} 3.1.4 (the latest stable release) as a eupspkg installable third-party package.  The last approved version of is 1.05p4, but we haven't used it since we removed {{pex_harness}}.  I will mark this package as approved for automatic six-month uprev.     - Provide a dummy ""mpi"" package that contains only a dependency on {{mpich2}} and an sconsUtils {{.cfg}} (and that only until the implementation of RFC-69 is completed, presumably).  This can be overridden by users (via a manual {{eups declare}} and the {{manifest.remap}} mechanism) for systems where a different MPI library has already been installed.     - Provide {{mpi4py}} 1.3.1 (latest stable version) as a eupspkg installable third-party package, depending only on the ""mpi"" package.  This is an entirely new third-party package, and I will mark this package as approved for automatic six-month uprev.    {{ctrl_pool}} and its dependencies will be included in {{lsst_distrib}} but not {{lsst_apps}}.  Once a package containing drivers that utilize {{ctrl_pool}} it is created (to be called {{pipe_drivers}}), it will also only go in {{lsst_distrib}}, but since it will depend on nearly everything in {{lsst_apps}} (aside from {{obs_*}}), it will be useful as a direct install target itself for users who want to install it without installing the remainder of {{lsst_distrib}}.    *Please note the relatively short discussion time for this RFC; I expect it to be mostly noncontroversial and I'd like to get this done ASAP.*",NULL
RFC-8,"Revisit priorities in Jira, in particular the default","Default story in Jira is ""Major"". One step below is ""Minor"". A quick check reveals the distribution for DM stories is as follows: * 23 Blocker * 22 Critical * 1383 Major * 1196 Minor * 28 Trivial  It feels like we need something in between Major and Minor. Proposal: add ""Default"" (in between Major and Minor), and default to that.  It also feels like there is no real need to have ""Blocker"" and ""Critical"". So, how about we pick one and drop one? ",NULL
RFC-80,"Tech talk on C++11/14 needed","One or more experts should give a talk/training session on useful features and changes in C\+\+11/14 as supported by gcc 4.8.",NULL
RFC-81,"More pythonic API","The current APIs for many of the LSST python classes are essentially mirrors of the underlying C++ interfaces as made visible through Swig. Whilst this is logical and easily understandable for people who spend their time deep in the C++ layers it can be confusing and scary for people who spend most of their time in Python land and especially so for astronomers from the community who have been using Astropy.    Some examples that are commonly raised:    * Attributes do not look like Python attributes but always appear as explicit getter and setter methods.  * Camel case versus underscores for method naming.  * Constructors are black boxes and when the arguments are incorrect an incomprehensible error message is reported. Any help documentation reports C++ signatures, some of which are not actually implemented in the Python interface.  * Keyword arguments appear to work from the Python help but do not really work because the help reports the C++ interface but the python interface is (in general) {{\*args}} and does not really include {{\*\*kwargs}}.   * {{repr()}} sometimes returns C++ class names and a pointer.    I propose that we change our interfaces so that we no longer mirror the C++ view of the world. A test implementation of some AFW classes has been written and is available at: [https://github.com/lsst-dm/python-experiments]    In particular {{lsstx.geom}} and {{lsstx.image}} provide new interfaces for AFW Geom and Image. The approach taken here is to provide a shim layer that sits between the user and the Swig interface. There are a number of reasons for this approach:    * It is possible for the existing interface and the new interface to co-exist. This is important given the amount of code that relies on the old interfaces (not just in DM but also in Sims and external packages). The old interface will be deprecated when the first {{lsst_apps}} release is made that no longer uses it, and will be removed two releases later.  * The shim layer is easy to understand and experiment with. No knowledge of C++ is required.  * Once the Swig layer has been removed from the public interface the implementation layer can be changed at any time. This could involve using a different interface layer (such as boost.python) or rewriting a C++ component in Python.  * The shim layer could provide a means of automatically translating C++ native LSST implementations to Astropy implementations. This could involve new APIs explicitly designed to retrieve Astropy coordinate objects or Pandas data frames, or even going so far as making coordinate objects always be Astropy objects. The details of this are outside the scope of this initial RFC.    There is also the general issue that the documentation available to a Python user is minimalist at best and usually simply reports the C++ method signature. Documentation has to be obtained by reading the C++ Doxygen output. The documentation problem is out of scope of this RFC and will be the subject of another RFC later.",NULL
RFC-82,"Pre-meeting for End-to-End work at Bremerton","One of the key DM components for the LSST2015 meting in Bremerton is to attempt to have an end-to-end demonstration running for data release production and alert production. In addition we will attempt to calculate key performance metrics and compare them to the summer 15 cycle requirements.    I propose to have a meeting in the 11th August slot at 12:30pm Pacific to ensure that we will be prepared for the meeting. This will be an open discussion rather than a presentation.    Attached are two draft diagrams. One is an outline of the processing flow for DRP from raw data to SUI visualization. The other describes the hardware resources required at NCSA for the test.",NULL
RFC-83,"Rename pipe_tasks' ingest.py","The {{ingestImages.py}} bin script provides a camera-agnostic manner of creating a data repository (including a registry). The back-end code resides in pipe_tasks under the name {{ingest.py}}, and the {{IngestTask._DefaultName = ""ingest""}}, which means that configuration files in obs packages are also named {{ingest.py}}. This choice of name was unfortunate, as it may be confused with ingest of sources into the database. DM-3439 proposes changing {{ingest.py}} to {{ingestImages.py}} to reduce confusion.  Suggestions for better names are also welcome.    This will require coordinated changes to pipe_tasks, obs_subaru, obs_cfht and obs_decam.",NULL
RFC-84,"Is anything slowing you down?","I'd like to gather opinions on what, if anything, is getting in the way of people being productive.  I propose that people comment here or [in the new Discourse|http://community.lsst.org/t/rfc-84-what-is-slowing-you-down/56], send E-mails to me if they prefer to not speak in public, and that we perhaps use some time at Bremerton or an RFD slot to have further discussions.    Some possibilities to provoke thought:  * Too much bureaucracy with code review, branch mechanics, documentation requirements.  * (Related to the above) Unclear distinctions between fast prototype code and ""permanent"" quality code  * Unclear direction as to what is to be built and why  * Too many meetings  * Bombardment by communications  * Insufficient communication with key peers, leaders, customers  * Overly centralized design  * Widespread ""design veto"" powers  * Poor API design  * Poor documentation  * Lack of needed tools, datasets, hardware  * Insufficient training  * Slow decision-making  * T/CAMs or management not clearing barriers  * Planet rotates/revolves too quickly",NULL
RFC-85,"Adopting Discourse as an official DM communications platform","To better facilitate DM communications and community support, and following suggestions and input from DM members, we propose adoption of the Discourse forum platform. A demonstration forum is at http://community.lsst.org and is available right now for anyone to create an account and use. This RFC is to determine if the demonstration forum at community.lsst.org should be designated as an official DM communications channel.    h2. Background    Online communications underpin the efficiency of DM due to our geographical distribution and size. To date we use a myriad of tools to address various niches: JIRA, Confluence, Blue Jeans/Google Hangouts, HipChat, email lists, and to a minor extent, GitHub.    We have identified real pain-points with the current toolset that will likely get worse as the project evolves.    * *Our primary communications channel, HipChat, is built around a gatekeeper model.* This is a real impediment to communicating with people on the fringe of our collaboration. As a recent example, recall the discussions of how to put DES astronomers in contact with DM on HipChat. There was a real cost (money per user) that complicated this matter. *This will only get worse*. As the project grows, more stack users will want to be in contact with DM. We want to build a community, but that community won't fit on HipChat.  * *Technical support won't scale on HipChat.* When a new DM user or developer joins the fold, our admitted issues with documentation necessitates that they get real-time tech support on HipChat to get up an running. This works on a small scale, but as hiring ramps up, this type of repetitive tech support will become a drain on DM developers.  * *There is an unhealthy fear of missing out on HipChat.* Important, useful and consequential conversations happen on HipChat everyday, but if you're not in a HipChat room at that moment you're either not part of a decision making process, or worse, you're not aware of what's going on. We do have a policy where important decisions are summarized on `dm-devel`, but this is not always followed. The real-time stream design of HipChat makes it hard for multiple conversations to happen at the same time, or for conversations on the same topic that happen sporadically over several days to be linked together. HipChat can be searched, but search is often difficult with chat logs.  * *Email lists have terrible user experience.* Email lists are okay for real-time communications, but their archives are often useless. See https://lists.lsst.org/mailman/private/dm-devel/. There is no way to search the entire archive without browsing each month. ([Mailman encourages people to use Google|http://wiki.list.org/DOC/How%20do%20I%20make%20the%20archives%20searchable]), which is not an ideal UX either) This becomes an impediment to new users who want to be responsible and search for an answer before creating new and potentially repetitive traffic on a mailing list.  * *The information architecture of our confluence site is ill considered.* A new hire or user needs to do a lot of spelunking to find information. For example, the [Confluence Questions|https://confluence.lsstcorp.org/questions] site is only visible to those logged into Confluence. This makes no sense. Information that is hidden behind a login wall or in a wiki page are often neglected.    h2. Our proposed solution: community.lsst.org    community.lsst.org is built on the Discourse.org forum platform. Discourse is open source and built by the same team who brought us StackOverflow. We firmly believe this is the best tool that will foster the best content and experience.    We believe that community.lsst.org will absorb traffic some from HipChat, Confluence and the email lists, though this RFC is not to deprecate those services at this time. The technical features of the Discourse platform are:    * Discourse will be hosted by discourse.org, automatically giving us their CDN and security expertise. community.lsst.org will be far faster and more reliable than both HipChat and Confluence.  * Anyone can create an account, at any time, without any need for DM action. When someone downloads our open source stack and needs help, they can instantly become part of our community and get in touch, or just read the archives.  * Information is easy to find, whether it is being created now or was created months or years ago. Conversations are organized into topics that have useful titles. These topics can be tagged and are also organized into categories. It is trivial for a new user to simply browse the site by category or tag to get up to speed. Discourse is proactive in linking information by showing suggested topics and showing titles of linked topics in sidebars. Discourse also has great search.  * Discourse provides smart and user-customizable notifications. Anyone can watch/track categories or topics to be notified when there is new content. User '@' mentions can also draw people into conversations, like on GitHub or Twitter. It is also possible to subscribe via RSS to new topics. Finally, Discourse can email an activity digest to  anyone who has not logged on recently. There should be no fear of missing out on community.lsst.org.  * Discourse can replace email lists, as outlined in this help document: http://community.lsst.org/t/how-to-subscribe-to-emails-of-all-new-posts-in-categories-or-tags/37?u=jsick  * Discourse is an effective Q&A platform. We've created a Q&A category and enabled an 'accepted answer plugin.' When a question is answered, either the original poster or a moderator can mark the answer as accepted. This makes it easy for new users to find answers without any uncertainly.  * Discourse allows posts to be marked as 'wikis,' making it easy for members to curate how-to guides. Regular posts can also be edited by very trusted users and moderators. Change logs and diffs are easily available to anyone.  * Discourse can be navigated almost entirely with keyboard shortcuts. It is made for power users.  * The Discourse community will largely moderate itself. New users are given limited powers that will prevent spam. Users can also flag posts to alert moderators of problematic content or users. Five flags on a post automatically hide a post until it can be managed by a moderator.  * Discourse allows for private messaging either on a 1-1 or group basis.  * Discourse allows for permissions to be set on a per category basis. Groups can be granted 'private' categories as needed.  * It is easy to write beautiful content on a Discourse site. Markdown text and LaTeX math are allowed. Images can be dragged and dropped into a post.  * Discourse allows us to set explicitly community guidelines, terms of service, privacy policies and content licensing policies. We suggest content on community.lsst.org be permissively licensed CC-NC-SA.    We anticipate that over time a corpus of useful information will grow on community.lsst.org, particularly in the Q&A category. Just as StackOverflow is immensely useful to those who only search but don't ask questions, we expect the same of our community. community.lsst.org will nicely complement our more formal documentation efforts.    We also anticipate that this community will scale gracefully. Besides serving the needs of DM personnel, this community will serve the astronomy community as a whole in adopting and using our software and data. Although it is outside the scope of this RFC, we foresee that other groups within LSST, such as the science collaborations, may choose to participate in community.lsst.org. Thanks to Discourse's categories and user groups, this scope evolution can be gracefully accommodated.",NULL
RFC-86,"Remove --trace in pipe_tasks ArgumentParser; use --loglevel instead","I would like to remove the --trace argument from pipe_tasks' ArgumentParser and enhance the existing --loglevel argument so that it can be used as a replacement. In detail, --loglevel would be changed to accept multiple values, where each value could have either of two forms:  - A single value that sets the level of the default log; the value may be an integer or a string of the form ""info"".  This is what --loglevel currently does.  - A string of the form component=level, where level is as above. This is what --trace currently does, except that --trace requires a numeric value for the level, and the resulting log level is the negative of the specified trace level.    This will simplify the interface and simplify a future transition to a different logging system. I believe Trace has long been deprecated and it is unlikely to be supported by any replacement to pex_logging.",NULL
RFC-87," make hipchat history ephemeral","Hipchat is currently configured to keep message history ""forever"".  I feel that IM should be treated as fairly immediate communication mechanism which is more similar to a conference call rather than an indelible history.  In light of the discussion at the 2015 AHM about moving durable communication to other mediums (I.e., discourse), I propose that we limit hipchat's message retention to 30 days.",NULL
RFC-88,"Internal software and servers can be natively Python 3","As part of the discussion on RFC-60 at the Bremerton meeting it was noted that some packages of python code developed for the project would not need to support both Python 2.7 and Python 3 as there are no existing external users and no expectation for backwards compatibility.    This RFC proposes that teams may request that individual packages can be written using native Python 3 (minimum version of Python 3 is TBD but would be at least 3.4). Initially we will use a whitelisting approach where the list of Python 3 only packages are listed explicitly.    Obvious examples for whitelisting could be the SQuaRE {{jirakit}} package and the data access web services. Anything in {{lsst_distrib}} would not be eligible at this time.    An RFC should be issued for each package before dropping support for Python 2.  ",NULL
RFC-89,"Update wcslib to version 5.9","Currently we are using wcslib version 4.14 dating from 2012. Since then the library has been updated to handle more coordinate systems and the distortions from WCS ""Paper IV"". This includes TPV, which we would like to have because it is being used by the Decam community pipeline in their output images.     The update is pretty simple as the API has apparently not changed. afw compiles with the new version without modification, and with some fixes I have it returning coordinates on decam images.    This was previously discussed in RFC-76, and it was agreed to separate the wcslib update from that RFC, possibly into a general update of afw::image::Wcs. I think we can go to 5.9 and get some of the benefit before doing the refactoring.    (Thanks to Frossie for the prompting the RFC.)",NULL
RFC-9,"new interface for configuring source slots","The ""slot"" mechanism for identifying default measurements in a particular category (e.g. centroid, shape, aperture flux) is currently configured by a simple flat Config class, in which a single string config field for each slot holds the table field name that contains the measurement to be used, and any subfields are determined strictly by the naming conventions for typical measurements.  For example, to use the SincFlux and SdssCentroid algorithms, you'd say: {code:hide-linenum} root.measurement.slots.apFlux = ""base_SincFlux"" # uses ""_flux"", ""_fluxSigma"" subfields root.measurement.slots.centroid = ""base_SdssCentroid""  # uses ""_x"", ""_y"", ""_xSigma"", ""_ySigma"", ""_x_y_Cov"" subfields {code} This has several drawbacks: 1. The names that must be used are *usually* the names of plugins, but sometimes are not, when the value to be used is nested.  This can be confusing, and can cause spurious errors in config validation, which attempts to check that all slot algorithms are actually being run. 2. Some algorithms cannot use the standard naming conventions, and hence cannot be used in slots at all - this is a particular problem for array-valued aperture fluxes, which cannot be used in the aperture flux slot (see DM-1218). 3. The names in configuration differ in capitalization from the slot names that appear elsewhere in the codebase (""apFlux"" vs. ""ApFlux"").  These drawbacks apply only to the configuration interface; the actual mechanism that controls the slots (accessible from regular Python or C++ code) is already much more flexible, but much more verbose to use.  I propose changing the slot configuration system to use ConfigDictField to address these problems, so the new configuration interface would look more like this: {code:hide-linenum} root.measurement.slots[""Centroid""].plugin = ""base_SdssShape"" root.measurement.slots[""ApFlux""].plugin = ""base_SincFlux"" {code} or, for an array-valued flux algorithm: {code:hide-linenum} root.measurement.slots[""ApFlux""].plugin = ""base_CircularApertureFlux"" root.measurement.slots[""ApFlux""].extra = {""index"": 0} {code} In this approach, it will be the responsibility of the plugins themselves to setup the Schema aliases that control the slots; the .plugin config field sets the name of the responsible plugin, and .extra field contains additional keyword arguments to pass to the plugin class method that actually does the work of defining the slot.  These keyword arguments are plugin-dependent, but will be unnecessary for the vast majority of algorithms.  In addition, because the CircularApertureFlux algorithm can do everything the NaiveFlux and SincFlux algorithms can do aside from participate in slots at present, I plan to remove these after the changes to the slot mechanism are complete.",NULL
RFC-90,"Cost updates to Site Specific Infrastructure Estimation (LDM-144) and Explanation (LDM-143)","This RFC is to solicit feedback and approval for changes to LDM-143 and LDM-144 such that these latest versions can become preferred versions. This is part of a 6-month, reoccurring update to these documents. Note that the technology update occurs on a 12-month cycle (which this isn't). This have been checked into docushare (not the preferred version). This was a pricing update not a technology update and as such I found pricing on equivalent components when possible. When not possible (eg component is no longer available(cpu)) I did find a comparable component and priced it. There may be other comments of interest in DM-3623.",NULL
RFC-91,"Allow all C++11/14 features modulo compiler and SWIG","While we have had [an RFC|https://jira.lsstcorp.org/browse/RFC-56] on updating the minimum gcc compiler level, we have not had a formal one on adopting C++11/14 fully.    I propose to update the [Policy on C++11|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399] and the [C++ coding standards|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908756] as follows:  * Standard 2-2: change from ""We are writing C\+\+03 with some C\+\+11 extensions"" to ""We are writing C\+\+11/14 with some restrictions"".  * Policy: change from:  {quote}  Unfortunately, only certain features are supported by the default compiler provided with the oldest operating system distribution commonly used to install the LSST Stack.  Furthermore, we have not extensively tested SWIG support for C++11/14 features. As a result, a limited list of such features is permitted to be used. These features may be used at will in .cc implementation files.  {quote}  to  {quote}  C++11/14 features supported by the default compiler provided with the oldest operating system distribution commonly used to install the LSST Stack, currently gcc 4.8.3, may be used at will in .cc implementation files.  {quote}  * Delete the ""List of permitted features"".  * Add a ""Background Information"" item pointing to [the instructions for using the devtoolset-3 Software Collection|https://confluence.lsstcorp.org/display/LDMDG/Developer+Tools+at+NCSA#DeveloperToolsatNCSA-AlternateDevelopmentEnvironment].    Eventually a list of C++11/14 best practices will likely need to be developed (e.g. use uniform initialization), and the coding standards may be needed to adjusted accordingly.",NULL
RFC-92,"support SCONSUTILS_FLAGS to augment SCONSFLAGS","Many users current set the environment variable {{SCONSFLAGS}} to automatically include certain command-line arguments when running SCons.  This environment variable affects all SCons scripts, however, not just those written with our sconsUtils package, and most users who set {{SCONSFLAGS}} typically include sconsUtils-specific options.  This causes problems when building packages that use SCons without sconsUtils (such as GalSim, or, at least until DM-3749, psfex).    Instead, sconsUtils read a new environment variable for sconsUtils-specific options; I'm proposing we call that {{SCONSUTILSFLAGS}} (though the lack of word separators is annoying, that seems most analogous with {{SCONSFLAGS}}).  When this is available, users should use this new environment variable for sconsUtils-specific options like ""opt=3"", while continuing to use {{SCONSFLAGS}} for generic options like ""-j4"".",NULL
RFC-93,"Whitelist code in lsst-sqre as Python 3 only permitted. ","  Following the adoption of RFC-88, I request that code in the lsst-sqre Github organisation be whiltelisted as being allowed to be Python 3 only.     The repositories in lsst-sqre (where they are not experimental or forks) are:    - Server-side and deployment code for SQuaRE services (so far not in python, so it's a bit moot)    - Standalone helper utilities such as jirakit and codekit aimed at people like T/CAMs, who can be assisted with setting up virtualenv to run them under python3 if they need assistance.     No repository in lsst-sqre is a dependency of anything in lsst_distrib.     ",NULL
RFC-94,"Move astrometry_net wrapper code out of meas_astrom","As per DM-2186 we would like to move our {{astrometry_net}} wrapper code to a separate package. This is a step towards our long-term goal of making {{astrometry_net}} optional. The code to be moved includes {{LoadAstrometryNetObjectsTask}}, {{ANetAstrometryTask}}, {{ANetBasicAstrometryTask}} and low-level wrapper code.    h3. Package Name    I propose naming the new package {{meas_extensions_anet}}, and will use that name in the rest of this proposal. Alternatives I considered include:  - {{meas_astrometry_net}}: too easily confused with the existing {{meas_astrom}} package  - {{meas_extensions_astrometry_net}}: too long  - {{meas_anet}}: too vague (not that {{meas_extensions_anet}} is much clearer)    If you think of a better name, please suggest it.    h3. Dependencies    Detangling the dependencies will require some work. {{meas_astrom}} {{AstrometryTask}} uses {{LoadAstrometryNetObjectsTask}}, because we have no good alternative loader for reference objects. Thus {{meas_astrom}} must depend on {{meas_extensions_anet}}, for now. However, {{ANetAstrometryTask}} and {{ANetBasicAstrometryTask}} both use {{meas_astrom}}'s low-level TAN-SIP WCS fitter.    To break this circular dependency I propose moving {{FitTanSipWcsTask}} and its underlying C++ code to {{meas_astrom}}. This leaves {{meas_astrom}} dependent on {{meas_extensions_anet}} until we have a new reference object loader, but I see no way to avoid that.    h3. Summary    The net result is:  - {{FitTanSipWcsTask}} and its C++ code moves to {{meas_astrom}}.  - {{ANetAstrometryTask}}, {{ANetBasicAstrometryTask}}, {{LoadAstrometryNetObjects}} and associated C++ code all move to the new {{meas_extensions_anet}} package.    I further propose removing ""ANet"" from the astrometry task names, because ""ANet"" is implied by the package name. However, I prefer to retain the name {{LoadAstrometryNetObjects}} because the name tells users what kind of reference data it loads, and that information is crucial.    A reasonable alternative is to delay splitting the packages until we have a new object loader, so {{meas_astrom}} can be entirely independent of {{meas_extensions_anet}}. However, even if we do this we will either have to move the WCS fitter code or make {{meas_extensions_anet}} depend on {{meas_astrom}}.    h3. Future concerns    We plan to implement a new way of saving and loading reference catalogs using a simpler format than {{astrometry_net}} index files. The question is whether we will be satisfied to move to this new format as our default, or if we are likely to end up regularly using multiple different reference catalog formats. In the latter case we will probably want to simplify the loading process by making a loader that can handle all our standard catalog formats.  ",NULL
RFC-95,"Verification datasets: filesystem organization and argument parser features","This proposal includes three related topics:  • How to organize verification datasets, preprocessing from external pipelines, and outputs from our own pipeline on disk.  • Another try at the --rerun convenience argument for the pipe_base argument parser.  • Functionality in pipe_base/daf_butlerUtils for handling preprocessing from external pipelines.    First off, I think we need to put all of our datasets in a central location (rather than multiple /lsstN volumes), or at least make it appear that way using symlinks.  I'm hoping we can eventually just have a large distributed filesystem that can include everything, but I think it's important to make it appear that way even before that's feasible.  Thus, for some (short, but otherwise unimportant) value of {{$root}}, we'd have a layout like this:  {code:hide-linenum}  $root/cfht  $root/decam  $root/sdss  $root/hsc  $root/lsstSim  {code}  The subdirectories are per-camera; everything below these directories should use the same mapper and obs package.  Where possible, the name should reflect the name of the obs package.    For real data, the {{_mapper}} and {{registry.sqlite}} files would go directly in the per-camera directory; there'd be only one input data repository and one registry for all data from that camera (no splitting it up based on survey, proposal, or patch of sky - managing those differences is what the registry is for).  The rest of the directory structure below that level (with some exceptions below) would be managed by the mapper for that camera.    For simulated data, we need another level before we get to the data repository root, because we can have multiple realizations of the sky that should never be processed together.  I propose we create a JIRA issue for each distinct simulation run, and form directory names from the issue name and optionally the date (e.g. ""DM-NNNN(_YYYYMMDD)?"", but I don't care about the details of the formatting).   Furthermore, we should distinguish between ""public"" simulation runs that have a wide audience and ""private"" runs used only by a small group of people with a particular goal in mind (""private"" != ""secret"").  Public simulation directories should always have a date, go directly within the camera-level directory, should be publicized somehow when they're created, and should require an RFC to remove.  Private simulation directories should go under an additional username-based subdirectory, and may be removed by that user whenever desired, and may or may not have a date.  For example:  {code:hide-linenum}  $root/lsstSim/DM-4567_20160708   # public simulation  $root/lsstSim/krughoff/DM-5678  # private simulation created by Simon  {code}  For public simulation runs that are important enough to have a widely-recognized name of their own, the directory can use this name instead of an issue number (though I think the date should still generally be included).    Data products that are produced by an external pipeline that may be used in our own processing (e.g. DECam community pipeline ISR or SDSS Photo PSF models) should go within a ""preprocessed"" subdirectory in the camera-level directory and additional directories below that to indicate both the source and the version of the processing.  These additional directories have camera-dependent names.  For instance:  {code:hide-linenum}  $root/decam/preprocessed/cp/20150519  $root/sdss/preprocessed/dr7  {code}  All such directories for a particular camera should have the same layout below this level; they'll be treated the same as the root of the raw data repository by the mapper, with this path set by a new {{\-\-preprocessed}} command-line argument in the pipe_base argument parser (similar to the way we use {{\-\-calib}} to specify the calibration root directory now).  Like {{\-\-calib}}, we'll have environment variables and a search path to provide the default when the argument is not provided.    Outputs from our own pipeline should *usually* go within the camera-level root repository, in a subdirectory of a ""rerun"" directory there.  Like the simulation runs, these should *usually* be named using JIRA issue numbers and dates, again with a distinction between public and private reruns. For example:  {code:hide-linenum}  $root/hsc/rerun/DM-3456_20150607     # a public rerun useful for many people  $root/hsc/rerun/jbosch/DM-7890/fixed-psf   # one of several private reruns used to debug a single issue  $root/lsstSim/krughoff/DM-5678/rerun/jbosch/DM-5679   # simulated by Simon, processed by Jim  {code}  Once we have CI-generated reruns, these should go within e.g. {{rerun/weekly}} with a TBD naming convention.  Unlike simulation runs, for which issues are generally created just to document the run, most private rerun issues will refer to an issue that is mostly about something else, for which the rerun is just part of testing/resolving the issue.  As with any other output repository, reruns can be chained, but the name of the parent rerun is *not* generally recorded in the name of the child.    To better support this convention, I propose we bring over the {{\-\-rerun}} argument from the HSC fork.  This serves the same purpose as the {{\-\-output}} argument (which will be retained), but the value supplied to {{\-\-rerun}} is relative to the ""rerun"" directory of the root input data repository (i.e. we follow the _parent chain all the way back, then add ""rerun"" to that).  This saves the user from typing in the input repository twice.  Unlike {{\-\-output}}, {{\-\-rerun}} also supports a two-value form separated by a colon, which specifies both a parent rerun and the new output directory, making chaining much easier.  For example:  {code:hide-linenum}  processCcd.py $root/hsc --rerun price/DM-5680 ...           # Paul processes some CCDs  makeCoaddTempExp.py $root/hsc --rerun price/DM-5680:jbosch/DM-5681 ...   # Jim uses Paul's CCD processing to start a coadd  {code}  Compare that to:  {code:hide-linenum}  processCcd.py $root/hsc --output $root/hsc/rerun/price/DM-5680 ...  makeCoaddTempExp.py $root/hsc/rerun/price/DM-5680 --output $root/hsc/rerun/jbosch/DM-5681 ...  {code}  While we considered adding {{\-\-rerun}} a long time ago, and we couldn't reach a decision on whether it was valuable, I think it certainly has been on the HSC side; we essentially always use it, and never use {{\-\-output}}.  And we've had a much easier time of sharing and making use of each other's outputs as a result.  But this is coupled with the directory structure above; without those conventions, {{\-\-rerun}} isn't much use, so it's not at all surprising that it wasn't unanimously considered valuable on the LSST side in the past.",NULL
RFC-96,"Iostream-style formatting in log package","Current implementation of the log package provides two ways to format messages:  - via sprintf-type formatting, e.g. {{LOG_INFO(""ra = %f, decl = %f"", ra, decl);}}  - via boost::format-like syntax, e.g. {{LOGF_INFO(""ra = %1%, decl = %2%"" % ra % decl);}}    Both of those approaches suffer from the common problem - they are not type safe which can lead to run-time crashes. It is too easy to mess format string in a ways that are hard to detect during code review and it's not always possible to unit-test every generated message produced by the code. This drawback makes developers write code like this:  {code:cpp}      if (LOG_CHECK_DEBUG()) {          std::ostrstream str;          str << ""ra = "" << ra << "", decl = "" << decl;          LOGF_DEBUG(""%1%"" % str.str());      }  {code}  which is safer but both non-efficient and unnecessary.    The proposal is to extend log interfaces to support type-safe iostream-style formatting. This can possibly be done in one of the two ways:  - by introducing new set of macros with names like LOGS_DEBUG, LOGS_INFO, etc.  -- logging message will look like {{LOGS_INFO(""ra = "" << ra << "", decl = "" << decl);}}  -- drawback here is that there is already large set of macros defined in Log.h, extending this set may make it harder to remember  - by reusing existing macro names, basically creating ""macro overloads"", e.g. re-using LOG_INFO one could write {{LOG_INFO(""ra = "" << ra << "", decl = "" << decl);}}  -- logic here would be: if LOG_INFO has a single argument then treat it as an expression for stream insertion, otherwise do sprintf-like formatting.  -- (I believe this can be done with not-too-complicated preprocessing magic)    Since there are actually two proposals here, please vote for one of these options:  - no extension necessary  - prefer new macro names for new syntax (LOGS(), LOGS_DEBUG(), etc.)  - prefer re-using existing macro (LOG(), LOG_DEBUG(), etc.)    I personally prefer latter option to keep the number of macro names to remember at reasonable minimum.  ",NULL
RFC-97,"Allow PEP8 naming conventions in python style guide","The [LSST DM Python Style Guide|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard] mostly jives with PEP8, although it recommends camelCase over lowercase_with_underscores for methods, class attributes, and module names. This is contradictory with PEP8, which recommends that camelCase convention only be used when backwards compatibility is an issue.    When a software product allows (e.g. pure python and no SWIG), I'd like to recommend we allow, and possibly encourage, PEP8 naming conventions.",NULL
RFC-98,"Rename or redefine assembleCoadd to make way for safe clipping","I am in the process of porting, from HSC, a new framework for assembling coadds (DM-2915). This framework attempts to do clipping in a smarter and safer way. It attempts to avoid clipping any star (real object), as the objects would then not match the corresponding psf model.    The new tasked, called SafeClipAssembleCoaddTask, is a new class which inherits from AssembleCoaddTask. I spoke with the original author of the code and asked why the safe clipping was implemented in this manor and not as an option within the original AssembleCoaddTask. The consensus seems to be that even though this routine currently seems to give better results when processing HSC data, as compared to AssembleCoaddTask, it is not the final long term solution that should be adopted. As such it seemed that having AssembleCoaddTask as a base class would make it easier to design and implement a different solution in the future rather than try and pull out just the bits related to the current safe clipping.     SafeClipAssembleCoaddTask will also have a new corresponding command line task to run when creating a coadd. If a coadd is created with this method (which is the new preferred way) it will add a mask plane to indicate what is clipped. The PixelFlags algorithm will then, by default look at this plane and set corresponding pixel flags within a catalog measured on the coadd. If measurements (which write pixelFlags based on the coadd mask plane) are done on a coadd which was produced with the old AssembleCoaddTask using the default configuration, they will fail, as the expected mask plane will not be present.    That leads us to the RFC at hand, what to do about the currently existing AssembleCoadd command line task. As we want users to use the new code (and the old code will not work without a configuration change), we somehow need to indicate this to the user.     We could change the name of the current AssembleCoadd command line task to something like AssembleCoaddDeprecated, and have the new code referred to by SafeClipAssembleCoadd.     We could replace AssembleCoadd entirely with the new safe clip command line task.    Like the last option we could rename the class AssembleCoaddTask to BaseAssembleCoaddTask to indicate that it is what other assemblers should inherit. In this case SafeClipAssembleCoaddTask would become AssembleCoaddTask, and the command line task would be the same. I tend to not favor this one, as potentially we may have multiple versions of coadd assembly at some point with different methods as we test which is best long term (or there may not be a best for every situation). In this case we could still rename AssembleCoaddTask to BaseAssembleCoaddTask, but each of the derived classes would have their own name and command line task.    I am a little wary of removing the ability to run the existing AssembleCoaddTask (with the command line task) completely (at least in the short term) as I can imagine people still needing to run it for various reasons, and there is no reason you could not, providing the appropriate changes to measurement configs. I therefore am leaning to simply renaming the command line task in the short term, to make it easy to run both methods of assembling a coadd. In the long term if this proves to be the way we want to go, we can at that point rename the base class to something with base in it, and have command line tasks just for the derived classes.",NULL
RFC-99,"Stack tagging / versioning convention","h1. Version RFC    h2. Scope    The following naming scheme is applied to Science Pipeline Stack repos, currently defined as:    - any repo explicitly pulled in by the lsst_distrib TLP  - any repo belonging to LSST:DM Auxiliaries on Github  - any repo belonging to LSST:DM Externals on Github    Other TLPs for DM teams are welcome to use these conventions (or in the case of the official releases, may have these conventions applied) by appending their TLP to the suggested tag. Eg. r.Summer2015.qserv or r.Winter2015.firefly. Non-DM teams are welcome to adopt these conventions (eg r.Winter2015.sims). Teams that adopt these conventions can request that SQuaRE apply these tags if they so wish.     h2. Reserved namespaces for administrative releases: r    Permanent (won't be deleted) git tags for end-of-cycle administrative releases reserve the r namespace for the cycle name. Eg.    - r.Summer2015  - r.Winter2016    Note: PMCS/LDM-240 will be changed to refer to these release names in lieu of the current 5.0 6.0 etc numeric nonmenclature) for consistency and traceability.    h2. Reserved ""namespaces"" for engineering releases: w/m/d    These namespaces will refer to:    1. Impermanent (see below) git tags  2. Impermanent (see below) distributions    Examples:    - Weeklies: w.YYYY.WW eg w.2015.37 is 37th week of 2015  - Monthlies: m.YYYY.MM   - Dailies: d.YYYY.MM.DD    These will be published on the EUPS distrib server as w_2015_37 etc.      These will NOT be EUPS product versions (the latest version of EUPS with Mario's fix to ignore them will be installed on lsst-dev). They will be pruned to reduce noise on a timescale consistent with their granularity - eg no more than six months of monthlies.    h2.  Numeric (Public) Releases: v.N.n and N.n    Right now there is a numeric release scheme; currently this a non-semantic representation mapping to cycle and number of release in cycle. For example: 11.0 is the first release of W16, 11.1 is the second release in that semester, etc.    Without getting into the merits of numeric release versions or their convention, there are two git tags for numeric releases, bareword and prepended with a v. For releases aimed at external (non-DM) consumption, both will be applied: v.11.0 and v11.0.    NOTE: In this RFC, the bare numeric tag (11.0) is the only one that is reflected as an EUPS product version. The rest are ""invisible"" to EUPS (though will be used for eups distrb install publication, as in weeklies). In the future, internal packages may go to different bareword numbering systems (e.g. semver or YYYY_MM).      h2.  Candidate releases for Official Releases: *_rcN    Candidate release conventions are tied into the SQuaRE's release preparation process and so may change arbitrarily, as they are evanescent and largly for internal consumption. They are tagged on a naming scheme     {code:java}  {bare numeric version}_rc{candidate number}   {code}  - do not apply these tags without direction from SQuaRE.      h2.  Other schemes    Groups are free to make tags as useful to them, provided they don't stomp on the conventions above. Each product could adopt its own monotonically-increasing numeric bareword tagging scheme, but the   {code:java}   [dmrvw].  {code}    space is reserved for the above - including   {code:java}  [dmrvw].*.{product}  {code}  ",NULL
