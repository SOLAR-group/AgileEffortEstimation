"issuekey","created","title","description","storypoint"
"DM-9","03/05/2014 09:49:35","Open up LSST software mailing lists","We all benefit from making LSST software development as open as possible and conducive to outside volunteer contributions (*). One way to increase community involvement is to open up our development mailing lists to the public, analogous to the way other open source projects do. For example, we could have:  * software-devel@lsstcorp.org: the development mailing list, equivalent to current lsst-data * software-users@lsstcorp.org: the users mailing list, equivalent to current lsst-dm-stack-users mailing list (but it could possibly be replaced by StackOverflow/Confluence Questions) * lsst-dm@lsstcorp.org: internal, DM-staff only mailing list, for the *rare* discussions/notices that should go out to staff only.  (*) Though we don't rely on them for meeting the project specs (legally required disclaimer :) ).",1
"DM-52","03/07/2014 21:53:05","Qserv configuration - detailed design","Detailed design covering how all Qserv components will be configured for runtime. ",3
"DM-68","03/07/2014 22:31:39","Qserv should fail when table not registered in CSS","""Table does not exist"" message from CSS is currently ignored and query proceeds. Example, a query ""select count(*) from t"" where t does not exist will successfully return 0 rows.",0
"DM-71","03/07/2014 22:37:23","Data Distribution Design v1","Need to come up with detailed design covering how we will deal with data distribution: managing multiple replicas, recovering from faults, adding new nodes to the cluster, registering new data from L2 ingest and user data (L3).",8
"DM-78","03/11/2014 10:48:10","Save a git-branch when a forced push is detected","Create a gitolite hook that will save a branch when a forced push is detected.  E.g., if we have a ticket: 'tickets/DM-AAAA' and someone rebases it and pushes  with '--force' before applying the update --- then the hook will branch off the old state into (say):  backups/tickets/DM-AAAA/NNNN where NNNN is a monotonically increasing number (per branch).",1
"DM-98","03/13/2014 12:59:23","clean up isr utility code","There is some commented code in isr.py.  This should be removed or updated so that it works.",2
"DM-148","03/13/2014 15:29:35","Improve naming of getters in AmpInfoTable","The names of the methods to get values from a record on AmpInfoCatalog are potentially confusing.    This is because the convention is to call the getters get[attributename].  We could change the method names in the AmpInfoCatalog, or add methods in the SWIG wrapper.",1
"DM-199","03/14/2014 10:23:59","Develop new master-worker result system ","Reimplement how results are returned from worker to the czar. Currently it relies on mysqldump, which is fairly inefficient. This is related to DMTF-1650-045",20
"DM-202","03/14/2014 10:30:34","Qserv: unit testing (query execution) ","Design and build toy prototype of a test framework for testing query execution module. This might require a mock framework, as we want to be able to test things in isolation, without testing everything around the query execution module at the same time. This is related to DMTF-16570-21.",8
"DM-213","03/14/2014 14:36:50","Setup multi-node testbed","It'd be useful to test Qserv using Winter2014 or Summer2014 data set on a multi-node cluster, just to exercise all pieces of the software and double check we are not missing anything.",5
"DM-224","03/14/2014 14:57:43","Switch to MariaDB","We should switch Qserv to the MariaDB Foundation based MySQL.",3
"DM-228","03/14/2014 20:30:31","Setup dev test environment","Setup whole Qserv environment, including installing data set, and validate it by running some simple queries. Suggest changes/improvements as appropriate.",8
"DM-242","03/16/2014 07:11:32","switch from '.' to '_' in afw::table fields","We've been mapping '.' to '_' in afw::table I/O, which unnecessarily complicates lots of things.  We'd like to switch to using '_' in the field names themselves, which requires ending this mapping in I/O, but we need to be backwards compatible.  So we'll add a version to the FITS headers, and continue the mapping if the version is not present or is less than some value.  Until we do this, the new field names being used in meas_base won't round-trip.",3
"DM-271","03/19/2014 15:49:27","Setup the new Buildbot CI system","Setup the Buildbot 0.8.8 testbed for the DM environment.  This includes: (1) setting up slaves on the set of common OS on which the DM stack runs; (2) creating a new continuous integration slave using the new eupsPkg-based build and distribution support,  Definition of done: * Every git change of master should trigger a build of master * If a build failed, an e-mail will be sent to lsst-data (if the build succeeds, nothing happens) * Failures due to internal buildbot issues (e.g., config problems, transient system availability issues, timeouts, etc.) should go to the buildbot owner. * Allow user-triggered builds via web page (with specified refs to be built), with a common user (until we get LSSTC LDAP directory hooked up). It's understood that locking may not be fully implemented/fleshed out in this story. * It should be possible for a user on lsst-dev to easily setup the stack for either a failed or a successful build. ",100
"DM-272","03/19/2014 16:03:50","Move TCT-relevant  twiki documentation to Confluence","Congregate all the trac TCT-relevant documents (standards, policies, guidelines, meeting history) onto Confluence.",2
"DM-273","03/19/2014 16:23:28","Develop and then create the organizational structure for DM Confluence space","Before we start populating the DM Confluence space with active pages, we should define an overall organizational structure/taxonomy.",1
"DM-278","03/19/2014 22:16:00","Improve handling errors occuring in AsyncQueryManager","AsyncQueryManager is initialized based on configuration file, if the configuration is invalid, an exception should be thrown (eg in _readConfig()) and gracefully handled upstream.",1
"DM-280","03/20/2014 10:18:58","clean up multiple aperture photometry code","I've been doing some minor work on the HSC-side ApertureFlux algorithm, and I wanted to record some concerns here (from both me and RHL) that should be addressed in the new meas_base version:  - We should consider merging ApertureFlux and EllipticalApertureFlux into the same algorithm (with a config field to choose whether to use elliptical apertures).  We could still register it twice, with a different default config value, and this should eliminate a lot of code duplication.  We could also consider having them inherit from a common base class (instead of having EllipticalApertureFlux inherit from ApertureFlux, as is done now).  - We should test that the threshold at which we switch from Sinc to naive apertures is obeyed exactly.  - We should create a flag for the failure mode in which an aperture cannot be measured because we go off the edge of the image, and test that it appears at the right point.  If possible, we should set this flag and measure what area we can within that aperture, instead of just bailing out.  - (Somewhat off-topic) We should consider having utility functions on SourceSlotConfig to set all slots to None for use in unit tests.",5
"DM-290","03/21/2014 17:40:19","Eliminate dependence of query analysis on parser and antlr","I would like to write and compile query analyzer code completely independently of the parser and ANTLR (transitively). This doesn't seem to work right now. This is not currently possible.  This might take any where from a day to a week. (I'm not sure if we can finish anything in half a day, if you include the testing, review, feedback, and revision process, but perhaps unit testing will make that faster).  Updates to follow after the scope is estimated.  Dependencies to be broken: query --> parser, antlr (due to predicate depending on antlr nodes) qana --> parser, antlr ",8
"DM-296","03/22/2014 16:51:48","fix namespaces in all Qserv core modules","This was suggested by the code review for ticket 1945 (https://dev.lsstcorp.org/trac/wiki/SAT/CodeReviews/1945), pasted below:  common/src/*:  While it's not required by the coding standards, I'm a big proponent of using namespace scopes in .cc files, which usually save you from needing namespace aliases and will certainly save you from having prefix every declaration with qserv::.  At some point I'd recommend changing the header file extension from .hh to .h to match the rest of the LSST DM code, unless it's a big backwards compatibility issue.  (transferred from trac ticket 2528)",5
"DM-298","03/22/2014 17:03:03","restarting mysqld breaks qserv","Restaring mysqld results in unusable qserv (even if the restart happens when qserv is completely idle). The error message is:  ERROR 2013 (HY000): Lost connection to MySQL server during query This happens most likely because qserv caches the connection, which becomes invalid when server is restarted. I am guessing the same will happen when there is a long period of inactivity (the connection times out).  (transferred from trac 2853)",1
"DM-309","03/24/2014 11:54:05","Jira for Qserv","Jira setup for Qserv, includes things like adding new tasks, transferring tasks from trac, epic/story/task division, assigning story points, setting scrum board, just learning things and more...",8
"DM-313","03/24/2014 13:34:00","cleanup includes in Qserv core modules","Includes need cleanup: group into standard lib, boots and local, sort as appropriate etc. Also, unify forward declarations.",2
"DM-321","03/25/2014 14:07:02","Re-think thread.cc and dispatcher.cc python interface","The mess of thread.cc and dispatcher.cc need to be re-thought and re-designed so that the interface is smaller and more obvious.  ",2
"DM-322","03/25/2014 15:12:41","Trim python importing by czar in app.py","Clean up the way modules are imported in qserv master, use relative import when appropriate instead of lsst.qserv.master.<package>   (migrated from Trac #2369)",2
"DM-326","03/26/2014 12:42:16","Libraries being built in lib64 on OpenSUSE, when EUPS tables assume lib","A report from Darko Jevremovic <darko@aob.rs>: {quote} Hi Mario,  I managed to build stack v8 on OpenSuse13.1  There were standard problems with lib/lib64 - namely system builds libraries in $PREFIX/lib64 and some programs are hard wired for $PREFIX/lib  if you could  change the last line of  mysqlclient-5.1.65+3/ups/eupspkg.cfg.sh  from  (cd $PREFIX/lib && ln -s mysql/* . )  to  ( cd $PREFIX && if [ ! -f ""lib"" ] ; then  ln -sf lib64 lib; fi &&cd $PREFIX/lib && ln -s mysql/* . )  or something along that line (am not sure whether the syntax would  work).  Also if you could add  in the same manner to ups/eupspkg.cfg.sh  ( cd $PREFIX && if [ ! -f ""lib"" ] ; then  ln -sf lib64 lib; fi)  for the following packages:  minuit2 gsl cfitsio wcslib {quote} ",1
"DM-334","03/28/2014 12:16:46","Cut Qserv release","It'd be very useful to have fully functioning Qserv release with the latest set of changes (build, packaging, CSS, Daniel's fixes etc) during the Hackathon week.",2
"DM-335","03/28/2014 14:04:02","Migrate std::lists to std::vectors","Suggested by Andy when reviewing DM-296, discussed at Qserv mtg 3/27.  std::list --> std::vector  * why? Default now is vector, iterating over vector is much more efficient than over list  * revisit on case by case bases, do not blindly replace  * preferred solution: typedef, and name it in a way that conveys the intent (e.g., might call it a ""container""), underneath use vector",8
"DM-337","03/28/2014 14:06:55","removed dead code in stringUtil.h","Remove obsolete strToDoubleFunc (and more) in util/stringUtil.h.",1
"DM-354","03/31/2014 13:45:06","Add cameraGeom overview to Doxygen documentation","The CameraGeom package needs an overview page (part of afw's main.dox) as part of the Doxygen documentation. I think it's up to Simon or me to add this.",2
"DM-355","03/31/2014 15:00:36","Install and tag multiple Qserv versions on the same distserver","Done in DM-366",0.5
"DM-365","04/01/2014 16:02:27","Integration tests dataset should be packaged in eupspkg","A qserv-integration-tests package should be created : - it would allow to manage easily, in ups/qserv.table, tests version for a given Qserv version. - it would allow to install Qserv dependencies related to testing, like partition (and other data ingest code which may arrive.",3
"DM-366","04/01/2014 16:05:42","Refactor install/distribution procedures using lsst-sw"," Here's Andy Salnikov remark, during #3100 review : https://dev.lsstcorp.org/trac/ticket/3100#comment:18",5
"DM-370","04/01/2014 23:16:44","improved how default values for CSS are handled","Need to improve how defaults are handled in qserv_admin. There seems to be some desire to warn when values are not set--how about setting defaults and just printing what configuration is being used? If this is something human-created, we should have reasonable defaults and not bother the user, unless no default is viable. I think we should only be strict on machine-generated input, where we would like to catch bugs as soon as possible.   (This came up in the review of DM-56, the review comments are captured in DM-225)",5
"DM-372","04/02/2014 09:55:07","fix testQueryAnalysis","5 tests fail in the testCppParser.",2
"DM-380","04/03/2014 13:47:41","loadLSST bug(s) for csh, ksh","A flaw in the v8.0 loadLSST scripts (and/or in eups/bin/setups) causes the following errors:     1) When using ksh:   {code}     $INSTALL_DIR/loadLSST.ksh     ksh: /path/to/INSTALL_DIR/eups/bin/setups.ksh: cannot open [No such file or directory]  {code}  And indeed, there is no eups/bin/setups.ksh file.     2) When attempting to run the installation demo (v7.2.0.0):  {code}     $> printenv SHELL     /bin/tcsh  {code}  [The same issue appears with csh, unsurprisingly.]  {code}     $> source /path/to/install_dir/loadLSST.csh     $> cd /path/to/demo     $> setup obs_sdss     $> ./bin/demo.sh     ./bin/demo.sh: line 7:  /volumes/d0/lsst/stack80/eups/*default*/bin/setups.sh: No such file or directory     ./bin/demo.sh: line 12: setup: command not found  {code}  After hand-editing the demo.sh script to omit the ""/default"" string from the offending line, the demo runs normally to completion.     Note that everything works fine for bash with v8.0, which is what I tested awhile back. ",1
"DM-384","04/03/2014 18:42:33","Add Versioning to SourceTable in lsst::afw::table","Add version to afw::table::SourceTable.  Persist that version number to fits file when the table is saved, and restore when the table is restored.  Tables created and saved to disk prior to this modification will have the version number 0, by default.  Tables created with the S14 version will have the version number 1.    This change is to enable a new version of slots and field naming conventions as needed by the Measurement Framework overhaul, at the same time allowing current clients of SourceTable to continue to function.  The work to define and persist the slots depending on the version will be on a separate issue.  Should not appear as an alterable member of the metadata, but should be saved with the metadata and reloaded when the file is reloaded.  getVersion and setVersion methods will be used to allow clients to alter this number.",1
"DM-405","04/08/2014 02:43:27","Write Linux Standard Base - compliant init.d scripts","Qserv services init.d scripts have to rely on LSB, in order to work on multiple systems.  Remark : xrootd has to be launched as a background process (i.e. with a & at the end). But this always send of return code equal to 0, even if xrootd fails to start, a shell function wait_for_pid will be implemented in xrootd init.d scritps to solve that (inspired from mysqld init.d script).",5
"DM-429","04/08/2014 13:00:59","Make NoiseReplacer outputs reproduceable","We need a way to get back the noise-replaced Exposure as it was when a particular source was measurement, after the measurement has been run, without having to run noise-replacement on all the previous objects again.  There is already code in afw::math::Random to output its state as a string; I think we should probably just save this string in the output catalog.  This will require some API changes to allow the NoiseReplacer to modify the schema and set a field in the output records.",3
"DM-435","04/08/2014 14:05:36","add aperture-correction measurement code to the end of calibrate","At the end of CalibrateTask, we'll want to compute the PSF and aperture fluxes of the PSF stars, and send those to the PSF model to be stored and interpolated (using the featured added via DM-434).  We'll also need to run any other flux measurement algorithms that need to be tied to the PSF fluxes on these same stars; because these can be somewhat slow, we probably want to limit these measurements to only the PSF stars, rather than requiring all these algorithms to be run as part of calibrate.measurement.  The relationships between these fluxes and the PSF fluxes will be additional fields to be added to and interpolated by the PSF.  The HSC implementation of this work (as well as that of DM-436) was done on issue HSC-191: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-191 There were changes to many packages, but the relevant ones for LSST are: https://github.com/HyperSuprime-Cam/afw/commit/057fb3c0581c512d5664f1883a72da950c9eae9d https://github.com/HyperSuprime-Cam/meas_algorithms/compare/HSC-3.0.0...u/jbosch/DM-191 https://github.com/HyperSuprime-Cam/pipe_tasks/compare/4c3a53e7238cbe9...u/jbosch/DM-191",8
"DM-446","04/09/2014 18:33:25","Setup PeakLikelihoodFlux with new Algorithm Framework","Move PeakLikehoodFlux to meas_base framework ",1
"DM-468","04/11/2014 10:30:47","Alias measurement.plugins to measurement.algorithms","The config item in the old measurement task, measurement.algorithms was changed to measurement.plugins in meas_base.  The creates a backward compatibility issue for code which refers to this class member.  Jim's suggested fix is to alias plugins with algorithms in the new measurement task.",1
"DM-470","04/11/2014 14:21:27","Rework exceptions in css (python side)","Rework exceptions in css/KwInterface.py: split into key-value related exceptions, possibly moving the rest that deals with db/tables into client.  This came up in the CSS review, see DM-225: ""CssException feels a bit out of place....""",1
"DM-481","04/14/2014 03:48:47","Unit tests install directory","Hello,  Tests target seems to be correctly defined in core/modules/SConstruct (cf. getTests() function), but tests doesn't seems to be nor built or runned.  In DM-58 branch i've introduced a few line of code which now build the tests.  For now, tests binaries are located in build/moduleName/ directory, do you think we should let the tests here or install it :  - in the Qserv install directory ? - in a tests/ subdirectory of Qserv install directory ? - other solution ? - do we set an option to scons procedure in order to build, and run, these unit tests ?  Thanks,  Fabrice",0
"DM-488","04/15/2014 13:24:07","Make JIRA notification e-mail more useful","From HipChat/Data Management:  [12:09] Mario Juric: @jbosch @KTL @KSK Could you double-check if any of you got an e-mail from Jira on Saturday (Apr 12th) re issue DM-78 (I made you reviewers, but it looks like you weren't notified)? [12:10] K-T Lim: I don't recall and can't determine now; it would have been deleted (irrevocably). [12:10] Simon Krughoff: I did get an email. [12:10] Jim Bosch: @mjuric, ah, it appears that I actually did.  The fact that I was a reviewer was just buried, and I didn't notice it. [12:10] Simon Krughoff: I must have missed that I was a reviewer. [12:10] Mario Juric: OK, thanks!   That gives me not one, but two useful data points (#1 -- emails work, #2 -- they're useless :) ). [12:12] Simon Krughoff: I'm not sure why they are useless.  The emails from trac were a very important part of my workflow as far as being notified of review responsibility goes.   Maybe it's just the volume from Jira. [12:14] Jim Bosch: Yeah, same here.  Though the volume from JIRA hasn't been so bad, so I don't think that's it.  Maybe my brain just has to get used to the new email format. [12:14] K-T Lim: (In my case, I'm mostly paying attention to the RSS feed although the mailbox serves as a backup.) [12:22] Robert Lupton: One of the things that made gnats a good bug tracker was that the emails contained the right amount of information (I did have source code...), and trac was pretty good too when we tuned it;  bugzilla always used to be awful.  I bet we can fiddle with Jira to make its mail more useful;  I don't just mean filtering what it sends, but making sure that each email is self contained, but not too long",1
"DM-505","04/16/2014 13:16:34","improve initialization of kvMap in testQueryAnalysis","Build the kvMap at build-time and embed it into the executable. (this was brought up in DM-225)",1
"DM-506","04/16/2014 13:19:40","improve generating kvMap in testFacade.cc","Generating the kvmap file, and pasting it into a string inside the test program. (this was brought up in DM-225)",1
"DM-508","04/16/2014 13:23:37","shorten internal names in zookeeper","rename DATABASE_PARTITIONING to PARTITIONING  rename DATABASES to DBS",2
"DM-509","04/16/2014 13:25:28","rename ""dbGroup"" to ""storageClass"" in CSS metadata","It is meant to be used to indicate L1, L2, L3... At Qserv design week we decided to rename it (original plan was to remove it all together) ",1
"DM-510","04/16/2014 13:39:14","Tweak metadata structure for driving table and secondary index","There seem to be confusion about driving table and secondary index. At the moment in zookeeper structure we have {code} /DATABASES/<dbName>/objIdIndex /DATABASES/<dbName>/TABLES/<tableName>/partitioning/secIndexColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/drivingTable /DATABASES/<dbName>/TABLES/<tableName>/partitioning/latColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/lonColName /DATABASES/<dbName>/TABLES/<tableName>/partitioning/keyColName {code}  Issues to think about:  * we can't call it objIdIndex, it is too lsst-specific.  * drivingTable and keyColName - perhaps these should be at database level, which means we would only allow one drivingTable and one secondary index per database?  * or, maybe instead of database level, it is a partitioning parameter? Note that two databases might use different name for secondary index or driving table, yet they might be joinable. That argues for introducing a new group, something like /DATABASE/partitioning in addition to /DATABASE_PARTITIONING.  * consider renaming drivingTable to keyTable  * do we really need secIndexColName and keyColName? Can't we get rid of one, and rename to keyColName? ",2
"DM-513","04/16/2014 14:29:19","fix threading issues in CSS watcher","Fix problems with threads in watcher.py brought up in DM-225 by Serge:  * A thread per database doesn't scale  * There is a thread leak when a database is deleted  * There is another design problem, in that each database thread looks like it is holding on to the same lsst.db.Db instance under the hood. I don't remember any consideration for thread safety from the lsst.db code when I reviewed it. Note for one that it is not safe to use a MySQL connection simultaneously from multiple threads (and I seem to recall that you are caching a connection inside Db instances). In practice, even the Python GIL may not save you, since calls into C code (i.e. the mysql client library) may very well release it.",8
"DM-518","04/16/2014 16:52:18","Rework exceptions in qserv client","There is a bunch of (I think) unnecessary translation from KvException to QservAdmException. Can't you just handle printing KvException in CommandParser.receiveCommands(), and get rid of the CSSERR error code? (This is in /admin/bin/qserv-admin.py)  (this came up in DM-225)",1
"DM-520","04/16/2014 17:21:32","Remove old partitioner/ loader and duplicator","Once Fabrice has migrated the integrated tests towards using the new partitioner and duplicator, we should delete the old partitioner/duplicator (in {{client/examples}}).",1
"DM-521","04/16/2014 17:49:23","Confusing error message (non-existing column referenced)","A query that references non existing column for non-partitioned table results in a confusing message: ""read failed for chunk(s): 1234567890"".  To repeat, run something like {code} SELECT whatever FROM <existingTable>; {code}  Similar error occurs when we try to reference non-existing table, try something like:  {code} SELECT sce.filterName  FROM StrangeTable AS s,       Science_Ccd_Exposure AS sce  WHERE  (s.scienceCcdExposureId = sce.scienceCcdExposureId); {code} ",5
"DM-527","04/17/2014 12:48:33","make Image construction robust against integer overflow","I just fixed a bug on the HSC side (DM-523) in which integer overflow in the multiplication of width and height in image construction caused problems.  We should backport this fix to LSST.",1
"DM-530","04/17/2014 21:31:18","Table column names in new parser","Running tests (qserv-testdata.sh) on pre-loaded data I have observed that many test fail for the only reason that the column names in the dumped query results are different between mysql and qserv. Here is an example of query reqult returned from mysql: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +------------+-------+--------+------+ | filterName | field | camcol | run  | +------------+-------+--------+------+ | g          |   670 |      2 | 7202 | +------------+-------+--------+------+ {code} and this is the same query processed by qserv: {code} mysql> SELECT sce.filterName, sce.field, sce.camcol, sce.run FROM Science_Ccd_Exposure AS sce WHERE sce.filterName = 'g' AND sce.field = 670 AND sce.camcol = 2 AND sce.run = 7202; +----------+----------+----------+----------+ | QS1_PASS | QS2_PASS | QS3_PASS | QS4_PASS | +----------+----------+----------+----------+ | g        |      670 |        2 |     7202 | +----------+----------+----------+----------+ {code}  We discussed this already with Daniel yesterday and at qserv meeting today, here I just want to collect what we know so far so that we can return to this again later.   As Daniel explained to me this is the result of the new parser assigning aliases to the columns which do not define aliases for themselves. This helps with tracking query proceeding through the processing pipeline. Daniel's observation is that different database engines may assign different names to result columns (or some may not even assign any names), there is no standard in that respect so there is no point in trying to follow what one particular implementation does. Additionally there are issues with conflicting column names and names which are complex expressions.  Difference in column names breaks our tests which dump complete results including table header. The tests could be fixed easily, we could just ignore table headers when dumping the data. More interesting issue is that there may be use cases for better compatibility between mysql and qserv including result column naming. In particular standard Python mysql interface allows one to use column names to retrieve values from queiry result. If qserv assigns arbitrary aliases to the columns it may confuse this kind of clients.  This issue depends very much on what kind of API qserv is going to provide to clients. If mysql (wire-level) protocol is going to be the main API (which would allow all kinds of mysql clients to talk to qserv directly) then we should probably think more about compatibility with mysql. OTOH if we decide to provide our own API then this may not be an issue at all (but we still need to fix current test setup which is based on mysql).  We probably should discuss API question at our dev meeting.",5
"DM-546","04/22/2014 13:20:30","scons rebuilds targets without changes","I'm seeing something strange when I run scons from current master - running 'scons install' after 'scons build' re-compiles several C++ files even though nothing has changed between these two runs: {code:bash} $ scons build scons: Reading SConscript files ... ... scons: Building targets ... scons: `build' is up to date. scons: done building targets.  $ scons install scons: Reading SConscript files ... ... scons: Building targets ... swig -o build/czar/masterLib_wrap.cc -Ibuild -I/usr/include/python2.6 -python -c++ -Iinclude build/czar/masterLib.i g++ -o build/czar/masterLib_wrap.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/czar/masterLib_wrap.cc g++ -o build/control/AsyncQueryManager.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/AsyncQueryManager.cc g++ -o build/control/dispatcher.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/dispatcher.cc g++ -o build/control/thread.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/control/thread.cc g++ -o build/merger/TableMerger.os -c -g -fPIC -D_FILE_OFFSET_BITS=64 -fPIC -I/u2/salnikov/STACK/stack/Linux64/protobuf/2.4.1/include -I/u2/salnikov/STACK/stack/Linux64/xrootd/qs5/include/xrootd -I/u2/salnikov/STACK/stack/Linux64/mysql/5.1.65/include -Ibuild -I/usr/include/python2.6 build/merger/TableMerger.cc scons: `install' is up to date. scons: done building targets. {code}  This is kind of unexpected, or at least I can't understand now why it happens. Trying to run with --debug=explain shows that some dependencies have disappeared and in some dependencies order is different. No clue yet what that means and how it could happen. Need to study our scons scripts to understand what is going on.",3
"DM-559","04/24/2014 12:44:30","clean up include <> --> """" for third party includes","According to our coding standard 4.15: https://dev.lsstcorp.org/trac/wiki/C%2B%2BStandard/Files#a4-15.OnlysystemincludefilepathsSHALLbedelimitedwith  we should be using """" for boost, but in quite a few places we do not:  {code} grep 'include <boost' */* |wc      146     314    7916 {code} ",1
"DM-560","04/24/2014 12:47:56","cleanup includes - add module name","Change places like {code}#include ""cssException.h""{code} to {code}#include ""css/cssException.h""{code}   ",1
"DM-566","04/24/2014 23:19:40","Fix Doxygen Doc for new meas_base classes","The final stage of moving the old algorithms to meas_base will be to generate the doxygen documentation and be sure that it is useful.  We will do this after the  cleanup of the old meas_algorithms code.",3
"DM-583","04/25/2014 15:58:21","Investigate Approaches to Dcr","This story captures the work of investigating the implications of the different options to compensate for Dcr: ignore objects with extreme colors, compensate for Dcr in measurement, and compensate per-image at the per-object/pixel level.  We anticipate the latter option will be required going forward, but we need to do the background work to justify this.  We need to understand exactly which classes will need to make use of Dcr information, and what the limitations are of operating at the per-pixel level (e.g. blends).",20
"DM-587","04/28/2014 12:48:27","The way the observation date is translated by obs_cfht breaks the defect registry.","The fields of the observation date and time as stored in the CFHT-LS headers are not padded with zeros.  This makes the sqlite DATETIME constructor return a NULL string when the observation time is < 10hrs.  The fix is to force the fields to be padded when the metadata from the input files are ingested.",1
"DM-590","04/28/2014 15:50:32","load non-LSST FITS tables as version 1","In DM-384 and DM-242, we disabled the periods-to-underscores translations for new tables (""version 1"") but left it in place for old tables (""version 0"").  In addition, when reading a table without a version number, we assumed it was version 0, to maintain backwards compatibility.  I think we should modify this slightly: we should assume a table without a version number is version 0 if and only if it also has the AFW_TYPE key.  Otherwise we should assume it is version 1.  This will allow us to load externally-produced tables without turning any underscores they contain into periods, while still maintaining backwards compatibility with older tables written by afw.",1
"DM-593","04/29/2014 16:32:39","Update all DM Software Copyright and License Agreement notices to reflect AURA/LSST","The lsstcorp.org/LegalNotices/{LsstLicenseStatement.txt  LsstSourceCopyrightNotice.txt} need to be updated to reference AURA/LSST. The referenced list of LSST partner institutions needs to be either resurrected or the reference deleted.  The git repository for devenv/templates needs the Copyright templates to be  updated.  LsstLicenseStatement.txt needs to be updated to include recent additions of 3rd party tools' Licenses (~10 tools)  to the DM stack  and all the QSERV 3rd party tools' Licenses (~25 tools).  The Copyright banner in all software needs to be updated to reflect the new reality of AURA/LSST in place of LSST Corporation.  Files with no Copyright banner, need to add it.  Update may occur 'the next time' the code file is updated. THis needs to be broadcast to the developers once the Copyright templates and the website versions are updated.",2
"DM-596","04/29/2014 21:25:46","Fix automated tests after css migration","After yesterday's merge of DM-58 into master automated tests do not work any more. The part which is broken now is loading of metadata into qserv. We need to replace old script which created metadata with something different that creates metadata using new CSS.   The code which loads metadata in tests is in QservDataLoader class, createQmsDatabase() method (in tests/python/lsst/qserv/test/ directory).",2
"DM-597","04/29/2014 22:21:49","reorganize client module","move everything in the client package (qserv_admin*, associated tests and examples) to admin/  move css/bin/watcher.py to admin/  ",1
"DM-608","04/30/2014 16:23:43","referring to table without database context crashes czar","Running a query like ""select * from Object"" if we do not have database context results in  {code} terminate called after throwing an instance of 'lsst::qserv::css::CssException_InternalRunTimeError'   what():  Internal run-time error. (*** css::KvInterfaceImplZoo::exists(). Zookeeper error #-8. (/DATABASES/)) {code}  Need to gracefully catch the zookeeper -8. Need to detect that empty database name is passed in Facade. Need to avoid it higher up. ",3
"DM-609","04/30/2014 17:46:01","afw unit tests not built unless afwdata available","If afw_data is not available then the afw unit tests are not built. I think it would make more sense to build all of them and run those we can (not all depend on afw_data). One advantage is that a version of afw installed using ""eups distrib install"" would include built tests, so the tests could be run. This is presently not practical because the SConstruct file is not installed (I intend to open a ticket about that, as well).",1
"DM-611","05/01/2014 11:27:39","Switch kazoo version to 2.0b1 or later","While we aren't using any of the new features of Kazoo's 2.x series, the removal of zope.interface as a dependency is a worthwhile feature.  The 2.0b1 release seems at least as stable as our own code, so I don't think we'll see any negative effects.  This ticket covers: - Upgrade of the packaged kazoo from 1.3.1 to 2.0b1 (or later). - (optionally) patches for kazoo's setup.py so that it doesn't search for and try to download any dependencies. This can be done in a later ticket, though.  I note that kazoo can be run without installation: you can untar it, cd into the directory, and if you run python from there, you can immediately ""import kazoo"" and use it. Hence we could avoid setup.py completely and just copy the ""kazoo"" subdirectory into some directory in the PYTHONPATH.",2
"DM-612","05/01/2014 13:37:08","remove obsolete QMS-related code","try running  {code} find core css admin client tests site_scons | xargs grep -i qms {code}  There is a lot of old unused qms related code.",2
"DM-614","05/01/2014 17:50:02","rename qserv_admin.py to qserv-admin.py","We have 6 scripts in qserv/admin/bin that start with ""qserv-"", and one that starts with ""qserv_"", we should rename qserv_admin to qserv-admin.",1
"DM-621","05/02/2014 10:49:51","User friendly single node loading script","This entails the creation of an end-to-end loading script that, given database and table param files, a SQL table schema and one or more CSV data files, places appropriate metadata into zookeeper, runs the partitioner, and finally creates and loads chunk tables.",8
"DM-625","05/02/2014 12:34:19","Too many connections from czar to zookeeper","I have just managed to crash qserv czar by running repeated queries against it. What I see in the logs:  qserv-czar.log: {code} 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response 2014-05-02 13:21:10,861:22525(0x7f76e37ab700):ZOO_ERROR@handle_socket_error_msg@1723: Socket [127.0.0.1:12181] zk retcode=-4, errno=104(Connection reset by peer): failed while receiving a server response terminate called after throwing an instance of 'lsst::qserv::css::CssException_ConnFailure'   what():  Failed to connect to persistent store. {code}  and zookeeper.log: {code} 2014-05-02 13:21:10,861 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:10,861 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxnFactory@193] - Too many connections from /127.0.0.1 - max is 60 2014-05-02 13:21:14,585 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440015, likely client has closed socket         at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)         at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)         at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,585 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49913 which had 2014-05-02 13:21:14,585 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@357] - caught end of stream exception EndOfStreamException: Unable to read additional data from client sessionid 0x145bdd1b8440016, likely client has closed socket         at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)         at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)         at java.lang.Thread.run(Thread.java:744) 2014-05-02 13:21:14,586 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:12181:NIOServerCnxn@1007] - Closed socket connection for client /127.0.0.1:49939 which had {code}",1
"DM-626","05/02/2014 12:53:55","ORDER BY and DISTINCT do not work reliably in qserv","Queries with ORDER BY and DISTINCT are buggy. For example, results do not always come ordered and order changes from one run to another: {code} mysql> SELECT objectId FROM Object   WHERE qserv_areaspec_box(0, 0, 3, 10)  ORDER BY objectId; +-----------------+ | objectId        | +-----------------+ | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | +-----------------+ 9 rows in set (1.27 sec)  mysql> SELECT objectId FROM Object   WHERE qserv_areaspec_box(0, 0, 3, 10)  ORDER BY objectId; +-----------------+ | objectId        | +-----------------+ | 424042121137958 | | 424046416102057 | | 427134497589192 | | 430226874040426 | | 417857368235490 | | 417861663199589 | | 417865958163688 | | 420949744686724 | | 420954039650823 | +-----------------+ 9 rows in set (1.24 sec) {code}  This was done with testdata/case01 data, let me know if you need to load that data.  Also, using case03 data, e.g. {code} SELECT distinct run, field  FROM   Science_Ccd_Exposure WHERE  run = 94 AND field = 535; {code} returns 6 rows in qserv (vs 1 in mysql).  The full list of DISTINCT failures is (all with testdata/case03): - 0002_fetchRunAndFieldById.txt - 0021_selectScienceCCDExposure.txt - 0030_selectScienceCCDExposureByRunField.txt ",1
"DM-630","05/02/2014 16:11:25","Non-partitioned table query returns duplicated rows","Running automated test I noticed that a query on non-partitioned table returns multiple copies of the same row, one copy per chunk. Here is example: {code} mysql> SELECT offset, mjdRef, drift FROM LeapSeconds where offset = 10; +--------+--------+-------+ | offset | mjdRef | drift | +--------+--------+-------+ |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | |     10 |  41317 |     0 | +--------+--------+-------+ 13 rows in set (5.62 sec) {code}  Czar log file shows that it correctly finds that table is non-chunked but sends query to each chunk anyway: {code} 20140502 16:22:08.745081 0x3172430 INF *** KvInterfaceImplZoo::exist(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning 20140502 16:22:08.745735 0x3172430 INF *** LSST.LeapSeconds is NOT chunked. 20140502 16:22:08.745762 0x3172430 INF *** KvInterfaceImplZoo::get2(), key: /DATABASES/LSST/TABLES/LeapSeconds/partitioning/subChunks 20140502 16:22:08.746393 0x3172430 INF *** LSST.LeapSeconds is NOT subchunked. 20140502 16:22:08.746409 0x3172430 INF getChunkLevel returns 0 ..... 20140502 16:22:08.757832 0x3172430 INF <py> Using 85 stripes and 12 substripes. 20140502 16:22:08.775586 0x3172430 INF <py> Using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt as default empty chunks file. 20140502 16:22:08.791559 0x3172430 INF <py> empty_LSST.txt not found while loading empty chunks file. 20140502 16:22:08.791592 0x3172430 ERR <py> Couldn't find empty_LSST.txt, using /usr/local/home/salnikov/dm-613/build/dist/etc/emptyChunks.txt. 20140502 16:22:08.891239 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891498 0x7fbb28003660 INF Msg cid=6630 with size=153 20140502 16:22:08.891682 0x7fbb28003660 INF Added query id=6630 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6630_0 20140502 16:22:08.891694 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6630 20140502 16:22:08.891705 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.891882 0x7fbb28003660 INF Msg cid=6631 with size=153 20140502 16:22:08.892077 0x7fbb28003660 INF Added query id=6631 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6631_0 20140502 16:22:08.892087 0x7fbb28003660 INF Opening xroot://qsmaster@127.0.0.1:1094//q/LSST/6631 20140502 16:22:08.892097 0x7fbb28003660 INF QuerySession::_buildChunkQueries() : Non-subchunked 20140502 16:22:08.892275 0x7fbb28003660 INF Msg cid=6800 with size=153 20140502 16:22:08.892462 0x7fbb28003660 INF Added query id=6800 url=xroot://qsmaster@127.0.0.1:1094//q/LSST/6800 with save /dev/shm/qserv-salnikov-b93a8b7cca1128f50fa5531feb93f8f24a185f162d36c10ee76b8dca/1_6800_0 ... {code}  Looking at the code together with Daniel we found that at the Python level (czar/app.py) the code that dispatches query does not check for chunkLevel, this is likely why this happens. The code to look at is in {{InbandQueryAction._applyConstraints()}} method.",5
"DM-633","05/03/2014 18:03:39","Query sessions are never destroyed","Please see DM-625, when I run say 10 ""select count(*) from LSST.Object"" queries, for each query a new AsyncQueryManager is created in dispatcher, but the sessions are never destroyed.",3
"DM-637","05/06/2014 01:36:08","complexity of eups dependencies relationships  for db package","Hello,  I'm currently trying to use the very last version of db package (the one which relies on sconsUtils), but, in order to make it works with Qserv, I had to introduce next update : {code:bash} fjammes@clrlsstwn02-vm:~/src/qserv-packager/dist/dependencies/db (master) $ git diff HEAD~1 diff --git a/ups/db.cfg b/ups/db.cfg index e1ae31b..a469061 100644 --- a/ups/db.cfg +++ b/ups/db.cfg @@ -3,7 +3,7 @@  import lsst.sconsUtils    dependencies = { -    ""required"": [""mysqlclient"", ], +    ""required"": [""mysql"", ],  }    config = lsst.sconsUtils.Configuration( diff --git a/ups/db.table b/ups/db.table index 8c8d831..9e770a3 100644 --- a/ups/db.table +++ b/ups/db.table @@ -1,5 +1,5 @@  setupRequired(python) -setupRequired(mysqlclient) +setupRequired(mysql)  setupRequired(mysqlpython)  setupRequired(sconsUtils) {code}  Is there a solution to describe  in eups that mysqlclient is included in mysql ?  Thanks,  Fabrice",1
"DM-646","05/08/2014 14:21:01","Implement DISTINCT aggregate in qserv","It looks like DISTINCT aggregate is not supported yet in qserv. Daniel told me that this should be relatively straightforward to add. Adding this ticket so that we do not forget it.",2
"DM-648","05/08/2014 15:38:36","Add support for running unit tests in scons","Add code in scons that runs unit tests for Qserv.",5
"DM-661","05/09/2014 23:13:12","Parser has inverted order for ""limit"" and ""order by""","{code} SELECT run FROM LSST.Science_Ccd_Exposure order by field limit 2 {code}  Works in MySQL, fails in Qserv (ERROR 4120 (Proxy): Error executing query using qserv.)  {code} SELECT run FROM LSST.Science_Ccd_Exposure limit 2 order by field {code}  Works in Qserv, fails in MySQL (limit should be after order by) ",0.5
"DM-664","05/12/2014 08:55:44","""out of range value"" message when running qserv-testdata (loader.py)","Fabrice  I am getting ""out of range value"" when I run the qserv-testdata:  Are you seeing that too?   2014-05-09 18:11:55,975 {/usr/local/home/becla/qserv/1/qserv/build/dist/lib/python/lsst/qserv/admin/commons.py:134} INFO     stderr : /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_any' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_cr_center' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_gaussian_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_naive_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_centroid_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweightedbad' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_unweighted' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_shift' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'shape_sdss_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_psf_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_naive_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_gaussian_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flux_sinc_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_psf_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_dev_flags_largearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_maxiter' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_tinystep' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_r' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_constraint_q' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_exp_flags_largearea' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flux_flags' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_psffactor' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'multishapelet_combo_flags_badcorr' at row 1   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_detected' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_candidate' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'calib_psf_used' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_negative' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_badcentroid' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'centroid_sdss_flags' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_edge' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_any' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_interpolated_center' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_any' at row 2   self.cursor.execute(stmt) /usr/local/home/becla/qserv/1/qserv/build/dist/bin/loader.py:99: Warning: Out of range value for column 'flags_pixel_saturated_center' at row 2   self.cursor.execute(stmt) ",2
"DM-666","05/12/2014 09:59:30","partition package has to detect eups-related boost","partition package doesn't detect eups-related boost. This has to be fixed by using sconsUtils, or hand-made procedure.  {code:bash} [fjammes@lsst-dev lsstsw]$ export LD_LIBRARY_PATH=""$LSSTSW/anaconda/lib:$LD_LIBRARY_PATH"" [fjammes@lsst-dev lsstsw]$ setup boost 1.55.0.1+1 [fjammes@lsst-dev lsstsw]$ rebuild partition            partition:  ok (0.5 sec).                boost:  ok (0.3 sec).               python:  ok (0.3 sec).                scons:  ok (0.4 sec). # BUILD ID: b49               python: master-gcbf93ab65b (already installed).                scons: 2.1.0+8 (already installed).                boost: 1.55.0.1+1 (already installed).            partition: master-gf2ef2cf2dc ERROR (1 sec). *** error building product partition. *** exit code = 1 *** log is in /lsst/home/fjammes/src/lsstsw/build/partition/_build.log *** last few lines: :::::  scons: Reading SConscript files ... :::::  Checking for C++ library boost_system-mt... no :::::  Checking for C++ library boost_system... no :::::  Checking for C++ library boost_thread-mt... no :::::  Checking for C++ library boost_thread... no :::::  Checking for C++ library boost_filesystem-mt... no :::::  Checking for C++ library boost_filesystem... no :::::  Checking for C++ library boost_program_options-mt... no :::::  Checking for C++ library boost_program_options... no :::::  Missing required boost library! # BUILD b49 completed. {code}",3
"DM-674","05/13/2014 18:28:02","fix handling of nested control objects","Work on the HSC side has revealed some problems with nested control objects being wrapped into config objects.  This is a pull request for those changes (along with writing a unit test for some of them).  Some (but not all of these changes) are part of Trac ticket #3163 (https://dev.lsstcorp.org/trac/ticket/3163), which I'll now close as a duplicate.",2
"DM-676","05/14/2014 08:40:56","Implement HTCondor dynamic classad solution for Slot based values","The HTCondor team will be updating their HOWTO for managing Slot based classads/dynamic classads set by a cron startd process.  We currently have a technique for  dynamic slot based values that is iinefficient from a negotiation perspective, and we will want to update to a more optimal approach that the HTCondor team plans to provide.",2
"DM-689","05/16/2014 09:13:25","During scons configure : check if mysql isn't runing","Mysqld can't be configured is its running before configuration step.",1
"DM-703","05/19/2014 15:17:24","Use of HipChat for Buildbot CI failure notifications should be explored","K-T recommended the use of HipChat rather than email when notifying users of a buildbot build failure.  The purpose was twofold: get immediate attention from the developers and help change the culture towards using HipChat more.  This Issue is to explore the feasibility of using HipChat for the notifications.",1
"DM-704","05/19/2014 16:23:16","Better review notification e-mails","Russell writes:  {quote} I think our system for getting code reviewed using JIRA needs some improvements. It seems that people don't always know that they have been assigned to review a ticket. Also, even if I know I have been assigned to review a ticket, I find it hard to find on JIRA.  More concretely, I would like to see these improvements: - Much clearer notification that one has been assigned as a reviewer. Presently the email is quite generic and easy to miss. In fact I find that most JIRA notifications are rather hard to read -- it's not always easy to see what has changed and thus why I should care. The signal to noise ratio is poor.  - By default a user should see which issues they have been assigned as reviewer when they log into JIRA. (If there is a way to reconfigure the dashboard for this, I'd like to know about it, but it really should be the default). One way to fix this, of course, is to reassig the ticket when putting it into review, but we have good reasons to avoid that.  -- Russell {quote}  and I added:  {quote} In fact, you don't know that the ticket has passed into review unless you scroll all the way to the bottom of the comment.  If the comment associated with the change in status is long and you don't scroll all the way down, then you may not know that you were assigned to review.  With Trac, the important information was at the top of the e-mail. {quote}",2
"DM-706","05/19/2014 21:56:26","cleanup extra file names in docstring","Reported by Serge in email:  When using doxygen to document C++ source, you can mark a comment block with just:  {code} /** @file   * Blah blah   */ {code}  in which case doxygen assumes you want the comment block tied to the file it appears in. We seem to have lots of ""@file <fileName>” statements all over the place, which is an extra thing we have to remember to change when renaming files. Is there some reason to do it that way that I’m missing?",1
"DM-707","05/19/2014 21:59:41","cleanup exception code in CSS","Reported by Serge:  In CssException.h you’ve got:  {code} class CssRunTimeException: public std::runtime_error { … }; class CssException_XXXX : public CssRunTimeException { … }; {code}  This is inconsistent (shouldn’t it be CssRunTimeException_XXX, or maybe even CssRunTimeError?), lengthy, violates the LSST C++ naming conventions, and doesn’t match the KvInterface docs, which all still talk about a CssException class that does not exist. Can we consider changing this to something more like:  {code} class CssError : public std::runtime_error class KeyError : public CssError class NoSuchTable : public KeyError class NoSuchDb : public KeyError class AuthError : public CssError class ConnError : public CssError {code}  ? Then we can succinctly throw and catch css::NoSuchTable, css::AuthError etc…",2
"DM-710","05/20/2014 09:59:00","Reduce and comment client configuration file","Client configuration file '~/.lsst/qserv.conf) is used by integration test procedure.  Next improvments are required : 1. use templates in it 2. client config file should retrieve templated values from mother config   file""",1
"DM-720","05/21/2014 12:30:01","Upgrade various external packages","I note that the LSST and HSC versions of external packages are slightly out of sync.  I propose uprevving the LSST packages to match as HSC has tested these versions.  {quote} cfitsio               3.360             HSC cfitsio               3310+2            sims Winter2014 current b4 b5 b6 b3 doxygen               1.8.2+2           sims b4 Winter2014 current b5 b6 b3 doxygen               1.8.5             HSC eigen                 3.1.1+2           Winter2014 current b5 b6 b3 b4 eigen                 3.2               HSC fftw                  3.3.2+2           Winter2014 current b5 b6 b3 b4 fftw                  3.3.3             HSC gsl                   1.15+2            Winter2014 current b5 b6 b3 b4 gsl                   1.16              HSC minuit2               5.22.00+2         Winter2014 current b5 b6 b3 b4 minuit2               5.28.00           HSC mysqlclient           5.1.65+3          Winter2014 current b5 b6 b3 b4 mysqlclient           5.1.73            HSC pyfits                3.1.2+2           sims b4 Winter2014 current b5 b6 b3 pyfits                3.2               HSC scons                 2.1.0+7           sims b4 Winter2014 current b5 b6 b3 scons                 2.3.0             HSC sqlite                3.7.14+2          Winter2014 current b5 b6 b3 b4 sqlite                3.8.2             HSC wcslib                4.14        wcslib                4.14+3            b4 Winter2014 current b5 b6 b3 xpa                   2.1.14+2          Winter2014 current b5 b6 b3 b4 xpa                   2.1.15            HSC {quote} ",20
"DM-737","05/21/2014 16:49:28","Rendering an IR node tree should produce properly parenthesized output","It appears that rendering a tree of IR nodes doesn't always result in correct generation of parentheses. Consider the following tree: {panel} * OrTerm ** BoolFactor *** NullPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""refObjectId"") ** BoolFactor *** CompPredicate **** ValueExpr ***** ValueFactor: ColumnRef(""flags"") **** Token(""<>"") **** ValueExpr ***** ValueFactor: Const(""2"") {panel}  which corresponds to the SQL for: ""refObjectId IS NULL OR flags<>2"". If one prepends this (via {{WhereClause.prependAndTerm()}}) to the {{WhereClause}} obtained by parsing ""... WHERE foo!=bar AND baz<3.14159;"" and renders the result using {{QueryTemplate}}, one obtains:      {{... WHERE refObjectId IS NULL OR flags<>2 AND foo!=bar AND baz<3.14159}}  This is equivalent to      {{... WHERE refObjectId IS NULL OR (flags<>2 AND foo!=bar AND baz<3.14159)}}  which doesn't match the parse tree - one should obtain:      {{... WHERE (refObjectId IS NULL OR flags<>2) AND foo!=bar AND baz<3.14159}}  This issue involves surveying all IR node classes and making sure that they render parentheses properly. {color:gray}(One way we might test for this is to parse queries containing parenthesized expressions where removal of the parentheses changes the meaning of the query. This would give us some IR that we can render to a string and reparse back into IR. If the rendering logic is correct, one should obtain identical IR trees).{color} Other possibilities that might explain the behavior above is that the input tree is somehow invalid or that {{WhereClause.prependAndTerm}} creates invalid IR.",8
"DM-742","05/23/2014 07:10:18","Use geom eups package for installing geometry","Use geom eups package instead of downloading geometry.py during Qserv configuration step.",3
"DM-751","05/26/2014 10:02:12","Replacing boost system lib with eups libs breaks scons build","While detecting boost, Qserv build system checks for both system lib and then eups lib. This procedure use next code :  {code:python} class BoostChecker:     def __init__(self, env):         self.env = env         self.suffix = None         self.suffixes = [""-gcc41-mt"", ""-gcc34-mt"", ""-mt"", """"]         self.cache = {}         pass      def getLibName(self, libName):         if libName in self.cache:             return self.cache[libName]          r = self._getLibName(libName)         self.cache[libName] = r         return r      def _getLibName(self, libName):         state.log.debug(""BoostChecker._getLibName() LIBPATH : %s, CPPPATH : %s"" % (self.env[""LIBPATH""], self.env[""CPPPATH""]))         if self.suffix == None:             conf = self.env.Configure()              def checkSuffix(sfx):                 return conf.CheckLib(libName + sfx, language=""C++"", autoadd=0) {code}  and this last line run next gcc command :  {code:bash} g++ -o .sconf_temp/conftest_10.o -c -g -pedantic -Wall -Wno-long-long -D_FILE_OFFSET_BITS=64 -fPIC -I/data/fjammes/stack/Linux64/protobuf/master-g832d498170/include -I/data/fjammes/stack/Linux64/boost/1.55.0.1/include -I/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/include -I/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/include -I/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/include -I/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/include/xrootd -Ibuild -I/data/fjammes/stack/Linux64/anaconda/1.8.0/include/python2.7 .sconf_temp/conftest_10.cpp g++ -o .sconf_temp/conftest_10 .sconf_temp/conftest_10.o -L/data/fjammes/stack/Linux64/xrootd/master-gfc9bfb2059/lib -L/data/fjammes/stack/Linux64/protobuf/master-g832d498170/lib -L/data/fjammes/stack/Linux64/antlr/master-gc05368a54f/lib -L/data/fjammes/stack/Linux64/zookeeper/master-gc48457902f/c-binding/lib -L/data/fjammes/stack/Linux64/mysql/master-g5d79af2a50/lib -L/data/fjammes/stack/Linux64/boost/1.55.0.1/lib -lboost_regex-mt scons: Configure: yes {code}  As the ""-mt"" suffix is searched before the empty suffix, previous command succeed.In my example boost_regex-mt is a system lib. When launching ""scons build"", then CheckLib only looks for boost in /data/fjammes/stack/Linux64/boost/1.55.0.1/lib, not in /usr/lib/. This behaviour is eups-correct, but prevents to find boost_regex-mt.  In this example, a trivial solution is to reverse self.suffixes in python code, but a better solution would be to prevent g++ to use default search paths (e.g. : /usr/lib and /usr/include) in the second command. Is it possible to to it with scons ?  Mario, did you meet the same problem with sconsUtils ?    Thanks  Fabrice",3
"DM-764","05/27/2014 16:49:37","Exception naming convention","The naming convention for exceptions in pex_exceptions is quite redundant.  This issue will make the convention more compact and update all packages that make use of pex_exceptions.",5
"DM-766","05/27/2014 17:00:29","Improve afw::CameraGeom::utils code","Some of the utility code in CameraGeom was not completely ported in W13 and documentation is in need of updating.",3
"DM-767","05/27/2014 17:13:18","Determine scope of XY0 convention update","It's unclear exactly how much effort will be involved in making a change to how the XY0 is used.  If the parent/child argument is removed completely this change could be quite invasive and wide reaching.",2
"DM-772","05/28/2014 10:18:07","Package log4cxx","Fabrice, can you package log4cxx? I should have asked you earlier, sorry I waited so long, not it becoming urgent! Bill is almost done with his logging prototype and will be turning it into a real package, and we need to have log4cxx packages. Many thanks.  log4cxx version 0.10.0, which was released in 4/3/2008 but is still undergoing ""incubation"" at Apache. ",2
"DM-775","05/28/2014 11:03:44","XLDB-2015 report","Writing the report, most work done by Daniel, with input from Jacek and K-T.",8
"DM-778","05/28/2014 22:22:14","Restructure and package logging prototype","Restructure and package log4cxx-based prototype (currently in branch u/bchick/protolog). It should go into package called ""log""",8
"DM-780","05/28/2014 22:37:50","Access patterns for data store that supports data distribution ","Data distribution related data store includes things like. chunk --> node mapping, locations of chunk replicas, runtime information about nodes (and maybe also node configuration?). Need to understand access patterns - who needs to access, how frequently etc. ",5
"DM-781","05/28/2014 22:41:33","research mysql cluster ndb","Checkout mysql cluster ndb from the perspective of data distribution - could it be potentially useful to store data related to data distribution?",2
"DM-783","05/29/2014 10:40:20","Disable failing test cases in automated tests","There are currently 4 test cases failing in out automated tests. Until we have a fix we want to disable them.",1
"DM-786","05/29/2014 12:48:39","JOIN queries are broken","Running a simple query that does a join:  {code} SELECT s.ra, s.decl, o.raRange, o.declRange FROM   Object o JOIN   Source s USING (objectId) WHERE  o.objectId = 390034570102582 AND    o.latestObsTime = s.taiMidPoint; {code}  results in czar crashing with: {code} 2terminate called after throwing an instance of 'std::logic_error'   what():  Attempted subchunk spec list without subchunks. {code}  This query has been taken from integration tests (case01, 0003_selectMetadataForOneGalaxy.sql) ",3
"DM-794","05/29/2014 19:02:42","SQL injection in czar/proxy.py","Running automated tests for some queries I observe python exceptions in czar log which look like this: {code} 20140529 19:47:19.364371 0x7faacc003550 INF <py> Query dispatch (7) toUnhandled exception in thread started by <function waitAndUnlock at 0x18cd8c0> Traceback (most recent call last):   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 78, in waitAndUnlock     lock.unlock()   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 65, in unlock     self._saveQueryMessages()   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/proxy.py"", line 87, in _saveQueryMessages     self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp))   File ""/usr/local/home/salnikov/qserv-master/build/dist/lib/python/lsst/qserv/czar/db.py"", line 95, in applySql     c.execute(sql)   File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute     self.errorhandler(self, exc, value)   File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3+8/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler     raise errorclass, errorvalue _mysql_exceptions.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'r' AND sce.tract=0 AND sce.patch='159,3';', 1401410839.000000)' at line 1"") ok 0.000532 seconds {code}  I believe this is due to how query string is being constructed in czar/proxy.py: {code:py} class Lock:      writeTmpl = ""INSERT INTO %s VALUES (%d, %d, '%s', %f);""  # ...................             self.db.applySql(Lock.writeTmpl % (self._tableName, chunkId, code, msg, timestamp)) {code}  If {{msg}} happens to contain quotes then resulting query is broken. One should not use Python formatting to construct query strings, instead the parameters should be passed directly to {{cursor.execute()}} method. ",2
"DM-800","05/31/2014 00:53:54","Zookeeper times out","I noticed running some queries, leaving system up and them returning few hours later and running more queries can result in:  {code} ZOO_ERROR@handle_socket_error_msg@1723:  Socket [127.0.0.1:12181] zk retcode=-4, errno=112(Host is down):  failed while receiving a server response {code}  It needs to be investigated (if we can reproduce) ",3
"DM-814","06/03/2014 08:29:59","Cleanup in core/examples and core/doc","- core/examples and core/doc seems to be out of data.  Some cleanup here would be welcome.",1
"DM-817","06/03/2014 17:35:51","qserv have to use boost from stack","To quote Jacek and KT: {code} Andy, re dm-751, KT says never use the system version.  J. {code}  So we need to switch qserv to eups-boost. This should be easy once DM-751 is done, just add boost to qserv.table. Then one can remove conditional part of {{BoostChecker}} which works with system-installed boost. ",1
"DM-827","06/09/2014 10:05:17","Reimplement C++/Python Exception Translation","I'd like to reimplement our Swig bindings for C++ exceptions to replace the ""LsstCppException"" class with a more user-friendly mechanism.  We'd have a Python exception hierarchy that mirrors the C++ hierarchy (generated automatically with the help of a few Swig macros).  These wrapped exceptions could be thrown in Python as if they were pure-Python exceptions, and could be caught in Python in the same language regardless of where they were thrown.  We're doing this as part of a ""Measurement"" sprint because we'd like to define custom exceptions for different kinds of common measurement errors, and we want to be able to raise those exceptions in either language.",8
"DM-829","06/09/2014 10:19:41","Algorithm API without (or with optional) Result objects","In this design prototype, I'll see how much simpler things could be made by making the main algorithm interface one that sets record values directly, instead of going through an intermediate Result object.  Ideally the Result objects would still be an option, but they may not be standardized or reusable.",3
"DM-832","06/09/2014 12:07:51","add persistable class for aperture corrections","We need to create a persistable, map-like container class to hold aperture corrections, with each element of the container being an instance of the class to be added in DM-740.  A prototype has been developed on DM-797 on the HSC side: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-797 and the corresponding code can be found on these changesets: https://github.com/HyperSuprime-Cam/afw/compare/32d7a8e7b75da6f5327fee65515ee59a5b09f6c7...tickets/DM-797",2
"DM-833","06/09/2014 12:09:16","implement coaddition for aperture corrections","We need to be able to coadd aperture corrections in much the same way we coadd PSFs.  See the HSC-side HSC-798 and HSC-897 implementation for a prototype: https://hsc-jira.astro.princeton.edu/jira/browse/HSC-798 https://hsc-jira.astro.princeton.edu/jira/browse/HSC-897 with code here: https://github.com/HyperSuprime-Cam/meas_algorithms/compare/d2782da175c...u/jbosch/DM-798 https://github.com/HyperSuprime-Cam/meas_algorithms/compare/c4fcab3251...u/price/HSC-897a https://github.com/HyperSuprime-Cam/pipe_tasks/compare/6eb48e90be12d...u/price/HSC-897a",3
"DM-837","06/09/2014 13:00:49","Rewrite multiple-aperture photometry class","We've never figured out how to handle wrapping multiple-aperture photometry algorithms.  They can't use the existing Result objects - at least not out of the box.  We should try to write a new multiple-aperture photometry algorithm from the ground up, using the old ones on the HSC branch as a guide, but not trying to transfer the old code over.  The new one should:  - Have the option of using elliptical apertures (as defined by the shape slot) or circular apertures.  - Have a transition radius at which we switch from the sinc photometry algorithm to the naive algorithm (for performance reasons).",2
"DM-840","06/09/2014 16:27:00","Change code so ImageOrigin must be specified (temporary)","Image-like classes have a getBBox method and various constructors that use an ImageOrigin argument which in most or all cases defaults to LOCAL. As the first stage in cleaning this up, try to break code that uses the default as follows: * Remove the default from getBBox(ImageOrigin) so an origin must be specified. * Change the default origin of constructors to a temporary new value UNDEFINED  * Modify code that uses image origin to fail if origin is needed (it is ignored if bbox is empty) and is UNDEFINED.  Note: this is less safe than changing constructors to not have a default value for origin, because the error will be caught at runtime rather than compile time. However, that is messy because then the bounding box will also have to be always specified, and possibly an HDU, so it would be a much more intrusive change.",2
"DM-841","06/09/2014 16:29:21","Change data butler I/O of image-like objects to require imageOrigin if bbox specified (temporary)","As part of making PARENT the default for image origin, change the data butler to require that imageOrigin be specified if bbox is specified when reading or writing image-like objects.  Note: this ticket turns out to be unnecessary, as all the few necessary change are done as part of DM-840.",2
"DM-843","06/09/2014 16:43:52","Restore names of methods that return pixel iterators and locators","Restore the names of methods that return pixel iterators and pixel locators on image-like classes. (This is part of the final stage of eliminating LOCAL pixel indexing).",2
"DM-845","06/09/2014 16:46:45","Eliminate image origin argument from butler for (un)persisting image-like objects","Eliminate the image origin argument for butler get and put when dealing with image-like objects.",2
"DM-854","06/10/2014 00:47:31","duplicate column name when running near neighbor query","Running a simplified version of near neighbor query on test data from case01:  {code} SELECT DISTINCT o1.objectId, o2.objectId FROM   Object o1,         Object o2 WHERE  scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1   AND  o1.objectId <> o2.objectId {code}  Result in an error on the worker:  {code} Foreman:Broken! ,q_38f9QueryExec---Duplicate column name 'objectId' Unable to execute query: CREATE TABLE r_13237cd4cfc9e0fa01497bcf\ 67a91add2_6630_0 SELECT o1.objectId,o2.objectId FROM Subchunks_LSST_6630.Object_6630_0 AS o1,Subchunks_LSST_6630.Object_6630_0 AS o2\  WHERE scisql_angSep(o1.ra_PS,o1.decl_PS,o2.ra_PS,o2.decl_PS)<1 AND o1.objectId<>o2.objectId; {code}  It is fairly obvious what is going on. ""SELECT t1.x, t2.x"" is perfectly valid, but if we add ""INSERT INTO SELECT t1.x, t2.x"", we need to add names, eg. something like ""INSERT INTO SELECT t1.x as x1, t2.x as x2""",8
"DM-863","06/11/2014 17:51:37","near neighbor does not return results","A query from qserv_testdata (case01/queries/1051_nn.sql) runs through Qserv, but it returns no results, while the same query run on myql does return results.  The exact query for qserv is:   {code} SELECT o1.objectId AS objId  FROM Object o1, Object o2  WHERE qserv_areaspec_box(0, 0, 0.2, 1)  AND scisql_angSep(o1.ra_PS, o1.decl_PS, o2.ra_PS, o2.decl_PS) < 1 AND o1.objectId <> o2.objectId; {code}",1
"DM-869","06/12/2014 14:20:20","disable extraneous warnings from boost (gcc 4.8)","Compiling qserv on ubuntu 14.04 (comes with gcc 4.8.2) results in huge number of warnings coming from boost. We should use the flag ""-Wno-unused-local-typedefs"".",0.5
"DM-873","06/13/2014 19:18:55","XLDB - strategic positioning","Discussions with strategic partners. Improving website and adding new context (community, speakers). 1-pager document",3
"DM-874","06/16/2014 07:46:13","W'14 newinstall.sh picks up wrong python?","newinstall.sh fails with:  Installing the basic environment ...  Traceback (most recent call last):   File ""/tmp/test_lsst/eups/bin/eups_impl.py"", line 11, in ?     import eups.cmd   File ""/tmp/test_lsst/eups/python/eups/__init__.py"", line 5, in ?     from cmd        import commandCallbacks   File ""/tmp/test_lsst/eups/python/eups/cmd.py"", line 38, in ?     import distrib   File ""/tmp/test_lsst/eups/python/eups/distrib/__init__.py"", line 30, in ?     from Repositories import Repositories   File ""/tmp/test_lsst/eups/python/eups/distrib/Repositories.py"", line 8, in ?     import server   File ""/tmp/test_lsst/eups/python/eups/distrib/server.py"", line 1498     mapping = self._noReinstall if outVersion and outVersion.lower() == ""noreinstall"" else self._mapping                                  ^ SyntaxError: invalid syntax  Perhaps from running the wrong version of python.  Full script/log is attached. ",1
"DM-875","06/16/2014 10:11:49","lsst_dm_stack_demo","lsst-dm_stack_demo has obsolete benchmark files (circa Release 7.0)  which fail to serve the purpose of validating, for the user, the correct functioning of a freshly built Release v8.0 stack.   At the very least,  the benchmark files should be regenerated for each official Release. Tasks:   (1) Build the benchmark files for Release v8.0  (2) Debate (a) recommending the  use of 'numdiff'  to check if the output is within realistic bounds.   Or, (b) develop another procedure to better show how the current algorithms compare to the algorithms used at the benchmarked Release. (3) Depending on result of the debate on #2: for: (a) provide appropriate 'numdiff' command invocation in manual.; for (b) implement the new procedure.",40
"DM-903","06/25/2014 19:14:47","SourceDetectionTask should only add flags.negative if config.thresholdParity == ""both""","The SourceDetectionTask always adds ""flags.negative"" to the schema (if provided) but it is only used if config.thresholdParity == ""both"".  As adding a field to a schema requires that the table passed to the run method have that field this is a significant nuisance when reusing the task.  Please change the code to only modify the schema if it's going to set it. ",1
"DM-911","06/27/2014 20:20:25","Provide Task documentation for DipoleMeasurementTask","See Summary. ",2
"DM-913","06/27/2014 20:26:35","Provide Task documentation for ImagePsfMatchTask","See summary",2
"DM-914","06/27/2014 20:27:19","Provide Task documentation for SnapPsfMatchTask","See summary",2
"DM-933","06/30/2014 12:52:38","Photometric calibration uses a column ""flux"" not the specified filter unless a colour term is active","The photometric calibration code uses a field ""flux"" in the reference catalog to impose a magnitude limit.  If a colour term is specified, it uses the primary and secondary filters to calculate the reference magnitude, but if there is no colour term it uses the column labelled ""flux"" and ignores the filtername.    Please change the code so that ""flux"" is ignored, and the flux associated with filterName is used.",1
"DM-951","07/08/2014 13:28:14","Add Doxygen documentation on rebuilds","Master-branch doxygen documentation should be rebuild on every full master build.",20
"DM-957","07/11/2014 11:00:01","Use aliases to clean up table version transition","The addition of schema aliases on DM-417 should allow us to clean up some of the transitional code added on DM-545, as we can now alias new versions of fields to the old ones and vice versa.",2
"DM-964","07/15/2014 07:21:49","Include aliases in Schema introspection","Schema stringification and iteration should include aliases somehow.  Likewise the extract() Python methods.",1
"DM-966","07/15/2014 12:25:35","fix int/long conversion on 32-bit systems and selected 64-bit systems","tests/wrap.py fails in pex_config on 32-bit systems and some 64-bit systems (including Ubuntu 14.04) with the following: {code:no-linenum} tests/wrap.py  ...EE.E. ====================================================================== ERROR: testDefaults (__main__.NestedWrapTest) Test that C++ Control object defaults are correctly used as defaults for Config objects. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 89, in testDefaults     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testInt64 (__main__.NestedWrapTest) Test that we can wrap C++ Control objects with int64 members. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 95, in testInt64     self.assert_(testLib.checkNestedControl(control, config.a.p, config.a.q, config.b))   File ""/home/boutigny/CFHT/stack_5/build/pex_config/tests/testLib.py"", line 987, in checkNestedControl     return _testLib.checkNestedControl(*args) TypeError: in method 'checkNestedControl', argument 2 of type 'double'  ====================================================================== ERROR: testReadControl (__main__.NestedWrapTest) Test reading the values from a C++ Control object into a Config object. ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/wrap.py"", line 82, in testReadControl     config.readControl(control)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 212, in readControl     __at=__at, __label=__label, __reset=__reset)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 217, in readControl     self.update(__at=__at, __label=__label, **values)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 515, in update     field.__set__(self, value, at=at, label=label)   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/config.py"", line 310, in __set__     raise FieldValidationError(self, instance, e.message) FieldValidationError: Field 'a.q' failed validation: Value 4 is of incorrect type long. Expected type int For more information read the Field definition at:   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 184, in makeConfigClass     fields[k] = FieldCls(doc=doc, dtype=dtype, optional=True) And the Config definition at:   File ""/home/boutigny/CFHT/stack_5/build/pex_config/python/lsst/pex/config/wrap.py"", line 131, in makeConfigClass     cls = type(name, (base,), {""__doc__"":doc})   ---------------------------------------------------------------------- Ran 8 tests in 0.017s  FAILED (errors=3) {code}  There is a partial fix on u/jbosch/intwrappers; this seems to work for Ubuntu 14.04, but not on 32-bit systems.",2
"DM-967","07/16/2014 10:56:37","qserv-configure.py is broken in master","It looks like there was a bug introduced either during the merge of DM-622 with master or right before that. Running {{qserv-configure.py}} from master fails now: {code} $ qserv-configure.py    File ""/usr/local/home/salnikov/qserv-master/build/dist/bin/qserv-configure.py"", line 229     (""Do you want to update user configuration file (currently pointing                                                                       ^ SyntaxError: EOL while scanning string literal {code} I assign this to myself, Fabrice is on vacation now and we need to fix this quickly.",1
"DM-976","07/18/2014 09:51:02","Detailed documentation for meas_base tasks","We should follow RHL's example for detailed task documentation and document all meas_base tasks.",2
"DM-977","07/18/2014 09:52:29","Documentation audit and cleanup for meas_base plugins","Many meas_base Plugins and Algorithms have poor documentation, including several whose documentation is a copy/paste relic from some other algorithm.  These need to be fixed.",2
"DM-978","07/18/2014 09:56:09","add base class for measurement tasks","We should consider adding a base class for measurement tasks (SingleFrameMeasurementTask, ForcedMeasuremedTask) that includes the callMeasure methods.  I'm hoping this will help cleanup callMeasure and improve code reuse.",1
"DM-980","07/18/2014 14:40:06","convert measurement algorithms in ip_diffim","ip_diffim includes a few measurement algorithms which need to be converted to the new framework.",5
"DM-981","07/18/2014 14:49:33","convert measurement algorithms in meas_extensions_shapeHSM","This is a low-priority ticket to replace the old-style plugins in meas_extensions_shapeHSM with new ones compatible with meas_base.  As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete.",3
"DM-982","07/18/2014 14:50:28","convert meas_extensions_photometryKron to new measurement framework","This is a low-priority ticket to replace the old-style plugins in meas_extensions_photometryKron with new ones compatible with meas_base.  As this isn't a part of the main-line stack, we should delay it until other the meas_base conversion is nearly (or perhaps fully) complete.",3
"DM-984","07/21/2014 14:48:27","allow partial measurement results to be set when error flag is set","We need to be able to return values at the same time that an error flag is set.  The easiest way to do this is to have Algorithms take a Result object as an output argument rather than return it.  We'll revisit this design later. ",2
"DM-989","07/24/2014 12:05:05",".my.cnf in user HOME directory breaks setup script","Presence of {{.my.cnf}} file in the user HOME directory crashes {{qserv-configure.py}} script if parameters in {{.my.cnf}} conflict with parameters in {{qserv.conf}}.  How to reproduce: * create .my.cnf file in the home directory: {code} [client] user     = anything # host/port and/or socket host     = 127.0.0.1 port     = 3306 socket   = /tmp/mysql.sock {code} * try to run {{qserv-configure}}, it fails with error: {code} /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: connect: Connection refused /usr/local/home/salnikov/qserv-run/u.salnikov.DM-595/tmp/configure/mysql.sh: line 13: /dev/tcp/127.0.0.1/23306: Connection refused ERROR 2003 (HY000): Can't connect to MySQL server on '127.0.0.1' (111) {code}  It looks like {{~/.my.cnf}} may be a left-over from some earlier qserv installation. If I remove it and re-run {{qserv-configure.py}} now it's not created anymore. Maybe worth adding some kind of protection to {{qserv-configure.py}} in case other users have this file in their home directory.",2
"DM-991","07/24/2014 19:51:44","add query involving a blob to the integration tests","We need to add a query (or more?) to the qserv_testdata that involve blobs. Blobs are interesting because they might break some parts of the qserv if we failed to escape things properly etc. ",2
"DM-993","07/24/2014 23:01:32","improve message from qserv_testdata","Currently, when I try to run qserv-benchmark but qserv_testdata was not setup, I am getting  {code} CRITICAL Unable to find tests datasets. -- FOR EUPS USERS : Please run :    eups distrib install qserv_testdata    setup qserv_testdata FOR NON-EUPS USERS : Please fill 'testdata_dir' value in ~/.lsst/qserv.conf with the path of the directory containing tests datasets or use --testdata-dir option. {code}  It is important to note in the section for eups users that this has to be called BEFORE qserv is setup, otherwise it has no effect. ",1
"DM-999","07/28/2014 11:23:18","rename config file(s) in Qserv","Rename local.qserv.cnf to qserv-czar.cnf. It is quite likely there are some other config files that would make sense to rename. If you see some candidates, let's discussion on qserv-l and do the renames.",1
"DM-1001","07/28/2014 12:29:08","Modify assertAlmostEqual in ip_diffim subtractExposures.py unit test","In unit test, the comparison     self.assertAlmostEqual(skp1[nk][np], skp2[nk][np], 4)   fails.  However if changed to    self.assertTrue(abs(skp1[nk][np]-skp2[nk][np]) < 10**-4)  which is the desired test, this succeeds.   This ticket will remove all assertAlmostEquals from subtractExposure.py and replace with the fundamental comparison operator of the absolute value of the differences.",1
"DM-1004","07/29/2014 14:22:10","Provide Task documentation for ModelPsfMatchTask","See Description (it's currently called PsfMatch) ",2
"DM-1010","08/05/2014 13:39:40","fix names of meas_base plugins to match new naming standards","Some meas_base plugins still have old-style algorithm names.",1
"DM-1012","08/05/2014 13:44:44","remove temporary workaround in new SkyCoord algorithm","SingleFrameSkyCoordPlugin is using the Footprint Peak, not the centroid slot.  According to comments in the code, this is a workaround for some problem with centroids.  This needs to be fixed.",1
"DM-1013","08/05/2014 13:51:07","Classification should set flags upon failure","The classification algorithm claims it can never fail.  It can, and should report this.",2
"DM-1015","08/05/2014 14:29:56","convert GaussianFlux to use shape, centroid slots","We should cleanup and simplify the GaussianFlux algorithm to simply use the shape and centroid slot values instead of either computing its own or having configurable field names for where to look these up.",1
"DM-1017","08/05/2014 15:51:57","fix testForced.py","testForced.py is currently passing even though it probably should be failing: it's trying to get centroid values from a source which has neither a valid centroid slot or a Footprint with Peaks (I suspect because transforming a footprint might remove the peaks).  Prior to DM-976, that would have caused a segfault; on DM-976, I've turned it into an exception, which is then turned into a warning by the measurement framework.",2
"DM-1018","08/06/2014 11:40:26","Fix incorrect eupspkg config for astrometry_net","The clang patch from 8.0.0. version was (correctly) deleted. However, the patch identity was still left in the eupspkg config's protocol.  This will delete the last vestige of the formerly necessary clang patch.",2
"DM-1022","08/08/2014 16:10:29","fix warnings related to libraries pulled through dependent package","This came up during migrating qserv to the new logging system, and it can be reproduced by taking log4cxx, see DM-983, essentially:  {code} eups distrib install -c log4cxx 0.10.0.lsst1 -r http://lsst-web.ncsa.illinois.edu/~becla/distrib -r http://sw.lsstcorp.org/eupspkg {code}  cloning log package (contrib/log.git), building it and installing in your stack, and finally taking the branch u/jbecla/DM-207 of qserv and building it.  The warnings looks like:  {code}/usr/bin/ld: warning: libutils.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libpex_exceptions.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) /usr/bin/ld: warning: libbase.so, needed by /usr/local/home/becla/qservDev/Linux64/log/1.0.0/lib/liblog.so, not found (try using -rpath or -rpath-link) {code}  and they show up when I build qserv package, and are triggered by the liblog. I suspect sconsUtils deal with that sort of issues, but since we have our own scons system for qserv it is not handled. Fabrice, can you try to find a reasonable solution for that? Thanks!",0.5
"DM-1028","08/11/2014 14:46:59","qserv-version.sh produces incorrect version number","I have just installed qserv on a clean machine (this is in a new virtual machine running Ubuntu12.04) which got me version 2014_07.0 installed: {code} $ eups list qserv    2014_07.0    current b76 $ setup qserv $ eups list qserv    2014_07.0    current b76 setup $ echo $QSERV_DIR /opt/salnikov/STACK/Linux64/qserv/2014_07.0 {code}  but the {{qserv-version.sh}} script still thinks that I'm running older version: {code} $ qserv-version.sh 2014_05.0 {code}",2
"DM-1029","08/11/2014 15:12:47","""source"" command is not in standard shell","{{qserv-start.sh}} script fails when installed on Ubuntu12.04: {code} $ ~/qserv-run/2014_05.0/bin/qserv-start.sh /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 4: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: source: not found /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: 6: /home/salnikov/qserv-run/2014_05.0/bin/qserv-start.sh: check_qserv_run_dir: not found {code}  It complains about {{source}} command. {{source}} is not standard POSIX shell command, it is an extension which exists in many shells. Apparently in older Ubuntu version {{/bin/sh}} is stricter about non-standard features.   To fix the script one either has to use standard . (dot) command or change shebang to {{#!/bin/bash}}. This of course applies to all our executable scripts.",2
"DM-1038","08/11/2014 22:29:12","S15 Implement Query Mgmt in Qserv","Initial version of system for managing queries run through qserv. This includes capturing information about queries running in Qserv. Note, we are not dealing with query cost estimate here, (it will be covered through DM-1490).",40
"DM-1041","08/12/2014 14:13:17","eliminate confusing config side-effects in CalibrateTask","CalibrateTask does some unexpected things differently if you configure it certain ways, because it perceives certain processing as only being necessary to feed other steps.  In particular, if you disable astrometry and photometric calibration, it only runs measurement once, because it assumes the only purpose of the post-PSF measurement is to feed those algorithms.  This (as well as poor test coverage) made it easy to break CalibrateTask in the case where those options are disabled a few branches back.  After conferring with Simon and Andy, we think the best solution is to remove this sort of conditional processing from CalibrateTask, which should also make it much easier to read.  Instead, we'll always do both the initial and final phase of measurement, even if one of those phases is not explicitly being used within CalibrateTask itself.",1
"DM-1045","08/13/2014 11:27:12","Create a permanent and accessible mapping of the BB# and the bNNN. ","Create a permanent and accessible mapping of the BB# and the bNNN.   The users are interested in the BB# since is is used to point to the STDIO file form the entire stack build. The bNNN is needed because the daily life of the developer revolves around the stack tagged alternately by the bNNN tags and/or the DM Release tags. ",2
"DM-1054","08/14/2014 11:14:23","init.d/qserv-czar needs LD_LIBRARY path","With the addition of log we now need to find some shared libraries from stack. Current version of qserv-czar init.d script does not capture LD_LIBRARY_PATH, so we should add it there. ",0.5
"DM-1055","08/14/2014 13:44:22","Remove unnecessary pieces from qserv czar config","The config file for the qserv czar has some items that are no longer relevant, and in this issue, we focus on the ones that are clearly the responsibility of our qserv css.  This ticket includes: -- removing these items from the installation/configuration templates -- removing these items from sample configuration files -- removing these items from the code that reads in the configuration file and sets defaults for these items -- fixing things that seem to break as a result of this cleanup.  danielw volunteers to assist on the last item, as needed.  ",2
"DM-1058","08/18/2014 13:48:39","fix SubSchema handling of ""."" and ""_""","SubSchema didn't get included in the rest of the switch from ""."" to ""_"" as a field name separator.  As part of fixing this, we should also be able to simplify the code in the slot definers in SourceTable.",1
"DM-1059","08/18/2014 14:00:16","track down difference in SdssShape implementation","The meas_base version of SdssShape produces slightly different outputs from the original version in meas_algorithms, but these should be identical.  We should understand this difference rather than assume its benign just because it's small.",2
"DM-1067","08/19/2014 13:34:06","move algorithm implementations out of separate subdirectory","We should move the code in the algorithms subdirectory (and namespace) into the .cc files that correspond to individual algorithms.  They should generally go into anonymous namespaces there.  After doing so, we should do one more test to compare the meas_base and meas_algorithms implementations.",1
"DM-1068","08/19/2014 14:11:39","audit and clean up algorithm flag and config usage","Check that meas_base plugins and algorithms have appropriate config options and flags (mainly, check that there are no unused config options or flags due to copy/paste relics).",1
"DM-1070","08/19/2014 14:14:23","switch default table version to 1","Now that all tasks that use catalogs explicitly set the table version, it should be relatively straightforward to set the default version to 1 in afw.  Code that cannot handle version > 0 tables should continue to explicitly set version=0.",2
"DM-1071","08/19/2014 14:15:40","Switch default measurement tasks to meas_base","We should set the default measurement task in ProcessImageTask to SingleFrameMeasurementTask, and note that SourceMeasurementTask and the old forced photometry drivers are deprecated.",2
"DM-1072","08/19/2014 14:17:02","create forced wrappers for algorithms","We have multiple algorithms in meas_base which could be used in forced mode but have no forced plugin.  We should go through the algorithms we have implemented and create forced plugin wrappers for these.",1
"DM-1073","08/19/2014 14:18:09","remove old forced photometry tasks","After meas_base has been fully integrated, remove the old forced photometry tasks from pipe_tasks",1
"DM-1076","08/19/2014 14:52:21","convert afw::table unit tests to version 1","Most afw::table unit tests explicitly set version 0.  We should change these to test the new behaviors, not the deprecated ones.",2
"DM-1077","08/19/2014 15:01:35","Audit TCT recommendations to ensure that all standards updates were installed into Standards documents.","Audit TCT recommendations to ensure that all standards updates were installed into Standards documents.  It was found that the meeting recorded in: [https://dev.lsstcorp.org/trac/wiki/Winter2012/CodingStandardsChanges] failed to include two recommendations:   * recommended: 3-30: I find the Error suffix to be usually more appropriate than Exception. ** current: 3-30. Exception classes SHOULD be suffixed with Exception.  * recommended but not specifically included: Namespaces in source files: we should use namespace blocks in source files, and prefer unqualified (or less-qualified) names within those blocks over global-namespace aliases. ** Rule 3-6 is an amalgam of namespace rules which doesn't quite have the particulars desired. FYI: The actual vote was to:  ""Allow namespace blocks in source code (cc) files.""  To simplify the future audit, all other recommendations in that specific meeting were verified as installed into the standards.",2
"DM-1083","08/20/2014 12:50:52","Fix overload problems in SourceCatalog.append and .extend","This example fails with an exception: {code:py} import lsst.afw.table as afwTable schema = afwTable.SourceTable.makeMinimalSchema() st = afwTable.SourceTable.make(schema) cat = afwTable.SourceCatalog(st) tmp = afwTable.SourceCatalog(cat.getTable()) cat.extend(tmp) {code}  Expected behavior is that the last line is equivalent to {{cat.extend(tmp, deep=False)}}.",1
"DM-1088","08/21/2014 07:59:46","Investigate HTCondor config settings to control speed of ClassAd propagation","With default settings we do not have good visibility as to whether an updated ClassAd on a compute node (e.g., CacheDataList now has ccd ""S00"") will be in effect on the submit node in time for a Job to be matched to an optimal HTCondor node/slot.   There are several components (negotiator, schedd, startd) and their associated activities that could impact the time that it takes for a new ClassAd on a worker node to 'propagate' back to the submit side. We investigate these configuration settings to try to determine what thresholds for configuration settings are required to meet a given time cadence of job submissions.",2
"DM-1113","08/22/2014 10:39:51","Make the API for ISR explicit","The run method of the IsrTask currently takes a dataRef which has getters for calibration products.  This makes the task hard to re-use because one needs a butler and because the interface is opaque.  This task will make the IsrTask API more transparent.  JK: In PMCS this would be Krughoff S",20
"DM-1125","08/26/2014 13:38:38","avoid usage of measurement framework in star selectors","At least one of the star selectors uses the old measurement framework system to measure the moments of a cloud of points.  With the new versions of all the measurement plugins, it should be much easier (and cleaner) to just call the SdssShape algorithm directly, instead of dealing with the complexity of applying the measurement framework to something that isn't really an image.",3
"DM-1126","08/26/2014 15:29:40","design new Footprint API","This issue is for *planning* (not implementing) some changes to Footprint's interface, including the following:  - make Footprint immutable  - create a separate SpanRegion class that holds Spans and provides geometric operators does not hold Peaks or a ""region"" bbox (Footprint would then hold one of these).  - many operations currently implemented as free functions should be moved to methods  - we should switch the container from vector<PTR(Span)> to simply vector<Span>, as Span is nonpolymorphic and at last as cheap to copy as a shared_ptr.  The output of this issue will be a set of header files that define the new interface, signed off by an SAT design review.  Other issues will be responsible for implementing the new interface and fixing code broken by the change.",8
"DM-1135","08/26/2014 16:37:24","test how large pixel region used in galaxy fitting needs to be","Using simulations built on DM-1132 and driver code from DM-1133, test different pixel region sizes and shapes, and determine at what point shear bias due to finite fit region drops below a TBD threshold.",20
"DM-1137","08/26/2014 19:07:34","Evaluate python/c++ documentation generation and publication tools ","This epic related to documentation that is provided as part of normal development activities. The desire is to keep this documentation in and near the codebase as this is best practice for it being maintainable. At the other end, we wish to publish this documentation in a coherent and searchable way for users. A number of tools exist in this area and this item requires a preliminary evaluation to be made.   This is part of curating our documentation infrastructure.  [FE 75% DOC 100% starting August 20th] ",20
"DM-1138","08/26/2014 19:23:33","Demonstrate & iterate with team on documentation toolchain   ","Following from DM-1137, this epic relates to demonstrating various options for documentation tools workflows to the team, gathering input as to the preferred solution, adopting a workflow, and defining any specific implementation choices.   This is part of curating our documentation infrastructure. ",5
"DM-1143","08/26/2014 19:33:31","Investigate candidates for Verification and Integration Data Sets","The task here is to develop a data set that can be used both for continuous integration (build tests) and automatic QA (integration tests). We want to maximise the richness of the data set in terms of its usefulness, but minimise it in terms of its size. DN to co-ordinate contributions.     [DN 95%  FE 5%]",40
"DM-1147","08/29/2014 08:36:27","Create a top-level qserv_distrib package","qserv_distrib will be a meta-package embedding qserv, qserv_testdata and partition.",2
"DM-1151","08/29/2014 15:19:09","Fix example of IsrTask to be callable with data on disk","Currently the example of the IsrTask takes a fake dataref.  This is hard to use with real data.  In DM-1113 we will update IsrTask to not take a dataRef.  This will make it easy to update the example script to work with real data.  This ticket will also include removing from the unit tests any fake dataRefs that have become unnecessary as a result of DM-1299.  ",2
"DM-1152","08/29/2014 18:32:13","Css C++ client needs to auto-reconnect","The zookeeper client in C++ that the czar uses doesn't auto-reconnect. This is a capability provided in the kazoo library that qserv's python layer provides, but isn't provided in the c++ client.  The zookeeper client disconnects pretty easily: if you step through your code in gdb, the zk client will probably disconnect because its threads expect to keep running. zk sessions may expire too. Our layer should reconnect unless there is really no way to recover without assistance from the calling code (e.g. configuration is wrong, etc.).  This ticket includes only basic reconnection attempting, throwing an exception only when some ""reconnection-is-impossible"" condition is met.",2
"DM-1160","09/01/2014 14:29:41","SUI catalog and image interactive visualization with LSST data","Using the current software components developed in IPAC to put together a prototype of visualization capabilities. The purpose is to exercise the data access APIs developed by SLAC and get feedback from DM people and potential users of the tool.  20% Goldina, Zhang 10% Roby, Ly, Wu, Ciardi ",20
"DM-1161","09/02/2014 12:12:47","Cleanup SdssShape","We should do a comprehensive cleanup of the SdssShapeAlgorithm class.  This includes removing the SdssShapeImpl interface (never supposed to have been public, but it became public) from other code that uses it, and integrating this code directly into the algorithm class.  We should also ensure that the source from which the algorithm is derived is clearly cited -- that's Bernstein and Jarvis (2002, http://adsabs.harvard.edu/abs/2002AJ....123..583B); see also DM-2304.",8
"DM-1188","09/17/2014 13:25:49","rewrite low-level shapelet evaluation code","While trying to track down some bugs on DM-641, I've grown frustrated with the difficulty of testing the deeply-buried (i.e. interfaces I want to test are private) shapelet evaluation code there.  That sort of code really belongs in the shapelet package (not meas_multifit) anyway, where I have a lot of similar code, so on this issue I'm going to move it there and refactor the existing code so it all fits together better.",2
"DM-1192","09/18/2014 10:11:14","Write a transition plan to move gitolite and Stash repositories to GitHub","As recommended by the SAT meeting on 2014-09-16, we need this document to promote the use of GitHub by other subsystems within the project and to understand the impacts on DM.  The plan should include, but is not limited to: * Whether and how the repositories should be reorganized. * How existing commit attributions will be translated. * Moving comments in Stash to GitHub",20
"DM-1195","09/18/2014 18:09:24","There is a bug in the prescan bbox for megacam.","The bounding box of the prescan region in the megacam camera should have zero y extent (I think).  Instead it goes from y=-1 to y=2.  This is either a bug in the generation of the ampInfoTables or in the way the bounding boxes are interpreted.",1
"DM-1196","09/18/2014 18:32:01","exampleUtils in ip_isr is wrong about read corner","https://dev.lsstcorp.org/cgit/LSST/DMS/ip_isr.git/tree/examples/exampleUtils.py#n95 Says that the read corner is in assembled coordinates.  This is not true, it is in the coordinates of the raw amp.  That is, if the raw amp is in electronic coordinates (like the lsstSim images) it is always LL, but if it is pre-assembled, it may be some other corner.  This should probably use the methods in cameraGeom.utils to do the image generation.",1
"DM-1197","09/18/2014 19:49:31","Support some mixed-type operations for Point and Extent","The current lack of automatic conversions in python is pretty irritating, and I think it's a big enough issue for people writing scripts that we should fix it.  In particular, allow {code} Point2D + Extent2I Point2D - Extent2I Point2D - Point2I  Extend2D + Extent2I Extend2D - Extent2I {code} (and the respective operations in the opposite order where well defined) It would also be good to allow the all functions expecting PointD to accept PointI, but I'm not sure if swig makes this possible.  It's probably not worth providing C++ overloads for all of these functions (and to be consistent we should probably do all or none).  I realize that you invented these types to avoid bare 2-tuples, but I'm not convinced that we shouldn't also provide overloads to transparently convert tuples to afwGeom objects.",2
"DM-1211","09/24/2014 17:30:16","anaconda is too outdated to work with pip","The version of anaconda distributed with the stack is too outdated to be used with pip (and probably other things). The issue is an unsafe version of ssh.  A workaround is to issue this command while anaconda is setup: {code} conda update conda {code} Warning: it is unwise to try to update anaconda itself (with ""conda update anaconda"") because that will revert some of the changes and may result in an unusable anaconda.  I think what is required is an obvious change to ups/eupspkg.cfg.sh  The current version of anaconda is 2.0.1 based on http://repo.continuum.io/archive/  Note: there is no component for anaconda. I will submit another ticket.",2
"DM-1213","09/24/2014 17:41:06","cleanup order/grouping of header files","We want: * header for the class * then system * then third party * then lsst * then qserv  We currently don't have the ""lsst"" group (with a few exceptions), and we call the last one ""local"" in most places.",1
"DM-1217","09/25/2014 13:52:17","Refactor meas_base Python wrappers and plugin registration","meas_base currently has a single Swig library (like most packages), defined within a single .i file (like some packages).  It also registers all of its plugins in a single python module, plugins.py.  Instead, it should:  - Have two Swig libraries: one for the interfaces and helper classes, and one for plugin algorithms.  Most downstream packages will only want to %import (and hence #include) the interface, and having them build against everything slows the build down unnecessarily.  The package __init__.py should import all symbols from both libraries, so the change would be transparent to the user.  - Have separate .i files for each algorithm or small group of algorithms.  Each of these could %import the interface library file and the pure-Python registry code, and then register the plugins wrapped there within a %pythoncode block.  That'd make the implementation of the algorithms a bit less scattered throughout the package, making them easier to maintain and better examples for new plugins.",3
"DM-1218","09/25/2014 16:08:29","Support multiple-aperture fluxes in slots","We should be able to use multiple-aperture flux results in slots.  While this is technically possible already by setting specific aliases, it doesn't work through the usual mechanisms for setting up slots (the define methods in SourceTable and the SourceSlotConfig in meas_base).  After addressing this, we should remove the old SincFlux and NaiveFlux algorithms, as the new CircularApertureFlux algorithm will be able to do everything they can do.",2
"DM-1231","09/30/2014 03:25:03","LSE-69: Bring to Phase 3","Reflects work needed in Summer 2015",0
"DM-1232","09/30/2014 03:27:04","LSE-72: Bring Summer 2014 work to CCB approval","Remaining work is to proofread the SysML-ization by Brian Selvy of the LSE-72 draft, do any required cleanup in conjunction with the OCS team, and advocate for LCR-202 (already exists) at the CCB.",3
"DM-1233","09/30/2014 03:43:46","Refine requirements and use cases for Level 3 facilities","Refine the requirements and use cases for the three branches of Level 3 capabilities exposed to users:  * Level 3 programming toolkit (user reconfiguration / extension of DM pipelines and stack)  * Level 3 compute cycle delivery (user access to 10% of compute base)  * Level 3 data product storage    Deliverables:  * Refinement, if necessary, to Level 3 requirements in DMSR  * Flowed-down requirements as a separate document.  Sufficient detail to allow a breakdown of the deliverables in the three areas of Level 3 by annual release cycle through construction period.",20
"DM-1237","09/30/2014 06:58:01","LSE-75: Refine WCS and PSF requirements","Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis.  Depends on the ability of the T&S group to engage with this subject during the Winter 2015 period.  Can be deferred to Summer 2015 without major impacts.  Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.",8
"DM-1242","09/30/2014 07:32:12","Risk Register refresh 1/2015","Periodic review of DM risk register contents.  Covers preparation for a review expected at the end of January 2015, the only one during Winter 2015.",3
"DM-1245","09/30/2014 07:37:06","Install scisql plugin (shared library) outside of eups stack.","sciSQL plugin is currently deployed in eups stack (i.e. $MYSQL_DIR/lib/plugin) during configuration step. Nevertheless eups stack should be immutable during configuration step.  MySQL plugin-dir option may allow to deploy sciSQL plugin outside of eups stack (for example in QSERV_RUN_DIR).",3
"DM-1251","09/30/2014 15:34:14","CSS design for query metadata v1","The goal of this ticket (and DM-1250) is to try to understand what kind of per-query metadata is necessary to provide client-transparent query processing in case when czar/proxy could die or be restarted.  Some relevant info is in the Trac: https://dev.lsstcorp.org/trac/wiki/db/Qserv/CSS/RunTimeState",3
"DM-1254","09/30/2014 15:41:35","Metadata Store - design v1","Research potential off-the-shelf candidates. Propose initial version of metadata design. ",5
"DM-1281","10/01/2014 11:01:05","add Schema method to join strings using the appropriate delimiter","Delimiters in Schema field names are version-dependent.  One can currently use {{schema[""a""][""b""].getPrefix()}} to join fields using the appropriate delimiter, but this is confusing to read.",1
"DM-1282","10/01/2014 11:18:26","multi-level replacement in Schema aliases","Schema aliases should support more than one level (i.e. an alias may resolve to another alias).",2
"DM-1285","10/01/2014 14:13:21","Improve Startup of HTCondor Jobs","Adjust configuration parameters of HTCondor config and/or submission files to improve speed at which HTCondor jobs start in both the replicator pool and worker pool.",2
"DM-1287","10/03/2014 02:40:29","Propose and document a recipe to build Qserv in eups","In-place build is available and documented.",2
"DM-1293","10/03/2014 15:13:03","Implement designed tests for processCcd","Implement the designed tests with the installed data.",8
"DM-1305","10/05/2014 19:05:22","Tests fail in shapelet when building on OS X 10.9","When building the master on pugsley.ncsa.illinois.edu, shapelet builds successfully, but two tests fail:  {code} pugsley:lsstsw mjuric$ cat build/shapelet/tests/.tests/*.failed tests/testMatrixBuilder.py  .F..... ====================================================================== FAIL: testConvolvedCompoundMatrixBuilder (__main__.MatrixBuilderTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMatrixBuilder.py"", line 310, in testConvolvedCompoundMatrixBuilder     self.assertClose(numpy.dot(matrix1D, coefficients), checkVector, rtol=1E-14)   File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/50 elements differ with rtol=1e-14, atol=2.22044604925e-16 0.175869366369 != 0.175869366369 (diff=1.99840144433e-15/0.175869366369=1.13629876856e-14)  ---------------------------------------------------------------------- Ran 7 tests in 0.323s  FAILED (failures=1) tests/testMultiShapelet.py  ...F... ====================================================================== FAIL: testConvolveGaussians (__main__.MultiShapeletTestCase) ---------------------------------------------------------------------- Traceback (most recent call last):   File ""tests/testMultiShapelet.py"", line 88, in testConvolveGaussians     self.compareMultiShapeletFunctions(msf3a, msf3b)   File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 107, in compareMultiShapeletFunctions     self.compareShapeletFunctions(sa, sb, rtolEllipse=rtolEllipse, rtolCoeff=rtolCoeff)   File ""/Users/mjuric/test/lsstsw/build/shapelet/python/lsst/shapelet/tests.py"", line 86, in compareShapeletFunctions     rtol=rtolEllipse)   File ""/Users/mjuric/test/lsstsw/stack/DarwinX86/utils/9.2+8/python/lsst/utils/tests.py"", line 328, in assertClose     testCase.assertFalse(failed, msg=""\n"".join(msg)) AssertionError: 1/5 elements differ with rtol=1e-14, atol=2.22044604925e-16 2.44929359829e-16 != 1.13310777953e-15 (diff=8.881784197e-16/1.13310777953e-15=0.783842839795)  ---------------------------------------------------------------------- Ran 7 tests in 0.131s  FAILED (failures=1) {code}  ===============  More info on pugsley.ncsa.illinois.edu:   pugsley:lsstsw mjuric$ sw_vers  ProductName: Mac OS X  ProductVersion: 10.9.5  BuildVersion: 13F34   pugsley:lsstsw mjuric$ clang -v  Apple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn)  Target: x86_64-apple-darwin13.4.0  Thread model: posix  ============  The files are in {{/Users/mjuric/test/lsstsw/build/shapelet/}}.",1
"DM-1309","10/06/2014 04:32:58","Edit agreed-upon changes into Word version of LSE-69","A meeting around 9/26/2014 agreed on a set of revisions to LSE-69, with some language still needed from [~gpdf].  This action is to edit the tracked-changes Word version of LSE-69 containing the notes from that meeting into a final copy that can be reviewed by the Camera team and used as input to editing the SysML version of the ICD.",3
"DM-1310","10/06/2014 04:34:55","Create change request for LSE-69","Create a change request to bring LSE-69 up to date and capture the Summer 2014 work.",1
"DM-1311","10/06/2014 04:36:53","Enter LSE-69 update into EA as SysML","Covers entering the contents of the LSE-69 update into EA as SysML, with associated updating of diagrams, and the creation of a docgen'ed version for CCB action.",1
"DM-1312","10/06/2014 04:40:50","Proofread docgen'ed version of LSE-72","Brian Selvy is producing a SysML version of the LSE-72 updated edited by [~gpdf].  The action here is to proofread the docgen of that version once it is ready.",2
"DM-1313","10/06/2014 05:01:34","Identify Conditions information in LSE-130 that is required for Alert Production","LSE-69 declares that there are two categories of Conditions data (telemetry) required by DM from the Camera: those items that are needed for Alert Production (for which the AP components at the Base will need a whitelist, and for which the Camera has a tighter latency requirement), and those that are not (but are then presumably needed in DRP or other deferred productions).  It states that the subset needed for AP should be enumerated in LSE-130.  The action here is to create an initial version of that list.",2
"DM-1314","10/06/2014 07:51:25","Publish Qserv S14 version on lsst distribution server","In order to publish this version please tag Qserv master tip with ""2014_09.0"" and then run: {code:bash} ssh lsstsw@lsst-dev # command below can't be runned in buildbot, as it doesn't support qserv_distrib build rebuild -t 2014_09.0 qserv_distrib # bXXX is provided by previous command publish -t qserv -b bXXX qserv_distrib publish -t 2014_09 -b bXXX qserv_distrib {code} ",0.5
"DM-1316","10/06/2014 13:34:23","Deploy LSST stack within OpenStack instances on ISL testbed","Deploy the LSST Stack within OpenStack instances within the ISL testbed -- this could be for multiple flavors CentOS, Ubuntu, etc, and this could be done by pulling Docker Images to the instances.   There will also likely be some initial debugging of starting instances within the ISL platform as a new installation has been stood up Sept 2014. ",1
"DM-1317","10/06/2014 13:40:59","Create Docker Image / Dockerfile for LSST Stack for ubuntu","Create an installation of the LSST Stack v9_2  within a Docker Image for ubuntu for easing the import of LSST software into an OpenStack instance,  We create  the image utilizing a Dockerfile to make systematic  the creation of such images. ",2
"DM-1318","10/06/2014 14:31:59","update expected results file in SDSS demo test","Update the expected outputs in the SDSS DM stack demo repo to match what we expect from the new meas_base framework.",1
"DM-1331","10/06/2014 15:13:52","squash edge errors in SdssCentroid","SdssCentroid doesn't trap exceptions that are thrown due to being too close to the edge, resulting in noisy warnings in the logs.  Instead, it should catch the low-level exception and re-throw as MeasurementError, after defining a flag field for this specific failure mode.",1
"DM-1332","10/06/2014 15:17:49","address no-shape warnings in GaussianFlux","GaussianFlux relies on the shape slot, and puts noisy warnings in the logs when the shape slot fails.  However, we probably don't want to add a new flag for GaussianFlux to indicate this failure mode, because it'd be entirely redundant with the shape slot flag.  We should figure out some other way to squash this warning - how we do that may depend on whether this is addressed before or after the C++ redesign.  We should also consider having GaussianFlux add an alias to the schema to point back at the shape slot flag, creating what looks like a specific flag for this failure while actually just being a link back to the shape slot flag.  That's probably not worth doing within the current C++ interface, however, as it'd require some unpleasant mucking around with ResultMappers.",2
"DM-1333","10/06/2014 15:28:05","resolve factor of two difference in GaussianFlux","After changing the implementation of GaussianFlux to use the shape slot rather than estimate the shape itself by re-running the SdssShape code, Perry saw a 5-15% difference in the fluxes (I'm not sure of the sign).  The new behavior (using the shape) is consistent with what we'd have gotten with the old code when the little-used ""fixed"" config option was enabled (not surprising, as that just looked up the SdssShape measurement by name, instead of via slots).  I suspect the difference is coming in because of the factor of two between SdssShape's ""raw"" measurements - the actual Gaussian-weighted moments - and the factor of 2 it applies to make its measurements equivalent to ideal unweighted moments.  The correct weight function to use for GaussianFlux includes this factor of 2 (i.e. it's larger than the ""raw"" moments), and it's likely either the old code wasn't including this or the new code isn't.  We need to determine which one, and if necessary, fix the new code.",2
"DM-1334","10/06/2014 22:56:10","Test the creation of basic OpenStack instances on the new ISL testbed [IceHouse]","A new version & implementation of the ISL OpenStack testbed is up and running. The new cloud is using IceHouse, the ninth OpenStack release. We get started on this platform by verifying that basic instance creation is working.  We target the creation of an instance through the (Horizon) GUI interface,  and via the nova CLI. ",1
"DM-1335","10/06/2014 23:18:19","Create instance with a Floating IP Associated through the nova CLI","We see that in working with the Horizon GUI, it is fairly straightforward to give an instance a public IP address by associating a Floating IP with the current local IP.    However, we will want to be able to accomplish this task both remotely and programmatically within workflow.  As a step towards this, we target the solution of this via the nova CLI.",2
"DM-1340","10/07/2014 14:04:32","Read through log4cxx documentation and log.git code","Read through the log4cxx documentation and become familiar with how the log.git package is set up.",2
"DM-1343","10/07/2014 14:11:29","Write Unit test for new DM message appender class","Write unit tests for DM message appender class.  This might also require some tests for a configurator class, if that class is created.",2
"DM-1347","10/07/2014 14:36:33","Refine Event base class to allow ActiveMQ filterable settings","The current Event.cc base class needs to be refined to remove and old-style data release terms that aren't used anymore.  Plus, it needs to be easily extensible to allow other types of dictionaries of terms that will be used in the message headers to make them filterable on the server side.",2
"DM-1348","10/07/2014 14:37:59","Update tests to use unit test framework","The tests for this package predate the unit test framework that other package use.  Update the tests to uses the unit test framework and get rid of any duplicate  or obsolete tests.",5
"DM-1352","10/07/2014 15:31:29","ORIGINATORID value can churn too quickly.","The ORIGINATORID is a 64-bit word consisting of an IPv4 host address, 16-bit process id, and 16-bit local value.   In addition to the 16-bit process id not being standard across platforms (Mac OS >Leopard goes to 99999), the churn rate for the local value should be much higher than just 16 bits.  This could be fixed to changing ORIGINATORID to a 32-bit process id and a separate value for the local value, which would be specified together in the DM event selector.   I have to look into this more to see if this is a viable solution.  This might need to go to three separate values to future proof it (i.e., ipv6).",8
"DM-1354","10/08/2014 14:46:38","Install docker 1.1.2 in an ISL OpenStack CentOS  instance, perform basic checks","Install docker 1.1.2 in an ISL OpenStack CentOS 6.5 instance, and perform  basic checks such stopping and starting the docker daemon, changing default settings such as size limit of containers/images,  pulling  standard images from docker hub, starting containers from these images, etc. ",2
"DM-1355","10/08/2014 16:43:59","Re-arrange how Qserv directories are installed","we already touched question about installation directory structure at a meeting, maybe we can improve things by re-arranging how things are installed      we are currently installing stuff into four directories: cfg, bin, lib, proxy     to make it look more standard and to avoid clash with qserv source directories we could move cfg and proxy to a different location (something like share/qserv to make it more root-install-friendly in case we ever want to install under /usr)     this change (if you want to do it) deserves separate ticket, do not do it in this ticket ",3
"DM-1358","10/09/2014 10:11:29","End-to-end demo  fails to exit with the correct status  when Warning would be correct.","The output  of demo2012 results in an output file which is compared against a benchmarked file. Currently the comparison allows a deviation from the benchmark based ""on the the number of digits in the significands used in multiple precision arithmetic"";  that  number is currently set to 11.  An example using that setting is: @ Absolute error = 5.9973406400e-1, Relative error = 9.1865836000e-4 ##2566    #:25  <== 29.7751550737 ##2566    #:25  ==> 29.7478269835  Additionally, the current use returns: a 'Fatal' error if the code itself fails to execute correctly; a ""Warning' error if the any of the benchmarked quantities do not meet the comparison criteria; 'Success' if the comparison meets all criteria.  It is noted that the buildbot 'Warning' color indicator is not currently being displayed when the comparison fails. That is a coding error.  This Issue will: * ensure that BUILDBOT_WARNING(S) causes the correct display color when appropriate.   This Ticket has been split into two parts. This is part 1.  Part 2 is DM-1379 ",2
"DM-1360","10/09/2014 16:42:13","Fix minor loose ends from new result plumbing","Some of the more minor issues raised in comments from DM-199 were left undone prior to merging. This ticket addresses them.  For more information, please see:  https://github.com/LSST/qserv/pull/2 . ",1
"DM-1362","10/09/2014 23:27:06","Edit pull interface and other Summer 2014 work into LSE-68 in Word","Deliverable: circulate a Word-based draft of LSE-68 in which the ""push"" interface is removed, and the ""pull"" interface is refined to include the guider and other Summer 2014 work.  Note that the use of ""pull"" for the guider applies whether or not the proposed guider redesign is accepted.",2
"DM-1363","10/10/2014 01:06:28","Avoid use of ~/.my.cnf (used by css watcher)","See https://jira.lsstcorp.org/browse/DM-1258?focusedCommentId=29230&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-29230.  {{~/.my.cnf}} is used by css watcher, an optional tool used for monitoring css. It is a symlink to $QSERV_RUN_DIR/etc/my-client.cnf.  The css watcher could use MySQL credentials located in ~/.lsst/qserv.conf (used by integration tests wich are a Qserv/MySQL client)",2
"DM-1364","10/10/2014 14:04:07","replace ""bad data"" flag in SdssCentroid","SdssCentroid has a ""bad data"" flag that doesn't actually convey any information about what went wrong.  This should be replaced with one or more flags that provide more information.",3
"DM-1372","10/13/2014 17:17:19","Errors in testHealpixSkyMap.py","There is a failing unit test when healpy is supplied.  The problem is that the method boundary is not defined in the version of healpy we supply for the sims, however boundaries does exist.  If I replace boundary with boundaries, the test passes.",0.5
"DM-1375","10/13/2014 19:13:49","Create Docker Image / Dockerfile for LSST Stack for CentOS6.5","Make a Dockerfile for systematic generation of docker images using a Centos6.5 base image  containing the LSST Stack (v9_2 at the moment) and library dependencies.",2
"DM-1376","10/13/2014 19:25:38","Ensure that the partition package is C++11 clean and compiles on OSX 10.9","The LSST buildbot infrastructure recently changed to building everything with --std=c++0x, which broke the partition package, and hence automated Qserv builds. While debugging this, I discovered that the partition package does not build on OSX 10.9, and considering how minimal its dependencies are, it really should. The OSX issue can be fixed by avoiding {{using boost::make_shared}}.  The partition package should be cleaned up to avoid all use of {{using}}. If we decide to use C++11 in Qserv, then the codebase should also be modernized (in particular, there are use-cases for static_assert, nullptr, etc... ). ",2
"DM-1386","10/15/2014 16:30:54","Merge Footprints from different bands/epochs","The current concept of the deblender assumes that the inputs are  - A merged set of Footprints that define which pixels are part of the blend  - A merged set of Peaks within that merged Footprint  Please generate these merged Footprints (which will be defined in (x, y) coordinates in the tract/patch coordinate system). ",5
"DM-1387","10/15/2014 16:32:03","Generate a master list of Objects given detections in multiple bands/epochs","Once we've detected Sources in multiple bands we need to merge the positions to generate Objects.  This is a little complicated (or at least messy):  - The positions have errors  - If the seeing is different in different visits, objects may be blended in some but not all exposures  - If we use more than one detection pass (e.g. smoothing when looking for faint objects, not smoothing for bright) this has similar-but-different consequences (but we should probably deal with in the per-band processing)  - Objects move, so even if the positions are within the errors the motion may still be detectable ",5
"DM-1388","10/15/2014 18:48:03","Submit LCR for LSE-68","Create an LCR, including a summary of changes, for LSE-68.",2
"DM-1394","10/16/2014 16:45:22","Eups 1.5.4 requires each new shell to source the eups setups.sh","eups v 1.5.4 requires each new shell to source ...eups/../bin/setups.sh.  This requires the buildbot scripts: runManifestDemo.sh, create_xlinkdocs.sh, be updated to individually  do that task.  Add  demo2012: bin/demo.sh . ",2
"DM-1396","10/17/2014 00:26:11","Design CSS schema to support database deletion","Need to implement deleting databases. Deliverable: a design of the system that will be capable of deleting a distributed database including all copies of that database on all workers, all replicas of all chunks are deleted. It should be possible to ""create database x"" at any time later.",2
"DM-1404","10/21/2014 12:58:03","Create suite of Dockerfiles / docker images for LSST Stack for ubuntu, CentOS","Building on issues DM-1317 and DM-1375 where initial images and Dockerfile's were constructed, we can now use these Dockerfile's as prototypes to extend the set of Dockerfiles & images.  We observe that by making  simple (scriptable) edits to the initial Dockerfile, we can run 'docker build' to make docker images for several combinations of OS and base compiler gcc version.",2
"DM-1407","10/23/2014 10:24:18","Add return code for integration tests","Integration test have to returns non-zero when failing. This will ease use of CI or debugging tools ({{git bisect}}, buildbot).",0.5
"DM-1424","10/24/2014 10:39:48","Create persistent volume of Cinder block storage and attach to instance","We create a persistent volume of Cinder block storage and attach to working instance.  When it was created, the instance does have a specified amount of ephemeral disk, but this disk will be destroyed with the instance.  We want to test that we can create a persistent volume of block storage, attach it to an instance, format the storage with a file system, and mount the volume for use with processing, where data/output can be retained after the instance is destroyed. ",1
"DM-1431","10/27/2014 12:48:35","Process sample sdss data with LSST Stack in a Docker container in OpenStack","Process sample sdss data, starting with the lsst_dm_stack_demo, with LSST Stack in a Docker container in an OpenStack instance. ",2
"DM-1434","10/28/2014 13:16:55","State diagram for jobs in progress","Build a state diagram showing job progress throughout a run.",13
"DM-1439","10/28/2014 15:14:37","afw tests use the same name for a dummy file: test.fits","The afw package  uses a file named: test.fits in multiple testers.  If the user sets up the build to use multiple CPU (-j #), then there is the risk that the shared filename will be affected by more than one tester at a time.   In the case which provoked this Issue, the tester: testSimpleTable.py, reported that the file was missing.  A simple rerun managed to get past the error.  I recommend that the different testers use uniquely named demo files.",1
"DM-1440","10/28/2014 22:50:03","Github Transition: Naming conventions for repositories","The Simulations team has requested that repos in general and the numerous DM repos in particular are prefixed in a way that would make fitering them out of searches and display easy (for example, their repos are prefixed sims_*)  This would be an evident useability aid to DM developers and outside contributors too.   Obtain a decision on how to allow users to quickly isolate repositories they are interested in. ",3
"DM-1443","10/28/2014 22:59:01","Github Transition: pull request discussion, retention - proof of concept","Store review comments / PR discussion into relevant git repositories  Code & process to do this on a continuous basis post-transition will be a different issue, ",1
"DM-1444","10/29/2014 09:57:16","Test absence of individual components","Test absence of individual components of the AP simulator.  Bring each down, run the system, and restart just those components to see if the system still operates as expected",2
"DM-1448","10/29/2014 17:44:31","Move code for mock images into afw so it reusable.","There is some code in the exampleUtils in $IP_ISR_DIR/examples that could be of wider use.  Specifically there is code to generate mock darks, flats, and raw data from a mock camera.  There is also code to generate a mock dataRef.  It could be used more widely if moved someplace else.  Russell suggested afw.cameraGeom.utils.",1
"DM-1461","10/31/2014 18:35:40","C++ Redesign -- Result definition for custom algorithms","Additions to Jim's redesign to make it easier to define custom results.",3
"DM-1462","10/31/2014 18:38:50","Add NaN check to PixelFlags","The test of PixelFlags in measureSources.py (from measAlg) requires a check to be sure that the center inputs are not NaN.",1
"DM-1463","10/31/2014 18:40:20","SdssShape shiftMax config item is being ignored","The code we ported from meas_algorithms sets the maxShift to 2 without regard to the config item which is supposed to set that value.",1
"DM-1464","11/03/2014 08:09:33","Design Review prep for C++ redesign","Write up the design developed on DM-829 and push it through review.",1
"DM-1469","11/03/2014 15:30:20","Github Transition Plan: Reverse mirror for beta-tester repositories","Test the reverse mirror for beta-testers.  Straw man:  1. Break the mirror for anybody beta-testing github workflow 2. Mirror back to new gitolite area: ""mirror"")  Method:  https://help.github.com/articles/duplicating-a-repository/  ",1
"DM-1475","11/04/2014 16:10:54","Fix 2014_09 documentation","Replace {{NEWINSTALL_URL=http://sw.lsstcorp.org/pkgs/}} with: {{NEWINSTALL_URL=http://sw.lsstcorp.org/eupspkg/}}  and {{eups distrib install qserv_distrib 2014_10.0}} with:  {{eups distrib install qserv_distrib -t 2014_10}}",0.5
"DM-1480","11/05/2014 14:19:46","Buildbot master takes exception when exiting from mail notifier after dynamic email sent.","Buildbot master exits without posting the required statically-addressed email notification if a dynamically-addressed was sent.   This fix needs to ensure that the required (by buildbot specification) static email is sent even if it has to be directed to a dead-letter box (which it is).",3
"DM-1481","11/05/2014 15:30:53","Discover/learn what others are doing in astronomy software","Attend the annual  ADASS conference to keep up with the software development in the astronomy community.  Trey Roby, Tatiana Goldina, Xiuqin Wu plan to attend the ADASS 24.",20
"DM-1497","11/10/2014 15:02:24","Package Qserv mono-node instance in Docker","Learn Docker basics and then package a Qserv mono-node instance.",5
"DM-1498","11/10/2014 15:03:13","Package Qserv master and worker instance in Docker","Learn Docker basics and then package a Qserv mono-node instance.",5
"DM-1505","11/11/2014 13:20:13","confusing error message when enabling unregistered items in RegistryField","pex_config seems to split out this confusing error message when trying to enable (i.e. append to .names) a registry item that doesn't exist: {code:hide-linenum}   File ""/home/lam3/tigress/LSST/obs_subaru/config/processCcd.py"", line 51, in <module>     root.measurement.algorithms.names |= [""jacobian"", ""focalplane""]   File ""/tigress/HSC/LSST/lsstsw/anaconda/lib/python2.7/_abcoll.py"", line 330, in __ior__     self.add(value)   File ""/tigress/HSC/LSST/lsstsw/stack/Linux64/pex_config/9.0+26/python/lsst/pex/config/configChoiceField.py"", line 72, in add     r = self.__getitem__(value, at=at) AttributeError: 'SelectionSet' object has no attribute '__getitem__' {code}",1
"DM-1506","11/11/2014 13:38:48","Support new version of newinstall.sh","newinstall.sh now creates loadLSST.bash instead of loadLSST.sh. This has to be taken in account in Qserv automated install script: qserv-install.sh and in Qserv documentation.",0.5
"DM-1514","11/13/2014 12:40:33","calling extend with a SchemaMapper should support positional arguments","Calling {{catalog.extend(other, mapper)}} isn't equivalent to {{catalog.extend(other, mapper=mapper)}} because the second argument is the boolean {{deep}}.  When a SchemaMapper is passed as the second argument, we should recognize it for what it is.",1
"DM-1515","11/13/2014 12:57:11","sconsUtils fails to identify Ubuntu's gcc","On Ubuntu 12.04, gcc --version says: {code:hide-linenum} gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3 Copyright (C) 2011 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. {code}  This apparently isn't quite what sconsUtils expected, because it says: {code:hide-linenum} scons: Reading SConscript files ... Checking who built the CC compiler...(cached) error: no result CC is unknown version unknown {code}  Happily, everything seems to work anyway, as the fall-back options for the unknown compiler work fine with this one.",1
"DM-1524","11/17/2014 12:21:49","Switch readMetadata' return value from PropertySet to PropertyList","It'd be useful if readMetadata would return PropertyList instead of PropertySet, because in many situations order matters. For details, see discussion in DM-1517",2
"DM-1529","11/17/2014 14:38:58","Reorganize docker image repositories and align with github","A heterogeneous collection of docker images have been accumulating within the public docker repository  daues/lsstdistrib . Such a heterogeneous collection prevents the assignment of a ""latest"" tag to allow users to easily obtain the most recent image for a particular item (detailed version numbers, labels currently required.)     Thus we should break out the single repository into multiple repositories where are ""latest"" tag will be effective.  We also make  github repositories of matching names to hold the Dockerfiles which produced images (a common pattern for github/dockerhub usage, especially with automated builds; so we start this practice.) ",2
"DM-1538","11/18/2014 13:34:43","Fix qserv_testdata documentation","qserv_testdata relies on sconsUtils, and its build procedure has to be clearly documented.",0.5
"DM-1539","11/18/2014 13:40:21","Add support for mysql JDBC driver","SUI which rely on JDBC fail because they internally issue some queries that are not yet supported by Qserv. Need to patch it (in the short term), and add proper support (in the long term). This story covers the patching only. The queries that upset Qserv are listed below.   {code} SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect'  SELECT @@session.auto_increment_increment  SET NAMES latin1  SET character_set_results = NULL  SET autocommit=1  SET sql_mode='STRICT_TRANS_TABLES' {code}",2
"DM-1541","11/18/2014 14:08:19","Add support for transmitting [VAR]BINARY column data","The code that pulls data out of a {{MYSQL_ROW}} and puts it into a protobuf {{RowBundle}} does not handle binary data correctly. See [_fillRows|https://github.com/LSST/qserv/blob/master/core/modules/wdb/QueryAction.cc#L217] for the relevant code.  The issue is that the generated {{add_column(const char*)}} member function of {{RowBundle}} treats the input as C-style NULL-terminated strings. But in the case of {{BINARY}} column data (and {{VARBINARY}}/{{BLOB}} variants, maybe also {{BIT\(n\)}}), the contents can contain embedded NULLs. We are currently using such columns for user defined types in MySQL (e.g. image bounding polygons), so it's important to get this right. On the protobuf side the fix is as simple as calling {{add_column(const char* value, int size)}} instead. I'm not a MySQL C API expert, but the size will presumably have to be obtained/derived from the corresponding {{MYSQL_FIELD}}.  ",8
"DM-1545","11/19/2014 15:29:04","Span-based shrink operations for Footprint","Analogous to DM-1128, but shrinking rather than growing footprints.",5
"DM-1571","11/25/2014 11:07:06","Setup Qserv for SUI tests","Setup Qserv on lsst-db2 with and load some reasonable data set (perhaps PT 1.2). One potential caveat: we need to setup access for some accounts that are ideally other than our internal qsmaster.",2
"DM-1573","11/25/2014 14:13:38","Basic validation of LSST pipeline on HSC data","Get the pipeline running on HSC data to the point where nothing is obviously wrong.",8
"DM-1578","11/25/2014 17:56:16","Move photocal out of meas_astrom","It is confusing that photocal is in meas_astrom.  I assume that is historical.  I think it could probably live in pipe_tasks.",2
"DM-1579","11/25/2014 17:58:40","Implement replacement for A.net index files","Astrometry.net index files are hard to generate and hard to read.  We need another, more flexible, more standard system for storing reference files.  We should also be able to read FITS files and other formats, but having a standard format with the utilities to create and query them is a must.  Coming up with a format that satisfies astrometry.net's solver may be too hard, because a.net requires quads, which a non-blind solver may not need. However, a format that we can convert to something suitable for a.net would probably suffice (conversion would presumably be a separate task that is run once).  It will be easier to identify a suitable format once we have identified at least one solver other than than a.net that we wish to implement or adapt. hscAstrom appears to use a catalog of star positions, which is nice and simple.",5
"DM-1582","11/25/2014 20:04:18","Qserv spatial restrictor names are case sensitive","The SQL grammar treats Qserv spatial restrictor names case insensitively, but {{qana/QservRestrictorPlugin.cc}} does not, which means that one must use e.g. {{qserv_areaspec_box}} rather than {{qserv_areaSpec_box}}. We are loose with case in a lot of our wiki pages, so we really should fix this to avoid confusing users. Also, case insensitivity is consistent with MySQL behavior for UDF names.",1
"DM-1587","11/26/2014 11:37:24","Define structure of web form for collecting metadata about existing data sets","Web alpha version of the form (using django or Fermi Java webservices code) that collects input from users about data repositories. Authentication not covered in this version.",2
"DM-1590","11/26/2014 11:46:07","Break down & discuss DM-1074","I will take the lead on DM-1074. First step will be to sit with [~jbosch], get a feeling for what needs to be done, and sketch out a set of stories.",1
"DM-1600","12/01/2014 13:00:16","Determine if Astrometry class is desired","The question is whether the Astrometry class is the thing that is overridden or if the AstrometryTask has component configurables that are overridden.  Also, determine location default implementation.",2
"DM-1601","12/01/2014 15:04:13","Add support for c-style comments in front of queries sent to qserv","Connecting to qserv from java fails because the jdbc driver inserts comments. ""/* ... */"" in front of queries (example pasted below). The fix involves removing the comments at the proxy level   {code} SQLException: Qserv error: 'ParseException:ANTLR parse error:unexpected token: /:'  Query being executed when exception was thrown: /* mysql-connector-java-5.1.34 ( Revision: jess.balint@oracle.com-20141014163213-wqbwpf1ok2kvo1om ) */SHOW VARIABLES WHERE Variable_name ='language' OR Variable_name = 'net_write_timeout' OR Variable_name = 'interactive_timeout' OR Variable_name = 'wait_timeout' OR Variable_name = 'character_set_client' OR Variable_name = 'character_set_connection' OR Variable_name = 'character_set' OR Variable_name = 'character_set_server' OR Variable_name = 'tx_isolation' OR Variable_name = 'transaction_isolation' OR Variable_name = 'character_set_results' OR Variable_name = 'timezone' OR Variable_name = 'time_zone' OR Variable_name = 'system_time_zone' OR Variable_name = 'lower_case_table_names' OR Variable_name = 'max_allowed_packet' OR Variable_name = 'net_buffer_length' OR Variable_name = 'sql_mode' OR Variable_name = 'query_cache_type' OR Variable_name = 'query_cache_size' OR Variable_name = 'license' OR Variable_name = 'init_connect' {code}",1
"DM-1603","12/02/2014 13:09:09","Qserv scons scripts do not pick up the version of swig provided by eups","See the summary. The consequence is that the Qserv integration tests fail on systems that provide swig 2.x+ - there seems to be some implicit dependency on SWIG 1.x. The reason may be that swig is getting confused about shared_ptr to objects that are defined in one module, but used in another (recent swig reorganization split czar and css into two separate swig modules).",2
"DM-1607","12/02/2014 19:16:27","Add unit tests to test c-style comments in/around queries","I should have thought about it when doing DM-1601 but I didn't... it'd be good to add test queries that test comments before/after/inside query.",1
"DM-1608","12/03/2014 11:06:30","Move meas_algorithms unit tests to meas_base framework","The following tests in meas_algorithms need to be ported to the meas_base framework:  measure.py psfSelectTest.py testPsfDetermination.py ticket2019.py testCorrectFluxes.py (though this cannot be done until the algorithm exists)",2
"DM-1614","12/03/2014 14:53:03","Add support for mysql JDBC driver (v2)","MySQL client 4.1 and higher is stripping out comments before sending them to server, so the fixes done in DM-1539 are not sufficient.",1
"DM-1615","12/03/2014 15:48:52","Design and implement CSS structure for distributed Qserv setup","For management of the distributed databases/tables we need info in CSS about all workers and tables. The info will be created by data loader and updated by replicator which do not exist yet. For this issue we need to provide python API which can fill the same information in CSS so that we can build and test other pieces needed for this epic.",5
"DM-1616","12/03/2014 15:56:25","Implement remote host access for management framework","To manage remote workers we need a way to access services on remote machines that run workers. Services may be something like mysql (which we would prefer to run without TCP port open) and optionally xrootd. This ticket will implement Python module which will hide a complexity of doing things like ssh/port forwarding/authentication from the client.",5
"DM-1617","12/03/2014 16:05:22","Client library for accessing distributed database/table information from CSS","Provide Python interface for accessing information in CSS which is relevant to distributed management, such as database/table/node data. This interface can be used also by data loader and replicator.",8
"DM-1621","12/03/2014 18:00:56","Add unit test to to verify zookeeper versioning works","This is a follow u pto DM-1453, we need to add a unit test that will prove that mismatched versions in zookeeper and software are properly handled.",1
"DM-1626","12/03/2014 18:12:23","Build 2014_12 Qserv release","The title says it all. Please open ticket for next release when closing this one.",1
"DM-1629","12/03/2014 22:55:57","Adopted/Retired RFCs are not counted as resolved","Also, marking an RFC as Adopted brings up a box with a message applicable to the Closed status.",1
"DM-1630","12/03/2014 22:58:23","New RFCs should result in dm-devel E-mails and HipChat postings","Email notices of new RFCs filed with a DM component go to dm-devel Email notices of new RFCs filed with a DM component go to Data Management chat room Email notices of all new RFCs go to Bot: RFC room ",1
"DM-1632","12/04/2014 11:33:57","Build 2014_11 Qserv release","The title says it all. Please open ticket for next release when closing this one.  Tasks to do:  - publish last buildbot build under a temporary eups-tag (""qserv-dev"")and test it, if it works fine: - create git-tags for Qserv and dependencies - publish the release with eups-tags ""qserv"" and ""YYYY_MM"" - generate and publish the doc for release ""YYYY_MM"" - update release number in Qserv code and set ""YYYY_MM+1"" as release in dev and ""YYYY_MM"" as stable release (update {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}}) - generate and publish the doc for release ""YYYY_MM+1""  - look at the doc - commit  {{admin/bin/qserv-version.sh}}, {{doc/source/conf.py}}, {{doc/source/toplevel.rst}} with current ticket number   This procedure should be validated and documented.",1
"DM-1635","12/04/2014 14:31:37","Remove redundant CORS headers from Firefly's http response","Make sure CORS related headers are not sent when the Origin header is missing.  Firefox does not like it.",1
"DM-1653","12/11/2014 12:32:38","Extend data loading script to support multi-node setup","Implement simplest use case for data loading with one or more worker database separate from czar database. Simplest means minimal functionality in what concerns access to workers, just assume for now that we can connect to every worker directly using regular TCP connection. It should be possible to just add a list of worker nodes as an option to loader script and send the chunks in some random order to the workers in that list. Of course chunks for different tables should end up on the same host, so some form of chunk management is needed (use for now whatever is defined in CSS doc on trac).",8
"DM-1655","12/12/2014 12:49:08","Working with SLAC on definition of metadata store","Follow up metadata store schema development to ensure SUI will be able to use it. Define the fields that should go into Data Definition table. Define the fields that must be present in the image metadata table, which SUI will be searching.",2
"DM-1658","12/12/2014 15:51:28","Add git bisect tool for Qserv repos","{code:bash} fjammes@clrlsst-dbmaster-vm:~/src/qserv (u/fjammes/DM-627 $%) $ qserv-test-head.sh -h  Usage: qserv-test-head.sh [options]    Available options:     -h          this message     -q          quick: only rebuild/install new Qserv code,                 and perform test case #01    Rebuild from scratch, configure and run integration tests against   a Qserv git repository.   Pre-requisite:     source loadLSST.bash     setup qserv_distrib -t qserv     setup -k -r ${QSERV_SRC_DIR}    Can be used with 'git bisect' :     cd ${QSERV_SRC_DIR}     git bisect start     git bisect bad     git bisect good git-commit-id     git bisect run /home/fjammes/src/qserv_testdata/bin/qserv-test-head.sh {code}  Code is in DM-627 ticket branch.",2
"DM-1659","12/12/2014 16:42:48","Aperture Flux back into an abstract class","The last change to ApertureFlux to make it work with the new C++ design changed ApertureFluxAlgorithm into an instantiatable class.  However, I have now figured out how to make this work with SWIG while still allowing measure and fail to be defined by default at the ApertureFlux level..  So this issue is to put things back in order.",1
"DM-1661","12/12/2014 18:47:03","czar log4cxx link/load bug","Under ubuntu 14.04 (at least), the czar falls over at load time with an unresolved sym for typeinfo for log4cxx::helpers::ObjectPtrBase while loading the css python wrapper shared lib.",2
"DM-1667","12/16/2014 16:22:02","Install PgMySQL and use to connect to local Qserv.","Used ""pip"" to install it. ""Conda"" should work as well. Therefore, it should be easy to make it part of the delivered system: VM, container, tar file, after the fact download, etc. It has documentation, uses the MIT license, under active development and available from PyPI. DB connection is straight forward and requires little experience to get meaningful work done.",2
"DM-1668","12/16/2014 16:23:43","Create SQL code to read Qserv into Python Pandas data frame","This works well, at least for a simple case. You can move directly from a query statement to a Pandas data frame for analysis in just a few lines of code. Here is the start of an iPython Qserv session showing how easy it is.  In [6]: import pandas as pd In [7]: import pymysql as db  In [8]: conn = db.connect(host='lsst-db1.ipac.caltech.edu',port=4040, user='qsmaster', passwd='', db='LSST')  In [11]: df = pd.read_sql(""select deepCoaddId, tract, patch, ra, decl from DeepCoadd"", conn)  In [12]: df Out[12]:     deepCoaddId  tract   patch        ra      decl 0      26607706      0  406,11  0.669945  1.152218 1      26673242      0  407,11  0.449945  1.152218 2      26804242      0   409,2  0.011595 -0.734160 3      26673154      0   407,0  0.449945 -1.152108 … ",2
"DM-1670","12/16/2014 16:26:12","Begin looking at how Python Pandas can be used for LSST data analysis.","Pandas is well integrated with the other parts of SciPy: numpy, matlibpy, etc.  It’s a good candidate for data analysis, especially where time series are involved. However, there are no multidimensional columns, poor metadata support for FITS files and a need to use masks instead of NaN values. These may, or may not, be problems.  There is a 400 page book about Pandas, so it will take some further time to learn its value, especially with astronomical data in different situations.",5
"DM-1673","12/16/2014 20:45:01","Allow SWIG override for broken SWIG installations","Dependency on SWIG 2.0+ was introduced into Qserv, and this broke Qserv building on systems relying on SWIG 1.3.x.  This ticket introduces basic code to override SWIG_LIB on those systems to allow use of the broken installation (some SWIG search paths are fixed during its build process otherwise).",1
"DM-1685","12/17/2014 17:22:32","Minor bug in a test","tests/centroid.py has a bug in testMeasureCentroid: ""c"" is undefined in the following bit of code: {code} if display:     ds9.dot(""x"", c.getX(), c.getY(), ctype=ds9.GREEN) {code}",1
"DM-1695","12/17/2014 20:13:14","Implement interfaces for Data Access Services","Implement proof of concept, skeleton of the prototype. The work will continue in follow up stories in February and in S15.",8
"DM-1705","12/18/2014 13:22:14","S15 Tune Qserv","Fix scalability and performance issues uncovered through large scale tests DM-1704",100
"DM-1706","12/18/2014 13:24:21","S15 Analyze Qserv Performance","Final analysis of Qserv performance, measure KPIs. Based on LDM-240, we are aiming to demonstrate:  * 50 simultaneous low volume queries, 18 sec/query  * 5 simultaneous high-volume queries, 24 h/query  * data size: 10% of DR1 level.  * Continuous running for 24 h with no software failures.  ",5
"DM-1709","12/18/2014 15:07:07","Implement result sorting for integration tests","We need to be able to sort results, because we can't always rely on ORDER BY. So we need a formatting per query in the integration tests (sort result for some, don't sort for others etc.)    The following queries have been disabled because we don't have result sorting, so once it is implemented, we will need to re-enabled them prior to closing this ticket:  {code}  case02/queries/0003_selectMetadataForOneGalaxy_withUSING.sql  case02/queries/3001_query_035.sql  case02/queries/3008_selectObjectWithColorMagnitudeGreaterThan.sql  case02/queries/3011_selectObjectWithMagnitudes.sql  case02/queries/3011_selectObjectWithMagnitudes_noalias.sql  {code}",8
"DM-1710","12/18/2014 16:04:45","ValueError in lsst.afw.table.Catalog.extend()","{code} from lsst.afw.table import BaseCatalog, Schema  s = Schema() c1 = BaseCatalog(s) c2 = BaseCatalog(s)  c1.extend(c2) {code}  The above fails, saying:  {code} Traceback (most recent call last):   File ""test.py"", line 7, in <module>     c1.extend(c2)   File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/afw/10.0+3/python/lsst/afw/table/tableLib.py"", line 6909, in extend     _tableLib.BaseCatalog_extend(self, iterable, deep) ValueError: invalid null reference in method 'BaseCatalog_extend', argument 3 of type 'lsst::afw::table::SchemaMapper const &' {code}",1
"DM-1713","12/18/2014 20:49:43","S15 Image & File Archive v2","System for tracking existing image data sets integrated with metadata services.",5
"DM-1715","12/18/2014 22:23:17","Disable query killing","Apparently killing a query through Ctrl-C is confusing xrootd. Disable query killing (which seems to be only partly implemented).",1
"DM-1720","12/19/2014 14:07:22","Make secondary index for director table only","Following discussion on qserv-l, we only need to generate ""secondary"" index for director table, no other table is supposed to have it. Need to modify data loader to recognize which table is director table and generate index only for that table. ",2
"DM-1721","12/19/2014 14:13:30","S15 Improve Query Coverage in Qserv","Query coverage in the qserv integration testing is very limited, we have been turning off more and more queries and we were making the qserv code and the data loader more strict. This epic covers work (fixes and improvements) related to * re-enabling test queries marked as ""fixme"" (when it make sense, some queries are for features that are not implemented yet) * adding more queries to test interfaces and features that are implemented but are not currently tested.",40
"DM-1731","12/31/2014 12:11:44","fix table file handling of MANPATH in dependencies","As discussed on DM-1220, the table files for:  - mysqlproxy  - protobuf  - lua  - expat should have the MANPATH entry removed entirely, while:  - xrootd should have "":"" added to the end of its MANPATH value, to allow the default paths to be searched as well.",1
"DM-1733","01/05/2015 14:46:48","Build 2015_01 Qserv release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"DM-1735","01/06/2015 10:16:17","Have newinstall.sh check itself against distrib version","We want to alert people who are just using a newinstall.sh they have lying around (old or hacked up or...) that they are not using the official server version.  ",1
"DM-1738","01/06/2015 14:59:08","deblender artifacts in noise-replaced images","We still see noise artifacts in some deblended images on the LSST side when running the M31 HSC data.  They look like the result of running NoiseReplacer on HeavyFootprints in which the children can extend beyond the parents.  This was fixed on the HSC side on DM-340 (before the HSC JIRA split off), and I *think* we just need to transfer the fix to LSST.",1
"DM-1743","01/07/2015 18:26:54","CSV reader for Qserv partitioner doesn't handle no-escape and no-quote options properly","Both the no-quote and no-escape CSV formatting command line options should not have a default value, as specifying any value turns off field escaping and quoting. Furthermore, when quoting is turned off, the reader incorrectly treats embedded NUL characters as a quote character.",1
"DM-1744","01/08/2015 12:37:07","Fix SWIG_SWIG_LIB empty list default value","See Serge message to Qserv-l ""xrootd premature death"": {quote} However, there are bigger problems. First of all, master doesn’t build for me. I get this error:    File ""/home/lsstadm/qserv/SConstruct"", line 104:     env.Alias(""dist-core"", get_install_targets())   File ""/home/lsstadm/qserv/SConstruct"", line 90:     exports=['env', 'ARGUMENTS'])   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 609:     return method(*args, **kw)   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 546:     return _SConscript(self.fs, *files, **subst_kw)   File ""/home/lsstadm/stack/Linux64/scons/2.3.0+1/lib/scons/SCons/Script/SConscript.py"", line 260:     exec _file_ in call_stack[-1].globals   File ""/home/lsstadm/qserv/build/SConscript"", line 39:     canBuild = detect.checkMySql(env) and detect.setXrootd(env) and detect.checkXrootdLink(env)   File ""/home/lsstadm/qserv/site_scons/detect.py"", line 225:     xrdLibPath = findXrootdLibPath(""XrdCl"", env[""LIBPATH""])   File ""/home/lsstadm/qserv/site_scons/detect.py"", line 213:     if os.access(os.path.join(path, fName), os.R_OK):   File ""/home/lsstadm/stack/Linux64/anaconda/2.1.0/lib/python2.7/posixpath.py"", line 77:     elif path == '' or path.endswith('/'):  which is caused by the fact that env[“LIBPATH”] looks like:  [[], '/home/lsstadm/stack/Linux64/antlr/2.7.7/lib', '/home/lsstadm/stack/Linux64/boost/1.55.0.1.lsst2/lib', '/home/lsstadm/stack/Linux64/log4cxx/0.10.0.lsst1+2/lib', '/home/lsstadm/stack/Linux64/xrootd/4.0.0rc4-qsClient2/lib', '/home/lsstadm/stack/Linux64/zookeeper/3.4.6/c-binding/lib', '/home/lsstadm/stack/Linux64/mysql/5.1.65.lsst1/lib', '/home/lsstadm/stack/Linux64/protobuf/2.4.1/lib', '/home/lsstadm/stack/Linux64/log/10.0+3/lib']  The first element is [], which comes from https://github.com/LSST/qserv/blob/master/site_scons/state.py#L173 where a PathVariable called SWIG_SWIG_LIB is given a default value of []. I can fix the build by changing the default to an empty string… but I don’t know enough scons to say whether that’s the right thing to do. Can one of the scons gurus confirm that’s the right fix? {quote}",1
"DM-1754","01/09/2015 18:14:10","Update auto build tool to work with new split repositories ","After the repository split, changes are required to get the auto build tool to work properly. Firefly and Firefly based applications are built using Gradle system.  ",8
"DM-1761","01/13/2015 08:56:06","Provide input data for exampleCmdLineTask.py","{{pipe_tasks/examples/exampleCmdLineTask.py}} reads data from a repository. The comments in {{pipe_tasks/python/lsst/pipe/tasks/exampleCmdLineTask.py}} suggest that  {code} # The following will work on an NCSA lsst* computer: examples/exampleCmdLineTask.py /lsst8/krughoff/diffim_data/sparse_diffim_output_v7_2 --id visit=6866601 {code}  There are a few problems with that:  * External contributors don't have access to {{lsst*}}; * Even though that data exists now, it's unclear how long it will remain there, or what steps are being taken to preserve it; * The mention of this data is fairly well buried -- it does appear in the documentation, but it's certainly not the first thing a new user will stumble upon.  At least the first two points could be addressed by referring to a publicly available data repository. For example, the following works once {{afwdata}} has been set up:  {code} examples/exampleCmdLineTask.py ${AFWDATA_DIR}/ImSim --id visit=85408556 {code}  Although this has the downside of only providing a single image.",1
"DM-1762","01/13/2015 09:49:03","Export SUI data (DC_W13_Stripe82_subset)","- import sui.sql.bzip2.out (produced by Serge) into MySQL for DeepSource and DeepForcedSource tables: - remove columns chunkId and subChunkId for each chunk table - merge all chunk table into the main table - join DeepSource and DeepForcedSource to add coordinates of DeepSource (director) object in DeepForcedSource table. then dump  DeepSource and DeepForcedSource  to files DeepSource.csv and DeepForcedSource.csv {code:sql} SELECT f.*, COALESCE(s.ra, f.ra), COALESCE(s.decl, f.decl) FROM DeepForcedSource f LEFT JOIN DeepSource s ON (f.deepSourceId = s.deepSourceId) INTO OUTFILE '/db1/dump/DeepForcedSource.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n'; {code} - Load this file using Qserv loader.  A sample should be made and tested first to validate this procedure. This sample could be added in qserv_testdata",3
"DM-1770","01/13/2015 12:50:27","Support DDL in MetaServ - design","DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves designing the procedure involving loading DDL information into MetaServ. We need to be ready to support a variety of scenarios: * we are getting already preloaded database, need to just load metadata about it to metaserv (we might have the original ascii file with extra information, or not) * we are starting from scratch, need to initialize database (including loading schema), and need to load the information to the metaserv * we already have the database and metadata in metaserv, but we want to change something (eg. alter table, or delete table, or delete database).",2
"DM-1771","01/13/2015 15:02:01","move executionOrder from plugin config class to plugin class","We originally put the executionOrder parameter (which determines when a plugin is run, relative to others), in the config object, simply because that's where it was in the old framework.  But it's really not something that should be configurable, as it depends only on the inputs the algorithm needs, which don't change.",1
"DM-1783","01/16/2015 15:06:25","fix faint source and minimum-radius problems in Kron photometry","This transfers some improvements to the Kron photometry from the HSC side:  - HSC-983: address failures on faint sources  - HSC-989: fix the minimum radius  - HSC-865: switch to determinant radius instead of semimajor axis  - HSC-962: bad radius flag was not being used  - HSC-121: fix scaling in forced photometry  The story points estimate here is 50% of the actual effort, as the work (already done) also benefited HSC.",5
"DM-1785","01/16/2015 19:25:55","Add rotAngle to baseline schema","Add ""rotAngle DOUBLE"" to every table that has image ra/decl.  ",1
"DM-1792","01/20/2015 11:02:20","Update documentation and automatic install script w.r.t. new newinstall.sh script","newinstall.sh script has evolved and breaks Qserv install procedure.",1
"DM-1793","01/20/2015 11:21:45","remove reference data members from FootprintFunctor","The FootprintFunctor class uses references for data members, which could cause memory problems if the class (or a subclass) is ever initialized with a temporary.  Fixing this would probably require changing the constructor to take a shared_ptr, however, so it would break a lot of downstream code.  I'd rather actually rewrite FootprintFunctor entirely (one of the goals for Epic DM-1107), but it's not clear when that will happen; if it slips too much, this issue is to remind us to fix at least this problem.",0
"DM-1797","01/21/2015 10:06:46","Package flask","The Data Access Webservice APIs are relying on flask, so we need to package flask according to the LSST standards. For my initial testing, I just run ""sudo aptitude install python-flask"".  ",1
"DM-1802","01/21/2015 15:46:09","remove unused local typedefs","gcc 4.8 now warns about locally-defined typedefs that aren't used.  We have a few of these in ndarray and afw::gpu that should be removed.",1
"DM-1803","01/21/2015 16:32:13","S15 Explore Qserv Authorization","Explore authorization centrally: use information generated by parser. Either generate dummy query and run on mysql that runs near czar, or use info produced by parser to determine if user is authorized.  Note, we want to limit this to ~1 week, just to reveal potential problems, or do a quick proof of concept.",8
"DM-1810","01/22/2015 12:49:53","segfaults in ip_diffim on gcc 4.8","I'm seeing test segfaults in ip_diffim on gcc 4.8, similar to those resolved on DM-1725, but with no similar smoking gun yet.  Preliminary indication is that the problem is actually in meas_algorithms.",2
"DM-1812","01/23/2015 10:28:09","Determine LSE-130 impact of collimated projector calibration plan","During a working meeting with Robert Lupton and Chris Stubbs, determine the impact on LSE-130 of the introduction of the collimated projector for calibration.",8
"DM-1814","01/23/2015 10:30:39","Support Camera CD-2 (mainly re: LSE-130)","Provide slides and other information needed for CD-2, mainly relative to the open questions around LSE-130",2
"DM-1816","01/23/2015 10:33:43","Convert LSE-130 to SysML","Following CCB recommendation of approval of LSE-130 draft, convert Word draft to SysML and provide a docgen to Robert McKercher for final posting. ",2
"DM-1818","01/23/2015 11:35:40","Support completion of final document","Based on CCB approval of LSE-72 on 10 October, support the completion of the final copy of the document for posting on Docushare.",1
"DM-1819","01/23/2015 11:53:38","Complete LSE-140 work as needed to produce final document","Complete any review-driven revisions of LSE-140 and support the CCB meeting and following final document preparation.",2
"DM-1820","01/23/2015 11:57:39","LSE-140: Collect desired changes for future release","Prepare for a future revision (Phase 3) of LSE-140.  Collect issues to be addressed in the revision.  Determine if any affect Phase 2 scope (which would require a prompt revision).  It is not anticipated that there will be an actual revision of LSE-140 during the Winter 2015 cycle, because additional detail on calibration requirements will not be available in time.",1
"DM-1821","01/23/2015 12:05:46","Clarify scope of DM data quality analysis requirement","Clarify in LSE-140 that the DM data quality analysis referred to is primarily that of the Level 1 data products.",0
"DM-1824","01/23/2015 16:14:23","Define issues to be addressed","Work with TCS contacts (Jacques Sebag, Paul Lotz, etc.) to define the principal issues",1
"DM-1841","01/26/2015 13:10:12","Fix query error on case03: ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata"" ","Xrootd prevents the worker to return more than 2MB data.  On GB-sized data: {code} mysql --host=127.0.0.1 --port=4040 --user=qsmaster --batch  -e ""SELECT scienceCcdExposureId FROM Science_Ccd_Exposure_Metadata""                                                                                                                                                                     ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case03_qserv/1234567890): 20150123-16:27:45, Error merging result, 1420, Result message MD5 mismatch (-1) {code}  On integration test case 04: {code} qserv@clrinfoport09:~/src/qserv (u/fjammes/DM-1841 *)⟫ mysql --host=127.0.0.1 --port=4040 --user=qsmaster qservTest_case04_qserv  -e ""SELECT * FROM DeepForcedSource""   ERROR 4120 (Proxy) at line 1: Error during execution: -1 Ref=1 Resource(/chk/qservTest_case04_qserv/6970): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=2 Resource(/chk/qservTest_case04_qserv/7138): 20150204-16:23:43, Error merging result, 1420, Result message MD5 mismatch Ref=3 (-1) {code}",5
"DM-1843","01/27/2015 09:47:02","Permit PropertySets to be represented in event payloads","In the old marshalling code, property sets were representable within the payload of the event.   This was removed in the new marshalling scheme.   There are things (ctrl_orca) that still used this, so this needs to be added to the new marshaling code.  At the same time, new new filtering code can not allow this to be added, because the JMS headers only take simple data types.",2
"DM-1844","01/27/2015 11:13:51","Test Qserv on SL7","Needed to run Qserv on CC-IN2P3 cluster.",2
"DM-1854","01/27/2015 22:56:11","SUI propose a structure definition for user workspace","Workspace is an integral part of SUI. We want to start the discussion and definition of workspace concept and structure.     SUI team had several discussions and Xiuqin presented the results at the DM AHM at SLAC. The slides and the discussion notes are here: https://confluence.lsstcorp.org/display/DM/Workspace+discussion",20
"DM-1860","01/28/2015 00:28:21","Update documentation for v10_0 release","All done bar obtaining some release notes. ",2
"DM-1868","01/28/2015 10:17:49","Define JSON Results for Data Access Services","As discussed at [Data Access Hangout 2015-02-23|https://confluence.lsstcorp.org/display/DM/Data+Access+Hangout+2015-02-23], we should support json format. This story covers defining structure of JSON results for Data Access Services (dbserv, imgserv, metaserv) ",3
"DM-1873","01/28/2015 19:29:35","SUI 2D data visualization (XY plot)","Better algorithm in spatial binning to visualize large number of catalog sources Plot histogram for tabular data Plot basic light curve ",40
"DM-1875","01/28/2015 23:35:15","SUI infrastructure implementation","Identify the hardware resources needed at NCSA for short term development and  Set up the basic git repository and build system Explore multi resolution images display for background iamge",40
"DM-1878","01/29/2015 09:21:33","Collect, understand, and define more use cases","This is an on-going effort. The collected use cases will be posted at confluence page https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41784036. ",20
"DM-1880","01/29/2015 11:20:19","Implement RESTful interfaces for Database (GET)","Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""GET"" type requests only, ""POST"" will be handled separately.",5
"DM-1885","01/29/2015 11:57:21","Contribute to the workspace capability discussion ","This include past experience, collection of use cases. ",2
"DM-1887","01/29/2015 17:34:40","HDF5 file format study","Xiquin, Loi, Trey, and myself discussed HDF5 as a default format to return result set and metadata from lower-level database services vs. traditional IPAC table. Here is the summary:  Advantages of IPAC Table format  - Simple and human-readable, contains a single table - Fixed length rows (easy to page through) - Supported by many astronomical tools  - Provides a way to pass data type, units, and null values in the header - More metadata can be added through keywords (attributes)  Disadvantages of IPAC table format   - Steaming can not be started before all data are received – need to know column width before the table can be written (csv is better alternative) - Only alpha-numeric and '_' characters are allowed in column names (small subset of available characters) - Only predefined datatypes and one attribute type (string) - ASCII representation requires about twice as much storage to represent floating-point number data than the binary equivalent.  Advantages of HDF5  - Can represent complex data and metadata (according to LOFAR, good to represent time series) - Structured data, arbitrary attribute types, datatypes can be combined to create structured datatypes - Flexible datatypes: can be enumerations, bit strings, pointers, composite datatypes, custom atomic datatypes - Access time and storage space optimizations - Partial I/O: “Chunked” data for faster access - Supports parallel I/O (reading and writing) - Built-in compression (GNU zlib, but can be replaced with others) - Existing inspection and visualization tools (HDFView, MATLAB, etc.)  Disadvantages of HDF5  - Complex - Tuned to do efficient I/O and storage for ""big"" data (hundreds of megabytes and more), not efficient for small reads/writes. - Requires native libraries (available in prepackaged jars, see below) - Not human readable - (?) Not yet widely supported by astronomical tools (counter-examples: AstroPy, IDL, more at hdfgroup site)  Tools and Java wrappers:  * JHI5 - the low level JNI wrappers: very flexible, but also quite tedious to use. * Java HDF object package - a high-level interface based on JHI5. * HDFView - a Java-based viewer application based on the Java HDF object package.  * JHDF5 - a high-level interface building on the JHI5 layer which provides most of the functionality of HDF5 to Java. The API has a shallow learning curve and hides most of the house-keeping work from the developer. You can run the Java HDF object package (and HDFView) on the JHI5 interface that is part of JHDF5, so the two APIs can co-exist within one Java program. (from StackOverflow answer, 2012)  * NetCDF-Java is a Pure Java Library, that reads HDF5. However, it's hard to keep pure java version up-to-date with the standard, does not support all the features.  A way to set up native libraries (3rd option from JHDF5 FAQ):      ""Use a library packaged in a jar file and provided as a resource (by putting the jar file on the class path). Internally this uses the same directory structure as method 2., but packaged in a jar file so you don't have to care about it. Jar files with the appropriate structure are cisd-jhdf5-batteries_included.jar and lib/nativejar/.jar (one file for each platform). This is the simplest way to use the library.""       ",1
"DM-1897","01/30/2015 13:06:15","Modify CSS structure to support table deletion","Modify CSS structures to support DROP TABLE, as defined in DM-1896.",2
"DM-1900","01/30/2015 13:20:39","Worker management service - design","We need to replace direct worker-mysql communication and other administrative channels with a special service which will control all worker communication. Some light-weight service running alongside other worker  servers, probably HTTP-based. Data loading, start/stop should be handled by this service.",5
"DM-1901","01/30/2015 13:25:08","Re-implement data loading scripts based on new worker control service","Once we have new service that controls worker communication we'll need to reimplement WorkerAdmin class based on that.",8
"DM-1903","02/02/2015 06:35:40","Implementation of calibration transformation framework","Following DM-1598 there will be a detailed design and prototype implementation for the calibration & ingest system. This issue covers cleaning up that code, documenting it, having it reviewed, and merging to master.",2
"DM-1904","02/02/2015 10:20:18","Continued footprint improvements","A redesigned API and support for topological operations within the Footprint class.  This continues the work started in DM-1107 in W15.  Breakdown: jbosch 15%; swinbank 85%",8
"DM-1917","02/02/2015 16:35:02","Fix missing virtual destructors","The compiler is warning about some derived class hierarchies that are lacking virtual destructors.  We should add at least empty implementations to the base classes of these hierarchies.",1
"DM-1919","02/02/2015 16:55:46","Address misc. compiler warnings","Fix places where compiler is warning about some things we are doing on purpose and which we don't intend to change.  This helps keep compiler noise down so its easier to notice ""real"" warnings.",1
"DM-1943","02/04/2015 13:11:15","HSC backport: convert Peak to PeakRecord","This issue covers transferring all changesets from [HSC-1074|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1074] and its subtasks, as well as:  - An RFC to propose the API change, and any requested modifications generated by the RFC.  - Additional fixes to downstream code that's broken by this change (HSC-side changesets should be present for most of downstream fixes, but perhaps not all).",8
"DM-1945","02/04/2015 13:36:48","HSC backport: multiband processing for coadds","This issue includes transferring changesets from many HSC issues:  - HSC-1060  - HSC-1064  - HSC-1065  - HSC-1061  Most of this is in multiBand.py in pipe_tasks, but there are scattered changes elsewhere (including updates to camera mappers to include the new datasets, for which we'll need to modify more than just obs_subaru).  However, before we make these changes, we'll need to open an RFC to gather comments on the design of this task.  We should qualify there that this is not a long-term plan for consistent multiband processing (which we'll be starting to design on DM-1908), but a step towards better processing in the interim.  Note: while I've assigned this to [~lauren], as I think it will be very helpful for her to get familiar with this code by doing the transfers, the RFC will have to involve a collaboration with [~jbosch], [~price], and Bob Armstrong, as we can't expect someone who wasn't involved in the design to be able to write a document justifying it.",8
"DM-1952","02/04/2015 16:13:25","Change log priority for message ""Unknown column 'whatever' in 'field list'""  ","Next message should be logged with ERROR priority:  {code} 0204 15:08:03.748 [0x7f1f4b4f4700] INFO  Foreman (build/wdb/QueryAction.cc:250) - [1054] Unknown column 'whatever' in 'field list'   {code}",0.5
"DM-1953","02/04/2015 18:37:18","Post meas_base move changes to Kron","These are to note leftovers from DM-982.  They could be done in a single issue. 1.  I commented code out referring to correctfluxes, but it will need to be restored once it is available in the new framework.  2.  Jim asked me to replace the computeSincFlux which is currently in PsfImage.cc in meas_algorithms with a similar call in meas_base/ApertureFlux.cc.  I did not do this because it became rather complicated, and can just as easily be done when the meas_algorithms routine is moved or removed.  Basically, the templating in ApertureFlux is on Pixel type, whereas in meas_algorithms it is on ImageT (where ImageT is not necessarily a single class hierarchy -- e.g., Image and MaskedImage).  So I left this for now.",1
"DM-1954","02/04/2015 23:47:11","HSC backport: deblended HeavyFootprints in forced photometry","This is a transfer for changesets for [HSC-1062|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1062].    Unlike most of the HSC backport issues for multiband deblending, these changes will require significant modification the LSST side, because we need to apply them to the new forced measurement framework in meas_base rather than the old, HSC-only one in meas_algorithms and pipe_tasks.    Also include [HSC-1256|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1256], [HSC-1218|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1218], [HSC-1235|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1235], [HSC-1216|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1216].",20
"DM-1972","02/06/2015 15:12:29","upgrade SWIG to 3.0.8 or later","SWIG 3.0.5 is now out and has several useful fixes w.r.t. 3.0.2 (which we are presently using) including:  - A bug we've had to work around involving templated methods of classes  - improved handling of new-style enums (they are no longer hoisted into the global namespace, which was a serious misfeature of SWIG 3.0.2)    I propose we try it out using buildbot (when we have some time), and if it works, we adopt it. Adopting it will help us relax the restrictions on what C++11 features can be used in C++ header files.",2
"DM-1973","02/06/2015 15:56:23","Build 2015_02 Qserv release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"DM-1974","02/06/2015 18:30:02","Fix enclose, escape, and line termination characters in qserv-data-loader","Add this string to mysql loader 'LOAD DATA INFILE' command:   {code}  q += ""ENCLOSED BY '%s' ESCAPED BY '%s' LINES TERMINATED BY '%s'"" % (enclose, escape, newline) {code} and add params in cfg file.",2
"DM-1982","02/09/2015 12:30:15","Fix JDBC timestamp error","JDBC driver returns an error on next query:  {code:sql} sql> select * from Science_Ccd_Exposure [2015-02-06 13:39:37] 1 row(s) retrieved starting from 0 in 927/970 ms [2015-02-06 13:39:37] [S1009] Cannot convert value '0000-00-00 00:00:00' from column 32 to TIMESTAMP. [2015-02-06 13:39:37] [S1009] Value '[B@548997d1' can not be represented as java.sql.Timestamp {code}",1
"DM-1987","02/10/2015 23:04:48","Redesign/Refactor WCS and Coord","%50 KSK, %50 RO Currently WCS is mutable and Coord objects are heavyweight.  Refactor WCS to be immutable and make Coord less heavyweight.  Include lists of Coord objects.  It's possible astropy could inform in that area.  Also, remove TanWcs in favor of TanSipWcs since TanWcs can have SIP terms.",40
"DM-1994","02/11/2015 14:30:29","Story point display and roll-up in epic display","I understand that there is a pending request to display the story points for individual story issues in the mini-table in which they are displayed for an epic.  It would also be useful to see a rolled-up total of the story points for the defined set of stories - so that, among other things, this could be compared to the story point value for the epic.  Ideally the story points for the roll-up might be displayed as ""nn (mm)"" where nn is the total points and mm is the number of points remaining to do (or done already - I don't care which as long as the definition is clear).",1
"DM-2005","02/12/2015 18:07:11","switch ndarray to external package","There is already an external ndarray project on GitHub (we've been using a fork of that).  We should merge the forks and switch to using the external package. ",2
"DM-2009","02/12/2015 18:12:43","Please add cbegin and cend to afw tables","It would be helpful if afw tables had the C++11 iterator methods cbegin and cend that return iterators-to-const.",2
"DM-2029","02/13/2015 10:32:12","Update Confluence build instructions to match github move","The Build tool documentation   https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool  refers to   git clone git@git.lsstcorp.org:LSST/DMS/devenv/lsstsw.git  This should be updated to reflect the move to GitHub  git clone https://github.com/lsst/lsstsw.git   ",0
"DM-2050","02/18/2015 04:09:10"," Integration and test monitoring architecture Part I","[retitled to better capture cycle scope]    Develop and deploy a layer to capture the outputs, initially numeric,  of integration testing afterburners such as sdss_demo, hsc_demo, and  others developed this cycle. Also capture meta-information such as  execution time and memory footprint. Propose log format to standardise  production of such informations. Investigate notification system based  on trending away from expected values. Investigate data provisioning  of integration tests such as storage of test data in GithubLFS.    [75% JMP 25% JH]        ",100
"DM-2052","02/18/2015 04:32:29","Maintain list of OSes that pass build and integration testing ","Provide an automatiically generated and updated pages showing operating systems that are successfully building  and integrating the stack from source.   [FE at 75%, JH at 75%]",20
"DM-2054","02/18/2015 04:39:22","Release engineering  Part One","Bucket for public stack releases  [FE at 75%, JH at 75%]",40
"DM-2057","02/18/2015 13:36:11","Attend Scale 13x conference","Attend database talks, in particular the MaxScale proxy talk (http://www.socallinuxexpo.org/scale/13x/presentations/advanced-query-routing-and-proxying-maxscale?utm_campaign=north-american-trade-shows&utm_source=hs_email&utm_medium=email&utm_content=16099082&_hsenc=p2ANqtz-_MFjfxvpCdmV_Ax2RKDdOGypHPQ85UL-UMuy0eRs_MrlJ2qJVp-MXx-g7_-dAQsq0trpA61hkZrzO-3gp6bKVkpK52fQ&_hsmi=16099082).  If anyone has questions they would like me to ask, please post them here as well.  I will post notes to this issue. ",2
"DM-2058","02/18/2015 14:14:32","Data loader should always create overlap tables"," We have discovered that some overlap tables that are supposed to exist were not actually created. It looks like partitioner is not creating overlap files when there is no overlap data and loader is not creating overlap table if there is no input file. Situation is actually symmetric, there could be non-empty overlap table but empty/missing chunk table. When we create one table we should always make another as well. ",2
"DM-2060","02/18/2015 15:17:18","Rename TaskMsgFactory2","Rename TaskMsgFactory2 to TaskMsgFactory.    Please see DM-211 for more information.",0.5
"DM-2094","02/19/2015 17:50:20","Port metaREST.py to db","metaREST_v0.py in metaserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it. ",1
"DM-2095","02/19/2015 18:27:38","Port dbREST.py to db","dbREST_v0.py in dbserv is currently using MySQLdb instead of going through the db API, because we need to use parameter binding for security reasons. We should switch to using db, once the db interfaces will support it. ",1
"DM-2096","02/19/2015 23:16:01","Long term database work planning","Long term planning (updating LDM-240).",8
"DM-2097","02/19/2015 23:47:00","Package andyH xssi fixed version (>2MB answer pb) in eups","See DM-1847 - Andy made a patch, it'd be good to the xrootd we use for our stack.",1
"DM-2129","02/20/2015 14:54:26","S19 Improve Query Coverage in Qserv","This epic holds budgeted effort for work directed at improving query coverage (additional or previously unsupported query types) in Qserv",40
"DM-2131","02/20/2015 17:10:14","Resolve compiler warnings in new measurement framework","When building {{meas_base}}, or any other measurement plugins which follow the same interface, with clang, I see a bunch of warnings along the lines of:  {code} In file included from src/ApertureFlux.cc:34: include/lsst/meas/base/ApertureFlux.h:197:18: warning: 'lsst::meas::base::ApertureFluxAlgorithm::measure' hides overloaded virtual function       [-Woverloaded-virtual]     virtual void measure(                  ^ include/lsst/meas/base/Algorithm.h:183:18: note: hidden overloaded virtual function 'lsst::meas::base::SimpleAlgorithm::measure' declared here:       different number of parameters (4 vs 2)     virtual void measure( {code}  This is an artefact of a [workaround for SWIG issues|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284390]; the warnings aren't indicative of a fundamental problem, but if we can avoid them we should.  While we're at it, we should also fix:  {code} include/lsst/meas/base/ApertureFlux.h:233:1: warning: 'ApertureFluxResult' defined as a struct here but previously declared as a class       [-Wmismatched-tags] struct ApertureFluxResult : public FluxResult { ^ include/lsst/meas/base/ApertureFlux.h:65:1: note: did you mean struct here? class ApertureFluxResult; ^~~~~ struct {code}",1
"DM-2139","02/22/2015 01:24:23","Support DDL in MetaServ - implementation","DDL information is embedded as comments in the master version of the schema (in ""cat"" repo). Currently we are only using it for schema browser. This story involves building tools that will load the DDL schema into MetaServ. Design aspects are covered in DM-1770.",8
"DM-2141","02/22/2015 15:21:45","Add meas_extensions_shapeHSM to lsstsw, lsst_distrib","meas_extensions_shapeHSM has just been resurrected from bitrot, and should be included in our distribution.    Contrary to DM-2140, it should probably not be included in lsst_apps, as it's not clear we want to add a dependency on tmv and GalSim there.",1
"DM-2157","02/23/2015 17:37:12","Data loader crashes on uncompressed data.","Vaikunth just mentioned to me that the is a crash in data loader when it tries to load uncompressed data: {noformat} root - CRITICAL - Exception occured: local variable 'outfile' referenced before assignment Traceback (most recent call last): File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 312, in <module> sys.exit(loader.run()) File ""/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 248, in run self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data) File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 168, in load return self._run(d atabase, table, schema, data)   File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 192, in _run     files = self._gunzip(data)   File ""/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 388, in _gunzip     result.append(outfile) UnboundLocalError: local variable 'outfile' referenced before assignment {noformat}  It looks like we never tested loader on uncompressed data and there is a bug in handling uncompressed data. ",1
"DM-2159","02/23/2015 19:17:07","Implement Image Response for ImgServ","This story covers implementing proper response, and the header metadata for the fits image response.",3
"DM-2161","02/23/2015 20:12:54","Setup webserv for SUI tests","We need to setup a service (eg on lsst-dev) that can be used by the IPAC team to play with our webserv/metaserv/dbserv/imgserv.  The server runs on lsst-dev machine, port 5000. To ssh-tunnel, try: {code} ssh -L 5000:localhost:5000 lsst-dev.ncsa.illinois.edu {code}  An example usage:  {code}   curl 'http://localhost:5000/db/v0/query?sql=SHOW+DATABASES+LIKE+""%Stripe%""'   curl 'http://localhost:5000/db/v0/query?sql=SHOW+TABLES+IN+DC_W13_Stripe82'   curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.DeepForcedSource'   curl 'http://localhost:5000/db/v0/query?sql=DESCRIBE+DC_W13_Stripe82.Science_Ccd_Exposure'   curl 'http://localhost:5000/db/v0/query?sql=SELECT+deepForcedSourceId,scienceCcdExposureId+FROM+DC_W13_Stripe82.DeepForcedSource+LIMIT+10'   curl 'http://localhost:5000/db/v0/query?sql=SELECT+ra,decl,filterName+FROM+DC_W13_Stripe82.Science_Ccd_Exposure+WHERE+scienceCcdExposureId=125230127'   curl 'http://localhost:5000/image/v0/raw/cutout?ra=7.90481567257&dec=-0.299951669961&filter=r&width=30.0&height=45.0' {code} ",2
"DM-2171","02/24/2015 20:55:15","Implement JSON Results for MetaServ and DbServ","Implement JSON results for Metadata Service (see all M* in https://confluence.lsstcorp.org/display/DM/API),  and Database Service (see all D*) as defined in DM-1868",3
"DM-2173","02/25/2015 12:08:08","Disable testDbLocal.py in db if auth file not found","tests/testDbLocal.py can easily fail if required mysql authorization file is not found in user home dir. Skip the test instead of failing in such case.",1
"DM-2186","02/26/2015 13:56:25","Move astrometry_net wrapper code from meas_astrom to a new package","Remove all astrometry.net wrapper code from {{meas_astrom}} and put it in a new package named {{meas_extensions_astrometryNet}}.  ",2
"DM-2190","02/26/2015 14:58:07","Documentation for data loader","Vaikunth had some ""expected"" troubles playing with data loader options for his DM-1570 ticket. Main issue I believe is the absence of the documented use cases and their corresponding data loader options. I'll try to add a bunch of common use cases to RST documentation and also verify that all options behave as expected.",2
"DM-2193","02/27/2015 11:15:38","Add assertXNearlyEqual to afw","We often want to compare two WCS for approximate equality. afw/image/testUtils has similar functions to compare images and masks and I would like to add one for WCS    This ended up being expanded to adding functions for many afw classes (not yet including image-like classes, though existing functions in image/testUtils for that purpose should probably be wrapped or rewritten on a different ticket)",5
"DM-2199","02/27/2015 16:20:43","Build 2015_03 Qserv release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe. ",1
"DM-2241","03/02/2015 17:40:38","Raw image definition and usage","SUI needs to serve raw data to the user community. We want to understand the use cases and definition of raw data. More specifically, what meta data will be available in the FITS file that we call raw image? ",1
"DM-2243","03/02/2015 22:00:41","Extend API: expose cursor","Extend API to expose cursor. This was brought up by Andy in DM-2137. ",1
"DM-2257","03/03/2015 16:07:11","Allow eups xrootd install script to be relocatable","xrootd lib/ directory should be s relative symlink to lib64, no a full path link.",1
"DM-2270","03/04/2015 15:50:35","Move VMs to Docker containers","We anticipate being able to move from the VMs that we currently use to using docker.  This will require some coordination with Greg Daues to see how HTCondor is configured.  ",2
"DM-2277","03/05/2015 15:09:34","Document HOW-TO setup-up krb5 for easy cluster access","{code:bash} su aptitude install krb5-user # edit /etc/krb5.conf w.r.t ccage one # then as desktop user kinit ssh ccqservxxx {code}  /etc/krb5.conf {code:bash} [libdefaults]  default_realm = IN2P3.FR  ...  allow_weak_crypto = true   ... [realms]  IN2P3.FR = {   kdc = kerberos-1.in2p3.fr:88   kdc = kerberos-2.in2p3.fr:88   kdc = kerberos-3.in2p3.fr:88       master_kdc = kerberos-admin.in2p3.fr:88       admin_server = kerberos-admin.in2p3.fr       kpasswd_server = kerberos-admin.in2p3.fr       default_domain = in2p3.fr {code}  sshconfig: {code:bash} Host ccqservbuild GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName ccqservbuild.in2p3.fr #ProxyCommand ssh -W %h:%p cc   Host ccqserv1* GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ForwardX11 yes HostName %h.in2p3.fr ProxyCommand ssh -W %h:%p ccqservbuild {code}",2
"DM-2279","03/05/2015 22:21:00","Fix problems with mysql timeout","We added some code for supporting reconnecting (see https://dev.lsstcorp.org/trac/ticket/3042) but clearly not enough to recover from connection timeouts. This needs to be addressed.",1
"DM-2280","03/06/2015 09:19:00","The TAN_PIXELS cameraGeom coordinate system should be with respect to the center of the focal plane","The TAN_PIXELS cameraGeom coordinate system (the position on a detector if there is no optical distortion) is presently defined with respect to the center of the detector -- i.e. a star at the center of the detector will have the same position in PIXELS and TAN_PIXELS coordinates. That is a mistake. TAN_PIXELS should be defined with respect to the center of the focal plane, since it then reflects the effects of having optical distortion or not.  Fixing this will help meas_astrom match stars. The effects of not fixing it are making the matcher search farther for a fit. As long as we allow sufficient offset in the matcher config the current system will work, but it is not ideal.",2
"DM-2281","03/06/2015 11:08:04","Implement connection pool","Implement a class that manages a connection pool, and optionally, if configured, restarts connection as needed in case of timeout.",1
"DM-2282","03/06/2015 11:09:52","Switch to using db connection pool","Switch to using the db connection pool. Note, in addition to getting auto-reconnect, in metaserv that would handy if we need to talk to multiple database servers simultaneously.",1
"DM-2294","03/09/2015 13:31:01","Unable to start cmsd on Qserv worker node","Some build issues have qlready been fixed in commit: 9dd378829e8751a6852356967411c20580e2a1c3  Here's the log:  {code:bash} [fjammes@ccqserv101 ~]$ cat /qserv/qserv-run/var/log/worker/cmsd.log 150309 21:19:46 9794 Starting on Linux 3.10.0-123.8.1.el7.x86_64 Copr.  2004-2012 Stanford University, xrd version v20140617-203cf45 ++++++ cmsd worker@ccqserv101.in2p3.fr initialization started. Config using configuration file /qserv/qserv-run/etc/lsp.cf =====> all.adminpath /qserv/qserv-run/tmp =====> xrd.port 1094 =====> xrd.network nodnr Config maximum number of connections restricted to 4096 Config maximum number of threads restricted to 2048 Copr.  2007 Stanford University/SLAC cmsd. ++++++ worker@ccqserv101.in2p3.fr phase 1 initialization started. =====> all.role server =====> ofs.osslib libxrdoss.so  =====> oss.localroot /qserv/qserv-run/xrootd-run =====> cms.space linger 0 recalc 15 min 10m 11m =====> all.pidpath /qserv/qserv-run/var/run =====> all.adminpath /qserv/qserv-run/tmp =====> all.manager ccqserv100.in2p3.fr:2131 =====> all.export / nolock The following paths are available to the redirector: w  /   ------ worker@ccqserv101.in2p3.fr phase 1 server initialization completed. ++++++ worker@ccqserv101.in2p3.fr phase 2 server initialization started. Plugin Unable to find  required version information for XrdOssGetStorageSystem in osslib libxrdoss.so ------ worker@ccqserv101.in2p3.fr phase 2 server initialization failed. 150309 21:19:46 9794 XrdProtocol: Protocol cmsd could not be loaded ------ cmsd worker@ccqserv101.in2p3.fr:1094 initialization failed {code}",2
"DM-2305","03/11/2015 11:10:03","Measurement transforms for Flux","Provide calibration transforms for flux measurements to magnitudes.",3
"DM-2306","03/11/2015 11:10:56","Measurement transforms for centroids","Provide calibration transforms for all algorithms measuring centroids.",5
"DM-2307","03/11/2015 11:11:34","Measurement transforms for shapes","Provide calibration transforms for algorithms measuring shapes.",2
"DM-2309","03/11/2015 15:12:51","Update dev quick-start guide to new git repositories","The quick-start documentation for developers still points to the old git repositories. The RST document needs to be updated to the GitHub repos.",0.5
"DM-2312","03/12/2015 12:19:41","obs_test's table file is out of date","obs_test's table file is somewhat out of date. Problems include:  - afw is required but missing  - meas_algorithms and skypix are used by bin/genInputRegistry.py, which is only used to create the input repo so these can be optional  - daf_persistence is not used  - daf_base is only used by bin/genInputRegistry.py, so it can be optional (though it is presumably setup by daf_butlerUtils in any case)",1
"DM-2316","03/12/2015 19:47:27","Clarify expectations for unauthenticated user data access","h4. Short version:  Clarify what existing community practices, notably including VO interfaces, appear to rely on the availability of unauthenticated access to information in astronomical archives.  h4. Details:  At the February DM All Hands, [~frossie] raised an objection when it was mentioned that there is a presumption that all user access to LSST data through the DM interfaces (as opposed to through EPO) will be authenticated.  We don't appear to have ever documented an explicit requirement that all access be authenticated.  The basic controlling requirement is OSS-REQ-0176, ""The LSST Data Management System shall provide open access to all LSST Level 1 and Level 2 Data Products, as defined in the LSST System Requirements and herein, in accordance with LSSTC Board approved policies. ..."", which was a carefully crafted indirection at a time when the policy for non-US/Chile access was still being developed.  However, this presumption has been around for a long time.  It is inherent to the project policy that access to the non-Alert data will be limited to individuals who are entitled to it.  No matter what we think the final policy might be, we do have to design a system that can be consistent with this policy.  [~frossie] stated that the astronomical community relies on certain types of data and metadata - she mentioned coverage maps, among others - being available through unauthenticated interfaces.  This ticket is to ask her (and others) to collect documentation of those existing practices, so that we can figure out what the expectations may be and how to respond to them in our design.",2
"DM-2334","03/16/2015 09:28:21","Simplify interactions with XrdOss","The qserv code is still using the old ssi scheme for the cmsd, this needs to be rewritten. For  details, see  https://listserv.slac.stanford.edu/cgi-bin/wa?A1=ind1503&L=QSERV-L#3",5
"DM-2340","03/16/2015 11:08:46","Reprise SDRP processing metrics","In support of an SDRP-based science talk of Yusra AlSayyad, we spent some cycles gathering/summarizing processing middleware results and metrics from the US side of processing of the Split DRP.  This information from notes, logs, databases, etc provided contextual information on the processing campaign that produced the SDRP science results. ",2
"DM-2341","03/16/2015 11:24:43","Use parallel ssh to manage Qserv on IN2P3 cluster","IN2P3 sysadmin won't manage Qserv through puppet. So Qserv team has to provide ssh scripts to do this.  ",5
"DM-2343","03/16/2015 13:33:59","Move afw_extensions_rgb functionality into afw proper","See RFC-32 ",1
"DM-2347","03/16/2015 17:35:27","(In)equality semantics of Coords are confusing","Viz:  {code} In [1]: from lsst.afw.coord import Coord In [2]: c1 = Coord(""11:11:11"", ""22:22:22"") In [3]: c1 == c1, c1 != c1 Out[3]: (True, False) In [4]: c2 = Coord(""33:33:33"", ""44:44:44"") In [5]: c1 == c2, c1 != c2 Out[5]: (False, True) In [6]: c3 = Coord(""11:11:11"", ""22:22:22"") In [7]: c1 == c3, c1 != c3 Out[7]: (True, True) {code}  {{c1}} is simultaneously equal to *and* not equal to {{c3}}!",1
"DM-2349","03/17/2015 08:34:11","Add unit tests to SchemaToMeta","Add unit tests, also improve variable names as suggested by K-T in comments in DM-2139",1
"DM-2356","03/18/2015 10:15:46","Identify the hardware resources needed at NCSA for short term development ","Supply the hardware resources needed at NCSA for short term development. It is captured in DM-2327  ",1
"DM-2363","03/19/2015 08:56:12","RGB code introduces dependency on matplotlib","While the new RGB code looks like it's just calling NumPy, NumPy is actually delegating to matplotlib under the hood when it writes RGB(A) arrays.  It also turns out that code is broken in matplotlib prior to 1.3.1 (though that shouldn't be a problem for anyone but those who - like me - are trying to use slightly older system Python packages).  I think think this means we should add an optional dependency on matplotlib to the afw table file, and condition the running of the test code on matplotlib's presence (and, ideally, having the right version).  I'm happy to do this myself (since I'm probably the only one affected by it right now).",1
"DM-2364","03/19/2015 10:23:32","Revisit the choice of using flask","We should quickly revisit if flask is the right choice for us.  Related: reportedly, our simple flask-based webserver is using more CPU in an idle state than expected. It might be useful to profile things, and look into that. ",1
"DM-2367","03/19/2015 11:10:50","run lsstswBuild.sh in a clean sandbox","The ""driver"" script, lsstswBuild.sh, used by the buildbot slave on lsst-dev to initiate a ""CI run"" has a number of environment assumptions (binaries in the $PATH, paths to various components, hostnames, etc.).  This prevents it from [easily] being invoked on any other host.  As lsstswBuild.sh builds a number of packages that are not in the lsst_distrib product, the os level dependencies for these other products need to be determined.  In addition, the current version of lsstswBuild.sh and related scripts on lsst-dev are not version controlled.",8
"DM-2380","03/23/2015 10:27:45","Retrieve HSC engineering data","HSC data becomes public 18 months after it was taken, so data taken during commissioning are now available.  We would like to use this data for testing the LSST pipeline.  It needs to be downloaded from Japan.",2
"DM-2382","03/23/2015 12:34:53","Make sure the command-line parser warns loudly enough if no data found","A user recently got confused when calling parseAndRun didn't call the task's run method. It turns out there was no data matching the specified data ID. Make sure this generates a loud and clear warning.",1
"DM-2383","03/23/2015 16:19:09","migrate package deps from sandbox-stackbuild to a proper puppet module","There is a growing list of known package dependencies in the sandbox-stackbuild repo and a need to use this information for independent environments (such as CI).  This list of packages should be lifted out into an independent puppet module that can be reused.",2
"DM-2387","03/24/2015 11:46:00","Build testQDisp.cc on ubuntu","testQDisp.cc needs flags -lpthread -lboost_regex to build on ubuntu.",1
"DM-2390","03/24/2015 16:08:54","Errors need to be checked in UserQueryFactory from QuerySession objects","UserQueryFactory doesn't check its QuerySession object for errors after setQuery. Thus it continues setting things up after the QuerySession knows the state is invalid.",1
"DM-2411","03/25/2015 16:29:09","Allow qserv-admin.py to delete a node","Registered workers in CSS with qserv-admin.py are currently not able to be removed (no DELETE NODE type command). Also, changing node status from ACTIVE to INACTIVE needs to be fixed.",1
"DM-2417","03/26/2015 17:23:45","Data loader script crashes trying to create chunk table","Vaikunth discovered a bug in data loader when trying to load a data into Object table: {noformat} [CRITICAL] root: Exception occured: Table 'Object_7480' already exists Traceback (most recent call last):   File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 318, in <module>     sys.exit(loader.run())   File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-data-loader.py"", line 254, in run     self.loader.load(self.args.database, self.args.table, self.args.schema, self.args.data)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 171, in load     return self._run(database, table, schema, data)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 209, in _run     self._loadData(database, table, files)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 586, in _loadData     self._loadChunkedData(database, table)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 653, in _loadChunkedData     self._makeChunkAndOverlapTable(conn, database, table, chunkId)   File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/dataLoader.py"", line 727, in _makeChunkAndOverlapTable     cursor.execute(q)   File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 176, in execute     if not self._defer_warnings: self._warning_check()   File ""build/bdist.linux-x86_64/egg/MySQLdb/cursors.py"", line 92, in _warning_check     warn(w[-1], self.Warning, 3) Warning: Table 'Object_7480' already exists {noformat} It looks like I did not do enough testing after my recent improvement in creating chunk tables. It tries to create the chunk table with ""CREATE TABLE IF NOT EXISTS ..."" but that actually generates ""warning exception"" on mysql side when table is already there. Need to catch this exception and ignore it.",1
"DM-2423","03/27/2015 11:32:02","Weighting in photometric calibration is incorrect","Dominique points out that the zero point calibration uses errors not inverse errors to calculate the zero point.  git annotate reveals: bq. 24c9149f python/lsst/meas/photocal/PhotoCal.py (Robert Lupton the Good 2010-12-13 05:03:12 +0000 353)     return np.average(dmag, weights=dmagErr), np.std(dmag, ddof=1), len(dmag)  Please fix this.  At the same time, we should add a config parameter to soften the errors. ",1
"DM-2426","03/27/2015 13:34:31","Implement SpanSet+ellipse operations","Implement the following SpanSet operations:   - Construct from an ellipse - note geom::ellipses::PixelRegion; this should do most of the work.   - Compute centroid - see old Footprint implementation   - Compute shape (quadrupole moments) - see old Footprint implementation    One complication here is that this will introduce a circular dependency between afw::geom and afw::geom::ellipses.  That's easy to address at the C++ level, but it's tricky in Python (which package imports the other?)  I'll be emailing dm-devel shortly to start a discussion on how to address this problem.",2
"DM-2427","03/27/2015 13:36:53","Implement SpanSet applyFunctor methods","Implement methods that apply arbitrary functors to pixels within a SpanSet, as described on RFC-37.    The only tricky part of this implementation will be the ""traits"" classes that allow different target objects to interpreted differently.  I'd be happy to consult on this; I have a rough idea in my head, but it needs to be fleshed out.",3
"DM-2429","03/28/2015 12:29:12","Add aperture corrections to meas_extensions_photometryKron","When transitioning {{meas_extensions_photometryKron}} to the new measurement framework, aperture correction was omitted pending the completion of DM-85. It needs to be re-enabled when that epic is complete.",1
"DM-2430","03/29/2015 07:48:52","Make qserv server-side log messages more standard","Qserv server-side Python logging appears to mostly use a common format: ""{{%(asctime)s %(name)s %(levelname)s: %(message)s}}"".  It also mostly uses a common date format: ""{{%m/%d/%Y %I:%M:%S}}"".  But I see instances of: * ""{{%(asctime)s %(levelname)s %(message)s}}"" * ""{{%(asctime)s - %(name)s - %(levelname)s - %(message)s}}"" *  ""{{%(asctime)s \{%(pathname)s:%(lineno)d\} %(levelname)s %(message)s}}"" * and now, after DM-2176, ""{{%(asctime)s \[PID:%(process)d\] \[%(levelname)s\] (%(funcName)s() at %(filename)s:%(lineno)d) %(name)s: %(message)s}}""  Unless these are used in very different contexts, it will aid automated log processing for them to be more standardized.  In addition, the date format is unacceptable as it does not use RFC 3339 (ISO8601) format and does not include a timezone indicator (which means the default {{datefmt}} is insufficient).  This must be fixed.  See also DM-1203.",1
"DM-2435","03/29/2015 21:00:34","Reading an Exposure from disk aborts if the Psf is of an unknown type","Attempting to read an Exposure (in this case via the butler) fails if the PSF class isn't available.  An exception would be reasonable, but an assertion failure is not.  Running the attached script on tiger-sumire with bq. setup python anaconda; setup -T v10_1_rc2 lsst_apps; setup -j distEst -t HSC; setup -j -r ~/LSST/obs/subaru  {code}  WARNING: Could not read PSF; setting to null: PersistableFactory with name 'PsfexPsf' not found, and import of module 'lsst.meas.extensions.psfex' failed (possibly because Python calls were not available from C++). {0}; loading object with id=4, name='PsfexPsf' {1}; loading object with id=28, name='CoaddPsf' {2} python: src/table/io/InputArchive.cc:109: boost::shared_ptr<lsst::afw::table::io::Persistable> lsst::afw::table::io::InputArchive::Impl::get(int, const lsst::afw::table::io::InputArchive&): Assertion `r.first->second' failed. Aborted {code}",1
"DM-2436","03/30/2015 07:48:57","Cherry-pick ""fix makeRGB so it can replace saturated pixels and produce an image"" from HSC","HSC-1196 includes fixes and test cases for {{afw}}. After review on HSC, they should be checked/merged to LSST.",0.5
"DM-2441","03/30/2015 11:53:09","iRODS test: Register data in place","In our first tests of iRODS, we have used ""iput"" to load data into iRODS cache spaces (the iRODS Vault).  For large collections already in a well known location on a server, one may want to leave the data in place but still manage it with iRODS. To do this one can use ""ireg"" to register the data with IRODS without the upload process.",2
"DM-2442","03/30/2015 11:55:51","iRODS usage, devel survey","Read up on current IRODS usage and development track. ",3
"DM-2451","03/31/2015 15:35:09","Fix interface between QservOss and new cmsd version","QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet:  {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code} ",1
"DM-2455","03/31/2015 19:12:48","uncaught exceptions in GaussianFlux","{{SdssShapeAlgorithm::computeFixedMomentsFlux}}, which is used to implement {{GaussianFlux}}, now throws an exception when the moments it is given are singular.  That shouldn't have affected the behavior of {{GaussianFlux}}, as it contains an earlier check that should have detected all such bad input shapes.  But that doesn't seem to be the case: we now see that exception being thrown and propagating up until it is caught and logged by the measurement framework, resulting in noisy logs.  We need to investigate what's going wrong with these objects, and fix them, which may be in {{SdssShape}} or in the {{SafeShapeExtractor}} {{GaussianFlux}} uses to sanitize its inputs.",1
"DM-2456","03/31/2015 23:36:12","Participate in April design process","Most work here was with designing firefly tools API related details.",8
"DM-2466","04/01/2015 10:46:29","lsstsw ./bin/deploy needs LSSTSW set to install products in the right place","I  cloned lsstsw into ~/Desktop/templsstsw and cd'd into it and typed ./bin/deploy and was shocked to find it installed everything into ~/lsstsw, leaving an unsable mess: some files were in templsstsw and some in ~/lsstsw.  The short-term workaround is to manually set LSSTSW before running ./bin/deploy, but this should not be necessary; bin/deploy should either set LSSTSW or not rely on it. I don't recall this problem with earlier versions of lsstsw; I think this is a regression.  For now I updated the instructions at https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool but I look forward to being able to revert that change.",1
"DM-2467","04/01/2015 13:45:00","Implement stitching multiple patches across tract boundaries in a coadd v2","* Find region that returns multiple tractPatchLists for testing.  * Request region via central point (RA, Dec) with width and height definable in arcseconds and pixels.  * May be extend web interface to other data sets, and/or good seeing SkyMaps. ",8
"DM-2475","04/02/2015 11:14:05","Build 2015_04 Qserv release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"DM-2477","04/02/2015 16:34:27","Design API and RFC design","Use the HSC implementation of the base class as a point of reference for designing an integrated Approximate and Interpolate class.  The design take into account Chebyshev, spline, and Gaussian process mechanisms.  Want to take into consideration client code.  I.e. it shouldn't make current consumers more complicated (background and aperture correction to name two).  RFC the designed API.",8
"DM-2491","04/06/2015 12:49:49","Initial survey of Datacat for LSST ","Jacek, Brian Van Klaveren have sent along some initial overview/description of their work on Datacat;      https://confluence.slac.stanford.edu/display/~bvan/LSST+Datacat+Overview  We start examining this in the context of our studies of managing data collections at NCSA.",1
"DM-2492","04/06/2015 16:09:13","shapelet unit tests attempts to access display on failure","When tests/profiles.py tests fail, they attempt to create live plots without checking for any variables that indicate that the display should be used.  These plots should be disabled, as they obscure the real error when the display is not available.",1
"DM-2497","04/07/2015 18:06:53","Fix g++ 4.9 return value implicit conversion incompato","g++ 4.9 enforces the ""explicit"" keyword on type conversion operators in return value context.  This mean bool checkers along the lines of  bool isValidFoo() { return _smartPtrFoo; }  require an explicit cast to compile under g++ 4.9 with -std=c++0x.  There were a handful of these in our code; found and fixed.",1
"DM-2506","04/09/2015 00:02:13","Document structure of our custom ddl ascii schema","Need to better document what is supported / accepted by schemaToMeta.py. We are currently relying on cat/sql/baselineSchema.sql as the guide.",2
"DM-2508","04/09/2015 01:01:49","Information exchange between processes - implementation","Implement system for information exchange between cmsd and xrootd, per instructions in DM-2507",8
"DM-2511","04/09/2015 09:35:09","The distance field of match lists should be set","The meas_astrom AstrometryTask returns a match list that has distance = 0 for all elements. Neither the matcher nor the WCS fitter are setting this field, and both ought to.",2
"DM-2518","04/10/2015 02:25:44","Add a CFHT-based post-build integration test to the sandbox build","From [~boutigny]    I have installed some simple stack validation tools working on CFHT data in {{/lsst8/boutigny/valid_cfht}}    Here is the content of the README file :    ------------------------------------------------------------------------------------------------------------------------  This directory contains a set of utilities to validate a stack release with CFHT data    At the moment, only validation plots for the astrometry are produced    Directories :  -------------  rawDownload     : contain raw CFHT images (flat, dark, bias, fringe,... corrected)  reference_plots : contain reference plots corresponding to the best results obtain so far.    Files :  -------  setup.cfht       : stack environment setup  valid_cfht.sh    : run processCcd taks on the cfht images     valid_cfht.sh init : create the input/ouput directories, ingest raw images and run processCcd     valid_cfht.sh      : without the ""init"" argument, runs processCcd assuming that the directory structure exists and that the raw images have been ingested.  valid_cfht.py    : run some analysis on the output data produced by valid_cfht.sh  processConfig.py : configuration parameters for processCcd  run.list         : list of vistits / ccd to be processed by processCcd    Requirements :  --------------  obs_cfht : tickets/DM-1593  astrometry_net_data : SDSS_DR9 reference catalog corresponding for CFHT Deep Field #3  ------------------------------------------------------------------------------------------------------------------------    Basically it produces a set of plots stored in a png image that can be compared to a reference plot corresponding to the best results obtained so far with stack_v10_0    I hope that this is useful. Just be careful that I wrote these scripts with my own ""fat hand full of fingers"" and that it is just basic code from a non expert. If it is useful, I can certainly add more plots to validate the psf determination, photometry, etc.    Comments, suggestions and criticisms are very welcome.",1
"DM-2521","04/10/2015 12:06:52","Update repo.yaml for first set of Sims Stash repo moves","The repos.yaml file needs to be updated with correct repository locations once SIM-1074 is completed.",1
"DM-2533","04/13/2015 12:43:05","Remove version attribute from Schema","Remove the Schema attribute and its getters and setters.  This change won't be something we can merge to master on its own, as it doesn't provide backwards-compatible FITS reading that will added in future tasks.",1
"DM-2535","04/13/2015 12:47:14","Backwards compatibility for reading compound fields from FITS","Read old-style afw::table compound fields in as scalar fields, using the new FunctorKey conventions.",2
"DM-2536","04/13/2015 12:48:33","Backwards compatibility for reading slots and measurements from FITS","Rename fields to match the new slot and measurement naming conventions.",2
"DM-2538","04/13/2015 13:32:38","RESTful python client","Develop basic abstractions for restful apis in a python client",3
"DM-2543","04/14/2015 10:58:35","Python APIs for Firefly ","We need Python APIs to interface with Firefly visualization components.  This is the first set of many functions.  ",8
"DM-2545","04/15/2015 06:48:44","LaTeX support in Doxygen broken","LaTeX markup in Doxygen documentation ought to be rendered properly for display in HTML. It isn't: it's just dumped to the page as raw text. See, for example, [the documentation for {{AffineTransform}}|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_04_15_07.01.28/classlsst_1_1afw_1_1geom_1_1_affine_transform.html#details].",1
"DM-2546","04/15/2015 11:02:52","Host.cc doesn't find gethostname and HOST_NAME_MAX under el7","el7 gives an error that it can't find HOST_NAME_MAX.",1
"DM-2547","04/15/2015 13:29:21","Fix again interface between QservOss and new cmsd version","QservOSS gives an error when attempting to run queries on the worker from the czar. Error log snippet:  {code} QservOss (Qserv Oss for server cmsd) ""worker"" 150331 16:06:17 9904 Meter: Unable to calculate file system space; operation not supported 150331 16:06:17 9904 Meter: Write access and staging prohibited. ------ worker@lsst-dbdev3.ncsa.illinois.edu phase 2 server initialization completed. ------ cmsd worker@lsst-dbdev3.ncsa.illinois.edu:36050 initialization completed. {code} ",8
"DM-2549","04/15/2015 18:48:27","The string repr of Coord should show the coordsys and angles in degrees","The default string representation of Coord (e.g. std::cout << coord in C++ and str(coord) in Python) is to show class name and a pair of angles in radians.  It would be much more useful if the default display showed the angles in degrees, as that is what people are used to. Also, it would be very helpful if the display included the name of the coordinate system. This is especially needed for the base class, as it is quite common to get shared_ptr to Coord and have no idea what coordinate system it is.  At present there is a lot of code that unpacks the angles and explicitly displays them as degrees to get around this problem. But it seems silly to have to do that.",2
"DM-2551","04/15/2015 19:08:56","ANetAstrometryTask's debug doesn't fully work","{{ANetAstrometryTask}}'s debug code calls (deprecated) method {{Task.display}}, which raises an AttributeError on this coce:  {code}  try:      sources[0][0]  except IndexError:              # empty list      pass  except (TypeError, NotImplementedError): # not a list of sets of sources  {code}  ",1
"DM-2552","04/16/2015 03:24:06","xrootd can't be started via ssh","{code:bash} qserv@clrinfopc04:~/src/qserv$ ssh localhost -vvv ""~qserv/qserv-run/2015_02/etc/init.d/xrootd start"" ... debug3: Ignored env _ debug1: Sending command: ~qserv/qserv-run/2015_02/etc/init.d/xrootd start debug2: channel 0: request exec confirm 1 debug2: callback done debug2: channel 0: open confirm rwindow 0 rmax 32768 debug2: channel 0: rcvd adjust 2097152 debug2: channel_input_status_confirm: type 99 id 0 debug2: exec request accepted on channel 0 Starting xrootd.. debug1: client_input_channel_req: channel 0 rtype exit-status reply 0 debug1: client_input_channel_req: channel 0 rtype eow@openssh.com reply 0 debug2: channel 0: rcvd eow debug2: channel 0: close_read debug2: channel 0: input open -> closed {code}  Here ssh command freeze, it is possible to lauch xrootd with this (example) script: {code:bash} set -e set -x  . /qserv/run/etc/sysconfig/qserv export QSW_XRDQUERYPATH=""/q"" export QSW_DBSOCK=""${MYSQLD_SOCK}"" export QSW_MYSQLDUMP=`which mysqldump` QSW_SCRATCHPATH=""${QSERV_RUN_DIR}/tmp"" QSW_SCRATCHDB=""qservScratch"" export QSW_RESULTPATH=""${XROOTD_RUN_DIR}/result"" export LSST_LOG_CONFIG=""${QSERV_RUN_DIR}/etc/log4xrootd.properties""  eval '/qserv/stack/Linux64/xrootd/xssi-1.0.0/bin/xrootd -c /qserv/run/etc/lsp.cf -l /qserv/run/var/log/xrootd.log -n worker -I v4 &'  echo ""SCRIPT STARTED"" {code} and the same problem occurs. So the problem seems to be with xrootd, and not the startup scripts.   ",5
"DM-2554","04/16/2015 13:23:11","Remove most compound fields from afw::table","Remove all Point, Moment, Coord, and Covariance compound fields.  Array fields should be retained for now; it's not clear if we want to remove it or not, or how to handle variable-length arrays if we do.",2
"DM-2555","04/17/2015 12:23:15","Create and advertise Firefly mailing list","Create an IPAC mailing list for all users of Firefly.  Advertise it to the interested communities (including the LSST Camera group) and through the Github site.  The mailing list firefly@ipac.caltech.edu has been created and all the interested partied have been subscribed to the list.",1
"DM-2579","04/20/2015 14:59:15","Calling AliasMap::get("""") can return incorrect results","It looks like empty string arguments can cause AliasMap to produce some incorrect results, probably due to the partial-match logic being overzealous.",1
"DM-2580","04/21/2015 04:21:31","Implement user-friendly template customization","Qserv configuration tool has to be improved to allow developers/sysadmin to easily use their custom configuration files (with custom log level, ...) for each Qserv services.    An optional custom/ config file directory will be added, and configuration files templates which will be here will override the ones in the install directory.    This should be thinked alongside configuration management inside Docker container.",5
"DM-2581","04/21/2015 06:32:49","log4cxx build failure on OS X","[~frossie] writes:  {quote} I have a log4cxx failure on a Macp while building lsst_distrib. Attaching file in case someone has any bright ideas for me in the morning {quote}",1
"DM-2582","04/21/2015 10:08:33","Research MaxScale as a mysql-proxy replacement","We have been told by Monty that MaxScale is the replacement of the mysql-proxy. Based on DM-2057 the sentiment is that it won't work for our needs. We should very briefly document what our needs are, how we use the proxy now, and if we think MaxScale is not good-enough, say it why, and discuss with Monty and his team.",5
"DM-2593","04/22/2015 13:16:47","Client API for new worker management service","We have new worker management service which has HTTP interface, now we need to provide simple way to access it from Python basically wrapping all HTTP details into simple Python API. ",8
"DM-2594","04/22/2015 14:26:14","Change repos.yaml for next set of Simulations Stash repos","The next set of Simulations Stash repository migrations is laid out in SIM-1121.",1
"DM-2595","04/23/2015 03:36:21","Symlink data directory at configuration","We decided to introduce symlinks in order to protect data. This is in particular useful when we need to reinstall qserv, but we have valuable, large data set that we want to preserve. This story introduces symlinks to data: when Qserv is reinstalled, only the symlink is destroyed, and the data stay untouched.",5
"DM-2599","04/23/2015 21:42:12","afw.Image.ExposureF('file.fits.fz[i]') returns the image in 'file.fits.fz[1]' ","It seems that afwImage.ExposureF ignores the extension number when this is passed on as part of the filename and uses the image in extension number 1. This is not the case with afwImage.MaskedImageF which correctly uses the input extension number passed in the same way.  The problem has been checked on OSX Yosemite 10.10.3 with  the is illustrated in  the following code https://gist.github.com/anonymous/d10c4a79d94c1393a493  which also requires the following image in the working directory: http://www.astro.washington.edu/users/krughoff/data/c4d_130830_040651_ooi_g_d1.fits.fz ",3
"DM-2606","04/24/2015 13:39:59","HSC backport: recent Footprint fixes","This is a backport issue to capture subsequent HSC-side work on features already backported to afw.  It includes (so far) the following HSC issues:   - [HSC-1135|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1135]   - [HSC-1129|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1129]   - [HSC-1215|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1215]",2
"DM-2623","04/27/2015 16:32:06","Design Basic Watcher","Design watcher, including its interactions with other components (mysql, css, etc). In the near term, the watcher will handle deleting tables and databases.",2
"DM-2624","04/27/2015 16:33:12","Implement DROP table in watcher","Implement DROP table using the watcher designed in DM-2623.",1
"DM-2625","04/27/2015 16:34:43","Create service for managing watcher","We need to be able to start/stop the watcher implemented through DM-2624. This story involves extending our scripts for starting various qserv services to manage watcher.",1
"DM-2627","04/28/2015 09:59:10","Add support for configuring multi-node integration tests","The multi-node integration test software produced through DM-2175 has hardcoded node names. This story will allow user to configure it. Current plan is to pre-set integration test for several different configurations, e.g., single-node, 2-node, 8-node (and maybe eg 24-node), and user would supply node names through a configuration file.",5
"DM-2629","04/28/2015 10:38:52","Fix build for gcc 4.7.2 and gcc 4.8.2","#include <condition_variable> is missing in threadSafe.h",1
"DM-2630","04/28/2015 10:50:49","Document configuration tool main use cases","- Document main use case for qserv-configure.py: install Qserv master/worker node with externalized data directory  - Hide complex configuration options?  {code} Configuration steps:   General configuration steps    -d, --directory-tree  Create directory tree in QSERV_RUN_DIR, eventually                         create symbolic link from QSERV_RUN_DIR/var/lib to                         QSERV_DATA_DIR.   -e, --etc             Create Qserv configuration files in QSERV_RUN_DIR                         using values issued from meta-config file                         QSERV_RUN_DIR/qserv-meta.conf   -c, --client          Create client configuration file (used by integration                         tests for example)  Components configuration:   Configuration of external components    -X, --xrootd          Create xrootd query and result directories   -C, --css-watcher     Configure CSS-watcher (i.e. MySQL credentials)  Database components configuration:   Configuration of external components impacting data,   launched if and only if QSERV_DATA_DIR is empty    -M, --mysql           Remove MySQL previous data, install db and set                         password   -Q, --qserv-czar      Initialize Qserv master database   -W, --qserv-worker    Initialize Qserv worker database   -S, --scisql          Install and configure SciSQL {code}  ",3
"DM-2634","04/28/2015 11:37:14","add new image stretch algorithm to Firefly visualization ","There is a need to include two new stretch algorithms, which are asinh and power law gamma.  The algorithm is as follow: * asinh ## input        zp: zero point of data        mp: maximum point of data        dr:  dynamic range scaling factor of data.  It ranges from 1-100,000        bp: black point for image display        wp: white point for image display ## calculate rescaled data value        rd = dr *(xPix - zp)/mp ## calculate normalized stretch data value         nsd = asinh(rd)/asinh(mp-zp) ## calculate display pixel value        dPix = 255 * (nsd-bp)/wp       Note: The bp, wp values specify how far outside of the scale data one wants the image to display.  By default, setting bp=0 and wp=dr.    * power law gamma ## input \br        zp: zero point of data        mp: maximum point of data        gamma: gamma value for exponent ## calculate rescaled data value        rd = xPix - zp ## calculate normalized stretch data value         nsd =  rd^(1/gamma) / (mp0zp)^(1/gamma) ##  calculate display pixel data value         dPix = 255 * nsd       ",8
"DM-2635","04/28/2015 13:35:00","Provide a function to return the path to a package, given its name","As per RFC-44 we want a simple function in utils that returns the path to a package given a package name. This has the same API as eups.getProductDir, but hides our dependence on eups, as per the RFC.",2
"DM-2636","04/28/2015 13:37:13","Update code to use the function provided in DM-2635","As per RFC-44: update existing code that finds packages using eups.getProductDir or by using environment variables to use the function added in DM-2635",3
"DM-2671","05/01/2015 10:13:55","Build 2015_05 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"DM-2672","05/01/2015 10:15:07","Build 2015_06 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"DM-2679","05/01/2015 17:28:45","Fix default LOAD DATA options","Integration tests in multi-node produced the following error during data loading: {code} 2015-05-01 17:03:03,030 - lsst.qserv.admin.dataLoader - CRITICAL - Failed to load data into non-partitioned table: Data truncated for column 'poly' at row 60 2015-05-01 17:03:03,031 - root - CRITICAL - Exception occured: Data truncated for column 'poly' at row 60 {code}  The default options for MySQL LOAD DATA need to be fixed for this.",1
"DM-2682","05/04/2015 09:25:52","Add missing empty-chunk-path on Ubuntu 14.04","QSERV_DATA_DIR/var/lib/qserv wasn't created on Ubuntu 14.04 and this was breaking loader script. It was working on SL7 for unknown reason. Creation of the directory has been added to qserv-czar config script.",1
"DM-2683","05/04/2015 09:32:50","Fix case05 3009_countObjectInRegionWithZFlux freeze","This prevents 2014_05 release to pass integration tests.",1
"DM-2692","05/04/2015 12:42:50","Rule for automatic replication in iRODS","Maintaining extra copies/replicas on separate resources is an important tenet in iRODS, with this practice considered key for prevention of data loss. The automatic replication of files upon ingest can be encoded via a system rule, so that data is preserved as a inherent part of storing in iRODS.",2
"DM-2694","05/04/2015 14:47:46","Revisit mysql connections from worker","Revisit the code that handles mysql connections in qserv. At the moment Qserv will maintain a connection per chunk-query, up to a hardcoded limit (GroupScheduler: 4, ScanScheduler:32).  Also, we have to gracefully handle connection issues (such as dropped connection, or if we hit the max_connections limit).",8
"DM-2708","05/06/2015 11:09:56","Understand race condition in Executive::_dispatchQuery","Inserting a log (presumably just a delay) in Executive::_dispatchQuery after the new QueryResource but before the Provision call causes queries to fail.  The particular test query was ""select count(*) from Object"" on test case 01.",2
"DM-2710","05/06/2015 14:20:34","Mutex use before creation","qana/QueryPlugin.cc contains a static boost::mutex, that is used by static class member functions to register plugin implementations. Its constructor is not guaranteed to be called before the static registerXXXPlugin (see e.g. qana/AggregatePlugin.cc) instances use it to register plugin classes.",1
"DM-2711","05/06/2015 14:23:05","Migrate boost:thread to std::thread","We are mixing boost and std threading libraries. This should be cleaned up - use std:thread consistently everywhere.",5
"DM-2712","05/06/2015 14:25:34","Migrate boost::shared_ptr to std::shared_ptr","We are mixing boost and std shared_ptrs. This should be cleaned up - use std:shared_ptr consistently everywhere. In a few places we have other types of pointers, (e.g weak_ptr). Migrate these too.",2
"DM-2716","05/07/2015 19:35:38","Fix connection leak (2nd iteration)","Fix connection leak (and memory leak and thread leak) -- we are leaking 2 per query.",2
"DM-2718","05/08/2015 03:49:07","Upgrade EUPS used by lsstsw","As discussed, bump it up when you get a chance please. ",1
"DM-2722","05/08/2015 13:55:59","Revisit design of query poisoner","As we discovered through DM-2698, poisoner tends to hold onto query resources even after the query completes. We should revisit whether than can be redesigned and improved, so that when query finishes, all resources related to that query are immediately automatically released. This story involves just the planning part, implementation will be done through separate stories.",1
"DM-2728","05/09/2015 17:26:03","Build should fail if node.js is not present","Problem: I built Firefly by mistake w/o having node on my path. The build didn't signal any errors, but generated an unusable webapp that wouldn't load.  Expected behavior: the build should have failed and warned the user that node.js is missing.",2
"DM-2729","05/09/2015 17:43:54","Fix a few more g++ 4.9.2 compatos","Some of the recent boost -> std changes don't compile/link under gcc 4.9.2, because of some poor #include hygiene (including <thread> when we should include <condition_variable>, not explicitly including <unistd.h>, etc.)  Also, -pthread linker option is required when using std::thread under gcc 4.9.2. ",1
"DM-2734","05/11/2015 15:19:05","Add config file for test dataset 04 tables","Following the changes to default LOAD DATA settings in DM-2679, two tables in test case 04 need to have a config file to include their in.csv format.",1
"DM-2737","05/12/2015 11:13:29","Build a DiscreteSkyMap that covers a collection of input exposures","This is essentially a rehash of the old trac Ticket #[2702| https://dev.lsstcorp.org/trac/ticket/2702], originally reported by [~jbosch], which reads:  ""I'd like to add a Task and bin script to create a DiscreteSkyMap that bounds a set of calexps specified by their data IDs. This makeDiscreteSkyMap.py could be used instead of makeSkyMap.py when the user would rather compute the pointing and size of the skymap from the input data than decide it manually.""  The work was done by [~jbosch] & [~price] and exists on branch {{u/price/2702}} in {{pipe_tasks}}, but it was never merged to master.  I plan to simply rebase the commits in that branch onto master.",1
"DM-2738","05/13/2015 01:41:35","Remove #include ""XrdOuc/XrdOucTrace.hh"" from Qserv code","See next emails:  Hi Fabrice,  Absolutely!  Andy  On Wed, 13 May 2015, Fabrice Jammes wrote:  > Hi Andy, > > Thanks, > > In my understanding, you're ok if I remove the existing > #include ""XrdOuc/XrdOucTrace.hh"" > from Qserv source code. I'll do it soon. > > Have a nice day, > > Fabrice > > Le 12/05/2015 23:41, Andrew Hanushevsky a écrit : >> Hi Fabrice, >> >> Well, no. We have a long-standing approach that qserv should not depend on anything outside of XrdSsi public interfaces. This is the only way to easily protect sqserv code from infrastructure changes. So, I would not. If you want to copy something like that for >> >> qserv please do, it's simple enough. But in the end qserv needs to be self-contained in that it does not depend on xrootd code just the public ssi interfaces. >> >> Andy >> >> -----Original Message----- From: Fabrice Jammes >> Sent: Tuesday, May 12, 2015 9:06 AM >> To: Andrew Hanushevsky >> Subject: About xrdssi client logging >> >> Hi Andy, >> >> Hope you're doing well. >> Could you please tell me if its usefull to include >> #include ""XrdOuc/XrdOucTrace.hh"" >> in our xrdssi client code? >> >> Indeed client seems to only print DBG macro output, that's why I was >> wondering if XrdOucTrace was only use on the server side. >> If yes, I will remove it from our client. >> >> Thanks, and have a nice day, >> >> Fabrice ",1
"DM-2740","05/13/2015 18:01:34","Make ANetAstrometryTask more configurable","The current ANetAstrometryTask has a solver that is not easy to retarget. This makes testing with hscAstrom needlessly difficult. My suggestion is to make the solver a true Task instead of a task-like object, and make it retargetable using a ConfigurableField instead of a ConfigField. This is very easy to do because the solver is already a task in all but name. ",2
"DM-2748","05/15/2015 03:02:03","Add clear message when integration test fails","Integration test fails without printing a clear message at the end, and for now a query is broken: 0011_selectDeepCoadd.txt but it isn't printed at the end of tet output.",2
"DM-2752","05/15/2015 13:42:46","db 10.1+4 tests randomly fail with python egg installation error","The unit tests for DB seem to fail at random and always pass on a second build attempt.  My hunch is that multiple tests are running in parallel all attempting to install the mysql module but I haven't investigated.  {code}                   db: 10.1+4 ERROR (0 sec). *** error building product db. *** exit code = 2 *** log is in /home/build0/lsstsw/build/db/_build.log *** last few lines: :::::  [2015-05-15T19:12:35.557258Z] scons: done reading SConscript files. :::::  [2015-05-15T19:12:35.558276Z] scons: Building targets ... :::::  [2015-05-15T19:12:35.558409Z] scons: Nothing to be done for `python'. :::::  [2015-05-15T19:12:35.570007Z] makeVersionModule([""python/lsst/db/version.py""], []) :::::  [2015-05-15T19:12:35.686733Z] running tests/testDbLocal.py... running tests/testDbRemote.py... running tests/testDbPool.py... failed :::::  [2015-05-15T19:12:35.695011Z] passed :::::  [2015-05-15T19:12:35.698811Z] passed :::::  [2015-05-15T19:12:35.706360Z] 1 tests failed :::::  [2015-05-15T19:12:35.706703Z] scons: *** [checkTestStatus] Error 1 :::::  [2015-05-15T19:12:35.708443Z] scons: building terminated because of errors. {code}  {code} [root@ip-192-168-123-151 .tests]# cat * tests/testDbLocal.py  Traceback (most recent call last):   File ""tests/testDbLocal.py"", line 53, in <module>     from lsst.db.db import Db, DbException   File ""/home/build0/lsstsw/build/db/python/lsst/db/db.py"", line 49, in <module>     import MySQLdb   File ""build/bdist.linux-x86_64/egg/MySQLdb/__init__.py"", line 19, in <module>        File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 7, in <module>   File ""build/bdist.linux-x86_64/egg/_mysql.py"", line 4, in __bootstrap__   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 937, in resource_filename   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1632, in get_resource_filename   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1662, in _extract_resource   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 1003, in get_cache_path   File ""/home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py"", line 983, in extraction_error pkg_resources.ExtractionError: Can't extract file(s) to egg cache  The following error occurred while trying to extract file(s) to the Python egg cache:    [Errno 17] File exists: '/home/build0/.python-eggs'  The Python egg cache directory is currently set to:    /home/build0/.python-eggs  Perhaps your account does not have write access to this directory?  You can change the cache directory by setting the PYTHON_EGG_CACHE environment variable to point to an accessible directory.  tests/testDbPool.py  /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-test.txt' not found. tests/testDbRemote.py  /home/build0/lsstsw/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg/pkg_resources.py:1032: UserWarning: /home/build0/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable). 05/15/2015 07:12:35 root WARNING: Required file with credentials '/home/build0/.lsst/dbAuth-testRemote.txt' not found. {code}",1
"DM-2762","05/18/2015 16:54:16","Avoid leaking memory allocated by mysql_thread_init","mysql/MySqlConnection.cc contains the following comment: {code}     // Dangerous to use mysql_thread_end(), because caller may belong to a     // different thread other than the one that called mysql_init(). Suggest     // using thread-local-storage to track users of mysql_init(), and to call     // mysql_thread_end() appropriately. Not an easy thing to do right now, and     // shouldn't be a big deal because we thread-pool anyway. {code}  The comment is not really correct with regards to thread pooling. Instead, each rproc::InfileMerger has an rproc::InfileMerger::Mgr which contains a util::WorkQueue that spawns a thread, and so we are failing to call mysql_thread_end at least once per user query. This has been verified using the memcheck valgrind tool. ",3
"DM-2770","05/20/2015 19:02:42","sconsUtil install target does not respond to either force=True or --force","I've been unable to figure out how to bypass the install 'force' check, but have confirmed that this is the correct expression by commenting it out:    https://github.com/lsst/sconsUtils/blob/54c983ffe9714a33657c4388de3506fe7a40518d/python/lsst/sconsUtils/installation.py#L92    {code}  $ SCONSUTILS_DIR=. scons -Q force=True install   Unable to import eups; guessing flavor  CC is gcc version 4.8.3  Checking for C++11 support  C++11 supported with '-std=c++11'  Error with git version: uncommitted changes  Found problem with version number; update or specify force=True to proceed  {code}  ",1
"DM-2777","05/21/2015 13:58:16","Fix races in BlendScheduler","_integrityHelper() from wsched/BlendScheduler inspects a map of tasks and is sometimes called without holding the corresponding mutex. My theory is that it is observing the map in an inconsistent state, leading to assert failure and hence worker death, and finally to hangs/timeouts on the czar.",2
"DM-2779","05/21/2015 16:45:55","Fix race in Foreman","The Foreman implementation passes a TaskQueue pointer corresponding to running tasks down to the task scheduler without holding a lock. This means that the scheduler can inspect the running task list (usually to determine its size) while it is being mutated.",2
"DM-2782","05/22/2015 13:13:06","Firefly Tools API: Add advance region support","Firefly Tools API: Add advance region support  Improve firefly's region functionality to support a ""dynamic region"".  Data can be added or removed from this region by API calls.  Allow any amount of region lines to be added or removed.  Make sure performance is good.  Also, document the current Firefly region support.",2
"DM-2787","05/22/2015 13:46:55","Footprint dilation performance regression","In DM-1128 we implemented span-based dilation for footprints. A brief test on synthetic data indicated that this was a performance win over the previous version of the code.    In May 2015, this code was merged to HSC and applied to significant quantities of real data for the first time. A major performance regression was identified:    {quote}  [May-9 00:26] Paul Price: processCcd is now crazy slow.  [May-9 00:29] Paul Price: Profiling...  [May-9 00:40] Paul Price: I'm thinking it's the Footprint grow code...  [May-9 00:44] Paul Price: And the winner is…. Footprint construction:  [May-9 00:44] Paul Price: 2    0.000    0.000  702.280  351.140 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:191(makeSourceCatalog)         2    0.005    0.002  702.274  351.137 /home/astro/hsc/products/Linux64/meas_algorithms/HSC-3.8.0/python/lsst/meas/algorithms/detection.py:228(detectFootprints)       15    0.001    0.000  698.597  46.573 /home/pprice/hsc/afw/python/lsst/afw/detection/detectionLib.py:3448(__init__)       15  698.596  46.573  698.596  46.573 {_detectionLib.new_FootprintSet}  [May-9 00:53] Paul Price: If I revert HSC-1243 (""Port better Footprint-grow code from LSST""), then the performance regression goes away.  @jbosch @jds may be interested...  {quote}    The source of the regression must be identified and resolved for both HSC and LSST.",5
"DM-2789","05/22/2015 17:41:44","rename CameraMapper.getEupsProductName() to getPackageName() and convert to abstract method","Per discussion on this PR related to DM-2636:  https://github.com/lsst/daf_butlerUtils/pull/1#issuecomment-104785055    The CameraMapper.getEupsProductName() should be renamed to getPackageName() and converted to an abstract method.  This will eliminates a runtime, and thus ""test time"", dependency on EUPS.  As part of the rename/conversion, all subclasses that are not already overriding getEupsProductName() will concurrently need to have getPackageName() implemented.",3
"DM-2792","05/26/2015 12:45:59","Make the new astrometry task the default task","The new astrometry task should be the default astrometry task, but we need to make sure it is good enough first.",1
"DM-2799","05/27/2015 12:32:07","Tests for daf_butlerUtils should not depend on obs_lsstSim","Currently two of the tests in {{daf_butlerUtils}} depend on {{obs_lsstSim}}. They will never run in a normal build because {{obs_}} packages can not be a dependency on {{daf_butlerUtils}}.    After discussing the options with [~ktl] the feeling is that {{ticket1640}} should be rewritten to remove the dependency and {{ticket1580}} can probably be removed.",2
"DM-2803","05/28/2015 00:14:31","Adapt multi-node tests to latest version of qserv / loader","The multi-node integration tests have to be updated to work with the latest changes to qserv, in particular the loader, which broke already working tests lately.",8
"DM-2804","05/28/2015 00:24:27","Implement query metadata skeleton","Skeleton implementation of the Query Metadata - including the APIs and core functionality (accepting long running query and saving the info about it)",8
"DM-2827","05/28/2015 14:39:08","Implement RESTful interfaces for Database (POST)","Implement RESTful interfaces for Database (see all D* in https://confluence.lsstcorp.org/display/DM/API), based on the first prototype developed through DM-1695. The work includes adding support for returning appropriately formatted results (support the most common formats). This covers ""POST"" type requests only, ""GET"" will be handled separately.",8
"DM-2847","05/31/2015 22:46:55","SUI Firefly server side Python job management","In order to support Camera team needs and L3 data production, Firefly server needs to be able to start a Python job with proper input data and get the output data as a result of running the Python job. This will make the future integration of Firefly and DM pipeline stack much easier. ",40
"DM-2849","06/01/2015 09:54:56","Tweaks to OO display interface","When I wrote the initial version of display_firefly I found a few minor issues in the way I'd designed the Display class; at the same time, [~lauren] found some missing functions in the backward-compatibility support for ds9.    Please fix these;  note that this implies changes to afw, display_ds9, and display_firefly.  ",2
"DM-2854","06/01/2015 14:29:39","Fix Qserv SsiSession worker race","The worker SsiSession implementation calls ReleaseRequestBuffer after handing the bound request to the foreman for processing. It therefore becomes possible for request processing to finish before ReleaseRequestBuffer is called by the submitting thread, resulting in a memory leak.",2
"DM-2864","06/02/2015 10:37:30","Fix bug related to selecting rows by objectId from non-director table","The following example illustrates the problem:    Let's select one raw from qservTest_case01_qserv    {code}  select sourceId, objectId FROM Source LIMIT 1;  +-------------------+-----------------+  | sourceId          | objectId        |  +-------------------+-----------------+  | 29763859300222250 | 386942193651348 |  +-------------------+-----------------+  {code}    Then select it, but use ""sourceId"" in the query, all good here:  {code}  select sourceId, objectId FROM Source WHERE sourceId=29763859300222250;  +-------------------+-----------------+  | sourceId          | objectId        |  +-------------------+-----------------+  | 29763859300222250 | 386942193651348 |  +-------------------+-----------------+  {code}    But if we add ""objectId"", the row is not found:    {code}  select sourceId, objectId FROM Source WHERE sourceId=29763859300222250 and objectId=386942193651348;  Empty set (0.09 sec)  {code}    Similarly, even without sourceId constraint, the query fails:  {code}  select sourceId, objectId FROM Source WHERE objectId=386942193651348;  Empty set (0.09 sec)  {code}    ",8
"DM-2865","06/02/2015 12:00:18","Merge BoundedField from HSC as is","To make headway on aperture corrections, we are bringing the HSC implementation of BoundedField over.",2
"DM-2866","06/02/2015 18:09:24","Learn about Butler","Transferring knowledge from K-T to the DB team.",2
"DM-2867","06/02/2015 18:09:44","Learn about Butler","Transferring knowledge from K-T to the DB team.",2
"DM-2868","06/02/2015 18:10:09","Learn about Butler","Transferring knowledge from K-T to the DB team.",2
"DM-2869","06/02/2015 18:10:26","Learn about Butler","Transferring knowledge from K-T to the DB team.",2
"DM-2870","06/02/2015 18:10:37","Learn about Butler","Transferring knowledge from K-T to the DB team.",2
"DM-2883","06/03/2015 22:00:04","wcslib is unable to read PTF headers with PV1_{1..16} cards","SCAMP writes distortion headers in form of PVi_nn (i=1..x, nn=5..16) cards, but this is rejected (correctly) by wcslib 4.14;  there is a discussion at https://github.com/astropy/astropy/issues/299    The simplest ""solution"" is to strip the values PV1_nn (nn=5..16) in makeWcs()  for CTYPEs of TAN or TAN-SIP and this certainly works.    I propose that we adopt this solution for now.  ",1
"DM-2885","06/05/2015 15:26:15","Improve confusing error message","Selecting a column that does not exist results in confusing error. Example:    {code}  SELECT badColumnName  FROM qservTest_case01_qserv.Object   WHERE objectId=386942193651348;  {code}    ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:42, Error in result data., 1, (-1)    Similarly,     {code}  select whatever   FROM qservTest_case01_qserv.Object;  {code}    prints  ERROR 4120 (Proxy): Error during execution:  -1 Ref=1 Resource(/chk/qservTest_case01_qserv/6630): 20150605-16:23:52, Error in result data., 1,   Ref=2 Resource(/chk/qservTest_case01_qserv/6631): 20150605-16:23:52, Error merging result, 1990, Cancellation requested  Ref=3 Resource(/chk/qservTest_case01_qs (-1)    (note, sourceId does not exist in Object table)      ",5
"DM-2887","06/05/2015 22:46:25","Fix broken IN - it now takes first element only","IN is broken - it only uses the first element from the list. Here is the proof:    {code}  select COUNT(*) AS N FROM qservTest_case01_qserv.Source   WHERE objectId=386950783579546;  +------+  | N    |  +------+  |   56 |  +------+  1 row in set (0.10 sec)    mysql> select count(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId=386942193651348;  +------+  | N    |  +------+  |   39 |  +------+  1 row in set (0.09 sec)    mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId IN(386942193651348, 386950783579546);  +------+  | N    |  +------+  |   39 |  +------+  1 row in set (0.09 sec)    mysql> select COUNT(*) AS N FROM qservTest_case01_qserv.Source  WHERE objectId IN(386950783579546, 386942193651348);  +------+  | N    |  +------+  |   56 |  +------+  1 row in set (0.11 sec)  {code}",8
"DM-2890","06/06/2015 09:19:56","isrTask assumes that the Exposure has a Detector","While trying to use the isrTask to interpolate over bad columns in PTF data I discovered that the code assumes that the Exposure has a Detector attached.    Please remove this restriction.  ",1
"DM-2891","06/06/2015 17:27:51","meas.algorithms.utils uses measurement algorithms that are no longer available","meas.algorithms.utils uses GaussianCentroid and SdssShape, but now that they have moved to meas_base the code no longer works.    Please fix this.  I'd prefer to leave the functionality to visualise PSFs in meas_algorithms, but if necessary file an RFC to move it elsewhere.  ",2
"DM-2892","06/06/2015 23:39:48","Keep track of database of the director table","An L3 child table might very well have an LSST data release Object table as its director, while almost certainly not living in the DR database. To support it, we should keep track of the database name holding director's table. Note, this is related to DM-2864 - the code touched in that ticket should be checking the director's db name.    Don't forget to add a unit test that will exercise it!",1
"DM-2895","06/08/2015 11:21:29","treat lsst_apps, lsst_libs and lsst_thirdparty as top level products not required by lsst_distrib","Per discussion on RFC-55, it was determined that  lsst_apps and lsst_libs and lsst_thirdparty maybe be treated as separate top level products that lsst_distrib need not depend on them nor do they need to be included as part of CI builds.",1
"DM-2900","06/08/2015 18:25:27","Add queries that exercise non-box spatial constraints","Qserv has code to support:   * qserv_areaspec_box   * qserv_areaspec_circle   * qserv_areaspec_ellipse   * qserv_areaspec_poly    but only the first one (box) is exercised in our integration tests. This story involves adding queries to test the other 3.",2
"DM-2905","06/09/2015 10:34:53","Update Scons to v2.3.4","Scons has not been updated in over a year. RFC-61 agreed that we should upgrade it now before tackling some other {{scons}} issues.",1
"DM-2909","06/09/2015 17:17:45","Remove unused code from sconsUtils","The code in {{deprecated.py}} in {{sconsUtils}} is not used by anything anywhere. [~jbosch] has indicated that the file can simply be removed.",1
"DM-2910","06/09/2015 18:45:12","obs_cfht is broken with the current stack","obs_cfht's camera mapper is missing the new packageName class variable, so it is not compatible with the current stack.    I suggest fixing obs_sdss and obs_subaru as well, if they need it.",1
"DM-2911","06/09/2015 23:17:40","Build 2015_07 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"DM-2917","06/11/2015 09:12:18","obs_cfht unit tests are broken","obs_cfht has one unit test ""testButler"" that uses git://git.lsstcorp.org/contrib/price/testdata_cfht. 4 of the tests fail, as shown below.    In addition, testdata_cfht is huge, and the tests barely use any of it. It's worth considering making a new test repo that is smaller, or if the amount of data is small enough, move it into afwdata or obs_cfht itself.    {code}  localhost$ tests/testButler.py   CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  ECameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  .CameraMapper: Loading registry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/DATA/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /Users/rowen/LSST/code/testdata/testdata_cfht/CALIB/calibRegistry.sqlite3  E.  ======================================================================  ERROR: testBias (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 122, in testBias      self.getDetrend(""bias"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testFlat (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 117, in testFlat      self.getDetrend(""flat"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testFringe (__main__.GetRawTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 127, in testFringe      self.getDetrend(""fringe"")    File ""tests/testButler.py"", line 110, in getDetrend      flat = self.butler.get(detrend, self.dataId, ccd=ccd)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 218, in get      location = self.mapper.map(datasetType, dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/mapper.py"", line 116, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/cameraMapper.py"", line 287, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 118, in map      actualId = self.need(self.keyDict.iterkeys(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 199, in need      lookups = self.lookup(newProps, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 345, in lookup      return Mapping.lookup(self, properties, newId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/mapping.py"", line 168, in lookup      where, self.range, values)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/10.1-3-g302a9ed/python/lsst/daf/butlerUtils/registries.py"", line 120, in executeQuery      c = self.conn.execute(cmd, values)  OperationalError: no such column: extension    ======================================================================  ERROR: testRaw (__main__.GetRawTestCase)  Test retrieval of raw image  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testButler.py"", line 101, in testRaw      raw = self.butler.get(""raw"", self.dataId, ccd=ccd, immediate=True)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 244, in get      return callback()    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 242, in <lambda>      innerCallback(), dataId)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 238, in <lambda>      callback = lambda: self._read(pythonType, location)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/butler.py"", line 426, in _read      location.getCppType(), storageList, additionalData)    File ""/Users/rowen/LSST/lsstsw/stack/DarwinX86/daf_persistence/10.1-1-g6edbc00+1/python/lsst/daf/persistence/persistenceLib.py"", line 1430, in unsafeRetrieve      return _persistenceLib.Persistence_unsafeRetrieve(self, *args)  FitsError:     File ""src/fits.cc"", line 1064, in lsst::afw::fits::Fits::Fits(const std::string &, const std::string &, int)      cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r' {0}  lsst::afw::fits::FitsError: 'cfitsio error: could not open the named file (104) : Opening file '/Users/rowen/LSST/code/testdata/testdata_cfht/DATA/raw/08BL05/w2.+2+2/2008-11-01/i2/1038843o.fits.fz[1]' with mode 'r''      ----------------------------------------------------------------------  Ran 6 tests in 3.544s    FAILED (errors=4)  {code}",1
"DM-2919","06/11/2015 13:28:54","PhotoCalTask mis-calling Colorterm methods","When I implemented DM-2797 I made a few errors in pipe_tasks:  - PhotoCalTask mis-calls two methods of Colorterm by providing filterName, which is not needed  - ColortermLibrary.getColorterm mis-handles glob expressions (the two arguments to fnmatch.fnmatch are swapped).    We also need a unit test for applying colorterms, but that will require enough work that I have made a separate ticket for it: DM-2918. Meanwhile I have tested my changes by running Dominique's CFHT demo. This proves that the colorterm code runs, but does not prove that the terms are correctly applied.",1
"DM-2934","06/12/2015 14:40:49","Add RFD issue type to RFC project","To support the RFD process adopted in [RFC-53], an RFD issue type in the RFC project is required.  While we could add RFD-specific fields to it, I think it's simplest if it's just generic with details provided in the Description.",1
"DM-2927","06/12/2015 15:25:40","Modernize sconsUtils code to python 2.7 standard","As part of the work investigating DM-2839 I modernized the sconsUtils code to meet current coding standards (using {{in}} rather than {{has_key}}, using {{items()}} rather than {{iteritems}} etc). Since I'm highly doubtful that DM-2839 is going to be closed any time soon I will separate out the modernization patches into this ticket.",1
"DM-2929","06/12/2015 23:09:46","Some AFW tests are not enabled with no explanation","Running {{coverage.py}} on the AFW test suite indicated that two test classes in {{tests/wcs1.py}} are disabled. {{WCSTestCaseCFHT}} was added by [~rhl] in 2007 but disabled during a merge a long time ago by [~jbosch] in 2010 but with no indication as to why. {{WCSRotateFlip}} appeared in 2012 (added by [~krughoff]) but doesn't appear in the {{suite}} list at the end and so does not execute.    Similarly {{testSchema.py}} has two tests that are not run: {{xtestSchema}} and {{testJoin}}. I assume {{xtestSchema}} is deliberately disabled but could there at least be a comment in the test explaining why?    My feeling is that we should either run the tests or they should be removed. Having them their gives the impression they are doing something useful.    Less importantly, {{warpExposure.py}} has some support code for comparing masked images that was written in 2009 by [~rowen] but which is not used anywhere in the test.",2
"DM-2930","06/13/2015 01:08:17","Fix problem with Qserv related to restarting mysql","I noticed some strange (reproducible!) behavior: if I run:    {code}qserv-check-integration.py --case=01{code}    then restart mysqld    {code}<runDir>/etc/init.d/mysqld restart{code}    then the query:  {code}mysql --host=127.0.0.1 --port=4040 --user=qsmaster   qservTest_case01_qserv -e   ""SELECT COUNT(*) as OBJ_COUNT   FROM Object   WHERE qserv_areaspec_box(0.1, -6, 4, 6)""{code}    consistently fails every single time.    To fix it, it is enough to restart xrootd.",5
"DM-2931","06/13/2015 07:34:02","We write truncated Wcs data to  extended HDU tables in Exposures","When we write Wcs to extra HDUs in Exposures they are truncated if other than TAN/TAN-SIP.  Please don't write them.    A better long term solution is needed.  In particular, we shouldn't be duplicating this information unnecessarily, and we need to be able to persist e.g. TPV to the tables so as to support CoaddPsf.  These issues are not included here.",1
"DM-2936","06/15/2015 23:06:30","Refactor Histogram in edu.caltech.ipac.visualize.plot package.","The Histogram has 6 constructors to handle 6 bitpixel data types which are byte, short integer,  integer, long integer, float and double.  Since FitsRead has now only works on float, there the  Histogram should be refactored accordingly.",3
"DM-2938","06/16/2015 13:40:24","CalibrateTask has an unwanted ""raise"" in it","On 2014-06-30 commit 696b641 a developer added a bare ""raise"" as a debugging aid to the CalibrateTask in pipe_tasks. That change was accidentally merged to master. I confirmed it was an accident and am filing this ticket as a way to remove the raise and run buildbot before merging to master.",1
"DM-2940","06/17/2015 15:34:11","DS9 tests fail if DS9 not running in some configurations","There are a few issues with the robustness of the {{testDs9.py}} tests in AFW.    * The tests are skipped if the {{display_ds9}} package can not be loaded but they should also skip if {{ds9}} is missing or if {{ds9}} can not be loaded. The latter is especially important during builds that unset {{$DISPLAY}}.  * The launching code in {{initDS9}} can not notice the simple case of {{ds9}} immediately failing to load. It simply assumes that there are delays in launch. The reason for this is that {{os.system}} does not return bad status if the command has been started in the background. Another scheme for starting {{ds9}} should be considered. Maybe a different exception could be raised specifically for failing to start it.  * At the moment each test independently has a go at starting {{ds9}}. This makes the tests take a very long time (made worse by {{_mtv}} also trying multiple times) despite it being clear pretty quickly that {{ds9}} is never going to work.  * Currently the {{mtv}} tests must run early as they are the only tests that attempt to start {{ds9}} if it is not running. If the two tests that call {{mtv}} are disabled two other tests fail. Ideally the {{initDS9}} code should be called in all cases.",1
"DM-2944","06/18/2015 15:37:31","SourceMeasurementTask still referenced in our stack","SourceMeasurementTask is gone, but we still have code that refers to it, including:  {code}  /Users/rowen/LSST/lsstsw/build/meas_algorithms/python/lsst/meas/algorithms/debugger.py:     21: from lsst.meas.algorithms.measurement import SourceMeasurementTask     26:     measurement = ConfigurableField(target=SourceMeasurementTask, doc=""Measurements"")    /Users/rowen/LSST/lsstsw/build/meas_algorithms/python/lsst/meas/algorithms/detection.py:    209: The example also runs the SourceMeasurementTask; see \ref meas_algorithms_measurement_Example for more explanation.    /Users/rowen/LSST/lsstsw/build/meas_deblender/examples/utils.py:     15: class DebugSourceMeasTask(measAlg.SourceMeasurementTask):     41:         measAlg.SourceMeasurementTask.preMeasureHook(self, exposure, sources)     74:         measAlg.SourceMeasurementTask.postMeasureHook(self, exposure, sources)     80:         measAlg.SourceMeasurementTask.preSingleMeasureHook(self, exposure, sources, i)    102:         measAlg.SourceMeasurementTask.postSingleMeasureHook(self, exposure, sources, i)    /Users/rowen/LSST/lsstsw/build/meas_deblender/python/lsst/meas/deblender/deblendAndMeasure.py:     31: from lsst.meas.algorithms import SourceMeasurementTask     50:         target = SourceMeasurementTask,    /Users/rowen/LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/calibrate.py:    180: <DT> initialMeasurement \ref SourceMeasurementTask_ ""SourceMeasurementTask""    189: <DT> measurement \ref SourceMeasurementTask_ ""SourceMeasurementTask""    /Users/rowen/LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/imageDifference.py:     36: from lsst.meas.algorithms import SourceDetectionTask, SourceMeasurementTask, \    104:         target=SourceMeasurementTask,    /Users/rowen/LSST/lsstsw/build/pipe_tasks/python/lsst/pipe/tasks/measurePsf.py:    136: The example also runs SourceDetectionTask and SourceMeasurementTask; see \ref meas_algorithms_measurement_Example for more explanation.  {code}    I will handle pipe_tasks calibrate.py as part of DM-435.",1
"DM-2945","06/18/2015 15:41:32","Wmgr refuses to serve queries from remote interface","Vaikunth discovered that wmgr returns 404 for all operations. It looks like wmgr can serve requests coming from 127.0.0.1 interface but returns 404 for queries from non-local interface.",1
"DM-2948","06/19/2015 09:06:03","Remove explicit buildbot dependency on datarel","The buildbot scripts have an explicit dependency on the {{datarel}} package, which we'd like to remove from the stack.  It uses {{datarel}} as the top-level product when building the cross-linked HTML documentation; {{lsstDoxygen}}'s {{makeDocs}} script takes a single package, and generates the list of packages to include in the Doxygen build by finding all dependencies of that package.    So, to remove the explicit dependency on {{datarel}}, we need to either:   - find a new top-level product with a Doxygen build to pass to {{makeDocs}} (e.g. by adding a trivial Doxygen build to {{lsst_distrib}})   - modify the argument parsing in {{lsstDoxygen}} to take a list of multiple products (it *looks* like the limitation to one package is only in the argument parsing), and pass it a list of top-level products in the buildbot scripts.    This is currently a blocker for DM-2928, which itself a blocker for DM-1766, which has now been lingering for a few weeks now.  I'm going to look for other ways to remove the block on the latter, but I don't have a solution yet.",3
"DM-2949","06/19/2015 10:13:34","remove dead code and dependencies from datarel","Removing the {{datarel}} package entirely has proved to be difficult (DM-2928, DM-2948), so instead I'm simply going to remove non-ingest code (and dead ingest code) from the package, along with its dependencies on {{ap}} and {{testing_endToEnd}}.  Other dependencies will be retained even if they aren't necessary for the code that will remain in {{datarel}}, to support {{lsstDoxygen}}'s use of {{datarel}} as a top-level package for documentation generation.",1
"DM-2952","06/19/2015 15:38:58","Crop needs to be refactored","This class needs to be refactored to be in consist with FitsRead class which treats all data type as float.  Thus the bitpix in this class does not have to be treated based on its value.",3
"DM-2966","06/23/2015 02:07:48","Design CSS that supports updates","Design how to redesign CSS, we currently take a snapshot when char starts. It is too static. ",2
"DM-2976","06/25/2015 12:51:12","SourceCatalog.getChildren requires preconditions but does not check them","This is a code transfer from HSC-1247.",2
"DM-2977","06/25/2015 12:54:48","Miscellaneous CModel improvements from HSC","This improves handling of several edge case failure modes, tweaks the configuration to improve performance, and adds some introspection useful for Jose Garmilla's tests.    Includes HSC-1288, HSC-1284, HSC-1228, HSC-1250, HSC-1264, HSC-1273, HSC-1240, HSC-1249, HSC-1238, HSC-990, HSC-1155, HSC-1191",2
"DM-2980","06/25/2015 14:53:10","refactor coaddition code","The HSC fork has coaddition code in two places: pipe_tasks and hscPipe.  The code in hscPipe is what we use (though that depends on the code in pipe_tasks in places), while the code in pipe_tasks is more similar to what's currently on the LSST side.    We want to bring the refactored version in hscPipe back to LSST, but we want to put it directly in pipe_tasks to remove the code duplication that currently exists on the HSC side.    Work on this issue should begin with an RFC that details the proposed changes.    Note that this should not bring over the ""safe coadd clipping"" code, which is DM-2915.",5
"DM-2981","06/25/2015 15:15:17","polygon masking in CoaddPsf","We need to create polygon-based masks of the usable area of the focal plane, persist them with exposure, and include them in coaddition of PSFs and aperture corrections.    This includes HSC issues HSC-972, HSC-973, HSC-974, HSC-975, HSC-976.    At least some of this will be blocked by DM-833, which is the port issue for coaddition of aperture corrections.",8
"DM-2982","06/25/2015 16:38:56","Updating node status in qserv-admin to INACTIVE fails","In qserv-admin.py when attempting to update a node status from ACTIVE to INACTIVE the following error is produced:    {code}  > update node worker2 state=INACTIVE;  Traceback (most recent call last):  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 650, in <module>  main()  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 645, in main  parser.receiveCommands()  File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 163, in receiveCommands  self.parse(cmd[:pos])  File ""/usr/l  ocal/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 180, in parse      self._funcMap[t](tokens[1:])    File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 380, in _parseUpdate      self._parseUpdateNode(tokens[1:])    File ""/usr/local/home/vaikunth/src/qserv/bin/qserv-admin.py"", line 405, in _parseUpdateNode      self._impl.setNodeState(**options)    File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/admin/qservAdmin.py"", line 660, in setNodeState      self._kvI.set(nodeKey, state)    File ""/usr/local/home/vaikunth/src/qserv/lib/python/lsst/qserv/css/kvInterface.py"", line 415, in set      self._zk.set(k, v)    File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1170, in set      return self.set_async(path, value, version).get()    File ""/usr/local/home/vaikunth/qserv/Linux64/kazoo/2.0b1+1/lib/python/kazoo-2.0b1-py2.7.egg/kazoo/client.py"", line 1182, in set_async      raise TypeError(""value must be a byte string"")  {code}",1
"DM-2985","06/26/2015 16:37:58","Integrate javascript build with gradle","Integrate javascript build tools webpack with gradle.",2
"DM-2987","06/26/2015 17:03:12","Modify IpacTableParser to support extra wide table.","IpacTableParser fail to load IPAC table with extra wide headers and columns.  Replace the logic for reading headers and columns information so that it will support any file/size.",2
"DM-2989","06/26/2015 17:17:23","XY plot need to be able to handle multiple tables with the same name","XY plot was relying on a table request object to cache previously loaded tables.  This was done for performance reason.  However, table request is not reliable since the same request may be submitted multiple times.",2
"DM-2992","06/26/2015 22:03:30","Search processors to get image, table, or json from an external task","Implement three search processors, which use the External Task Launcher (DM-2991):    - to get a table (possibly in binary FITS format)  - to get an image  - to get JSON",8
"DM-2993","06/28/2015 01:10:08","Products must not depend on anaconda","{{setupRequired(anaconda)}} should be removed from webservcommon.table.    We want to keep the stack buildable with any python 2.7, and should not explicitly depend on anaconda.",1
"DM-2997","06/29/2015 11:45:23","Bump eups anaconda package to 2.2","By popular request. ",1
"DM-3029","07/01/2015 00:55:34","Set up Slack for evaluation","  Free account procured and tested by various volunteers; next step is to apply for non-profit status which gives us the first paid tier free to 100 users. ",1
"DM-3030","07/01/2015 00:57:24","Set up Discourse for evaluation.","  Server up on DO at community.lsst.org. Email needs fixing before volunteer users can be invited. ",1
"DM-3031","07/01/2015 07:05:37","Addressing File corruption in iRODS 4.1.x","We examine solutions for repairing corrupt files within an iRODS 4.1.x zone.",2
"DM-3037","07/02/2015 02:04:59","remove lsst/log wrapper from Qserv","lsst/log API looks stable now, so removing the wrapper would simplify the code.",1
"DM-3090","07/02/2015 18:38:08","Implement test suite for new class SqlTransaction","Some test that shows that transactions are properly committed/aborted would be nice to have.",1
"DM-3091","07/03/2015 10:39:02","Remove unused function populateState() ","Qserv doesn't seem to relaunch no more chunk query in case it fails (see DM-2643)    And this function is now unused:  {code:bash}  qserv@clrinfopc04:~/src/qserv (master)$ grep -r populateState core/  core/modules/qdisp/Executive.cc:void populateState(lsst::qserv::qdisp::ExecStatus& es,  {code}  ",0.5
"DM-3102","07/07/2015 10:58:52","Resolve segmentation fault in LoggingEvent destructor","There seems to be a possible race condition in log4cxx::spi::LoggingEvent::~LoggingEvent. I've had multiple segmentation faults in that function. In all cases, another thread was involved in writing. In at least 2 cases, the second thread was in XrdCl::LogOutFile::Write.  ",5
"DM-3104","07/08/2015 13:48:31","Add ""ORDER BY"" clause to lua SQL query on result table","If user query has ""ORDER BY"", then lua  can't just execute ""SELECT * FROM result"" because the order for such query is not guaranteed. To fix that, we need to add ""ORDER BY"" clause to the ""SELECT * FROM result"" query on the lua side.    Once we have the above, we might want to remove ""ORDER BY"" from the query class which runs a merge step on the czar (this has to be done in query analysis step).",8
"DM-3106","07/09/2015 08:13:11","Add slot for calibration flux","This is a port of [HSC-1005|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1005].",2
"DM-3108","07/09/2015 11:33:07","Use aperture flux for photometric calibration","This is a port of work performed on HSC but without a ticket. Relevant commits are:    * [05bef6|https://github.com/HyperSuprime-Cam/meas_astrom/commit/05bef629adc37e44ea8482aab88e2eb38a47e3a0]  * [4a6be5|https://github.com/HyperSuprime-Cam/meas_astrom/commit/4a6be51c53f61e70f151de7f29863cb723197a99]  * [69d35a|https://github.com/HyperSuprime-Cam/obs_subaru/commit/69d35a890234e37c1142ddbeff43e62fe36e6c45]  * [9c996d|https://github.com/HyperSuprime-Cam/obs_subaru/commit/9c996d75c423ce03fb54c4300d9c7561b5c1ea99]",1
"DM-3109","07/09/2015 11:41:15","Add support for accessing schema from QueryContext","When we are analyzing a query, sometimes there are situations where we need to know the schema of tables involved in a query. It will also be useful for checking if user is authorized to run query, and for queries like ""SHOW CREATE TABLE"". This story involves writing code that will provide access to schema.",3
"DM-3110","07/09/2015 14:44:37","qserv code cleanup","I made some random cleanup of the qserv code while playing with css v2. I want to push these changes to master, thus I am creating this story for this. It involves improvements to logging in UserQueryFactory and Facade (both are now per-module), removing unnecessary namespace qualifiers, and whitspace cleanup.",1
"DM-3126","07/13/2015 16:24:18","gcc 4.8 package does not create a symlink bin/cc","I created a new lsst package named ""gcc"" that contains Mario's gcc 4.8 package. I used it to build lsst_distrib on lsst-dev and it worked just fine. Unfortunately the package does not include bin/cc (which should be a symlink to bin/gcc), and this is wanted because the LSST build system uses cc to build C code.    The desired fix is to modify the installer to make a symlink bin/cc that points to bin/gcc.",2
"DM-3133","07/13/2015 22:53:59","add ""dax_"" prefix to data access related packages","As agreed at [Data Access Mtg 2015/07/13|https://confluence.lsstcorp.org/display/DM/Data+Access+Meeting+2015-07-13], add dax_ prefix towebserv, webservcommon, webserv_client, dbserv, imgserv, metaserv",1
"DM-3137","07/14/2015 10:29:00","Handle bad pixels in image stacker","We currently OR together all mask bits, but we need to be cleverer about how we handle pixels that are bad in some but not all inputs.    This is a port of work carried out on [HSC-152|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-152].",1
"DM-3139","07/14/2015 12:50:18","HSC backport: extra ""refColumn"" class attributes in multiband","This is a transfer for changesets for [HSC-1283|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1283].  ",0.5
"DM-3140","07/14/2015 14:01:53","add gcc to list of packages in lsstsw","Add gcc to the list of packages in etc/repos.yaml in lsstsw",1
"DM-3141","07/14/2015 15:39:02","Reduce verbosity of astrometry","The astrometry.net solver that runs by default in meas_astrom 10.1 is very verbose.  Here's an example running HSC data with an SDSS reference catalog:  {code}  $ processCcd.py /tigress/HSC/HSC --output /tigress/pprice/lsst --id visit=904020 ccd=49 --clobber-config  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/processCcd.py'  WARNING: Unable to use psfex: No module named extensions.psfex.psfexPsfDeterminer  hscAstrom is not setup; using LSST's meas_astrom instead  Cannot import lsst.meas.multifit: disabling CModel measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Cannot enable shapeHSM ('MEAS_EXTENSIONS_SHAPEHSM_DIR'): disabling HSM shape measurements  : Loading config overrride file '/home/pprice/LSST/obs/subaru/config/hsc/processCcd.py'  : input=/tigress/HSC/HSC  : calib=None  : output=/tigress/pprice/lsst  CameraMapper: Loading registry registry from /tigress/pprice/lsst/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /tigress/HSC/HSC/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2013-11-02', 'pointing': 671, 'visit': 904020, 'dateObs': '2013-11-02', 'filter': 'HSC-I', 'field': 'STRIPE82L', 'ccd': 49, 'expTime': 30.0}  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  processCcd.isr: Applying linearity corrections to Ccd 49  processCcd.isr.crosstalk: Applying crosstalk correction  afw.image.MaskedImage WARNING: Expected extension type not found: IMAGE  : Empty WCS extension, using FITS header  processCcd.isr: Set 0 BAD pixels to 647.04  processCcd.isr WARNING: There were 6192 unmasked NaNs  processCcd.isr WARNING: Cannot write thumbnail image; hsc.fitsthumb could not be imported.  processCcd.isr: Flattened sky level: 647.130493 +/- 12.733898  processCcd.isr: Measuring sky levels in 8x16 grids: 648.106765  processCcd.isr: Sky flatness in 8x16 grids - pp: 0.024087 rms: 0.006057  processCcd.calibrate: installInitialPsf fwhm=5.88235294312 pixels; size=15 pixels  processCcd.calibrate.repair: Identified 80 cosmic rays.  processCcd.calibrate.detection: Detected 303 positive sources to 5 sigma.  processCcd.calibrate.detection: Resubtracting the background after object detection  processCcd.calibrate.initialMeasurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  LoadReferenceObjects: read index files  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667372351 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.307471 vs. 0.320229 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.047945 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.measurePsf: Measuring PSF  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  /tigress/HSC/LSST/stack10_1/Linux64/anaconda/2.1.0-4-g35ca374/lib/python2.7/site-packages/numpy/core/_methods.py:71: RuntimeWarning: invalid value encountered in double_scalars    ret = ret.dtype.type(ret / rcount)  /home/pprice/LSST/meas/algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py:143: RuntimeWarning: invalid value encountered in less    update = dist < minDist  processCcd.calibrate.measurePsf: PSF star selector found 163 candidates  processCcd.calibrate.measurePsf: PSF determination using 114/163 stars.  processCcd.calibrate.repair: Identified 92 cosmic rays.  processCcd.calibrate: Fit and subtracted background  processCcd.calibrate.measurement: Measuring 303 sources (303 parents, 0 children)   processCcd.calibrate.astrometry: Applying distortion correction  processCcd.calibrate.astrometry: Solving astrometry  processCcd.calibrate.astrometry.solver: Number of selected sources for astrometry : 258  Solver:    Arcsec per pix range: 0.153025, 0.18516    Image size: 2054 x 4186    Quad size range: 205.4, 4662.78    Objs: 0, 50    Parity: 0, normal    Use_radec? yes, (320.343, 0.500178), radius 1 deg    Verify_pix: 1    Code tol: 0.01    Dist from quad bonus: yes    Distractor ratio: 0.25    Log tune-up threshold: inf    Log bail threshold: -230.259    Log stoplooking threshold: inf    Maxquads 0    Maxmatches 0    Set CRPIX? no    Tweak? no    Indexes: 3      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_0.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_1.fits      /tigress/HSC/astrometry_net_data/sdss-dr9-fink-v5b/sdss-dr9-fink-v5b_and_263_2.fits    Field: 258 stars  Quad scale range: [641.674, 2208.56] pixels  object 1 of 50: 0 quads tried, 0 matched.  object 2 of 50: 0 quads tried, 0 matched.  object 3 of 50: 0 quads tried, 0 matched.  object 4 of 50: 0 quads tried, 0 matched.  object 5 of 50: 0 quads tried, 0 matched.  object 6 of 50: 0 quads tried, 0 matched.  Got a new best match: logodds 787.099.    log-odds ratio 787.099 (inf), 178 match, 1 conflict, 75 distractors, 220 index.    RA,Dec = (320.343,0.500213), pixel scale 0.167612 arcsec/pix.    Hit/miss:   Hit/miss: ++-+++++-++++++++++++--++-+--+++++-+-+++++++++-+++++++-+++++-+++++++++++++-++++++-++++++-+++++++-+++  Pixel scale: 0.167612 arcsec/pix.  Parity: pos.  processCcd.calibrate.astrometry.solver: Got astrometric solution from Astrometry.net  LoadReferenceObjects: getting reference objects using center (1023.5, 2084.5) pix = Fk5Coord(320.3431396, 0.5002365, 2000.00) sky and radius 0.00194896 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3431396, 0.5002365, 2000.00) with radius 0.111667328272 deg  LoadReferenceObjects: found 495 objects  LoadReferenceObjects: trimmed 257 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 2 because it had less linear scatter than the next iter: 0.306732 vs. 0.320115 pixels  processCcd.calibrate.astrometry: 186 astrometric matches  processCcd.calibrate.astrometry: Refitting WCS  processCcd.calibrate.astrometry: Astrometric scatter: 0.048271 arcsec (with non-linear terms, 174 matches, 12 rejected)  processCcd.calibrate.photocal: Not applying color terms because config.applyColorTerms is False  processCcd.calibrate.photocal: Magnitude zero point: 30.685281 +/- 0.058711 from 173 stars  processCcd.calibrate: Photometric zero-point: 30.685281  processCcd.detection: Detected 1194 positive sources to 5 sigma.  processCcd.detection: Resubtracting the background after object detection  processCcd.deblend: Deblending 1194 sources  processCcd.deblend: Deblended: of 1194 sources, 143 were deblended, creating 358 children, total 1552 sources  processCcd.measurement: Measuring 1552 sources (1194 parents, 358 children)   processCcd WARNING: Persisting background models  processCcd: Matching icSource and Source catalogs to propagate flags.  processCcd: Matching src to reference catalogue  LoadReferenceObjects: getting reference objects using center (1023.5, 2087.5) pix = Fk5Coord(320.3429016, 0.5001781, 2000.00) sky and radius 0.00195667 rad  LoadReferenceObjects: search for objects at Fk5Coord(320.3429016, 0.5001781, 2000.00) with radius 0.112109149864 deg  LoadReferenceObjects: found 499 objects  LoadReferenceObjects: trimmed 261 out-of-bbox objects, leaving 238  processCcd.calibrate.astrometry.solver: Fit WCS: use iter 1 because it had less linear scatter than the next iter: 0.300624 vs. 0.300652 pixels  {code}    The verbosity of the astrometry module is out of proportion with the rest of the modules, which makes it difficult to follow the processing.    This is a pull request for fixes I have made.",1
"DM-3142","07/14/2015 16:38:01","Port HSC optimisations for reading astrometry.net catalog","Some astrometry.net catalogs used in production can be quite large, and currently all of the catalog must be read in order to determine bounds for each component.  This can make the loading of the catalog quite slow (e.g., 144 sec out of 177 sec to process an HSC image, using an SDSS DR9 catalog).  We have HSC code that caches the required information, making the catalog load much faster.  The code is from the following HSC issues:    * [HSC-1087: Make astrometry faster|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1087]  * [HSC-1143: Floating point exception in astrometry|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1143]  * [HSC-1178: Faster construction of Astrometry.net catalog|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178]  * [HSC-1179: Assertion failure in astrometry.net|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1178]    While there have been some changes to the LSST astrometry code that will mean we can't directly cherry-pick the HSC code, yet I think the main structure remains, so the approach can be copied without much effort.",3
"DM-3151","07/15/2015 14:29:35","CI validation of lsstsw's repos.yaml","Having some sort of automatic ""lint check"" of the repos.yaml file is desirable due to the length of time required to do a full up test of lsstsw.  It should be possible to cobble a sanity checker together that can be run from travis-ci.",1
"DM-3153","07/16/2015 13:20:51","meas_base still uses eups in tests","{{tests/centroid.py}} uses EUPS to determine the location of the data file used by the test. This needs to be fixed to use a location relative to the test file.",1
"DM-3154","07/16/2015 14:30:21","meas_astrom still using eups in tests","In DM-2636 we modified the tests to be skipped if EUPS is not available. I've had a closer look and all the ones I have glanced at seem to be easily fixable to run without EUPS. The tests seem to be using EUPS to locate the {{meas_astrom}} (effectively asking EUPS for the location of the test file), then a path to the astrometry.net test data within the {{tests/}} directory is located and then EUPS is asked to setup {{astrometry_net_data}} using that path. Since the table files are all empty this is the equivalent to simply assigning the {{ASTROMETRY_NET_DATA_DIR}} environment variable directly to the path in the tests sub-directory.    Making this change to one of the tests seems to work so I will change the rest.",2
"DM-3160","07/16/2015 17:17:11","Improve name and default value of MeasureApCorrConfig.refFluxAlg","The config name refFluxAlg should be refFluxField (since it is a flux field name prefix) and the default should be  base_CircularApertureFlux_5 instead of base_CircularApertureFlux_0 (thus giving a reasonable radius instead of one that is ridiculously too small).    I should have handled it on DM-436 but it slipped through.",1
"DM-3173","07/17/2015 14:28:14","In CalibrateTask if one disables psf determination then aperture correction will fail","In pipe_tasks CalibrateTask, by default aperture correction uses source flag ""calib_psfUsed"" to decide if a source is acceptable to use for measuring aperture correction. If PSF determination is disabled then this flag is never set and aperture correction will fail with a complaint that there are 0 sources.  ",1
"DM-3174","07/17/2015 15:57:16","CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the wrong schema","CalibrateTask instantiates measureApCorr, applyApCorr and photocal subtasks using the initial schema ""schema1"" instead of the final schema. Normally this would not matter since most of the fields are shared, but aperture correction wants aperture flux at a larger radius than the narrowest option, and schema1 may only provide the narrowest option.    In any case it is safer to instantiate those three subtasks using the final schema, since they are only ever run on the final schema. (Several other subtasks are run on both the initial and final schema, and should continue to be instantiated using schema1).",1
"DM-3175","07/17/2015 16:23:28","Build 2015_08 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"DM-3176","07/17/2015 16:27:45","Build and Test 2015_09 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"DM-3177","07/17/2015 16:27:56","Build and Test 2015_10 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"DM-3178","07/17/2015 16:28:05","Build and Test 2015_11 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"DM-3179","07/17/2015 16:28:14","Build and Test 2015_12 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",3
"DM-3181","07/17/2015 16:28:32","Build and Test 2016_02 Qserv Release","See https://confluence.lsstcorp.org/display/DM/Qserv+Release+Procedure for recipe.",1
"DM-3182","07/19/2015 11:32:19","Aperture correction not applied for some measurements","Aperture correction needs to be applied every time a measurement is run after it is first measured in CalibrateTask. As of DM-436 aperture correction is only being applied in CalibrateTask, which for example means the information is overwritten during the final measurement of ProcessImageTask.run.    This is probably best done by adding code to apply aperture correction to BaseMeasurementTask, so it is inherited by SingleFrameMeasurementTask and ForcedMeasurementTask.",5
"DM-3192","07/20/2015 13:19:15","Re-implement watcher based on new CSS implementation","Current watcher implementation (in {{admin/bin/watcher.py}}) is based on direct watching of zookeeper updates via kazoo. If we are to re-implement CSS based on mysql then watcher needs to be updated to support it. Mysql does not have watch mechanism, so it has to be done via polling or using some other mechanism if synchronous notifications are needed.",8
"DM-3194","07/21/2015 02:46:15","Fix cluster install procedure and improve docker support","Document how-to update cluster from Qserv release:    See  http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/cluster-deployment.html",0.5
"DM-3196","07/21/2015 23:25:57","makeWcs() chokes on decam images in 10.1","In 10.0, processCcdDecam.py could process decam images to completion (whether the WCS was read correctly is a different question). Now it fails on makeWcs() (see traceback below), and I suspect this change in behavior is related to DM-2883 and DM-2967.    Repository with both data and code to reproduce:  http://www.astro.washington.edu/users/yusra/reproduce/reproduceMakeWcsErr.tar.gz  (apologies for the size)    The attachment is a document describing the WCS representation in the images from the community pipeline, courtesy of Francisco Forster.    Please advise. This ticket captures any changes made to afw.     {code}  D-108-179-166-118:decam yusra$ processCcdDecam.py newTestRepo/ --id visit=0232847 ccdnum=10 --config calibrate.doPhotoCal=False calibrate.doAstrometry=False calibrate.measurePsf.starSelector.name=""secondMoment"" doWriteCalibrateMatches=False --clobber-config  : Loading config overrride file '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/processCcdDecam.py'  : Config override file does not exist: '/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/config/decam/processCcdDecam.py'  : input=/Users/yusra/decam/newTestRepo  : calib=None  : output=None  CameraMapper: Loading registry registry from /Users/yusra/decam/newTestRepo/registry.sqlite3  processCcdDecam: Processing {'visit': 232847, 'ccdnum': 10}  makeWcs WARNING: Stripping PVi_j keys from projection RA---TPV/DEC--TPV  processCcdDecam FATAL: Failed on dataId={'visit': 232847, 'ccdnum': 10}:     File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs()      Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0}  lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value'    Traceback (most recent call last):    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/pipe_base/10.1-3-g18c2ba7+49/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/processCcdDecam.py"", line 77, in run      mi = exp.getMaskedImage()    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 41, in __getattribute__      subject = oga(self, '__subject__')    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/readProxy.py"", line 136, in __subject__      set_cache(self, get_callback(self)())    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 242, in <lambda>      innerCallback(), dataId)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/daf_persistence/10.1-1-g6edbc00+28/python/lsst/daf/persistence/butler.py"", line 236, in <lambda>      location, dataId)    File ""/Users/yusra/lsst_devel/LSST/repos/obs_decam_ya/python/lsst/obs/decam/decamMapper.py"", line 118, in bypass_instcal      wcs         = afwImage.makeWcs(md)    File ""/Users/yusra/lsst_devel/LSST/DMS5/DarwinX86/afw/10.1-26-g9124caf+1/python/lsst/afw/image/imageLib.py"", line 8706, in makeWcs      return _imageLib.makeWcs(*args)  RuntimeError:     File ""src/image/Wcs.cc"", line 130, in void lsst::afw::image::Wcs::_initWcs()      Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value {0}  lsst::pex::exceptions::RuntimeError: 'Failed to setup wcs structure with wcsset. Status 5: Invalid parameter value'  {code}    ",2
"DM-3199","07/22/2015 13:38:05","Standardize Qserv install procedure: step 1 build docker container for master/worker instance and development version ","- shmux could be used for parallel ssh (remove Qserv builtin one)  - look at ""serf and consul"" (See Confluence pages)  - improve doc: http://www.slac.stanford.edu/exp/lsst/qserv/2015_07/HOW-TO/index.html  - run multiple instances/versions of Qserv using different run dir/ports and the same data",8
"DM-3204","07/23/2015 16:02:53","W16 Data Access and Db Release Documentation","Write Release documentation covering Data Access and Database work.",5
"DM-3209","07/23/2015 17:13:03","Add debugging for astrometry.net solver","To be able to debug astrometric matching, it helps to be able to visualise the source positions, the distorted source positions, and the reference positions.  This is a pull request to add these.",0.5
"DM-3214","07/27/2015 10:39:34","ChebyshevBoundedField should use _ not . as field separators for persistence","ChebyshevBoundedField uses ""."" instead of ""\_"" as field separators in its afw table persistence. This is the old way of doing things, and unfortunately causes errors when reading in older versions of tables, becaus afw converts ""."" to ""_"" in that situation.    This shows up as a unit test failure in DM-2981 (brought over from HSC) when an older version table is read in.    It is an open question whether to fix this as part of DM-2981 (which conveniently has a test that shows the problem, though not intentionally so) or separately, in which case a new test is wanted. In the former case I'm happy to do the work so I can finish DM-2981.    Many thanks to Jim Bosch for diagnosing the problem.",1
"DM-3218","07/27/2015 15:43:47","unable to create public images","Errors are returned when attempting to upload an image marked as public.",1
"DM-3223","07/27/2015 16:23:26","Improve czar-worker communication debugging","Add features to make it easier to debug communication problems. Particularly, record the source of a message, and remove extraneous messages.",2
"DM-3227","07/27/2015 17:15:58","openstack API endpoint is broken","Similar to what was observed in DM-3226, the referral endspoint returned by   {code:java}  https://nebulous.ncsa.illinois.edu:5000  {code}  are not FQDNs.  This fundamentally breaks any attempt to use the API one step past authenticating with keystone.    This is an example HTTP response:    {code:java}  HTTP/1.1 200 OK  Date: Mon, 27 Jul 2015 23:11:02 GMT  Server: Apache/2.4.10 (Ubuntu)  Vary: X-Auth-Token  X-Distribution: Ubuntu  x-openstack-request-id: req-ac7bb613-86ef-43ab-a663-75c2ed3fb124  Content-Length: 1656  Content-Type: application/json    {""access"": {""token"": {""issued_at"": ""2015-07-27T23:11:02.342216"", ""expires"": ""2015-07-28T00:11:02Z"", ""id"": ""99b843d4baf94569a0d34ca4fecb470c"", ""tenant"": {""description"": null, ""enabled"": true, ""id"": ""d1f16653856540d386224fb057b5b00c"", ""name"": ""LSST""}, ""audit_ids"": [""fAP8851vTQi1n5pYmNoIjw""]}, ""serviceCatalog"": [{""endpoints"": [{""adminURL"": ""http://nebula:9292"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9292"", ""id"": ""49365a8e8fe743af9d517e84a98e3ee9"", ""publicURL"": ""http://nebula:9292""}], ""endpoints_links"": [], ""type"": ""image"", ""name"": ""glance""}, {""endpoints"": [{""adminURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c"", ""id"": ""c1e31df3656042ef9c5502efd7d574f2"", ""publicURL"": ""http://nebula:8774/v2/d1f16653856540d386224fb057b5b00c""}], ""endpoints_links"": [], ""type"": ""compute"", ""name"": ""nova""}, {""endpoints"": [{""adminURL"": ""http://nebula:9696"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:9696"", ""id"": ""266c9dc8e0344f8fa3f078652e868443"", ""publicURL"": ""http://nebula:9696""}], ""endpoints_links"": [], ""type"": ""network"", ""name"": ""neutron""}, {""endpoints"": [{""adminURL"": ""http://nebula:35357/v2.0"", ""region"": ""RegionOne"", ""internalURL"": ""http://nebula:5000/v2.0"", ""id"": ""5d474008bcee4f44800546e3f3302404"", ""publicURL"": ""http://nebula:5000/v2.0""}], ""endpoints_links"": [], ""type"": ""identity"", ""name"": ""keystone""}], ""user"": {""username"": ""jhoblitt"", ""roles_links"": [], ""id"": ""6ea0c8e153b04ae29572c5fd877b6ac3"", ""roles"": [{""name"": ""user""}], ""name"": ""jhoblitt""}, ""metadata"": {""is_admin"": 0, ""roles"": [""142761bd922e453294e9b7086a227cbc""]}}}    {code}  ",1
"DM-3228","07/27/2015 18:59:01","evaluate NCSA OpenStack against SQRE requirements and provide feedback - part 1","See also https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LDMDG&title=NCSA+Nebula+OpenStack+Issues",2
"DM-3237","07/28/2015 15:04:04","Fix problems with no-result queries on multi-node setup","For queries like:        select * from Object where id = <non existent id>    qserv can't map it to any chunk, and it ends up executing      SELECT *     FROM qservTest_case01_qserv.Object_1234567890 AS QST_1_     WHERE objectId=<non existent id>    the chunk 1234567890 is a special chunk and it exists on all nodes.    And that fails with:    (build/qdisp/QueryResource.cc:61) - Error provisioning, msg=Unable to  write  file; multiple files exist. code=2 ",1
"DM-3241","07/28/2015 18:21:21","Create images for the mask bits at server side","LSST FITS images will have a extension that indicate the mask bits. In order to overlay the masks on the primary image, we need to turn the mask bits into a set of images. This task is to take the requested bits and FITS as input, output a set of images for each requested bit. Each bit will have different color. ",20
"DM-3243","07/28/2015 18:29:00","Include polygon bounds in CoaddPsf logic","This is a port of [HSC-974|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-974]. Original description:    The {{CoaddPsf}} class should use the polygon bounding areas that were added to {{Exposure}} and {{ExposureRecord}} in DM-2981 (was: HSC-973) when determining which PSF images to coadd.",1
"DM-3245","07/28/2015 22:55:27","Add support for SUBMIT query parsing to czar","We need to be able to pass information from user about query type (sync/async). This may require tweaking the parser.  ",2
"DM-3249","07/28/2015 23:06:31","Revisit and document user-facing aspects of async queries","Outline all aspects of async queries that are affecting users, discuss with the DM team, and document. This includes things like:   * managing async queries (checking status, terminating)   * retrieving results from async queries   * managing query results (purging policies etc)   * probably more, need to think about it...",8
"DM-3253","07/29/2015 00:10:14","Unify KVInterface python and c++ interfaces","Swig the C++ mysql-based KvInterface implementation.   ",8
"DM-3257","07/29/2015 07:53:01","Port flux.scaled from HSC","[HSC-1295|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1295] introduces {{flux.scaled}}, which measures the flux within a circular aperture that is set from the size of the PSF, scaled by some factor.  Stephen Gwyn recommends using this as our fiducial calibration flux.",2
"DM-3258","07/29/2015 08:27:36","CoaddPsf.getAveragePosition() is not a valid position","This is a port of [HSC-1138|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1138] to LSST. That is an aggregate of two related minor fixes:    * {{CoaddInputRecorder}} should default to {{saveVisitGoodPix=True}} so that average positions in the {{CoaddPsf}} can be properly weighted;  * {{computeAveragePosition}} and {{doComputeKernelImage}} should be consistent about the data included when determining whether a source is off image.",1
"DM-3259","07/29/2015 09:38:37","Define polygon bounds for CCDs based on vignetted regions","This is a port of [HSC-976|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-976] to LSST. The original issue description was:    We should set the polygon bounds (added in DM-2981 [was HSC-973]) for HSC CCD exposures to cover the non-vignetted regions. This should probably be done in ISR or some other camera-specific location.    Note that, contrary to the description in DM-2981, this functionality was not included there.",1
"DM-3347","07/30/2015 10:26:53","assertWcsNearlyEqualOverBBox and friends is too hard to use as a free function","assertWcsNearlyEqualOverBBox and similar functions elsewhere in afw were written to be methods of lsst.utils.tests.TestCase, so their first argument is a testCase. This is fine for use in unit tests, but a hassle to use as free functions because the user must provide a testCase argument (though it need only be a trivial class with a fail(self, msgStr) method). Worse, that minimal requirement is not documented, so technically providing a simple mock test case is unsafe.    I have two proposals:  - Document the fact that testCase need only support fail(self, msgStr). This makes it clear how to safely use these functions as free functions.  - Allow testCase to be None, in which case RuntimeError is raised. That makes these functions even easier to use as free functions.  ",1
"DM-3349","07/30/2015 14:57:35","Add test case for ExposureRecord::contains","In DM-3243 we ported from HSC the ability to take account of the associated {{validPolygon}} when checking whether a point falls within an {{Exposure}}. This functionality was not accompanied by an adequate unit test.",2
"DM-3355","07/31/2015 12:47:47","Support the FITS cube reader","RSA needs to be able to read in the FITS cube generated by Herschel project. We need to guide the effort so the code is generic enough for non-Herschel data.",1
"DM-3356","07/31/2015 14:34:00","Fix Firefly build script so it'll work with latest version of gradle","Firefly build was failing when using gradle version 2.5.  Minor changes to the dependencies declaration fixed it.",1
"DM-3358","07/31/2015 16:09:53","Add mysql-based test to multi-node integration test","At the moment multi-node integration test runs only on multi-node using Qserv, it does not run on plain mysql, and thus we can't validate results. The story involves tweaking qserv_testdata such that we can run mysql test on the czar, and compare results from mysql and qserv.",5
"DM-3377","08/03/2015 07:01:01","Initial issue investigation for the nebula openstack","    The nebula openstack system at NSCA first became available ~Fri Jul 24 and  the week of Jul 27 -- 31 was spent testing and debugging issues that the                 LSST team identified within, for example, DM-3225, DM-3219, DM-3227 and others.  ",20
"DM-3387","08/03/2015 13:22:38","Make use of good pixel count when building CoaddPsfs","When building a CoaddPsf we have the ability to take account of the number of pixels contributed by the inputs (see http://ls.st/paj and DM-3258). However, the {{CoaddPsf}} constructor fails to use this information. It should copy this field when copying the provided {{ExposureCatalog}}, so that {{computeAveragePosition}} can use it.",1
"DM-3390","08/03/2015 16:58:35","Re-generate data for large scale tests at in2p3","Sources were incorrectly duplicated, need to be redone",3
"DM-3391","08/03/2015 17:13:54","Refactor Zscale.java class ","In early this year, the decision all data types would be converted to float in FitsRead.  Thus,the bitpixel is not relevant.  In Zscale, it still uses bitpixel to test the data type.  It should be refactored in the same manner as FitsRead etc. ",2
"DM-3398","08/04/2015 16:44:21","Fix problem with default_engine","Fix the problem:    {quote}  08/04/2015 05:39:47 werkzeug INFO: 141.142.237.30 - - [04/Aug/2015 17:39:47] ""GET /meta/v0/ HTTP/1.1"" 200 -  08/04/2015 05:39:49 __main__ ERROR: Exception on /meta/v0/db [GET]  Traceback (most recent call last):    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1817, in wsgi_app      response = self.full_dispatch_request()    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1477, in full_dispatch_request      rv = self.handle_user_exception(e)    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1381, in handle_user_exception      reraise(exc_type, exc_value, tb)    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1475, in full_dispatch_request      rv = self.dispatch_request()    File ""/home/becla/stack/Linux64/flask/0.10.1+8/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1461, in dispatch_request      return self.view_functions[rule.endpoint](**req.view_args)    File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 59, in getDb      return _resultsOf(text(query), scalar=True)    File ""/nfs/home/becla/stack/repos/dax_metaserv/python/lsst/dax/metaserv/metaREST_v0.py"", line 122, in _resultsOf      engine = current_app.config[""default_engine""]  KeyError: 'default_engine'    {quote}",1
"DM-3400","08/05/2015 09:44:21","Eliminate circular aliases in slot centroid definition","[~smonkewitz] has discovered that our schema aliases for even the default configuration of measurement algorithms involve cycles, because the slot centroid algorithm contains a reference to its own flag.  Fixing this should just involve an extra check in {{SafeCentroidExtractor}}.",1
"DM-3404","08/05/2015 13:22:49","Port HSC updates to ingestImages.py","ingestImages.py provides a camera-agnostic manner of creating a data repository (including a registry).  The HSC fork contains multiple improvements not present on the LSST side.  We need these in order to ingest the HSC data.",2
"DM-3411","08/05/2015 18:19:32","workspace functions specification document","The first version of the document is here https://confluence.lsstcorp.org/pages/viewpage.action?pageId=41783931",20
"DM-3419","08/07/2015 13:07:45","obs_decam unit test for reading data ","The unit test wasn't working before and I edited the unit test of reading raw data. This got included with DM-3462.   This unit test needs testdata_decam to be setup.      The test fails with the stack b1597 at makeWcs (DM-3196).   The afw branch u/yusra/DM-3196 is a temporary fix before DM-3196 is resolved.      ",2
"DM-3437","08/10/2015 18:28:09","Add column names metadata to db query results","Per discussion at data access meeting Aug 10, it'd be good to send column names with the query results.",2
"DM-3440","08/11/2015 09:30:52","add meas_extensions_photometryKron to lsstsw, lsst_distrib","meas_extensions_photometryKron should be added to the CI system, since we are trying to keep it updated.    This is blocked by DM-2429 because that includes a fix for a unit test (which the CI system would have caught).",1
"DM-3442","08/11/2015 16:37:10","Processing y-band HSC data fails in loading reference sources","{code}  processCcd.py /lsst3/HSC/data/ --output /raid/price/test --id visit=904400 ccd=50  [...]  processCcd.calibrate.astrometry.solver.loadAN: Loading reference objects using center (1023.5, 2091) pix = Fk5Coord(319.8934727, -0.0006943, 2000.00) sky and radius 0.111920792477 deg  processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}: Could not find flux field(s) y_camFlux, y_flux  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processCcd.py"", line 85, in run      result = self.process(sensorRef, postIsrExposure)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/processImage.py"", line 160, in process      calib = self.calibrate.run(inputExposure, idFactory=idFactory)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/pipe_tasks/10.1-28-gf9582e4+2/python/lsst/pipe/tasks/calibrate.py"", line 457, in run      astromRet = self.astrometry.run(exposure, sources1)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 177, in run      results = self.astrometry(sourceCat=sourceCat, exposure=exposure, bbox=bbox)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetAstrometry.py"", line 292, in astrometry      astrom = self.solver.determineWcs(sourceCat=sourceCat, exposure=exposure, bbox=bbox)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 409, in determineWcs      return self.determineWcs2(sourceCat=sourceCat, **margs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 437, in determineWcs2      astrom = self.useKnownWcs(sourceCat, wcs=wcs, **kw)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/anetBasicAstrometry.py"", line 308, in useKnownWcs      calib = None,    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 173, in loadPixelBox      loadRes = self.loadSkyCircle(ctrCoord, maxRadius, filterName)    File ""/home/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+15/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/lsstsw/stack/Linux64/meas_astrom/10.1-19-g6e01b25+5/python/lsst/meas/astrom/loadAstrometryNetObjects.py"", line 141, in loadSkyCircle      fluxField = getRefFluxField(schema=refCat.schema, filterName=filterName)    File ""/home/lsstsw/stack/Linux64/meas_algorithms/10.1-15-g0d3ecf6/python/lsst/meas/algorithms/loadReferenceObjects.py"", line 40, in getRefFluxField      raise RuntimeError(""Could not find flux field(s) %s"" % ("", "".join(fluxFieldList)))  RuntimeError: Could not find flux field(s) y_camFlux, y_flux  {code}    We should be able to fix this by setting config parameters (e.g., {{calibrate.astrometry.solver.defaultFilter}} or {{calibrate.astrometry.solver.filterMap}}), but how do we keep that synched with the choice of reference catalog?  And once we get past astrometry, we also have the same problem in photocal.",2
"DM-3450","08/12/2015 22:07:19","Tweaks to configurations discovered during S15 tests","Apply tweaks we found useful when running large scale tests. This includes:  # etc/my.cnf: change max_connections to 512  # add"":  {quote}export XRD_REQUESTTIMEOUT=64000  export XRD_STREAMTIMEOUT=64000  export XRD_DATASERVERTTL=64000  export XRD_TIMEOUTRESOLUTION=64000{quote}  to init.d/qserv-czar  # add :  {quote}ulimit -c unlimited{quote}  to all startup scripts in init.d. This will make sure core file is always dumped when we have problems.",1
"DM-3451","08/12/2015 22:09:19","Resolve problem with running many simultaneous queries","When we run with 110 simultaneous queries, czar fails with ""uncaught exception""",2
"DM-3452","08/13/2015 01:01:32","Integrate pipelines with MySQL and Qserv","Load data produced by pipelines into MySQL (on lsst10), and Qserv",5
"DM-3453","08/13/2015 10:22:10","AstrometryTask.run return not consistent with ANetAstrometryTask","ANetAstrometryTask.run returns matchMetadata but AstrometryTask.run returns matchMeta. The two must agree. It turns out that matchMeta is more widely used, so I'll standardize on that.",1
"DM-3454","08/13/2015 11:56:09","Odd error message in getDistortedWcs","lsst.afw.image.utils.getDistortedWcs complains as follows if the provided exposure has no WCS:  ""exposure must have a WCS to use as an initial guess"". It should not say anything about an initial guess. This is presumably a leftover from when the code was part of meas_astrom.    Thanks to [~price] for pointing this out.",0
"DM-3455","08/13/2015 12:26:56","ProcessImageTask.matchSources fails if using ANetAstrometryTask","ProcessImageTask.matchSources fails when using ANetAstrometryTask with the following error:  {code}  processCcd.calibrate.astrometry: Applying distortion correction  processCcd FATAL: Failed on dataId={'taiObs': '2013-11-03', 'pointing': 672, 'visit': 904400, 'dateObs': '2013-11-03', 'filter': 'HSC-Y', 'field': 'STRIPE82L', 'ccd': 50, 'expTime': 30.0}:     File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem<T> lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string<char>]      Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.'    Traceback (most recent call last):    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/cmdLineTask.py"", line 320, in __call__      result = task.run(dataRef, **kwargs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processCcd.py"", line 85, in run      result = self.process(sensorRef, postIsrExposure)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 219, in process      srcMatches, srcMatchMeta = self.matchSources(calExposure, sources)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_tasks/tickets.DM-3453-g086c9ddd0a/python/lsst/pipe/tasks/processImage.py"", line 250, in matchSources      astromRet = astrometry.loadAndMatch(exposure=exposure, sourceCat=sources)    File ""/ssd/rowen/lsstsw/stack/Linux64/pipe_base/10.1-4-g6ba0cc7+3/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 321, in loadAndMatch      with self.distortionContext(sourceCat=sourceCat, exposure=exposure) as bbox:    File ""/ssd/rowen/lsstsw/anaconda/lib/python2.7/contextlib.py"", line 17, in __enter__      return self.gen.next()    File ""/ssd/rowen/lsstsw/stack/Linux64/meas_astrom/tickets.DM-3453-gbb2ad1f49c/python/lsst/meas/astrom/anetAstrometry.py"", line 295, in distortionContext      sourceCat.table.defineCentroid(self.distortedName)    File ""/ssd/rowen/lsstsw/stack/Linux64/afw/10.1-37-gaedf466/python/lsst/afw/table/tableLib.py"", line 8887, in defineCentroid      return _tableLib.SourceTable_defineCentroid(self, *args)  NotFoundError:     File ""src/table/Schema.cc"", line 239, in lsst::afw::table::SchemaItem<T> lsst::afw::table::detail::SchemaImpl::find(const string&) const [with T = double; std::string = std::basic_string<char>]      Field or subfield withname 'astrom_distorted_x' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield withname 'astrom_distorted_x' not found with type 'D'.'  {code}  This is probably a result of DM-2939. The basic problem is that the distortion context in ANetAstrometryTask should not be run at that point in processing. [~price] suggests that a simple clean fix is to make the distortion context a no-op if the WCS already contains distortion, if that works. This is what I will try first.",1
"DM-3456","08/13/2015 12:52:24","Fix problems with talking from webserv to qserv","Flask or sqlalchemy which are part of webserv are producing some extra queries that are confusing qserv. So basically, at the moment even the simplest query run via webserv that is directed to qserv fails.",2
"DM-3459","08/13/2015 18:23:07","make forced and SFM interfaces more consistent","From [~rowen]:  {quote}  SimpleMeasurementTask.run and ForcedMeasurementTask.run now both take a source catalog, but the two use the opposite order for the first two arguments (one has the catalog first, the other has the exposure first)  {quote}",1
"DM-3460","08/14/2015 11:38:24","applyApCorr mis-handles missing data","In ApplyApCorrTask.run the following lines do not behave as expected because get returns None if the data is missing, rather than raising an exception:  {code}              try:                  apCorrModel = apCorrMap.get(apCorrInfo.fluxName)                  apCorrSigmaModel = apCorrMap.get(apCorrInfo.fluxSigmaName)              except Exception:  {code}",1
"DM-3462","08/14/2015 14:13:17","Make obs_decam handle raw data ","The current obs_decam expects instrument calibrated data from the community pipeline, i.e. it  requires matching instcal (Instrument Calibrated), dqmask (the associated mask file), and wtmap (weight map) data from the same visit.  This issue is to add functionality so that raw DECam images can be ingested into registry and retrieved by the data butler.     Practically, this will create new or expand existing sub-classes of CameraMapper and IngestTask.        A brief summary of changes:  - The unit test getRaw.py is updated and should pass, with DM-3196     - Working testdata_decam for the unit test is currently at lsst-dev /lsst8/testdata_decam and https://uofi.box.com/testdata-decam  - DecamInstcalMapper is renamed to DecamMapper, to reflect that Butler can also get ""raw"" now besides ""instcal"". Please update _mapper in your data repositories.  - To create a registry for raw data, run  {code:java}     ingestImagesDecam.py /path/to/repo --mode=link --filetype=""raw"" /path/*.fits.fz  {code}  - The default filetype is ""instcal"" for ingestImagesDecam.py, so previous use for instcal stays.  ",13
"DM-3463","08/14/2015 15:09:17","psfex lapack symbols may collide with built in lapack","On my Mac meas_extensions_psfex fails to build due to the numpy config test failing. ""import numpy"" fails with:  {code}  dlopen(/Users/rowen/LSST/lsstsw/anaconda/lib/python2.7/site-packages/numpy/linalg/lapack_lite.so, 2): can't resolve symbol __NSConcreteStackBlock in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib because dependent dylib #1 could not be loaded in /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib  {code}  Our best guess (see discussion in Data Management 2015-08-14 at approx. 1:57 PM Pacific time) is that the special lapack functions in psfex are colliding with the lapack that anaconda uses.    In case it helps I see this on OS X 10.9.5. I do not see it on lsst-dev.",2
"DM-3468","08/14/2015 20:49:19","drawing text to ds9 fails if size or the font family is set","Commands like  {code}  ds9.dot('xxxx', 100, 100, size=3)  ds9.dot('xxxx', 100, 120, fontFamily=""times"")  {code}  silently fail.  The problem is that commands like  {code}  xpaset -p ds9 regions command '{text 100 100 # text=xxxx color=red font=""times 12""}'  {code}  fail; you need to say {{font=times 12 normal}}",1
"DM-3470","08/15/2015 16:21:12","Install/deploy SUI web application at NCSA","For summer 15 release,  we will deploya SUI web app on NCSA accessible to DM team.    - work with NCSA to have a server setup  - install necessary software packages  - install SUI software  - deploy the system and test ",5
"DM-3480","08/17/2015 17:49:43","Design SQL APIs for async queries","Need SQL API for:   * submitting async query, note that we should be able to specify where the results are going / what is the format of the results   * retrieving status of async query   * retrieving results of async query   * retrieving partial results of async query while it is running  ",2
"DM-3481","08/18/2015 01:15:48","adapt sandbox-jenkins-demo to changes in jfryman/nginx 0.2.7","Changes in the way  jfryman/nginx 0.2.7 handles tls cert files since 0.2.6 have run awful of selinux permissions issues.",2
"DM-3483","08/18/2015 10:03:56","Calibration transformation should not fail on negative flux","Before database ingest, measured source fluxes are converted to magnitudes as per DM-2305. The default behaviour of {{afw::image::Calib}} is to throw when a negative flux is encountered, which derails the whole transformation procedure. Better is to return a NaN.",0.5
"DM-3488","08/18/2015 16:12:47","Debug problem with large results set","Query returning 2 billion rows causes problems for czar - czar is using nearly 16 GB or memory. Need to understand why RAM usage in czar is correlated with result size.",1
"DM-3490","08/18/2015 17:20:21","Quick-and-dirty n-way spatial matching","This issue will add limited N-way spatial matching of multiple catalogs with identical schemas, sufficient for measuring FY15 KPMs.  It will be a simple wrapper on our existing 2-way matching code in afw, and will not be intended for long term use (as it won't be an efficient algorithm or an ideal interrface).",2
"DM-3491","08/18/2015 17:57:47","Update ip_diffim to use the new NO_DATA flag instead of EDGE","In ip_diffImm some uses of EDGE were converted to or supplemented with NO_DATA, but others were not. This ticket handles the missing instances.",1
"DM-3492","08/18/2015 22:20:19","Correct for distortion in matchOptimisticB astrometry matcher","matchOptimisticB does not correct for distortion, although an estimate of the distortion is available.  We suspect that doing the matching on the celestial sphere might be ideal, but matching on a tangent plane has worked for HSC.",5
"DM-3493","08/18/2015 22:36:13","Fix crosstalk following ds9 interface changes","crosstalk.py in obs_subaru uses ds9 without actually displaying anything, which causes trouble if display_ds9 is not setup.",1
"DM-3506","08/19/2015 12:46:55","W16 Support Dynamic CSS Metadata in Czar","Czar needs to support dynamic CSS metadata. This epic involves reworking Facade and related code so that czar can have up to date CSS metadata (per query) instead of relying on static snapshots",40
"DM-3522","08/21/2015 13:07:41","Releasing un-acquired resources bug","Running a mix of queries: 75 low volume and 10 high volume that include near neighbor failed at some point with    {quote}  terminate called after throwing an instance of 'lsst::qserv::Bug'    what():  ChunkResource ChunkEntry::release: Error releasing un-acquired resource  {quote}    Stack trace:    {quote}  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  Missing separate debuginfos, use: debuginfo-install expat-2.1.0-8.el7.x86_64 glibc-2.17-78.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.12.2-14.el7.x86_64 libcom_err-1.42.9-7.el7.x86_64 libgcc-4.8.3-9.el7.x86_64 libicu-50.1.2-11.el7.x86_64 libselinux-2.2.2-6.el7.x86_64 libstdc++-4.8.3-9.el7.x86_64 nss-softokn-freebl-3.16.2.3-9.el7.x86_64 openssl-libs-1.0.1e-42.el7_1.9.x86_64 pcre-8.32-14.el7.x86_64 xz-libs-5.1.2-9alpha.el7.x86_64 zlib-1.2.7-13.el7.x86_64  (gdb) where  #0  0x00007fb6b0cce5e9 in raise () from /lib64/libc.so.6  #1  0x00007fb6b0ccfcf8 in abort () from /lib64/libc.so.6  #2  0x00007fb6b15d29b5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6  #3  0x00007fb6b15d0926 in ?? () from /lib64/libstdc++.so.6  #4  0x00007fb6b15cf8e9 in ?? () from /lib64/libstdc++.so.6  #5  0x00007fb6b15d0554 in __gxx_personality_v0 () from /lib64/libstdc++.so.6  #6  0x00007fb6b1069913 in ?? () from /lib64/libgcc_s.so.1  #7  0x00007fb6b1069e47 in _Unwind_Resume () from /lib64/libgcc_s.so.1  #8  0x00007fb6ab554247 in lsst::qserv::wdb::ChunkResourceMgr::Impl::release (this=0x21d1cc0, i=...) at build/wdb/ChunkResource.cc:398  #9  0x00007fb6ab552696 in lsst::qserv::wdb::ChunkResource::~ChunkResource (this=0x7fb68a5f9b70, __in_chrg=<optimized out>)      at build/wdb/ChunkResource.cc:131  #10 0x00007fb6ab560f0f in lsst::qserv::wdb::QueryAction::Impl::_dispatchChannel (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:392  #11 0x00007fb6ab55f5ab in lsst::qserv::wdb::QueryAction::Impl::act (this=0x7fb65848c4d0) at build/wdb/QueryAction.cc:187  #12 0x00007fb6ab562084 in lsst::qserv::wdb::QueryAction::operator() (this=0x7fb658050548) at build/wdb/QueryAction.cc:450  #13 0x00007fb6ab544f46 in lsst::qserv::wcontrol::ForemanImpl::Runner::operator() (this=0x7fb67400fa20) at build/wcontrol/Foreman.cc:302  #14 0x00007fb6ab551cf0 in std::_Bind_simple<lsst::qserv::wcontrol::ForemanImpl::Runner ()>::_M_invoke<>(std::_Index_tuple<>) (      this=0x7fb67400fa20) at /usr/include/c++/4.8.2/functional:1732  #15 0x00007fb6ab551a8b in std::_Bind_simple<lsst::qserv::wcontrol::ForemanImpl::Runner ()>::operator()() (this=0x7fb67400fa20)      at /usr/include/c++/4.8.2/functional:1720  {quote}    Tail of log file from xrootd log:    {quote}  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:139) - _getNextTasks(1)>->->  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:151) - Returning 1 to launch  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG GroupSched (build/wsched/GroupScheduler.cc:154) - _getNextTasks <<<<<  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:172) - _getNextTasks(29)>->->  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:199) - ChunkDisk busyness: yes  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:171) - ChunkDisk getNext: current= (scan=10436,  cached=8360,8259,) candidate=10301  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ChunkDisk.cc:184) - ChunkDisk denying task  0821 19:08:58.530 [0x7fb68a6fb700] DEBUG ScanSched (build/wsched/ScanScheduler.cc:196) - _getNextTasks <<<<<  0821 19:08:58.531 [0x7fb68a6fb700] INFO  root (build/xrdsvc/SsiSession.cc:120) - Enqueued TaskMsg for Resource(/chk/LSST/2732) in 0.001016 seconds  0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:175) - Registered runner 0x7fb66c141ab0  0821 19:08:58.531 [0x7fb6895f8700] DEBUG Foreman (build/wcontrol/Foreman.cc:209) - Started task Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0   0821 19:08:58.531 [0x7fb6895f8700] INFO  Foreman (build/wcontrol/Foreman.cc:296) - Runner running Task: msg: session=434445 chunk=2732 db=LSST entry time= frag: q=SELECT o.deepSourceId,o.ra,o.decl,s.coord_ra,s.coord_decl,s.parent FROM LSST.Object_2732 AS o,LSST.Source_2732 AS s WHERE scisql_s2PtInBox(o.ra,o.decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND scisql_s2PtInBox(s.coord_ra,s.coord_decl,48.482655,-54.274507,48.555903,-54.196952)=1 AND o.deepSourceId=s.objectId, sc= rt=r_4344458c9456ede5cbe0b5f42e1a1571d5dd73_2732_0   0821 19:08:58.531 [0x7fb6895f8700] INFO  Foreman (build/wdb/QueryAction.cc:177) - Exec in flight for Db = q_fd51ad249f62fb765e173d7b3cae5d94  0821 19:08:58.531 [0x7fb6895f8700] WARN  Foreman (build/wdb/QueryAction.cc:109) - QueryAction overriding dbName with LSST  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=106  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=210  0821 19:08:58.718 [0x7fb6a0d8e700] INFO  root (build/wdb/QueryAction.cc:261) - &&& _fillRows size=316    ...(thousands of _fillRows lines)    terminate called after throwing an instance of 'lsst::qserv::Bug'    what():  ChunkResource ChunkEntry::release: Error releasing un-acquired resource  {quote}  ",3
"DM-3544","08/25/2015 08:57:38","Cleanup of initial astrometry improvements","The astrometry improvements are working, but some cleanup would be good to remove dependencies on A.net and to provide default reference catalog loaders.",20
"DM-3546","08/25/2015 11:37:05","Move LDM-151 to Sphinx/Read the Docs","Move the LDM-151 (DM applications design document) to restructuredText (built with Sphinx) and published automatically via readthedocs.org.    See discussion at http://community.lsst.org/t/requesting-comments-for-design-documentation-format-for-dm/132?u=jsick    This is an experiment.",1
"DM-3555","08/25/2015 22:47:09","Ignore ""SELECT @@tx_isolation"" queries","Looks like one of the queries we registered in webserv is: cursor.execute('SELECT @@tx_isolation') and that is bound to confuse Qserv. Need to suppress it at mysql proxy level.",1
"DM-3558","08/26/2015 00:21:36","Experiment with Jupyter widget technology and Firefly Tools","Based on DM-2047 work to date, investigate the feasibility of using the Jupyter widget interface to wrap up Firefly tools.",8
"DM-3587","08/27/2015 11:02:40","Firefly infrastructure improvement to support new functions (W16)","This epic will capture the necessary changes of Firefly infrastructure to support new functions needed. It does not include the changes caused by the the conversion from GWT infrastructure to pure JavaScript based system using React and FLUX platform. ",40
"DM-3593","08/27/2015 14:55:12","Firefly support for pipeline visualization needs  (W16)","Data products pipeline needs visualization capabilities for display. Firefly needs to have new capabilities to support it. ",40
"DM-3596","08/27/2015 15:34:33","More bug fixes in Firefly JS code ","Bug fixes and improvements for Firefly JS code  ",40
"DM-3608","08/27/2015 18:02:57","provide detailed information needed to DAX meta API","SUIT needs certain specific information through DAX meta service when searching for meta data. For Example, what kind of table it is, does it have spatial index to search by position, which set of (ra, dec) columns is the primary one, etc?",1
"DM-3609","08/27/2015 18:06:42","The Alert subscription system requirement gathering (F16)","Solidify the requirement for the alert subscription system.     Nov. 2, 2016 XW    After much discussion with AP team and the re-plan exercise, the requirement for alert subscription has been identified as the following:  *use the API that AP team will provide to*    # provide a UI for user to specify the filters on alerts of their interests, the destination of the alert to be sent  # save the specification in a DB  # provide UI to allow users to make modification of the filters and destinations of alerts  # possibly to annotate the alerts and allow user to access the annotation    This will involve SLAC for DB, NCSA for user management  ",8
"DM-3610","08/27/2015 18:11:05","CCB review and posting of final updated document","Carry out the CCB review, respond to questions, support final implementation of updated document.",2
"DM-3611","08/27/2015 18:18:02","Prepare for Winter 2016 work on LSE-68","Use a session at the LSST 2015 all-hands meeting to prepare for LSE-68 work in the Winter 2016 cycle.",3
"DM-3615","08/27/2015 18:33:15","expose region overlay on image function through JavaScript API","expose region overlay on image function through JavaScript API",5
"DM-3616","08/27/2015 18:35:16","Expose image XY readout at cursor point function in JavaScript API","Expose image XY readout at cursor point function in JavaScript API",2
"DM-3618","08/27/2015 21:31:16","Fix bug related to restarting xrootd in wmgr","Changes from DM-2930 are failing integration tests because wmgr is restarting xrootd and now we need to also restart mysqld if xrootd pid changes.",1
"DM-3630","08/28/2015 14:51:26","Change root to config in config override files","Implement RFC-62 by using {{config}} rather than {{root}} in config override files for the root of the config.    Note that I propose not modifying astrometry_net_data configs because those are numerous and hidden. They have their own special loader in LoadAstrometryNetObjectsTask._readIndexFiles which could easily be updated later. if desired. An obvious time to make such a transition would be when overhauling the way this data is unpersisted.",2
"DM-3638","08/28/2015 16:13:14","RangeField mis-handles max < min","RangeField contains the following bit of code to handle the case that max < min:  {code}           if min is not None and max is not None and min > max:              swap(min, max)  {code}    This is broken because there is no swap function and if there was it could not work in-place like this. However, rather than replace this with the standard {{min, max = max, min}} I suggest we raise an exception. If max < min then this probably indicates some kind of error or sloppiness that should not be silently ignored. If we insist on swapping the values then at least we should print a warning.    The fact that this bug has never been reported strongly suggests that we never do set min > max and thus that an exception will be fine.",1
"DM-3639","08/28/2015 16:24:31","OCS-CCS-DAQ-DM teleconference, April 2015","Prepare for and attend a half-day teleconference on OCS issues.",2
"DM-3641","08/28/2015 17:06:54","Firefly server side extensions using DM stack (F16)","Design and implement a control system to extend Firefly server side capabilities using task in DM stack.  This will make it easier to use DM stack for customized data processing. ",40
"DM-3642","08/28/2015 17:31:38","Support OCS revision of LSE-70, LSE-209","Support the OCS efforts to update LSE-70 and create a new associated document, LSE-209.  Getting current versions of these under change control will allow us to complete a round of work on LSE-72.",20
"DM-3646","08/28/2015 18:03:55","LSE-72: OCS-CCS-DAQ-DM workshop, July 2015","Work associated with Workshop IV in the series, held at NCSA July 8-10, 2015.",2
"DM-3648","08/28/2015 18:27:24","SUIT design document outline","SUI/T design document outline.  ",2
"DM-3650","08/28/2015 18:32:24","on-going support to Camera team in UIUC","Attend UIUC weekly meeting and give support as needed. ",2
"DM-3651","08/28/2015 18:33:15","MakeDiscreteSkyMapRunner.__call__ mis-handled returning results","{{MakeDiscreteSkyMapRunner.\_\_call\_\_}} will fail if {{self.doReturnResults}} is {{True}} due to trying to reference undefined variables. This is at least approximately a copy of a problem that was fixed in pipe_base {{TaskRunner}}.    {{MakeDiscreteSkyMapRunner.\_\_call\_\_}} should be fixed in a similar way, and (like {{TaskRunner}}) changed to return a pipe_base {{Struct}}.  ",2
"DM-3652","08/28/2015 18:39:31","SUIT design document outline","Work with Gregory on the SUIT design document outline    1.  Requirements flow down, making sure that we design the system satisfying the current requirements.  2.  Use cases collection. at least one typical use case in each major science theme  3.  Levels of different users  ** novice: treat the web portal as a archive to get some information, don't know much about LSST  ** novice expert: has some ideas of what special functions they would like, has some knowledge of LSST data  ** domain expert: knows LSST data very well and want some special functions ready to use  ** savvy expert: knows LSST data very well and like to use API to their own programming    4. functions for all different levels of users  5 system design   ** system diagram  ** details of the different parts  *** Firefly server  *** Firefly client  *** Firefly server extension  *** Firefly JavaScript API  *** Firefly Python API  *** Firefly Python API, Jupyter notebook, and other Python applications  *** workspace and level3 data  *** SUI web portal sketch, workflow  ** dependency on other capabilities of other institutes    6. development and test plan,  timeline  7. deployment plan                ",1
"DM-3653","08/28/2015 18:40:52","SUIT design document outline","work with John Rector on SUIT design document outline",1
"DM-3656","08/30/2015 00:31:52","Data loader doesn't work for match tables","qserv-data-loader.py fails to load match tables:   - it does not invoke the correct partitioner executable for them   - not all CSS parameters required for match tables are passed down to the CSS update code",1
"DM-3657","08/31/2015 04:04:21","Create change request for LSE-75","Create a change request for LSE-75, the TCS - to - DM ICD.",2
"DM-3658","08/31/2015 04:14:10","Discussions on LSE-75 with Telescope & Site personnel","Pursue interactions with Telescope and Site personnel regarding LSE-75, and in particular the issues surrounding calibration data products for the wavefront and guider data analysis pipelines.    Covers work through the end of August 2015.",3
"DM-3659","08/31/2015 08:50:41","Initial discussions with Patrick Ingraham","This story is a catch-all for preliminary conversations about LSE-140 with the new Calibration Instrumentation Scientist, Patrick Ingraham.",1
"DM-3667","08/31/2015 15:38:57","PSFEX does not build if PLplot is installed","During the configure phase PSFEX checks for the presence of PLplot. If PLplot is found then the build fails (at least on a Mac using homebrew):  {code}  /bin/sh ../libtool  --tag=CC   --mode=link clang  -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include   -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib -lfftw3f -lm  -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd  libtool: link: clang -g -O2 -I/Users/timj/work/lsstsw/src/psfex/lapack_functions/include -o psfex check.o context.o cplot.o diagnostic.o fft.o field.o field_utils.o fitswcs.o homo.o main.o makeit.o makeit2.o misc.o pca.o prefs.o psf.o sample.o sample_utils.o vignet.o wcs_utils.o xml.o  ./fits/libfits.a ./levmar/liblevmar.a ./wcs/libwcs_c.a -L/Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib /Users/timj/work/lsstsw/stack/DarwinX86/fftw/3.3.3-1-g8fdba61+da39a3ee5e/lib/libfftw3f.dylib -L/Users/timj/work/lsstsw/src/psfex/lapack_functions/lib -llapackstub -lf2c -lm -lplplotd  Undefined symbols for architecture x86_64:    ""_plwid"", referenced from:        _cplot_drawloccoordgrid in cplot.o        _cplot_fwhm in cplot.o        _cplot_ellipticity in cplot.o        _cplot_moffatresi in cplot.o        _cplot_asymresi in cplot.o        _cplot_counts in cplot.o        _cplot_countfrac in cplot.o        ...  ld: symbol(s) not found for architecture x86_64  {code}    This particular error is caused by PSFEX using a deprecated PLplot API ({{plwid}}) that is not enabled by default and whose name is not translated to {{c_plwid}}. This PLplot change occurred in version 5.9.10 released in 2013. I assume upstream PSFEX has a fix for this.    Given that LSST does not need the PLplot functionality I think the simplest fix may well be to disable the test for PLplot in our version.    It seems likely that there will be a reasonable number of systems ""in the wild"" who will have PLplot installed so I'm inclined to think that this should be a blocker for the v11.0 release.    If we are lucky people will have all upgraded their PLplot installs to v5.11.0 because in that version PLplot change the name of the library from {{libplplotd}} to {{libplplot}} and PSFEX has hard-wired the former rather than using pkg-config. This results in configure not finding PLplot. I don't think this eventuality is likely though.  ",0.5
"DM-3670","08/31/2015 19:19:07","obs_test needs to override map_camera and std_camera","The Butler can't get a camera unless the map_camera and std_camera are defined correctly.  In most cases the camera can be built by the map_camera method.  In the case of obs_test, the camera is built in the constructor of the Mapper, so std_camera should just return the camera attribute.",1
"DM-3675","08/31/2015 23:42:31","Resourcing Verification runs","  Identify required resources for Verification runs and communicate them to NCSA.   ",2
"DM-3678","09/01/2015 14:05:01","HSC backport: Standalone updates to star object selection","This involves pulling over the following standalone (i.e. non-ticket) HSC commits:  [Updated star selection algorithm.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/071fcadc016908a10583c746f0a8e79df2a45ead]    [Appropriate config parameter for a unit test of testPsfDetermination.py.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/e73c5e447ac0b8a71926d3e78fec30aad4beee91]    [Remove HSC specific codes.|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/15bb812578531766199e9a1ee41cc707fb3d9873]  (Note, the above reverts some unwanted camera-specific clauses added in the first commit.  May just squash them to only add the desired features)    [ObjectSizeStarSelector: push non-fatal errors to DEBUG level|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/44f75bc60b41c5f77b323a8d9981048ef7e5f3c4]    [We don't use focal plane coordinates anywhere, and detector may be None|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/4413db4610e4793727e591f395f5ad8cd0cb6030]    [Fixed axis labels|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/67efacaccf8346fdfa1b450617aebabddb2b7ec0]    [Improved PSF debugging plots|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/b1bc91ed1538607eb90e070881a82498fd551909]    [Worked on star selector|https://github.com/HyperSuprime-Cam/meas_algorithms/commit/6b36f4d757187d30142a7e026754a07ffeb8dea2]",1
"DM-3679","09/01/2015 14:40:40","Allow building/publishing components off branches other than master","Support of xrootd within the stack is currently complicated by the fact that qserv depends on features that are not available on upstream master (only available on an upstream non-master branch).  Since we can currently only publish packages from master, this means that our lsst fork of xrootd cannot be a ""pure"" fork -- we end up merging/rebasing from an upstream branch, then force-pushing the downstream master.  Upstream and downstream xrootd repos thus have completely different branch topologies, labels, etc., and history of master in the lsst fork is being continually rewritten to carry local patches forward.  The processes of both adopting upstream changes into the lsst fork and the pushing lsst changes back upstream are cumbersome, confusing, and labor intensive.    It is proposed that we extend our tools to allow publishing components from branches other than master.  This would allow us to have xrootd for example be a ""pure"" fork of upstream -- we could then create our own branch based off any upstream branch, carry our downstream patches there, and release off of that.    This functionality could be used similarly for any of our current ""t&p"" components where it would be convenient to track the upstream repo directly and/or carry changes in git instead of in an agglomerated patch file (e.g. when we might want to update frequently and/or contribute general purpose changes back upstream regularly with pr's, etc.)",2
"DM-3684","09/01/2015 16:53:40","Release engineering Part Two","This epic covers testing and co-ordination work associated with making  engineering and official releases, and code to support them.      [FE at 70%, JH at 20%, JS at 10%]",40
"DM-3686","09/01/2015 18:04:48","Fix PATH and compiler version detection in qserv scons","In recently merged DM-3662 compiler version testing was done using OS tools with regular $PATH. This is inconsistent with other scons tools which reset PATH when executing actions.   We want to do two things:  - propagate PATH to the command execution  - Use scons tools to run ""$CXX --version"" instead of OS tools to keep things consistent",1
"DM-3689","09/02/2015 11:42:37","Slack notification of discourse activity","It would be nice to have a hipchat channel with notifications of discourse activity.  As a power up, perhaps new topics under certain categories, I'm thinking specifically of DM Notifications, could generate posts to select general HC channels -- similar to how RFC notifications are currently handled.    A quick google search turns up this plugin for integration:    https://github.com/binaryage/discourse-hipchat-plugin",0.5
"DM-3691","09/02/2015 14:01:10","CalibrateTask has outdated, incorrect code for handling aperture corrections","The CFHT-specific CalibrateTask tries to apply aperture correction once just after measuring it (which is too early) and again later, at the right time. The error probably has no effect on the final results, but it is confusing and needlessly divergent from the standard CalibrateTask. The required changes are small. I plan to test by running [~boutigny]'s CFHT demo.",1
"DM-3692","09/02/2015 16:18:57","HSC backport: Allow for some fraction of PSF Candidates to be reserved from the fitting","This is a port of the changesets from [HSC-966|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-966].    It provides the ability to reserve some fraction of PSF candidates from the PSF fitting in order to check for overfitting and do cross validation.",1
"DM-3693","09/02/2015 16:25:10","HSC backport: allow photometric and astrometric calibrations to be required","This is a port of the standalone changesets:  [calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/e9db5c0dcdca20e8f7ba71f24f8b797e71699352]  [fixup! calibrate: make astrometry failures non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/c2d89396923f9d589822c043ed8753647e70f3f6]  (the above is a fixup, so will likely be squashed)  [make failure to match sources non-fatal|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/cf5724b852937cfcef1b71b7a372552011fda670]  [calibrate: restore original Wcs after initial astrometry solution|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/ab6cb9e206d0456dc764c5ef78ac80ece937c610]  [move CalibrateTask from ProcessImageTask into ProcessCcdTask|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/08a8ec029dd52ac55e47b707a6905df061a40506]  [processCoadd: set detection to use the declared variances|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/9e8563fd8d630dad967786387b1f27b6bc7ee039]  [adapt to removal of CalibrateTask from ProcessImageTask in pipe_tasks|https://github.com/HyperSuprime-Cam/obs_subaru/commit/52733a7ab1731a15cbb93151851f57cec276f928]  and HSC tickets:  [HSC-1085: background not saved in processCcd|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1085] and  [HSC-1086: psf - catalog scatter is very large in some coadds|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1086]",2
"DM-3694","09/03/2015 02:14:05","Decrease buildbot  frequency","Buildbot frequency is now down to two builds, one at 19:42 machine time (NCSA) and one at 1:42. This is to stop people needing buildbot runs to eups publish to have to wait before a CI build, since they are now done on[ https://ci.lsst.codes ]/ Jenkins.     ",1
"DM-3698","09/03/2015 09:42:03","Replace --trace with --loglevel in pipe_base ArgumentParser","Replace the --trace argument with an enhanced version of --loglevel that supports named values and numeric log levels (which are the negative of trace levels). This simplifies the interface for users and potentially reduces the log level/trace level confusion, though that won't fully happen until we finish replacing use of pex_logging Trace and Debug with Log.    This work was already done as part of DM-3532; it just needs to be copied with minor changes (since there are no named trace levels in pex_logging).",1
"DM-3702","09/03/2015 12:13:57","Assemble the report on DCR","Everything learned through the literature search and project wide meeting should be synthesized into a single readable report that details the expected effects of DCR on difference imaging as well as possible mitigation techniques.    This may involve some preliminary analysis work to measure effectiveness of various techniques.    Note that I expect this to be two weeks of work for two people, thus the 40 story points.  I don't know how to assign a story to two people.",40
"DM-3705","09/03/2015 14:58:49","Update ndarray to use current numpy API","When we build software that uses SWIG and ndarray we get {{warning ""Using deprecated NumPy API}}. Please update ndarray so we are using the current API.    If in the interim you want to suppress the warnings, I suggest issuing a separate ticket for that. I have mixed feelings about suppressing the warnings. On the one hand false warnings do make it harder to spot real problems. On the other hand these warnings are not very numerous (building afw only results in 8 of them) and so are fairly easy to ignore.",1
"DM-3707","09/03/2015 19:43:59","qserv scons - do not copy files to variant_dir","Some people are not happy with our current scons setup which copies source files from source directories to variant_dir, it makes it harder to trace errors using tools like eclipse or debug code. Would be nice to get rid of the extra copy, but we still want to have separate build directory (variant_dir). It should be simple enough, I think, but will need some testing of course.",2
"DM-3749","09/04/2015 07:51:03","Scons build of lapack_functions in PSFex fails if SCONSFLAGS are set","The scons build system is unaware of extra flags which may be set in SCONSFLAGS environment variable, which are used from scons utils. This will cause the build to fail. The package needs to behave properly and build in the presence of these flags",2
"DM-3763","09/04/2015 13:37:23","Be friendlier to New Users on Discourse","Please adjust New Users thresholds so that the following common use case on Discourse will work:    1. I have a question/thread I'm really interested in so I join.  2. I post answers, questions, and links.  I'm probably going to be heavily involved in this thread (it's why I bothered to create an account, after all).    The current defaults allow me only 3 replies, posting of 2 URLs, and no attachments.  I don't know what the exact values should be, but the current defaults don't seem to work for what would seem to me to be a pretty common use case.",0
"DM-3768","09/04/2015 18:17:16","Resolve the issues found in the S15 end-to-end system exercise","There are a few items we need to take care to finish the end-to-end system for S15. ",8
"DM-3769","09/04/2015 18:22:26","access the database created and populated for Bremerton end-to-end system","Collect the information for the tables populated for Bremerton end-to-end exercise. Use them in SUI/T so we can access them using the DAX API. ",2
"DM-3770","09/04/2015 18:24:23","build the SUI system on NCSA to use the right database and tables","Due to the changes of the database and tables, the system has to be rebuilt.",1
"DM-3771","09/04/2015 18:28:05","Resolve the issues accessing the newly populated tables","There are several issues need to be resolved for the system to work properly. ",5
"DM-3772","09/07/2015 02:27:31","Fix compiler detection for non-default gcc/g++ compiler","{{scons CXX=g+\+-4.4}} launches {{g\+\+-4.4 --version}} which returns {{g++-4.4 (Debian 4.4.7-2) 4.4.7}}. Nevertheless the {{-4.4}} is not supported by Qserv compiler detection tool. Support will be added here",0.5
"DM-3773","09/08/2015 08:20:04","add RUNID option to EventAppender","A RUNID needs to be added as an option to EventAppender to allow event logging selectors to receive only events for a particular run.",3
"DM-3774","09/08/2015 12:21:47","lsst_build's default ref from repos.yaml support is broken when building multiple packages","A problem with the default ref in {{repos.yaml}} support implemented in DM-3679 was discovered last Friday, shortly after deploying this feature to the production CI systems.    The default ref for {{xrootd}} was changed/overridden in {{repos.yaml}} to {{legacy/master}}.  This worked as expected (and as was tested) when setting {{xrootd}} as the sole {{lsstswBuild.sh}} product or when running {{rebuild}} by hand.  However, when building any package that pulled in {{xrootd}} as a recursive dependency, the {{master}} branch was being used (this case had not been manually tested).",1
"DM-3775","09/08/2015 13:09:19","HSC backport: updates to tract and patch finding","This is a port of the following HSC updates to how tracts and patches are found and listed given a set of coordinates.  These are all standalone commits (i.e. not associated with a ticket):  [Add findTract() and findTractPatchList() in ringsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/761e915dde25ce8ed5622c2d84b83793e9580fd7]  [move RingsSkyMap.findTractPatchList to BaseSkyMap.findClosestTractPatchlist|https://github.com/HyperSuprime-Cam/skymap/commit/56476142060bdb7d8c7fb59eacc383f0e0d5c85b]  [Small bug fix for RingsSkyMap.findTract().|https://github.com/HyperSuprime-Cam/skymap/commit/f202a7780ebb89166f03479d7447ace1555027c1]  [Add fast findTractPatchList() in RingsSkyMap.|https://github.com/HyperSuprime-Cam/skymap/commit/7e49c358501f95ce4c0e1aa8f48103a24391fc22]  [Fixed the problems regarding poles and RA wrap.|https://github.com/HyperSuprime-Cam/skymap/commit/841b0c9eda7462a7a4f182b7971d5e8e81478bfe]  [Add spaces around '+' and '-' to match LSST standard coding style.|https://github.com/HyperSuprime-Cam/skymap/commit/f7e2f036494afe382e653194c82bb15728c60fc3]",1
"DM-3778","09/08/2015 16:51:37","Fix compiler warns in protobuf clients","Google protobufs 2.6.1 includes a few unnecessary semicolons in some of its supplied header files; these generate a lot of compiler warnings when compiling client packages.    Proposed fix is to add a patch to our eups t&p protobufs package to remove the offending semicolons.",1
"DM-3779","09/08/2015 17:04:34","clean up gcc and eclipse code analyzer warns","We've been ignoring some accumulating warns in the qserv build for some time now.  Now that it is possible to develop qserv in eclipse, it would be useful to address warns and analyzer issues so that we can start to notice when new ones pop up.",1
"DM-3780","09/08/2015 17:13:13","Rationalize lsst/xrootd repo and maintenance procedures","The procedure for pulling/pushing xrootd changes from/to the upstream official xrootd repo is cumbersome, confusing, and error-prone.    Buildbot now has support for releasing packages from branches other than master.  Given this, we can now reasonably replace our lsst/xrootd repo with a fresh genuine fork (shared history) of upstream, then carry our lsst-specific work forward on a dev-branch.  This will make it much easier to track and contribute to the xrootd project moving forward.    Existing legacy branches and tags are to be migrated to the fresh fork, so historical builds will not be broken.",1
"DM-3790","09/09/2015 11:20:18","Top level product for obs_decam ","We have an action to allow people to pull obs_decam and a working stack without including obs_decam in lsst_distrib. And we want to CI it.     The strawman plan is to make a new TLP for decam. SQuaRE will discuss. ",0
"DM-3792","09/09/2015 13:06:13","obs_test data mis-assembled","obs_test images are mis-assembled and need to be regenerated. This may affect some existing unit tests that rely on the data.",2
"DM-3797","09/10/2015 11:56:38","Enable SSL to community.lsst.org","Enable SSL (https) for the Discourse site at community.lsst.org",1
"DM-3798","09/10/2015 12:22:06","Update flag names and config override files to current conventions","The {{deblend.masked}} and {{deblend.blendedness}} flag names in {{meas_deblender}} need to be updated to use underscores instead of periods.  Various flag names in the {{examples}} scripts also need updating to the underscore and camelCase format.    A search for these flags throughout the database revealed a number of config files that need updating to current conventions.  These are also included here.",0.5
"DM-3800","09/10/2015 15:31:45","testProcessCcd.py computes values that are too different between MacOS and linux","tests/testProcessCcd.py runs processCcd on visit 1 of obs_test's data repository. The result on MacOS is surprisingly different than on linux in at least one case: psfShape.getIxx() computes 2.71 on MacOS X and 2.65 on linux. Iyy and Ixy are likely different. It's worth checking all other computed values, as well. These differences likely indicate that something is wrong, e.g. in obs_test, processCcd, or the way the test runs processCcd.    This showed up as part of fixing DM-3792, but it is not clear if the changes on DM-3792 actually caused or increased the difference between MacOS and linux, or if the difference was always too large, but was masked by an intentionally generous tolerance in the unit test.",2
"DM-3803","09/10/2015 15:55:51","Fix Qserv compiler warnings with clang","Qserv triggers numerous warnings with clang on OS X. Full details are in the attached ticket, here we summarize the distinct warnings classes:    h5. Protobuf    {code}  /Users/timj/work/lsstsw/stack/DarwinX86/protobuf/2.6.1+fbf04ba888/include/google/protobuf/unknown_field_set.h:214:13: warning: anonymous types declared in an anonymous union        are an extension [-Wnested-anon-types]      mutable union {              ^  {code}    h5. Qserv    {code}  In file included from core/modules/sql/statement.cc:32:  core/modules/sql/Schema.h:74:1: warning: 'Schema' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Schema {  ^  core/modules/sql/statement.h:35:1: note: did you mean struct here?  class Schema; // Forward  ^~~~~  struct  {code}    {code}  core/modules/proto/WorkerResponse.h:34:1: warning: 'WorkerResponse' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct WorkerResponse {  ^  core/modules/ccontrol/MergingRequester.h:38:3: note: did you mean struct here?    class WorkerResponse;    ^~~~~    struct  {code}    {code}  In file included from core/modules/qana/QueryMapping.cc:46:  core/modules/qproc/ChunkSpec.h:51:1: warning: 'ChunkSpec' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ChunkSpec {  ^  core/modules/qana/QueryMapping.h:44:5: note: did you mean struct here?      class ChunkSpec;      ^~~~~      struct  {code}    {code}  core/modules/qana/TableInfo.h:186:1: warning: 'DirTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct DirTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:86:1: note: did you mean struct here?  class DirTableInfo;  ^~~~~  struct  core/modules/qana/TableInfo.h:221:1: warning: 'ChildTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ChildTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:87:1: note: did you mean struct here?  class ChildTableInfo;  ^~~~~  struct  core/modules/qana/TableInfo.h:260:1: warning: 'MatchTableInfo' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct MatchTableInfo : TableInfo {  ^  core/modules/qana/TableInfo.h:88:1: note: did you mean struct here?  class MatchTableInfo;  ^~~~~  struct  In file included from core/modules/qana/ColumnVertexMap.cc:36:  core/modules/qana/RelationGraph.h:513:1: warning: struct 'Vertex' was previously declared as a class [-Wmismatched-tags]  struct Vertex;  ^  core/modules/qana/ColumnVertexMap.h:44:7: note: previous use is here  class Vertex;        ^  In file included from core/modules/qana/ColumnVertexMap.cc:36:  core/modules/qana/RelationGraph.h:547:1: warning: 'Vertex' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct Vertex {  ^  core/modules/qana/ColumnVertexMap.h:44:1: note: did you mean struct here?  class Vertex;  ^~~~~  struct  {code}    {code}  core/modules/wbase/Base.h:72:1: warning: 'ScriptMeta' defined as a struct here but previously declared as a class [-Wmismatched-tags]  struct ScriptMeta {  ^  core/modules/wbase/Task.h:41:5: note: did you mean struct here?      class ScriptMeta;      ^~~~~      struct  {code}    {code}  In file included from core/modules/parser/BoolTermFactory.cc:46:  core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual]      virtual std::ostream& putStream(std::ostream& os) = 0;                            ^  core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none)      virtual std::ostream& putStream(std::ostream& os) const = 0;                            ^  {code}    {code}  core/modules/parser/FromFactory.cc:62:15: warning: unused function 'walkToSiblingBefore' [-Wunused-function]  inline RefAST walkToSiblingBefore(RefAST node, int typeId) {                ^  core/modules/parser/FromFactory.cc:72:1: warning: unused function 'getSiblingStringBounded' [-Wunused-function]  getSiblingStringBounded(RefAST left, RefAST right) {  ^  {code}    {code}  In file included from core/modules/wsched/ChunkDisk.cc:25:  core/modules/wsched/ChunkDisk.h:130:10: warning: private field '_completed' is not used [-Wunused-private-field]      bool _completed;           ^  {code}    {code}  In file included from core/modules/parser/PredicateFactory.cc:45:  core/modules/query/Predicate.h:86:27: warning: 'lsst::qserv::query::GenericPredicate::putStream' hides overloaded virtual function [-Woverloaded-virtual]      virtual std::ostream& putStream(std::ostream& os) = 0;                            ^  core/modules/query/Predicate.h:71:27: note: hidden overloaded virtual function 'lsst::qserv::query::Predicate::putStream' declared here: different qualifiers (const vs none)      virtual std::ostream& putStream(std::ostream& os) const = 0;                            ^  {code}    {code}  core/modules/parser/WhereFactory.cc:265:31: warning: binding reference member 'c' to stack allocated parameter 'c_' [-Wdangling-field]      PrintExcept(Check c_) : c(c_) {}                                ^~  core/modules/parser/WhereFactory.cc:291:28: note: in instantiation of member function 'lsst::qserv::parser::PrintExcept<lsst::qserv::parser::MetaCheck>::PrintExcept' requested        here      PrintExcept<MetaCheck> p(mc);                             ^  core/modules/parser/WhereFactory.cc:269:12: note: reference member declared here      Check& c;             ^  {code}    {code}  core/modules/rproc/ProtoRowBuffer.cc:44:11: warning: unused variable 'largeRowThreshold' [-Wunused-const-variable]  int const largeRowThreshold = 500*1024;            ^  {code}    {code}  core/modules/util/testIterableFormatter.cc:85:43: warning: suggest braces around initialization of subobject [-Wmissing-braces]      std::array<std::string, 6> iterable { ""1"", ""2"", ""3"", ""4"", ""5"", ""6""};                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                                            {                           }  {code}    {code}  In file included from core/modules/qdisp/XrdSsiMocks.cc:37:  core/modules/qdisp/XrdSsiMocks.h:64:16: warning: private field '_executive' is not used [-Wunused-private-field]      Executive *_executive;                 ^  {code}    {code}  core/modules/xrdoss/QservOss.cc:77:1: warning: unused function 'print' [-Wunused-function]  print(std::ostream& os, lsst::qserv::xrdoss::QservOss::StringSet const& h) {  ^  {code}    h5. OS X    {code}  core/modules/qdisp/QueryRequest.h:54:25: warning: 'lsst::qserv::qdisp::BadResponseError::what' hides overloaded virtual function [-Woverloaded-virtual]      virtual char const* what() throw() {                          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function        'std::exception::what' declared here: different qualifiers (const vs none)      virtual const char* what() const _NOEXCEPT;                          ^  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:37:  core/modules/qdisp/QueryRequest.h:67:25: warning: 'lsst::qserv::qdisp::RequestError::what' hides overloaded virtual function [-Woverloaded-virtual]      virtual char const* what() throw() {                          ^  /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/exception:95:25: note: hidden overloaded virtual function        'std::exception::what' declared here: different qualifiers (const vs none)      virtual const char* what() const _NOEXCEPT;                          ^  {code}    {code}  core/modules/proto/TaskMsgDigest.cc:55:5: warning: 'MD5' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      MD5(reinterpret_cast<unsigned char const*>(str.data()),      ^  /usr/include/openssl/md5.h:116:16: note: 'MD5' has been explicitly marked deprecated here  unsigned char *MD5(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  {code}    {code}  core/modules/util/StringHash.cc:78:24: warning: 'SHA1' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      return wrapHashHex<SHA1, SHA_DIGEST_LENGTH>(buffer, bufferSize);                         ^  /usr/include/openssl/sha.h:124:16: note: 'SHA1' has been explicitly marked deprecated here  unsigned char *SHA1(const unsigned char *d, size_t n, unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  core/modules/util/StringHash.cc:83:24: warning: 'SHA256' is deprecated: first deprecated in OS X 10.7 [-Wdeprecated-declarations]      return wrapHashHex<SHA256, SHA256_DIGEST_LENGTH>(buffer, bufferSize);                         ^  /usr/include/openssl/sha.h:150:16: note: 'SHA256' has been explicitly marked deprecated here  unsigned char *SHA256(const unsigned char *d, size_t n,unsigned char *md) DEPRECATED_IN_MAC_OS_X_VERSION_10_7_AND_LATER;                 ^  {code}    h5. Xrootd    {code}  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:33:  In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a        struct here but previously declared as a class [-Wmismatched-tags]  struct  XrdSsiRespInfo  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here?  class XrdSsiRespInfo;  ^~~~~  struct  {code}    {code}  core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual]      virtual int Opendir(const char *) { return XrdOssOK; }                  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function        'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1)  virtual int     Opendir(const char *, XrdOucEnv &)           {return -ENOTDIR;}                  ^  {code}    {code}  In file included from core/modules/xrdsvc/SsiSession.h:32:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of        non-void function [-Wreturn-type]                            }                            ^  {code}    h5. boost    {code}  /Users/timj/work/lsstsw/stack/DarwinX86/boost/1.55.0.1.lsst2+fbf04ba888/include/boost/regex/v4/regex_raw_buffer.hpp:132:7: warning: 'register' storage class specifier is        deprecated [-Wdeprecated-register]        register pointer result = end;        ^~~~~~~~~  {code}",0.5
"DM-3804","09/10/2015 16:44:05","Fix order of arguments - run method of meas_base SingleFrameMeasurementTask","In sfm.py on line 271, a comment indicates that some code is a temporary work around until the switch from meas_algorithms to meas_base is complete. This work is complete, so this temporary workaround should be removed, or if it is decided it should be kept, the comment should be removed. See https://github.com/lsst/meas_base/blob/tickets/DM-2915/python/lsst/meas/base/sfm.py#L271",2
"DM-3808","09/10/2015 20:11:13","Setup lsst_sphinx_kit package structure","Setup the lsst_sphinx_kit package, including    * setup.py  * unit tests, tox and Travis CI  * README stub  * Sphinx stub and readthedocs",0.5
"DM-3816","09/13/2015 10:11:24","levels in DecamMapper.paf is not quite right","When ccdnum is not given as part of the dataId, instead of iterating over it, an error like this happens    {code:java}    RuntimeError: No unique lookup for ['ccdnum'] from {'visit': 205344}: 61 matches  {code}    Likely a problem in policy/DecamMapper.paf",1
"DM-3821","09/14/2015 08:15:37","Recent CModel bugfixes from HSC","I've just fixed two rather critical bugs in the CModel code on the HSC side (they would have been introduced on the LSST side in the last transfer, DM-2977):   - The {{minInitialRadius}} configuration parameter had a default that is too small, causing many galaxies to be fit with point source models, leading to bad star/galaxy classifications.  This is HSC-1306.   - There was a simple but important algebra error in the uncertainty calculation, making the uncertainty a strong function of magnitude.  This is HSC-1313.    On the LSST side, the transfer should be quite simple; we'll have to rewrite a bit of code due to the difference in measurement frameworks, but there was very little to begin with (most of the effort in the HSC issues was in debugging).",1
"DM-3836","09/14/2015 12:41:42","Migrate LDM-129 to new design docs platform","Convert LDM-129 from Word to restructuredText and deploy onto readthedocs.org",2
"DM-3840","09/14/2015 13:58:21","LSE-72: Phase 3 in X16","Advance Phase 3 details as needed to eliminate obstacles to OCS and DM development during F16.",8
"DM-3841","09/14/2015 14:01:34","LSE-75: Refine WCS and PSF requirements in W16","Clarify the data format and precision requirements of the TCS (or other Telescope and Site components) on the reporting of WCS and PSF information by DM on a per-image basis.    Depends on the ability of the T&S group to engage with this subject.    Current PMCS deadline for Phase 3 readiness of LSE-75 is 29-Sep-2015.",8
"DM-3850","09/14/2015 19:06:16","Nebula metadata service is intermittent","Upon restarting one of my nebula instances (ktl-test), I noticed a failure in the logs:  {quote}  Sep 14 18:07:29 ktl-test cloud-init: 2015-09-14 18:07:29,157 - util.py[WARNING]: Failed fetching metadata from url http://169.254.169.254/latest/meta-data  {quote}    Attempting to retrieve that URL seems to randomly vary between succeeding, which returns:  {quote}  ami-id  ami-launch-index  ami-manifest-path  [...]  {quote}    and failing, which returns:  {quote}  <html>   <head>    <title>500 Internal Server Error</title>   </head>   <body>    <h1>500 Internal Server Error</h1>    Remote metadata server experienced an internal server error.<br /><br />         </body>  </html>  {quote}  These failures may be contributing to observed sporadic {{ssh}} key injection failures.",2
"DM-3860","09/15/2015 13:38:38","Communication Toolchain support","This epic covers support of communication tools primarily used by DM  and/or supported by DM on behalf of other parts of the project - JIRA,  Discourse, Hipchat, etc     The source of this work is primarily driven by short-term user  requests, and so the outcome is timeboxed rather than planned.     [JS 50% FE 50%]  ",20
"DM-3863","09/15/2015 14:17:14","Web design fixes DM Design Documents on Sphinx/Read The Docs","Solve fit-and-finish issues with the stock readthedocs.org Sphinx template when rendering DM design documents. Issues include:    * Sections need to be numbered and those numbers need to appear in TOC  * RTD's TOC does not properly collapse sub-topics  * Appropriate styling for document title and author list  * Wrapping the changelog table  * Adapt section references so that just the section number can be referenced, independently of the section number and title in combination  * Section labels given explicitly in the reST markup are different from the anchors that Sphinx gives to the {{<hN>}}tags; the former are simply divs inserted in the HTML.    The solutions may involve    # reconfiguring the Sphinx installation of individual documents  # forking the RTD HTML template, and/or  # developing extensions for Sphinx in {{sphinxkit}}.",2
"DM-3887","09/16/2015 09:28:05","Review ICD flowdown to DMSR and design documents","This epic will result in a plan for how DM will manage the ICD requirements and their relationship to LSE-61 (DMSR). This plan should make it easier for authors of design documents to understand which requirements are relevant.",8
"DM-3892","09/16/2015 11:58:27","Review current version of LSE-78, prepare for LCR","Do a comprehensive read-through of the previous released version of LSE-78.  Look for self-consistency and for consistency with the rest of the DM and overall system design.  Report issues to appropriate people.",3
"DM-3898","09/16/2015 12:44:10","Fix xrootd compiler warnings with clang","h5. Xrootd    {code}  In file included from core/modules/qdisp/Executive.cc:64:  In file included from core/modules/qdisp/XrdSsiMocks.h:33:  In file included from /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRequest.hh:37:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiRespInfo.hh:43:1: warning: 'XrdSsiRespInfo' defined as a        struct here but previously declared as a class [-Wmismatched-tags]  struct  XrdSsiRespInfo  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiSession.hh:45:1: note: did you mean struct here?  class XrdSsiRespInfo;  ^~~~~  struct  {code}    {code}  core/modules/xrdoss/QservOss.h:64:17: warning: 'lsst::qserv::xrdoss::FakeOssDf::Opendir' hides overloaded virtual function [-Woverloaded-virtual]      virtual int Opendir(const char *) { return XrdOssOK; }                  ^  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdOss/XrdOss.hh:63:17: note: hidden overloaded virtual function        'XrdOssDF::Opendir' declared here: different number of parameters (2 vs 1)  virtual int     Opendir(const char *, XrdOucEnv &)           {return -ENOTDIR;}                  ^  {code}    {code}  In file included from core/modules/xrdsvc/SsiSession.h:32:  /Users/timj/work/lsstsw/stack/DarwinX86/xrootd/u.timj.DM-3584-ge22410fa7f+da39a3ee5e/include/xrootd/XrdSsi/XrdSsiResponder.hh:177:27: warning: control may reach end of        non-void function [-Wreturn-type]                            }                            ^  {code}",0.5
"DM-3910","09/18/2015 04:22:27","Run and document multi-node test with docker","In order to validate Docker setup on CC-IN2P3 cluster, it is required to launch some test on consistent data. S15 LargeScaleTest data doesn't seems to be compliant with latest Qserv version so running multi-node test would be interesting. Nevertheless the multi-node setup doesn't seems to be documented and, hence, is difficult to reproduce.",3
"DM-3911","09/18/2015 11:27:19","HSC backport: avoid I/O race conditions config write out","This is a port of [HSC-1106|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1106]    When running tasks that write out config settings files ({{processCcd.py}}, for example), if multiple processes start simultaneously, an I/O race condition can occur in writing these files.  This is solved here by writing to temp files and then renaming them to the correct destination filename in a single operation.  Also, to avoid similar race conditions in the backup file creation (e.g. config.py~1, config.py~2, ...), a {{--no-backup-config}} option (to be used with --clobber-config) is added here to prevent the backup copies being made.  The outcome for this option is that the config that are still recorded are for the most recent run.",1
"DM-3912","09/18/2015 14:16:30","some ctrl_events tests execute outside of execution domain","There are a couple of ctrl_events tests that attempt to execute outside of the valid domains acceptable by the tests, when they shouldn't be.  There's a check in place for tests to find this, but a couple of the tests do not have this check.",0.5
"DM-3922","09/21/2015 14:41:19","Update multi-node setup documentation","Workers in multi-node setup no longer require granting mysql permissions for test datasets since direct mysql connections are no longer used by the data loader.",1
"DM-3926","09/22/2015 11:58:04","Implement iostream-style formatting in log package","Implement proposed in RFC-96 change to log macros. This ticket only covers defining new set of macros (LOGS() and friends) which use ostringstream for formatting messages. Migration of all clients and removal of LOGF macros will be done in separate ticket.",1
"DM-3940","09/24/2015 06:02:33","NaiveDipoleCentroid/NaiveDipoleFlux algorithms should not require centroid slot","The {{NaiveDipoleCentroid}} and {{NaiveDipoleFlux}} algorithms in {{ip_diffim}} have members which are instances of {{meas::base::SafeCentroidExtractor}}. Due to the prerequisites that imposes, it is impossible to initialize these algorithms without first defining a {{centroid}} slot.    However, there is nothing in these algorithms which actually uses the {{SafeCentroidExtractor}} or any of the information stored in the slot; this seems to be an entirely arbitrary restriction which is likely a legacy of the port to the {{meas_base}} framework. We should remove the  use of {{SafeCentroidExtractor}} to simply the code and make it easier to run the test suite (since it will no longer be necessary to run a centroider).",0.5
"DM-3943","09/24/2015 16:13:58","QMeta thread safety","Initial QMeta implementation is not thread safe, it uses sql/mysql modules which also do not have any protection (there are some mutexes there but not used). Need an urgent fix to avoid crashes due to concurrent queries in czar.",1
"DM-3947","09/25/2015 17:55:18","Remove dependency on mysqldb in wmgr","Move remaining code that depends on mysqldb to db module",0.5
"DM-3949","09/25/2015 21:27:30","Remove dependency on mysqldb in qserv","Remove remaining dependencies on mysqldb in qserv.:  {code}  ./core/modules/tests/MySqlUdf.py  ./core/modules/wmgr/python/config.py  {code}    and use the sqlalchemy from db module instead.",2
"DM-3951","09/28/2015 03:04:19","Remove qserv_objectId restrictor","qserv_objectId restrictor can be replaced by the IN restrictor. This story involves checking if performance is acceptable if we use IN restrictor instead of qserv_objectId restictor, and if it is, doing the switch and removing the qserv_objectId restictor code.",3
"DM-3957","09/28/2015 13:01:39","Enable CModel in CalibrateTask prior to PhotoCal","CModel needs to run in CalibrateTask before PhotoCal in order to compute aperture corrections, but it also needs a Calib objects as input, and that isn't available until after PhotoCal is run.    On the HSC side, we dealt with this by adding preliminary PhotoCal run before CModel is run, but we could also deal with it by removing the need for a Calib as input, at least in some situations.",1
"DM-3971","09/29/2015 02:34:48","Package sqlalchemy in eups","Db module is expected to be used by science pipelines, and (per K-T, see qserv hipchat room) we have to package it through eups.",1
"DM-3980","09/30/2015 10:07:32","Post SQLAlchemy-migration tweaks","Implement some minor tweaks take came in late through PR comments, mostly related to sqlalchemy related migration",0.5
"DM-3981","09/30/2015 11:42:58","Improve the performance for making the image plot","Make FitsRead work better for multi-threads",1
"DM-3982","09/30/2015 16:40:45","CalibrateTask is incompatible with older astrometry tasks","[~lauren] and I recently discovered an incompatibility between {{CalibrateTask}}'s schema-handling and the older astrometry tasks, such as {{ANetAstrometryTask}} (the problem affects any astrometry task that utilizes its {{schema}} constructor argument).    The problem is that astrometry is called twice in {{CalibrateTask}}:   - Just before PSF estimation, in order to get matches to support {{CatalogStarSelector}}.  This call is passed {{sources1}}, a catalog containing only the initial, pre-PSF measurements.   - After the second measurement stage, to determine the {{Wcs}} and get matches to feed {{PhotoCal}}. This call is passed {{sources}}.    The problem is that {{sources.schema != sources1.schema}}, and the astrometry subtask is initialized with {{schema1 == sources1.schema}}.  So if the astrometry task needs to fields it added to the schema (the default {{AstrometryTask}} does not), it will likely fail in the second run, as it's being given a catalog that doesn't correspond to the schema it was initialized with.    Comments in the code indicate that there's a desire that future {{AstrometryTask}} s will not take a schema argument, and that would mean that they could be called repeatedly with different schemas (as we are doing).  I'm not sure that's viable; I think it'd probably be appropriate for {{AstrometryTask}} to set flags indicated the sources it's using (or considered using), as the other calibrate subtasks do.    Instead, I think we're probably best off creating two separate astrometry subtasks, each with a different schema, and using the appropriate subtask for each input/output catalog.    Clearly we need to come up with a better solution when in the upcoming {{CalibrateTask}} redesign; two subtasks is obviously clunky.  Whether this is high-priority before then depends on how much we're dependent on {{ANetAstrometryTask}} as a fallback option, and I haven't been following that conversation closely.",0
"DM-3987","10/01/2015 10:25:19","remove unnecessary 'psf' arg to SourceDeblendTask.run()","{{SourceDeblendTask.run}} takes both an {{Exposure}} and a {{Psf}}, even though it can get the latter from the former and always should.",1
"DM-3993","10/01/2015 11:07:10","Display.dot origin swaps x and y","Correcting for xy0 in {{dot}} currently does:  {code:hide-linenum}  r -= x0  c -= y0  {code}  which is backwards.",0.5
"DM-4002","10/01/2015 14:02:55","Add doc dir and main.dox to obs_test","No obs_* packages have a doc dir. I suggest starting with obs_test since it is perhaps less obvious what it is used for than the others and in some ways acts as a template.",0
"DM-4003","10/01/2015 14:20:04","Replace zookeeper CSS with mysql","To switch from QservAdmin to CssAccess interface in our Python tools we will need to replace zookeeper with mysql implementation because we do not have C++ KvInterface implementation for zookeeper.",2
"DM-4009","10/02/2015 13:04:33","Allow FlagHandler to be used from Python","The {{FlagHandler}} utility class makes it easier to manage the flags for a measurement algorithm, and using it also makes it possible to use the {{SafeCentroidExtractor}} and {{SafeShapeExtractor}} classes.  Unfortunately, its constructor requires arguments that can only be provided in C++.  A little extra Swig wrapper code should make it usable in Python as well.",3
"DM-4014","10/05/2015 17:56:47","Replace boost::tuple with <tuple>","Replace boost::tuple with <tuple>    This ticket will be completed as part of the DM bootcamp at UW.",1
"DM-4020","10/06/2015 10:00:18","Remove #!/usr/bin/env from pipe_tasks library code","Many tasks in pipe_tasks start with #!/usr/bin/env..., yet are not executable.",0
"DM-4021","10/06/2015 16:00:25","Replace boost::unordered_map with std::unordered_map","DM boot camp tutorial.    ",2
"DM-4022","10/07/2015 08:31:57","forcedPhotCoadd.py fails on HSC data","When trying to run {{forcedPhotCoadd.py}} on HSC data, I see the following error:    {code}  $ forcedPhotCoadd.py /raid/swinbank/rerun/LSST/bootcamp --id filter='HSC-I' tract=0 patch=7,7  : Loading config overrride file '/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py'  Cannot import lsst.meas.extensions.photometryKron: disabling Kron measurements  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/meas_base/11.0+2/bin/forcedPhotCoadd.py"", line 24, in <module>      ForcedPhotCoaddTask.parseAndRun()    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/cmdLineTask.py"", line 433, in parseAndRun      parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides)    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 360, in parse_args      self._applyInitialOverrides(namespace)    File ""/home/lsstsw/stack/Linux64/pipe_base/11.0+2/python/lsst/pipe/base/argumentParser.py"", line 475, in _applyInitialOverrides      namespace.config.load(filePath)    File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 529, in load      self.loadFromStream(stream=code, root=root)    File ""/home/lsstsw/stack/Linux64/pex_config/11.0/python/lsst/pex/config/config.py"", line 549, in loadFromStream      exec stream in {}, local    File ""/nfs/home/swinbank/obs_subaru/config/forcedPhotCoadd.py"", line 10, in <module>      config.deblend.load(os.path.join(os.environ[""OBS_SUBARU_DIR""], ""config"", ""deblend.py""))  AttributeError: 'ForcedPhotCoaddConfig' object has no attribute 'deblend'  {code}    This is with the stack version 11.0+3 and {{obs_subaru}} 5.0.0.1-676-g4ae362c.",0.5
"DM-4036","10/09/2015 16:12:52","Change from boost::math","Most boost::math contents (not including pi) are now available in standard C++. Please convert the code accordingly.    In addition to the packages listed above, boost/math is used in ""partition"" a package I don't recognize and not a component JIRA accepts.",5
"DM-4043","10/09/2015 23:19:01","update memory management in jointcal","jointcal currently uses a combination of raw pointers and a custom reference-counted smart pointer class, {{CountedRef}} (similar to {{boost::intrusive_ptr}}).  The code needs to be modified to use a combination of {{shared_ptr}} (most code), {{unique_ptr}} local-scope variables and factory functions, and {{weak_ptr}} (at least some will be necessary to avoid cycles in some of the more complex data structures).  As part of this work, we'll also have to remove a lot of inheritance from {{RefCounted}}, which is part of the {{CountedRef}} implementation.    This ticket looks like it will require a lot of work, because we'll have to be careful about every conversion to avoid cycles and memory leaks.  Nevertheless, I think it will be necessary to do this conversion before attempting any other major refactoring, as I'm worried that having a newcomer make changes to the codebase without first making the memory management less fragile could be very dangerous.",8
"DM-4063","10/12/2015 11:02:15","Support new casting requirements in NumPy 1.10","The function imagesDiffer() in testUtils attempts to OR an array of unit16s (LHS) against an array of bools(RHS) {{valSkipMaskArr |= skipMaskArr}} and errors with message  {code}  TypeError: ufunc 'bitwise_or' output (typecode 'H') could not be coerced to provided output parameter (typecode '?') according to the casting rule ''same_kind''  {code}  preventing afw from building correctly. ",1
"DM-4065","10/12/2015 11:26:11","Discuss with MySQL team","This story captures issues/topics that we want to bring up with mysql team.",2
"DM-4071","10/12/2015 12:12:33","testPsfDetermination broken due to NumPy behaviour change","Old NumPy behaviour (tested on 1.6.2):  {code}  In [1]: import numpy    In [2]: a = numpy.array([])    In [3]: numpy.median(a)  /usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2374: RuntimeWarning: invalid value encountered in double_scalars    return mean(axis, dtype, out)    Out[3]: nan  {code}    New NumPy behaviour (1.10.0):  {code}  In [1]: import numpy    In [2]: a = numpy.array([])    In [3]: numpy.median(a)  [...]  IndexError: index -1 is out of bounds for axis 0 with size 0  {code}    This breaks {{testPsfDeterminer}} and {{testPsfDeterminerSubimage}}, e.g.:  {code}  ERROR: testPsfDeterminerSubimage (__main__.SpatialModelPsfTestCase)  Test the (PCA) psfDeterminer on subImages  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""./testPsfDetermination.py"", line 342, in testPsfDeterminerSubimage      trimCatalogToImage(subExp, self.catalog))    File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 377, in selectStars      widthStdAllowed=self._widthStdAllowed)    File ""/Users/jds/Projects/Astronomy/LSST/src/meas_algorithms/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 195, in _kcenters      centers[i] = func(yvec[clusterId == i])    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3084, in median      overwrite_input=overwrite_input)    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 2997, in _ureduce      r = func(a, **kwargs)    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 3138, in _median      n = np.isnan(part[..., -1])  IndexError: index -1 is out of bounds for axis 0 with size 0  {code}",0.5
"DM-4075","10/13/2015 17:19:47","Assemble eslint rules for JavaScript code quality control","Review and assemble eslint rules, which enforce clean JavaScript and JSX code.    Code cleanup to avoid too many rule violations.",8
"DM-4076","10/13/2015 17:26:41","JavaScript code cleanup - remove unused packages","Remove es6-promise, react-modal, other cleanup",2
"DM-4078","10/13/2015 21:44:57","convert underscore to lodash","lodash has become a superset of underscore, providing more consistent API behavior, more features, and more thorough documentation. We'd like to convert our underscore package dependencies to lodash while we have only ~20 calls to underscore functions",2
"DM-4080","10/14/2015 10:19:18","Shutdown mechanism doesn't work when logging process is disabled.","If the logging mechanism is turned off in ctrl_execute, the ctrl_orca Logger doesn't get launched.  The current shutdown mechanism waits for the last logging message to be transmitted before shutting down so it doesn't kill off that process.   If the logger.launch config file option is set to false, this process never get launched and ctrl_orca hangs after the shutdown waiting for the message to arrive.",8
"DM-4085","10/14/2015 11:29:08","Attend DM boot camp","Attend DM boot camp to learn more about DM stack, butler, and task. ",2
"DM-4086","10/14/2015 11:30:13","Attend DM boot camp","Attend DM boot camp to learn more about DM stack, butler, and task.     Most of the presentations are located at URL https://community.lsst.org/t/dm-boot-camp-announcement/249. Presentations like afw, eups, tasks, and butler are necessary to participate in LSST, so everyone on LSST must understand these concepts. Look at the list of presentations covering these topics and make sure your understand them. Some of the remaining talks go into more detail or cover more specialized topics. Those talks should be scanned to see if they are of interestß to you.",3
"DM-4087","10/14/2015 11:32:24","Attend DM boot camp ","Attend DM boot camp to learn more about DM stack, butler, and task. ",3
"DM-4092","10/14/2015 14:25:18","Update qserv for lastest xrootd","Small API change in latest xrootd, requires a parallel change to qserv.  Paves the way for DM-2334",0.5
"DM-4095","10/15/2015 14:05:04","Please port showVisitSkyMap.py from HSC","The HSC documentation at http://hsca.ipmu.jp/public/scripts/showVisitSkyMap.html includes a useful script for displaying the skymap and CCDs from a set of visits. It would be convenient if a version of this script was available in the LSST stack.",1
"DM-4098","10/16/2015 00:45:17","Update Trust Level of all LSST DM Staff to Level 4 via the API","It seems safe to update the Discourse trust level of all members of the LSSTDM group on community.lsst.org to Level 4 (full permissions). See https://meta.discourse.org/t/consequences-of-using-or-bypassing-trust-levels-for-company-organization-staff/34564?u=jsick    This should alleviate concerns that DM staff are being prevented from fully using the forum.    This ticket implements a small notebook to exercise the Discourse API to make this trust level migration possible.",0.5
"DM-4100","10/16/2015 14:18:50","Replace use of image <<= with [:] in python code","Replace all use of the afw image pixel copy operator {{<<=}} with {{\[:]}} in Python code.    See DM-4102 for the C++ version. These can be done independently.",2
"DM-4102","10/16/2015 16:21:44","Remove use of <<= from C++ code in our stack","Replace usage of deprecated Image operator {{<<=}} in C++ code with {{assign(rhs, bbox=Box2I(), origin=PARENT)}} as per RFC-102    Switch from [:] to assign pixels in Python code where an image view is created for the sole purpose of assigning pixels (thus turning 2-4 lines of code to one and eliminating the need to make a view).",3
"DM-4104","10/16/2015 16:28:23","Document that <<= is deprecated","Update our documentation to replace Image {{<<=}} pixel copy operator with {{[:] =}} for Python code and {{set(rhs)}} for C++ code (or whatever RFC-102 decides), with a note that {{<<=}} exists but is deprecated.",0
"DM-4105","10/16/2015 16:33:00","Update user documentation","{{ORDER BY}}, {{objectId IN}} and {{objectId BETWEEN}} predicates support have been improved, this should be documented.    ",1
"DM-4117","10/16/2015 18:24:17","Clean up lsst_stack_docs for preview","Improve the presentation of the New docs overall:    # Add a Creative Commons license  # Remove stub documents from the presentation  # Put READMEs in all doc directories to explain what content will go in them  # Clean up and update the source installation guide to reflect 11_0",0.5
"DM-4125","10/20/2015 09:37:29","pipe_tasks/examples/calibrateTask.py fails","The self contained example calibrateTask.py in pipe_tasks/examples/ fails when attempting to set field ""coord"" in refCat. Exact error message -     {code}  11:04:19-vish~/lsst/pipe_tasks (u/lauren/DM-3693)$ examples/calibrateTask.py --ds9  calibrate: installInitialPsf fwhm=5.40540540548 pixels; size=15 pixels  calibrate.repair: Identified 7 cosmic rays.  calibrate.detection: Detected 4 positive sources to 5 sigma.  calibrate.detection: Resubtracting the background after object detection  calibrate.initialMeasurement: Measuring 4 sources (4 parents, 0 children)   Traceback (most recent call last):    File ""examples/calibrateTask.py"", line 150, in <module>      run(display=args.ds9)    File ""examples/calibrateTask.py"", line 119, in run      result = calibrateTask.run(exposure)    File ""/home/vish/lsst/lsstsw/stack/Linux64/pipe_base/11.0-2-g8218aaa+5/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/vish/lsst/pipe_tasks/python/lsst/pipe/tasks/calibrate.py"", line 478, in run      astromRet = self.astrometry.run(exposure, sources1)    File ""examples/calibrateTask.py"", line 90, in run      m.set(""coord"", wcs.pixelToSky(s.getCentroid()))    File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 2372, in set      self.set(self.schema.find(key).key, value)    File ""/home/vish/lsst/lsstsw/stack/Linux64/afw/11.0-5-g97168e0+1/python/lsst/afw/table/tableLib.py"", line 1064, in find      raise KeyError(""Field '%s' not found in Schema."" % k)  KeyError: ""Field 'coord' not found in Schema.""  {code}    Note that {{wcs.pixelToSky(s.getCentroid())}} is set to {{Fk5Coord(15.007663073114244 * afwGeom.degrees, 1.0030133772819259 * afwGeom.degrees, 2000.0)}}",0.5
"DM-4129","10/20/2015 15:09:29","Database connection problems in daf_ingest","The DbAuth connection fallback in ingestCatalogTask passes the ""password"" keyword argument to {{MySQLdb.connect}} instead of ""passwd"", which fails. Also, the ""port"" command line argument isn't marked as an integer, causing port strings to be passed down to MySQLdb. This results in a type error. ",0.5
"DM-4133","10/20/2015 23:00:57","Change type of LTV1/2 from int to float when writing afw images to FITS","The LTV1/2 problem is originally my bug.  I used integer LTV1/2 in  {code}  afw/src/image/ExposureInfo.cc:    data.imageMetadata->set(""LTV1"", -xy0.getX());  afw/src/image/ExposureInfo.cc:    data.imageMetadata->set(""LTV2"", -xy0.getY());  {code}  whereas a more careful reading of the NOAO page [http://iraf.noao.edu/projects/ccdmosaic/imagedef/imagedef.html] introducing them includes floating point examples.    The fix is to cast the XY0 values to float.  I'm not sure if there'll be any side effects of fixing this, but if so they'll be obvious and trivial.  ",1
"DM-4137","10/21/2015 13:13:18","Update DECam CCDs gain, read noise, and saturation values","The values of DECam gain, read noise, and saturation value need to be updated.     This ticket is to update them in the Detector amplifier information, which is used in IsrTask.     Talked to Robert Gruendl. These values should take precedence over the values in the fits header. They seem stable and do not seem to vary with time.   ",1
"DM-4141","10/21/2015 20:47:59","cmdLineTasks should provide proper unix return codes","When a cmdLineTask fails it doesn't appear to return a non-0 exit code to the shell, making it hard to write shell scripts that chain commands together.    Please fix this.    E.g.  {code}  $ bin/assembleCoadd.py /lustre/Subaru/SSP --rerun yasuda/SSP3.8.5_20150810_cosmos:rhl/brightObjectMasks --id tract=9813 patch=5,5 filter=HSC-I --selectId ccd=0..103 visit=1238..1246:2 -c doMaskBrightObjects=True && echo ""Success""   ...  2015-10-22T02:44:13: assembleCoadd FATAL: Failed in task initialization:     Your Eups versions have changed.  The difference is:   ---   +++   @@ -48 +48 @@  -obs_subaru                     HSC-3.11.0a_hsc  /data1a/ana/products2014/Linux64/obs_subaru/HSC-3.11.0a_hsc  +obs_subaru                     LOCAL:/home/rhl/LSST/obs/subaru rev:ef3c892f clean-working-copy  @@ -55 +55 @@  -pipe_tasks                     LOCAL:/home/rhl/LSST/pipe/tasks-HSC-1342 rev:84b0f3c4 2 files changed, 5 insertions(+), 6 deletions(-)  +pipe_tasks                     LOCAL:/home/rhl/LSST/pipe/tasks-HSC-1342 rev:9e8ed18b 2 files changed, 47 insertions(+), 42 deletions(-)  @@ -60 +59,0 @@  -pyflakes                       git              /home/rhl/Src/pyflakes  Please run with --clobber-config to override    Success  {code}",1
"DM-4143","10/21/2015 23:52:51","Demonstrate using Breathe for Python & C++ API reference in New Docs","Demonstrate use of breathe for utilizing the existing Doxygen API documentation in the new Sphinx-based doc platform.",3
"DM-4145","10/22/2015 09:42:56","Reduce scons output in qserv","Yesterday AndyH expressed a valid concern that qserv prints too much info which makes it hard to find errors. By default scons prints whole command line for C++ compilation and linking which are quite long (~half screen depending on your screen size). Most of the time we don't need to see that, so it would be better to replace that with shorter messages like ""Compiling Something.cxx"" and have an option to print full command with --verbose option.",0.5
"DM-4151","10/22/2015 16:36:03","Search for uses of current afw.wcs in the stack","Search through the stack for all the uses of our Wcs implementation (Wcs, TanWcs, makeWcs, and any other hidden objects) and make a list of all of those uses (on Community for example). This list should note whether the usage is in C++ or python.",2
"DM-4158","10/23/2015 06:23:07","Allow configuring more statistical options for assembleCoadds.py ","The assembleCoadd.py task has a configuration option doSigmaClip which chooses between MEAN and MEANCLIP. Please replace this with an option to specify the algorithm to be used. Note that afwMath.stringToStatisticsProperty can be used to convert a string to an enum value. In particular, this would allow me to specify a MEDIAN stack if desired (e.g. to make pretty RGB images)  ",3
"DM-4160","10/23/2015 06:38:36","Unused variables in meas.algorithms.utils","Pyflakes 1.0.0 reports:  {code}  $ pyflakes-2.7 utils.py  utils.py:232: local variable 'chi2' is assigned to but never used  utils.py:481: local variable 'numCandidates' is assigned to but never used  utils.py:482: local variable 'numBasisFuncs' is assigned to but never used  utils.py:487: local variable 'ampGood' is assigned to but never used  utils.py:492: local variable 'ampBad' is assigned to but never used  {code}  In the best case, those variables are simply unnecessary, and they should be removed to simplify the code and avoid wasting time. Alternatively, it's possible that they ought to be used elsewhere in the calculation but have been omitted accidentally. Please establish this for each one, then either remove them or fix the rest of the code.",1
"DM-4165","10/23/2015 11:22:06","Take upstream boost 1.59 patch to squelch warnings for gcc 5.2.1","Under gcc 5.2.1, use of boost 1.59.0 produces a torrent of compiler warns from within boost headers about use of deprecated std::auto_ptr (see https://svn.boost.org/trac/boost/ticket/11622).    A patch for this is already committed upstream in boost.  It is proposed that we take this patch into the lsst t&p in interim until the next official boost release.",0.5
"DM-4194","10/26/2015 16:59:15","Python LogHandler does not pass logger name to log4cxx","Not sure how or why it happened, but presently Python LogHandler for lsst.log does not pass logger name to log4cxx layer and all messages from Python logging end in root logger. ",1
"DM-4202","10/27/2015 14:14:49","Revert temporary disabling of CModel in config override files","Revert the temporary disabling of CModel that relates to a bug noted in DM-4033 that was causing too many failures to test that processCcd (etc.) would run all the way to completion (most of the other fixes/updates related to the initial disabling in the multiband tasks have now been completed in DM-2977 & DM-3821).     Relevant files:  {code}  config/processCcd.py   config/forcedPhotCcd.py   config/forcedPhotCoadd.py   config/measureCoaddSources.py  {code}  ",1
"DM-4206","10/28/2015 10:13:52","wmgr should delete database from inventory when dropping it","When wmgr drops database it should also cleanup chunk inventory for that database.    ",2
"DM-4219","10/29/2015 15:12:53","Package capnproto for eups","Prototype is now getting to the point where a wire-protocol package like capnproto or protobuf is needed.  capnproto is the new hotness, and we're probably going to want to migrate qserv from protobuf->capnproto at some point.    This task is to go ahead and get capnproto packaged and published for use in the replication prototype.",2
"DM-4223","10/30/2015 07:15:26","IsrTask calls removeFringe in FringeTask but the method does not exist","The method {{removeFringe}} of {{FringeTask}} is called in {{IsrTask}} but there is no {{removeFringe}}.      Not sure if {{removeFringe}} was meant to be a place holder",1
"DM-4225","10/30/2015 11:07:43","Collect single-host performance data for secondary index","Run production-scale (billions of entries) tests on different index options, collect performance statistics for allocation (CPU, memory) and for queries.",3
"DM-4229","10/30/2015 11:31:52","Identify candidate technology for secondary index","Evaluate results of production-scale performance tests, both single and multiple host.  Identify the technology most likely to meet requirements, and estimate performance capability with respect to those requirements",3
"DM-4230","10/30/2015 12:30:38","Port HSC-1355: Improved fringe subtraction","[HSC-1355|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1355]: ""with this fix, we get much  better fringe subtraction"".",2
"DM-4232","11/02/2015 10:54:08","Variance is set after dark subtraction","In the default {{IsrTask}}, the variance is currently set after dark subtraction.  This means that photon noise from the dark is not included in the variance plane, which is incorrect.  The variance should be set after bias subtraction and before dark subtraction.    [~hchiang2] also points out (DM-4191) that the {{AssembleCcdTask}} with default parameters requires amplifier images with variance planes, even though the variance cannot be set properly until after full-frame bias subtraction.  I believe that {{AssembleCcdTask}} only requires a variance plane in the amp images because it does an ""effective gain"" calculation, but I suggest that this isn't very useful (an approximation of an approximation, and you're never going to use that information anyway because it's embedded in the variance plane with better fidelity).  I therefore suggest that this effective gain calculation be stripped out and that {{AssembleCcdTask}} not require variance planes.",5
"DM-4236","11/02/2015 14:00:55","Specify default output location for CmdLineTasks","When neither {{\--output}} or {{\--rerun}} is specified as an argument to a {{CmdLineTask}}, any output from that task appears to be written back to the input repository. Note the use of the term ""appears"": from a preliminary inspection of the code and documentation, it's not clear if this behaviour can be overridden e.g. by environment variables.    The HSC stack behaves differently, using {{$INPUT/rerun/$USER}} as a default output location. A [brief discussion|https://community.lsst.org/t/new-argument-parser-behavior-rerun-flag-introduction-discussion/345] suggests that this is the preferred behaviour.    Please update the LSST stack to match the HSC behaviour.",2
"DM-4237","11/02/2015 15:12:45","unable to upload images to nebula","I seem to be unable to upload an image to neblua from a URL via either horizon or the nova cli client.  The request seems to queue briefly and then reports a status of {{killed}}. Eg    {code}  glance image-create --name ""centos-7.1-vagrant"" --disk-format qcow2 --container-format bare --progress --copy-from http://sqre-kvm-images.s3.amazonaws.com/centos-7.1-x86_64 --is-public False --min-disk 8 --min-ram 1024  {code}",2
"DM-4238","11/02/2015 15:30:19","Fix integer casting error in numpy version 1.10 in obs subaru","Fix type casting in obs_subaru in lates numpy in obs_subaru",0.5
"DM-4239","11/02/2015 15:43:47","Identify Qserv areas affected by secondary index","Evaluate Qserv software for the Czars and workers to identify where an interface to the secondary index will be required for efficient operation.",5
"DM-4245","11/02/2015 16:30:48","Image Viewer memory leak","When reloading the same 500MB RAFT image into an image viewer (see the script below), it was discovered that single node Firefy server with 3G memory runs out of memory after ~15 reloads    Test case: keep reloading the html file with the following Javascript, creating an image viewer with 500MB image:    function onFireflyLoaded() {          var iv2= firefly.makeImageViewer(""plot"");          iv2.plot({               ""Title""      :""Example FITS Image'"",               ""ColorTable"" :""16"",               ""RangeValues"":firefly.serializeRangeValues(""Sigma"",-2,8,""Linear""),               ""URL""        :""http://localhost/demo/E000_RAFT_R01.fits""});  }    Follow up:    The bug was traced to java.awt.image.BufferedImage objects not being evicted from VIS_SHARED_MEM cache.    Further search showed that java.awt.image.BufferedImage (along with java.io.BufferedInputStream) is in src/firefly/java/edu/caltech/ipac/firefly/server/cache/resources/ignore_sizeof.txt, which lists the classes that have to be ignored when calculating the size of cache.    Testing on single node server (VIS_SHARED_MEM cache is not replicated), using [host:port]/fftools/admin/status page:    BEFORE (java.awt.image.BufferedImage was commented out in ignore_sizeof.txt)    After 14 reloads:  Memory    - Used                      :      3.7G    - Max                       :     3.55G    - Max Free                  :    488.0M    - Free Active               :    488.0M    - Total Active              :     3.55G     Caches:    VIS_SHARED_MEM @327294449   Statistics     : [  Size:15  Expired:0  Evicted:0  Hits:246  Hit-Ratio:NaN  Heap-Size:1120MB  ]  OUT OF MEMORY on next reload    AFTER THE CHANGE (Commented java.awt.image.BufferedImage in ignore_sizeof.txt)    After 36 reloads:  Memory    - Used                      :   1672.9M    - Max                       :     3.55G    - Max Free                  :   1968.0M    - Free Active               :   1468.0M    - Total Active              :      3.6G    Caches:    VIS_SHARED_MEM @201164543   Statistics     : [  Size:3  Expired:0  Evicted:34  Hits:659  Hit-Ratio:NaN  Heap-Size:1398MB  ]    ",2
"DM-4251","11/04/2015 10:29:07","Please include obs_subaru in CI","{{obs_subaru}} should be included in the CI system.",1
"DM-4252","11/04/2015 11:50:54","Create GitLFS Technical Note","Create a SQuaRE Technical Note describing the architecture of the GitLFS service implementation.",2
"DM-4256","11/04/2015 15:06:53","Sphinx support of sqr-001 technical note","Support the distribution of a technical note SQR-001    - Remove oxford comma in author list (documenteer)  - Solve issue where title is repeated if the title is included in the restructured text document  - Solve issue where name of the HTML document is README.html",1
"DM-4259","11/04/2015 15:45:42","Create a set of tests (or update the current ones) to facilitate refactoring of dipole measurement","This will create a test (not necessarily a unit test) that will simulate dipoles and measure them so that the measurement can be compared to truth values.  This may be simply refactoring the current tests.    This task should also include generating more generalizable utilities needed to create the dipoles and incorporating these and other test data into the stack so that they can be used in other studies.",5
"DM-4264","11/05/2015 13:06:45","SQuaRE supertask design meeting 2","Hold a teleconference design discussion with members of the SQuaRE team.  ",1
"DM-4266","11/05/2015 13:20:44","Should only read fringe data after checking the filter","The fringe subtraction is not necessarily performed if {{doFringe}} is True. It is only if the filter of the raw exposure is listed in config fringe.filters.      Fringe data should not be read unless the filter is indicated. There are likely no such filter data and it would cause runtime errors.      Seems related to changes from RFC-26 and DM-1299. ",8
"DM-4286","11/06/2015 22:35:34","Processing of DECAM COSMOS field - Part I","This story covers work on the verification plan done in October.     The DECAM Cosmos field was selected, however DECAM ISR is not available so the starting point for now is Community Pipeline reduced data. Have put the data through processCcdDecam and makeCoaddTempExp but had a number of failures that we have so far not been able to pin down. A list of user experience issues is being collated and [~frossie] will generate Summer 16 cycle stories to address those that fall within SQuaRE's defined scope of activities.     Story closed to fit within month, but work is ongoing. ",20
"DM-4295","11/09/2015 12:05:49","Run and document multinode integration tests on Openstack+Docker","Boot openstack machines using vagrant, then deploys docker images and finally launch multinodes tests.    FYI, lack of DNS on OpenStack Cloud cause problems, but a vagrant plugin seems to solve this.",8
"DM-4298","11/09/2015 15:13:16","want equiv of m1.xlarge flavor with smaller disk","I'd like to be able to build images with vcpus & ram from the {{m1.xlarge}} flavor that can be run on a {{m1.medium}} with it's smaller disk image, this would require a new flavor with 16GiB ram/8vcpus but only 40GiB of disk.  Something along the lines of:    {{openstack flavor create --ram 16384 --disk 40 --vcpus 8 ...}}    Is that possible?",2
"DM-4301","11/10/2015 12:17:50","Convert banner and menu to react/flux","Add flux data model to capture menu and banner information.  Convert banner and menu UI from gwt to react.  As part of this task, bring in Fetch API to simplify client/server interactions.",8
"DM-4303","11/10/2015 17:15:45","re-deploy lsstsw on Jenkins","Pandas was added to the bin/deploy script in lsstsw to support  sims development.  This has already been merged to master in 4b1d1a0fa.  The ticket is to ask that lsstsw be redeployed so the sims team can build branches that use pandas.",3
"DM-4304","11/10/2015 17:38:12","Add unit testing into gradle build for Firefly's server-side code","Add a test task to Firefly's common build script.  This can be used by any sub-project to run unit test.  Added unit testing to Jenkins continuous integration job to ensure new code does not break unit testing.",1
"DM-4305","11/10/2015 19:34:35","assembleCoadd broken","A recent update to assembleCoadd to bring over changes to do clipped coadds breaks coadd generation.  There are two specific problems.    1. There is an infinite recursion because of SafeClipAssembleCoaddTask calling its own constructor in the \_\_init\_\_ method.  2. The overridden assemble method does not adhere to the original assemble call signature, so when the default run method is called by ParseAndRun, it raises an exception.    Additionally I find the flow fairly confusing as the overridden assemble method is called by the default run method which then calls the default assemble method on the parent class.",1
"DM-4307","11/11/2015 08:24:15","Please add HSC tests to CI","In DM-3663 we (= [~price]) provided an integration test for processing HSC data through the stack with the intention that it should be integrated with the CI system.    Having this test available and regularly run would be enormously helpful with the HSC port -- we've already run into problems which it could have helped us avoid (DM-4305).",0.5
"DM-4310","11/11/2015 10:53:36","Missing Doxygen documentation","As of [2015-11-10 02:53.26|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_02.53.27/] there were 19 ""mainpages in subpackages"" available through Doxygen.    In the next build, [2015-11-10 21:16.19|https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_21.16.19/], most of them have vanished and we only provide links for {{ndarray}} and {{lsst::skymap}}.    As of filing this issue, they were still missing from the [latest build|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/].    Please bring them back!",1
"DM-4311","11/11/2015 11:54:03","Oct. on-going support to Camera team in UIUC","Attend UIUC weekly meeting and give support as needed. ",2
"DM-4312","11/11/2015 11:55:52","Nov. on-going support to Camera team in UIUC","Attend UIUC weekly meeting and give support as needed. ",2
"DM-4323","11/11/2015 16:56:17","Replace fitsthumb in obs_subaru (port HSC-1196)","{{fitsthumb}} is now obsolete; all the functionality we need is available in {{afw}}. Further, we want to drop it as a dependency to make the job of integrating {{obs_subaru}} with CI easier.",1
"DM-4329","11/12/2015 09:18:31","Coadd_utils tests should run and skip if afwdata is missing","Currently, the {{coadd_utils}} tests are completely skipped at the scons layer if afwdata can not be located. This is bad for two reasons:  1. Are there any tests that can be run even if afwdata is missing?.  2. When we switch to a proper test harness (e.g. DM-3901) an important metric is the number of tests executed compared with the number of tests skipped.  Each test file (or even each test) should determine itself whether it should be skipped based on afwdata availability. This should not be a global switch.",1
"DM-4344","11/13/2015 08:58:40","pipe_tasks 11.0-14-ga314014+5 fails a test on os/x 10.10.5","Running  {code}  SCONSFLAGS=""-j 6 opt=3"" eups distrib install pipe_tasks 11.0-14-ga314014+5  {code}  on my laptop running ox/x 10.10.5 results in a test failure:  {code}  F.  ======================================================================  FAIL: testEdge (__main__.interpolationTestCase)  Test that we can interpolate to the edge  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testInterpImageTask.py"", line 127, in testEdge      validateInterp(miInterp, useFallbackValueAtEdge, fallbackValue)    File ""tests/testInterpImageTask.py"", line 90, in validateInterp      self.assertAlmostEqual(val0, fallbackValue, 6)  AssertionError: 2.5515668 != 2.5515660972664267 within 6 places  {code}  ",0
"DM-4347","11/13/2015 10:31:46","dax_imgserv 2015_10.0 build error","{{2015_10.0}} has a build error under a current {{lsstsw/bin/deploy}} environment.  Current speculation is that this is related to the conda version of numpy being upgraded to {{1.10.1}}.",1
"DM-4349","11/13/2015 12:52:26","Fix publishing script async issue and add additional release notes.","Async command execution causes unpredictable and unreliable results.  Switches to synchronous where possible.  Also add additional description to the release notes.",2
"DM-4360","11/16/2015 12:10:02","obs_subaru fails to compile after DM-3200","Due to atypical calls in {{obs_subaru}}'s {{hsc/SConscript}} to run scripts in the {{bin}} directory, {{obs_subaru}} fails to compile after the changes made in DM-3200.",0.5
"DM-4362","11/16/2015 14:42:56","SuperTask phase 1 implementation","This story represents the implementation of the first part of the SuperTask framework design,",8
"DM-4366","11/17/2015 09:19:38","Improve overscan correction for DECam raw data","Currently, the default overscan correction from IsrTask is used for processing DECam raw data. Overscan subtraction is done one amplifier at a time.     However, a bias jump occurs due to the simultaneous readout of the smaller ancillary CCDs on DECam, some images show discontinuity in the y direction across one amplifier, as in the example screen shot.     This ticket is to improve overscan correction for DECam data so to mitigate this discontinuity in the ISR processing.    Arrangement of CCDs on DECam: http://www.ctio.noao.edu/noao/sites/default/files/DECam/DECamPixelOrientation.png      h3. More details:  There are 6 backplanes in the readout system, shown by the colors in DECamPixelOrientation.png. In raw data files, the CCD's backplane is noted in the header keyword ""FPA"".  Examination of some images suggests that science CCDs on orange and yellow backplanes show bias jump at 2098 pixels from the y readout. That is the y size of the focus CCDs.     h3. Actions:  For CCDs on the affected backplanes, divide the array into two pieces at the jump location, and do overscan correction on the upper and lower pieces separately.    ",5
"DM-4367","11/17/2015 09:46:41","Fix bug and add unit tests for PsfShapeletApprox ","We discovered during this Sprint that this plugin was giving us faulty values for all the models except for SingleGaussian.  I will fix that bug on this issue.    Obviously, a better unit test would have caught this.  I am adding a DoubleGaussian unit test, plus a test that the default models provide different results.  Also a timing test for all the models, as we do not really have enough information about the performance of the shapelet approximation.  ",3
"DM-4370","11/17/2015 12:03:47","Migrate testdata for DECam from disk to git-lfs","There is a package full with test data for the obs_decam.  It is an eups package currently, but not a git repository.  I would like to migrate that into our hosted git-lfs so the obs_decam package can be built by Jenkins.  [~jmatt] I'm hoping you would be willing to handle this for me.  If not I can find somebody else.  Thanks!",0.5
"DM-4373","11/17/2015 13:37:36","HSC backport: Add tract conveniences","This is a port of [HSC-715: Add tract conveniences|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-715].  Here is the original description:    {panel}  In regular HSC survey processing, we'll run with a ""rings"" skymap to cover the entire survey area. meas_mosaic does not currently efficiently or conveniently iterate over tracts. For example:  {code}  mosaic.py /tigress/HSC/HSC --rerun price/cosmos --id field=SSP_UDEEP_COSMOS filter=HSC-R  {code}  Note the lack of a tract in the --id specifier — we want to iterate over all tracts. This is not currently possible. Instead, if we do not know the tract of interest (which the user should not be required to know), we have to iterate over all the tracts (e.g., tract=0..12345), but the user should not be required to know the number of tracts, and this is slow (and possibly memory-hungry: currently consuming 11GB on tiger3 just for 12 exposures).  We need an efficient mechanism to iterate over all tracts by not specifying any tract on the command-line.  {panel}    As this functionality was added specifically for {{meas_mosaic}}, it was going to be ported as part of DM-2674.  Due to a recent desire to use this functionality, this ticket will be ported here.",1
"DM-4375","11/17/2015 17:01:00","A slimmer testdata_decam","Before this ticket, the files in {{testdata_decam}} are as they are downloaded from the archive.  Some are MEF, and the total is a bit big (1.2G).    I made a trimmed down version of {{testdata_decam}}, 109M and available here:  [https://github.com/hchiang2/testdata_decam.git]  I trimmed it down by only saving the primary HDU and one data HDU.  However the unit tests in {{getRaw.py}} become less meaningful and I am not sure if we really want to do this, because some complexities of DECam files are about the MEF. {{getRaw.py}} tests Butler retrieval of multiple dataset types, in particular tests if the correct HDU is retrieved.     Nonetheless, {{getRaw.py}} can pass  (with branch u/hfc/DM-4375 of obs_decam)    Note: the old testdata_decam still live on lsst-dev:/lsst8/testdata_decam/",2
"DM-4381","11/18/2015 10:33:53","""SHUTOFF"" nebula instances consume core/ramIt  quota","It appears that halted/shutoff instances have no effect on resource quota usage.  Eg:    {code:java}  $ openstack server list  +--------------------------------------+-----------------------+-------------------+----------------------------------------+  | ID                                   | Name                  | Status            | Networks                               |  +--------------------------------------+-----------------------+-------------------+----------------------------------------+  ...  | 1956c6d0-8aec-4f42-a781-8a68fd10179d | el7-jhoblitt          | SHUTOFF           | LSST-net=172.16.1.171, 141.142.208.150 |  ...  $ nova absolute-limits  +--------------------+--------+--------+  | Name               | Used   | Max    |  +--------------------+--------+--------+  | Cores              | 141    | 150    |  | FloatingIps        | 0      | 10     |  | ImageMeta          | -      | 128    |  | Instances          | 44     | 100    |  | Keypairs           | -      | 100    |  | Personality        | -      | 5      |  | Personality Size   | -      | 10240  |  | RAM                | 342016 | 400000 |  | SecurityGroupRules | -      | 20     |  | SecurityGroups     | 1      | 10     |  | Server Meta        | -      | 128    |  | ServerGroupMembers | -      | 10     |  | ServerGroups       | 0      | 10     |  +--------------------+--------+--------+  $ openstack server delete 1956c6d0-8aec-4f42-a781-8a68fd10179d  $ nova absolute-limits  +--------------------+--------+--------+  | Name               | Used   | Max    |  +--------------------+--------+--------+  | Cores              | 133    | 150    |  | FloatingIps        | 0      | 10     |  | ImageMeta          | -      | 128    |  | Instances          | 43     | 100    |  | Keypairs           | -      | 100    |  | Personality        | -      | 5      |  | Personality Size   | -      | 10240  |  | RAM                | 325632 | 400000 |  | SecurityGroupRules | -      | 20     |  | SecurityGroups     | 1      | 10     |  | Server Meta        | -      | 128    |  | ServerGroupMembers | -      | 10     |  | ServerGroups       | 0      | 10     |  +--------------------+--------+--------+    {code}  ",2
"DM-4383","11/18/2015 15:33:46","Avoid restarting czar when empty chunk list changes","Currently czar caches empty chunk list after it reads the list from file. This complicates things when we need to update the list, integration test for example has to restart czar process after it loads new data to make sure that czar updates its cached list. Would be nice to have simpler mechanism to resetting cached list in czar without restarting it completely. It could be done via special query (abusing FLUSH for example) or via sending signal (problematic if czar runs remotely).    This can be potentially useful even after we replace empty chunk list file with some other mechanism as I expect that cache will stay around even for that.",2
"DM-4385","11/18/2015 17:35:42","Remove stringToAny from the utils package","I accidentally left stringToAny in Utils.cc when implementing DM-2635, even though I removed it from Utils.h and it is not used anywhere. Remove it and mark RFC-47 Done.",0
"DM-4386","11/19/2015 10:51:31","Clean up ProcessCcdDecam","ProcessCcdDecam needs some cleanup:  * {{run}} method simply delegates to the base class  * {{propagateCalibFlags}} is a no-op (deliberately in {{cab69086}}, need to explore if the original problem still exists)  * The config overrides (in config/processCcdDecam.py):  ** Uses the catalog star selector, which isn't wise given the current heterogeneity of reference catalogs.  ** Sets the background {{undersampleStype}} to {{REDUCE_INTERP_ORDER}}, which is the default.",1
"DM-4387","11/19/2015 12:08:06","Skymap fails tests on testFindTractPatchList","When skymap is built and healpy is loaded, {{testFindTractPatchList}} fails with:    {quote}======================================================================  FAIL: Test findTractPatchList  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 245, in testFindTractPatchList      self.assertClosestTractPatchList(skyMap, [tractInfo.getCtrCoord()], tractId)    File ""/Users/ctslater/lsstsw/build/skymap/tests/SkyMapTestCase.py"", line 284, in assertClosestTractPatchList      tractPatchList = skyMap.findClosestTractPatchList(coordList)    File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/baseSkyMap.py"", line 146, in findClosestTractPatchList      tractInfo = self.findTract(coord)    File ""/Users/ctslater/lsstsw/build/skymap/python/lsst/skymap/healpixSkyMap.py"", line 97, in findTract      index = healpy.ang2pix(self._nside, theta, phi, nest=self.config.nest)    File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 367, in ang2pix      check_theta_valid(theta)    File ""/Users/ctslater/lsstsw/stack/DarwinX86/healpy/1.8.1+12/lib/python/healpy-1.8.1-py2.7-macosx-10.5-x86_64.egg/healpy/pixelfunc.py"", line 110, in check_theta_valid      assert (np.asarray(theta) >= 0).all() & (np.asarray(theta) <= np.pi + 1e-5).all(), ""Theta is defined between 0 and pi""  AssertionError: Theta is defined between 0 and pi{quote}    This was missed during regular CI testing since healpy is not normally setup. ",1
"DM-4391","11/20/2015 10:32:09","Update testCoadds.py to accommodate changes in DM-2915","As of DM-2915, the config setting:  {code}self.measurement.plugins['base_PixelFlags'].masksFpAnywhere = ['CLIPPED']{code}  is set as a default for {{MeasureMergedCoaddSourcesTask}}.  However, this *CLIPPED* mask plane only exists if a given coadd was created using the newly implemented {{SafeClipAssembleCoaddTask}}.  If a coadd was built using {{AssembleCoaddTask}}, the *CLIPPED* mask plane is not present, so the above default must be overridden to exclude it when using {{MeasureMergedCoaddSourcesTask}}.  This is the case for the mock coadd that is assembled in the unittest code in {{testCoadds.py}}, so the config needs to be set for the test to run properly.    Note that the associated tests for {{SafeClipAssembleCoaddTask}} will be added as part of DM-4209.",0.5
"DM-4395","11/20/2015 12:50:19","Update cmsd configuration for multi-node tests","A particular cmsd configuration parameter prefixes a hardcoded path for QueryResource, which needs to be removed. This seems to appear only during multi-node tests.",1
"DM-4396","11/20/2015 12:55:17","ctrl_execute test fails to find test binary","There's a test in ctrl_execute that exercises the bin/dagIdInfo.py test program.   Since the rewrite_shebang rewrites happen after the tests are executed, the test that looks for the bin/dagIdInfo.py binary fails, since it's not there before the tests execute.",0.5
"DM-4398","11/20/2015 14:44:48","Fix regexp for gcc48","DM-2622 inttoduced some regexes which raise exceptions when built with gcc48 (e.g. on centos7). gcc48 support for regexes is generally broken, so it's better to replace that with boost regexes.",1
"DM-4399","11/20/2015 15:15:58","ctrl_execute test fails under El Capitan","The test/testDagIdInfo.py because it runs a script from bin.src, rather than bin.   This test needs to be rewritten.",1
"DM-4402","11/23/2015 12:06:44","Experiment with light-weight SQL databases for secondary index","Evaluate the use of light-weight SQL, such as InnoDB, TokuDB (now Kyoto Cabinet), or RocksDB to create and manage the secondary index.",8
"DM-4408","11/24/2015 10:02:54","HSC backport: fix memory leak in afw:geom:polygon","This is a backport of a bug fix that got included as part of [HSC-1311|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1311].  It is not related to that issue in particular, so is being ported here as an isolated bug fix.    {panel}  Original commit message:  pprice@tiger-sumire:/tigress/pprice/hsc-1311/afw (tickets/HSC-1311=) $ git sub  commit 55ad42d37fd1346f8ebc11e4077366dff4eaa87b  Author: Paul Price <price@astro.princeton.edu>  Date:   Wed Oct 21 10:59:56 2015 -0400         imageLib: import polygonLib to prevent memory leak             When doing ""exposure.getInfo().getValidPolygon()"", was getting:             swig/python detected a memory leak of type 'boost::shared_ptr< lsst::afw::geom::polygon::Polygon > *', no destructor found.             This was due to the polygonLib not being imported in imageLib.      Using polygonLib in imageLib then requires adding polygon.h to all      the swig interface files that use imageLib.i.      examples/testSpatialCellLib.i              | 1 +   python/lsst/afw/cameraGeom/cameraGeomLib.i | 1 +   python/lsst/afw/detection/detectionLib.i   | 1 +   python/lsst/afw/display/displayLib.i       | 1 +   python/lsst/afw/geom/polygon/Polygon.i     | 1 +   python/lsst/afw/image/imageLib.i           | 2 ++   python/lsst/afw/math/detail/detailLib.i    | 1 +   python/lsst/afw/math/mathLib.i             | 1 +   8 files changed, 9 insertions(+)  {panel}",0.5
"DM-4410","11/24/2015 12:45:53","Port detection task footprint growth changes from HSC","In hsc the default behavior for the detection task is to updated footprints with a footprint which has been grown by the psf. This behavior needs to be ported to LSST, as some source records have footprints which are too small. When making this change, the new default needs to be overridden for the calibrateTask, as it needs the original size.    The port includes 8e9fb159a3227f848e0db1ecacf7819599f1c03b from meas_algorithms and 8bf0f4a44c924259d9eefbd109aadec7d839e0f2 from pipe_tasks",5
"DM-4421","11/25/2015 10:35:24","faulty assumption about order dependency in ctrl_event unit tests","A recent change to daf_base uncovered a couple of faulty tests in ctrl_events that incorrectly assumes the order in which assumed the order in which data in a PropertySet would be received.   We can't assume which order these values will be put into the property set, and therefore into the list retrieved from the Event object.",0.5
"DM-4431","11/25/2015 16:47:08","setup mechanism to measure the portal performance","Setup the method to measure the query response time:   # query sent to the data provider from client (from portal to FF server, to data provider)   # result returns from data provider (FF server gets data, prepare the data for portal)   # result displayed in the client    Setup the method to measure image preparation in Firefly server:   * measure the time needed to prepare the image (generate the image in PNG or other suitable format) for client display.   * Track/identify the reasons if performance is not satisfactory   * Make plan for design/code change    Setup the method to measure image/plot rendering in Firefly client (portal):   * Measure the time needed to render the image after the data received by the client for display.   * Measure the time needed to generate the 2D plot after the data is received by the client   * Track/identify the reasons if the performance is not satisfactory   * Make plan for design/code change",40
"DM-4438","11/26/2015 08:32:14","Replace sed with stronger template engine in docker scripts","Dockerfile are generated using templates and sed, this should be strengthened.",2
"DM-4440","11/26/2015 08:41:45","Remove QSW_RESULTPATH and XROOTD_RUN_DIR if useless","These parameters may be useless (see DM-4395). If yes they can be removed to simplify configuration procedure.",2
"DM-4443","11/30/2015 06:58:16","Please document the --rerun option","DM-3371 adds the {{--rerun}} option to command line tasks. The help for this option reads:  {quote}  rerun name: sets OUTPUT to ROOT/rerun/OUTPUT; optionally sets ROOT to ROOT/rerun/INPUT  {quote}  While essentially correct, that's not particularly helpful in understanding what's actually going on here. A motivation and description of this functionality is available in RFC-95: please ensure that, or some variation of it, is included in the stack documentation.",1
"DM-4451","11/30/2015 11:42:24","F17 Qserv Disconnected Queries","* Design and implement *basic* system for determining whether particular query is synchronous or asynchronous. The complete version will come through DM-1490. Note that this work is related to shared scans (e.g., we need to know what scans we have running)  * Design SQL API for starting and interacting with async queries.  * Modify Qserv to support async queries (starting, getting status, retrieving results)    Note, async queries are indirectly related to authentication (users should not see each other' async queries).    Deliverable: Qserv that accepts and executes queries asynchronously, and allows users to retrieve results.",20
"DM-4454","11/30/2015 12:35:47","Fix multiple patch catalog sorting for forcedPhotCcd.py","{{forcedPhotCcd.py}} is currently broken due to the requirement of the {{lsst.afw.table.getChildren()}} function that the *SourceCatalog* is sorted by the parent key (i.e. {{lsst.afw.table.SourceTable.getParentKey()}}).  This occurs naturally in the case of *SourceCatalogs* produced by the detection and deblending tasks, but it may not be true when concatenating multiple such catalogs.  This is indeed the case for {{forcedPhotCcd.py}} as a given CCD can be overlapped by multiple patches, thus requiring a concatenation of the reference catalogs of all overlapping patches.    There two places in the running of {{forcedPhotCcd.py}} where calls to {{getChildren()}} can cause a failure: one in the {{subset()}} function in {{references.py}}, and the other in the {{run}} function of *SingleFrameMeasurementTask* in {{sfm.py}}.",2
"DM-4457","11/30/2015 15:55:02","Investigate MemSQL","Take a look at the MemSQL distributed database.",8
"DM-4503","12/01/2015 22:43:18","FITS Visualizer porting: selecting points of catalog from image view, showing selected points","able to draw a rectangle on the image, and select the catalog entries overlaid on the image",5
"DM-4504","12/01/2015 22:44:20","FITS Visualizer porting: Image Select Panel/Dialog","Converting the image select dialog/panel is a very big job and should be to be broken up into several tickets: Each ticket should reference this ticket as the base.    Panel includes the following:  * issa, 2mass, wise, dss, sdss tabs  * file upload tab, upload widget might have to be written  * url tab  * blank image tab  * target info reusable widget  * 3 color support - any panel should show for 3 times, for read, green, and blue in 3 color mode  * must be able to appear in a panel or dialog  * must add or modify a plot  * Allow to create version with most or less than the standard tabs. example - see existing wise 3 color or finder chart 3 color  * A plot might need to be tied to specific type of image select dialog, we need a way to tie a plotId to and non-standard image select panel.",1
"DM-4510","12/02/2015 10:55:36","makeDocs uses old style python","{{makeDocs}} is written in python 2.4 style. This ticket is for updating it to python 2.7.",0.5
"DM-4511","12/02/2015 11:39:08","Improve reStructuredText documentation","Enhance docs by covering    - Images as links  - Table spans  - Abbreviations  - :file: semantics, etc.",2
"DM-4515","12/02/2015 13:48:28","Flag out the glowing edges of DECam CCDs","Pixels near the edges of the DECam CCDs are bigger/brighter and correcting them is not trivial. One way to move forward is to mask them out.      DESDM and CP mask 15 pixels on each edge.  The cut was later raised to 25 pixels, with the inner 10 pixels flagged as SUSPECT.  ",5
"DM-4523","12/03/2015 03:12:05","Fix startup.py inside Docker container","qserv tag should be replace with qserv_latest",0.5
"DM-4529","12/03/2015 10:17:24","Compilation errors from CLang (Apple LLVM 7.0) in XCode 7 on MacOSX","Compiling on MacOSX Yosemite with XCode 7, a number of files fail compilation.  ----  {{core/modules/util/EventThread.h,cc}} fails because {{uint}} is used as a data type.  This is non-standard (though some compilers support it), and should be replaced with {{unsigned int}}.  ----  {{core/modules/wbase/SendChannel.h,cc}} fails because {{#include <functional>}} is missing.  ----  {{core/modules/wsched/ChunkState.cc}} fails because {{#include <iostream>}} is missing.  ----  {{build/qmeta/qmetaLib_wrap.cc}} (generated by SWIG) fails with many errors because the {{typedef unsigned long int uint64_t}} included in {{qmetaLib.i}} conflicts with MacOSX's typedef of it as {{unsigned long long}}.",1
"DM-4534","12/03/2015 14:56:34","shellcheck linting of lsstsw bash scripts","This issue is to recover a branch from DM-4113 that was not merged due to issues with installing shellcheck under travis.",1
"DM-4544","12/04/2015 16:27:33","Revisit short and long term plans for butler","Revisit short and long term requirements and needs and capture it through stories.",5
"DM-4556","12/05/2015 16:54:40","Fix docker workflow","Some issues where discovered while trying to package DM-2699 in Docker (for IN2P3 cluster deployment), they're fixed here.    - apt-get update times out: why?  - git clone then pull is too weak (if building the first clone fails, pull never occurs) => step merged  - eupspkg -er build creates lib/python in /qserv/stack/.../qserv/... and next install can't remove it for unknow reason => build and install merged.",2
"DM-4564","12/07/2015 09:55:38","Convert basic table functionalities to JS.","Task includes server-side json conversion, data modeling, and a simple React table for presentation.",20
"DM-4572","12/07/2015 10:46:58","Table (JS): table options","This task is composed of:  - adding table options panel to TablePanel.  - providing features:    - show/hide units in header    - show/hide columns, reset to defaults, etc    - page size",5
"DM-4574","12/07/2015 10:49:37","Table (JS): text view","This task is composed of:  - adding text view option to TablePanel",2
"DM-4576","12/07/2015 11:03:46","XY Scatter Plot (JS) ","Implement basic scatter plot widget using react-highcharts library",8
"DM-4580","12/07/2015 11:18:36","XY Plot view of a table (JS) - Toolbar","Toolbar, which toggles plot options, selection and filter buttons    Extra:   - handling zoom from the toolbar rather than using built-in zoom  - ability to switch between histogram and scatter plot view",8
"DM-4582","12/07/2015 11:30:51","XY Plot View of a table (JS) - selection support","Show/change selected/highlighted points. Ideally, this should be done without redrawing the whole plot. ",8
"DM-4583","12/07/2015 11:38:15","SUIT: search returning images in a directory","- Create a sample search processor, which returns images in a given directory.  - It should be using an external python task  - Update search form configuration to use this search processor to return image metadata",2
"DM-4591","12/07/2015 13:05:45","GWT conversion: System notifications","This task is composed of:  - adding notification panel to the application  - convert server-side code to use messaging for notifications  - use messaging on client-side to handle notifications  - creating action, action creator, and reducing functions  - depends on DM-4578 Integrate websocket messaging into flux  ",3
"DM-4596","12/07/2015 14:15:02","Remove deprecated versions of warpExposure and warpImage","afw.math supports two templated variants of warpExposure and warpImage, one that takes a warping control and the other which does not. The latter have been deprecated for a long time and are no longer used. I think it is time to remove them.",1
"DM-4603","12/08/2015 10:37:07","sconsUtils tests should depend on shebang target","Some tests rely on code in the {{bin}} directory. Whilst these tests have been modified to use {{bin.src}} the general feeling is that the test code should be able to rely upon the {{shebang}} target having been executed before they are run.",0.5
"DM-4609","12/09/2015 13:15:06","Partition package should use the standard package layout","The partition package does not build on OS X El Capitan because the package is not laid out in the standard manner and whilst {{sconsUtils}} is used most of the default behaviors are over-ridden. This means that fixes implemented for DM-3200 do not migrate over to {{partition}}. I think the best approach would be to reorganize the package so that it does build in the normal way.",1
"DM-4617","12/09/2015 19:44:22","Send all chunk-queries to primary copy of the chunk","We are planning to distribute chunks / replicas across worker nodes such that each node will have a mix of primary copies for some chunks, and backup copies for some chunks. While doing shared scan, we are going to always rely on the primary chunks (e.g., all queries that need a given chunk should be sent to the same machine so that we read that chunks only once on one node). This story involves tweaking xrootd to ensure we don't send chunk-queries to nodes hosting non-primary copies.",5
"DM-4631","12/11/2015 08:58:10","Create IDL pipeline workflow for DRP processing - processCcdDecam","For the verification datasets work we need the ability to run a dataset all the way through DRP processing that takes advantage of many cores (orchestration), keeps track of successful and failed dataIDs, creates individual log files, and creates helpful QA plots/metrics/webpages.  Nidever will use his PHOTRED IDL workflow and rewrite it for the stack.  The first step is processCcdDecam.",5
"DM-4643","12/14/2015 11:47:13","Add utility function to handle client-side download requests.","Create utility function to handle client-side download requests.  It needs to be done in a way that does not mess with history and current page state.",1
"DM-4648","12/14/2015 16:25:39","Support sqlalchemy use with qserv","When one tries to connect to qserv using sqlalchemy there is an exception generated currently:  {noformat}  $ python -c 'import sqlalchemy; sqlalchemy.create_engine(""mysql+mysqldb://qsmaster@127.0.0.1:4040/test"").connect()'  /u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py:298: SAWarning: Exception attempting to detect unicode returns: InterfaceError(""(_mysql_exceptions.InterfaceError) (-1, 'error totally whack')"",)    ""detect unicode returns: %r"" % de)  Traceback (most recent call last):    File ""<string>"", line 1, in <module>    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2018, in connect      return self._connection_cls(self, **kwargs)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 72, in __init__      if connection is not None else engine.raw_connection()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2104, in raw_connection      self.pool.unique_connection, _connection)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2078, in _wrap_pool_connect      e, dialect, self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1405, in _handle_dbapi_exception_noconnection      exc_info    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 199, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 2074, in _wrap_pool_connect      return fn()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 318, in unique_connection      return _ConnectionFairy._checkout(self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 713, in _checkout      fairy = _ConnectionRecord.checkout(pool)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 480, in checkout      rec = pool._do_get()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1060, in _do_get      self._dec_overflow()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 60, in __exit__      compat.reraise(exc_type, exc_value, exc_tb)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 1057, in _do_get      return self._create_connection()    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 323, in _create_connection      return _ConnectionRecord(self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/pool.py"", line 454, in __init__      exec_once(self.connection, self)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 246, in exec_once      self(*args, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/event/attr.py"", line 256, in __call__      fn(*args, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 1312, in go      return once_fn(*arg, **kw)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/strategies.py"", line 165, in first_connect      dialect.initialize(c)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/dialects/mysql/base.py"", line 2626, in initialize      default.DefaultDialect.initialize(self, connection)    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 256, in initialize      self._check_unicode_description(connection):    File ""/u2/salnikov/STACK/Linux64/sqlalchemy/2015_10.0/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 343, in _check_unicode_description      ]).compile(dialect=self)    File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/cursors.py"", line 174, in execute      self.errorhandler(self, exc, value)    File ""/u2/salnikov/STACK/Linux64/mysqlpython/1.2.3.lsst1/lib/python/MySQL_python-1.2.3-py2.7-linux-x86_64.egg/MySQLdb/connections.py"", line 36, in defaulterrorhandler      raise errorclass, errorvalue  sqlalchemy.exc.InterfaceError: (_mysql_exceptions.InterfaceError) (-1, 'error totally whack')  {noformat}    The reason for that is that sqlalchemy generate few SELECT queries to figure out unicode support by the engine, and those selects are passed to qserv which cannot parse them. Here is the list of SELECTs which appears in proxy log:  {code:sql}  SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1  SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1  SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8) COLLATE utf8_bin AS anon_1  SELECT 'x' AS some_label  {code}",3
"DM-4652","12/14/2015 17:10:24","CI debugging","diagnosing build failures and refreshing build slaves",1
"DM-4656","12/15/2015 00:12:29","Port code style guidelines to new DM Developer Guide","Verbatim port of DM Coding style guidelines to Sphinx doc platform from Confluence.    - https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy  - https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666 and contents    I’m unclear whether these pages should be included:    - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++ ‘using’)  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190 (how to use C++ templates)  - https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399 (C++11/14; which should seem to belong in the code style guide)    Any temptation to amend and update the style guideline content will be avoided.",5
"DM-4667","12/15/2015 14:25:03","Improve sphgeom documentation","Per RFC-117, the sphgeom package needs decent overview documentation, linked from the top-level README.md. The doxygen output should also be reviewed.",2
"DM-4677","12/15/2015 18:33:13","Design Interfaces for Memory Management for Shared Scans","Part of the shared scans involve memory management - a system that will be used by Qserv that will manage memory allocation / pin chunks in memory. This story involves designing the API between Qserv and the memory management system.  ",8
"DM-4679","12/16/2015 09:56:40","work with database team to exercise all the APIs for data access (F16)","SUI will continue to work with database team to exercise all the APIs for data access. All known issues should be worked out in S16 cycle.",40
"DM-4688","12/16/2015 19:25:12","Changed the implementation of HistogramProcessorTest due to the minor change about the algorithm in the HistogramProcessor","In Histogram, when the data points fall on the bin edges,  the following rules are used:  #  For each bin, it contains the data points fall inside the bin and the data point fall on the left edge.  For example, if binSize=2, the bin[0] is in the range of [0,2].  The data value 0 is in bin[0] .  #  For each bin, the data point falls on the right edge is not included in the number point count. For example if binSize=2, the bin[0] is having the range of [0,2].  The data value 0 is in bin[0] but the data value 2 is not in the bin[0].  # For the last bin, the data points fall inside the bin or fall on the left or right bin are counted as the number of bin points.    The last rule is newly introduced.      ",2
"DM-4704","12/17/2015 17:28:46","Qserv integration tests fail on CentOS7 with gcc 4.8.5","The version of gcc that ships with CentOS7, {{gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)}}, appears to miscompile the qserv worker source in a way that makes it impossible to actually run queries. Installing {{devtoolset-3-toolchain}} and {{devtoolset-3-perftools}} via {{yum}} to get gcc 4.9 resolves the issue.",2
"DM-4705","12/17/2015 19:54:09","qdisp/testQDisp fails with mariadb","Fabrice fried to build qserv with mariadb and it caused failure in one of the unit test: qdisp/testQDisp with the message:  {noformat}  pure virtual method called  terminate called without an active exception  {noformat}    Runnig it with GDB it' obvious that there is a problem with resource lifetime management in qdisp/testQDisp.cc. The problem is that XrdSsiSessionMock is destroyed sooner than other objects that use it.     One way to resolve this problem is to instantiate XrdSsiSessionMock earlier than other objects that use it (to reverse the order of destructors), possibly make it a global instance.    Big mystery here is how mariadb could trigger this interesting behavior and why did not we see this earlier.",1
"DM-4706","12/17/2015 22:28:39","Rerun and create a repository for CFHT astrometry test.","Understand, re-rerun, and recreate clean version of [~boutigny] 's CFHT astrometry test for the astrometry RMS for two sample CFHT observations.  This test is on the NCSA machines in  /lsst8/boutigny/valid_cfht    Create a repository for this test with an eye toward it becoming integrated in a validation suite for the stack.        ",1
"DM-4708","12/17/2015 22:37:20","Integrate astrometry test into SDSS demo lsst_dm_stack_demo","Incorporate the astrometry test as an optional component in lsst_dm_stack_demo.    This is chosen because lsst_dm_stack_demo currently serves as the very loose stack validation and understanding how to do astrometric repeatibility testing in this demo will help explore how it would make sense to put in a fuller CFHT validation test of the DM stack.",0.5
"DM-4710","12/18/2015 08:22:48","host identification info needs to be part of log message","The EventAppender needs to add host identification (host/process/id) information to the log message it transmits.   This was inadvertently left out.",3
"DM-4711","12/18/2015 09:51:04","Edit testdata_cfht to pass obs_cfht unit tests","This ticket covers the first half of the issues in DM-2917.     {{testdata_cfht}} was left unedited while some past changes in {{obs_cfht}} {{MegacamMapper}} required coordinated changes.  The goal of this ticket is to simply pass the unit tests currently in {{obs_cfht}}.   ",1
"DM-4716","12/20/2015 12:18:20","Track down reason for slow performance when running many jobs of processCcdDEcam on bambam","During the processing of the COSMOS data for the verification dataset work I ran many jobs of processCcdDecam.py on the new linux server, bambam.  The performance was very slow, 4x longer than running a single job at a time.  Figure out what is going on.",2
"DM-4722","12/21/2015 13:43:34","File tickets for list of stack deficiencies and suggested upgrades","K-T suggested that I take my list of ""stack deficiencies and suggested improvements"" [https://confluence.lsstcorp.org/display/SQRE/Stack+Deficiencies+and+Suggested+Upgrades] on confluence and (with Tim J.'s help) create tickets for each item (as much as possible) so that the work could be scheduled.  ",3
"DM-4728","12/22/2015 06:22:03","Doxygen package fails to build with flex 2.6","To wit:    {code}  $ flex --version  flex 2.6.0    $ bash newinstall.sh    LSST Software Stack Builder  [...stuff...]  eups distrib: Failed to build doxygen-1.8.5.eupspkg: Command:   source /Users/jds/Projects/Astronomy/LSST/stack/eups/bin/setups.sh; export EUPS_PATH=/Users/jds/Projects/Astronomy/LSST/stack; (/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.sh) >> /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log 2>&1 4>/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.msg  exited with code 252    $ grep error /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/doxygen-1.8.5/build.log  commentscan.l:1064:55: error: use of undeclared identifier 'yy_current_buffer'  commentscan.l:1126:58: error: use of undeclared identifier 'yy_current_buffer'  {code}    Builds fine using {{flex 2.5.35 Apple(flex-31)}}.",1
"DM-4729","12/23/2015 10:42:51","HSC backport: Add functions to generate 'unpacked matches' in a Catalog","The qa analysis script under development (see DM-4393) calls to HSC {{hscPipeBase}}'s [matches.py|https://github.com/HyperSuprime-Cam/hscPipeBase/blob/master/python/hsc/pipe/base/matches.py] which adds functions to generate ""unpacked matches"" in a Catalog (and vice versa).  It will be ported into {{lsst.afw.table}}.    The port includes following HSC commits:  *Add functions to generate 'unpacked matches' in a Catalog.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/210fcdc6e1d19219e2d9365adeefd9289b2e1186    *Adding check to prevent more obscure error.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/344a96de741cd5aafb5e368f7fa59fa248305af5    *Some little error handling helps.*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/61cc053b873d42802581adff8cbbdb52a348879e  (from branch: {{stage-ncsa-3}})    *matches: add ArrayI to list of field types that require a size*  https://github.com/HyperSuprime-Cam/hscPipeBase/commit/d4ccd11d8afbcdd9cf0b35eba948cca4b5d09ba5  (from branch {{tickets/HSC-1228}})    Please also include a unittest.",3
"DM-4731","12/23/2015 15:56:58","Add labels to qa analysis plots for better interpretation","The plots output by the qa analysis script (see DM-4393) currently do not display any information regarding the selection/rejection criteria used in making the figures and computing the basic statistics.  This includes magnitude and clipping thresholds.  This information should be added to each plot such that the figures can be interpreted properly.",2
"DM-4732","12/23/2015 21:52:31","butler parentSearch incorrectly returns '[]' instead of 'None' in one case.","The contract for `daf.butlerUtils.CameraMapping.parentSearch` states that `None` will be returned if no matches are found.    But in one of the options  `            if os.path.realpath(pathPrefix) != os.path.realpath(root):`  the code says `return []` instead of `return None`.    This is then not caught properly by `daf.butlerUtils.mapping.map`, which was checking just for the return value `is not None`.      The behavior of parentSearch is clearly an error (it's againt the documentation for the function).    I would also suggest that `daf.butlerUtils.mapping.map` should check more robustly for `if newPath:` rather than `if newPath is not None:`.    Thus we explicitly set up this if/else to return `None` if `not paths` instead of `[]`.    ",0
"DM-4734","12/30/2015 08:10:24","afw fails to build on a machine with many cores","The afw package does not build reliably (if at all) on a linux box at UW (""magneto"", which has 32 cores and 128 Gb of RAM). The failure is that some unit tests fail with the following error:  {code}      OpenBLAS: pthread_creat error in blas_thread_init function. Error code:11  {code}    For the record, /usr/include/bits/local_lim.h contains this:  {code}  /* The number of threads per process.  */  #define _POSIX_THREAD_THREADS_MAX 64  /* We have no predefined limit on the number of threads.  */  #undef PTHREAD_THREADS_MAX  {code}    It appears that the build system is trying to use too many threads when building afw, which presumably means it is trying to use too many cores. According to [~mjuric] the package responsible for this is {{eupspkg}}, and it tries to use all available cores.    A workaround suggested by [~mjuric] is to set environment variable {{EUPSPKG_NJOBS}} to the max number of cores wanted. However, I suggest we fix our build system so that setting this variable is unnecessary. I suggest we hard-code an upper limit for now, though fancier logic is certainly possible.    A related request is to document the environment variables that control our build system. I searched for {{NJOBS}} on confluence and found nothing.",0.5
"DM-4752","01/06/2016 15:47:54","Build on Mac very slow due to running fc-list","Builds on MacOS 10.11 have been painfully slow (for instance 23 minutes to build {{afw}} instead of the more typical 12 minutes, 30 minutes to rebuild {{ip_diffim}}, 20 minutes to rebuild {{meas_astrom}}) and much of this time is spent running fc-list in 8 cores.    I suspect {{matplotlib}} is triggering this process, but I have not verified it.    I see this using lsstsw to build a fresh stack and with manual builds.    A workaround is to repeatedly kill {{fc-list}}, e.g. with this bash script:  {code}  while sleep 1; do pkill fc-list; done  {code}    I checked my fonts with Apple's Font Book and found a few dozen with ""minor errors"" that I deleted, but nothing serious. I'm still seeing the problem.",0.5
"DM-4753","01/06/2016 15:56:44","Cleanup location of anonymous namespaces","we place anonymous namespace in two ways: (a), INSIDE lsst::qserv::<module> namespace, or (b) BEFORE. This story involves cleaning it up - move them to before lsst::qserv::<module>",1
"DM-4759","01/07/2016 09:35:18","Port Data  set info converter achitechture","defines various image data types, how to get them, groupings, artifacts.   I am not quite happy with how we did in in GWT so the design needs to be improved.  Must be less complex.",8
"DM-4780","01/08/2016 18:08:24","meas_extensions_shapeHSM seems to be broken","I have installed the meas_extensions_shapeHSM package together with galsim and tmv (I documented it at : https://github.com/DarkEnergyScienceCollaboration/ReprocessingTaskForce/wiki/Installing-the-LSST-DM-stack-and-the-related-packages#installing-meas_extensions_shapehsm) and tried to run it on CFHT cluster data.     My config file is the following:    {code:python}  import lsst.meas.extensions.shapeHSM  config.measurement.plugins.names |= [""ext_shapeHSM_HsmShapeRegauss"", ""ext_shapeHSM_HsmMoments"",                                      ""ext_shapeHSM_HsmPsfMoments""]  config.measurement.plugins['ext_shapeHSM_HsmShapeRegauss'].deblendNChild=''  config.measurement.slots.shape = ""ext_shapeHSM_HsmMoments""  {code}    When I run measCoaddSources.py, I get the following error :    {code}  Traceback (most recent call last):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/bin/measureCoaddSources.py"", line 3, in <module>      MeasureMergedCoaddSourcesTask.parseAndRun()    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 444, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run      if self.precall(parsedCmd):    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall      task = self.makeTask(parsedCmd=parsedCmd)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/cmdLineTask.py"", line 363, in makeTask      return self.TaskClass(config=self.config, log=self.log, butler=butler)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_tasks/2015_10.0-10-g1170fd0/python/lsst/pipe/tasks/multiBand.py"", line 530, in __init__      self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pipe_base/2015_10.0-3-g24e103a/python/lsst/pipe/base/task.py"", line 255, in makeSubtask      subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)    File ""/sps/lsst/Library/lsstsw/stack/Linux64/pex_config/2015_10.0-1-gc006da1/python/lsst/pex/config/configurableField.py"", line 77, in apply      return self.target(*args, config=self.value, **kw)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/sfm.py"", line 247, in __init__      self.initializePlugins(schema=self.schema)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins      self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 15, in __init__      self.cpp = self.factory(config, name, schema, metadata)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_base/python/lsst/meas/base/wrappers.py"", line 223, in factory      return AlgClass(config.makeControl(), name, schema)    File ""/sps/lsst/dev/lsstprod/clusters/my_packages/meas_extensions_shapeHSM/python/lsst/meas/extensions/shapeHSM/hsmLib.py"", line 964, in __init__      def __init__(self, *args, **kwargs): raise AttributeError(""No constructor defined - class is abstract"")  AttributeError: No constructor defined - class is abstract  {code}",1
"DM-4781","01/09/2016 00:25:30","MariaDB does not work together with mysql-proxy","We have switched to MAriaDB but there is one issue that complicates things - mysql client from mariadb fails to connect to mysql-proxy with an error:  {noformat}  ERROR 1043 (08S01): Bad handshake  {noformat}  so Fabrice had to find a workaround for our setup to use client from mysqlclient package instead. This workaround is not perfect and it complicates other things. Would be nice to make things work transparently for mariadb.    ",2
"DM-4782","01/10/2016 21:52:31","JIRA project for the publication board","The LSST Publication Board requests a JIRA project for managing its workload.       ",2
"DM-4785","01/11/2016 16:01:59","Update provenance in baseline schema","Current provenance schema in baseline (cat/sql) is very old and no longer reflect latest thinking. This story involves bringing cat/sql up to data and replacing existing prv_* tables with tables we came up with in the epic.",2
"DM-4786","01/12/2016 03:28:31","Packge mysqlproxy 0.8.5","See https://mariadb.atlassian.net/browse/MDEV-9389",2
"DM-4789","01/12/2016 11:31:00","FITS Visualizer porting: Mouse Readout: part 3: Lock by click & 3 color support","add toggle button that make the mouse readout lock to last position click on.  It will not longer update on move but by click  Include: 3 Color Support",8
"DM-4793","01/12/2016 17:45:21","Refactor prototype docs into “Developer Guide” and Science Pipelines doc projects","Refactor [lsst_stack_docs|https://github.com/lsst-sqre/lsst_stack_docs] into two doc projects    - LSST DM Developer Guide that will be published to {{developer.lsst.io}}, and  - LSST Science Pipelines that will be published to {{pipelines.lsst.io}}",3
"DM-4794","01/12/2016 18:04:15","Write Zoom Options Popup","Write the simple zoom options popup that is show when the user clicks zoom too fast or the zoom level exceeds  the maximum size.      activate this popup from visualize/ui/ZoomButton.jsx",2
"DM-4798","01/13/2016 10:03:31","DetectCoaddSourcesTask.scaleVariance gets wrong result","DetectCoaddSourcesTask.scaleVariance is used to adjust the variance plane in the coadd to match the observed variance in the image plane (necessary after warping because we've lost variance into covariance). The current implementation produces the wrong scaling in cases where the image has strongly variable variance (e.g., 10 inputs contributed to half the image, but only 1 input contributed to the other half) because it calculates the variance of the image and the mean of the variance separately so that clipping can affect different pixels.    Getting this scaling very wrong can make us dig into the dirt when detecting objects, with drastic implications for the resultant catalog.    This is a port of [HSC-1357|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1357] and [HSC-1383|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1383].",1
"DM-4801","01/13/2016 14:48:50","Update the ground truth values in the lsst_dm_demo to reflect new defaults in deblending","In DM-4410 default configuration options were changed such that footprints are now grown in the detection task, and the deblender is run by default. This breaks the lsst_dm_demo, as now the results of processing are slightly different. The short term solution as part of DM-4410 was to run the demo with the defaults overridden to be what they were prior to DM-4410. In the long term the values used in the compare script should be updated to reflect what would be generated with running processCcd with the stack defaults. ",0.5
"DM-4806","01/14/2016 13:27:18","Test stack with mariadbclient","Now that we switched Qserv to mariadb, it'd be good to switch the rest of the stack. This story involves trying out if things still work if we switch mysqlclient to mariadbclient.",2
"DM-4820","01/15/2016 11:33:49","Improvement of raw data handling in DecamMapper","Two minor improvements with better coding practice:  - Be more specific copying FITS header keywords. Avoid potential problems if unwelcome keywords appear in the header in the future. Suggested in the discussions in DM-4133.   - Reuse {{isr.getDefectListFromMask}} for converting defects. A more efficient method that uses the FootprintSet constructor with a Mask and a threshold has just been adopted in DM-4800.     Processing is not changed effectively.  ",1
"DM-4821","01/15/2016 15:58:30","HSC backport: Remove interpolated background before detection to reduce junk sources","This is a port of [HSC-1353|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1353] and [HSC-1360|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1360].    Descriptions from HSC:   {panel:title=HSC-1353}  We typically get a large number of junk detections around bright objects due to noise fluctuations in the elevated background. We can try to reduce the number of junk detections by adding an additional local background subtraction before object detection. We can then add this back in after detection of footprints and peaks.  {panel}  {panel:title=HSC-1360}  I forgot to set the useApprox=True for the background subtraction that runs before footprint and peak detection. This will then use the Chebyshev instead of the spline.  {panel}",1
"DM-4823","01/15/2016 17:45:01","Add Dropdowns to Vis toolbar","Add the dropdown to the vis tool bar",2
"DM-4824","01/15/2016 18:02:30","Clean up div and css layout on FitsDownloadDialog","FitsDownload dialogs html and css is not quite right. Needs some clean up.",1
"DM-4825","01/15/2016 18:35:53","makeDiscreteSkymap has a default dataset of 'raw'","The default dataset type for command line tasks is raw.  In the case MakeDiscreteSkyMapTask is asking the butler for calexp images.  This shouldn't be a problem, but in my case I have calexp images, but no raw images.  This causes the task to think there is no data to work on, so it exits.",1
"DM-4831","01/18/2016 10:45:46","Add bright object masks to pipeline outputs","Given per-patch inputs providing   {code}  id, B, V, R, ra, dec, radius    {code}  for each star to be masked, use this information to set:  * A bit in the mask plane for each affected pixel  * A flag in the source catalogues for each object that has a centroid lying within this mask area    This is a port of [HSC-1342|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1342] and [HSC-1381|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1381].",3
"DM-4833","01/18/2016 11:08:13","Update configuration for Suprime-Cam","The {{obs_subaru}} configuration for Suprime-Cam needs updating to match recent changes in the stack.    Port of [HSC-1372|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1372].",1
"DM-4834","01/18/2016 11:11:55","Preliminaries for LSST vs HSC pipeline comparison through coadd processing","This is the equivalent of DM-3942 but through coadd processing.    Relevant HSC tickets include:    * [HSC-1371|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1371]",1
"DM-4835","01/18/2016 11:17:39","Allow slurm to request total CPUs rather than nodes*processors.","On some systems, we are asked to request a total number of tasks, rather than specify a combination of nodes and processors per node.    It also makes sense to use the SMP option this way.    This is a port of [HSC-1369|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1369].",2
"DM-4841","01/18/2016 15:07:01","Use high S/N band as reference for multiband forced photometry","We are currently choosing the priority band as the reference band for forced photometry as long as it has a peak in the priority band regardless of the S/N.  Please change this to pick the highest S/N band as the reference band when the priority band S/N is sufficiently low.    This is a port of [HSC-1349|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1349].",1
"DM-4842","01/18/2016 15:17:15","Don't write HeavyFootprints in forced photometry","There's no need to persist {{HeavyFootprint}}s while performing forced photometry since retrieving them is as simple as loading the _meas catalog.    This is a port of [HSC-1345|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1345].",0.5
"DM-4847","01/18/2016 17:08:07","Add new blendedness metric","[HSC-1316|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1316] shifts the calculation of blendedness from {{meas_deblender}} to {{meas_algorithms}} and defines a new blendedness metric in the process. Please port it.",3
"DM-4849","01/18/2016 21:26:22","LDM-151 - comments from Jacek","I am reading your https://github.com/lsst/LDM-151/blob/draft/DM_Applications_Design.tex, and I have some minor comments suggestions. I am going to add comments to this story to capture it. Feel free to apply to ignore :)",1
"DM-4850","01/19/2016 06:20:23","Factor out duplicate setIsPrimaryFlag from MeasureMergedCoaddSourcesTask and ProcessCoaddTask","{{MeasureMergedCoaddSourcesTask.setIsPrimaryFlag()}} and {{ProcessCoaddTask.setIsPrimaryFlag()}} are effectively the same code. Please split this out into a separate task which both of the above can call.    This is a (partial) port of [HSC-1112|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1112] and should include fixes from [HSC-1297|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1297].",2
"DM-4856","01/19/2016 11:19:54","Add __setitem__ for columns in afw.table","It's confusing to have to use an extra {{[:]}} to set a column in afw.table, and we can make that unnecessary if we override {{\_\_setitem\_\_}} as well as {{\_\_getitem\_\_}}.",2
"DM-4858","01/19/2016 15:13:06","imagesDiffer doesn't handle overflow for unsigned integers","I'm seeing a test failure in afw's testTestMethods.py, apparently due to my numpy (1.8.2) treating images that differ by -1 as differing by 65535 in both {{numpy.allclose}} and array subtraction (which doesn't promote to an unsigned type).    Does this still cause problems in more recent versions of {{numpy}}?  If not, I imagine it's up to me to find a workaround for older versions if I want it fixed?    (assigning to [~rowen] for now, just because I know he originally wrote this test and I hope he might know more)",1
"DM-4862","01/20/2016 10:08:08","Add point selection","click and highlight a point.  Is on when mouse readout ""Lock by Click"" is on. However, can me turned on externally by adding toolbar context menu options.",2
"DM-4867","01/20/2016 15:25:37","scisql build scripts are buggy ","The scisql build script logic for MySQL/MariaDB version checking is broken on all platforms. There are also assumptions about shared library naming that do not hold on OS/X, which means that the deployment scripts are likely broken on all platforms other than Linux.",2
"DM-4873","01/20/2016 17:02:45","Test the matchOptimisticB astrometric matcher","The matchOptimisticB matcher fails on many visits of the bulge verification dataset.  This prompted a deeper investigation of the performance of the matcher.  Angelo and David developed a test script and discovered that the matcher works well with offsets of the two source catalogs of up to 80 arcsec, but fails beyond that.  This should be robust enough for nearly all datasets that the LSST stack will be used on.",3
"DM-4876","01/20/2016 17:09:10","Compile list of DM simulation needs for Andy Connolly","Compile list of DM simulation needs over the next ~6 months to give to Andy Connolly (simulations lead).",3
"DM-4878","01/20/2016 17:18:51","Propagate flags from individual visit measurements to coadd measurements","It is useful to be able to identify suitable PSF stars from a coadd catalogue. However, the PSF is not determined on the coadd, but from all the inputs. Add a mechanism for propagating flags from the input catalogues to the coadd catalogue indicating stars that were used for measuring the PSF.    Make the inclusion fraction threshold configurable so we can tweak it (so we only get stars that were consistently used for the PSF model; the threshold might be set it to 0 for ""or"", 1 for ""all"" and something in between for ""some"").    Make the task sufficiently general that it can be used for propagating arbitrary flags.    This is a port of work carried out on [HSC-1052|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1052] and (part of) [HSC-1293|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1293].",2
"DM-4882","01/20/2016 20:41:57","base_Variance plugin generates errors in lsst_dm_stack_demo","Since DM-4235 was merged, we see a bunch of messages along the lines of:  {code}  processCcd.measurement WARNING: Error in base_Variance.measure on record 427969358631797076: The center is outside the Footprint of the source record  {code}  in the output from {{lsst_dm_stack_demo}}. (See e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/7482/console#console-section-3]). It's not fatal, but the warnings are disconcerting and could be indicative of a deeper problem.",2
"DM-4885","01/20/2016 21:44:41","Improve/simplify multi-worker tests","The idea is that our current integration test is using ""mono"" configuration which is only useful for integration test but it's not used anywhere else. It would be more useful to have an integration test which is close to real setup, e.g. used more than one worker. It should still be possible to run the whole shebang on a single node though to keep it usable for regular development tasks.",5
"DM-4887","01/21/2016 09:48:04","Refactor measurement afterburners into a new plugin system","Some of the operations we currently run as part of measurement (or would like to) share some features that make them a bit different from most plugin algorithms:   - They must be run after at least some other high-level plugins, and may be run after all of them.   - They do not require access to pixel data, as they derive their outputs entirely from other plugins' catalog outputs.   - They may require an aggregation stage of some sort to be run on the regular plugin output before they can be run.    Some examples include:   - Star/Galaxy classification (with training done after measurement and before classification).   - Applying aperture corrections (estimating the correction must be done first).   - BFD's P, Q, R statistics (requires a prior estimated from deep data).    We should move these algorithms to a new plugin system that's run by a new subtask, allowing these plugins to be run entirely separately from {{SingleFrameMeasurementTask}}.  This will simplify some of the currently contorted logic required to make S/G classification happen after aperture correction, while making room for hierarchical inference algorithms like BFD and Bayesian S/G classification in the future.    (We will not be able to support BFD immediately, as this will also require changes to our parallelization approach, but this will be a step in the right direction).    This work should *probably* be delayed until after the HSC merge and [~rowen]'s rewrite of {{ProcessCcdTask}} are complete, but it's conceivable that this refactoring could solve emergent problems there and be worth doing earlier as a result.",8
"DM-4893","01/22/2016 10:00:42","Write tutorial describing remote IPython + ds9 on lsst-dev","[~mfisherlevine] recently figured out how to set up his system to run a remote IPython kernel on {{lsst-dev}} and interact with it from his laptop, including streaming image display from the remote system to a local instance of {{ds9}}.    He will write all this up so that others in the community can easily do the same.",2
"DM-4894","01/22/2016 10:07:13","Ingest DECam/CBP data into LSST stack","[~mfisherlevine] will ingest the data taken in DM-4892 into the LSST stack. Initial experiments indicate problems with:    * Bias subtraction  * Flat fielding  * Bad pixel masks    These may already be remedied by work on {{obs_decam}}; if not, he will file stories and fix them.",3
"DM-4904","01/22/2016 20:59:03","Buffer overrun in wcslib causes stack corruption","The buffer 'msg' in wcsfix.c is used to report attempts by wcslib to re-format units found in fits files. It is allocated on the stack (in function 'unitfix') using a pre-processor macro defined size of 160 chars (set in wcserr.h). When attempting to run the function 'unitfix' in wcsfix, this buffer can overflow on some fits files (the raw files generated by HSC seem particularly prone to triggering this behavior) and results in the session being terminated on Ubuntu 14.04 as stack protection is turned on by default i.e. the stack crashes with a 'stack smashing detected' error. We have reported the bug to the creators of wcslib. As a temporary workaround, users affected by the bug should increase the default size of 'msg' by increasing WCSERR_MSG_LENGTH defined in wcserr.h      We are providing a small python example that demonstrates the problem. Run it as  python test.py <path to ci_hsc>/raw/<any fits file in this directory>    We are also providing a simple c program to demonstrate the bug. Compile it as  cc -fsanitize=address -g -I$WCSLIB_DIR/include/wcslib -o test test.c -L$WCSLIB_DIR/lib -lwcs (on Linux)  cc -fsanitize=address -g -L$WCSLIB_DIR/lib -lwcs -I$WCSLIB_DIR/include/wcslib -o test test.c (on Mac OS X)",2
"DM-4916","01/26/2016 09:18:58","Test obs_decam with processed data","Sometimes DECam-specific bugs only reveal in or affect the processed data. For example the bug of DM-4859 reveals in the {{postISRCCD}} products.  If the bugs are DECam-specific, some changes in {{obs_decam}} are likely needed.  It would be useful to have a more convenient way to test those changes. In this ticket I modify {{testdata_decam}} so that those data can be processed, and then allow wider options in the {{obs_decam}} unit tests.    I add {{testProcessCcd.py}} in {{obs_decam}} that runs {{processCcd.py}} with raw and calibration data in {{testdata_decam}}.  Besides a short sanity check, I add a test (testWcsPostIsr) that tests DM-4859. {{testWcsPostIsr}} fails without the DM-4859 fix, and passes with it.  ",3
"DM-4917","01/26/2016 11:37:24","Porting encodeURL of the java FitsDownlaodDialog code to javascript ","When download an image,  the proper name needs to be resolved based on the URL and   the information about the image.  In Java code, it has the following three methods:  {code}   encodeUrl  makeFileName  makeTitleFileName  {code}    These method should be ported to javascript.  Thus, the javascript version of the FitsDownloadDialog will save the file in the same manner. ",2
"DM-4921","01/26/2016 14:23:55","Make obs_subaru build with OS X SIP","Because of OS X SIP, {{obs_subaru}} fails to build on os x 10.11. In the {{hsc/SConscript}} file, the library environment variables need properly set, and scripts need to be delayed until the shebang rewriting occurs. ",0.5
"DM-4926","01/27/2016 07:02:07","Centroids fall outside Footprints","In DM-4882, we observed a number of centroids measured while running the {{lsst_dm_stack_demo}} routines fall outside their associated {{Footprints}}. This was seen with both the {{NaiveCentroid}} and the {{SdssCentroid}} centroiders.    For the purposes of DM-4882 we quieted the warnings arising from this, but we should investigate why this is happening and, if necessary, weed out small {{Footprints}} entirely.",8
"DM-4929","01/27/2016 11:32:50","Fix build of MariaDB on OS X El Capitan","The current MariaDB EUPS package does not build on OS X El Capitan because OS X no longer ships with OpenSSL developer files. MariaDB has a build option to use a bundled SSL library in preference to OpenSSL but the logic for automatically switching to this version breaks when the Anaconda OpenSSL libraries are present.",1
"DM-4931","01/27/2016 11:48:11","Qserv build fails on El Capitan with missing OpenSSL","Qserv does not build on OS X El Capitan due to the absence of OpenSSL include files. Apple now only ship the OpenSSL library (for backwards compatibility reasons). Qserv only uses SSL in two places to calculate digests (MD5 and SHA). This functionality is available in the Apple CommonCrypto library. Qserv digest code needs to be taught how to use CommonCrypto.",2
"DM-4933","01/27/2016 13:50:01","Create a utility function do do spherical geometry averaging","I would like to calculate a correct average and RMS for a set of RA, Dec positions.    Neither [~jbosch] nor [~price] knew of an easy, simple function to do that that existed in the stack.  [~price] suggested:    {code}  mean = sum(afwGeom.Extent3D(coord.toVector()) for coord in coordList, afwGeom.Point3D(0, 0, 0))  mean /= len(coordList)  mean = afwCoord.IcrsCoord(mean)  {code}    That makes sense, but it's a bit unobvious (it's obvious how it works, but would likely never occur to someone that they should do it that way in the stack).    Pedantically it's also not the best way to do a mean while preserving precision, but I don't anticipate that to be an issue in practice.    Creating a function that did this would provide clarity.  I don't know where that function should live.    Note: I know how to do this in Astropy.  I'm intentionally not using astropy here.  But part of the astropy dependency discussion is likely ""how much are we otherwise rewriting in the LSST stack"".",1
"DM-4934","01/27/2016 14:57:01","on-going support to Camera team in visualization at UIUC","Attend the weekly meeting and answer questions as needed",2
"DM-4936","01/28/2016 09:11:22","Enable validateMatches in ci_hsc","{{python/lsst/ci/hsc/validate.py}} in {{ci_hsc}} [says|https://github.com/lsst/ci_hsc/blob/69c7a62f675b8fb4164065d2c8c1621e296e40ad/python/lsst/ci/hsc/validate.py#L78]:  {code:python}      def validateMatches(self, dataId):          # XXX lsst.meas.astrom.readMatches is gone!          return  {code}  {{readMatches}} (or its successor) should be back in place as of DM-3633. Please enable this test.",2
"DM-4937","01/28/2016 09:12:05","multiple CVEs relevant to mariadb 10.1.9 and mysql","Multiple CVEs have been released this week for mysql & mariadb.  The current eups product for mariadb is bundling 10.1.9, which is affected.  Several of the CVEs do not yet provide details, which typically means they are ""really bad"".    https://github.com/lsst/mariadb/blob/master/upstream/mariadb-10.1.9.tar.gz    https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0505  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0546  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0596  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0597  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0598  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0600  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0606  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0608  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0609  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-0616  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-2047",0.5
"DM-4938","01/28/2016 10:33:52","Update scisql to v0.3.5","In order to update MariaDB to v10.1.10 {{scisql}} needs to also be updated to deal with the hard-coded version checking. For the current version we get this error with the latest MariaDB:  {code}  :::::  [2016-01-28T16:51:40.539306Z]     user_function(self)  :::::  [2016-01-28T16:51:40.539334Z]   File ""/home/build0/lsstsw/build/scisql/wscript"", line 63, in configure  :::::  [2016-01-28T16:51:40.539346Z]     ctx.check_mysql()  :::::  [2016-01-28T16:51:40.539392Z]   File ""/home/build0/lsstsw/build/scisql/.waf-1.6.11-30618c54883417962c38f5d395f83584/waflib/Configure.py"", line 221, in fun  :::::  [2016-01-28T16:51:40.539410Z]     return f(*k,**kw)  :::::  [2016-01-28T16:51:40.539432Z]   File ""tools/mysql_waf.py"", line 85, in check_mysql  :::::  [2016-01-28T16:51:40.539451Z]     (ok, msg) = mysqlversion.check(version)  :::::  [2016-01-28T16:51:40.539473Z]   File ""tools/mysqlversion.py"", line 74, in check  :::::  [2016-01-28T16:51:40.539514Z]     if not comparison_op(version_nums, constraint_nums):  :::::  [2016-01-28T16:51:40.539547Z] UnboundLocalError: local variable 'constraint_nums' referenced before assignment  Failed during rebuild of DM stack.  {code}",0.5
"DM-4939","01/28/2016 11:51:18","IRSA developer mentoring effort","IRSA is contributing to the Firefly package development.  we need to put in time to mentor the developers. ",2
"DM-4940","01/28/2016 11:56:35","IRSA developer mentoring effort","IRSA is contributing to Firefly development. We need to mentor the new developers.",2
"DM-4952","01/29/2016 09:44:52","delegate argument parsing to CmdLineTask instances","Command-line argument parsing of data IDs for {{CmdLineTask}} s is currently defined at the class level, which means that we cannot make data ID definitions dependent on task configuration.  That in turn requires custom {{processCcd}} scripts for cameras that start processing at a level other than ""raw"" (SDSS, DECam with community pipeline ISR, possibly CFHT).    Instead, we should let {{CmdLineTask}} *instances* setup command-line parsing; after a {{CmdLineTask}} is constructed, it will have access to its final configuration tree, and can better choose how to parse its ID arguments.    I've assigned this to Process Middleware for now, since that's where it lives in the codebase, but it may make more sense to give this to [~rowen], [~price], or [~jbosch], just because we've already got enough familiarity with the code in question that we could do it quickly.  I'll leave that up to [~swinbank], [~krughoff], and [~mgelman2] to decide.",2
"DM-4955","01/29/2016 12:44:30","Update pyfits","The final version of {{pyfits}} has just been released. This ticket covers updating to that version. This will be helpful in determining whether the migration to {{astropy.io.fits}} will be straightforward or complicated.",1
"DM-4957","01/29/2016 13:42:33","Generate JSON output from validate_drp for inclusion in a test harness","Generate JSON output from validate_drp for inclusion in a test harness.    Generate a file that summarizes the key metrics calculated by `validate_drp`.      Develop naming conventions that will make it easy to plug into the eventual harness being developed as part of DM-2050.",2
"DM-4959","01/30/2016 19:50:52","ci_hsc fails to execute tasks from with SCons on OSX 10.11/SIP","The {{ci_hsc}} package executes a number of command line tasks directly from SCons based on {{Command}} directives in a {{SConstruct}} file. On an OSX 10.11 system with SIP enabled, there are two distinct problems which prevent the necessary environment being propagated to the tasks:  * -The {{scons}} executable starts with a {{#!/usr/bin/env python}}. Running through {{/usr/bin/env}} strips {{DYLD_LIBRARY_PATH}} from the environment.- (duplicates DM-4954)  * SCons executes command using the [{{sh}} shell on posix systems|https://bitbucket.org/scons/scons/src/09e1f0326b7678d1248dab88b28b456fd7d6fb54/src/engine/SCons/Platform/posix.py?at=default&fileviewer=file-view-default#posix.py-105]. By default, that means {{/bin/sh}} on a Mac, which, again, will strip {{DYLD_LIBRARY_PATH}}.    Please make it possible to run {{ci_hsc}} on such a system.",0.5
"DM-4961","01/31/2016 15:37:38","Obs_Subaru camera mapper has wrong deep_assembleCoadd_config","When lsst switched to using SafeClipAssembleCoaddTask, the camera mapper for hsc was not updated accordingly. This causes ci_hsc to fail when it attempts to verify the config class type for the deep_coadd. Camera mapper should be updated accordingly",0.5
"DM-4983","02/01/2016 11:39:26","upstream patches/deps from conda-lsst","Where ever possible, missing dep information and patches from conda-lsst should be upstreamed.  The patches have already been observed to cause builds to fail due to upstream changes.",3
"DM-18241","02/01/2016 14:19:59","Create initial M1M3, M2 simulators","Initial simulator support",3
"DM-17268","02/01/2016 14:34:10","SAL release 4 build and distribute","Release new version",5
"DM-4991","02/01/2016 14:45:18","Save algorithm metadata in multiband.py","The various {{Tasks}} in {{multiband.py}} do not attach the {{self.algMetadata}} instance attribute to their output tables before writing them out, so we aren't actually saving information like which radii were used for apertures.    We should also make sure this feature is maintained in the processCcd.py rewrite.",3
"DM-4993","02/01/2016 20:37:35","review of dependency on the third party packages","We need to periodically review the status of the third party software packages that Firefly depends on. Making a plan to do upgrade if needed.   package.json lists out the dependencies Firefly has on the third party software. The attached file was last modified 2016-02-09.    package.json_version lists the current version of the third party packages, major changes were indicated by (M). The attached file was created on 2016-02-29.     bq.      ""babel""     : ""5.8.34"",                           6.5.2 (M)      ""history""   : ""1.17.0"",                           2.0.0 (M)      ""icepick""   : ""0.2.0"",                            1.1.0 (M)                ""react-highcharts"": ""5.0.6"",                      7.0.0 (M)      ""react-redux"": ""3.1.2"",                           4.4.0 (M)      ""react-split-pane"": ""0.1.22"",                     2.0.1 (M)      ""redux-thunk"": ""0.1.0"",                           1.0.3 (M)      ""redux-logger"": ""1.0.9"",                          2.6.1 (M)      ""validator"" : ""4.5.0"",                            5.1.0 (M)      ""chai"": ""^2.3.0"",                                 3.5.0 (M)      ""esprima-fb"": ""^14001.1.0-dev-harmony-fb"",        15001.1001.0-dev-harmony-fb (M)      ""babel-eslint""      : ""^4.1.3"",                   5.0.0 (M)      ""babel-loader""      : ""^5.3.2"",                   6.2.4 (M)      ""babel-plugin-react-transform"": ""^1.1.0"",         2.0.0 (M)      ""babel-runtime""     : ""^5.8.20"",                  6.6.0 (M)      ""eslint""            : ""^1.10.3"",                  2.2.0 (M)      ""eslint-config-airbnb"": ""0.1.0"",                  6.0.2 (M) works with eslint 2.2.0      ""eslint-plugin-react"": ""^3.5.1"",                  4.1.0 (M)  works with eslint 2.2.0      ""extract-text-webpack-plugin"": ""^0.8.0"",          1.0.1 (M)      ""html-webpack-plugin"": ""^1.6.1"",                  2.9.0 (M)      ""karma-sinon-chai"": ""^0.3.0"",                     1.2.0 (M)      ""redux-devtools""    : ""^2.1.2"",                   3.3.1 (M)      ""webpack"": ""^1.8.2""                               1.12.14, 2.1.0 beta4 (M)          ",2
"DM-4995","02/02/2016 01:07:10","Extend webserv API to pass security tokens","Extend the [API|https://confluence.lsstcorp.org/display/DM/AP] to pass security tokens.",8
"DM-4996","02/02/2016 09:34:33","Update validate_drp for El Capitan","validate_drp does not work on El Capitan due to SIP (System Integrity Protection) stripping DYLD_LIBRARY_PATH from shell scripts. The simple fix is to add  {code}  export DYLD_LIBRARY_PATH=${LSST_LIBRARY_PATH}  {code}  near the top of the scripts.",1
"DM-4998","02/02/2016 12:54:03","Fix rotation for isr in obs_subaru","Approximately half of the HSC CCDs are rotated 180 deg with respect to the others.  Two others have 90 deg rotations and another two have 270 deg rotations (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]) .  The raw images for the rotated CCDs thus need to be rotated to match the rotation of their associated calibration frames prior to applying the corrections.  This is accomplished by rotating the exposure using the *rotated* context manager function in {{obs_subaru}}'s *isr.py* and the *nQuarter* specification in the policy file for each CCD.  Currently, *rotated* uses {{afw}}'s *rotateImageBy90* (which apparently rotates in a counter-clockwise direction) to rotated the exposure by 4 - nQuarter turns.  This turns out to be the wrong rotation for the odd nQuarter CCDs as shown here:   !ccd100_nQuarter3.png|width=200!  top left = raw exposure as read in  top right = flatfield exposure as read in  bottom left = _incorrectly_ rotated raw exposure prior to flatfield correction",2
"DM-5002","02/02/2016 16:02:48","Make ci_hsc resumable","if ci_hsc fails for any reason, (or is cancelled) it must start from the beginning of processing again. This is because of the use of functools.partial to generate dynamic function. These differ enough in their byte code that scons thinks each build has a new function definition passed to the env.command function. Using lambda would suffer from the same problem. This ticket should change how the function signature is calculated such that scons can be resumed.    This work does not prevent this from being used as a ci tool, as the .scons directory can be deleted which will force the whole SConstruct file to run again.",2
"DM-5005","02/02/2016 16:11:36","Please trim config overrides in validate_drp","validate_drp will test more of our code if it uses default config parameters wherever possible. To that effect I would like to ask you to eliminate all config overrides that are not essential and document the reasons for the remaining overrides.    For DECam there are no overrides that are different than the defaults, so the file can simply be emptied (for now).    For CFHT there are many overrides that are different, and an important question is whether the overrides in this package are better for CFHT data than the overrides in obs_cfht; if so, please move them to obs_cfht.    As a heads up: the default star selector is changing from ""secondMoment"" to ""objectSize"" in DM-4692 and I hope to allow that in validate_drp, since it works better and is better supported.    Sorry for the incorrect component, but validate_drp is not yet a supported component in JIRA (see DM-5004)",0.5
"DM-5013","02/03/2016 09:34:31","Convert Confluence DM Developer Guide to Sphinx (hack day) ","This is a hack day sprint to convert all remaining content on https://confluence.lsstcorp.org/display/LDMDG to reStructuredText content in the Sphinx project at https://github.com/lsst-sqre/dm_dev_guide and published at http://developer.lsst.io.    The top priority for this sprint is to port all content into reST and have it tracked by Git.    h2. Sprint ground rules    # Before the sprint, clone {{https://github.com/lsst-sqre/dm_dev_guide.git}} and {{pip install -r requirements.txt}} in a Python 2.7 environment so that you can locally build the docs ({{make html}}).  # Claim a page from the list below by putting your name on it. Put a checkmark on the page when you’ve merged it to the ticket branch (see below).  # See http://developer.lsst.io/en/latest/docs/rst_styleguide.html for guidance on writing our style of reStructuredText. Pay attention to the [heading hierarchy|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#sections] and [labelling for internal links|http://developer.lsst.io/en/latest/docs/rst_styleguide.html#internal-links-to-labels].  # If you use Pandoc to do an initial content conversion, you still need to go through the content line-by-line to standardize the reStructuredText. I personally recommend copy-and-pasting-and-formatting instead of using Pandoc.  # Your Git commit messages should include the URL of the original content from Confluence.  # Merge your work onto the {{tickets/DM-5013}} ticket branch. Rebase your personal work branch before merging. JSick is responsible for merging this ticket branch to {{master}}.  # Put a note at the top of the confluence page with the new URL; root is {{http://developer.lsst.io/en/latest/}}.    h2. Planned Developer Guide Table of Contents    We’re improving the organization of DM’s Developer Guide; there isn’t a 1:1 mapping of Confluence pages to developer.lsst.io pages. Below is a proposed section organization and page structure. These sections can still be refactored based on discussion during the hack day.    h3. Getting Started — /getting-started/    * ✅ *Onboarding Checklist* (Confluence: [Getting Started in DM|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM]). I’d like this to eventually be a quick checklist of things a new developer should do. It should be both a list of accounts the dev needs to have created, and a list of important developer guide pages to read next. The NCSA-specific material should be spun out. [[~jsick]]  * *Communication Tools* (new + DM Confluence [Communication and Links|https://confluence.lsstcorp.org/display/DM/Communication+and+Links]). I see this as being an overview of what methods DM uses to communicate, and what method should be chosen for any circumstance.  * *Finding Code on GitHub* (new). This should point out all of the GitHub organizations that a developer might come across (DM and LSST-wide), and point out important repositories within each organization. Replaces the confluence page [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]    h3. Processes — /processes/    * ✅ *Team Culture and Conduct Standards* (confluence)  * ✅ *DM Development Workflow with Git, GitHub, JIRA and Jenkins* (new & Confluence: [git development guidelines for LSST|https://confluence.lsstcorp.org/display/LDMDG/git+development+guidelines+for+LSST] + [Git Commit Best Practices|https://confluence.lsstcorp.org/display/LDMDG/Git+Commit+Best+Practices] + [DM Branching Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Branching+Policy])  * ✅ *Discussion and Decision Making Process* (new & [confluence|https://confluence.lsstcorp.org/display/LDMDG/Discussion+and+Decision+Making+Process])  * ✅ *DM Wiki Use* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/DM+Wiki+Use]) [[~swinbank]]  * ✅ *Policy on Updating Doxygen* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Updating+Doxygen]); needs to be addressed with TCT. Inter-link with the developer workflow page. [[~jsick]] (we’re just re-pointing the Confluence page to the workflow document)  * ✅ *Transferring Code Between Packages* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Transferring+Code+Between+Packages]) [[~swinbank]]  * -*Policy on Changing a Baseline Requirement*- ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Policy+on+Changing+a+Baseline+Requirement])  * ✅ *Project Planning for Software Development* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Project+Planning+for+Software+Development]) [[~swinbank]]  * ✅ *JIRA Agile Usage* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/JIRA+Agile+Usage]) [[~swinbank]]  * -*Technical/Control Account Manager Guide*- ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=21397653]) (Do not port; see discussion below.)  * *Licensing* (new) Need a centralized page to discuss license and copyright policies; include boilerplate statements.    h3. Coding Guides — /coding/    * ✅ *Introduction* and note on stringency language (confluence: [DM Coding Style Policy|https://confluence.lsstcorp.org/display/LDMDG/DM+Coding+Style+Policy])  * ✅ *DM Python Style Guide* (confluence: [Python Coding Standard|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standard])  * ✅ *DM C++ Style Guide* (confluence pages: [C++ Coding Standard|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908666] + [C++ General Recommendations|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908756] + [C++ Naming Conventions|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908685] + [C++ Files|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908674] + [C++ Statements|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908706] + [C++ Layout and Comments|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=16908737] + [Policy on use of C++11/14|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283399] + [On Using ‘Using’|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283856])  * Coding Style Linters (new; draft from confluence [C++ Coding Standards Compliance|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20283861] and [Python Coding Standards Compliance|https://confluence.lsstcorp.org/display/LDMDG/Python+Coding+Standards+Compliance]  * ✅ *Using C++ Templates* ([confluence|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=20284190]); this page needs to severely edited or re-written, however.  * ✅ *Profiling* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Profiling|]). Also add a section ‘Using Valgrind with Python' (new) [[~jsick]]  * ✅ *Boost Usage* ([TRAC|https://dev.lsstcorp.org/trac/wiki/TCT/BoostUsageProposal]) [[~tjenness]]  * ✅ *Software Unit Test Policy* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Software+Unit+Test+Policy]) [[~swinbank]]  * ✅ *Unit Test Coverage Analysis* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Coverage+Analysis]) [[~swinbank]]  * ✅ *Unit Testing Private C++ Functions* ([trac|https://dev.lsstcorp.org/trac/wiki/UnitTestingPrivateFunctions]) [[~swinbank]]    h3. Writing Docs — /docs/    * *Introduction* (new): Overview of DM’s documentation needs; links resources on technical writing.  * *English Style Guide* (new): Supplement the [LSST Style Manual|https://www.lsstcorp.org/docushare/dsweb/Get/Document-13016/LSSTStyleManual.pdf] and provide English style guidance specific to DM. Capitalization of different heading levels; use of Chicago Manual of Style; a ‘this, not that’ table of spelling and word choices.  * ✅ *ReStructuredText Style Guide* (new)  * ✅ *Documenting Stack Packages* (new)  * ✅ *Documenting Python Code* (new)  * ✅ *Documenting C++ Code* (confluence, adapted from [Documentation Standards|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards]); needs improvement  * ✅ *Writing Technotes* (new; port README from [lsst-technote-bootstrap|https://github.com/lsst-sqre/lsst-technote-bootstrap/blob/master/README.rst])    h3. Developer Tools — /tools/    * ✅ *Git Setup and Best Practices* (new)  * ✅ *Using Git Large File Storage (LFS) for Data Repositories* (new)  * ✅ *JIRA Work Management Recipes* (new)  * ✅ *Emacs Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Emacs+Support+for+LSST+Development]). See DM-5045 for issue with Emacs config repo - [~jsick]  * ✅ *Vim Configuration* ([Confluence|https://confluence.lsstcorp.org/display/LDMDG/Config+for+VIM]) - [~jsick]    h3. Developer Services — /services/    * ✅ *NCSA Nebula OpenStack Guide* (Confluence: [User Guide|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+User+Guide] + [Starting an Instance|https://confluence.lsstcorp.org/display/LDMDG/Introduction+to+Starting+a+Nebula+Instance] + [Using Snapshots|https://confluence.lsstcorp.org/display/LDMDG/Start+an+Instance+using+a+base+snapshot+with+the+LSST+Stack]. Add the [Vagrant instructions too from SQR-002|http://sqr-002.lsst.io]? [[~jsick]]  * ✅ *Using lsst-dev* (Confluence: [notes Getting Started|https://confluence.lsstcorp.org/display/LDMDG/Getting+Started+in+DM] + [Developer Tools at NCSA|https://confluence.lsstcorp.org/display/LDMDG/Developer+Tools+at+NCSA]  * ✅ *Using the Bulk Transfer Server at NCSA* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Using+the+Bulk+Transfer+Server+at+NCSA]) [[~jsick]]    h3. Build, Test, Release — /build-ci/    * *Eups for LSST Developers* (new) [[~swinbank]]  * ✅ *The LSST Software Build Tool* → ‘Using lsstsw and lsst-build' ([confluence|https://confluence.lsstcorp.org/display/LDMDG/The+LSST+Software+Build+Tool]); lsstsw and lsst-build documentation. [[~swinbank]]  * *Using DM’s Jenkins for Continuous Integration* (new) [~frossie]   * ✅ *Adding a New Package to the Build*([confluence|https://confluence.lsstcorp.org/display/LDMDG/Adding+a+new+package+to+the+build]) [[~swinbank]]  * ✅ *Distributing Third-Party Packages with Eups* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Distributing+third-party+packages+with+EUPS]) [[~swinbank]]  * ✅  *Triggering a Buildbot Build* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Triggering+a+Buildbot+Build]) [~frossie]  * ✅ *Buildbot Errors FAQ* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+FAQ+on+Errors]) [~frossie]  * * Buildbot configuration ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Buildbot+Configuration+and+Setup] [~frossie]    * *Creating a new DM Stack Release* ([confluence|https://confluence.lsstcorp.org/display/LDMDG/Creating+a+new+DM+Stack+Release]); though this page or a modern equivalent should probably belong with the software docs? [~frossie]    _A lot of work should go into this section._ Have something about Scons? Or maybe that belongs in the doc of each relevant software product.    h2. Leftover Confluence pages    h3. The following pages should be moved to a separate Confluence space run by NCSA:    * [NCSA Nebula OpenStack Issues|https://confluence.lsstcorp.org/display/LDMDG/NCSA+Nebula+OpenStack+Issues]  * [DM System Announcements|https://confluence.lsstcorp.org/display/LDMDG/DM+System+Announcements]  * [NCSA Development Servers|https://confluence.lsstcorp.org/display/LDMDG/DM+Development+Servers]    h3. The following pages are either not relevant, generally misplaced, or need to be updated/recalibrated:    * [Git Crash Course|https://confluence.lsstcorp.org/display/LDMDG/Git+Crash+Course]  * [Basic Git Operations|https://confluence.lsstcorp.org/display/LDMDG/Basic+Git+Operations]  * [Handling Git Push Problems|https://confluence.lsstcorp.org/display/LDMDG/Handling+Git+Push+Problems]  * [LSST Code Repositories|https://confluence.lsstcorp.org/display/LDMDG/LSST+Code+Repositories]; see the proposed “Finding Code on GitHub” page for a replacement.  * [Standards and Policies|https://confluence.lsstcorp.org/display/LDMDG/Standards+and+Policies]: this is a good TOC for the Confluence docs; but not longer needed for the new docs.  * [Documentation Guidelines|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Guidelines]. Some of this could be re-purposed into an intro to the ‘Writing Documentation’ section; some of this should go in a ‘Processes' page.  * [DM Acknowledgements of Use|https://confluence.lsstcorp.org/display/LDMDG/DM+Acknowledgements+of+Use]: this probably belongs in documentation for the software projects that actually used this work.",5
"DM-5014","02/03/2016 11:35:35","Set doRenorm default to False in AssembleCcdTask","Change the default value of {{AssembleCcdConfig.doRenorm}} to {{False}} for the reasons given in RFC-157 and to implement that RFC.",1
"DM-5018","02/03/2016 12:17:28","Modernize version check scripts in matplotlib and numpy packages","The version check scripts in the stub {{matplotlib}} and {{numpy}} eups packages use old Python conventions. They should be updated to work with 2.7+.",0.5
"DM-5022","02/03/2016 13:55:47","Modernize python code in Qserv scons package","The {{site_scons}} Python code is not using current project standards. For example, print is not a function, exceptions are not caught {{as e}}, {{map}} is called without storing the result and {{map/filter/lambda}} are used where list comprehensions would be clearer.    Most of these fixes are trivial with {{futurize}}.",0.5
"DM-5026","02/03/2016 14:45:53","Fix dependencies for eups-packaged sqlalchemy","Eups-packaged sqlalchemy lists {{mysqlclient}} as required dependency which is not really right. sqlalchemy does not directly depend on mysql client stuff, instead it determines at run time which python modules it needs to load depending on what exact driver client code is requesting (and {{mysqlclient}} does not actually provides python module so this dependency does not even make anything useful). So dependency on specific external package should be declared on client side and not in sqlalchemy, {{mysqlclient}} should be removed from sqlalchemy.table.",1
"DM-5030","02/03/2016 16:56:05","Tests fail on Qserv on OS X El Capitan because of SIP","OS X El Capitan introduced System Integrity Protection which leads to dangerous environment variables being stripped when executing trusted binaries. Since {{scons}} is launched using {{/usr/bin/env}} the tests that run do not get to see {{DYLD_LIBRARY_PATH}}. This causes them to fail.    The same fix that was applied to {{sconsUtils}} (copying the path information from {{LSST_LIBRARY_PATH}}) needs to be applied to the test execution code used by Qserv's private {{site_scons}} utility code.",2
"DM-5050","02/04/2016 13:10:42","SingleFrameVariancePlugin takes variance of entire image","{{SingleFrameVariancePlugin}} takes the median variance of the entire image, rather than within an aperture around the source of interest.  A {{Footprint}} is constructed with the aperture, but it is unused.    This means that this plugin takes an excessive amount of run time (255/400 sec in a recent run of processCcd on HSC {{visit=1248 ccd=49}} with DM-4692).",1
"DM-5052","02/04/2016 17:52:19","Design replacement for A.net index files","We need a simple way to hold index files that will be easy to use and simple to set up.",2
"DM-5084","02/05/2016 13:32:30","PropagateVisitFlags doesn't work with other pipeline components","{{PropagateVisitFlags}}, which was recently ported over from HSC on DM-4878, doesn't work due to some inconsistencies with earlier packages/tasks:   - The default fields to transfer have new names: ""calib_psfCandidate"" and ""calib_psfUsed""   - We're not currently transferring these fields from icSrc to src, so those fields aren't present in src anyway.  I propose we just match against icSrc for now, since it has all of the fields we're concerned with.   - It makes a call to {{afw.table.ExposureCatalog.subsetContaining(Point, Wcs, bool)}}, which apparently exists in C++ but not in Python; I'll look into seeing which HSC commits may have been missed in that port.",1
"DM-5085","02/05/2016 13:35:55","Please add a package that includes obs_decam, obs_cfht and all validation_data datasets","It would be very helpful to have an lsstsw package that added all supported obs_* packages (certainly including obs_cfht and obs_decam, and I hope obs_subaru) and all validation_data_* packages. This could be something other than lsst_apps, but I'm not sure what to call it.",0.5
"DM-5086","02/05/2016 13:58:51","Enable aperture correction on coadd processing","Aperture corrections are now coadded, so we can enable aperture corrections in measurements done on coadds.",0.5
"DM-5094","02/08/2016 14:15:59","HSC backport: Set BAD mask for dead amps instead of SAT","This is a port of [HSC-1095|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1095] and a leftover commit from [HSC-1231|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1231]: [isr: don't perform overscan subtraction on bad amps|https://github.com/HyperSuprime-Cam/obs_subaru/commit/d6fe6cf5c4ecadebd5a344d163e1f1e60137c7e4] (noted in DM-3942).",3
"DM-5095","02/08/2016 14:51:18","Redirect confluence based pages to new developer guide.","Delete and apply redirects to all migrated pages in old Confluence-based Developer Guide",0.5
"DM-5100","02/09/2016 10:02:18","Docs for ltd-keeper","Create a documentation project within ltd-keeper that documents the RESTful API while it is being developed. This will allow the [SQR-006|http://sqr-006.lsst.io] technote to have a place to link to for detailed information.",1
"DM-5107","02/09/2016 17:12:57","Fix effective coordinates for defects in obs_subaru","The defects as defined in {{obs_subaru}} (in the {{hsc/defects/20NN-NN-NN/defects.dat}} files) are defined in a coordinate system where pixel (0, 0) is the lower left pixel.  However, the LSST stack does not use this interpretation, preferring to maintain the coordinate system tied to the electronics.  As such, the defect positions are being misinterpreted for the rotated CCDs in HSC (see [HSC CCD layout|http://www.naoj.org/Observing/Instruments/HSC/CCDPosition_20150804.png]).  This needs to be remedied.",2
"DM-5120","02/10/2016 12:46:51","Add intelligence to `validate_drp` so it does ""A Reasonable Thing"" on an unknown output repo","validate_drp current takes as input both a repository and a configuration file.  The configuration file contains information to construct the list of dataIds to analyze.    However, these dataIds could be extracted from the repo itself, in cases where the desired is to analyze the entire repo.      1.  Add a function that loads the set of dataIds from the repo. (/)  2.  Select reasonable defaults for the additional parameters specified in the config file. (/)  3.  Design how to handle multiple filters. (/)",5
"DM-5121","02/10/2016 12:50:27","Add multiple-filter capabilities to `validate_drp`","Design and refactor `validate_drp` to produce results for multiple filters.    1. Decide on the syntax for the YAML configuration file that denotes the multiple filters.  E.g., which visit goes with what filter? (/)  2. Organize the running of multiple filters in `validate.run` to sequentially generate statistics and plots for each filter. (/)  3. Add a filter designation to the default output prefix. (/)    Note: matching objects *across* filters is out-of-scope for this ticket.",1
"DM-5122","02/10/2016 13:30:01","LOAD DATA LOCAL does not work with mariadb","After we un-messed mariadb-mysqlclient we see errors now when trying to run integration tests:  {noformat}    File ""/usr/local/home/salnikov/dm-yyy/lib/python/lsst/qserv/wmgr/client.py"", line 683, in _request      raise ServerError(exc.response.status_code, exc.response.text)  ServerError: Server returned error: 500 (body: ""{""exception"": ""OperationalError"", ""message"": ""(_mysql_exceptions.OperationalError) (1148, 'The used command is not allowed with this MariaDB version') [SQL: 'LOAD DATA LOCAL INFILE %(file)s INTO TABLE qservTest_case01_mysql.LeapSeconds FIELDS TERMINATED BY %(delimiter)s ENCLOSED BY %(enclose)s                          ESCAPED BY %(escape)s LINES TERMINATED BY %(terminate)s'] [parameters: {'terminate': u'\\n', 'delimiter': u'\\t', 'enclose': u'', 'file': '/home/salnikov/qserv-run/2016_02/tmp/tmpWeAj6u/tabledata.dat', 'escape': u'\\\\'}]""}"")  2016-02-10 14:17:40,836 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : qserv-data-loader.py -v --config=/usr/local/home/salnikov/testdata-repo/datasets/case01/data/common.cfg --host=127.0.0.1 --port=5012 --secret=/home/salnikov/qserv-run/2016_02/etc/wmgr.secret --delete-tables --chunks-dir=/home/salnikov/qserv-run/2016_02/tmp/qserv_data_loader/LeapSeconds --no-css --skip-partition --one-table qservTest_case01_mysql LeapSeconds /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.schema /usr/local/home/salnikov/testdata-repo/datasets/case01/data/LeapSeconds.tsv.gz  {noformat}    It looks like mariadb client by default disables LOCAL option for data loading and it needs to be explicitly enabled.  ",1
"DM-5125","02/10/2016 15:05:54","qserv fails when it mixes mariadb and mariadbclient directories","When I tried to run qserv-configure after installing qserv 2016_01-7-gbd0349f I got this error:  {noformat}  2016-02-10 16:03:16,915 - lsst.qserv.admin.commons - CRITICAL - Error code returned by command : /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh  {noformat}    Running script configure/mysql.sh:  {noformat}  $ sh -x /home/salnikov/qserv-run/2016_02/tmp/configure/mysql.sh    + echo '-- Installing mysql database files.'  -- Installing mysql database files.  + /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov  + echo 'ERROR : mysql_install_db failed, exiting'  ERROR : mysql_install_db failed, exiting  + exit 1  {noformat}    and     {noformat}  $ /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/scripts/mysql_install_db --basedir=/u2/salnikov/STACK/Linux64/mariadbclient/10.1.11 --defaults-file=/home/salnikov/qserv-run/2016_02/etc/my.cnf --user=salnikov    FATAL ERROR: Could not find mysqld    The following directories were searched:        /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/libexec      /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/sbin      /u2/salnikov/STACK/Linux64/mariadbclient/10.1.11/bin  {noformat}    So it looks for mysqld in mariadbclient, the same directory as mysql_install_db script, mysql_install_db should be actually running from mariadb.  ",1
"DM-5129","02/11/2016 10:26:34","Create InputField for generic use cases.","Create a composable, validating InputField so it can use outside of the form/submit use-case.",2
"DM-5130","02/11/2016 11:50:35","B-F correction breaks non-HSC custom ISR, ci_hsc","The addition of brighter-fatter correction on DM-4837 breaks obs_cfht's custom ISR, since it slightly changes an internal ISR API by addding an argument that isn't expected by the obs_cfht version.  It also breaks ci_hsc, since the B-F kernel file isn't included in the calibrations packaged there.  ",0.5
"DM-5132","02/11/2016 19:27:30","obs_subaru install with eups distrib fails","Thus:  {code}  $ eups distrib install -t w_2016_06 obs_subaru  ...    [ 52/52 ]  obs_subaru 5.0.0.1-60-ge4efae7+2 ...    ***** error: from /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/build.log:  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/hscRepository.py"", line 91, in setUp      self.repoPath = createDataRepository(""lsst.obs.hsc.HscMapper"", rawPath)    File ""tests/hscRepository.py"", line 63, in createDataRepository      check_call([ingest_cmd, repoPath] + glob(os.path.join(inputPath, ""*.fits.gz"")))    File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 540, in check_call      raise CalledProcessError(retcode, cmd)  CalledProcessError: Command '['/Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/bin/hscIngestImages.py', '/var/folders/jp/lqz3n0m17nqft7bwtw3b8n380000gp/T/tmptUSKuf', '/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/testdata_subaru/master-gf9ba9abdbe/hsc/raw/HSCA90402512.fits.gz']' returned non-zero exit status 1    ----------------------------------------------------------------------  Ran 8 tests in 9.928s    FAILED (errors=7)  The following tests failed:  /Users/jds/Projects/Astronomy/LSST/stack/EupsBuildDir/DarwinX86/obs_subaru-5.0.0.1-60-ge4efae7+2/obs_subaru-5.0.0.1-60-ge4efae7+2/tests/.tests/hscRepository.py.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  + exit -4  {code}  Please fix it.",1
"DM-5135","02/12/2016 10:01:49","Make ci_hsc buildable by Jenkins","1. Make sure {{ci_hsc}} is buildable by {{lsstsw}} / {{lsst_build}}  (/)  2. Add {{ci_hsc}} to lsstsw/etc/repos.yaml so that one can request that Jenkins builds it.  (/)  3. Verify that the test in {{ci_hsc}} fails on known broken tags and passes on known successful tags. (/)    No dependencies will be added to {{lsst_sims}} or {{lsst_distib}}.  This is meant to provide the ability to request that Jenkins do these builds and to fail if something has broken them.    This will later be expanded to new packages {{ci_cfht}}, {{ci_decam}}, and {{ci_sim}}.    The key goal is to make sure one hasn't broken obs_ packages in their butler interface or in their processCcd    Additional Notes and Thoughts from HipChat Discussion  [~ktl]  Sounds good to me; we might have an ""lsst_ci"" top-level metapackage depending on all of them which is what Jenkins would run regularly.     If the goal is to test obs_ packages, then my first instinct would be to put that in the obs_ package.  Longer term goal to test the stack with different precursor datasets.  If this is testing obs_ packages on a slower cadence than the built-in tests, it's OK for that to be a separate package.    [~jbosch]  Eventually, I think we need to run a CI dataset for each camera, then run some camera generic tests on each of those, then run some camera-specific tests on each of those.  So we don't want to go too far down a road in which all tests are camera-specific, but maybe we don't have a choice until we have some better unifying framework for them.  I've certainly been putting some checks in {{ci_hsc}} that would be valid for all other cameras, if we had a CI package for them that went through to coadd processing.",2
"DM-5139","02/12/2016 11:57:08","Update apr and apr_util","{{apr}} and {{apr-util}} are outdated and lagging behind the versions on RHEL6. They should be updated as agreed in RFC-76.",0.5
"DM-5140","02/12/2016 12:39:57","Move luaxmlrpc to lsst-dm/legacy-","We no longer need luaxmlrpc because we run czar inside proxy. We should move it to lsst-dm/legacy-, and remove mentioning it in readme.",0.5
"DM-5147","02/12/2016 21:12:52","Provide usable repos in {{validation_data_*}} packages.","Re-interpreted ticket:  1. Provide already-initialized repositories in the `validation_data_cfht`, `validation_data_decam`, and `validation_data_hsc` packages alongside the raw data.  The goal is to allow both easy quick-start analyses as well as comparisons of output steps from processCcd.py and friends at each step of the processing. (/)  2. Add (Cfht,Decam,HSC).list files to provide for easy processing of the available dataIds in the example data. (/)  3. Update README files to explain available data.  (/)    [Original request:]  In validation_drp when I run examples/runXTest.sh I find that any data I had saved in CFHT or DECam is lost, even if I have carefully renamed it. This is very dangerous and I lost a lot of work due to it. At a bare minimum please do NOT touch any directories not named ""input"" or ""output"".    Lower priority requests that I hope you will consider:  - Have the the input repo be entirely contained in the validation_data_X packages, ready to use ""as is"". That would simplify the use of those packages by other code. It would also simplify validate_drp, and it would just leave the output repo to generate (which already has a link back to the input repo).  - Have runXTest.sh accept a single argument: the path to the output. (The path to the input is not necessary if you implement the first suggestion).",3
"DM-5156","02/15/2016 12:43:37","Please document MemoryTestCase","{{lsst.utils.tests.MemoryTestCase}} is used extensively throughout our test suite, but it is lacking in documentation and it's not clear under what circumstances its use is required or encouraged. Please add appropriate documentation to the [Software Unit Test Policy |http://developer.lsst.io/en/latest/coding/unit_test_policy.html].    See also [this thread on clo|https://community.lsst.org/t/what-is-the-policy-for-using-lsst-utils-tests-memorytestcase].",0.5
"DM-5159","02/15/2016 16:59:16","Please use angle and Coord where possible","validate_drp would be easier to follow and safer if it took advantage of lsst.afw.geom.Angle and lsst.afw.coord.IcrsCoord. For instance {{averageRaDecFromCat}} could return an IcrsCoord and positionRms could use coord1.angularSeparation(coord2) and handle wraparound and other effects simply and safely.",1
"DM-5161","02/16/2016 14:53:45","HSC backport: Support a full background model when detecting cosmic rays","This is a port of the following two standalone HSC commits:    [Support a full background model when detecting cosmic rays|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/3bae328e0fff4b2a02267e97cc1e53b5bbe431cb]  {code}  If there are strong gradients (e.g. M31's nucleus) we need to do more than  treat the background as a constant.  However, this requires making a copy  of the data so the background-is-a-constant model is preserved as a special  case  {code}  [Fixed cosmicRay() in RepairTask for the case background is subtracted.|https://github.com/HyperSuprime-Cam/pipe_tasks/commit/2cdb7c606270d84c7a05baf9949ff5724463fa6b]  {code}  When the background is subtracted with finer binsize, new exposure  will be created and cosmic rays will be detected on that exposure.  But the image of that exposure was not properly returned back.  {code}  ",1
"DM-5164","02/17/2016 12:25:31","Tests in daf_persistence should skip properly","Some of the tests in {{daf_persistence}} have a couple of problems that cause difficulties with modern test frameworks:  # unittest is not being used at all in some cases  # Skipping is done with a print and a {{sys.exit}}    They need to be modernized.",2
"DM-5169","02/17/2016 16:16:22","Fastly API interactions for LSST the Docs","Using Fastly’s API, have ltd-keeper setup new builds and editions:    - Add {{Surrogate-Key}} to headers of objects uploaded to S3 (happens on ltd-mason side)  - Configure Varnish to serve specific bucket directories as specific domains (DM-4951 has added Route 53 interactions to ltd-keeper)  - Purge content when editions switch or content is deleted.    DM-5167 is covering non-API driven work to configure fastly.    See https://www.hashicorp.com/blog/serving-static-sites-with-fastly.html for a write-up on serving static site via fastly. See also http://sqr-006.lsst.io for an overview of LSST the Docs.",8
"DM-5179","02/18/2016 15:02:48","miniconda2 eups package fails to install on OS X","The {{miniconda2}} eups package attempts to install the relevant conda packages by downloading a list from the {{lsstsw}} repository. This fails for the same reason that {{lsstsw}} fails in DM-5178 in that the list of packages is not OS-specific. This means that {{newinstall.sh}} does not work any more on OS X.",0.5
"DM-5182","02/18/2016 17:15:33","Hook up help system","We need to help system like we have in GWT.",8
"DM-5187","02/18/2016 19:02:07","Set Qserv master in env variable for Docker containers","This would allow use of pre-configured container on all clusters, indeed the only parameter which currently change in cluster install is master fqdn.  See http://xrootd.org/doc/dev42/Syntax_config.htm  and  {code}  if defined ?~EXPORTPATH    set exportpath = $EXPORTPATH    else    set exportpath = /tmp    fi    all.export $exportpath    {code}",3
"DM-5196","02/19/2016 11:49:49","swift API availability?","The downtime announcement email for {{Nebula unavailable Feb 9-10}} mentioned a ""roadmap"" for swift.  I have checked and post maintenance, there is not a swift endpoint available in the catalog.  Is there a time line for availability?",1
"DM-5197","02/19/2016 12:34:00","Test and robustify shapelet PSF approximations","The CModel code ported from HSC only works as well as the ShapeletPsfApproximation algorithm that runs before it, but we've switched on the LSST side to a more flexible algorithm that isn't as nearly as battle-tested as what's been running on the HSC side, and there are some concerning indications from [~pgee]'s work that it can be catastrophically slow on some reasonable PSFs.  On this issue, I'll run it on some real HSC data and try to improve it, even if that means reducing the flexibility back to what was on the HSC side in some ways.",8
"DM-5198","02/19/2016 13:14:41","FITS Visualizer porting: Statistics - part 2 - drawing overlay & 3 color support","drawing overlay 3 Color Support",8
"DM-5200","02/19/2016 15:57:33","instance I/O errors","The kernel dmesg for Instance {{bbfd7458-6dd6-4412-a8ba-8d417c3df56b}} has started reporting thousands of block I/O errors and these are starting to trickle up as a filesystem I/O errors.  I suspect this is likely a hypervisor I/O issue.    {code}  [687301.556430] Buffer I/O error on device dm-3, logical block 3768490  [687301.556433] Buffer I/O error on device dm-3, logical block 3768491  [687301.556436] Buffer I/O error on device dm-3, logical block 3768492  {code}    {code}  $ openstack server show bbfd7458-6dd6-4412-a8ba-8d417c3df56b  +--------------------------------------+-----------------------------------------------------------------------+  | Field                                | Value                                                                 |  +--------------------------------------+-----------------------------------------------------------------------+  | OS-DCF:diskConfig                    | MANUAL                                                                |  | OS-EXT-AZ:availability_zone          | nova                                                                  |  | OS-EXT-STS:power_state               | 1                                                                     |  | OS-EXT-STS:task_state                | None                                                                  |  | OS-EXT-STS:vm_state                  | active                                                                |  | OS-SRV-USG:launched_at               | 2016-02-11T23:36:25.000000                                            |  | OS-SRV-USG:terminated_at             | None                                                                  |  | accessIPv4                           |                                                                       |  | accessIPv6                           |                                                                       |  | addresses                            | LSST-net=172.16.1.115, 141.142.209.121                                |  | config_drive                         |                                                                       |  | created                              | 2016-02-11T23:36:12Z                                                  |  | flavor                               | m1.xlarge (5)                                                         |  | hostId                               | f7fbf308022d02f52e1111c91cf578d852784d290d0e0ddb0d69635c              |  | id                                   | bbfd7458-6dd6-4412-a8ba-8d417c3df56b                                  |  | image                                | centos-7-docker-20151116230205 (59a2a478-11ab-41c5-affc-29706d38d65a) |  | key_name                             | vagrant-generated-comshorc                                            |  | name                                 | el7-docker-jhoblitt                                                   |  | os-extended-volumes:volumes_attached | []                                                                    |  | progress                             | 0                                                                     |  | project_id                           | 8c1ba1e0b84d486fbe7a665c30030113                                      |  | properties                           |                                                                       |  | security_groups                      | [{'name': 'default'}, {'name': 'remote SSH'}]                         |  | status                               | ACTIVE                                                                |  | updated                              | 2016-02-11T23:36:25Z                                                  |  | user_id                              | 83bf259d1f0c4f458e03f9002f9b4008                                      |  +--------------------------------------+-----------------------------------------------------------------------+  {code}",1
"DM-5202","02/19/2016 17:54:03","Remove LOGF macros from log package","We have removed all uses of LOGF macros from qserv and as far as I know no other clients use those macros. It's time to clean up log package itself from those macros.",1
"DM-5204","02/19/2016 18:13:36","Remove remaining LOGF macros from qserv","There are still few cases of LOGF macros in qserv, have to replace them all.",1
"DM-5206","02/20/2016 11:21:58","Please do not write garbage to the FITS EQUINOX","The equinox is not relevant when dealing with ICRS coordinates.    When {{afw}} manipulates {{Wcs}} objects, it simply doesn't bother initializing the {{equinox}} field of its {{_wcsInfo}} struct when dealing with an ICRS system.    When {{afw}} persists the {{Wcs}} to FITS, it blindly writes whatever happens to be in that uninitialized field to the FITS header. Thus, we end up with something like:  {code}  EQUINOX =      9.87654321E+107 / Equinox of coordinates  {code}  This should be no problem, since, per the [FITS standard|http://fits.gsfc.nasa.gov/standard30/fits_standard30aa.pdf] (page 30), the {{EQUINOX}} is ""not applicable"" if they {{RADESYS}} is {{ICRS}}. The reader should thus ignore this value.    However, [SAOimage DS9|http://ds9.si.edu] version 7.4.1 (the latest release at time of writing) does _not_ ignore the {{EQUINOX}}. Rather, it refuses to handle the WCS for the image. Note that version 7.3 of DS9 does not seem to have the same issue.    While this does seem to be a bug in DS9, it's easy enough to work around by simply not writing {{EQUINOX}}.",0.5
"DM-5247","02/26/2016 12:01:14","Segfault in shapeHSM centroid extractor","[~boutigny] reports a segfault in {{meas_extenstions_shapeHSM}}. He provides the following backtrace:  {code}  Program received signal SIGSEGV, Segmentation fault.  0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement<lsst::afw::table::Flag> (this=0x21c8d60, key=...)  at include/lsst/afw/table/BaseRecord.h:61  61 typename Field<T>::Element * getElement(Key<T> const & key) {  (gdb) bt  #0 0x00007fffe7043156 in lsst::afw::table::BaseRecord::getElement<lsst::afw::table::Flag> (this=0x21c8d60, key=...)  at include/lsst/afw/table/BaseRecord.h:61  #1 0x00007fffdc8775f2 in set<lsst::afw::table::Flag, bool> (value=<synthetic pointer>, key=..., this=0x21c8d60)  at /home/boutigny/CFHT/lsstsw/stack/Linux64/afw/11.0-8-g38426eb/include/lsst/afw/table/BaseRecord.h:137  #2 setValue (value=true, i=0, record=..., this=0x1da2500) at include/lsst/meas/base/FlagHandler.h:73  #3 lsst::meas::base::SafeCentroidExtractor::operator() (this=<optimized out>, record=..., flags=...)  at src/InputUtilities.cc:134  #4 0x00007fffd03655c6 in lsst::meas::extensions::shapeHSM::HsmPsfMomentsAlgorithm::measure (this=0x1da2410,   source=..., exposure=...) at src/HsmMoments.cc:115  #5 0x00007fffd06708d5 in _wrap_HsmPsfMomentsAlgorithm_measure (args=0x7fffccc67b90)  at python/lsst/meas/extensions/shapeHSM/hsmLib_wrap.cc:14337  #6 0x00007ffff7aee37f in ext_do_call (nk=-859407472, na=<optimized out>, flags=<optimized out>,   pp_stack=0x7fffffff7d18, func=0x7fffd0c21878) at Python/ceval.c:4345  #7 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2720  #8 0x00007ffff7aefdbe in PyEval_EvalCodeEx (co=0x7fffd0a9ceb0, globals=<optimized out>, locals=<optimized out>,   args=<optimized out>, argcount=3, kws=0x7fffccd43b08, kwcount=0, defs=0x0, defcount=0, closure=0x0)  at Python/ceval.c:3267  {code}    See the discussion at DM-4780.",2
"DM-5251","02/26/2016 12:35:45","lsstsw breakage with spaces in paths","There are still some issues relating to using {{lsstsw}} to build the stack when spaces are in the path to the {{$LSSTSW}} location. This is a fine thing to sort out on Rodeo Day...",1
"DM-5252","02/26/2016 13:21:52","Base ""bright star"" cut on S/N instead of magnitudes","The astrometry histogram generated by validateDrp.py conflates astrometric and photometric calibration because it uses magnitude for brightness, and this relies on the accuracy of the photometric calibration. [~ctslater] suggests (and I agree) that brightness should be based on signal to noise ratio, thus making the astrometry histogram independent of photometric calibration.  ",2
"DM-5264","02/26/2016 15:41:01","Modernize python in lsst_build","The python in {{lsst_build}} uses old-style print and exception handling. These should be updated to the current standard.",1
"DM-5265","02/26/2016 15:47:11","Turn on bias-jump fix for all CCDs ","The overscan fix to handle bias jump in an amplifier done in DM-4366 introduced a new config parameter {{overscanBiasJumpBKP}}, and the fix is applied for CCDs on the backplanes specified in {{overscanBiasJumpBKP}}.  Previously, the default is to only fix CCDs on backplanes next to the focus chips. But [~mfisherlevine] also see the bias jump features in other CCDs.  It would make more sense to turn it on for all CCDs by default. ",0.5
"DM-5275","02/29/2016 17:17:05","make floating point exception handling cross-platform (or remove it)","jointcal currently has a couple of trapfpe() functions that wrap feenableexcept, which doesn't exist on OSX. Were these an important part of error handling in meas_simastrom, or can I just remove them?",2
"DM-5277","03/01/2016 08:30:39","replace buildbot with jenkins job(s)","Removing buildbot and replacing it with jenkins would provide a number of benefits    * one less dashboard for developers to know about / interact with  * one less system for SQRE to maintain  * lessening the cost of refactoring the CI drivers scripts as synchronized updates to two CI system configurations would no longer be necessary    It should also be easy to go one step further and try to eliminate the need for developers to manually log into the {{lsstsw}} account on {{lsst-dev}} to publish eups distrib packages. ",3
"DM-5279","03/01/2016 10:09:57","arrays not properly transmitted","Sending a property set with an array as one of the entries only passes the last element of the array.",1
"DM-5281","03/01/2016 11:20:16","Port HSC skymap, shapelet changesets to LSST","We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * skymap:  ** f83f71718eac5307d575d3113ee3757a63a16de2: Set vertex list in ExplicitTractInfo.    * shapelet:  ** bb928df3fc2fafe5183e0d075da19994f0af4fc7: Let the value to normalize to be specified in [Multi]ShapeletFunction  ",0.5
"DM-5283","03/01/2016 11:24:33","Port HSC daf_butlerUtils changesets to LSST","We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * daee24edba01b01a0412df7f9b4cf70be5b10860: CameraMapper: allow a default filter name to be provided  * e3fee95d6a1850dd2309d3ebe4e3ef3ffe38eef0: CameraMapper: normalize path names, and remove leading double slash  * 476b6ddccd9d0cceb2b89ca34bee7d0fdcd70694: preserve timestamps in cameraMapper.backup()  * b2491ef60e5e23afa7d9f0297f257e694aa1af35: Only attempt to update Wcs if it's available  * 9f62bcce588fa9abc8e1e44ff2f0275e5230f629: Registry: hold registry cache for a single thread only (HSC-1035)  * 412f03b95b7a5e82003ab33a61bd43adbf465188: Registry: use a pool of registries to avoid having too many open files",2
"DM-5284","03/01/2016 11:27:32","Port HSC meas_extensions_simpleShape package to LSST","HSC uses a package, meas_extensions_simpleShape, which needs to be ported to LSST.  The package is used for basic shape measurements for determining focus, and also serves as a simple guide for writing measurement plugins.",3
"DM-5286","03/01/2016 11:32:39","Port HSC meas_deblender changesets to LSST","We identified in DM-5162 a few changesets that still need to be ported from HSC to LSST:    * a8cf6c22df14494d6dcf2d7354c695cba9506301: Clarify tiny footprint limit  * 624790aa63a38fb7a328ebc21abfd1b10503aa26: config: change default strayFluxRule  * db7d705de93b43a5f32f771c716b1c5c7368d124: consolidate failed peak logic and downgrade warning    We also identified a few differences that should be resolved:  * clipStrayFluxFraction defaults to 0.01 for LSST, 0.001 for HSC  * Stray file, src/Baseline.cc.orig, on LSST side  ",1
"DM-5287","03/01/2016 11:34:32","Port HSC ip_isr changesets to LSST","We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * f1cee734998f1faf86c02af42ea599b077847eeb: IsrTask: allow fallback to a different filter when loading calibrations  * 89cd629bb8e1a72a545176311b1ef659358d95af: saturationDetection: apply to overscan as well as image  ",1
"DM-5288","03/01/2016 11:38:22","Port HSC pipe_tasks changesets to LSST","We identified in DM-5162 some changesets that still need to be ported from HSC to LSST:    * 31ab5f02f7722650ad0a0eb4e2f7f8b3e0073366, 0c9a4a06bfb34ed26c72109131ef9f4a8c8f237a: multiBand: save background-subtracted coadd as deepCoadd_calexp  * e99e140feafe28e6f034143e8ee2ae58e9a9358d: Rejig interface for DetectCoaddSourcesTask to provide non-dataRef-centric API  * 829ee0cdd605ed027af1fada4446b715d9a5180d: multiband: activate sky objects  * MeasureMergedCoaddSources.doMatchSources defaults to False  * ProcessImageConfig.doWriteHeavyFootprintsInSources defaults to False ?  * 56666e8feba6893ac95fd4982d3e0daf6baf2d34: WcsSelectImagesTask: catch imagePoly is None    We also noticed some differences:    * * CalibrateConfig.setDefaults doesn't call parent  * CalibrateTask.run isn't returning apCorrMap  * reserveFraction=-1 instead of 0.2  ",3
"DM-5289","03/01/2016 11:45:57","Port HSC obs_subaru changesets to LSST","We identified in DM-5162 several changesets that still need to be ported from HSC to LSST:    * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters  * 69d35a890234e37c1142ddbeff43e62fe36e6c45: Set radius for flux.naive, adjust comment for flux.sinc  * 8ea54d10f5ae56f8b6f244bca76d5796ae015216: config: disable sigma clipping in coadd assembly  * 8d2f4a02d0d668fc82e853b633444d8e0fe80010: config: reduce coadd subregionSize  * e36bd1b4410812ca314f50c01f899d92acc0e7a5: config: set pixelScale for jacobian correction  * Remove processCcdOnsiteDb.py, processStack.py  * Rename stacker.py to coaddDriver.py or whatever Nate chooses in DM-3369  * 49e9f5dcf16490f6be6438b89b17911a0cd35fb2: Fixed obvious errors caused by introducing VignetteConfig  * 8948917de4579e032c7bbb2c8316014446e3841b: config: add astrometry filter map for HSC narrow-band filters  * daa43eeac46e8708de6f37feeb5d5d16a3caca11: HscMapper: set unit exposure time for dark  * 77ff7c89d56bed94bca4f320f839dbd20fbab641: Set BAD mask for dead amps instead of SAT    We also noticed the following need to be done:    * Forced photometry configuration (CCDs and Coadds)  * Sanitize config of OBS_SUBARU_DIR (use getPackageDir)  * multiband config files need ""root"" --> ""config""  * No astrometry in measureCoaddSources  * Narrow bands missing from priority list  * detectCoaddSources removed from multiband  * Move filterMap from config/processCcd.py into own file",5
"DM-5290","03/01/2016 13:37:42","Add z-index for dialogs components","Some of the outside modules that we have brought in have a z-index.  We need to make sure that our dialog components stay on top of them.",2
"DM-5291","03/01/2016 13:44:37","Docker-ready configuration system for LTD Keeper","To deploy LTD Keeper in a Docker container (DM-5194), it’s best practice to handle all configurations through environment variables. In DM-4950, LTD Keeper was configured through files for test and dev/deployment profiles. What we should do is continue to allow hard-coded configurations for test and dev environments, but have a third fully fledged configuration environment that’s driven entirely by environment variables.    The environment variables should allow fine grained configuration (for example, to turn off calls to individual external services for testing).    This should also resolve how to deal with Google Container Engine/Kubernetes auth flow works with environment variables, config files, and profiles.",1
"DM-5298","03/01/2016 16:54:28","Document simple simulator","Document the simple simulator produced in DM-4899.  This will also involve some refactoring and adding unit tests to make it usable by others in the group.",8
"DM-5302","03/02/2016 11:03:46","manage jenkins core + plugin versions","There have been a couple of issues that have arisen when deploying test instances vs updating an existing instance due to slight differences between plugin versions.  This would be avoided by putting all plugin versions under change control.    Including:  * The versions of all jenkins components need to be explicitly specified  * The stored job {{config.xml}}'s should be updated to reflect plugin version changes  * The hipchat notification configuration should be updated to fix breakage caused by the production core/plugin update earlier this week  ",5
"DM-5312","03/02/2016 15:12:36","Additional vertical partitioning tests","Test potential improvements in many-vertical-shards test (20,50) run-times with query optimizer settings.",5
"DM-5319","03/03/2016 07:45:00","Fix mariadb CI","patch package is missing in docker container used by travis-CI.",1
"DM-5320","03/03/2016 08:04:34","Make Bright Object Masks compatible with all cameras","Currently all of the logic that goes into using bright object masks falls into obs_subaru and pipe_tasks. This ticket should move parts (such as the bright object mask class) out of obs_subaru, into a camera agnostic location. The work should also duplicate relevant camera configurations and parameter overrides in the other camera packages. Bright object masks were originally introduced in DM-4831",2
"DM-5321","03/03/2016 09:22:08","MeasureApCorrTask should use slot_CalibFlux as default ref flux","{{MeasureApCorrTask}} uses ""base_CircularApertureFlux_17_0"" as its default reference flux. It should use ""slot_CalibFlux"" instead.    Also check obs_sdss packages for overrides that can be removed; obs_sdss certainly has one in {{config/processCcdTask.py}}",0.5
"DM-5324","03/03/2016 13:13:09","Convert GWT code to pure JavaScript (X16, part2, basic)","Continue to work on the GWT code conversion to JavaScript.",100
"DM-5336","03/04/2016 08:25:53","Fix minor issues in docker procedure","- params.sh was missing at configuration  - startup.py wasn't importing correctly module ""utils""  - remove unused parameters in params.sh",1
"DM-5348","03/04/2016 12:07:31","Get rid of ProcessCcdSdssTask and ProcessCcdDecamTask","Update {{ProcessCcdTask}} so that it can be used with different datasete types as appropriate for the ISR task. This will allow us to get rid of obs-specific variants {{ProcessCcdSdssTask}} and {{ProcessCcdDecamTask}}    The plan is to change {{ProcessCcdTask}} as follows:  - set {{doMakeDataRefList=False}} in the call to {{add_id_argument}}  - get the dataset type from the ISR task (default to ""raw"") and set it in data container  - make the dataRef list by calling {{makeDataRefList}} on the data container    Question for DECam folks: do you want two executable scripts for DECam (one that processes data from the community pipeline and one that performs ISR)? Or do you prefer one exectutable (in which case you switch between performing ISR and reading the output of the community pipeline output by retargeting the ISR task)? If you prefer one binary, then which should be the default: perform ISR or read the output of the community pipeline?",2
"DM-5349","03/04/2016 12:15:18","Revise LSE-140 to account for recent changes to calibration instrumentation","Produce a revision of LSE-140, the DM - to - auxiliary instrumentation ICD, taking into account recent changes to the calibration instrumentation.",5
"DM-5350","03/04/2016 12:20:25","Establish goals and create EA framework for LSE-140 update","Deliverable: together with [~pingraham], identify the changes needed and develop initial content in EA.",2
"DM-5351","03/04/2016 12:27:24","Create change request for LSE-140","Deliverable: change request and document diffs for LSE-140",1
"DM-5355","03/04/2016 16:45:18","meas_algorithms uses packages that are not listed in table file","{{meas_algorithms}} directly uses the following packages not expressed in the table file:  * Minuit2  * daf_persistence  * daf_base  * pex_config  * pex_exceptions  * pex_policy  ",0.5
"DM-5356","03/05/2016 10:37:51","Test consistency of Shear Measurements with different Psfs","DM-1136 was done with a single Psf, partly to avoid some of the problems we found with PsfShapeletApprox.  In this issue, I will look at consistency of the measurement for different Psfs.",8
"DM-5359","03/05/2016 22:53:37","Update DMTN-002 to reflect last changes","Need to update documentation with latest changes on {{pipe_base}}, {{pipe_supertask}} and {{pipe_flow}}",1
"DM-5364","03/06/2016 17:53:15","Image Select Panel: Support add or modify of plot","previously the image select panel would only modify a plot.  Now give it the ability to add a plot.",8
"DM-5370","03/07/2016 13:56:37","Create lsst_ci package as a continuous integration build target","Create an {{lsst_ci}} package to be built for the continuous integration testing.    Plan:  1. Create empty package that has dependencies on {{obs_cfht}}, {{obs_decam}}, {{obs_subaru}}, {{testdata_cfht}}, {{testdata_decam}}, {{testdata_subaru}}. (/)  2.  Ensure above builds. (/)  3.  Add {{obs_lsstSim}} and ensure that it builds. (/)    The following were moved to DM-5381:  [ [~tjenness] : How can I get strikethrough to work in the following list?]  3. Add dependencies on {{validation_data_cfht}} and {{validation_data_decam}}, and {{validate_drp}}.  4. Run CFHT, DECam quick examples in {{validate_drp}}.  5. Test for successful running of the above examples.  Fail and trigger Jenkins FAILURE message if these examples fail.  6. Check performance of CFHT, DECam runs against reference numbers.  Fail if there is a significant regression.  7. Decide how to include {{ci_hsc}}, which currently can take at least 30 minutes to process the image data.--",1
"DM-5372","03/07/2016 18:06:30","Fix obs_* packages and ci tests broken by DM-4683","The butler changes in DM-4683, in particular the removal of {{.mapper}} from the interface exposed by a {{Butler}} object, broken {{obs_cfht}}, {{obs_decam}}, and {{ci_hsc}}.    This issue will fix those changes, and search for additional broken things.    This work is proceeding in conjunction with DM-5370 to test that the CI system, e.g. {{lsst_ci}}, is sensitive to these breakages and fixes.",1
"DM-5384","03/08/2016 12:28:38","Port SdssShape changes from HSC meas_algorithms to LSST meas_base","In porting {{meas_algorithm}} changes from HSC to LSST, modifications to the {{SdssShape}} algorithm were discovered. These changes should be transferred to LSST.",3
"DM-5385","03/08/2016 13:55:47","calib_psfReserved is only defined when candidate reservation is activated","The schema should in general not be a function of whether particular features are enabled or disabled so that users can have confidence looking for columns.  However, {{MeasurePsfTask}} only creates the {{calib_psfReserved}} column when {{reserveFraction > 0}}.  This causes warnings when attempting to propagate flags from calibration catalogs to deep catalogs.",1
"DM-5390","03/08/2016 21:22:32","JavaScript loading/caching plan","We need to ensure that the latest version of the application(javascript) is loaded. Conditions: 1. once loaded, it should be cached by the browser. 2. name of the script has to be a static, so it can be referenced by api user. 3. it also has to load dependencies(gwt scripts) after the main script is loaded.  To do this, we created a tiny firefly_loader.js script whose role is to load the main script and then its dependencies. firefly_loader.js is configured to never cache so that the latest main script is always picked up. The main script is appended with a unique hash on every build.  This ensures that the browser will pick up the new script the very first time, and then cache it for future use. ",2
"DM-5392","03/09/2016 09:53:03","Please stop leaving repoCfg.yaml files around","After a recent change to {{daf_persistence}} and possibly other packages I'm finding that many packages leave {{repoCfg.yaml}} files lying around after they run unit tests.    I'm not sure what is best to do about these files. If they are temporary, as I am guessing, then I think we need some way to clean them up when the tests that generated them have run. If they are intended to be permanent (which would be surprising for auto-generated files) then they should probably be committed?    I hope we can do better than adding them to .gitignore.",1
"DM-5394","03/09/2016 11:41:15","Investigate boost compiler warnings and update boost to v1.60","As reported in comments in DM-1304 clang now triggers many warnings with Boost v1.59:  {code}  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/archive/detail/check.hpp:148:5: warning: unused typedef 'STATIC_WARNING_LINE148' [-Wunused-local-typedef]      BOOST_STATIC_WARNING(typex::value);      ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:100:33: note: expanded from macro 'BOOST_STATIC_WARNING'  #define BOOST_STATIC_WARNING(B) BOOST_SERIALIZATION_BSW(B, __LINE__)                                  ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/serialization/static_warning.hpp:99:7: note: expanded from macro 'BOOST_SERIALIZATION_BSW'      > BOOST_JOIN(STATIC_WARNING_LINE, L) BOOST_STATIC_ASSERT_UNUSED_ATTRIBUTE;         ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:544:28: note: expanded from macro 'BOOST_JOIN'  #define BOOST_JOIN( X, Y ) BOOST_DO_JOIN( X, Y )                             ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:545:31: note: expanded from macro 'BOOST_DO_JOIN'  #define BOOST_DO_JOIN( X, Y ) BOOST_DO_JOIN2(X,Y)                                ^  /Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/boost/1.59.lsst5/include/boost/config/suffix.hpp:546:32: note: expanded from macro 'BOOST_DO_JOIN2'  #define BOOST_DO_JOIN2( X, Y ) X##Y                                 ^  <scratch space>:25:1: note: expanded from here  STATIC_WARNING_LINE148  ^  {code}  v1.60 is the current version so we should see if these warnings have been fixed in that version.",2
"DM-5402","03/09/2016 14:25:39","Make cluster deployment scripts more generic and enable ccqserv100...124","These scripts will be improved (i.e. more genericity) and integrated inside Qserv code. Qserv will be deployed on ccqserv100 to ccqserv125",3
"DM-5406","03/09/2016 18:13:18","Require fields listed in icSourceFieldsToCopy to be present","{{CalibrateTask}} presently treats config field {{icSourceFieldsToCopy}} as a list of fields to copy *if present*. This was required because one of the standard fields to copy was usually missing. However, [~price] fixed that problem in DM-5385. Now we can raise an exception if any field listed is missing (though I propose to continue ignoring {{icSourceFieldsToCopy}} if isSourceCatalog is not provided).",1
"DM-5410","03/10/2016 10:18:51","DecamIngestTask is mis-calling openRegistry","`DecamIngestTask` is mis-calling `lsst.pipe.tasks.RegistryTask`. Line 59:    {code}  with self.register.openRegistry(args.butler, create=args.create, dryrun=args.dryrun) as registry:  {code}  {{openRegistry}} is expecting a directory name, not a butler object for the first argument    Thanks to [~wmwood-vasey] for diagnosing this.",1
"DM-5416","03/10/2016 15:27:11","Ci Deploy and Distribution Improvements part IV","This is a bucket epic for ongoing improvements to the CI system",8
"DM-5419","03/10/2016 15:45:31","ci_hsc fails test requiring >95% of PSF stars to be stars on the coadd","Since the first week of March 2016, ci_hsc fails its test that requires that >95% of the PSF stars be identified as stars in the coadd.  I suspect this is related to the DM-4692 merge.    Here is a sample job that fails:  https://ci.lsst.codes/job/stack-os-matrix/9084/label=centos-6/console    The relevant snippet of the failure is:    {code}  [2016-03-10T17:12:06.667778Z] : Validating dataset measureCoaddSources_config for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:06.697383Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3  [2016-03-10T17:12:06.697615Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  [2016-03-10T17:12:07.716310Z] CameraMapper: Loading registry registry from /home/build0/lsstsw/build/ci_hsc/DATA/registry.sqlite3  [2016-03-10T17:12:07.716443Z] CameraMapper: Loading calibRegistry registry from /home/build0/lsstsw/build/ci_hsc/DATA/CALIB/calibRegistry.sqlite3  [2016-03-10T17:12:08.663566Z] : measureCoaddSources_config exists: PASS  [2016-03-10T17:12:08.721051Z] : measureCoaddSources_config readable (<class 'lsst.pipe.tasks.multiBand.MeasureMergedCoaddSourcesConfig'>): PASS  [2016-03-10T17:12:08.721077Z] : Validating dataset measureCoaddSources_metadata for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:08.721249Z] : measureCoaddSources_metadata exists: PASS  [2016-03-10T17:12:08.721663Z] : measureCoaddSources_metadata readable (<class 'lsst.daf.base.baseLib.PropertySet'>): PASS  [2016-03-10T17:12:08.721715Z] : Validating dataset deepCoadd_meas_schema for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:08.721878Z] : deepCoadd_meas_schema exists: PASS  [2016-03-10T17:12:08.726703Z] : deepCoadd_meas_schema readable (<class 'lsst.afw.table.tableLib.SourceCatalog'>): PASS  [2016-03-10T17:12:08.726834Z] : Validating source output for {'filter': 'HSC-R', 'tract': 0, 'patch': '5,4'}  [2016-03-10T17:12:10.203469Z] : Number of sources (7595 > 100): PASS  [2016-03-10T17:12:10.204166Z] : calib_psfCandidate field exists in deepCoadd_meas catalog: PASS  [2016-03-10T17:12:10.204772Z] : calib_psfUsed field exists in deepCoadd_meas catalog: PASS  [2016-03-10T17:12:10.205468Z] : Aperture correction fields for base_PsfFlux are present.: PASS  [2016-03-10T17:12:10.206159Z] : Aperture correction fields for base_GaussianFlux are present.: PASS  [2016-03-10T17:12:10.207193Z]  FATAL: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0): FAIL  [2016-03-10T17:12:10.207455Z] scons: *** [.scons/measure-HSC-R] AssertionError : Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0)  [2016-03-10T17:12:10.207481Z] Traceback (most recent call last):  [2016-03-10T17:12:10.207525Z]   File ""/home/build0/lsstsw/stack/Linux64/scons/2.3.5/lib/scons/SCons/Action.py"", line 1063, in execute  [2016-03-10T17:12:10.207556Z]     result = self.execfunction(target=target, source=rsources, env=env)  [2016-03-10T17:12:10.207593Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 133, in scons  [2016-03-10T17:12:10.207611Z]     return self.run(*args, **kwargs)  [2016-03-10T17:12:10.207646Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 122, in run  [2016-03-10T17:12:10.207663Z]     self.validateSources(dataId)  [2016-03-10T17:12:10.207732Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 191, in validateSources  [2016-03-10T17:12:10.207749Z]     0.95*psfStars.sum()  [2016-03-10T17:12:10.207786Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 52, in assertGreater  [2016-03-10T17:12:10.207816Z]     self.assertTrue(description + "" (%d > %d)"" % (num1, num2), num1 > num2)  [2016-03-10T17:12:10.207853Z]   File ""/home/build0/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 43, in assertTrue  [2016-03-10T17:12:10.207877Z]     raise AssertionError(""Failed test: %s"" % description)  [2016-03-10T17:12:10.207919Z] AssertionError: Failed test: 95% of sources used to build the PSF are classified as stars on the coadd (0 > 0)  [2016-03-10T17:12:10.209935Z] scons: building terminated because of errors.  {code}    This is the test that fails    https://github.com/lsst/ci_hsc/blob/74303a818eb5049a2015b5e885df2781053748c9/python/lsst/ci/hsc/validate.py#L169  {code}  class MeasureValidation(Validation):      _datasets = [""measureCoaddSources_config"", ""measureCoaddSources_metadata"", ""deepCoadd_meas_schema""]      _sourceDataset = ""deepCoadd_meas""      _matchDataset = ""deepCoadd_srcMatch""        def validateSources(self, dataId):          catalog = Validation.validateSources(self, dataId)          self.assertTrue(""calib_psfCandidate field exists in deepCoadd_meas catalog"",                          ""calib_psfCandidate"" in catalog.schema)          self.assertTrue(""calib_psfUsed field exists in deepCoadd_meas catalog"",                          ""calib_psfUsed"" in catalog.schema)          self.checkApertureCorrections(catalog)          # Check that at least 95% of the stars we used to model the PSF end up classified as stars          # on the coadd.  We certainly need much more purity than that to build good PSF models, but          # this should verify that flag propagation, aperture correction, and extendendess are all          # running and configured reasonably (but it may not be sensitive enough to detect subtle          # bugs).          psfStars = catalog.get(""calib_psfUsed"")          extStars = catalog.get(""base_ClassificationExtendedness_value"") < 0.5          self.assertGreater(              ""95% of sources used to build the PSF are classified as stars on the coadd"",              numpy.logical_and(extStars, psfStars).sum(),              0.95*psfStars.sum()          )  {code}    Note that the assertion failure messages is a bit confusing.  It should say  ""Fewer than 95% of the sources used to build the PSF are classified as stars on the coadd.""",1
"DM-5421","03/10/2016 17:57:52","Add --show history option to cmdLineTask","{{pex_config}} is able to report where a config parameter is set.  Please add a command line option {{--show history=config.parameter.name}} to the cmdLineTask parser.    The implementation will probably want to use something like:  {code:python}  import lsst.pex.config.history as pch  pch.Color.colorize(False)  print pch.format(config.calibrate.astrometry.solver, ""matchingRadius"")  {code}  ",2
"DM-5424","03/11/2016 10:11:50","Switch PropagateVisitFlags to use src instead of icSrc","On DM-5084 [~jbosch] switched PropagateVisitFlags to match against icSrc instead of src because we weren't yet matching `icSrc` to `src` in ProcessCcdTask.  That's now been done on DM-4692, so we can revert this.    After doing so, please verify with ci_hsc that this is working, as that's where the only test of this feature lives.",2
"DM-5427","03/11/2016 13:11:00","SingleFrameVariancePlugin can give numpy warnings","SingleFrameVariancePlugin can produce the following numpy warning, with no hint as to where the problem is coming from:  {code}  /Users/rowen/UW/LSST/lsstsw/miniconda/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  {code}  I tracked it down by adding the following code to the calling code:  {code}  import warnings  with warnings.catch_warnings():      warnings.filterwarnings('error')  {code}    It would be nice if the measurement plugin handled this situation more gracefully, such as turning the warning into an exception or testing for it and handling it.    One way to reproduce this problem is to run {{tests/testProcessCcd.py}} in {{pipe_tasks}}. However, it is commonly seen when running {{processCcd}} on other data, as well.",2
"DM-5428","03/11/2016 13:16:32","ObjectSizeStarSelector can produce numpy warnings","`ObjectSizeStarSelector` can produce the following numpy warning:   {code}  RuntimeWarning: invalid value encountered in less  {code}  This occurs at the following point in the code:  {code}          for i in range(nCluster):              # Only compute func if some points are available; otherwise, default to NaN.              pointsInCluster = (clusterId == i)              if numpy.any(pointsInCluster):                  centers[i] = func(yvec[pointsInCluster])  {code}  where `func` has been assigned to `numpy.mean`. When I have seen this occur I have found that `dist` is an array of `nan`    I suggest that the star selector handle this situation more gracefully, e.g. by reporting an appropriate exception or handling the data in an appropriate way. If logging a message would be helpful, then please do that (and if RFC-154 is adopted, a log will be available).    One way to reproduce this is to run `tests/testProcessCcd.py` in `pipe_tasks`. However, I often see it when running `processCcd.py` on other data, as well.",2
"DM-5431","03/11/2016 14:51:34","Changes to galaxy_shear_experiments Python code","This ticket describes changes which were made to the test runner and analysis scripts during the Dec 2015 - Feb 2016 period.  Most of these changes were made as a part of moving to a large computing cluster, where both the units of work and the output file organization had to be changed to make parallelization possible.    The large number of tests run during this period and the need to more efficiently analyze and compare also introduced some changed to the analysis and plot modules.    Since these changes do not pertain to any single test (though many were done during Dm-1136), I have put them on a separate ticket.",5
"DM-5435","03/13/2016 14:27:25","Provide a shared stack on lsst-dev & other relevant systems","Following the discussion in RFC-156, ensure that a documented, fast, easy to initialize shared stack is available for developers to use on shared systems, certainly to include {{lsst-dev}}.",3
"DM-5447","03/14/2016 16:15:19","Write technical note describing galaxy shear fitting experiments","Through S15 (DM-1108) and W16 (DM-3561), [~pgee] has conducted a large-scale investigation into galaxy shear fitting. Please summarize the motivation, methodology and results of this study as a [technical note|http://sqr-000.lsst.io/en/master/].",8
"DM-5448","03/14/2016 16:22:50","Familiarization with ngmix codebase","Download the ngmix codebase from https://github.com/esheldon/ngmix. Install it and its dependencies in the same environment as the LSST stack. Experiment with using it and understanding how it works",3
"DM-5449","03/14/2016 16:34:37","Convert GWT code to pure JavaScript (F16)","The remaining work for converting GWT code to pure JavaScript",100
"DM-5463","03/15/2016 15:39:43","Don't restore the mask in CharacterizeImageTask.characterize","CharacterizeImageTask.characterize presently restores the mask from a deep copy for each iteration of the loop to compute PSF. This is unnecessary because repair and detection both clear the relevant mask planes before setting new values.",1
"DM-5472","03/16/2016 05:47:26","Update meas_mosaic for compatibility with new single frame processing","Following [recent changes to single frame processing|https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581], {{icSrc}} no longer includes celestial coordinates and {{icMatch}} is no longer being written. {{meas_mosaic}} requires this information. Provide a work-around.",3
"DM-5473","03/16/2016 07:30:59","Jenkins/ci_hsc failure: 'base_PixelFlags_flag_clipped' already present in schema","Since 15 March, the {{ci_hsc}} build in Jenkins has been failing as follows:    {code}  [2016-03-16T14:23:13.548928Z] Traceback (most recent call last):  [2016-03-16T14:23:13.548956Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/bin/measureCoaddSources.py"", line 3, in <module>  [2016-03-16T14:23:13.548969Z]     MeasureMergedCoaddSourcesTask.parseAndRun()  [2016-03-16T14:23:13.548999Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun  [2016-03-16T14:23:13.549011Z]     resultList = taskRunner.run(parsedCmd)  [2016-03-16T14:23:13.549040Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 192, in run  [2016-03-16T14:23:13.549048Z]     if self.precall(parsedCmd):  [2016-03-16T14:23:13.549076Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 279, in precall  [2016-03-16T14:23:13.549087Z]     task = self.makeTask(parsedCmd=parsedCmd)  [2016-03-16T14:23:13.549115Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/cmdLineTask.py"", line 369, in makeTask  [2016-03-16T14:23:13.549132Z]     return self.TaskClass(config=self.config, log=self.log, butler=butler)  [2016-03-16T14:23:13.549160Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_tasks/2016_01.0-23-gcf99090/python/lsst/pipe/tasks/multiBand.py"", line 1008, in __init__  [2016-03-16T14:23:13.549179Z]     self.makeSubtask(""measurement"", schema=self.schema, algMetadata=self.algMetadata)  [2016-03-16T14:23:13.549206Z]   File ""/home/build0/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869/python/lsst/pipe/base/task.py"", line 226, in makeSubtask  [2016-03-16T14:23:13.549846Z]     subtask = configurableField.apply(name=name, parentTask=self, **keyArgs)  [2016-03-16T14:23:13.549901Z]   File ""/home/build0/lsstsw/stack/Linux64/pex_config/2016_01.0+1/python/lsst/pex/config/configurableField.py"", line 77, in apply  [2016-03-16T14:23:13.549915Z]     return self.target(*args, config=self.value, **kw)  [2016-03-16T14:23:13.549943Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/sfm.py"", line 248, in __init__  [2016-03-16T14:23:13.549954Z]     self.initializePlugins(schema=self.schema)  [2016-03-16T14:23:13.549985Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseMeasurement.py"", line 298, in initializePlugins  [2016-03-16T14:23:13.550004Z]     self.plugins[name] = PluginClass(config, name, metadata=self.algMetadata, **kwds)  [2016-03-16T14:23:13.550032Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 15, in __init__  [2016-03-16T14:23:13.550616Z]     self.cpp = self.factory(config, name, schema, metadata)  [2016-03-16T14:23:13.550647Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/wrappers.py"", line 223, in factory  [2016-03-16T14:23:13.550660Z]     return AlgClass(config.makeControl(), name, schema)  [2016-03-16T14:23:13.550688Z]   File ""/home/build0/lsstsw/stack/Linux64/meas_base/2016_01.0-12-gf26bc28+1/python/lsst/meas/base/baseLib.py"", line 3401, in __init__  [2016-03-16T14:23:13.552891Z]     this = _baseLib.new_PixelFlagsAlgorithm(*args)  [2016-03-16T14:23:13.552924Z] lsst.pex.exceptions.wrappers.InvalidParameterError:   [2016-03-16T14:23:13.552967Z]   File ""src/table/Schema.cc"", line 563, in lsst::afw::table::Key<lsst::afw::table::Flag> lsst::afw::table::detail::SchemaImpl::addField(const lsst::afw::table::Field<lsst::afw::table::Flag>&, bool)  [2016-03-16T14:23:13.552986Z]     Field with name 'base_PixelFlags_flag_clipped' already present in schema. {0}  [2016-03-16T14:23:13.553012Z] lsst::pex::exceptions::InvalidParameterError: 'Field with name 'base_PixelFlags_flag_clipped' already present in schema.'  [2016-03-16T14:23:13.553014Z]   [2016-03-16T14:23:13.613484Z] scons: *** [.scons/measure] Error 1  [2016-03-16T14:23:13.617577Z] scons: building terminated because of errors.  {code}    Please fix it.",1
"DM-5474","03/16/2016 10:37:43","Bugs in obs_subaru found by PyFlakes","I ran pyflakes on the code in obs_subaru and found a few bugs (beyond a few trivial ones that I am fixing as part of DM-5462)    {{ingest.py}} has undefined name {{day0}}    {{ccdTesting.py}} has at least three undefined variables: {{x}}, {{y}} and {{vig}} in the following:  {code}      ngood += pupilImage[y[good], x[good]].sum()    vig[i] = float(ngood)  {code}    {{crosstalkYagi.py}} has many undefined names, starting with {{makeList}}, {{estimateCoeffs}}",1
"DM-5478","03/16/2016 12:35:13","Write script to derive and collate QA metrics from data repository of processed data","I wrote a python script using stack components to derive QA metrics and collate other QA-relevant information for a data repository of processed data.  This is currently output to a CSV file that can be loaded into a SQL database.",20
"DM-5479","03/16/2016 12:37:07","Wrote script to print the names of all visits that overlap a patch","In order to finish the IDL workflow module for makeCoaddTempExp I needed a program to say which visits overlap a given path.  That's what this script does.",5
"DM-5480","03/16/2016 12:40:18","Processing of COSMOS data - Part II","Continued work on processing and QA work on the COSMOS verification dataset.  Running processCcDecam, making diagnostic plots, and nvestigating the results.  Most recently I've  reprocessed the COSMOS data through processCcdDecam using SDSS as the astrometric and photometric reference catalog and am redoing the QA work on those results.",20
"DM-5482","03/16/2016 12:45:20","Write presentation on verification datasets for AAS","Prepared and gave a talk at the NSF booth at the Florida AAS meeting on the progress of the verification datasets effort.",5
"DM-5483","03/16/2016 12:47:11","Work on script to test the astrometric matcher","We encouraged astrometric matching problems for the Bulge verification dataset.  Therefore, I wrote a script that tests the matcher by systematically shifting the coordinates of one sets of the data to see if the matcher still works.  It worked well until ~80 arcsec.",5
"DM-5484","03/16/2016 12:48:05","SdssMapper.paf has wrong python type for processCcd_config","[~npease] reports that {{Sdssmapper.paf}} has the wrong python data type for the dataset {{processCcd_config}}: it is {{lsst.obs.sdss.processCcdSdss.ProcessCcdSdssConfig}} instead of {{lsst.pipe.tasks.processCcd.ProcessCcdConfig}}",0.5
"DM-5485","03/16/2016 12:48:48","Work on plan to test specific algorithmic components of the stack","After working on a script to test the astrometric matcher, I decided to put together a plan to run similar tests on our algorithmic code.  The rough plan is here:  https://confluence.lsstcorp.org/display/SQRE/Stack+Testing+Plan",2
"DM-5486","03/16/2016 12:51:15","Work on putting together page of ""tips and tricks for using the stack""","Due to the incomplete state of the stack documentation and tutorials, I decided to write down various ""tips and tricks"" for using the stack as I learn them.  https://confluence.lsstcorp.org/display/SQRE/Tips+and+Tricks+for+using+the+Stack",2
"DM-5487","03/16/2016 14:34:54","Revise operations concept for Observation Processing System","Turn the L1 ConOps document into appropriate sections of LDM-230, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.     (Story points are for KTL drafting and initial contributions)",2
"DM-5488","03/16/2016 14:36:42","Field group updates","After some work we have realized that the following needs to be done to field groups:    * Tabs group should have a field group smart wrapper component  * field group needs to reinit on id change   * remove mixin, use Higher-Order Components instead  * support a function for a value, this function will return a value or a promise  * hidden fields - init field group with key/value object  * Sub-field groups? study only, unless it is easy to implement.  * maintain an option to keep unmount field value available  * determine if InitValue needs to be passed around  * passing fieldState around too much  * find reason for react warning every time popup is raised  * look at promise code make sure it is working the way we think  * if practical, remove all export default    FieldGroupConnector.  It is the high order component that replaces the mixin.   FieldGroupUtils.js:  (~line 33): The field value would be a function on the file upload case. Therefore the upload does not activate until validation. In the upload case the function would return a promise. However, It could return a value or an object with a value and a valid status. Now the value key of a field can contain a promise or function or primitive. The function can return a primitive, a promise, or an object with primitive and status.    fftools.js lines 102-158 you can see my experimenting with taking out the connector. It works fine and does eliminate one of the warning messages.    ",8
"DM-5489","03/16/2016 14:37:23","improvement of the north/east arrow on image","make the compass sticky when scroll the image",1
"DM-5490","03/16/2016 14:38:54","Develop operations concept for Batch Processing System","Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the batch processing environment, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3
"DM-5491","03/16/2016 14:39:35","Develop operations concept for Data Backbone","Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Backbone that contains, manages, and provides access to the Science Data Archive, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3
"DM-5492","03/16/2016 14:40:35","Develop operations concept for Data Access Processing System","Develop a ConOps document that can be included as appropriate sections of LDM-230 describing the Data Access Processing System that manages L3 computing in and interfaces to the Data Access Center, specifying automated operations sequences, how human intervention can occur, and processes to handle changes and updates.    (Story points are for KTL drafting and initial contributions)",3
"DM-5493","03/16/2016 14:47:26","Develop functional breakdown for Observation Processing System","Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Observation Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",3
"DM-5494","03/16/2016 14:47:51","Develop functional breakdown for Batch Processing System","Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Batch Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2
"DM-5495","03/16/2016 14:48:13","Develop functional breakdown for Data Backbone","Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Backbone, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2
"DM-5496","03/16/2016 14:48:41","Develop functional breakdown for Data Access Center Processing System","Write sections that can be incorporated into LDM-148 describing the functional breakdown of the Data Access Center Processing System, including, for each major element:  * overall function  * inputs, outputs, and control interfaces  * components used  * descriptions of functions to be performed    (Story points are for KTL drafting and initial contributions)",2
"DM-5498","03/16/2016 15:24:44","Coordinate completion of operations concepts","Coordinate the creation of a new version of LDM-230 incorporating DPS-WG-generated operations concepts.",2
"DM-5499","03/16/2016 15:25:35","Coordinate completion of functional breakdowns","Coordinate the creation of a new version of LDM-148 incorporating DPS-WG-generated functional breakdowns.",2
"DM-5501","03/16/2016 15:42:57","Solve the metadata sanitization problem","Applications need access to visit specific metadata: e.g. pointing, airmass, exposure length.  This information is typically carried around in a FITS header, but there are no conventions on spelling or even necessarily units of these metadata key, value pairs.  There needs to be a easy to use metadata sanitization process that allows data from many different systems to present a standardized interface to observation metadata to the algorithm code.",100
"DM-5502","03/16/2016 15:47:20","Collect usage of header metadata","Collect a comprehensive set of exposure oriented metadata used by science code.  This should also include metadata that is not currently needed but that could be utilized in the future.  In practice, I suspect this will involve looking for all calls to PropertySet.get since that is how FITS header metadata is currently passed around.",5
"DM-5515","03/17/2016 00:48:50","prepare Slack RFC","    https://jira.lsstcorp.org/browse/RFC-140",1
"DM-5530","03/17/2016 11:48:24","Documentation of Firefly functions and API (F16)","We are concentrating on the coding in X16. This epic will be capture the effort to write the document for using Firefly functions and API. ",40
"DM-5542","03/17/2016 14:33:15","AFW rgb.py has undefined variable that breaks a test in some situations","The {{rgb.py}} test is failing for me with current AFW master:  {code}  tests/rgb.py  .E............  ======================================================================  ERROR: testMakeRGBResize (__main__.RgbTestCase)  Test the function that does it all, including rescaling  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/rgb.py"", line 313, in testMakeRGBResize      with Tempfile(fileName, remove=True):  NameError: global name 'Tempfile' is not defined    ----------------------------------------------------------------------  Ran 16 tests in 7.296s    FAILED (errors=1)  {code}    {{Tempfile}} is definitely only used in line 313. It was introduced with commit c9864f49.    I'm not entirely sure how this is not picked up by Jenkins as the test will run if matplotlib and scipy are installed and Jenkins does have those.    ",0.5
"DM-5552","03/17/2016 15:18:14","Add renderer option to js table","TablePanel and BasicTable now accept optional renderers.  For each column, you can set a custom renderer for the header, cell, or both.  Also, created several commonly used renderer for images, links, and input field.",2
"DM-5553","03/17/2016 15:36:05","Z-scale stretch for image display","The z-scale stretch in current system is different from the one in OPS",8
"DM-5560","03/18/2016 13:29:55","Participate in October 2015 OCS-subsystems teleconference","Prepare for, attend, and follow up on the OCS-subsystems teleconference on October 8, 2015.",2
"DM-5563","03/18/2016 13:43:09","Participate in November 2015 OCS-subsystems teleconference (LSE-74)","Prepare for, attend, and follow up on the OCS-subsystems teleconference on November 11, 2015.  This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic.",1
"DM-5564","03/18/2016 13:57:43","Participate in December 2015 OCS-subsystems teleconference (LSE-70, LSE-209)","Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-70 and LSE-209; LSE-74 work was also done under a separate epic.",2
"DM-5565","03/18/2016 14:01:43","Participate in December 2015 OCS-subsystems teleconference (LSE-74)","Prepare for, attend, and follow up on the OCS-subsystems teleconference on December 9, 2015. This story covers work related to LSE-74; LSE-70 and LSE-209 work was also done under a separate epic.",1
"DM-5566","03/18/2016 14:16:47","Review of LSE-70 and LSE-209 drafts, September 2015","Arrange, prepare for, and attend a joint call with the Camera team to review the end-of-summer-2015 drafts of LSE-70 and LSE-209 from the OCS group.",3
"DM-5567","03/18/2016 14:25:26","CCB review of LCR-567 (LSE-70) and LCR-568 (LSE-209)","Review the LSE-70 and LSE-209 drafts submitted with change requests LCR-567 and LCR-568 in January 2016.",2
"DM-5568","03/18/2016 14:41:59","CCB review of LCR-603 (LSE-74)","Review LCR-603, ""LSE-74 document revision""",2
"DM-5580","03/21/2016 01:48:20","Docgen draft from EA content for LSE-140","Create a docgen from the LSE-140 content in Enterprise Architect.",2
"DM-5582","03/21/2016 08:10:03","Support LCR-385","Support getting LCR-385 against LSE-78 through the CCB.",3
"DM-5585","03/22/2016 08:52:17","SQuaRE Communication and Publication Platforms Document and Presentation - Clone","This is a clone of DM-5581 tracking [~frossie]'s SPs",5
"DM-5586","03/22/2016 09:28:44","Fix obs_decam butler level","There is a bug in {{obs_decam/policy/DecamMapper.paf}}, causing some butler features for the ""visit"" level or above working incorrectly. The {{hdu}} key is irrelevant for the visit level or above, but wasn't included in the policy file.    Because of this bug, the {{DemoTask}} in {{ctrl_pool}} (ctrlPoolDemo.py) runs incorrectly with DECam data. It incorrectly treats dataRef with different {{hdu}}s as they are from different visits, hence reads each ccd image multiple times (61 times for one visit with 61 hdu). Instead, each ccd image should be read once.        Besides fixing the policy file, I also added an optional test that only runs if {{testdata_decam}} is set up. The part with level=""visit"" in the test fails without the ticket changes in the policy.    (p.s. The raw data file in {{testdata_decam}} is modified and has only 2 hdus.) ",3
"DM-5590","03/22/2016 14:13:55","Fix afw build issues with recent clang","{{afw}} fails to build with recent versions of clang:    {code}  include/lsst/afw/image/MaskedImage.h:553:65: error: '_loc' is a protected member of 'lsst::afw::image::MaskedImage<unsigned short, unsigned short,        float>::MaskedImageLocatorBase<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<unsigned short,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >,        boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<unsigned short,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >,        boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float,        boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t>, boost::mpl::range_c<int, 0, 1> > > *> >, Reference>'                                       const_VarianceLocator(iter._loc.template get<2>())  {code}  and issues with statistics.i so far, more errors may turn up as these are cleared.    These problems are apparent with {{Apple LLVM version 7.3.0 (clang-703.0.29)}} (as shipped with the latest release of XCode, hence this now becoming an issue) and {{clang version 3.8.0 (branches/release_38 262722)}} (a recent release from LLVM; note that Apple uses its own versioning scheme). {{clang version 3.7.1 (tags/RELEASE_371/final)}} is not affected.",1
"DM-5591","03/22/2016 16:45:54","Archive in a box v1 (F16)","Several times we were asked a question about Firefly: Great software. How could I use it for my data now?     This epic capture the preparation work in Firefly for its final version of ""Archive in a box"". The work is also needed to improve the user experience in using Firefly.     8/1/2017  Future related work will be captured in stories of  epic DM-10853. ",40
"DM-5593","03/23/2016 12:55:36","fix issue where butler repository search returns list for single item","Backwards compatible behavior is that when butler returns a single item, it is NOT in a list. A recent change (when the Repository class was added) broke this behavior.     Change it back so that if an operation in repository would return a list with a  single item, it pulls it from the list.    Note this is only related to the case where a repository's parentJoin field is set to 'outer' and since no one is using this yet (they should not be, anyway) then the point is moot.     ",1
"DM-5594","03/23/2016 14:32:28","Fix qserv service timeout issue","After Qserv services have been running over ~couple of days, new queries fail and can also lead to a crash. Investigate and implement a solution.",5
"DM-5595","03/23/2016 20:31:14","daf_persistence build failure on OSX","I see the following build failure in {{daf_persistence}} on OSX 10.11:  {code}  c++ -o python/lsst/daf/persistence/_persistenceLib.so -bundle -F/ -undefined suppress -flat_namespace -headerpad_max_install_names python/lsst/daf/persistence/persistenceLib_wrap.os -Llib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/mariadbclient/10.1.11-2-gd04d8b7/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_policy/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_logging/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/daf_base/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/utils/2016_01.0+4/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/pex_exceptions/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/base/2016_01.0+3/lib -L/private/tmp/ssd/swinbank/shared_stack/DarwinX86/boost/1.59.lsst5/lib -L/tmp/ssd/swinbank/shared_stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/config -ldaf_persistence -lboost_serialization -lmysqlclient_r -lpex_policy -lpex_logging -lboost_filesystem -lboost_system -ldaf_base -lutils -lpex_exceptions -lbase -lboost_regex -lpthread -ldl -lpython2.7  ld: file not found: libz.1.dylib for architecture x86_64  clang: error: linker command failed with exit code 1 (use -v to see invocation)  scons: *** [python/lsst/daf/persistence/_persistenceLib.so] Error 1  scons: building terminated because of errors.  {code}    This happens with the current master ({{3484020}} at time of writing), but also with a recent weekly ({{3878625}}). ",1
"DM-5607","03/24/2016 11:35:18","check & correct comparison operators in daf_persistence and daf_butlerUtils","per comments in DM-5593, an incorrect comparison operator was found, that used {{is}} instead of {{==}} in a string comparison (e.g. {{var is 'left'}} which is incorrect, it should be {{var == 'left'}}.  This needs to be corrected in {{Repository}} (see DM-5593 for details), and the rest of daf_persistence and daf_butlerUtils should be checked for correct use of is vs. ==.",1
"DM-5633","03/24/2016 16:22:00","Add data products and config in obs_decam for multi-band processing","Add necessary data products and default config in order to run forcedPhotCcd, coaddDriverTask, and multiBandDriverTask with DECam data. ",3
"DM-5635","03/25/2016 07:14:37","not flagged NaN in sdssCentroid","In some rare cases base_SdssCentroid_x(y)Sigma can be NaN while base_SdssCentroid_flag is False",1
"DM-5641","03/26/2016 20:31:53","finish up afw.table to astropy.table view support","At an LSST/AstroPy summit hack session, we've put together a functional system for viewing afw.table objects as astropy.table objects on branch u/jbosch/astropy-tables of afw and https://github.com/astropy/astropy/pull/4740.    Before merging, we should add support for ""object"" columns for subclasses to hold e.g. Footprints in SourceCatalog, and add some documentation.  We may also want to add a convenience method to return an astropy.table.Table directly.",1
"DM-5643","03/26/2016 20:40:39","add method to convert Property[Set,List] to nested dict","In interfacing with AstroPy it'd be useful to easily convert PropertySet and PropertyList to nested dict and OrderedDict (respectively), converting elements with multiple values to lists in the process.",2
"DM-5659","03/30/2016 21:58:47","multiple dialog are not working well together","When several dialogs are up together.  The most recently click one should be one top. When table are in the dialogs such a fits header view. The scroll bars will go over other dialogs. This needs some though and work.  Another thing- when a message dialog is show because of a dialog error. It should center on the dialog.  Update- I don't think I will do the error centering now.  I am going to leave that and see if it is a real problem.",3
"DM-5660","03/31/2016 00:20:57","Add motivated model fits to validate_drp  photometric and astrometric scatter/repeatability analysis and plots","Implement well-motivated theoretical fits to the astrometric and photometric performance measurements based on derivations from LSST Overview paper.  http://arxiv.org/pdf/0805.2366v4.pdf    Photometric errors described by  Eq. 5  sigma_rand^2 = (0.039 - gamma) * x + gamma * x^2  [mag^2]  where x = 10^(0.4*(m-m_5))    Eq. 4  sigma_1^2 = sigma_sys^2 + sigma_rand^2    Astrometric Errors   error = C * theta / SNR    Based on helpful comments from [~zivezic]    {quote}  I think eq. 5 from the overview paper (with gamma = 0.039 and m5 = 24.35; the former I assumed and the latter I got from the value of your  analytic fit that gives err=0.2 mag) would be a much better fit than the adopted function for mag < 21 (and it is derived from first principles).  Actually, if you fit for the systematic term (eq. 4) and gamma and m5, it would be a nice check whether there is any “weird” behavior in  analyzed data (and you get the limiting depth, m5, even if you don’t go all the way to the faint end).     Similarly, for the astrometric random errors, we’d expect        error = C * theta / SNR,    where theta is the seeing (or a fit parameter), SNR is the photometric SNR (i.e. 1/err in mag), and C ~ 1 (empirically, and 0.6 for the idealized maximum likelihood solution and gaussian seeing).   {quote}",5
"DM-5663","03/31/2016 10:49:33","Config override fixes needed due to new star selector","As of DM-5532 a few config files need updating to not refer to star selector config fields as registries (not ones run by our normal CI, which is how I missed this).",2
"DM-5664","03/31/2016 11:40:20","Delete or document and test config/psfex.py","The file {{config/psfex.py}} has no documentation and is bit rotting. If you feel it should be kept then please document it and add a simple unit test that loads it and runs data using it. If it is not needed, then please get rid of it.",1
"DM-5675","03/31/2016 13:33:30","Cannot enable shapeHSM because RegistryField fails validation","When running ci_hsc after setting-up the meas_extensions_shapeHSM, meas_extensions_photometryKron and dependencies using setup -v -r . in the respective cloned folders, I get  {code}  Cannot enable shapeHSM (RegistryField 'calibrate.detectAndMeasure.measurement.plugins' failed validation: Unknown key 'ext_shapeHSM_HsmMoments' in Registry/ConfigChoiceField  For more information read the Field definition at:    File ""/home/vish/code/lsst/lsstsw/stack/Linux64/pex_config/2016_01.0+3/python/lsst/pex/config/registry.py"", line 179, in __init__      ConfigChoiceField.__init__(self, doc, types, default, optional, multi)  And the Config definition at:    File ""/home/vish/code/lsst/lsstsw/stack/Linux64/meas_base/2016_01.0-13-g779ee14/python/lsst/meas/base/sfm.py"", line 109, in <module>      class SingleFrameMeasurementConfig(BaseMeasurementConfig):  ): disabling HSM shape measurements  {code}  Find out why this is happening and find a fix  ",0.5
"DM-5681","03/31/2016 14:08:24","Provide single-visit processing capability as required by HSC","In DM-3368, we provided a means of running multiple processCcd tasks across an exposure, but without performing global calibration etc as provided by HSC's ProcessExposureTask.    Please augment this with whatever additional capability is required to enable HSC data release processing.",2
"DM-5686","03/31/2016 17:41:18","Accommodate pixel padding when unpersisting reference catalog matches","The reference object loader in {{meas_algorithm}}'s *loadReferenceObjects.py* grows the bbox by the config parameter pixelMargin:  doc = ""Padding to add to 4 all edges of the bounding box (pixels)"" . This is set to 50 by default but is not reflected by the radius parameter set in the metadata, so some matches may reside outside the circle searched within this radius. This increase needs to be reflected in the radius set in the metadata fed into {{joinMatchListWithCatalog()}}.  ",2
"DM-5689","03/31/2016 21:31:00","Table needs to fire another action when data completely loaded","When the data for a table is completely loaded fire another action such as TABLE_NEW_LOADED_DONE. This way the xyplots and the image overlays know to go fetch the data.    4/22/2026 from the pull request:  added new action TABLE_NEW_LOADED to table; fired when table is completely loaded.  added table error handling.  fix active table not updating after an active tab is removed.",2
"DM-5694","04/01/2016 12:11:29","Run StarFast simulated images through diffim","Determine the metadata and dependencies needed to fully process two images simulated with StarFast through diffim. ",2
"DM-5695","04/01/2016 12:23:32","Implement simple 1D DCR correction on simulated data","Nate Lust wrote a simple DCR correction recipe that runs in 1D in an ipython notebook. This ticket is to re-write the notebook in python modules that can be run on StarFast simulated images prior to image differencing. For this ticket, the simulated images will be 2D, but DCR will be purely along the x or y pixel grid, allowing columns or rows of pixels to be treated separately in 1D.",8
"DM-5702","04/01/2016 15:40:55","Create a new model in AST/GWCS to represent a complex distortion","Using lessons learned from DM-5701, create a more complex distortion model that cannot be represented from the basic models in GWCS or AST. A good example for this might be a rapidly varying sinusoidal tree-ring-like function that is not well represented by the standard polynomial basis functions. This will test our ability to extend each framework with new models that have not yet been decided on.    Once completed, we could plug this back into the composite model in DM-5701.",8
"DM-5703","04/01/2016 15:54:41","Evaluate performance of AST/GWCS over a range of numbers of pixels","Once we have a composite distortion model from DM-5701, evaluate the performance of AST and GWCS over a range of numbers of pixels, likely from ~100 through full-CCD (4k^2).    As part of this process, we will try to determine whether there is a way to efficiently warp images/postage stamps using python-only models in GWCS and whether bottlenecks could be worked around via optimizations in cython.",8
"DM-5720","04/05/2016 11:47:24","JIRA fixes","This tracks SPs spent on JIRA requests. ",2
"DM-5723","04/05/2016 18:34:56","make sure table can be resized properly","Test table to make sure it can be resized under a variety of layout.",2
"DM-5726","04/05/2016 22:19:00","attend the weekly meeting with UIUC camera team (May 2016)","Tatiana will attend the weekly meeting. Xiuqin and Gregory also attends when needed. ",2
"DM-5729","04/06/2016 10:24:38","Config.loadFromStream suppresses NameError","Within a config override file being executed via {{Config.load}} or {{Config.loadFromStream}}, using a variable that hasn't been defined results in a {{NameError}} exception, but this is silently suppressed and the user has no idea the following overrides have not been executed.",0.5
"DM-5734","04/06/2016 17:55:09","Fix the issues in the server side and the client side introduced by FitsHeaderViewer 's work","*  The testing data ""table_data.tbl"" in the testing tree was accidentally moved.  It should be added back so that IpactTableTest.java can run.    * The request in JsontableUtil was mistakenly moved out from the tableModel by the the line   * {code}  * if (request != null && request.getMeta().keySet().size()>1) {              tableModel.put(""request"", toJsonTableRequest(request));  }  {code}.  The meta can be null but the request is not null, the request should be put into the TableModel.     ",1
"DM-5748","04/07/2016 17:12:10","Upgrade mpi4py to latest upstream","[mpi4py|https://bitbucket.org/mpi4py/] version 2.0 was released in October 2015 with a number of changes. We should upgrade. When upgrading, we should check whether it contains a proper fix for DM-5409 and, if not, file a bug report upstream.    This issue should not be addressed until we have proper test coverage on code which uses mpi4py (DM-3845).",1
"DM-5756","04/11/2016 10:04:49","Update Scons to v2.5.0","Scons 2.5.0 came out over the weekend. There were many fixes to the dependency determination code. The next version of Scons is intended to be 3.0 which will be the first version to support Python 3. Since we fully intend to switch to Python 3.0 in the summer it is prudent for us to ensuer that 2.5.0 works fine before switching to 3.0.0 so that we do not get confused as to why there is breakage in jumping straight to 3.0.0.",2
"DM-5757","04/11/2016 10:34:47","FitsHeader's resize and sorting","DM-4494 has merged to the dev.  However, there are still two issues remained:  * Resize the popup with tabs does not work  * Sorting is depending on the BasicTable's sorting",1
"DM-5760","04/11/2016 11:42:23","XYPlot needs to be expandable","Make XYPlot expandable",2
"DM-5763","04/11/2016 12:27:25","XYPlot: decimation options","User needs to be able to control number of bins and bin size.",3
"DM-5765","04/11/2016 12:47:49","Remove unneeded imports in SConstruct","There's an outstanding pull request from an external contributor (Miguel de Val-Borro) [here|https://github.com/lsst/sconsUtils/pull/9] that makes some minor improvements to sconsUtils by cleaning up the imports. Somebody should review and (if appropriate) merge it. (Or, at least, reply to our community!)",1
"DM-5767","04/11/2016 14:45:35","Create custom basic coaddition code","Create script to do the following:  * Takes a list of DECam exposure numbers  * for each CCD, loads the corresponding calexps  * creates a naive pixel-by-pixel coadd of the underlying images  * Possibly either ANDs or ORs the masks (though perhaps not necessary)  * Either sums the expusure time info from the headers, or averages them, depending on whether the images were normalised to exposure times or not  * write the corresponding images out as coadded fits",1
"DM-5769","04/11/2016 15:00:27","Write spot visualisation snippets","Write some snippets to aide in the processing and visualisation of the CBP data/analysis.    Essentially, write some helper functions that you can throw sections of images at to help look at the shape of the CBP spots, as ds9 isn't great ideal this.    Some nice features would be:    A function that takes a list of images or arrays, and plots them side-by-side, which provides some intelligent options for the stretches, and optionally stretches each image as is best for it, or ties them all to be the same. This would be as 2D colour plots.    A function that takes part of an image and displays it as a colour-graded surface.    A function that takes part of an image and displays it as a 3D bar-chart (as in ROOT, but without using ROOT because there is already enough evil in the world)",2
"DM-5770","04/11/2016 15:05:01","Investigate image processing for feature enhancement","Whilst looking at an individual spot from the CBP on DECam I noticed a weird feature, and upon further investigation, several more, though these were very hard to see.    This ticket is to investigate what image processing techniques will make these hard-to-see features pop out so that they can be examined more closely.",2
"DM-5771","04/11/2016 15:12:09","Update config files","DM-46921 and DM-5348 changed ProcessCcd to the point where past config files are no longer valid as stuff has moved a lot (see https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581)    This ticket is to go through past configs and create a new config file to reproduce the reductions done, or at least make something sensible come out the end of processCcd",2
"DM-5773","04/11/2016 17:38:58","Firefly API plan and decision","We need a plan for  all the Firefly APIs development in the new React/Redux based JS framework, including JS API and Python API.    - Backward compatibility  - Syntax format for JS API  - Syntax format for Python API  - Schedule     - convert the existing API first     - list of new ones to be added, when    ",2
"DM-5775","04/12/2016 10:59:09","Change the TabPanel.jsx and TabPanel.css's properties to allow its children can be resizable","When an outside container is resizable (using css properties: resize: 'both', overflow: 'auto'...), in order for the child inside the container to be resizable, the child has to specify its height and width properties using percentage format (height: 90%, width:100%).   When the TabPanel is used, the table is put on TabPanel.  The table needs to access the size information of the outside container, ie,, the grandparent's width and height. The TabPanel has to pass the height and width to its child component.  Without specifying the height and width in the TabPanel, by default, the auto is used.  When the width (height) is auto, it allows to use the child's width (height).  However, the child replies on the parent to provide such information.  When this circular relations occur, the default size of the child is used.  That is why the table component forever has 75px when it was put in the TabPanel.  To be able to resize with the outside (root) contains all the ancestors of the component have to specify the width and height explicitly. ",0.5
"DM-5782","04/14/2016 07:36:52","Include obs_cfht, obs_decam in lsst-dev shared stack","The shared stack on {{lsst-dev}} provided in DM-5435 does not contain the {{obs_cfht}} or {{obs_decam}} camera packages. Please add them.",1
"DM-5784","04/14/2016 10:02:50","Port region serializer and data structures from GWT","The region serializer in: firefly/src/firefly/java/edu/caltech/ipac/util  * RegionFactory.java    Region container data structures files in : firefly/src/firefly/java/edu/caltech/ipac/util/dd    * ContainsOptions.java  * Global.java  * RegionFileElement.java  * RegParseException.java  * Region.java  * RegionAnnulus.java  * RegionBox.java  * RegionBoxAnnulus.java  * RegionCsys.java  * RegionDimension.java  * RegionEllipse.java  * RegionEllipseAnnulus.java  * RegionFont.java  * RegionLines.java  * RegionOptions.java  * RegionPoint.java  * RegionText.java  * RegionValue.java      Note - do not port CoordException, there are other ways to do this.",8
"DM-5791","04/15/2016 17:17:44","Why is doSelectUnresolved an argument?","The {{run}} method in the {{PhotoCalTask}} has an argument that selects whether to use the extendedness parameter to select objects for photometric calibration.  This is a good idea, but it should be configurable, I think. ",1
"DM-5793","04/16/2016 10:08:34","FITS Visualizer porting: Convert  Mask support","* convert the make support from GWT  * Make a temporary dialog to control it  * Add python/JS API supporty",8
"DM-5794","04/16/2016 10:19:00","Image Visualizer: Support image and drawing layer subgrouping","This will give user finer control of turning on/off the catalog overlays on one image, a group of images, or all the displayed images. ",8
"DM-5797","04/18/2016 14:50:56","Using 'CONSTANT' for background subtraction fails","Running processCcd (on a DECam file) with the following in the config file:    {code}  config.charImage.repair.cosmicray.background.algorithm='AKIMA_SPLINE'  config.charImage.background.algorithm='CONSTANT'  config.charImage.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT'  config.charImage.detectAndMeasure.detection.background.algorithm='CONSTANT'  config.calibrate.detectAndMeasure.detection.tempLocalBackground.algorithm='CONSTANT'  config.calibrate.detectAndMeasure.detection.background.algorithm='CONSTANT'  {code}    fails, and throws the following:    {code}  Traceback (most recent call last):    File ""/home/mfisherlevine/lsst/pipe_tasks/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 199, in run      resultList = mapFunc(self, targetList)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/cmdLineTask.py"", line 324, in __call__      result = task.run(dataRef, **kwargs)    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/processCcd.py"", line 170, in run      doUnpersist = False,    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 298, in run      background = background,    File ""/ssd/lsstsw/stack/Linux64/pipe_base/2016_01.0-6-g7751869+8/python/lsst/pipe/base/timer.py"", line 118, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/mfisherlevine/lsst/pipe_tasks/python/lsst/pipe/tasks/characterizeImage.py"", line 356, in characterize      image -= estBg.getImageF()    File ""/home/mfisherlevine/lsst/afw/python/lsst/afw/math/mathLib.py"", line 5788, in getImageF      return _mathLib.Background_getImageF(self, *args)  lsst.pex.exceptions.wrappers.InvalidParameterError:     File ""src/math/Interpolate.cc"", line 61, in std::pair<std::vector<double>, std::vector<double> > lsst::afw::math::{anonymous}::recenter(const std::vector<double>&, const std::vector<double>&)      You must provide at least 1 point {0}    File ""src/math/BackgroundMI.cc"", line 196, in void lsst::afw::math::BackgroundMI::_setGridColumns(lsst::afw::math::Interpolate::Style, lsst::afw::math::UndersampleStyle, int, const std::vector<int>&) const      setting _gridcolumns {1}  lsst::pex::exceptions::InvalidParameterError: 'You must provide at least 1 point {0}; setting _gridcolumns {1}  {code}",2
"DM-5799","04/18/2016 16:03:06","Asinh stretch algorithm corerction","in DM-2634, the Asinh stretch algorithm  was implemented, but the behavior was not quite right. We need to figure out the issue and make it right. One possibility is that the understanding the relationship  of zero point  and black point, maximum point and white point. ",8
"DM-5803","04/19/2016 12:35:20","fetchUrl is not handling post requests correctly.","Parameters are not sent to the server when requests are posted via fetchUrl.",2
"DM-5810","04/19/2016 18:23:32","Update imageDifferenceTask to cast template ids and use ObjectSizeStarSelector","A couple recent changes to the stack break imageDifferenceTask.     Requires updates to only a few lines.     While I'm updating it to reflect the star selector API, I'm also changing the default star selector from SecondMoment to ObjectSizeStarSelector (which I learned today is what the stack has been using by default for a while). ",1
"DM-5816","04/19/2016 19:13:00","Investigate behavior of Firefly and stretches for images with negative pixel values","Based on a discussion in the Tea Time HipChat room today, this is a ""note to ourselves"" to take a look at the behavior of Firefly when visualizing images with negative flux values.    This is important for difference imaging and is therefore highly relevant to both LSST and ZTF (if ZTF difference images become visible through Firefly at some point).    The behavior of the asinh stretch, in particular, should be looked at.",2
"DM-5819","04/20/2016 07:57:09","Incorporate Price suggestions to make `validate_drp` faster","Increase the loading and processing speed of {{validate_drp}} following suggestions by [~price]    1. Don't read in footprints  Pass {{flags=lsst.afw.table.SOURCE_IO_NO_FOOTPRINTS}} to {{butler.get}}    2. Work on speed of calculation of RMS and other expensive quantities.  Current suggestions:  a. {{calcRmsDistances}}  b. {{multiMatch}}  c. {{matchVisitComputeDistance}}  d. Consider boolean indexing in {{afw}}'s {{multiMatch.py}}  {code}     objById = {record.get(self.objectKey): record for record in self.reference}  to:     objById = dict(zip(self.reference[self.objectKey], self.reference))  {code}    Note that while this ticket will involve work to reduce the memory footprint of the processing, it will not cover work to re-architect things to enable efficient processing beyond the memory on one node.",2
"DM-5821","04/20/2016 18:32:03","Intermittent fault building ci_hsc through Jenkins","Occasionally (see e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-7/10437//console] and [here|https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/9594//console]) the {{ci_hsc}} job in Jenkins fails, reporting:  {code}  RuntimeError: dictionary changed size during iteration  {code}  The fault seems to be intermittent. Please fix it.",3
"DM-5822","04/21/2016 08:46:25","Afw fails unit test for convolve depending on compiler optimisation level","On OSX 10.11.4 with Apple LLVM version 7.3.0 (clang-703.0.29) afw fails {{test/convolve.py}} with the following error when either {{-O0}} or {{-O1}} is enabled but works fine for {{-O2}} and {{-O3}}.    {code:bash}  tests/convolve.py    .....FF/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py:283: RuntimeWarning: invalid value encountered in isnan    nan0 = np.isnan(filledArr0)  /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:113: RuntimeWarning: invalid value encountered in isinf    nx.logical_and(nx.isinf(x), ~nx.signbit(x), y)  /Users/pschella/Development/lsst/lsstsw/miniconda/lib/python2.7/site-packages/numpy/lib/ufunclike.py:176: RuntimeWarning: invalid value encountered in isinf    nx.logical_and(nx.isinf(x), nx.signbit(x), y)  F.F...  ======================================================================  FAIL: testSpatiallyVaryingAnalyticConvolve (__main__.ConvolveTestCase)  Test in-place convolution with a spatially varying AnalyticKernel  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 437, in testSpatiallyVaryingAnalyticConvolve      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel using brute force) wrote to edge pixels: image planes differ: maxDiff=1.09176e+38 at position (73, 18); value=-1.09176e+38 vs. 2825.0; NaNs differ    ======================================================================  FAIL: testSpatiallyVaryingDeltaFunctionLinearCombination (__main__.ConvolveTestCase)  Test convolution with a spatially varying LinearCombinationKernel of delta function basis kernels.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 556, in testSpatiallyVaryingDeltaFunctionLinearCombination      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of delta function kernels using brute force) wrote to edge pixels: image planes differ: maxDiff=9.06659e+36 at position (75, 29); value=9.06659e+36 vs. 2865.0    ======================================================================  FAIL: testSpatiallyVaryingGaussianLinerCombination (__main__.ConvolveTestCase)  Test convolution with a spatially varying LinearCombinationKernel of two Gaussian basis kernels.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 523, in testSpatiallyVaryingGaussianLinerCombination      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially Varying Gaussian Analytic Kernel with 3 basis kernels convolved using brute force) wrote to edge pixels: image planes differ: maxDiff=1.22472e+38 at position (74, 3); value=-1.22472e+38 vs. 2878.0; NaNs differ    ======================================================================  FAIL: testTicket873 (__main__.ConvolveTestCase)  Demonstrate ticket 873: convolution of a MaskedImage with a spatially varying  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/convolve.py"", line 623, in testTicket873      rtol = rtol)    File ""tests/convolve.py"", line 290, in runStdTest      self.runBasicConvolveEdgeTest(kernel, kernelDescr)    File ""tests/convolve.py"", line 317, in runBasicConvolveEdgeTest      doVariance = True, rtol=0, atol=0, msg=msg)    File ""/Users/pschella/Development/lsst/code/afw/python/lsst/afw/image/testUtils.py"", line 201, in assertMaskedImagesNearlyEqual      testCase.fail(""%s: %s"" % (msg, ""; "".join(errStrList)))  AssertionError: basicConvolve(MaskedImage, kernel=Spatially varying LinearCombinationKernel of basis kernels with low covariance, using brute force) wrote to edge pixels: image planes differ: maxDiff=3.19374e+38 at position (1, 46); value=3.19374e+38 vs. 2774.0    ----------------------------------------------------------------------  Ran 13 tests in 43.252s    FAILED (failures=4)  The following tests failed:  /Users/pschella/Development/lsst/code/afw/tests/.tests/convolve.py.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  {code}",2
"DM-5823","04/21/2016 09:28:12","ECL_B1950 coordinate was not defined correctly","The CoordSys.js defined ECL_B1950 incorrectly.  When I was testing WebGrid, the grid lines for  Ecliptic B1950 were not right.  Looked further, it was caused by wrong equinox value in its definition.",0.5
"DM-5829","04/22/2016 11:11:53","Create outline of Level 3 ConOps","Create an outline of the sections of the Level 3 ConOps document",2
"DM-5830","04/22/2016 11:13:57","Level 3 requirements flowdown","Document the flowdown of Level 3-related requirements from SRD, LSR, OSS, and DMSR.",3
"DM-5832","04/22/2016 11:25:22","LSE-140 post-CCB implementation","Following CCB approval of LSE-140, perform minor document work required for full implementation (application of standard cover page, change log, etc.).",2
"DM-5835","04/22/2016 12:18:01","Prepare a draft of the SUIT deployment timeline","Prepare a draft schedule, with some detail for 2016-2017, for deployments of the SUIT into (test) production, including the datasets that will be served.",2
"DM-5837","04/22/2016 13:14:07","Document pipe_drivers","Please provide a minimal level of documentation for {{pipe_drivers}}, to include:    * A {{doc}} directory with the usual content so that docstrings get generated by Doxygen;  * A package overview;  * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary).",2
"DM-5838","04/22/2016 17:04:44","3-color image label change","When generating 3-color image, the label for the image display should 'Project 3-color', i.e.   WISE 3-color, 2MASS 3-color ...  ",0
"DM-5839","04/23/2016 13:35:17","horizon console interface broken","It appears that at some point in the last few months the horizon console interface has stopped working.  I am still able to access the console log output via the API/CLI.",1
"DM-5840","04/23/2016 13:53:05","instance limit low vs available cores","The LSST project is currently at 81/100 instances but there are over 200 cores unused.  Is it possible to increase the instance limit or are we being encouraged to use large instance flavors?",1
"DM-5841","04/23/2016 14:19:56","unable to list nebula lsst project users","Currently, [with some difficulty] it is possible to discover the {{user_id}} that created an instance (might be possible for other resources as well) but it is not possible to map this back to a username / person.  This can make it difficult to 'self police' instances.    The administrative API endpoints are not publicly accessible and I doubt any end user has the appropriate permission. ",1
"DM-5847","04/25/2016 15:00:23","libxml build issue with mpich on OS X","On OS X with Xcode installed {{mpich}} fails to build because it can not locate the libxml include files:    {code}  CC       topology-xml-libxml.lo   topology-xml-libxml.c:17:10: fatal error: 'libxml/parser.h' file not found   #include <libxml/parser.h>            ^   1 error generated.  {code}  with {{pkg-config}} 0.29.1 installed. The problem is that {{configure}} determines that {{libxml-2.0}} is available and is installed into {{/usr}} with a CFLAGS of {{-I/usr/include/libxml2}}. {{configure}} does not itself test whether those parameters are reasonable. With Xcode there are no files installed into {{/usr/include}} and {{clang}} knows to look in specific SDK locations. When {{mpich}} builds it assumes that {{libxml2}} can be found but fails to find it.    Strangely, {{pkg-config}} v0.28 does not seem to be able to find {{libxml-2.0}} so there is no issue.    One solution is to install the Command Line Tools but it might be more portable to attempt to disable {{libxml2}}.  ",2
"DM-5848","04/25/2016 16:34:17","Investigate Jupyter internals, interactive widgets","In preparation for linking Jupyter notebooks with Firefly and other SUIT components, read Jupyter documentation. Learn how to build a sample widget or interactive dashboard in the Jupyter framework",2
"DM-5849","04/25/2016 16:38:09","Investigate Ginga and Glueviz visualization tools","Ginga and Glue (glueviz) are community visualization tools in Python. Become familiar with the capabilities of both, thinking from the point of view of using Firefly for the display but using Python for many other things.",2
"DM-5854","04/25/2016 17:36:26","Java array index out of bound error in VisSeverCommand.java","The class FileFluxCmdJson in VisServerCommand.java is calling   {code}              String[] res = VisServerOps.getFileFlux(fahAry, pt);  {code}    However, when the mouse is outside the image, the VisServerOps.getFileFlux(fahAry, pt) returns:  {code}  new String[]{PlotState.NO_CONTEXT}  {code}  It is fine for a single band.  However, for 2 or 3 bands, the for loop below caused the index out of bound error because res is an array of length=1 and the expected res is an array of length=no of bands.  {code}    JSONObject obj= new JSONObject();              obj.put(""JSON"", true);              obj.put(""success"", true);                int cnt=0;              JSONObject data= new JSONObject();              for(Band b : state.getBands()) {                  data.put(b.toString(), res[cnt++]);              }              data.put(""success"", true);  {code}    Thus,  res\[cnt++\] caused array index out of bound error.     To fix this issue, the for loop is changed as below:  {code}                        int cnt=0;              JSONObject data= new JSONObject();              Band[] bands = state.getBands();              for (int i=0; i<res.length; i++){                  data.put(bands[i].toString(), res[i]);              }              data.put(""success"", true);                JSONArray wrapperAry= new JSONArray();              obj.put(""data"", data);              wrapperAry.add(obj);  {code}    When the mouse is outside the image, the res returns a new String\[\]\{PlotState.NO_CONTEXT\}, it is added to the JSONObject only once.  ",1
"DM-5859","04/26/2016 11:03:44","Table: Add keyboard navigation","- Added arrow up/down to move between rows.  - Added page up/down to move between pages.    - Fixed table loading mask not showing  - Fixed PagingBar rendering more than it should  - Fixed annoying StandardView missing unique key warning",2
"DM-5870","04/26/2016 17:46:13","Update testdata_subaru to support calib changes","Merging DM-5124 broke obs_subaru because the test data in testdata_subaru wasn't updated.  Fix it.",0.5
"DM-5883","04/27/2016 19:36:41","Include more information in DECam registry","obs_decam originally included 'object' and 'proposal' fields in the registry but these were removed at some point.  Putting them back would allow users to more easily select data.    I also suggest surveying the headers for additional useful fields.",1
"DM-5885","04/28/2016 02:52:26","Create a JSON file for monitoring stack","Create a JSON or YAML file with:    - Qserv version  - libraries/deps version  - other idea welcome    This will interesting ""GROUP BY"" in monitoring tool (performance for each Qserv or xrootd version for example)",3
"DM-5887","04/28/2016 10:11:48","lsstswBuild.sh --print-fail flag broken","The {{--print-fail}} flag appears to broken for at least some failure modes.    https://github.com/lsst-sqre/buildbot-scripts/blob/master/lsstswBuild.sh#L145    Eg.    https://ci.lsst.codes/job/stack-os-matrix/label=centos-6/10673//console",0.5
"DM-5889","04/28/2016 12:22:05","Suppress gcc warnings about ""unused local typedefs""","We should add {{\-Wno\-unused\-local\-typedefs}} to our gcc options.  This cleans up the build significantly, because there's a flood of warnings of this type coming from boost.  If we suppress those, it might become possible to notice warnings that we care about.",0.5
"DM-5893","04/28/2016 14:46:49","LSST the Docs Fastly should redirect /en/latest/ to /","Previously we deployed documentation on Read the Docs. By default, Read the Docs would show the master version of documentation on ""/en/latest/"". Many links with that endpoint may already exist. We should configure Fastly to redirect such paths to ""/"".",0.5
"DM-5898","04/28/2016 20:49:16","Python EUPS package can use $PYTHON","The {{python}} eups package has a script that checks that the python being used is version 2.7. This script can optionally check {{$PYTHON}} rather than the python in the path but I am confused as to what that test is going to do for us. The problem is that {{sconsUtils}} uses {{python}} and most of the shebangs use {{/bin/env python}} (although shebang rewriting on all platforms could help with that). I think the check script should have the {{$PYTHON}} support removed due to excessive confusion.    It would also help if the check script worked with python 3 so that the wrong python could be caught.",1
"DM-5899","04/29/2016 06:34:16","Support LSST-produced calibs","obs_decam currently uses a {{%(path)s\[%(ccdnum)d\]}} template for its calibs (bias, dark, flat, fringe).  This is desirable in order to support calibs provided from external sources (e.g., NOAO).  However, our calib construction code does not support such a path ({{path}} cannot be inferred for new data, and we can't write MEFs from multiple processes).  We believe the best path forward is to split the {{DecamMapper}} into two mappers, one supporting calibs from external sources and one supporting calibs we construct ourselves.  The alternative is to copy the external calibs to match a template like {{BIAS/%(date)s/BIAS-%(date)s-%(ccdnum)02d.fits}}, which means breaking apart the MEFs.    I suggest also moving the defects into the obs_decam package because they're required for use, small and we don't have code to regenerate them.",0
"DM-5900","04/29/2016 10:23:55","Create psutil EUPS package","Add the python {{psutil}} package to the stack as {{python_psutil}}.",0.5
"DM-5904","04/29/2016 12:43:01","Create focus script","In DM-3368, we stripped out the focus calculation since it's not camera-generic, and the scatter/gather isn't necessary for general processing.  We need to reinstate the focus calculation in its own scatter/gather script.",2
"DM-5911","04/29/2016 15:24:35","Fix circular references in Mapper objects","Whilst running tests with pytest and the new file descriptor leak checker it became clear that Mapper objects were not freeing their resources when they were deleted. In particular, the registry objects remained and the associated sqlite database files were opened. This led to pytest running out of file descriptors when large test suites were being executed.    The problem turns out to be the dynamically created map functions. These are created as functions (not bound methods) attached to an instance. Since they are not bound methods the instance object (self) has to be passed in to closure. This leads to self containing a reference to a function that contains a reference to self and this prevents the Mapper from ever being garbage collected (leading to all the resources being retained).    A short term fix is pass the mappers into the closures using {{weakref}}.    Eventually it would be nice to consistently make the {{map_}} items bound methods rather than attaching them as functions but that is beyond the scope of this ticket.",1
"DM-5915","04/29/2016 17:47:34","Decide how to rework afw:Wcs guts with AST","Following the to-be-written recommendation for DM-4157, we plan to rework the guts of afw:Wcs to use AST. We need to decide how afw:Wcs will use AST, whether as a wrapper or as a complete replacement with AST.    The product is a design",8
"DM-5921","05/02/2016 09:29:00","Clarify how to work with ci_hsc's astrometry_net_data","ci_hsc's {{README.rst}} contains [a note|https://github.com/lsst/ci_hsc/blob/87b6ecb1cc0157cac8dafb356520f49f971bb1ec/README.rst#reference-catalog] on declaring & setting up the included reference catalogue data.    I believe this was rendered obsolete by DM-5135, which automatically sets up the reference catalogue when ci_hsc itself is set up. Attempting to follow the documentation therefore produces confusing warning messages, and may break things.    Please check if my understanding is correct and, if so, fix the documentation.",0.5
"DM-5922","05/02/2016 10:33:22","Rework camera geometry to use the replacement for XYTransform","As part of overhauling XYTransform we will likely need to replace the way we describe the transformations supported by camera geometry and {{Detector}}. This is likely to include a new way of describing the coordinate frames (e.g. {{PIXEL}}. {{FOCAL_PLANE}} and {{PUPIL}}).    If we adopt AST (as seems likely) then these frames will be AST {{Frames}}, the transforms will be AST {{Mappings}} and the collection described by {{Camera}} and {{Detector}} will be one or more AST {{FrameSets}}.    An RFC for the redesigned API for camera geometry will be required and this ticket is to implement the resulting design.",8
"DM-5926","05/02/2016 14:21:05","networking in strange state for newly created instances","When starting a new instance, occasionally something strange seems to happen with the  network setup.  The instance will come up but is inaccessible (icmp, ssh). When this happens, the console log shows that a DHCP address was obtained and cloud-init injected ssh-keys, so it isn't a total network setup failure.    I have seen this happen a few times in the last couple of weeks but I can't reliably reproduce it.  I'm wondering if neutron is logging anything interesting when this happens.    This failure mode happened  again a few minutes ago with 7adffa82-7221-454c-acfe-5f21cdd34ea8.  Which I killed and recreated as instance b6f64981-099b-46e5-a27e-e3694372f447 with the same private IP address.   The new instance is accessible as expected.",1
"DM-5927","05/02/2016 14:39:02","API errors when trying to start up multiple instances","I am attempting to start up 20 {{m1.medium}} instances without floating IPs to take available of the new instance cap from DM-5840.  This consistently fails after starting a few instances with an HTTP 403.    {code:java}  Error creating OpenStack server: Expected HTTP response code [201 202] when accessing [POST http://nebula.ncsa.illinois.edu:8774/v2/8c1ba1e0b84d486fbe7a665c30030113/servers], but got 403 instead  {""forbidden"": {""message"": ""Maximum number of ports exceeded"", ""code"": 403}}  {code}    Of the instances that do manage to start, most end up in an error state with.      {code:java}  (openstack) server show 134b69dc-56fc-4249-b92f-e958e561ae3b  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  | Field                                | Value                                                                                                                                               |  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  | OS-DCF:diskConfig                    | MANUAL                                                                                                                                              |  | OS-EXT-AZ:availability_zone          | nova                                                                                                                                                |  | OS-EXT-STS:power_state               | 0                                                                                                                                                   |  | OS-EXT-STS:task_state                | None                                                                                                                                                |  | OS-EXT-STS:vm_state                  | error                                                                                                                                               |  | OS-SRV-USG:launched_at               | None                                                                                                                                                |  | OS-SRV-USG:terminated_at             | None                                                                                                                                                |  | accessIPv4                           |                                                                                                                                                     |  | accessIPv6                           |                                                                                                                                                     |  | addresses                            |                                                                                                                                                     |  | config_drive                         |                                                                                                                                                     |  | created                              | 2016-05-02T20:29:54Z                                                                                                                                |  | fault                                | {'code': 500, 'message': 'No valid host was found. Exceeded max scheduling attempts 3 for instance 134b69dc-56fc-4249-b92f-e958e561ae3b. Last       |  |                                      | exception: [u\'Traceback (most recent call last):\\n\', u\'  File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 2235, in _do',   |  |                                      | 'created': '2016-05-02T20:29:57Z'}                                                                                                                  |  | flavor                               | m1.medium (3)                                                                                                                                       |  | hostId                               | b383eddb06f7a1cc5929e5fa8b6982cc523f5ac1cbe3c9c40120a700                                                                                            |  | id                                   | 134b69dc-56fc-4249-b92f-e958e561ae3b                                                                                                                |  | image                                | centos-7-slurm-20160422210744 (7364ada7-263e-4fb0-a9f4-219ab19e0be0)                                                                                |  | key_name                             | jhoblitt-slurm                                                                                                                                      |  | name                                 | slurm-slave4                                                                                                                                        |  | os-extended-volumes:volumes_attached | []                                                                                                                                                  |  | project_id                           | 8c1ba1e0b84d486fbe7a665c30030113                                                                                                                    |  | properties                           | slurm_node_type='slave'                                                                                                                             |  | status                               | ERROR                                                                                                                                               |  | updated                              | 2016-05-02T20:29:57Z                                                                                                                                |  | user_id                              | 83bf259d1f0c4f458e03f9002f9b4008                                                                                                                    |  +--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+  {code}      ",2
"DM-5933","05/02/2016 17:09:40","Replace jointcal.StarSelector with meas_algorithms.starSelector","jointcal has its own custom star selector. This should be removed and replaced with a star selector based on meas_algorithms.starSelector. A good choice might be meas_algorithms.objectSizeStarSelector.",2
"DM-5934","05/02/2016 18:47:22","Update developer guide with Astropy guidance","Once RFC-178 is adopted the developer guide has to be updated to include guidance as to how Astropy can be used in the stack (similar to how Boost is documented).",1
"DM-5935","05/02/2016 18:49:31","Package Astropy for the stack","Once RFC-178 is adopted Astropy needs to be packaged in an EUPS container. Given the complexity of Astropy dependencies the packaging will be done as for {{numpy}} and {{scipy}} by checking that Astropy is available (v1.1 will be the minimum version).",1
"DM-5936","05/03/2016 09:17:02","Make afw rgb unit test PEP440 compliant for matplotlib check","If a user has a version of matplotlib installed from a git clone, the afw rgb unit test fails at the matplotlib version check. The versioning scheme for this type of install is determined by pep 440. Make the unit test properly handle this type of version comparison.",0.5
"DM-5939","05/03/2016 11:17:34","Pre-release versions of matplotlib 2.0 break afw unit tests","In the afw rgb unit test, testWriteStarsLegacyAPI checks to make sure that a file name with an unknown extension raises a value error. In current version of matplotlib, saving a file with an unknown extension causes this error:  {code}  *** ValueError: Format ""unknown"" is not supported.  Supported formats: eps, jpeg, jpg, pdf, pgf, png, ps, raw, rgba, svg, svgz, tif, tiff.  {code}    In matplotlib 2.0 prerelease the file is saved as a png when an unknown extension is specified. Since the write call success the unit test fails as it is expecting a failure.     If nothing depends on this behavior, the unit test should probably be removed.",0.5
"DM-5940","05/03/2016 14:21:22","Create new build based on the converted firefly code.","- remove all of the gwt code except for a few remaining files.  - create separate build for the new firefly viewer, leaving the old fftools as it was before the JS conversion.  - repackage files as needed moving forward.",8
"DM-5941","05/03/2016 14:33:25","Private network not available across all instances","I'm setting up an ELK system. Part of that is an Elasticsearch system. When I bring up the system the private network is bisected. I attempted creating a security group, in case that was a problem but it didn't help. Note that the work around is to create security groups or use a firewall and use floating ips. This is far from ideal. I think the right solution is to use the private network.    Example:    First section {{p-es-1}} {{p-es-3}} {{p-es-k}}    {code:bash}  vagrant@es-1:~$ ifconfig  ens3      Link encap:Ethernet  HWaddr fa:16:3e:47:28:a7            inet addr:10.0.42.30  Bcast:10.0.42.255  Mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe47:28a7/64 Scope:Link            UP BROADCAST RUNNING MULTICAST  MTU:1454  Metric:1            RX packets:363265 errors:0 dropped:0 overruns:0 frame:0            TX packets:304215 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            RX bytes:95396177 (95.3 MB)  TX bytes:238466304 (238.4 MB)    lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0            inet6 addr: ::1/128 Scope:Host            UP LOOPBACK RUNNING  MTU:65536  Metric:1            RX packets:850 errors:0 dropped:0 overruns:0 frame:0            TX packets:850 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            RX bytes:138411 (138.4 KB)  TX bytes:138411 (138.4 KB)    vagrant@es-1:~$ ping 10.0.42.32  PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  64 bytes from 10.0.42.32: icmp_seq=1 ttl=64 time=0.284 ms  64 bytes from 10.0.42.32: icmp_seq=2 ttl=64 time=0.266 ms  64 bytes from 10.0.42.32: icmp_seq=3 ttl=64 time=0.265 ms  64 bytes from 10.0.42.32: icmp_seq=4 ttl=64 time=0.302 ms  ^C  --- 10.0.42.32 ping statistics ---  4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.265/0.279/0.302/0.019 ms  vagrant@es-1:~$ ping 10.0.42.34  PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  64 bytes from 10.0.42.34: icmp_seq=1 ttl=64 time=0.333 ms  64 bytes from 10.0.42.34: icmp_seq=2 ttl=64 time=0.325 ms  64 bytes from 10.0.42.34: icmp_seq=3 ttl=64 time=0.322 ms  64 bytes from 10.0.42.34: icmp_seq=4 ttl=64 time=0.319 ms  ^C  --- 10.0.42.34 ping statistics ---  4 packets transmitted, 4 received, 0% packet loss, time 2998ms  rtt min/avg/max/mdev = 0.319/0.324/0.333/0.022 ms  vagrant@es-1:~$ ping 10.0.42.31  PING 10.0.42.31 (10.0.42.31) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.31 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3017ms  pipe 3  vagrant@es-1:~$ ping 10.0.42.33  PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.33 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3008ms  pipe 3  vagrant@es-1:~$ ping 10.0.42.35  PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  From 10.0.42.30 icmp_seq=1 Destination Host Unreachable  From 10.0.42.30 icmp_seq=2 Destination Host Unreachable  From 10.0.42.30 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.35 ping statistics ---  5 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3999ms  pipe 3  {code}    Second section {{p-es-2}} {{p-es-4}} {{p-lfr}}    {code:bash}  vagrant@es-2:~$ ifconfig  ens3      Link encap:Ethernet  HWaddr fa:16:3e:6f:30:2c            inet addr:10.0.42.31  Bcast:10.0.42.255  Mask:255.255.255.0            inet6 addr: fe80::f816:3eff:fe6f:302c/64 Scope:Link            UP BROADCAST RUNNING MULTICAST  MTU:1454  Metric:1            RX packets:196344 errors:0 dropped:0 overruns:0 frame:0            TX packets:160561 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1000            RX bytes:47667399 (47.6 MB)  TX bytes:7135345 (7.1 MB)    lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0            inet6 addr: ::1/128 Scope:Host            UP LOOPBACK RUNNING  MTU:65536  Metric:1            RX packets:97268 errors:0 dropped:0 overruns:0 frame:0            TX packets:97268 errors:0 dropped:0 overruns:0 carrier:0            collisions:0 txqueuelen:1            RX bytes:8558096 (8.5 MB)  TX bytes:8558096 (8.5 MB)    vagrant@es-2:~$ ping 10.0.42.33  PING 10.0.42.33 (10.0.42.33) 56(84) bytes of data.  64 bytes from 10.0.42.33: icmp_seq=1 ttl=64 time=0.311 ms  64 bytes from 10.0.42.33: icmp_seq=2 ttl=64 time=0.309 ms  64 bytes from 10.0.42.33: icmp_seq=3 ttl=64 time=0.300 ms  ^C  --- 10.0.42.33 ping statistics ---  3 packets transmitted, 3 received, 0% packet loss, time 2000ms  rtt min/avg/max/mdev = 0.300/0.306/0.311/0.020 ms  vagrant@es-2:~$ ping 10.0.42.30  PING 10.0.42.30 (10.0.42.30) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  From 10.0.42.31 icmp_seq=4 Destination Host Unreachable  ^C  --- 10.0.42.30 ping statistics ---  5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4014ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.32  PING 10.0.42.32 (10.0.42.32) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  From 10.0.42.31 icmp_seq=4 Destination Host Unreachable  From 10.0.42.31 icmp_seq=5 Destination Host Unreachable  ^C  --- 10.0.42.32 ping statistics ---  5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 4023ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.34  PING 10.0.42.34 (10.0.42.34) 56(84) bytes of data.  From 10.0.42.31 icmp_seq=1 Destination Host Unreachable  From 10.0.42.31 icmp_seq=2 Destination Host Unreachable  From 10.0.42.31 icmp_seq=3 Destination Host Unreachable  ^C  --- 10.0.42.34 ping statistics ---  4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3006ms  pipe 3  vagrant@es-2:~$ ping 10.0.42.35  PING 10.0.42.35 (10.0.42.35) 56(84) bytes of data.  64 bytes from 10.0.42.35: icmp_seq=1 ttl=64 time=0.387 ms  64 bytes from 10.0.42.35: icmp_seq=2 ttl=64 time=0.278 ms  64 bytes from 10.0.42.35: icmp_seq=3 ttl=64 time=0.288 ms  ^C  --- 10.0.42.35 ping statistics ---  3 packets transmitted, 3 received, 0% packet loss, time 1998ms  rtt min/avg/max/mdev = 0.278/0.317/0.387/0.053 ms  {code}    This can be reproduced by sourcing your OpenStack credentials and running this [{{Vagrantfile}}|https://gist.github.com/jmatt/7b6eb6a042c4e63531d40d1a68069f33]. Use {{vagrant ssh p-es-1}} to connect to the {{p-es-1}} instance.  ",2
"DM-5973","05/04/2016 15:36:15","Update developer guide with pytest guidance","Now that DM-5561 explains how to migrate to pytest compatibility the developer guide must be updated to state how to use pytest in unittests.",5
"DM-5979","05/05/2016 11:59:15"," tests in testArgumentParser.py fail Jenkins run-rebuild on nfs","(1) {{testOutputs}} fails because paths are compared literally  Jenkins run-rebuild #139 failed with pipe_base  https://ci.lsst.codes/job/run-rebuild/139//console  {code:java}  FAIL: testOutputs (__main__.ArgumentParserTestCase)  Test output directories, specified in different ways  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testArgumentParser.py"", line 497, in testOutputs      self.assertEqual(args.input, DataPath)  AssertionError: '/nfs/home/lsstsw/stack/Linux64/obs_test/2016_01.0-3-gafa6dd0+10/data/input' != '/home/lsstsw/stack/Linux64/obs_test/2016_01.0-3-gafa6dd0+10/data/input'  {code}    Please make the comparison more robust.     (2) File descriptor leaks  Jenkins run-rebuild #138 failed with pipe_base  https://ci.lsst.codes/job/run-rebuild/138//console  {code:java}  FAIL: testFileDescriptorLeaks (lsst.utils.tests.MemoryTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/home/lsstsw/stack/Linux64/utils/2016_01.0-2-g97a6e33/python/lsst/utils/tests.py"", line 133, in testFileDescriptorLeaks      self.fail(""Failed to close %d files"" % len(diff))  AssertionError: Failed to close 1 files  {code}    {code:java}  File open: /nfs/home/lsstsw/build/pipe_base/.nfs000000000a20a3f700005679  {code}  This test passes on local disk.  ",2
"DM-5986","05/06/2016 08:30:23","Use sagas in place of side-effects in chart-related controllers ","Replace side-effects with saga and clean-up chart related controllers (TableStats, XYPlot and Histogram).",3
"DM-6022","05/09/2016 09:12:08","Lazy load related chart data on table data update","When new table data received, the related chart data should be updated only for the components on display. Hidden components' data should be lazily updated when a component becomes visible.",3
"DM-6026","05/09/2016 09:52:52","Make it possible to distinguish TABLE_NEW_LOADED actions triggered by sort","It would be beneficial to have in the TABLE_NEW_LOADED payload a  trigger field, which would differentiate actions triggered by sort (where  data do not change, only their order) or filter from other loads. We don't  need to reload table statistics or histogram on sort. But we do need to to  reload them on filter.      created TABLE_SORT action to distinguish sorting from filtering.  sorting should not reload xyplot nor catalog overlay.    Also:  - disable history when in api mode.  - ensure tableMeta.source reflects the file on the server.  - fix TablePanelOptions not resetting columns selection.  - remove 'Fits Data' tab when no images available.  - fix 'Coverage' appearing when it should.",2
"DM-6028","05/09/2016 10:05:53","Validation is not performed on unchanged fields","Currently, validation is performed only if a field has changed. We need to be able to validate all fields on form submit.    The issue is not limited to initial (ex. empty) value being invalid. The invalid message is lost when a field is unmounted/re-mounted.    You can test the following way:  - http://localhost:8080/firefly/;a=layout.showDropDown?view=AnyDataSetSearch  - Open chart settings, enter 1000 into X/Y ratio - the field is shown as invalid  - Switch to histogram and back, the invalid message is gone, the field appears to be valid    Another test case is Example Dialog tab 'X 3', 'X 3'  tab test field initial value 88 is invalid (it should be between 22 and 23), but it appears valid. ",2
"DM-6029","05/09/2016 10:08:05","Error message is not shown","The error message is not showing consistently when mouse is over tha exclamation icon.",1
"DM-6030","05/09/2016 10:32:39","Investigate possibilty of cosmic ray muons (etc) for precision gain calibration","In the era of CBPs, we care about absolute system throughput, and thus need to accurately know the gain of amplifiers in the CCDs.    Initially, this can be done by lab-based Fe55 characterisation (modulo the non-linearity, though that itself will need to be need to be characterised and corrected for), but changes in the relative gains of the various amplifiers need to be monitored, and this must be done in a way that is not degenerate with the optical transmission in any way.    Theoretically it should be possible to use cosmic ray muon tracks, and tracks from radioisotope contamination of the glass/dewar, to measure the (change in the) relative gains of the amplifiers.    Early work has shown that this does work in principle, but this ticket is for some further effort to see whether this method can provide the necessary accuracy given the amount of data available remains to be seen.    This ticket would normally need to be significantly more points, but as it builds on earlier work, it can, at least for initial results, be done quite cheaply.    Initial investigation will inform further work, which will be carried out in S17 as DM-8276.",8
"DM-6036","05/09/2016 11:42:32","Produce and ingest master calibs for USNO monocam data.","Use the construct*.py scripts added to pipe_drivers to produce temporally relevant master biases, darks, flats (and fringe frames?) for the recent USNO observing with monocam.    A small amount of hacking will be required due to the fact that the current ingestion model assumes that each CCD frame has a USNO counterpart which tells about the telescope pointing etc, but the bias frames do not have these.    Once the master calibs are produced, get them ingested.",2
"DM-6037","05/09/2016 11:50:43","Reduce sky data from USNO monocam run","Using the master calibs produced in DM-6036, push all the monocam data through processCcd.    Others will run sanity checks on the output (initial astrometry & photometry). From there I believe people will look at using the data to test jointcal & sim_astrom etc, but this ticket just related to the initial reduction.    As more data comes in from the 2nd telescope, a little further hacking may be necessary to keep everything running. Some of this will likely be hacky or need one-off solutions/header modification, hence the higher-than-normal number of story points assigned to what one might expect to be an hour-long job.",3
"DM-6041","05/09/2016 12:16:01","Functional use cases for light curve ","Work with Vandana Desai (IRSA) to tabulate science use cases for tools. Then transform the science use cases to functional use cases (""this is how we want the tool/interface to behave"").    This work is  to identify common functional and scientific use cases between PTF/ZTF and LSST to inform LSST on how the SUIT web portal might be organized for user interaction with LSST data. ",1
"DM-6044","05/09/2016 13:01:29","Add LoadReferenceObjectsTask to task documentation page","The documentation for {{LoadReferenceObjectsTask}} should show up on the task documentation page, despite its being an abstract base class.",0
"DM-6045","05/09/2016 14:46:48","Nominal PSF required by processCcd","After the processCcd refactor, non-sky images, or, in general, images for which an initial PSF estimate cannot be made, cause {{ProcessCcdTask}} to fail if {{config.doCalibrate=True}}.    To work around this, I put a dirty hack at line 157 of my {{processCcd.py}}:  {code}  from lsst.meas.algorithms.installGaussianPsf import InstallGaussianPsfTask, FwhmPerSigma  task = InstallGaussianPsfTask()  task.config.fwhm = 1.0 * 3.53223006755  task.run(exposure=exposure)  measFwhm = exposure.getPsf().computeShape().getDeterminantRadius() * FwhmPerSigma  {code}  to ensure that there would always be kind of PSF. This should be redone in a way that someone who knows what they're doing approves of.",2
"DM-6048","05/09/2016 16:07:02","Bundle up more HSC data for validate_drp","We would like to include a larger set of HSC data for validation.  I tested this while in Tucson.  My working dir was {{/tigress/pprice/frossie}}.  The raw and processed data should be stuffed into validation_data_hsc",3
"DM-6050","05/09/2016 16:11:21","Table caching optimizations","We need to avoid duplicate requests which result from minor differences in TableRequest parameters, which are not used to get data.  For example, loading catalog table, which triggers table statistics, and then getting an XY plot, results in 3 requests, returning identical data.    1. RequestClass=ServerRequest; *tbl_id=tbl_id-1;* UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; SearchMethod=Cone; catalog=wise_allwise_p3as_psd; RequestedDataSet=wise_allwise_p3as_psd; radius=200; use=catalog_overlay; catalogProject=WISE    2. RequestClass=ServerRequest;RequestedDataSet=wise_allwise_p3as_psd; catalog=wise_allwise_p3as_psd; use=catalog_overlay; UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; catalogProject=WISE; radius=200; SearchMethod=Cone    3. RequestClass=ServerRequest; *tbl_id=xyplot-tbl_id-1;* catalog=wise_allwise_p3as_psd; use=catalog_overlay; UserTargetWorldPt=10.68479;41.26906;EQ_J2000;m31;ned; SearchMethod=Cone; RequestedDataSet=wise_allwise_p3as_psd; catalogProject=WISE; radius=200; *decimate=decimate=ra,dec,10000,1,,,,*    The difference between 1 and 2 is tbl_id parameter. The difference between 2 and 3 is tbl_id and decimate parameters. As well as the order of the parameters. None of which change the catalog search result.    Test Case: Test Searches, Test catalog, AllWISE Source, radius=200",2
"DM-6051","05/09/2016 16:12:24","Add extendedness vs. star selector test to single-visit validation in ci_hsc","ci_hsc has a test that verifies that extendedness as measured on coadds broadly agrees with the star selection done for PSF estimation on individual frames.  This tests a bunch of stuff, including aperture corrections on the coadds and propagation of flags from visits to coadds.    It doesn't test that aperture correction vs. extendedness logic is correct in processCcd.py, but just copying this test to the appropriate validation function in ci_hsc should do the trick.  This is currently broken, but should be fixed in DM-5877.",2
"DM-6054","05/10/2016 10:35:46","Minor updates in suptertask from following DMTN-002","Some examples in the DMTN-002 seem slightly out of date.    Update supertask documentation and code to catch up with some recent developments in the stack. ",2
"DM-6063","05/11/2016 15:03:25","Fix how aperture correction is applied","[~lauren] committed a fix Jan 15 to how aperture correction is applied that I accidentally lost when refactoring in DM-4692. https://github.com/lsst/pipe_tasks/commit/d904e3d188698b4f57bf3dad1516b0bf201078f5 Restore the fix.    The need for this fix suggests a design flaw in measurement that will be fixed as part of DM-5877",1
"DM-6067","05/11/2016 18:32:58","S17 Butler DB storage","Initial implementation to be read-only, DB -> in-memory AFWTable.  DB connection info to reside in repository config.  Query substitution elements to be taken from data id.",40
"DM-6074","05/12/2016 11:00:32","Add RegistryField support to Task.makeSubtask","As part of implementing RFC-183 add support for tasks specified in {{lsst.pex.config.RegistryField}} to {{lsst.pipe.base.Task.makeSubtask}}  ",2
"DM-6075","05/12/2016 11:06:51","Document the need for abstract base tasks for tasks","As part of RFC-183 document the fact that variant tasks should have a common abstract base class that defines the API. If we add future tasks that we feel are likely to have variants, then we should create an abstract base class.    Candidates include star selectors, PSF determiners and ISR tasks.    Note that this applies to tasks LSST provides in its stack, not to variants users produce and other obscure one-off code.    Also document the desire that tasks with anticipated many variants, such as star selectors, and PSF determiners should be in registries. This explicitly excludes tasks such as ISR where only one task is likely to be useful for a given set of data.  ",2
"DM-6077","05/12/2016 11:15:20","Change PSF determiners into tasks","PSF determiners are already configurables, and some benefit from having a log. Take the logical next step and make them instances of {{lsst.pipe.base.Task}}.",1
"DM-6078","05/12/2016 13:15:51","Aperture correction fails to measure a correction for the final plugin in the list and reports misleading errors","Since the refactoring of DM-4692, runs of *processCcd.py* detail the following in their logs:    {code:title=With base_PsfFlux and base_GaussianFlux plugins registered}  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_PsfFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measurement: Measuring 65 sources (65 parents, 0 children)   processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 1 flux fields  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  ...  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 2 flux fields  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr WARNING: Could not find base_GaussianFlux_flux or base_GaussianFlux_fluxSigma in apCorrMap  {code}    {code:title=With base_PsfFlux, base_GaussianFlux, and ext_photometryKron_KronFlux plugins registered}  processCcd.charImage.detectAndMeasure.measureApCorr: Measuring aperture corrections for 2 flux fields  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_PsfFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measureApCorr WARNING: Only 0 sources for calculation of aperture correction for 'base_GaussianFlux'; setting to 1.0  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 2 flux fields  processCcd.charImage.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  ...  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Applying aperture corrections to 3 flux fields  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr: Use naive flux sigma computation  processCcd.calibrate.detectAndMeasure.measurement.applyApCorr WARNING: Could not find ext_photometryKron_KronFlux_flux or ext_photometryKron_KronFlux_fluxSigma in apCorrMap  {code}    I can confirm that for the latter, running HSC data with the fix on DM-6063, the aperture corrections are being measured and applied for the PsfFlux and GaussianFlux measurements, but NOT for the KronFlux measurements.      Looking at the output from the current ""expected"" values for the {{lsst_dm_stack_demo}} we see that there is an offset in the Psf-Gaussian fluxes, implying the Gaussian fluxes are not being measured (and hence not applied):  !demo_current.png|width=500!    From this I conclude that the aperture corrections are indeed being measured for all but the final entry in the plugin list.  This implies that the report of ""Only 0 sources for calculation of aperture correction for 'xxx_xxFlux'; setting to 1.0"" is incorrect for all but the final plugin measurement.    The demo previously successfully calculated aperture corrections and, after the logic fix of DM-4836, applied them in the correct order:  !demo_previous.png|width=500!    The sources of these issues and fixes for them are the goal of this issue.",2
"DM-6082","05/13/2016 09:31:43","Add Sublime Text configuration tips to Developer Documentation","[~rowen] and [~Parejkoj] have some good tips about setting up Sublime Text.  [~jsick] suggested that we add these configuration tips to the Developer Documentation.    http://developer.lsst.io/en/latest/#part-tools    Both want to include info about recommended packages, but also the linter configurations to support the DM styles.    I paste in here various helpful parts from the HipChat Software Development room discussion of this.  Both verbatim, and summarized.    1. Install {{Package Control}}    2. Packages:  {{Git}}, {{GitGutter}}, {{SideBarEnhancements}}, {{SublimeLinter}}, {{SublimeLinter-flake8}}, {{SublimeLinter-html-tidy}}, {{SumNumbers}}, {{Gist}}, {{BracketHighlighter}}, {{TrailingSpaces}}, {{Trimmer}}, {{OmniMarkupPreviewer}}, {{ReStructuredTextImproved}}, {{MarkDown Editing}}, {{Colorsublime}}    3. Themes:  {{Sunburst}} color scheme    * VIM users:  {{Vintageous}}  + Mac OS X configuration:  {{defaults write com.sublimetext.3 ApplePressAndHoldEnabled -bool false}}  so that holding down 'j' moves downward.  Note that {{Vintageous}} is not a complete implementation of {{vim}}, but it at least allows enough basics so that one doesn't go crazy switching back and forth.    link the {{subl}} command to {{/usr/local/bin}}     Quick Tips:  ""option-select (to select blocks) and select something then cmd-D are both extremely useful for modifying lots of things at once.""    ""Similarly, ctrl-shift-up/down arrow.""    ""cmd-click on multiple lines to have multiple synchronized cursors""    Configurations:  1. [~rowen]'s SublimeText Preferences file: https://jira.lsstcorp.org/secure/attachment/27846/Preferences.sublime-settings  2. Configuration {{flake8}} so that it works in the linting can take a bit of work if {{flake8}} isn't in your default path.  See SublimeLinter.sublime-settings attachment for [~rowen]'s configuration: https://jira.lsstcorp.org/secure/attachment/27845/SublimeLinter.sublime-settings    The above are useful, but we'll need someone to detail the linter stuff more.",1
"DM-6083","05/13/2016 09:45:56","Enable websocket client to pickup channel parameter from url","send websocket channel information via url.  keep channel information on browser reload.    This is needed for Firefly Python API and external (when Firefly viewer is invoked trough URL) API.  ",1
"DM-6084","05/13/2016 09:50:48","meas_algorithms cannot be built with XCode 7.2.1","The LSST stack fails to build on OS X 10.10 ""Yosemite"" using XCode 7.2.1, the last version that is compatible with that operating system. I have attached a build log from [~wmwood-vasey].    [~smonkewitz] suggests trying the following (HipChat UW DM room, 2016-05-12 18:12 Pacific): in BinnedWcs.h change:  {code}  virtual PTR(afw::image::Wcs) clone() const {      return PTR(afw::image::Wcs)(new BinnedWcs(_parent, _xBin, _yBin, _xy0));  }  {code}  to:  {code}  virtual PTR(afw::image::Wcs) clone() const {          return PTR(afw::image::Wcs)(std::make_shared<BinnedWcs>(_parent, _xBin, _yBin, _xy0));  }  {code}    with the following explanation (HipChat SQuaRE room 2016-05-12  18:26 Pacific):    {{enable_shared_from_this}} usually holds a {{weak_ptr}} to this internally, and the first {{shared_ptr}} to manage an instance is assigned to the internal {{weak_ref}}. It looks to me like this particular version of the STL only allows {{std::shared_ptr<A>}} to be assigned to a {{weak_ptr<B>}} if {{A}} is implicitly convertible to {{B}}. In this case the first {{shared_ptr}} to manage the {{BinnedWcs}} is a {{shared_ptr<Wcs>}}, and I think a static cast is required to get from {{Wcs *}} to {{BinnedWcs *}}, so the compiler barfs.",0
"DM-6100","05/13/2016 15:20:32","afw/tests/rgb.py fails due to .ttf files","afw/tests/rgb.py fails for me with the below error. We likely shouldn't be trying to track system resources like fonts, as we don't have any control over them.    {code}  [2016-05-12T19:46:12.528961Z] Failed test output:  [2016-05-12T19:46:12.536029Z] tests/rgb.py  [2016-05-12T19:46:12.536057Z]  [2016-05-12T19:46:12.536070Z] ...s......ss...F.  [2016-05-12T19:46:12.536106Z] ======================================================================  [2016-05-12T19:46:12.536138Z] FAIL: testFileDescriptorLeaks (lsst.utils.tests.MemoryTestCase)  [2016-05-12T19:46:12.536173Z] ----------------------------------------------------------------------  [2016-05-12T19:46:12.536192Z] Traceback (most recent call last):  [2016-05-12T19:46:12.536261Z]   File ""/Users/parejkoj/lsst/lsstsw/stack/DarwinX86/utils/2016_01.0-4-g52f464f/python/lsst/utils/tests.py"", line 134, in testFileDescriptorLeaks  [2016-05-12T19:46:12.536330Z]     self.fail(""Failed to close %d file%s"" % (len(diff), ""s"" if len(diff) != 1 else """"))  [2016-05-12T19:46:12.536352Z] AssertionError: Failed to close 2 files  [2016-05-12T19:46:12.536356Z]  [2016-05-12T19:46:12.536391Z] ----------------------------------------------------------------------  [2016-05-12T19:46:12.536404Z] Ran 17 tests in 3.451s  [2016-05-12T19:46:12.536407Z]  [2016-05-12T19:46:12.536424Z] FAILED (failures=1, skipped=3)  [2016-05-12T19:46:12.536445Z] File open: /Library/Fonts/NISC18030.ttf  [2016-05-12T19:46:12.536479Z] File open: /System/Library/Fonts/Apple Color Emoji.ttf  [2016-05-12T19:46:12.536495Z] The following tests failed:  [2016-05-12T19:46:12.539928Z] /Users/parejkoj/lsst/lsstsw/build/afw/tests/.tests/rgb.py.failed  [2016-05-12T19:46:12.540060Z] 1 tests failed  {code}",0.5
"DM-6111","05/15/2016 11:46:58","Browsers should cache editions for a shorter time period than Fastly","Currently we set {{Cache-Control: max-age=31536000}} so that Fastly caches uploads from LTD Mason for a year on its POPs. This has the side-effect of also having browsers potentially cache documentation on the client for up to a year. In practice, browsers churn through their cache space more quickly, but I've noticed that Safari has no cap on its cache space, and therefore can hold onto pages for a long time.    The solution is to set a {{Surrogate-Control}} max age to 1 year, and have {{Cache-Control: max-age=0, private, must-revalidate}}. This will be done on LTD Keeper during the copy phase of a build into an edition (since it is reasonable for a client to cache a build forever), but then give us the flexibility to update an edition instantly.    In the future we may want a more nuanced solution where CSS and JavaScript, for example, are cached longer on the browser.",1
"DM-6112","05/15/2016 16:08:03","Provide minimal documentation for meas_extensions_photometryKron","Please provide a minimal level of documentation for meas_extensions_photometryKron, to include:  * A doc directory with the usual content so that docstrings get generated by Doxygen;  * A package overview;  * All docstrings should be appropriate for parsing by Doxygen (ie, should start with {{""""""!}} where necessary).  ",0.5
"DM-6126","05/16/2016 17:37:33","LSST's version of Astrometry.net doesn't build on Ubuntu 16.04","Reproduced building on Ubuntu 16.04.    https://groups.google.com/forum/#!topic/astrometry/aDCjhfMYhpE    The current version (0.67) does build successfully standalone.    These two patches fix 0.5.0:  https://github.com/dstndstn/astrometry.net/commit/7ded70917d7cf1efa1d3af6d0da8b336ebbf9d92.diff and https://github.com/dstndstn/astrometry.net/commit/7c65b3cefc4f33c59af90c1a40b5f246002cdf28.diff  Though only the first one is needed, I believe the second one is part of the build already.",1
"DM-6127","05/16/2016 18:05:05","ngmix has no license","ngmix does not have a license, which means we shouldn't distribute it. Work with Erin Sheldon to see if he is willing to add one.",1
"DM-6128","05/16/2016 20:13:00","Expanded view not doing fit/fill consistently ","Expanded view not doing fit/fill consistently. Sometimes is seems to fit/fill and resize it correctly, other times it stays at the zoom level.  It should always fit/fill and change zoom level with resize when in expanded mode. (unless zoom type is FORCE_STANDARD).",1
"DM-6133","05/17/2016 11:06:44","mpi4py does not compile under Yosemite due to hardcoded MACOSX_DEPLOYMENT_TARGET","{{mpi4py}} build on Yosemite (Mac OS X 10.10) fails with   {code}  _build.log:[2016-05-17T16:51:55.847161Z] error: $MACOSX_DEPLOYMENT_TARGET mismatch: now ""10.9"" but ""10.10"" during configure  {code}    For details see attached build log.    The {{MACOSX_DEPLOYMENT_TARGET}} is being set in {{ups/eupspkg.cfg.sh}}    {code}  [serenity mpi4py] cat ups/eupspkg.cfg.sh  # If MACOSX_DEPLOYMENT_TARGET is not set, we force it to be at least 10.9  # (Mavericks). This is the earliest version of OS X expected to work with  # release 11 of the LSST stack.  # This works around DM-5409, wherein mpi4py was attempting to use an OS X 10.5  # SDK, based on querying Anaconda, and failing.  export MACOSX_DEPLOYMENT_TARGET=${MACOSX_DEPLOYMENT_TARGET:-10.9}  {code}    What is it that is supposed to be setting {{MACOSX_DEPLOYMENT_TARGET}}?  And why is it not set at the time when {{ups/eupspkg.cfg.sh}} is run, but is set to 10.10 by the time the actually compilation is done?   ",1
"DM-6138","05/17/2016 16:02:38","Change Fields groups to handle other actions better","The fields group can be out of sync with actions if they are trying to use store data when that actual value is changes.  This is a classic side-effect issue.  It can be solved with sagas.    Our current example.  The color panels updating from the plot when the activePlotId changes.    More to do:  * field groups need a sega to more effective respond to out side actions  * the dispatchChangeFieldGroup needs better, more documented parameters  * update multiple fields at the same time.  * should we have the field group support reset to init state? probably not, but look into it.  * change init values?  * Check example dialog and see if the large/smaller example is validating correctly.",2
"DM-6139","05/17/2016 16:05:23","Change server side hardcopy code to work better with the non-GWT call","The server hard to make a hard copy now takes a StaticDrawInfo object.  We want to use only a region array.  Change the server side to support this.",2
"DM-6147","05/18/2016 14:36:47","Set SUSPECT mask in ISR task and make saturation a double","Implement RFC-190 and mask suspect pixels:    Add {{selectLevel}} to {{lsst.afw.cameraGeom.AmpInfoCatalog}}, as a double, and add support for it to {{lsst.ip.isr.IsrTask}}, analogously to masking saturation: iif {{suspectLevel}} is not {{nan}} then set the {{SUSPECT}} flag for pixels above the suspect level.    Also change the type of {{saturation}} in the {{AmpInfoCatalog}} from {{int}} to {{double}}, so that the existing test for {{nan}} actually works",5
"DM-6149","05/18/2016 15:28:38","Reduce memory utilization in mysql proxy","Jon is trying to run tests with large result which kills proxy/czar because it runs out of virtual memory. Would be nice to reduce memory use and find a way not to keep query result in memory.",2
"DM-6151","05/18/2016 19:26:12","Failure to fail when fallbackFilterName is None","When no {{fallbackFilterName}} is set, we can get a confusing error message when failing to load a calib:  {code}  RuntimeError: Unable to retrieve dark for {'filter': 'U', 'date': '2016-05-12T02:58:56.591', 'ccd': 0, 'basename': '2016-05-12skyflats_02', 'object': 'FLAT', 'visit': 883, 'expTime': 20.0006, 'channel': 16} and no fallback filter specified: Unknown value type for filter: <type 'NoneType'>  {code}  This is unrelated to the calib load failure, and merely reflects the fact that {{fallbackFilterName=None}}.",1
"DM-6166","05/20/2016 17:41:31","Time AST and compare to our WCS code","Time TAN-SIP for our code and for AST, in order to get a sense of the performance impact of switching to AST for our WCS implementation.",3
"DM-6177","05/23/2016 03:26:20","Increase memory locked amount in container","In order to lock memory, the memory locking limit within the container for the qserv worker needs to be raised. My understanding is the container uses whatever is the host setting so the limit has to be set for the container user and whatever the user is inside the container. The particular limits is:    memorylocked 64 kbytes    notice that by default it's 64K. That needs to be raised to say 75% of the real machine size. I wouldn't make it unlimited as a memlock mistake may crash the whole machine. The limits are specified in ""/etc/security/limits.conf"". You will know that you are successul when you ssh into the container as the qserv worker user and the ""limit"" command tell you have can lock lots of memory.    We would also set the CAP_IPC_LOCK privilege but setting the soft/hard limit above should be good enough. So, let's start with that. ",2
"DM-6179","05/23/2016 08:35:50","Support Python 3 migration","Support the migration of the DM code to Python 3. This includes writing transition documentation, integration of a new scons, migrating a handful of low-level packages and liaising with the teams on their packages.    The final outcome of this epic is that everything would be in place for the migration at the August All Hands meeting.",40
"DM-6180","05/23/2016 08:39:53","Update LSE-61 requirements and traceability","With the updates to DPDD and LDM-151 in the early part of F16, there is a need to update LSE-61 (DMSR) such that it can directly trace requirements from OSS+DPDD through LSE-61 and down to implementation LDM documents.    This will require substantial rewrites of many of the existing requirements and possible addition of new requirements. It may also be necessary to add annotations to DPDD and other LDM documents to provide traceability anchors for DMSR.    The outcome of this epic is a new baselined DMSR approved by CCB.",40
"DM-6185","05/23/2016 21:17:40","Get jointcal running on minimum data","It is very important for other teams to have a version of jointcal running to remove the sensitivity on the errors in astrometric reference catalogs.  The suggestion is to get jointcal running with CFHT, HSC, DECam and lsstSim.",100
"DM-6188","05/23/2016 22:39:51","First draft of overview (""vision"") document","See https://dmtn-016.lsst.io",3
"DM-6199","05/24/2016 00:41:24","Stack API documentation ","Stack API Doc generation -> pipelines.lsst.io    Deliverables:     * a proof of concept of a CI-driven API doc build  * documentation content planning",20
"DM-6206","05/24/2016 03:11:10","CI Improvements: Jenkins 2 upgrade etc","This epic covers a timeboxed maintainance of the Jenkins-based CI system, including the Jenkins 2 upgrade as well as the required updates to the Jenkins-puppet module. It also may include work done as part of DM-6204 brought over to the apps CI service.     Highlights:     - Jenkins 2 upgrade in production at ci.lsst.codes    - Improvements releases as part of the widely used puppet Jenkins module https://github.com/jenkinsci/puppet-jenkins        ",8
"DM-6208","05/24/2016 03:18:45","SQuaRE services disaster recovery","This is a timeboxed effort to test and improve backups and disaster recovery for SQuaRE services. It is unlikely to be sufficient in itself.     Delivered: daily snapshots of git repos in lsst, lsst-dm and lsst-sqre orgs, with a progressive roll-off retention policy. Git-LFS repos are not currently serviced by this strategy, work on this will be scheduled at a later date.   ",8
"DM-6209","05/24/2016 03:19:49","Ad-hoc developer requests","This is a bucket epic for ad-hoc developer requests that cannot be postponed till the next planning cycle. In the event that it is underutilised for this purpose, it will be assigned to technical debt DM-5850",8
"DM-6224","05/24/2016 09:24:01","F16 Butler Repository Refactor","Per KT, the parent/peer repository relationship scheme was not an exact fit for what we need. We discussed and decided that butler should manage its own input and output repositories. Also discussed with KT and Gregory was the ability to select inputs by 'tagging' repositories. The design discussion with the larger group is captured in RFC-184.",40
"DM-6239","05/24/2016 11:22:28","The grid labels are not placed in the right position when the coordinate is Ecliptic coordianates","The algorithm to calculate the label position does not work well for the Ecliptic coordinate system.  The algorithm needs to be modified to work for all the coordinates.",2
"DM-6246","05/24/2016 13:17:24","Vertical overscan off by one again","In DM-5524 [~price] fixed the vertical overscan by directly editing the amp info catalogs, but didn't mark the camera generating code as bad. In DM-6147 I regenerated the files, reintroducing the problem. The problem seems to be a subtle bug in the camera generating code. Rather than try to fix it, I'll convert the fixed catalogs directly and mark the generating code as broken. [~price] will issue an RFC that suggests a better way to handle generating amp info and once that is dealt with we can come up with a more permanent fix (e.g. delete the generating code or fix it).",0.5
"DM-6247","05/24/2016 13:18:08","DRP Outline for LDM-151","Write outline for Data Release Production section of LDM-151, using the DRP Data Flow diagram as the organizing principle.",2
"DM-6248","05/24/2016 13:19:53","DRP Top-Level Diagram and Descriptions, Draft 1","Insert the content from the DRP Data Flow diagram on Confluence into LDM-151, adjusting it to the outline developed on DM-6247.",2
"DM-6251","05/24/2016 13:21:54","Convert DRP Top-Level Diagram to standard conventions","DM-6248 adds a large, complex diagram that will need to be cleaned up and converted to use the same conventions and colors as other diagrams in LDM-151.",2
"DM-6261","05/24/2016 13:48:00","Cleanup and standardize DRP background matching, coaddition, and diffim diagrams","DM-6256 will produce rough diagrams that will require cleanup and standardization.    [~ctslater] has made some suggestions for the current diagram that I'll implement on this issue, so I'm assigning it back to me.  I'll also go ahead and integrate his updated DRP overview diagram (currently on Confluence) into LDM-151 here.  ",1
"DM-6280","05/25/2016 12:37:26","The labels in HMS formate are wrong in WebGrid","The labels in HMS format no longer show hh:mm:ss anymore.  The porting introduced the bug.  ",1
"DM-6281","05/25/2016 13:01:02","More visualization features in S17","New features for visualization in Firefly: error-bars in 2D scatter plot, allow user to control symbol size and shapes for overlaid objects, histogram improvement, density plot improvements.",100
"DM-6288","05/25/2016 18:22:29","Chart options display","Make chart options ""in-place"" popup, similar to table options for consistent look. It will also alleviate resizing, because the chart size won't need to change when options are open.",3
"DM-6294","05/26/2016 11:56:58","Add support for pybind11 to build system","Add pybind11 as third party package to the stack. Update sconsUtils to support building with pybind11. Use daf_base DateTime to demonstrate that this works.",8
"DM-6297","05/26/2016 12:01:26","Wrap afw::detection with pybind11","The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-8467 is complete.    The tests included in this ticket are:  # {color:#14892c}testExposureTable.py{color}  # {color:#14892c}testFootprint1.py{color}  # {color:#14892c}testFootprint2.py{color}  # {color:#14892c}testFootprintEllipse.py{color}  # {color:#14892c}testFootprintMergeCatalog.py{color}  # {color:#14892c}testGaussianPsf.py{color}  # {color:#14892c}testHeavyFootprint.py{color}  # {color:#14892c}testRgb.py{color}  # {color:#14892c}testTicket2019.py{color}",5
"DM-6306","05/26/2016 14:43:29","Executable test in utils needs to test an executable","In DM-4036 all the test binaries were removed as no longer being needed. This had the unfortunate side effect that the {{testExecutables.py}} test no longer tests anything. This ticket will be used for adding a test file.",1
"DM-6309","05/27/2016 07:11:52","Update LDM-151 with SDQA Skeleton and Outline","1. Update the LDM-151 draft with the SDQA Skeleton from the DMLT + SciPipelines working group discussions of May 16-20.  Implement as bullet points in a semi-coherent list. (/)    2. Clean up list. (/)",1
"DM-18251","05/27/2016 10:34:25","Old T&S Epic carried over from old JIRA project - no longer valid","Old T&S Epic carried over from old JIRA project - no longer valid",2
"DM-6313","05/27/2016 11:43:40","Create miniconda3 EUPS package","{{newinstall.sh}} currently installs miniconda via EUPS. To replicate that functionality in Python3 we need to create a {{miniconda3}} package. This package should be almost identical to {{miniconda2}}.    Requires that {{lsstsw}} first be updated to support python 3.",1
"DM-6314","05/27/2016 11:51:39","Port lsstsw to Python 3","Get {{lsstsw}} working with Python 3:  * Update the {{deploy}} script to allow a Python 3 python to be installed and modify the version checking code.  * Demonstrate that {{lsstsw}} {{rebuild}} will successfully build and install a third-party non-Scons package.",5
"DM-6316","05/27/2016 12:18:25","Update newinstall.sh to support Python 3","{{newinstall.sh}} currently insists on installing and checking for python 2.7. This needs to be changed to allow Python 3.    Requires {{sconsUtils}} works with Python 3 as the {{lsst}} EUPS package is installed as part of {{newinstall.sh}}.",1
"DM-6317","05/27/2016 12:21:50","Update developer guide to include Python 3","Update the developer guide to indicate that Python 3 must be supported and that code must run on Python 2.7 and 3.    This ticket will reference the tech note delivered as part of DM-6315. Writing extensive user documentation on the {{future}} package is beyond the scope of this ticket.",2
"DM-6319","05/27/2016 12:43:15","Adjust sconsUtils to query python on path for executable location","{{sconsUtils}} has to be modified to ensure it works with Python 3. Additionally SWIG calls might need to be changed to trigger Python 3 mode.",3
"DM-6320","05/27/2016 12:45:05","Port utils to Python 3","Ensure that the {{utils}} package will work with Python 3.",2
"DM-6322","05/27/2016 12:47:00","Port base package to Python 3","Ensure that {{base}} works with Python 3.",2
"DM-6323","05/27/2016 13:13:36","Lead Python 3 migration at All Hands Meeting","* Prepare for all hands meeting.  * Present plan to developers.  * Advise developers doing migration.  * Contribute fixes as required.",8
"DM-6325","05/27/2016 14:50:55","Replace BOOST_STATIC_ASSERT with static_assert","Replace BOOST_STATIC_ASSERT with static_assert from C++11.",0.5
"DM-6326","05/27/2016 15:15:26","reST roles for mock code references","Add mock code reference roles so that authors can add semantics to their writing without attempting to make actual references to API documentation that does not _yet_ exist. Covers all roles in the Python domain, and supports tilde syntax for collapsing the namespace.",0.5
"DM-6328","05/28/2016 17:08:35","add hsc driver script to validate_drp","Add an equivalent of {{examples/runChftTest.sh}} to {{validate_drp}} to process {{validation_data_hsc}}.",8
"DM-6346","05/31/2016 11:55:05","User installation and operation instructions for conda ","Create documentation for the Stack conda binaries created in DM-5415 as part of the Science Pipelines documentation",3
"DM-6348","05/31/2016 14:01:06","Write Calibration Products Production section of LDM-151","Write photometric calibration pipeline section of LDM-151",20
"DM-6368","06/01/2016 10:26:54","Adjust version check of EUPS python package to allow v3","To enable Python 3 support of the stack the EUPS {{python}} stub package needs to allow Python 3.    ",0.5
"DM-6373","06/01/2016 14:52:12","Improve skeleton for LDM-151 Algorithmic Components","For all subsections in  Algorithms Components owned by [~jbosch]:   - Provide enough bullet points to capture scope.   - Add bullet points for subtly difficult aspects of components.   - Add extra level subsubsubsection level for Measurement.   - Create matrix of measurement algorithms and contexts.  ",1
"DM-6375","06/01/2016 15:18:58","New image visualization functions (F16)","TO support the pipeline QA and build the first web portal, there will be new functions need to be developed.  This epic is to collect those functions.",40
"DM-6378","06/01/2016 15:28:38","Persist output of simple DCR correction","DM-5695 will create transfer matrices stored as numpy arrays. This ticket extends that work to determine a useful format and write functionality to persist those arrays.",2
"DM-6383","06/01/2016 15:44:50","Use template DCR images for image differencing","DM-6382 creates template images of a field at arbitrary airmasses, which can be used to match the template airmass to the science image precisely to mitigate Differential Chromatic Refraction in image differencing. This ticket is to determine the best method to supply the new templates to image differencing, which may be simply to create a new exposure and ingest/process the template as though it were a real observation.",2
"DM-6392","06/01/2016 19:29:13","Text on variability characterization for LDM-151","Expand the variability characterization algorithmic section of LDM-151.",1
"DM-6437","06/02/2016 14:41:35","Convert GWT projection and Coorindate Conversion routine to JavaScript","convert Booth's projection code and Judy Bennet's coordinate conversion routines to pure javascript  ",8
"DM-6457","06/03/2016 15:57:22","Design and RFC for Repository Refactor","Drive the RFC for Repo Refactor to completion (this includes a lot of design work)",20
"DM-6458","06/03/2016 16:00:24","Expand skeleton in LDM-151","We need to flesh out the skeleton text to contain full descriptions of the algorithms and pipelines we expect the baseline design to use.",20
"DM-6459","06/03/2016 16:00:39","productize ""Repository Refactor""","After RFC-184 is closed: implement, unit tests, review, document, submit.    When this story closes, I think RFC-184 status is supposed to be changed from Adopted to Implemented.",20
"DM-6473","06/06/2016 15:38:18","Possible image related issues in firefly viewer","Image Meta Data tab  * images cannot be remove, but in expanded mode, it can.  * selecting image no longer highlight table.  the reverse works fine.  * visualize/saga/ImageMetaDataWatcher.js:272 returns -1.  * when a non-meta table is selected, images are shown, but not the toolbar.  * after table is removed, images are still there.    image external api does not mix well with firefly viewer.  * firefly viewer uses 'triViewImages’ viewer_id while api has no viewer_id.  as a result, images loaded by api will be lost once table or other searched data are returned.    * catalog overlay are drawn outside of the images.    more issues:    - -It's possible to select distance tool and then area selection. First drag would define area selection, all the following line. A click would be defining a 0 length line, even if point selection is enabled.- _moved to_ DM-6656  ",8
"DM-6474","06/06/2016 17:26:01","Restore star selector registry","Restore the registry for star selectors that was lost in DM-5532, now that tasks in registries can be used as subtasks.    Also use the registry where appropriate.",2
"DM-6490","06/07/2016 11:38:11","Investigate calibration zeropoint offset between HSC vs. LSST processCcd.py runs","As reported in DM-4730, while the scatter between single frame processing measurements of the same dataset on the HSC vs. LSST stacks is quite good (rms = 0.009 mag between Gaussian fluxes, for example, in the figure shown on that ticket), there is a clear offset (0.0166 mag in the figure shown) in the zeropoint between the two stacks (it is systematic, i.e. no trend with magnitude).  The cause may well be due to slight differences in the reference stars selected for calibration.  We also speculated about differences in slot definitions used in the calibrations steps (e.g. for aperture corrections, psfex, etc...), so I have rerun visit 1322 through both stacks having forced all apertures used in calibration to be the same, namely a circular aperture of 12 pixels measured using the sinc algorithm (as opposed to ""naive"").  I have attached the *processCcd.py* config files for the two runs so my settings can be reproduced.    Also of note, I am using a {{meas_algorithms}} branch on the HSC stack with the following commit:    {code}  commit 173ad0b32ed4f4ab074f1a942d2d3f758e189917  Author: Lauren MacArthur <lauren@astro.princeton.edu>  Date:   Wed Jan 13 16:35:59 2016 -0500        Hack to allow flux.aperture to be used in apCorr            Since it does not seem possible to access the nth element of a      schema element that is an array in the context of setting a config      override, this allows for flux.aperture to be set as      calibrate.measureApCorr.reference and it sets it to index 4 (which      corresponds to a radius of 12 pixels) in the __init__.  This was      selected to match the current LSST default.    diff --git a/python/lsst/meas/algorithms/measureApCorr.py b/python/lsst/meas/algorithms/measureApCorr.py  index 9f6c599..f1fa99d 100644  --- a/python/lsst/meas/algorithms/measureApCorr.py  +++ b/python/lsst/meas/algorithms/measureApCorr.py  @@ -81,6 +81,9 @@ class MeasureApCorrTask(lsst.pipe.base.Task):       def __init__(self, schema, **kwds):           lsst.pipe.base.Task.__init__(self, **kwds)           self.reference = KeyTuple(self.config.reference, schema)  +        if self.config.reference == 'flux.aperture':  +            print ""NOTE: setting aperture correction flux to flux.aperture[4] ==> radius = 12 pixels""  +            self.reference.flux = self.reference.flux[4]           self.toCorrect = {}  {code}    I attach some of the figures comparing the PSF fluxes from these runs which compare the output of the two stacks having matched the two src catalogs.  There are two sets: 1) having adjusted the flux for each source to the zeropoint calculated in the calibration and stored as *FLUXMAG0* 2) having adjusted the flux for all sources to a common zeropoint (zp=33.0, chosen to roughly match the calibrated zp).  Note that my figures do include aperture corrections (in DM-5301, many of the plots show fluxes pre-aperture correction).  I have also included plots that directly compare the aperture corrections applied (difference in mag units).  Finally, I also include plots comparing the 12 pixel circular aperture mags (i.e. to which no apCorr is added).    Clearly, the zeropoint determined in the calibration of the two stacks differs between the two stacks and, in particular, there seem to be some very problematic CCDs where the differences are particularly significant (~0.05 mag, and not always in the same direction).  Please investigate the source of this discrepancy.",8
"DM-6494","06/07/2016 15:43:06","Better error messages from the camera mapper when a template cannot be formatted","The CameraMapper produces a very unhelpful traceback if it cannot format a template string with the provided data ID dict. For example:  {code}  Traceback (most recent call last):    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_tasks/12.0.rc1-3-gb785bf9/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/cmdLineTask.py"", line 450, in parseAndRun      parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 479, in parse_args      self._processDataIds(namespace)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 577, in _processDataIds      dataIdContainer.makeDataRefList(namespace)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 126, in makeDataRefList      dataRef=dr)]    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/pipe_base/12.0.rc1-1-g832266b/python/lsst/pipe/base/argumentParser.py"", line 935, in dataExists      return butler.datasetExists(datasetType = datasetType, dataId = dataRef.dataId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/butler.py"", line 288, in datasetExists      locations = self.repository.map(datasetType, dataId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 392, in map      return self.doParents(Repository.doMap, *args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 325, in doParents      res = func(parent, *args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/repository.py"", line 405, in doMap      loc = self._mapper.map(*args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_persistence/12.0.rc1-1-gc553c11+4/python/lsst/daf/persistence/mapper.py"", line 169, in map      return func(self.validate(dataId), write)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 284, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/mapping.py"", line 123, in map      path = mapper._mapActualToPath(self.template, actualId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 732, in _mapActualToPath      return template % self._transformId(actualId)  TypeError: %d format: a number is required, not NoneType  {code}    It is unclear what string was being formatted with what data, making the problem difficult to diagnose and correct.    I suggest changing line 732 of CameraMapper.py from:  {code}  return template % self._transformId(actualId)  {code}  to something like the following:  {code}  try:      transformedId = self._transformId(actualId)      return template % transformedId  except Exception as e:      raise RuntimeError(""Failed to format %r with data %r: %s"" % (template, transformedId, e))  {code}    Here are the last few lines of the same traceback after applying this change:  {code}      path = mapper._mapActualToPath(self.template, actualId)    File ""/Users/rowen/UW/LSST/lsstsw/stack/DarwinX86/daf_butlerUtils/12.0.rc1+6/python/lsst/daf/butlerUtils/cameraMapper.py"", line 735, in _mapActualToPath      raise RuntimeError(""Failed to format %r with data %r: %s"" % (template, transformedId, e))  RuntimeError: Failed to format '%(date)s/%(filter)s/decam%(visit)07d.fits.fz[%(hdu)d]' with data {'date': '2013-02-10', 'ccdnum': 10, 'hdu': None, 'visit': 176837, 'filter': 'z'}: %d format: a number is required, not NoneType  {code}    A bit wordy, but it is much easier to figure out what went wrong.    I have stumbled across this problem twice in the last few weeks, so I consider this change fairly important. The first time it was caused by a defective format string in a paf file. This time I'm not yet sure what is causing it, but at least I have something to go on.",0.5
"DM-6497","06/08/2016 02:14:02","Assist IN2P3 engineer in loading DC2013 data sample","Bogdan Vulpescu, IN2P3 engineer, tried to load DC2013 data sample. Fabrice help was required to install Qserv in multi-nodes and understand data-loading system.    Some issues have been found and will be reported in future tickets:    - a script to publish loaded data (i.e. insert db name in qservw_worker.Dbs) would be useful  - mysql client might break proxy if option are not provided correctly (a bug report will be available soon)",3
"DM-6514","06/09/2016 09:06:59","Minor fixes to linearization","DM-5462 added linearization to {{IsrTask}} but had a few loose ends which this ticket aims to correct:  - I intended to enable linearization by default, but somehow lost that change.  - I intended to update obs_test to use null linearization, but I forgot and the previous item meant I didn't catch the omission  - It turns out that the butler data proxy object will not work with functors (attempting to call the retrieved item results in an error, rather than resolving the proxy). This is easily worked around by using immediate=True when retrieving linearizers. This didn't show up until DM-6356 because obs_decam is the only camera that uses linearization lookup tables, and obs_subaru avoids the problem by not returning a proxy.  ",1
"DM-6516","06/09/2016 11:09:11","Convert footprint support","Convert the footprint support from the GWT code",20
"DM-6519","06/09/2016 15:22:54","Temp local background broken","The temp local background feature has been broken and needs to be fixed.",0.5
"DM-6533","06/12/2016 15:58:52","LDM-151 adjustments","Adding text to LDM-151 where appropriate, working around the structure defined by Jim et al.",2
"DM-6542","06/14/2016 09:26:00","Prevent external viewer from popup blockers.","Currently, when external viewer launched, it is blocked by pop-up blockers. Need to change polling logic to a pushed solution so the 'launch' action can happen immediately. ",3
"DM-6566","06/15/2016 15:29:08","Make updateSourceCoords and updateRefCentroids more visible","Implement RFC-197 to make updateSourceCoords and updateRefCentroids more visible",1
"DM-6569","06/16/2016 09:51:33","Remove the extra init method from the SourceDetectionTask","SourceDetectionTask defines both {{init(self, schema=None, **kwds)}} and {{\_\_init\_\_(self, schema=None, **kwds)}}. The first exists purely because of a Doxygen bug that makes {{\copydoc \_\_init\_\_}} fail. However,   {code}  copydoc \_\_init\_\_  {code}  works. Remove the non-dunder init method and update the documentation with  {code}  \copydoc \_\_init\_\_  {code}.",1
"DM-6590","06/17/2016 14:08:38","Collection of small bugs and look and feel issues","1. In the color stretch dialog:  (in chrome Version 50.0.2661.102 (64-bit) on my Mac OSX 10.9.5)      * the asinh beta and powerLaw Gamma input field do not  look like part of the input fields group. Too much white space in between.      * When clicked on the Z scale option when the asinh and powerLaw types selected, the min and max values overlapped with the option choice. ",2
"DM-6592","06/17/2016 16:10:13","enable subMenu in the menu list","We want to have a subMenu component in the Firefly so we could group some of the actions together by putting them into subMenu.         Added submenu capability to drop down menu. Submenu can be nested to multiple levels.  Also, implemented ticket DM-14515 to place project footprints into submenu.  You can test this by loading any image, then select {{Overlay Markers}} to see drop down with submenu.",8
"DM-6601","06/20/2016 12:15:08","Port change to EXP-ID handling","From [HSC-1409|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1409]:  {quote}  Due to an operational reason (to meet the requirement of Subaru FITS dictionary), the definition of EXP-ID is soon to be changed in the data acquisition side.  In the new definition, EXP-ID is set to 'HSCE%08d' where the letter 'E' is fixed as requested in the dictionary, and the number part corresponds to exactly the same number as our familiar 'visit'.  obs_subaru:ingest.py needs to be updated to include this rule.  The data taken with this change so far are:  HSCA07441200--HSCA07441757  HSCA90925200--HSCA90929557  {quote}    The change made as part of HSC-1409 introduces a new code path for the updated data, while old data continue to be supported with the old code path.",0.5
"DM-6608","06/20/2016 16:46:35","Finalize v12 Pipelines release documentation","Add the release announcement and finalize other documentation details in pipelines.lsst.io for the v12 release.",1
"DM-6612","06/21/2016 02:39:18","Make HSC processing without bright object catalogs easier","obs_subaru enables bright object masks by default, as that's desirable for HSC production runs.      However, when HSC data is processed without bright object masks available (as will happen in most GO observations and development use), multiBandDriver.py will fail because the BRIGHT_OBJECT mask plane is not present but the base_PixelFlags algorithm is configured to make use of it. This is confusing, and it also requires the definition of a configuration file to fix the problem because base_PixelFlags cannot be configured directly on the command-line.    Some possibilities for fixing this:   - Add the BRIGHT_OBJECT mask plane in AssembleCoadd if doMaskBrightObjects is True but the external catalog is not found.  This will make the PixelFlags operation a silent no-op.   - Allow configuration options to allow PixelFlags algorithm to silently skip some flags if the appropriate masks are not available.    I am sure there are other options as well.  ",2
"DM-6613","06/21/2016 03:32:22","Typo in afw/display/interface.py","There's a typo in interface.py;  the patch is  {code}  diff --git a/python/lsst/afw/display/interface.py b/python/lsst/afw/display/interface.py  index 8599629..d7f58ea 100644  --- a/python/lsst/afw/display/interface.py  +++ b/python/lsst/afw/display/interface.py  @@ -537,7 +537,7 @@ class Display(object):               k, x, y = ev.k, ev.x, ev.y      # for now                  try:  -                if self.callbacks[k](k, x, y):  +                if self._callbacks[k](k, x, y):                       break               except KeyError:                   print >> sys.stderr, ""No callback is registered for %s"" % k  {code}",2
"DM-6614","06/21/2016 03:36:27","Include Kron parameters in algorithm metadata","The Kron code doesn't set the algorithm metadata.  E.g.  {code}  algMetadata.set(""ext_photometryKron_KronFlux_nRadiusForFlux"",                  config.plugins[""ext_photometryKron_KronFlux""].nRadiusForFlux)  {code}  ",1
"DM-6617","06/21/2016 10:08:30","Catalog Search Panel bugs","from pull request DM-6500:  https://github.com/Caltech-IPAC/firefly/pull/102  * 2MASS All-Sky 'Read Me!' link in Catalog Search panel points to http://localhost:8080/irsaviewer/Q'http://hades.ipac.caltech.edu/applications/Gator/GatorAid/irsa/scan.html'E * -If you search m31 2MASS scan info catalog (4th catalog in the list), then bring back catalog panel, the first catalog is highlighted, but if you search, you'll notice that you are still searching 2MASS scan info.-",2
"DM-6620","06/21/2016 13:20:18","Cannot instantiate LoadAstrometryNetObjectsTask without Config object","One should be able to create a LoadAstrometryNetObjectsTask without passing a Config object, if one only wants the default configuration. Currently it raises TypeError:    {code}  Traceback (most recent call last):    File ""testJointcal.py"", line 79, in setUp      refLoader = LoadAstrometryNetObjectsTask()  TypeError: __init__() takes at least 2 arguments (1 given)  {code}    If the config object really is a kwarg, it should default None and create a default config, so that one doesn't have to do, e.g.:    {code}  LoadAstrometryNetObjectsTask(LoadAstrometryNetObjectsConfig())  {code}",0.5
"DM-6627","06/21/2016 14:39:09","Fix base_* stuff in CcdImage.cc","CcdImage.cc currently has hard-coded a bunch of {{getSchema().find(""base_blah"").key}} things. These should either be replaced with ""slot_*"", config.blahName, or dealt with at a higher level (e.g. not loading all those values directly inside of ccdImage::LoadCatalog).    Once this is done, we should delete the comments at the top of the file.",1
"DM-6630","06/21/2016 17:02:41","Support ingesting reference catalogs from FITS files","Support a means of ingesting index reference catalogs from FITS tables (e.g. SDSS catalogs).",2
"DM-6631","06/22/2016 04:12:34","Single-frame processing tasks are no longer usable without a Butler","Adding a butler argument to the constructor signatures for {{CharacterizeImageTask}}, {{CalibrateTask}}, and {{ProcessCcdTask}} makes these tasks difficult to use without a butler.    The fix is to make the butler argument optional (with a default of None), while adding another argument that allows a fully-constructed reference object loader to be provided directly instead.    This is closely related to DM-6597, which has the opposite problem: pipe_drivers' {{SingleFrameDriverTask}} doesn't take a butler argument, but it needs to in order to provide one to {{ProcessCcdTask}}.    I have a fix for this just about ready, but I'd like to add some unit tests that verify we can run all of these tasks both from the command-line and directly before calling it complete.",3
"DM-6633","06/22/2016 05:31:23","HSC ISR configuration file is applied to ProcessCcdTask, not IsrTask","{{obs_subaru/config/hsc/isr.py}} has its config options specified relative to {{ProcessCcdTask}}'s config hierarchy, not {{IsrTask}}'s.  This allows the ISR task to be retargeted in this file, but it will prevent {{IsrTask}} from being run as a {{CmdLineTask}} directly.    ISR Task retargeting should be moved to {{config/processCcd.py}}, allowing the {{config/isr.py}} level to be moved to the appropriate level.",1
"DM-6635","06/22/2016 08:14:07","Typo in CoaddSrcTransformTask","[~price] points out  a typo [in {{CoaddSrcTransformTask}}|https://github.com/lsst/pipe_tasks/blob/0eef0fd518098cc25c66bfff40f53ccca9058431/python/lsst/pipe/tasks/transformMeasurement.py#L263]: {{self}} should not be repeated.",1
"DM-6638","06/22/2016 13:35:52","LTD Keeper: Auto slug for edition paths deals with underscores","Had a bug where {{utils.auto_slugify_edition}} did not replace underscores with a dash, and therefore failed {{utils.validate_path_slug}}. This created a silent breaked where a branch like {{u/rowen/r12_patch1}} did not get an edition created for it.    This ticket adds this replacement code and adds a test for such a case.",0.5
"DM-6647","06/23/2016 09:49:06","Adapt qa analysis script to apply corrections measured by meas_mosaic","DM-2674 involves getting HSC's {{meas_mosaic}} working with the LSST stack.  This issue consists of adapting the analysis.py script of DM-4393 & DM-4730 to (optionally) apply the astrometric and photometric solutions derived running {{meas_mosaic}} to the individual visits before comparison.  This is useful in general and is specifically useful in comparing the {{meas_mosaic}} results between the HSC and LSST stacks.",2
"DM-6651","06/23/2016 12:14:49","Move new reference loader so meas_astrom can use it and perform some cleanup","The new reference object loader code lives in pipe_tasks, which means it cannot be directly used by code in meas_astrom. This will hamper separating astrometry.net out of meas_astrom, because unit tests need reference catalogs and meas_astrom cannot depend on pipe_tasks.    Also, I'd like to take a cleanup pass on the module names, so the new code is easier to find, and improve the unit tests.",2
"DM-6652","06/23/2016 12:16:37","Remove database hack","DM-5988 introduced a hack in reading the raw files: we use a database to cache metadata from the shutter files and update the camera files at read time.  The camera files have now been ""sanitised"" (updated with the appropriate metadata), and it's time to remove the hack.    [~mfisherlevine] writes:  {quote}  Data is on lsst-dev in:    /nfs/lsst2/photocalData/data/monocam/sanitised9/1m3/1m3    Raw calibs are in:    /nfs/lsst2/photocalData/data/monocam/sanitised9/1m3/calibs    Regarding what I want: everything to be the same, but with a normal ingest, i.e. no splicing, just taking everything that is needed from one set of files. Some points to note:    * should be able to ingest all the raws and calibs files, and register their OBJECT types to allow processing with these as ids (inc. pipe_drivers scripts)  * pipe_drivers master calib scripts should still run (and their outputs still be ingestable)  * processCcd should run  {quote}",2
"DM-6653","06/23/2016 12:40:21","implement the active target","When a dialog such as catalog search is displayed, it should be able to pick up the active target or the coordinates from a highlighted row in a table. Please, implement the mechanism that will automatically pick up those coordinates and pre-fill the search form for you.",0.5
"DM-6657","06/23/2016 17:05:42","ffApi XYplot related issues found by irsa integration","* default  symbol size, shape,and color setting is different from that of original version.  * no XY Plot Options pop-out windows  *  the plot displays non-ascii characters on the panel (for example:  Fit  )  * miss Filter Dialog on the plot panel comparing with the original version.  *  does not accept default column names for the plot.",1
"DM-6660","06/23/2016 20:13:55","CR finder does not care about XY0 of input image","Port of [HSC-1391|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1391]:  {quote}  The current version of CR finder does not care about XY0 of the input image and when I try to run CR finder on warped (difference) image, PSF cannot be properly extracted.  {quote}  and:  {quote}  I have noticed that the center of warped image is a gap between CCDs and PSF estimation there will fail. So get PSF without specifying the position is good enough. PSF class will select the best position.  {quote}",0.5
"DM-6661","06/23/2016 20:35:08","ConfigDictField says ""Inequality in keys for..."" even if I give 2 same configurations","From [HSC-1401|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1401]:  {quote}  config.py:  {code:python}  from lsst.meas.photocal.colorterms import ColortermGroupConfig    for key in ['i', 'i2', 'y', 'r', 'N1', 'N2', 'N3', 'z']:      root.calibrate.photocal.colorterms.library[key] = ColortermGroupConfig.fromValues({}){code}    This comamnd line  {code:bash}  rm -fr output ; for i in {1..2} ; do processCcd.py ./HSC --output output -C config.py  ; done  {code}  raises following error  {noformat}  2016-06-01T02:43:45: processCcd FATAL: Comparing configuration: Inequality in keys for calibrate.photocal.colorterms.library: ['z', 'i', 'i2', 'r', 'y', 'N1', 'N2', 'N3'] != ['N3', 'i', 'i2', 'r', 'y', 'N1', 'N2', 'z']  2016-06-01T02:43:45: processCcd FATAL: Failed in task initialization: Config does match existing config on disk for this task; tasks configurations must be consistent within the same output repo (override with --clobber-config)  {noformat}  {quote}",0.5
"DM-6663","06/24/2016 10:47:21","Study iPlant as a potential candidate for workspace implementation","in 2015 iPlant was rebranded to CyVerse  (http://www.cyverse.org/) with a revised mandate to serve all life sciences.  CyVerse's  Data Store is based on technology iRODS (http://irods.org/).   CYVerse's Atmosphere provides a managing portal for users's VM instances and data spaces called volume. The instance and volume is tied under a project.     There are other tools and UIs under CyVerse. Data Store and Atmosphere are very close to what we wanted  for LSST workspace.  It provides many VM instance images preconfigured for life sciences domain-specific tasks. We could definitely learn from this if LSST provides the VM for its users.  LSST could supply instance images with specific astronomical tools preconfigured.      Deciding if the CyVerse could be used as LSST workspace implementation will need much more study and discussion with other teams involved in workspace design and implementation.     The direction we are going with Jupyter Notebook and JupyterHub most likely preclude us from using CyVerse directly.   ",2
"DM-6701","06/24/2016 16:21:39","Specify requirements for SuperTask extensions ","Specify requirements for SuperTask extensions according to the use cases and architecture.",2
"DM-6718","06/27/2016 14:20:07","afw table and record should have useful str() and repr()","To see the contents of an afw table in python, you have to do something like {code}catalog[0].extract(""*""){code} which is totally not discoverable and returns a dict which doesn't print well. Much more useful would be for str() on a record to produce a pretty-printed list of the contents, and str() on a table to produce some nicely formatted summary (like a numpy recarray does, only printing a few things separated ""..."" for large tables).    I'm not sure what the best repr() output would be (certainly for a Record it should be a full dump of the contents), but currently table.repr()==table.str(), which is equally unhelpful.    Maybe this will come ""for free"" when we get astropy.table views, but it makes exploring the contents of a table a pain right now.",2
"DM-6784","06/28/2016 16:40:08","Port meas_extensions_convolved from HSC","HSC has a new measurement extension: meas_extensions_convolved.  This performs aperture photometry with the PSF degraded to nominated seeings (similar to how galaxy photometry is commonly done these days).    Relevant HSC tickets are [HSC-1395|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1395] and [HSC-1408|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1408].",5
"DM-6785","06/28/2016 16:44:59","Port parent/child measurement from HSC","The deblender sometimes gets into trouble with cluster galaxies, and the deblended fluxes aren't accurate.  In that case it helps to have measurements on the image without any deblending having been performed.  This is a feature used in HSC's mid-2016 production run afterburner, ticket [HSC-1400|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1400].  This feature should be ported for use in LSST.",5
"DM-6788","06/29/2016 10:54:31","Document meas_extensions_ngmix","meas_extensions_ngmix has no useful documentation, not even a {{doc}} directory. Add some.    This should include at least an overview of the package contents, a description of its capabilities, and instructions on enabling it within the meas_base framework. The package should have a README.",2
"DM-6823","06/29/2016 15:59:18","Add new boilerplate to introduction of SQR-012","SQR-012 provides details of how the new unittests should be written, but it only gives an example of the old testing boilerplate in the introduction. It would be very helpful to have the first thing a reader sees be the new, correct, boilerplate, so they can immediately drop it into a new testing file.",2
"DM-6824","06/29/2016 18:15:40","Use meas.algorithms.astrometrySourceSelector in measOptimisticB","Now that there is a working astrometrySourceSelector (just merged in meas_algorithms from DM-5933), we should get matchOptimisticB working with it. This would entail replacing matchOptimisticB.SourceInfo with AstrometrySourceSelectorTask and tweaking the latter to do whatever matchOptimisticB needs, and removing SourceInfo.",2
"DM-6831","06/30/2016 13:50:01","Wrap base with pybind11","Split off from DM-6302.",2
"DM-6832","06/30/2016 13:50:50","Wrap utils with pybind11","Split off from DM-6302.",2
"DM-6833","06/30/2016 22:24:37","add 'placeholder' attribute to the input element","An attribute called placholder is available in html element <input> to give a hint to the user of what can be entered. The placeholder text must not contain carriage returns or line-feeds.      Add it as proptype to <inputfield> component.",1
"DM-6857","07/01/2016 11:16:00","Document that the catalog returned from star selectors is a view","Star selectors return a catalog whose records are shallow copies of the input catalog records. Document the shallow copy aspect. This is important for two reasons:  - The user should know  - Implementers must be told this, because if the records are deep copies then the code that sets a flag for stars will not set a flag in the input catalog, which loses most of the point of setting that flag.",0.5
"DM-6858","07/01/2016 12:34:47","Mapper tests require modification when new datasets are added","[~price] [recommends|https://community.lsst.org/t/centrally-defined-butler-datasets/841] a new way to define datasets common to all cameras in daf_butlerUtils, but modifying these yaml files require explicit lists of datasets to be modified in tests/cameraMapper.py.    If these tests are still useful, they need to depend on a minimal set of dataset definitions instead of the real ones.",2
"DM-6886","07/05/2016 04:42:36","forcedPhotCoadd.py fails on CFHT data due to a CModel bug","Hello,    forcedPhotCoadd fails while running on CFHT data due to a CModel bug. Here is an example on the error message that we get:    {code}  python: src/CModel.cc:1368: void lsst::meas::modelfit::CModelAlgorithm::measure(lsst::afw::table::SourceRecord&, const lsst::afw::image::Exposure<float>&, const lsst::afw::table::SourceRecord&) const: Assertion `measRecord.getFootprint()->getArea()' failed.  Aborted  {code}    Adding the following lines in cmodel.py (in CModelForcedPlugin.measure, before the call to self.algorithm.measure) allows to go around the problem for the time being, which seems to arise for null value of the number of pixel in a given footprint:    {code}  if not measRecord.getFootprint().getArea():      raise ValueError(""measRecord.getFootprint().getArea(): 0. No pixel in this footprint."")  {code}",1
"DM-6903","07/06/2016 14:04:58","Add an option to label ccd serial number on the showVisitSkyMap.py plot ","(actual assignee: Samuel Piehl)     Sometimes it is useful to know where the CCDs are on the plot. Add an option to label the CCD numbers. ",0.5
"DM-6905","07/06/2016 16:43:50","Locate the test dataset for PDAC","Locate and evaluate a dataset of SDSS Stripe82 which is going to be used for testing the prototype DAC.",2
"DM-6909","07/06/2016 18:46:11","Filtering from expanded mode cancels expanded mode","When a table is filtered from the expanded mode, the layout is changed back to unexpanded.    It looks like the issue is more general: table actions trigger layout changes, which are not always right. For example, TABLE_REMOVE action while in a dropdown makes the  dropdown to get closed. I've traced it to FireflyLayoutManager.js:layoutManager generator function.    Test sequence in firefly:   - When a table is loaded, open ""Charts"" dropdown, select Col link for X, then select Col link for Y. (At this point the previous table is removed).  - TABLE_REMOVE action on the second click triggers dropdown to go away.   ",2
"DM-6910","07/06/2016 18:50:26","Tab panel issues","There are several issues with tab titles:  - Sometimes we don't want them to be resized at all (like in Images panel since the tabs are all fixed at begining)  - When titles are resized, their size is proportional to the length of the title, use fixed-size tab length instead to make sure no tab is too long.    Please add other tab panel issues here.",3
"DM-6914","07/07/2016 10:58:41","git-lfs.lsst.codes certificate is expired","Per reports on hipchat, the tls certifcate on git-lfs.lsst.codes was not upgraded to the new *.lsst.codes cert.    {code:java}  John Swinbank  9:52 AM  @josh @jmatt I'm seeing the following, which I think might be the same as @srp's error above. Any ideas?  Get https://git-lfs.lsst.codes/objects/24874b686b9479a823987dc2bd2700cad5b73e74a43108fb61b91d7f79f0cd99: x509: certificate has expired or is not yet valid  Followed by git lfs failing.  (I assumed it was user error on my part at first, but if so it's coincidence that Steve's git lfs fails at the same time.)  {code}    ",1
"DM-6916","07/07/2016 12:53:17","Documenteer seeds Git revision date and branch name if not present in metadata.yaml","If {{last_revised}} and {{version}} are not present in metadata.yaml, then the Git commit date and branch name should be used while building metadata instead.    Also updates lsst-technote-bootstrap to take advantage of automated metadata for new projects.",0.5
"DM-6922","07/08/2016 02:34:27","Upgrade to new stack install procedure for containers","LSST stack install has evolved: https://pipelines.lsst.io/install/newinstall.html#  Release container creation script needs to be update.  Latest Docker version will be tested, as [~bvan] reported cmd line options have changed.",2
"DM-6923","07/08/2016 08:11:40","Apply distortion when searching for astrometric reference objects","While investigating DM-6529 I found that LSST generally finds fewer reference objects than HSC when doing astrometry.  For the CCDs on the edge of the focal plane the number of stars was typically very low causing frequent failures.  I found that in the HSC code, there is a distortion being applied that shifts the exposure bounding box when getting objects from the reference catalog.  This distortion is not being applied in the LSST code.",1
"DM-6929","07/12/2016 07:35:47","Image Viewer: Support circular selection","Support circular selection by choosing a point and dragging the mouse across the image. The callbacks and functions supported for rectangular selection should be also supported for circular selection.    (Circular region selection is a part of Paul O'Connor's visualization wish list and is a long promised feature.)         Copied from GitHub(XW 1/18/18)    This development includes,   * adding circular selection support for area selection.   * change select area icon on the tool bar from on/off button to a on/off button with dropdown selection at off state. The dropdown list supports the selection for 'rectangular selection', 'elliptical section' (circular area).   * add stats calculation for circular area selection on both rotated and non-rotated images.   * fix stats calculation for rectangular area selection on rotated image case.   * update 'select catalog' and 'filter catalog' functions for circular area selection.    Test:   - Do image selection:   1. Click 'select area' icon (on 'off' state)  to get selection dropdown list,   2. Select either 'rectangular selection' or 'elliptical selection' (this will move the 'select area' icon into 'on' state)  and draw a selected area by choosing a point over the image and dragging the mouse across the image.              3. Click  'select  area' icon to move it back to 'off' state to remove the selected area.             4. Select one of the extension buttons for 'crop', 'stats', 'zoom', or 'recenter' while either 'rectangular  selection' or 'elliptical selection' is selected.   - Repeat the steps by rotating the image first.     - Do catalog selection next:   repeat the 'select area' steps done as image selection, and select one of the extension buttons for 'crop', 'stats', 'select mark data', 'filter', 'zoom' or 'recenter' while either 'rectangular selection' or 'elliptical selection' is selected.",8
"DM-6945","07/13/2016 15:24:12","Add text to algorithmic components sections in LDM-151","While [~swinbank] has commented that the outlines are probably good enough for planning work (and I thnk that's broadly true), the lack of text in the algorithmic components section did occasionally lead to some misunderstandings in [~rhl]'s first review pass, so I think I should flesh that out with text sooner rather than later.    In this issue, I'll stick to sections that no one else has added text for, but eventually I'll also need to work with [~krughoff] and perhaps others to ensure that section has a consistent level of detail and focus.",8
"DM-6949","07/13/2016 18:22:17","Firefly has problem to render in other browsers than Chrome","Couple of problem using Firefly in  Safari:  * the components appears blank,    in Firefox:   * image and xyplot are not aligned (Gator).    The alignment can be reproduced in my Chrome and Safari.  Search parameters: ALLWISE source catalog, m81 100arcsec.   ",1
"DM-6968","07/15/2016 13:43:33","create a shared stack on NFS for use with  the current local condor pool","It is well known that building, setting up a stack, and interactive devel work with those operations on NFS has performance issues.  Hence the official shared stack on lsstdev uses /ssd .    However,  a shared stack on NFS is useful and adequate for one important  use case --   users need a stack that can be used for small productions on the local condor pool currently available  on lsstdev.   For this use case multiple ""source""/""setups"" on a node/against the file system  can be avoidable by using a script to directly declare the environment.  run_orca /ctrl_orca supports this feature.       While GPFS is coming soon, there is expected to be a transition period of 2-3 months and so the NFS file system and a stack on it can serve users for an interim period.   If building a shared stack on NFS is not a heavy labor, we think it is worth the effort for this interim period, and as such make this request for a shared stack on NFS. ",1
"DM-6969","07/15/2016 16:44:31","Fixes to LoadIndexedReferenceObjects","Bug fixes for using the new {{LoadIndexedReferenceObjectTask}} and its associated components.",2
"DM-6972","07/18/2016 15:43:40","Fix Qserv install doc and scripts for new newinstall.sh","Update qserv install docs per new info at https://pipelines.lsst.io/install/newinstall.html",1
"DM-6974","07/18/2016 17:33:01","Type of IngestIndexedReferenceTask_config wrong in obs_ paf files","In DM-6651 I moved the new HTM indexed reference catalog code from pipe_tasks to meas_algorithms, but didn't do a complete job. The type of IngestIndexedReferenceTask_config in obs_ paf files still must be updated.",1
"DM-6976","07/19/2016 12:09:55","watch for Highcharts update ","There is an issue in the density plot for displaying the legends. Highcharts does not support the setting of the symbol size in the legends. So when the symbol size is too small or too large, the legends are not displayed.     We don't want to do too much workaround currently. This ticket is to watch for the Highcharts update. ",1
"DM-6978","07/19/2016 12:22:42","Update qserv for changes in Log interface","DM-6521 improved Log class interface by replacing some static methods with non-static. Qserv is currently using couple of static methods which were retained in Log class for the duration of this migration. Once updated log package is released update qserv code to use new non-static methods and remove static methods from Log class after that.",2
"DM-6979","07/19/2016 12:26:38","Firefly strategy response to RFC-193 (f17)","RFC-193 has been adopted.   ""we propose to replace the current World Coordinate System handling and XYTransform code in the LSST stack with Starlink AST, using a new API that is under development in DM-3874 (that API will have its own RFC). ""     How does Firefly respond to this and display images as accurately as possible?   ",20
"DM-6981","07/19/2016 18:04:11","Add column setters for Flag types in catalogs","It should be possible to set Flag columns with bool arrays or scalar values.    This should just be a matter of adding a {{set}} overload or two around line 100 of {{specializations.i}}.",1
"DM-6982","07/20/2016 09:58:59","Fix oversampling settings in psfex","The current settings in psfex will only turn on oversampling only if the seeing is < 0.5"", even if you have configured it do oversampling. This needs to be changed so that everything is determined by the config parameters.    We have also seen on HSC data that oversampling in general does not work well in psfex.  We need to change the current configuration which does 2x oversampling to just use the native pixel scale.",1
"DM-6983","07/20/2016 10:09:11","ci_hsc failure: AttributeError: 'Butler' object has no attribute 'repository'","Following [~npease]'s [recent changes to the Butler|https://community.lsst.org/t/im-checking-in-butler-changes-related-to-rfc-184/959], ci_hsc is failing as follows:    {code}  [2016-07-20T07:57:31.954576Z] Traceback (most recent call last):  [2016-07-20T07:57:31.954643Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/bin/validate.py"", line 3, in <module>  [2016-07-20T07:57:31.954664Z]     main()  [2016-07-20T07:57:31.954732Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 53, in main  [2016-07-20T07:57:31.954756Z]     validator.run(dataId)  [2016-07-20T07:57:31.954825Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 155, in run  [2016-07-20T07:57:31.954851Z]     self.validateDataset(dataId, ds)  [2016-07-20T07:57:31.954923Z]   File ""/home/jenkins-slave/workspace/stack-os-matrix/compiler/gcc/label/centos-7/python/py2/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 117, in validateDataset  [2016-07-20T07:57:31.954956Z]     mappers = self.butler.repository.mappers()  [2016-07-20T07:57:31.954991Z] AttributeError: 'Butler' object has no attribute 'repository'  [2016-07-20T07:57:32.023212Z] scons: *** [.scons/ingestValidation-903342-100] Error 1  {code}    See e.g. https://ci.lsst.codes/job/stack-os-matrix/13274/compiler=gcc,label=centos-7,python=py2/console.    Please fix it. ",1
"DM-6984","07/20/2016 13:49:41","Suggest logging migration in daf_persistence and daf_butlerUtils","Use lsst::log instead of pex::logging in daf_persistence and daf_butlerUtils",2
"DM-6987","07/20/2016 16:45:20","Write up a description of Composite Datasets based on input from KT","Write a description of composite datasets as I understand them based on the email KT sent on May 20 (attached), and on conversation I had with KT and Fritz on July 20.",2
"DM-6989","07/20/2016 16:59:52","ctrl_events/tests/EventAppenderTest.py fails Jenkins run-rebuild","ctrl_events/tests/EventAppenderTest.py started failing on Jenkins ""run-rebuild"" last night:   https://ci.lsst.codes/job/run-rebuild/354//console    All test cases in EventAppenderTest.py did run and pass, but it failed with a Segmentation fault in the end.     Jenkins ""run-rebuild"" uses a stack on NFS on lsst-dev (/nfs/home/lsstsw).  The same test passes on regular Jenkins (stack-os-matrix).      ",3
"DM-6996","07/20/2016 21:51:35","produce a draft document of SUIT requirements","After combing through the current SUIT requirements, we feel that we need to re-organize and re-write the SUIT requirements to be in-line with SUIT vision document.    This story will be producing the first draft of the rewrite. ",20
"DM-6998","07/21/2016 07:07:48","Problems with MemoryTest ordering","{{MemoryTestCase}} (or a derivative thereof) must be run as the last of all tests in a module in order to properly catch leaks.    [Our documentation|https://developer.lsst.io/coding/python_testing.html#memory-and-file-descriptor-leak-testing] implies, and [SQR-012 states|https://sqr-012.lsst.io/#memory-test], that this can be achieved by listing it as the last test case in the file.    This works for py.test, but not when using plain old unittest: the latter does not, so far as I can see, guarantee any sort of ordering as a matter of principle, and, in practice, it sorts things lexicographically (it uses whatever order it gets from running {{dir()}} on the test module, and I don't *think* that's guaranteed to be anything in particular).    For example, consider [{{testAstrometrySourceSelector.py}}|https://github.com/lsst/meas_algorithms/blob/master/tests/testAstrometrySourceSelector.py]. I made the following change to introduce a memory leak:    {code}  --- a/tests/testAstrometrySourceSelector.py  +++ b/tests/testAstrometrySourceSelector.py  @@ -70,8 +70,9 @@ class TestAstrometrySourceSelector(lsst.utils.tests.TestCase):           self.sourceSelector = sourceSelector.sourceSelectorRegistry['astrometry']()         def tearDown(self):  -        del self.src  -        del self.sourceSelector  +        pass  +        #del self.src  +        #del self.sourceSelector         def testSelectSources_good(self):           for i in range(5):  {code}    Py.test catches it:  {code}  $ py.test-2.7 testAstrometrySourceSelector.py  [...]  testAstrometrySourceSelector.py .........F  [...]  {code}    But simply running the test suite does not:  {code}  $ python testAstrometrySourceSelector.py  ..........  ----------------------------------------------------------------------  Ran 10 tests in 0.105s  {code}    Rename the test case:  {code}  @@ -144,7 +145,7 @@ def setup_module(module):       lsst.utils.tests.init()      -class MyMemoryTestCase(lsst.utils.tests.MemoryTestCase):  +class xMyMemoryTestCase(lsst.utils.tests.MemoryTestCase):       pass     if __name__ == ""__main__"":  {code}    And boom:  {code}  $ python testAstrometrySourceSelector.py  .........  54 Objects leaked:  {code}    Based on a very quick check, I think [sconsUtils runs tests by simply invoking {{python}}|https://github.com/lsst/sconsUtils/blob/f9763768d999cefa4c26b9f3418c28394dfb38df/python/lsst/sconsUtils/tests.py#L133], and I'm pretty sure that this is hard-wired into the muscle memory of many developers. In these cases, memory tests written following current guidelines won't be being properly executed.    ",3
"DM-6999","07/21/2016 11:26:29","Use lsst::log in pipe_base and pipe_tasks","Per RFC-203, switch Task's logs from using pex.logging to lsst.log in pipe_base and pipe_tasks (stage 2)  This implements the main idea of RFC-203 and migrate the logging framework used in the task framework.",8
"DM-7008","07/22/2016 16:38:39","Check boost.python building with Python 3","We may want to disable boost.python in the build. There are hints that there are problems with python3.5.",0.5
"DM-7010","07/25/2016 08:46:47","Builds should be optimised by default","By default, our builds are not optimised ({{-O0}}), which requires everyone who doesn't want to wait until the heat death of the universe to set {{SCONSFLAGS=""opt=3""}}, but other packages that are built with scons may not recognise this.  This default is also contrary to the standard practise for open-source software, which is that by default builds are optimised.  I will change the default optimisation level to {{opt=3}} from the current {{opt=0}}.  I will also add support for {{-Og}}.    This change was approved in RFC-202.",0.5
"DM-7014","07/25/2016 12:45:42","Memory cache leak in firefly server","The visualization system is not update the memory accounting for the caching system.",2
"DM-7016","07/25/2016 12:54:47","Big image not showing working message when the load","This is a problem with uploads, large image loads, and Atlas.   When a big image is loading the user does  not get feedback.  The problem is the the UI is not creating the ImageViewer soon enough.",2
"DM-7018","07/25/2016 16:08:22","Firefly distribution build","We need to support regular Firefly distribution builds (with bundled tomcat server),  similar to the builds we did in lsst firefly repository before the conversion.    This is to get Camera team started with new API.",2
"DM-7019","07/25/2016 16:24:06","Setup standalone Firefly build using IPAC github","Modify the existing Firefly-Standalone build in Jenkins to use IPAC's github.  Make sure github auto-releases still works.",3
"DM-7021","07/26/2016 08:53:02","Update pex_exceptions to support Python 3","{{pex_exceptions}} needs to be updated to support Python 3.",1
"DM-7027","07/26/2016 13:28:19","Region issues","There are several region issues I have found when comparing old and new API:  (1, 2, 3, 7, 8 are implemented in DM-7147, 4, 5, 6 are implemented in DM-7190)    1. The default coordinate system when ""pure numbers"" are used should be pixel positions on the original image.     The [spec](http://ds9.si.edu/doc/ref/region.html) says the following:   ""...the default system is implicitly assumed to be PHYSICAL. In practice this means that for IMAGE and PHYSICAL systems, pure numbers are pixels.""  'If no coordinate system is specified, PHYSICAL is assumed.'    More explanation can be found [here](http://hea-www.harvard.edu/RD/funtools/regcoords.html)    2. PHYSICAL coordinate system does not mean screen pixels. The question is whether we can always use image pixels.    _If wcs is ""physical"", WCS is the pixel coordinate system of the original image, which may be different from the pixel coordinate system of the current image, if the current image is the result of an imcopy or other geometric transformation operation. In the ""physical"" coordinate system the ltv, ltm and the axis attribute parameters wtype, axtype, units, label, and format may be edited, but the FITS parameters crval, crpix, and cd cannot_. [reference](http://stsdas.stsci.edu/cgi-bin/gethelp.cgi?wcsedit)    3. When used in API, the actions are dispatched one after another. For example, dispatchCreateRegionLayer might be issued before image plot has finished loading. Can we make regionCreateLayerActionCreator wait for image with plotId to finish loading?     More details: If I create a region layer with a few regions in image coordinates right after firefly.showImage, two errors are loaded to console:   doOnAppReady: uncaught TypeError: Cannot read property 'getImageCoords' of null(…),   RegionFactory:602: Uncaught (in promise) TypeError: Cannot read property 'getImageCoords' of null(…)    4. When a region is selected, yellow dashed line appears around it. On zoom the line does not change color.    5. Can not add empty region. Line 124 of the test script.    6. Can not add or delete regions after the first one. (Load test script, click Show Regions, then Add Region, line 121 of the test script) When there is one region in the layer and you are adding another region with a different position but everything else the same, the added region is perceived to be the same.    7. It's possible to select regions only in in the first region layer added. In the other layer, you can sometimes see the selection blinking, but it does not stay.    8. If region format is wrong, for example  'image; box 40 400 72 72 # color=blue' instead of  'image; box 40 400 72 72 0 # color=blue',   the region is silently ignored, no warning or error is logged to console.    9. We need region selection action to support callbacks on region selection. (Currently the selected region is ""calculated internally based on the distance between the region and  mouse readout.)",1
"DM-7028","07/26/2016 14:22:55","Port daf_base to Python 3","Changes necessary to get daf_base to work with Python 3.",0.5
"DM-7031","07/27/2016 06:22:33","Assign initial responsibilities in LDM-151","Assign first thoughts on responsibilities to all software primitives and algorithmic components. This is my take. Simon will have his own take.",2
"DM-7039","07/27/2016 09:58:25","Familiarization with Footprint redesign","Familiarize yourself with the RFC-37 driven Footprint redesign. Start thinking about ideas for how you could implement it and what the transition plan from the current Footprints might be.    A great outcome would be to propose a set of stories which would tackle the new Footprint development effort.    A good outcome would not be to have the stories ready to go, but to be well prepared for a discussion with [~jbosch] & [~swinbank] where we'll come up with some stories as a group.",5
"DM-7040","07/27/2016 10:30:59","Stars selected by starSelector change when number of cores varies","Sogo Mineo writes in [HSC-1414|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1414]:  {quote}  See the following lines:    meas_algorithms/HSC-4.0.0/python/lsst/meas/algorithms/objectSizeStarSelector.py:466  in ObjectSizeStarSelector.selectStars():  {code}      if psfCandidate.getWidth() == 0:          psfCandidate.setBorderWidth(self._borderWidth)          psfCandidate.setWidth(self._kernelSize + 2*self._borderWidth)          psfCandidate.setHeight(self._kernelSize + 2*self._borderWidth)  {code}    In reduceFrames, these lines set the width of psfCandidate to be 21  for the first time the execution reaches there.    When the first CCD image has been processed, the worker process  continues to process another CCD image, and the execution reaches  here again.  This time, psfCandidate.getWidth() is 41, because  psfexPsfDeterminer has set it to be 41, and the value has been  retained because the width is a static member.  And so, for the second  CCD image, the width of psfCandidate is not 21 but 41.    Since psfCandidates are widened, stars positioned at edges of images  are rejected.  It results in a smaller number of PSF candidates than expected.    Only CCD images that are initially given to the worker processes  are processed with psfCandidate.getWidth() == 21. The other CCD images are  processed with psfCandidate.getWidth() == 41.  When the number of SMP cores changes, CCD images are processed with different  parameters.    The change in the number of PSF candidates results in different Psf, a different  result of image repair, and different catalogs.  {quote}    The line numbers are different on the LSST side because of refactoring (objectSizeStarSelector.py:466 has moved to starSelector.py:148), but the bug is still present.  The main problem appears to be that the {{PsfCandidate}} elements are {{static}}, are being set in both the star selector and the PSF determiner and one of those is conditional on what the value is.  I will investigate moving the {{static}} class variables to instance variables --- the desired size appears to vary by context, so it shouldn't be a class variable.",2
"DM-7043","07/27/2016 10:51:44","Update SQuaSH database model and JSON API with concepts from validate_drp measurement API","In DM-6629, a new measurement API was introduced into validate_drp that established a JSON data model for metrics, specifications of metrics, measurements, and general blob datasets. The intent of that work is to enable rich plots in the SQUASH dashboard, with access to data behind measurements. The new data model also clarifies the subtleties of metric specifications (filter dependence, and dependence on other specifications). This ticket will incorporate validate_drp’s new data model into the SQUASH database and API.    Also related to DM-7041, which will update the post-qa tool that submits validate_drp json to the SQUASH API.",5
"DM-7044","07/27/2016 14:12:29","Additional constraints on reference band selection for multiband","Reference band selection currently depends on the configured band priority order, with exceptions made for sources with low signal-to-noise in the high priority bands.  [HSC-1411|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1411] points out that some additional qualifications, such as success for major measurements (e.g., CModel and Kron), would be helpful.",3
"DM-7047","07/27/2016 16:40:30","Port pex_config to Python 3","Work involved in ensuring that pex_config passes all tests on Python 3 and legacy Python.",1
"DM-7048","07/27/2016 17:28:31","validate_drp is failing because it's accessing butler internals that have changed","need to change obs_decam's ingest task to use the newer class hierarchy to get the root of the butler's single repository. (longer term there should be a butler API for this or the task should get the value of root from somewhere else)",1
"DM-7050","07/28/2016 11:59:41","LTD Keeper: Use Google Cloud Platform SQL","Currently LTD Keeper uses a sqlite DB. This ticket will migrate that DB to Google’s Cloud Platform’s managed SQL. This solution provides automatic backups, and provides flexibility to run multiple ltd-keeper pods. Google’s SQL makes sense since LTD Keeper is run on Google Cloud Platform.",5
"DM-7058","07/28/2016 15:20:50","Generate useful plots of jointcal results (I)","JointcalTestBase currently generates a few different plots (old vs. new WCS quiver and heat maps, relative and absolute RMS histogram) to summarize the fitting results. After showing these to people and some discussion, we came up with a number of other plots that would be very useful to help understand the fitted WCS that jointcal produces. Since figuring out how to produce some of these plots may be tricky, or may be best done after the new WCS system is in place, I'm making separate stories for them.    It also would be good to have an external plotting framework that lives outside the tests.",20
"DM-7066","07/29/2016 15:10:58","Port pex_logging to Python 3","Work required to get pex_logging working on python 3. Will also include some package cleanups.",1
"DM-7067","07/29/2016 15:57:41","Break joincal's link to upstream lsst_france repo","lsst/jointcal is still linked to the upstream repo at lsst_france. I believe all the relevant changes have been ported. It's time to break that upstream link, so that pull requests can be made in a more obvious fashion.",0.5
"DM-7069","07/29/2016 16:52:10","Port daf_persistence to Python 3","Work relating to getting daf_persistence to run on python 3. Includes some code modernization.",1
"DM-7070","07/29/2016 17:45:18","Move consts from top of Associations.cc into JointcalConfig","There are three values at the top of Associations.cc under a TODO comment that should be lifted up into JointcalConfig so they can be configured at runtime. It would be good to try to add tests to check different values for them (and possibly just remove usnoMatchCut).",1
"DM-7080","08/01/2016 06:34:02","Doxygen isn't updating","The current build of our Doxygen documentation, as displayed at https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/index.html, is labelled ""Generated on Mon Jun 27 2016 03:52:22 for LSSTApplications"". At time of writing, that's more than a month ago. Important additions to the documentations made during the last month are missing.  ",0.5
"DM-7090","08/01/2016 19:46:36","IrsaViewer catalog panel, labels and input fields moved as you type","Catalog search panel in IrsaViewer, the target panel label, feedback, and input box are jumping as input is being typed.  Their position should be fixed.",2
"DM-7091","08/01/2016 20:16:44","F16 Qserv Release Mgmt","Developer work to support the monthly and end-of-cycle qserv releases.  Includes compiling release notes, updating package dependencies, updating installation docs, minor fixes in support of new compilers, etc.",20
"DM-7093","08/02/2016 12:11:59","Remove unused inPlace argument from PropertyList","In DM-1972 [~ktl] says:  {quote}  It looks like we don't actually use the inPlace argument to the PropertyList add and set methods anywhere in the stack besides the daf_base unit tests. I think it was originally put in to accommodate the addition of COMMENT and HISTORY items that might want to be placed at the end of the PropertyList instead of grouped together, but the implementation actually still groups all of these items together, just at the end of the list instead of where they were. If this is not actually useful, the argument could be removed and SWIG presumably appeased.  {quote}  This removal is no longer needed to appease SWIG but the code can usefully be removed and a test branch for this was created as part of DM-1972. This ticket will be used for the actual removal.  ",0.5
"DM-7095","08/02/2016 15:17:38","Pilot Sphinx/Breath/Doxygen-generated API docs for daf_base","Since daf_base is low in the dependency tree and dominated by C++ APIs, it is an excellent candidate for exploring Sphinx-based package documenation for C++ with breathe and doxygen. We will also see how astropy’s automodsumm sphinx extension works with SWIG’d Python APIs.    This prototype will build upon the system developed in DM-7094.    Lessons from this prototype will be fed into a ""Pipelines Documentation Implementation Planning"" tech note in a follow-up ticket.",3
"DM-7104","08/02/2016 19:34:15","support PDAC Qserv deploy","Support John in adapting scripts and methodology as necessary to support qserv deploy on the PDAC cluster at NCSA, as is currently done at IN2P3.  ",8
"DM-7106","08/02/2016 19:41:58","PDAC Qserv Deploy","Configure cluster and adapt scripts and methodology as necessary to support qserv deploy on the PDAC cluster at NCSA, as is currently done at IN2P3.  ",13
"DM-7114","08/03/2016 10:25:58","Guard against assertion failure","measureCoaddSources can hit an assertion failure in GalSim:  {code}  python: src/hsm/PSFCorr.cpp:731: void galsim::hsm::find_ellipmom_1(galsim::ConstImageView<double>, double, double, double, double, double, double&, double&, double&, double&, double&, double&, double&, boost::shared_ptr<galsim::hsm::HSMParams>): Assertion `iy1 <= iy2' failed.  {code}    This is due to GalSim using asserts rather than throwing an exception.  We fixed this on the HSC side ({{7a73869}}), but it didn't come over because we're using GalSim as a dependency rather than sucking it into the package.    I thought I reported this upstream at one point...",0.5
"DM-7116","08/03/2016 12:43:36","Drawing layers dialog title is not updated with context image mission","Before migration, layers dialog title included mission name such as for example:  ""Layers- IRIS: 25"".    Now the dialog title is a fixed string:   ""Drawing Layers"".    Make the title aware of the active image with the mission name since we know about it. ",2
"DM-7117","08/03/2016 13:55:30","measureCoaddSources fails with ""RuntimeError: Unable to match sources""","Running measureCoaddSources.py, I'm getting a {{RuntimeError: Unable to match sources}}.  The patch I'm running on doesn't have a lot of pixels illuminated, so it's not surprising that there would be no matches, but that fact shouldn't cause the operation to fail.  The behaviour of the matcher has changed in this respect: before the refactoring of astrometry, {{AstrometryTask.useKnownWcs}} would not raise an exception, but log an error and return an empty list of matches (it successfully matched zero sources, which might be cause for concern).    I think we should fix the matcher to return an empty list (as it did previously), but perhaps a case can be made that measureCoaddSources should catch the exception and continue.  I think that's wrong because measureCoaddSources would have {{matches = None}} rather than an empty list, which communicates something different (""I have no knowledge of matches"" vs ""I tried to match and there's nothing""); and measureCoaddSources shouldn't simply set {{matches = \[\]}} because it's not its responsibility to guess a type for what its subtask returns (breaks encapsulation).",1
"DM-7119","08/03/2016 14:19:53","revise the Firefly README file","Issues to think/discuss (in README)  # mention of Firefly Tools  # mention of viewer, plot  (for image display)  # testing data location     ",1
"DM-7134","08/04/2016 13:47:25","singleFrameDriver is only running with a single process","singleFrameDriver.py is only using a single process.  The problem appears to be the change to use a {{ButlerInitializedTaskRunner}}, which doesn't inherit from {{BatchTaskRunner}}.",0.5
"DM-7137","08/04/2016 15:13:25","utils raDecStr delimiters are untested and don't work","In working on DM-6320 I noticed that there do not seem to be any tests for the {{decStrToDeg}} and related functions. When I added the following test:  {code:python}     def testDecStrToDegDelim(self):          deg = lsstutils.decStrToDeg(""+15_00_00.00"", ""_"")          self.assertAlmostEqual(deg, 15.0, 6)  {code}  it failed with:  {code}  Failed to parse +15_00_00.00 as a declination with delimiter '_' and regex '([\d]+)_(\d+)_([\d\.]+)'  {code}  (I added some extra information to the exception).    Furthermore, {{decStrToRad}} (and the equivalent RA version) do not pass through the delimiter at all:  {code:C++}  double ut::decStrToRad(std::string decStr, std::string delimiter) {      return degToRad( decStrToDeg(decStr) );  }  {code}  Please add some tests. Alternatively, we could remove the delimiter option completely.    It might also be worth removing the code duplication in the RA and Dec variants.",2
"DM-7138","08/04/2016 20:32:09","Port ndarray to Python 3","I just tried to build LSST ndarray on Python 3 and it does not work. It's all going wrong as {{include/ndarray/swig}} seems to not be compiling.  {code}  In file included from tests/ndarray-python-mod.cc:25:  In file included from include/ndarray/swig.h:46:  In file included from include/ndarray/swig/numpy.h:33:  include/ndarray/swig/PyConverter.h:175:25: error: use of undeclared identifier 'PyString_AsString'              char * cs = PyString_AsString(s.get());                          ^  {code}  That API changed in Python 3 and it probably should be {{PyBytes_AsString}}.    http://stackoverflow.com/questions/22487780/what-do-i-use-instead-of-pystring-asstring-when-loading-a-python-module-in-3-3#22491037    ",0.5
"DM-7150","08/06/2016 15:32:29","Configure a default log for tests ","The default configurations such as format and levels are different between {{lsst.pex.logging}} and {{lsst.log}}. With DM-6999 logs of command line tasks are changed from {{lsst.pex.logging}} to {{lsst.log}}.  For running command line tasks, logs are configured in {{pipe_base}}.      Based on RFC-203, another main use case is debug logging by running unit tests. In this case, log are not (and cannot be) configured in {{pipe_base}} command line interface.  This ticket adds a default configuration in {{lsst.utils.tests}} so the default logs will look more similar to what they have been with pex.logging when running unit tests. Developers can put customized configuration in each unit test (after DM-6999).   ",1
"DM-7152","08/07/2016 15:15:17","Port afw to Python 3","Work required to allow AFW to work on Python 3.",1
"DM-7156","08/08/2016 15:26:09","Linter errors in daf_persistence","Running {{flake8}} on {{daf_persistence}} as part of DM-7069 I find two real errors.    # in {{butler.py}} the {{key}} argument was removed from {{queryMetadata}} in commit 3a169f but it is still documented as being there and is still used inside the function.  # {{mapper.py}} refers to class {{RepositoryMapperCfg}} but that class is not seemingly defined anywhere.  ",1
"DM-7157","08/08/2016 15:30:09","Test order in daf_persistence can cause some tests to skip","If all tests are run from py.test as:  {code}  py.test tests/*.py  {code}  The {{DbStorage_?.py}} tests skip. If they are run without {{DbAuth.py}} running they run and pass. Something in {{DbAuth.py}} is causing the availability test to fail.",2
"DM-7159","08/08/2016 17:41:40","Create a Light-curve viewer prototype to display time dependent dataset and compute periodogram","The prototype could be considered as a sort of IRSAViewer for handling LC objects / Time dependent dataset, with extra feature that would enable the existing relationship between catalog and image, in particular for WISE, PTF, which could be a reference to single-epoch image (IBE referenced URL typically).    For each observation a corresponding image should be retrieved and placed in a grid (image vs time) either in the same LC object (table) or separately.   One could expect that any LC data-point (time point) would be pointing to an image (single-epoch, exposure) taken from a dataset (image axis would be a 3rd dimension or a 3rd view of LC).  LC should display LC object as following views:  * X-Y plot (typically flux vs time)  * Table  * Images (corresponding image dataset single exposure at a particular time)  The high-level requirements are:  # Easily plot light curve  # Compute Period using Lomb-Scargle   # Display folded Light Curve and Periodogram   # Link Light Curve Points with Single-Epoch Images  # Download Cutouts and Light Curves    In order to create the Light curve viewer prototype, we need to *create a skeleton app to start adding the tri-views* (Upper: plot, Bottom left: table, bottom-right: grid image): (DM-7406)  * Should handle single (one position) LC (table & plot flux or magnitud vs. time, tipically 'mjd')  ** For the prototype it will be a fixed table given  * Image of the single exposure to be displayed at least single page mode  * Should display input for computing a periodogram (the result of an API call)  ** Power spectrum (Power vs log(period) ) as table and xyplot  ** Peaks as a table (Power vs. period)  ** Period (first Period in peak table) as a field (editable), tipically in days.    In order to connect table/plot to single exposure image, be able to compute periodogram and plot based on the table focused, we need a 2 new UI components, 3 new searches and 2 new controllers (tickets will be created separately and added here):      1. UI components:    * To handle the algorithm input parameters to compute ‘periodogram’  (DM-7160)  * To handle the period input (DM-7161)    2. Processors:    whether it is in one search processor or in different one, we need to have the following task processor:    * One to deal with API to compute periodogram that will result at least in 2 tables (periodogram and peaks table, + extra output parameters fi needed) (DM-7162)  * one for getting the image cutout (single exposure) (DM-7164)  * one to build the phase folded curve based on (DM-7165)  ** Original table searched  ** Period whether it comes from the API result or any other changes from user afterward    3. Controllers:  * For image to change on row or xy plot curve clicked (DM-7166)  * For plotting chart based on the type of the table (power vs period, or LC or phase folded) (DM-7167)  ",8
"DM-7166","08/08/2016 18:34:25","Add new controller to react when table/xy-plot is clicked to display the observed image","Create new controller to deal with actions on LC or periodogram or phase folded curve table and xy-plot:  * on row changed,  * on plot highlight.    On both actions, as time changed, the image should be updated in the image grid view with the new single exposure from the dedicated search processor (see DM-7164)    Note: Take into consideration that a user could select a different flux column     Fixed in DM-7167.",0
"DM-7168","08/09/2016 06:13:05","Fix Qserv CI build w.r.t xrootd upgrade","Qserv can't build using xrootd version tagged ""qserv-dev"":    {code}  Compiling shared object build/qdisp/QueryResource.os  In file included from core/modules/qdisp/QueryResource.cc:42:0:  core/modules/qdisp/QueryRequest.h:110:20: error: 'PRD_Xeq' in 'class XrdSsiRequest' does not name a type       XrdSsiRequest::PRD_Xeq ProcessResponseData(char *buff, int blen, bool last) override;                      ^  scons: *** [build/qdisp/QueryResource.os] Error 1  {code}",2
"DM-7170","08/09/2016 07:52:59","Implement Interface for SpanSets","Building off the work done in RFC-37, design a interface for a collection of sets to be called SpanSet. This should encompass most of what currently exists in afw footprints, but contain more topological set functionality.",3
"DM-7171","08/09/2016 08:20:46","Document interface for SpanSets class","Write up a brief technote describing the new SpanSet interface, both things such as functions etc, and descriptions about how it is expected to be used in the future, and made available through python. This will give others a chance to express their needs opinions for span sets while the development is still ongoing",5
"DM-7172","08/09/2016 08:26:04","Implement SpanSet overlap tests","Implement testing for overlapping spans in SpanSets",3
"DM-7177","08/09/2016 08:42:50","Document interface for new Footprints class","Create a technote and a community posting on the new Footprints class to both document the new api, and explain the differences with the old class.",8
"DM-7179","08/09/2016 12:51:31","sconsUtils reads SConscript files in alphabetical order","I have made the deeply embarrassing discovery that I knowingly made sconsUtils' reading of SConstruct files discovered in subdirectories ordered alphabetically, and considered this okay because ""include < lib < python < test"".  This needs to be made user-configurable, probably via a keyword argument to {{sconsUtils.scripts.BasicSConstruct}}.",1
"DM-7180","08/09/2016 13:15:53","Port HSC aperture correction fix","Port [HSC-1416|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1416] to prevent {{ZeroDivisionError}} in {{MeasureApCorrTask}}.",1
"DM-7182","08/09/2016 15:18:29","Discuss L1 database requirements (continuing)","There are few things that are not yet very well understood in the L1 database access from AP. I need to meet with pipeline people at LSST2016 and try to clarify all of them The list of questions will be added below.",8
"DM-7190","08/10/2016 12:31:31","Fix region bugs related to zoom selected region, add empty region layer and add/remove more regions","This one is made to include some issues report in DM-7027 (issues 4, 5, 6):    4. When a region is selected, yellow dashed line appears around it. On zoom the line does not change color.  5. Can not add empty region. Line 124 of the test script.  6. Can not add or delete regions after the first one. (Load test script, click Show Regions, then Add Region, line 121 of the test script) When there is one region in the layer and you are adding another region with a different position but everything else the same, the added region is perceived to be the same.",2
"DM-7192","08/10/2016 14:07:58","replaced image of the coverage is gone after expand mode","do a catalog search, then do an image search to replace coverage image by another image (say WISE), then go in expand mode and back, the image is replaced back to the initial coverage image",2
"DM-7195","08/10/2016 19:41:45","Add a download feature to get the single exposure cutout from a LC","The LC viewer need an extra download button to get all the cutout from the light-curve table.  The output can be a tar/zip file, or it can be also the wget (curl) scripts to download them if too many images.",2
"DM-7196","08/10/2016 20:02:10","Make a design document explaining the light-curve core server-side functions","We need a design schema document to identify and describe the core functions that will be commonly used by the Light-curve viewer as well as the inputs and outputs on:  - getting the single exposure image  - compute and getting a phase folded curve  - compute and getting periodogram  - getting the peaks table    Those functions will be used by the processors DM-7162, DM-7164, DM-7165 via one or several Firefly 'search' processor.  Would be also helpful to add the description on the expected table format and content that the (future) NexSci API will offer.",1
"DM-7199","08/11/2016 11:18:03","afwTable's .getX()/.getY() do not appear in dir()","On a SourceCatalog, I can call {{cat.getX()}} or {{cat.getY()}} to return an array of pixel coordinates, but neither of these functions appear in {{dir(cat)}}. This breaks introspection and makes it difficult to know what other such magic functions exist (e.g., I can't tell if there is an equivalent for ra/dec).",2
"DM-7203","08/11/2016 14:33:46","LDM-151 review meeting, 2016-08-08","Prepare materials for and participate in the meeting.",2
"DM-7204","08/11/2016 14:34:25","LDM-151 review meeting, 2016-08-08","Prepare materials for and participate in the meeting.",2
"DM-7205","08/11/2016 14:34:57","LDM-151 review meeting, 2016-08-08","Prepare materials for and participate in the meeting.",2
"DM-7206","08/11/2016 14:35:14","LDM-151 review meeting, 2016-08-08","Prepare materials for and participate in the meeting.",2
"DM-7207","08/11/2016 14:44:58","Modify afw tests to support pytest","This ticket is for the work of migrating the AFW tests such that they run with the py.test test runner.",8
"DM-7208","08/11/2016 15:02:25","Minor fixes for issues flagged by Eclipse codan","Fix a few minor errors that were flagged by the Eclipse Neon C++ code analyzer.",0.5
"DM-7212","08/11/2016 17:13:31","The feature 'lock-by-click' in the readout panel doesn't work","The feature 'lock-by-click' in the readout panel doesn't work, when checked, the readout doesn't display any value from any image click.",0
"DM-7214","08/12/2016 04:43:46","Replace docker_spy by hostfile management","docker_spy is broken and breaks travis CI. It is now replace with raw hostfile management, simpler and more robust",2
"DM-7221","08/12/2016 20:32:59","TUNIT header is not uniformly applied to columns in FITS table output from afw.table","In looking at the Source table outputs (SRC*.fits files) in the validation_data_hsc repository, we noticed that the headers are not consistently applied in the FITS files generated by afw.table output.  In particular, the TUNIT keywords are missing for the coord_ra and coord_dec columns, making it difficult for a client to determine that the columns are persisted in units of radians, especially since the FITS convention is for the default units for angles to be degrees.    {quote}KEYWORD:   TUNITn  REFERENCE: FITS Standard   STATUS:    reserved  HDU:       table  VALUE:     string  COMMENT:   column units  DEFINITION: The value field shall contain a character string describing  the physical units in which the quantity in field n, after any  application of TSCALn and TZEROn, is expressed.   The units of all FITS  header keyword values, with the exception of measurements of angles,  should conform with the recommendations in the IAU Style Manual. For  angular measurements given as floating point values and specified with  reserved keywords, degrees are the recommended units (with the units,  if specified, given as 'deg').{quote}    http://heasarc.gsfc.nasa.gov/docs/fcg/standard_dict.html    Accepting that we are not conforming to that convention, the proper value for TUNITn is ""rad"" for data in units of radians.    While much access to afw.table-produced files will be through the afw.table interfaces, where this issue is irrelevant as long as round-tripping is successful, for exploratory and debugging purposes it is very useful to be able to access the file contents with standard tools.    In addition, if the FITS tables produced from afw.table are to be used as the input to the database ingest, providing units for all columns will assist the database system in providing and validating correct column metadata to its clients, whether through VO standards or other means.",1
"DM-7225","08/13/2016 00:10:01","File upload function","Develop a file upload function:  - upload FITS, VOtable, IPAC table, CSV file,  ...  - distinguish file type  - list multiple extensions of FITS, to know the type (image or table) of each extension  - display header information of each extension or the data accordingly  ",20
"DM-7239","08/15/2016 18:41:40","Any table update ('reset', sorting..) will also reset image options","When table is updated after sorting or clicking on 'reset' option, the image is reset to default option. If you have a different color stretch/scale or zoom, the image gets reset to initial values loosing the operations applied.    Updates:    This is the case with coverage image only.  On every table update, it will re-calculate the coverage, then redraw it with default options.  Prior to conversion, coverage image also reset its options when it does a redraw.  However, it only does a redraw when the catalog data have changed, like when filter is applied.   ",2
"DM-7250","08/16/2016 18:02:23","Adapt display_ds9 to py3","Follow the steps in the [LSST 2016 presentation|https://project.lsst.org/meetings/lsst2016/sites/lsst.org.meetings.lsst2016/files/201608-LSST2016-py3.pdf] by [~tjenness] to update the Python code in the {{display_ds9}} package. This will be a learning exercise to prepare for updating {{display_firefly}} for SUIT.",1
"DM-7253","08/17/2016 10:27:42","Modify db package to support python3","LSST2016 DM hack session for updating LSST code to support python3.    Instructions: https://project.lsst.org/meetings/lsst2016/sites/lsst.org.meetings.lsst2016/files/201608-LSST2016-py3.pdf",1
"DM-7259","08/17/2016 14:42:41","jenkins user listing and per user builds is unreliable"," I'm seeing some odd behavior with /asynchPeople/ only listing a small number of the actual users.  The list keeps refreshing itself with a different partial list.  Possibly related, /user/<userid> isn't listing all builds for a users, in some cases, it lists no builds at all for users which should have 100s",1
"DM-7260","08/17/2016 14:48:52","Modify cat package to support python3","LSST2016 DM hack session for updating LSST code to support python3.  Instructions: https://project.lsst.org/meetings/lsst2016/sites/lsst.org.meetings.lsst2016/files/201608-LSST2016-py3.pdf  ",1
"DM-7267","08/17/2016 17:15:31","Add array equality helpers to TestCase","afw.math added equality helper method, e.g. {{assertImageEqual}}, that just call the appropriate NearlyEqual method with tolerance of 0. It would be good to have equivalent ""assertArrayEqual"" and ""assertArrayNotEqual"" methods that wrap assertClose, which has some useful functionality (e.g. plotting) beyond what numpy.testing.allClose provides.",1
"DM-7268","08/17/2016 20:24:29","Record source measurement time in the metadata","Currently the measurement of sources in {{processCcd.py}} and {{imageDifference.py}} is not wrapped by {{pipeBase.timeMethod}}, and thus the metadata does not record this processing time. Adding this wrapper will improve our ability to track processing speed improvements.",1
"DM-7271","08/18/2016 14:39:35","Change default format for float and double to %g","DM-7271: table enhancements    * do not apply default data format  * introduce optional display format  * add ""AUTO"" and ""NONE"" as format keywords.  * for fits table, set display format to ""%.9g"" for double type.  * add support for TDISPn fits table header  * remove condition so temp files will not get written into source directory    When reading fits table, set display format to ""%.9g"" for double if TDISPn is not given.",2
"DM-7273","08/18/2016 15:15:40","Smaller readout switch to bigger one unexpectedly","Either in firefly demo page or in Gator, the small readout switch unexpectedly into the exanded bigger readout.    Please, check that the readout size and information layout should stay constant consistently.     (I could duplicate it but there is no clear pattern step by step to reproduce, but definitively i could see readout switching back and forth - maybe related to 'lock-by-click' problem)",1
"DM-7274","08/18/2016 15:21:45","Although the table shows more than 1 position, xy plot displays 1 single datapoint","In IRSAViewer and Gator/Atlas:  one single point is shown in XY Plot (RA vs. DEC) despite the fact that the table contains more points (see attached snapshot)    Step to reproduce from IRSAViewer (external viewer from Atlas) or from Gator itself: Do a catalog search 'm82' on AllWise around 100 arcsec.  ",1
"DM-7275","08/18/2016 15:40:57","Image expand mode had name of image at top left whereas migrated image view has ""Tiled View""","Before migration, expanded single image had the name on top left corner.    The current migrated view says 'Tiled View' which is not correct with the view itself. Please, add correct information of the expanded mode and/or the image name.",1
"DM-7279","08/18/2016 16:32:39","Firefly table and xy plot 'reset' should returned default values","'reset' button before migration returned the initial default values, not the previous applied values.  Please, restore function so 'reset' feature would give back the initial default values.    For example, if table starts with 'show units', 50 rows per page, clicking 'reset' button should give back this values if user changed them before.",0.5
"DM-7280","08/18/2016 16:55:14","Atlas: table scrambled values when few columns are selected","Atlas table has inconsistent values when a few columns are selected.  Please fix. (Not happening in Gator btw)",2
"DM-7295","08/19/2016 17:07:12","Get astrometry.net that works with Python 3","The current release of astrometry.net does not support Python 3. Upstream needs to be fixed.",2
"DM-7299","08/19/2016 17:10:24","Check lmfit works with Python 3","lmfit is meant to work with Python 3 but this needs to be checked. Some code in the tree does not work with Python 3 but it is possible that this code is not used by LSST.",0
"DM-7309","08/19/2016 17:20:26","Port obs_cfht to Python 3","This ticket covers the work required to get obs_cfht working with Python 3.",1
"DM-7314","08/20/2016 09:33:32","Odd Jenkins failure","A Python 3 Jenkins build failed with a very unusual log full of ""#"" and no clear idea of what went wrong.    https://ci.lsst.codes/job/stack-os-matrix/label=centos-7,python=py3/14730//consoleFull",1
"DM-7317","08/21/2016 02:19:29","Log scale doesn't work with sample table attached","When using the attached sample ipac table with 2 columns 'period' and 'power' from 'Data Sets: Catalogs & Images' dialog (firefly viewer), the XY-plot display correctly the data 'period' vs. 'period.     Changing Y axis to plot 'power' column and selecting 'log' scale, the axis disappear and nothing seems to be converted in log scale.    An error message can be seen in the console related to highchart.js:  highcharts.js:15 Uncaught Highcharts error #10: www.highcharts.com/errors/10P @ highcharts.js:15setTickInterval @ highcharts.js:16setScale @ highcharts.js:17(anonymous function) @ highcharts.js:18each @ highcharts.js:8setSize @ highcharts.js:18shouldComponentUpdate @ XYPlot.jsx:350updateComponent @     Please, investigate why and fix.  Thanks!",2
"DM-7324","08/22/2016 13:27:35","Port ci_hsc to Python 3","Work associated with ensuring that {{ci_hsc}} works with Python 3.",1
"DM-7325","08/22/2016 13:34:50","Port meas_extensions_ngmix to Python 3","Work associated with adding support for Python 3 to {{meas_extensions_ngmix}}.",1
"DM-7326","08/22/2016 13:35:09","Adapt FireflyClient (renamed to firefly_client.py) to Python 3","The stack is being adapted to work with Python 3 as well as Python 2. Following the guide from LSST2016 for porting packages to Python 3, adapt the FireflyClient API. This will allow afw.display to work with Firefly in a Python 3 stack.",2
"DM-7327","08/22/2016 13:37:57","Check ngmix works on Python 3","Check that {{ngmix}} works on python 3.",1
"DM-7328","08/22/2016 13:40:28","Port validate_drp to Python 3","This ports {{validate_drp}} to Python 2+3 compatibility.    (Note that the work is still against the {{u/sqre/measurement-api}} integration branch until we can coordinate the coordinate the deployment with SQUASH.)",0.5
"DM-7329","08/22/2016 14:03:46","Modify meas_extensions_photometryKron tests to support pytest","This ticket is for the work of migrating the meas_extensions_photometryKron tests such that they run with the py.test test runner.",0.5
"DM-7332","08/22/2016 14:35:15","Modify meas_extensions_simpleShape tests to support pytest","This ticket is for the work of migrating the meas_extensions_simpleShape tests such that they run with the py.test test runner.  ",0.5
"DM-7336","08/22/2016 16:46:16","Rotation of image display bug","The rotation of a z-scale log stretched  or north-up image operation fails and changes the stretch. The image becomes black/white.   That doesn't happen with linear stretch btw.   It looks like a server -side problem.    Step to reproduce:  Either by searching from Spitzer GOALS images or just uploading the attached FITS file, then stretch on 'z-scale log', then rotate (45deg). The stretch changes. Same for north-up operation.",2
"DM-7338","08/22/2016 19:03:13","Fix-up any code that uses butler.repository","See https://community.lsst.org/t/access-to-root-in-butler/991  When running constructBias.py in lsst_apps w_2016_34 I get the error:  AttributeError in map: 'Butler' object has no attribute 'repository'  The source of the error is probably  https://github.com/lsst/pipe_drivers/blob/master/python/lsst/pipe/drivers/constructCalibs.py#L585  All code that uses butler.repository should be fixed. If that's too much work possibly the ticket could get changed to fix constructCalibs only for now.",2
"DM-7341","08/23/2016 09:59:44","Handle lsst.utils.tests dependency better in testExecutables.py","The {{testExecutables.py}} script that allows py.test to run executable unittests requires lsst.utils.tests. Several packages such as sphgeom explicitly avoid making lsst.utils a dependency. Hence {{testExecutables.py}} is unable to reside inside the {{sphgeom}} package and {{py.test}} is unable to run the executable unittests for now. Figure out which is better:    # Make sphgeom depend on lsst.utils  # Remove the lsst.utils dependency from testExecutables.py",1
"DM-7347","08/23/2016 12:45:50","Modify meas_deblender tests to support pytest","This ticket is for the work of migrating the meas_deblender tests such that they run with the py.test test runner.",0.5
"DM-7350","08/23/2016 13:43:07","Cleanup ip_diffim's afwData test skipping","There are a number of {{if not self.defDataDir: print(""skipping""); return}} blocks in the ip_diffim tests that should be converted to {{@unittest.skipIf}} decorators. Otherwise, those tests may silently not be run, which is bad.",1
"DM-7352","08/23/2016 14:51:39","Prepare for testing the candidate technology for the alert distribution system","In order to test out the suggested candidate technology for alert distribution within the LSST system (Kafka) several activities need to be carried out.    1. Perform a literature review of relevant documents: Kafka paper and Comet paper at least  2. Answer relevant questions for setting up testing: e.g.  * What are topics used for?  * What partition sizes are optimal?  * Time scales for tests?    Product will be a short writeup of the literature review and any questions that came up and were resolved.  ",20
"DM-7355","08/23/2016 15:11:22","Provide a set of parameters to measure and contexts to test","The tests will have clearly defined measurements and failure modes.  We will need a tool for monitoring all the parameters.    Example system params to measure:  * latency and throughput  * CPU usage  * mem usage  * network I/O  * disk I/O    Example distribution system tests:  * Kafka node goes down  * Vary rampup of producers  * Vary message size  * Vary message block size  * Vary number of consumers    Result will be a list of system parameters to monitor, a list of distribution system tests, and a tool to do the monitoring.",20
"DM-7356","08/23/2016 15:13:16","Run test on prototype alert distribution and report on results","Tests are defined in DM-7355, but should be done in a staged way to increase the complexity of the system to test scaling.  Not all test scenarios need to be tested with each configuration.    A scheme like the following could work:  # Test 1: Mac with single server, 189 prodcuers  #* 1 consumer  #* 10 consumers  #* 100 consumers  # Test 2: Mac with 3 servers, 189 producers  #* 1 consumer  #* 10 consumers  #* 100 consumers  # Test 3: on big Nebula virtual machine with single server, 189 producers  #* 1 consumer  #* 10 consumers  #* 100 consumers  # Test 4: on one Nebula virtual machine with 3 servers, 189 producers  #* 1 consumer  #* 10 consumers  #* 100 consumers  # Test 5: 3 Nebula virtual machines  #* 189 producers within Nebula: 1, 10 and 100 consumers internal to Nebula  #* 189 external producers: 1, 10, 100, and N consumers both internal and external to Nebula    Product will be a DMTN on the results of using Kafka as the backend technology for supporting the internal alert distribution system.",20
"DM-7358","08/23/2016 15:32:03","Modify meas_modelfit tests to support pytest","This ticket is for the work of migrating the meas_modelfit tests such that they run with the py.test test runner.",0.5
"DM-7372","08/24/2016 06:59:14","Add information on milestone relationships to DMTN-020","Per [this discussion of LDM-472|https://github.com/lsst/LDM-472/commit/5ad5f731463cb0b17f8919c6d6016e63441b0a2e#commitcomment-18628148], please add material to DMTN-020 about relationships between milestones.",1
"DM-7376","08/24/2016 09:57:48","Failure to identify location of clear supernova in a subtraction","I've run a subtraction of two images, one with a clear supernova (SNR~100), the other taken a year later.  The supernova shows up very clearly, but does not get properly identified by the object detection and measurement.    If run with {{config.doMerge=False}} then I get the first attachment, which shows the identification of a footprint in which the SN is found, but the X, Y position are set to be the presumably the first object that went in to the footprint.  It's not re-centered at the SN.    If I run with {{config.doMerge=True}}, then I get the third attachment, in which I believe the SN has been subsumed into the larger object of the dipole at the galaxy center.    Some notes  1. There is significant galaxy background (that's what I wanted to do the subtraction).  This changes the noise properties and, from my understanding of a discussion on Hipchat, breaks assumptions made by the de-correlation step.  2. These images are from a NIR detector, so they are created from a sequence of raw images in a grid dither pattern, which have been processed and combined to make these final images.  This means that the noise varies significantly across the image, from the center which is covered by all raw images, to the corners which are each just covered by one.     To recreate on lsst-dev.ncsa.illinois.edu:    Set up the LSST stack + some custom branches in {{pipe_tasks}}, {{ip_diffim}}, {{obs_file}}:  {code}  . ~wmwv/.bashrc.lsst  {code}    {code}  WIYN=/lsst8/wmwv/WIYN  imageDifference.py ${WIYN}/test_dr1 --id fileroot=SN2011gy_A_H_20111115  --templateId fileroot=SN2011gy_A_H_20121028 --output ${WIYN}/test_dr1 --configfile ~wmwv/wiyn/diffimconfig.py --logdest wiyn_imageDifference.log --clobber-config --clobber-versions  {code}    If you're testing for yourself, you may wish to change the output repo to your own cusomt repo so we don't accidentally confuse each other with various runs.    The active parts of {{diffimconfig.py}} are    {code}  config.doUseRegister=False  config.convolveTemplate=True  config.doWriteMatchedExp=True  config.doDecorrelation=True  config.doMerge=True  config.subtract.kernel.name='AL'  from lsst.ip.diffim.getTemplate import GetCalexpAsTemplateTask  config.getTemplate.retarget(GetCalexpAsTemplateTask)  {code}    For a quick start, the following will display the images:    {code}  ds9 /lsst8/wmwv/WIYN/test_dr1/calexp/SN2011gy_A_H_{20111115,20121028}.fits /lsst8/wmwv/WIYN/test_dr1/diff/SN2011gy_A_H_20111115/diffexp.fits  {code}",8
"DM-7379","08/24/2016 13:14:29","random seed not run in command line test","In the conversion of ip_diffim to pytest a random.seed was moved to inside the setup_module function as opposed to the seUp method. When run from the command line (for example using Jenkins) this test can fail.",1
"DM-7384","08/24/2016 14:56:06","Modify obs_lsstSim to support py.test","Migrate the {{obs_lsstSim}} tests such that they run with the {{py.test}} test runner.  ",0.5
"DM-7386","08/24/2016 15:10:27","Jenkins/lsstsw no longer seems to build optional dependencies in all cases","If I ask Jenkins (or {{lsstsw}}) to build {{skymap}} the optional dependency of {{healpy}} is not included in the build. If I try to build {{afw}} it does include the optional dependencies of {{pyfits}}, {{matplotlib}} and {{afwdata}}. {{pipe_base}} does bring in its optional dependencies as well.    I haven't worked out what is ""special"" about {{skymap}}.    For an example Jenkins build: https://ci.lsst.codes/job/stack-os-matrix/label=centos-7,python=py3/14837//console    Table file:  {code}  setupRequired(numpy)  setupRequired(afw)  setupOptional(healpy)  {code}  ",0.5
"DM-7387","08/25/2016 07:49:09","Modify obs_decam to support py.test","Migrate the {{obs_decam}} tests to support py.test",0.5
"DM-7392","08/25/2016 10:16:20","More LDM-151 Review Updates","Implement changes identified in walking through the Algorithmic Components and Software Primitives sections with [~swinbank] and [~krughoff].",1
"DM-7405","08/26/2016 15:45:09","Add VOTable support when uploading table ","Uploading a votable result from simple cone search is not currently supported.    We should provide VOTable xml file as part of the format supported by table upload.    Make sure that it handle also VOTable with BINARY2 encoding (attached sample from Gaia DR1).",8
"DM-7415","08/29/2016 09:38:56","Port dax_dbserv to Python 3","Work related to porting {{dax_dbserv}} to Python 3.",0.5
"DM-7416","08/29/2016 09:39:49","Port dax_imgserv to Python 3","Work related to porting {{dax_imgserv}} to Python 3.",2
"DM-7417","08/29/2016 09:40:30","Port dax_metaserv to Python 3","Work related to porting {{dax_metaserv}} to Python 3.",0.5
"DM-7418","08/29/2016 09:41:22","Port dax_webserv to Python 3","Work related to porting {{dax_webserv}} to Python 3.",0.5
"DM-7419","08/29/2016 09:42:11","Port dax_webservcommon to Python 3","Work related to porting {{dax_webservcommon}} to Python 3.",0.5
"DM-7424","08/29/2016 13:26:45","Rework joint photometric calibration in LDM-151","The current text in LDM-151 for joint calibration (by [~pyoachim]) mostly concerns global calibration, but references to it in the DRP pipelines section assume we'll use Gaia for global calibration.    I'll try to resolve that discrepancy with minimal modification on this issue, and then ask [~zivezic] and [~rhl] to review.  ",1
"DM-7425","08/29/2016 14:11:39","Port pipe_supertask to Python 3","Work for porting {{pipe_supertask}} to Python 3.",1
"DM-7429","08/29/2016 17:39:06","Publish LSE-163 to LSST the Docs","The treatment will be a static HTML landing page, similar to ldm-151.lsst.io",0.5
"DM-7433","08/29/2016 19:00:01","Create test case for processFile","Add a testing suite to {{processFile}}.  Specifically create a simple test case that verifies that {{processFile.py}} runs on a known set of test data.    Optionally create simple additional tests if any occur.    Implementation Plan  1. This test data will be from {{afwdata}}, as this adds minimal dependencies.  See https://community.lsst.org/t/should-i-take-example-test-files-from-afwdata-or-obs-test-to-write-a-test-case-for-processfile/1101/2 for a bit of discussion.  2. The dependency on {{afwdata}} will be {{setupOptional}}.  A user shouldn't have to download {{afwdata}} just to run {{processfile}}, but we will want to run this test as part of our regular Jenkins builds and CI efforts to make sure things don't get broken.    Tests of performance against a known standard will be deferred to the {{lsst_ci}} package.    ",2
"DM-7436","08/30/2016 07:24:05","Cleanup in admin/tools","Remove obsolete files and directories and update documentation.  See:  - admin/tools/cluster/  - admin/tools/*",2
"DM-7437","08/30/2016 09:38:03","Fix eupspkg command for qserv containers","A bug was introduced during DM-6444. Fix is provided here.",2
"DM-7442","08/30/2016 12:35:50","Write unit tests for DCR template generation code","DM-5697 and DM-6249 created code that generates DCR-matched template images for image subtraction. These functions should have unit tests.",8
"DM-7444","08/30/2016 12:41:31","Improve jointcal plotting backend","While working on the jointcal quiver plots, etc. I made all the plots with the default, interactive, backend. Now that I need to use larger datasets on lsst-dev, interactive mode won't work, but would be useful to keep as an option. I had started to convert to using Agg during the All Hands Meeting, but got stuck on some strange interactions with quiver.    This story is to capture the work I started on it, and to finish cleaning up that code. It might even be worth lifting the plotting code out of the tests, now that they're becoming more complicated. It would be useful to be able to generate these plots during non-testing jointcal runs.    This might be a good opportunity to develop the ""unified plotting abstraction layer"" described in DM-5790.",8
"DM-7445","08/30/2016 12:45:27","DCR template generation speed improvements","The current DCR template generation code takes approximately 30s per pixel on a single core to run. This code needs to be optimized to run much faster.",8
"DM-7447","08/30/2016 13:50:17","Run DCR template generation on real data","The DCR template generation code has been developed and tested using simulated data. It would be informative to try running it on real data to identify new issues and see if it works.",2
"DM-7448","08/30/2016 16:05:11","Table reset unset option 'Show unit' initially set","Reset feature works ok as expected unless if the table is created with 'show unit' option checked initially reset button will unset the option when it shouldn't changed it.  Please investigate and fix.    The problem can be shown in the table with filters and units on by default here:  http://localhost:8080/firefly/demo/ffapi-highlevel-test.html",1
"DM-7450","08/30/2016 17:27:59","Update coding standard to reflect non-executable tests (RFC-215)","Following the discussion at RFC-215, we need to update the developer guide to reflect that python tests should not be executable, nor have a shabang at the top. This is probably a few sentences to the python [testing portion|https://developer.lsst.io/coding/python_testing.html] part of the dev. guide, possibly in a new subsection at the end.",0.5
"DM-7455","08/30/2016 18:13:57","Scale up Kafka alert producers","Figure out how to use Docker compose or something similar to easily scale up from one producer to 189 producers, one per CCD.",5
"DM-7456","08/30/2016 18:15:01","Scale up Kafka alert consumers","Use Docker compose or something similar to easily scale up from one Kafka alert consumer to N consumers.",2
"DM-7458","08/31/2016 12:47:52","Cannot add base_GaussianFlux to the list of default plugins in DipoleFitTask.","The DipoleFitTask plugin in ip_diffim currently registers a number of default measurement tasks. However, if base_GaussianFlux is registered, the following error is produced:    Alias for 'slot_Shape_flag' must be defined before initializing 'base_GaussianFlux' plugin. {0}  lsst::pex::exceptions::LogicError: 'Alias for 'slot_Shape_flag' must be defined before initializing 'base_GaussianFlux' plugin.'    Track this down and fix it so that we can add base_GaussianFlux to the default list.",2
"DM-7460","08/31/2016 14:33:42","No help link/anchor option XY-plot component","Table component has an option to add a link to onlinehelp for table documentation.  Xy-plot is missing this feature and need an anchor to link to xy-plot help dcumentation.    Please add to the API a way to pass a help-id property and add an anchor in the toolbar.  ",2
"DM-7461","08/31/2016 14:37:45","afw test suite fails in the absence of afwdata","Running a fresh build of afw master on os x fails on two unit tests. One is in testExecutables.py and is related to not finding AFWData. On my system AFWData is not installed and the test should be skipped, but is still run. The other is DM-7474.",0.5
"DM-7464","08/31/2016 16:09:50","Repackage Firefly Python API in its own pip-installable repository","After a discussion between [~gpdf], [~xiuqin] and [~shupe], there is a strong rationale for separating the FireflyClient API into its own repository (not combined with widgets):  * FireflyClient has very few dependencies, while {{firefly_widgets}} already depends on Astropy and more dependencies may be added.  * FireflyClient is pure Python, while {{firefly_widgets}} includes both Javascript and Python.    The proposal is to move FireflyClient out of the {{firefly}} repository and into its own repository {{firefly_client}}. -Within this repository, FireflyClient needs to be packaged to be pip-installable via {{pip install firefly_client}}.-    -To conform to Python module naming conventions, the module needs to be all lower-case, so that the users will import the class with something like {{from firefly_client import FireflyClient}} after it has been installed.-    Examples of using the Python API could be included in an {{examples}} subdirectory in this repository.",2
"DM-7465","08/31/2016 16:34:59","Add requirement annotations to DPDD","When linking items in DPDD to requirements and design documents, we need to provide anchor links that can be referenced outside of DPDD and which are guaranteed not to change. Section numbers and ""item 5 in list on page 3"" are not acceptable anchors. Explicit numbers will be assigned as margin notes to requirements-like items in DPDD.",20
"DM-7470","08/31/2016 19:06:55","Firefly viewer (and IRSAVIewer) shows an extra coverage tab when doing a catalog search","Once the viewer shows an image search or externally calling the viewer to expand an image previously displayed, it will display a coverage image (extra tab) after doing a catalog search.    In IRSAVIewer, this is inconsistent with previous flow and appearance, a catalog search expect to return and overlaid on top of the previously image displayed. Coverage shouldn't appear if an image is already loaded and a search catalog is triggered.    Plus, when the user get rid off the catalog, the image left is the selected image (could be the added coverage image).    At some point, this was working as before but the flow had changed.  Please, restore the way it was before, no coverage after doing a catalog search when an image is already loaded from image search or pushed externally to the viewer.",1
"DM-7492","09/01/2016 17:02:01","Load calexps into PDAC","Igor has located calexps, and begun work to stage them to NCSA.  Figure out final location in PDAC needed to support imgserv, transfer images there, and drop duplicates.",8
"DM-7494","09/01/2016 17:07:54","PDAC imgserv Butler configuration"," Address butler configuration for imgserv to it can access calexps within PDAC  ",8
"DM-7503","09/02/2016 08:38:58","Discuss deblender requirements with stakeholders","Chat with stakeholders in the deblender (at least RHL, Jim Bosch) to get a better feeling for their vision of how it should be developed in the future. Understand the architectural requirements for multi-band deblending.",2
"DM-7504","09/02/2016 08:41:15","Compile list of missing or inadequate deblender functionality","As part of DM-3622, we aim to make the single-band deblender equal to ""the state of the art"". What does this mean in practice? Compile a list of features we need to add (the SDSS anti-shredding algorithm is the canonical example; are there others?) and pathological behaviour that we need to fix. Ensure they're all programmed into the epic as stories.",1
"DM-7509","09/02/2016 12:44:04","Logging system sensitive to %s in string","There are two bugs in the log package:    log.info(""%s"") results in:  {code}  TypeError: not enough arguments for format string  {code}  this is because Log._log is defined with this line:  {code}  self.log(level, os.path.split(frame.f_code.co_filename)[1],                       inspect.stack()[2][3], frame.f_lineno, fmt % args)  {code}  and in this case args is an empty tuple. A trivial fix is to replace this with:  {code}  msg = fmt % args if args else fmt  self.log(level, os.path.split(frame.f_code.co_filename)[1],                       inspect.stack()[2][3], frame.f_lineno, msg)  {code}    However, doing so exposes another bug. The following will segfault:  {code}  log.log(2000, ""foo"", ""bar"", 5, ""%s"")  {code}  This is because Log.log is extended in logLib.i as follows:  {code}      void log(int level, std::string const& filename,               std::string const& funcname, unsigned int lineno,               std::string const& msg) {          self->log(log4cxx::Level::toLevel(level),                    log4cxx::spi::LocationInfo(filename.c_str(), funcname.c_str(), lineno),                    msg.c_str());  {code}  and the version of Log.log it calls expects additional format arguments. In general the log package seems to use {{logMsg}} when no {{%s}} formatting is wanted, and {{log}} when such formatting is wanted, so I propose to fix this using:  {code}      void logMsg(int level, std::string const& filename,               std::string const& funcname, unsigned int lineno,               std::string const& msg) {          self->logMsg(log4cxx::Level::toLevel(level),                    log4cxx::spi::LocationInfo(filename.c_str(), funcname.c_str(), lineno),                    msg.c_str());  {code}    In addition, this requires the first fix to be changed to call {{logMsg}} instead of {{log}}",2
"DM-7510","09/02/2016 13:18:55","Add support for HSC-R2 filter","Port of [HSC-1419|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1419].",0.5
"DM-7511","09/02/2016 13:32:36","Firefly API JSDoc build in Jekins","We need to add JSDoc generation in Jekins build and bundle it in the firefly.war file when we make Firefly release.  ",2
"DM-7512","09/02/2016 13:35:17","obs packages need a unified test framework","The various obs* package test cases should derive from a set of parent TestCases, so that they all trivially have the same testing functionality. obs_decam has more tests written for it than most of the other packages, but most of those tests could be lifted into some higher package, with the obs_decam test looking like, e.g.    {code}  ...boilerplate...  class GetIdTestCase(obs_test_helper.tests.IdTestCase):      def setUp(self):          self.mapper = DecamMapper(root=""."")          super(GetIdTestCase, self).setUp()          self.dataId = {'visit'=229388, 'ccdnum'=13, 'filter'='z'}          self.result_id = 22938813'  ...boilerplate...  {code}    with all of the test* methods living {{obs_test_helper.tests.IdTestCase}}. This is just a sketch of a design, but I think it would both simplify deploying new obs packages and allow us to almost trivially improve testing coverage in the existing packages.",40
"DM-7518","09/03/2016 09:21:43","Wrap log package with pybind11 instead of swig","The {{log}} package was recently added as a future replacement for {{pex_logging}}.  Wrap this package with pybind11 instead of swig.",2
"DM-7520","09/03/2016 20:34:54","XY plot is not shown after closing one of the catalog tab","When 2 or more catalog search result tables are shown, the XY-plot displays the table selected. If one click on another tab, the xy-plot refresh and gets updated correctly plotting the table datapoints automatically.    If the user close the table, the next table gets focused and one expect to see the xy-plot to be updated.     The xy-plot doesn't refresh and shows empty grey background.    Please, investigate and fix.",2
"DM-7521","09/03/2016 22:18:50","Add image feature in the image viewer toolbar is inconsistent when single image","The toolbar of the image viewer shows a button to add or replace an image ('plot').    The option 'Create new plot' doesn't work when the viewer shows a single image without the 'grid'/'single' image expand mode toolbar (that happen for example in Gator but can be seen in demo page higher level API test).    The lock image feature is also not needed when only one image is shown.    The toolbar is different in Gator in OPS than it is in the migrated version. Please check the inconsistency and fix the toolbar.    9/7/2016  Xiuqin WU  After discussion today, the decision is to make the ""Create new plot"" option work  when the ""Add image"" icon (the one with + sign)   in both the normal mode and expanded mode.   Leave the lock icon as is. ",1
"DM-7523","09/05/2016 10:16:17","obs_sdss test should not print outside of test","In DM-7346 the {{testSelectSdssImages.py}} test was modified to skip if a server was unavailable but the reason for the lack of availability was printed to standard error at the module level. This print is not captured by pytest (it only captures test output). The reason has to be moved inside the skip message.",1
"DM-7526","09/06/2016 11:22:15","Python Coding Standard Copy Edits","Content from DM-5456 has some missing words that existed during drafts. It seems that these were lost by a Git rebase. This ticket will fix this issues.",0.5
"DM-7531","09/06/2016 13:01:14","Jenkins is no longer displaying test failure output","At some point in the past week or so failures from Jenkins have stopped making available the output of test {{.failed}} files or the output of the {{_build.log}} file. This makes is almost impossible to work out why Jenkins failed.    Examples: https://ci.lsst.codes/job/stack-os-matrix/label=centos-6,python=py2/15505//console and from last week: https://ci.lsst.codes/job/stack-os-matrix/label=centos-6,python=py2/15194//console",0.5
"DM-7532","09/06/2016 14:38:51","Add ""disallow if False:"" note to developer docs","This is the implementation ticket for RFC-205.    The suggested additional text is given below, which probably best fits as a subsection of section 10 of the python style guide:    Code must not be placed inside `if False:` or `if True:` blocks, nor left commented out. If one has debugging code or if one is undecided about which particular implementation to use, such code must be placed inside a ""named"" `if` statement. Such blocks may have a comment describing the conditions under which said code can be removed (e.g. completion of a ticket, a particular date). For example, for code that will likely be removed in the future, once testing is completed:    {code}  # Delete old_thing() and this ""if"" once we have all the unittests in place (DM-123456).  use_old_method = False  if use_old_method:      old_thing()  else:      new_thing()  {code}    Such debugging flags should usually be lifted up into the method's keyword arguments to allow users to decide which branch to run. For example:    {code}  def foo(x, debug_plots=False):      do_thing()      if debug_plots:          plot_thing()  {code}    or, using [lsstDebug|https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/base_debug.html], which can be controlled as part of a commandline task:    {code}  import lsstDebug  def foo(x):      do_thing()      if lsstDebug.Info(__name__).debug_plots:          plot_thing()  {code}",0.5
"DM-7533","09/06/2016 14:53:28","astrometry_net log is not controllable from the command line interface","The external logs from astrometry_net (https://github.com/dstndstn/astrometry.net) is now passed through to {{lsst.log}} with the logger name ""meas.astrom.astrometry_net"". But unlike other internal logger components, the astrometry_net log is not controllable from the command line intercae.  I suspect a bug in [this commit|https://github.com/lsst/meas_astrom/commit/bf568b1853fdd4f49e4fbae7f2fc731a15dc76aa].  ",1
"DM-7535","09/06/2016 14:56:42","list the JS source files that need JSDoc generated for JS API","Please make a list of JS source files that need JSDoc content updated in order to generate the JSDoc for Firefly JS API.     The update of all the source files will be separate tasks. ",1
"DM-7538","09/06/2016 18:19:04","CModel is producing SingularTransformException","This is running on private data because I haven't yet found a public example that produces the problem.  The particular data set may be specialised, but we shouldn't be so chatty in any case.    {code}  pprice@tiger-sumire:/scratch/gpfs/pprice $ processCcd.py DATA --rerun cmodel --id visit=42742 ccd=49 --clobber-config  /home/pprice/.local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.    warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')  : Loading config overrride file '/tigress/HSC/LSST/stack/Linux64/obs_subaru/12.0-16-gbc41bb6+2/config/processCcd.py'  : Loading config overrride file '/tigress/HSC/LSST/stack/Linux64/obs_subaru/12.0-16-gbc41bb6+2/config/hsc/processCcd.py'  : input=/scratch/gpfs/pprice/DATA  : calib=None  : output=/scratch/gpfs/pprice/DATA/rerun/cmodel  CameraMapper: Loading registry registry from /scratch/gpfs/pprice/DATA/rerun/cmodel/_parent/registry.sqlite3  CameraMapper: Loading calibRegistry registry from /scratch/gpfs/pprice/DATA/CALIB/calibRegistry.sqlite3  processCcd: Processing {'taiObs': '2015-10-15', 'pointing': 1383, 'visit': 42742, 'dateObs': '2015-10-15', 'filter': 'HSC-I', 'field': 'U105A_CEN', 'ccd': 49, 'expTime': 250.0}  processCcd.isr: Performing ISR on sensor {'taiObs': '2015-10-15', 'pointing': 1383, 'visit': 42742, 'dateObs': '2015-10-15', 'filter': 'HSC-I', 'field': 'U105A_CEN', 'ccd': 49, 'expTime': 250.0}  processCcd.isr.crosstalk: Applying crosstalk correction  processCcd.isr: Applying brighter fatter correction  processCcd.isr: Finished brighter fatter in 6 iterations  processCcd.isr: Set 0 BAD pixels to 3199.49  processCcd.isr: Flattened sky level: 3200.822754 +/- 38.878743  processCcd.isr: Measuring sky levels in 8x16 grids: 3196.589865  processCcd.isr: Sky flatness in 8x16 grids - pp: 0.151048 rms: 0.022820  processCcd.isr: Setting rough magnitude zero point: 34.594850  processCcd.charImage: Processing {'taiObs': '2015-10-15', 'pointing': 1383, 'visit': 42742, 'dateObs': '2015-10-15', 'filter': 'HSC-I', 'field': 'U105A_CEN', 'ccd': 49, 'expTime': 250.0}  processCcd.charImage.repair: Identified 831 cosmic rays.  processCcd.charImage.detection: Detected 664 positive sources to 50 sigma.  processCcd.charImage.detection: Resubtracting the background after object detection  processCcd.charImage.measurement: Measuring 664 sources (664 parents, 0 children)   processCcd.charImage.measurePsf: Measuring PSF  processCcd.charImage.measurePsf: PSF star selector found 303 candidates  processCcd.charImage.measurePsf: Reserved 75 candidates from the fitting  processCcd.charImage.measurePsf: PSF determination using 205/303 stars.  processCcd.charImage: iter 1; PSF sigma=1.26, dimensions=(41, 41); median background=3196.07  processCcd.charImage.repair: Identified 799 cosmic rays.  processCcd.charImage.detection: Detected 743 positive sources to 50 sigma.  processCcd.charImage.detection: Resubtracting the background after object detection  processCcd.charImage.measurement: Measuring 743 sources (743 parents, 0 children)   processCcd.charImage.measurement WARNING: Error in modelfit_CModel.measure on record 36715308886523905:     File ""src/geom/LinearTransform.cc"", line 66, in const lsst::afw::geom::LinearTransform lsst::afw::geom::LinearTransform::invert() const      Could not compute LinearTransform inverse {0}  lsst::afw::geom::SingularTransformException: 'Could not compute LinearTransform inverse'    processCcd.charImage.measurement WARNING: Error in modelfit_CModel.measure on record 36715308886523906:     File ""src/geom/LinearTransform.cc"", line 66, in const lsst::afw::geom::LinearTransform lsst::afw::geom::LinearTransform::invert() const      Could not compute LinearTransform inverse {0}  lsst::afw::geom::SingularTransformException: 'Could not compute LinearTransform inverse'    [many, many more follow...]  processCcd.charImage.measureApCorr: Measuring aperture corrections for 7 flux fields  /tigress/HSC/LSST/stack/Linux64/meas_algorithms/12.0-24-g3f92e4c+6/python/lsst/meas/algorithms/measureApCorr.py:226: RuntimeWarning: invalid value encountered in greater    fluxes > 0.0,  processCcd.charImage.measureApCorr: Aperture correction for modelfit_CModel_exp: RMS 0.073826 from 253  processCcd.charImage.measureApCorr: Aperture correction for base_PsfFlux: RMS 0.074407 from 255  processCcd.charImage.measureApCorr: Aperture correction for modelfit_CModel: RMS 0.073742 from 253  processCcd.charImage.measureApCorr: Aperture correction for modelfit_CModel_initial: RMS 0.072894 from 254  processCcd.charImage.measureApCorr: Aperture correction for ext_photometryKron_KronFlux: RMS 0.062862 from 255  processCcd.charImage.measureApCorr: Aperture correction for base_GaussianFlux: RMS 0.086553 from 255  processCcd.charImage.measureApCorr: Aperture correction for modelfit_CModel_dev: RMS 0.073533 from 253  processCcd.charImage.applyApCorr: Applying aperture corrections to 7 flux fields  processCcd.charImage.applyApCorr: Use naive flux sigma computation  processCcd.calibrate: Processing {'taiObs': '2015-10-15', 'pointing': 1383, 'visit': 42742, 'dateObs': '2015-10-15', 'filter': 'HSC-I', 'field': 'U105A_CEN', 'ccd': 49, 'expTime': 250.0}  processCcd.calibrate.detection: Detected 4386 positive sources to 5 sigma.  processCcd.calibrate.detection: Resubtracting the background after object detection  processCcd.calibrate.deblend: Deblending 4386 sources  processCcd.calibrate.deblend: Deblended: of 4386 sources, 1006 were deblended, creating 3754 children, total 8140 sources  processCcd.calibrate.measurement: Measuring 8140 sources (4386 parents, 3754 children)   processCcd.calibrate.applyApCorr: Applying aperture corrections to 2 flux fields  processCcd.calibrate.applyApCorr: Use naive flux sigma computation  [...]  {code}",1
"DM-7539","09/07/2016 09:30:09","""Docstrings SHOULD NOT be preceded or followed by a blank line"" needs update","The nicely updated Python style guide version 6.0 has a small error in ""Docstrings SHOULD NOT be preceded or followed by a blank line"". It's a nice simple rule, and one I followed for years, but autopep8 adds a blank line after the doc string for a class, so apparently PEP8 has slightly different requirements.    I hope fixing this won't require an RFC.",0.5
"DM-7542","09/07/2016 12:59:16","Renew TLS certificate for community.lsst.org (2016)","Renew *.lsst.org certificate and install on community.lsst.org. Procedure at https://meta.discourse.org/t/allowing-ssl-https-for-your-discourse-docker-setup/13847",0.5
"DM-7543","09/07/2016 14:50:37","Change background and text color in image toolbar and label (left)","compared to before migration, a blue background and transparent icon toolbar appears on top of the image viewer.    Blue background and zoom number is not readable enough. The color is not consistent with the rest of the application neither.    Change blue to same transparency as the toolbar icon (right hand) or change to black on white if doesn't improve.    ",1
"DM-7546","09/07/2016 15:10:53","create a shared stack in GPFS for Verification Cluster ","   Analogous to the work of   DM-6968 for NFS, we request a shared stack within the GPFS file system for  lsst-dev7 and the Verification Cluster (48 nodes, 1152 cores).       The familiar NFS home directories are available on the front end system lsst-dev7, but are not mounted on the compute nodes of the Verification Cluster.  As such, the NFS stack of DM-6968 is not usable for computation on the Verification Cluster.    Initial testing shows computation against a stack in GPFS can execute at least 300+ simultaneous tasks/pipelines without significant slowdown, suggesting a wide range of work can be performed with a stack in the shared GPFS space on the >~ 1000 cores of the Verification Cluster.    The suggested location for the GPFS stack  is under  /software/lsstsw , reachable on lsst-dev7 (i.e., the build of the stack would be done on lsst-dev7, running CentOS 7.2.1511 ).    The /software space is considered a location where various reference software stacks might be made available to the team.    In addition to /software/lsstsw , the 'lsstsw' user does have a 'GPFS home' directory /gpfs/fs0/home/lsstsw  that could also be used, though the organization offered by /software appears preferable.    (The 'lsstsw' user  would continue to have a home directory in NFS for the time being, with an eventual transition to that on GPFS. )",1
"DM-7547","09/07/2016 15:20:01","Remove the extra column on far right-hand side of a table when no description is available","An empty column seems to be visible on the right-hand side of a table when no description is available in option panel of the XY-plot column setting, table option and fits header table.",2
"DM-7548","09/07/2016 16:11:20","constructBias.py crashes with errors about log","[~aritter] got this error when running {{constructBias.py}}. It seems {{pipe_drivers}} needs changes following the logging framework transition in {{pipe_base}}.     {code:java}  bias WARN: Unable to process DataId(initialdata={'category': 'A', 'taiObs': '2015-12-04', 'visit': 6301, 'dateObs': '2015-12-04', 'site': 'S', 'filter': 'r', 'field': 'BIAS', 'spectrograph': 2, 'ccd': 5, 'arm': 'r'}, tag=set([])): Wrong number or type of arguments for overloaded function 'Log_log'.    Possible C/C++ prototypes are:      lsst::log::Log::log(lsst::log::Log,log4cxx::LevelPtr,log4cxx::spi::LocationInfo const &,char const *,...)      lsst::log::Log::log(log4cxx::LevelPtr,log4cxx::spi::LocationInfo const &,char const *,...)    NotImplementedError on tiger1:27163 in map: Wrong number or type of arguments for overloaded function 'Log_log'.    Possible C/C++ prototypes are:      lsst::log::Log::log(lsst::log::Log,log4cxx::LevelPtr,log4cxx::spi::LocationInfo const &,char const *,...)      lsst::log::Log::log(log4cxx::LevelPtr,log4cxx::spi::LocationInfo const &,char const *,...)    Traceback (most recent call last):    File ""/tigress/HSC/LSST/stack/Linux64/ctrl_pool/12.0+45/python/lsst/ctrl/pool/pool.py"", line 89, in wrapper      return func(*args, **kwargs)    File ""/tigress/HSC/LSST/stack/Linux64/ctrl_pool/12.0+45/python/lsst/ctrl/pool/pool.py"", line 207, in wrapper      return func(*args, **kwargs)    File ""/tigress/HSC/LSST/stack/Linux64/ctrl_pool/12.0+45/python/lsst/ctrl/pool/pool.py"", line 524, in map      return self._processQueue(context, func, zip(range(num), dataList), *args, **kwargs)    File ""/tigress/HSC/LSST/stack/Linux64/ctrl_pool/12.0+45/python/lsst/ctrl/pool/pool.py"", line 421, in _processQueue      return [func(self._getCache(context, i), data, *args, **kwargs) for i, data in queue]    File ""/tigress/HSC/LSST/stack/Linux64/pipe_drivers/12.0-3-g33a9219+35/python/lsst/pipe/drivers/constructCalibs.py"", line 459, in process      exposure = self.processSingle(sensorRef)    File ""/tigress/HSC/LSST/stack/Linux64/pipe_drivers/12.0-3-g33a9219+35/python/lsst/pipe/drivers/constructCalibs.py"", line 477, in processSingle      return self.isr.runDataRef(dataRef).exposure    File ""/tigress/HSC/LSST/stack/Linux64/obs_subaru/12.0-18-gb01a726+1/python/lsst/obs/subaru/isr.py"", line 268, in runDataRef      ccdExposure = self.assembleCcd.assembleCcd(ccdExposure)    File ""/tigress/HSC/LSST/stack/Linux64/ip_isr/12.0-7-ga6ffce9+2/python/lsst/ip/isr/assembleCcdTask.py"", line 229, in assembleCcd      self.postprocessExposure(outExposure=outExposure, inExposure=getNextExposure(ccd[0]))    File ""/tigress/HSC/LSST/stack/Linux64/ip_isr/12.0-7-ga6ffce9+2/python/lsst/ip/isr/assembleCcdTask.py"", line 243, in postprocessExposure      self.setWcs(outExposure = outExposure, inExposure = inExposure)    File ""/tigress/HSC/LSST/stack/Linux64/ip_isr/12.0-7-ga6ffce9+2/python/lsst/ip/isr/assembleCcdTask.py"", line 280, in setWcs      self.log.log(self.log.WARN, ""No WCS found in input exposure"")    File ""/tigress/HSC/LSST/stack/Linux64/log/12.0-5-g1df3bbf/python/lsst/log/logLib.py"", line 249, in log      return _logLib.Log_log(self, *args)  NotImplementedError: Wrong number or type of arguments for overloaded function 'Log_log'.    Possible C/C++ prototypes are:      lsst::log::Log::log(lsst::log::Log,log4cxx::LevelPtr,log4cxx::spi::LocationInfo const &,char const *,...)      lsst::log::Log::log(log4cxx::LevelPtr,log4cxx::spi::LocationInfo const &,char const *,...)    application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0  {code}  ",5
"DM-7549","09/07/2016 16:51:54","afw:testGaussianProcess has an intermittent test failure","Sometimes the {{afw}} test {{testGausisianProcess.py}} can fail:  {code}  ...  ======================================================================  FAIL: testTooManyNeighbors (__main__.GaussianProcessTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""./tests/testGaussianProcess.py"", line 62, in testTooManyNeighbors      gg.selfInterpolate(sigma, -1, nData-1)  AssertionError: Exception not raised  ----------------------------------------------------------------------  Ran 11 tests in 1.992s  FAILED (failures=1)  {code}    This is a very rare failure. It ran 102 times on my laptop before it stopped with this failure. So far we have only seen reports of this failure on Python 3.  ",3
"DM-7553","09/07/2016 18:06:32","Merge XRootD plugin link changes from upstream and adapt Qserv","Changes to plugin linking have been made upstream in XRootD (plugins now to link against import library).  This ticket is to update the lsst-dev XRootD branch from upstream and make the necessary parallel change to the Qserv build scripts.",1
"DM-7555","09/07/2016 22:05:06","Ad-hoc developer requests Part II","This is a bucket epic for ad-hoc developer requests that cannot be postponed till the next planning cycle. In the event that it is underutilised for this purpose, it will be assigned to technical debt DM-5850",8
"DM-7558","09/08/2016 07:16:56","Create dependency map for Calibration Products Production","Identify the relationships that the Calibration Products Pipeline have with other parts of LDM-151 (chiefly algorithmic components) and with external deliverables, and record these appropriately.",3
"DM-7560","09/08/2016 07:21:00","Audit Calibration Products Pipeline section of LDM-151","Detailed read-through of the CPP section of LDM-151. Identify areas of uncertainty, resolve them with stakeholders (ie, [~mfisherlevine], at least initially). Make copious notes.",5
"DM-7561","09/08/2016 07:21:39","Audit Data Release Production section of LDM-151","Detailed read-through of the CPP section of LDM-151. Identify areas of uncertainty, resolve them with stakeholders (ie, [~jbosch], at least initially). Make copious notes.",5
"DM-7562","09/08/2016 07:23:24","Take part in 2016-09-08 planning meeting","Participate in the meeting called by [~mjuric]:    {quote}          Just confirming we’ll meet for an initial planning meeting tomorrow at 10am PT. Roughly, the goal is to make sure we’re on the same page on the principles for the deployment plan you guys have    started developing, and to begin putting together a Gantt chart and milestones for what we believe thePipelines groups will deliver (and need) based on what we know is in LDM-151.  {quote}",1
"DM-7563","09/08/2016 07:28:19","Produce straw-man plan for DRP","Using the    - Understanding developed in DM-7561;  - Dependency map developed in DM-7557;  - Responsibility assignments developed in DM-7031;  - Rough resource requirements developed in DM-7032, DM-7034;  - Tooling decided in DM-7559;    develop an outline of a plan for Data Release Production.  ",8
"DM-7567","09/08/2016 12:14:23","Butler Repository fails to import mapper module","when a mapper is specified to the butler as a dot-delimited string (i.e. the module is part of the mapper specification), the importer fails. For example if you use   {code}  RepositoryArgs(mapper='lsst.daf.persistence.CameraMapper')}}  You get the output  {{Traceback (most recent call last):    File ""tests/testImportMapper.py"", line 52, in test      butler = dafPersist.Butler(outputs=repositoryArgs)    File ""/Users/n8pease/2/lsstsw/build/daf_persistence/python/lsst/daf/persistence/butler.py"", line 285, in __init__      self._addRepo(args, inout='out', defaultMapper=defaultMapper, butlerIOParents=butlerIOParents)    File ""/Users/n8pease/2/lsstsw/build/daf_persistence/python/lsst/daf/persistence/butler.py"", line 382, in _addRepo      repo = Repository(cfg)    File ""/Users/n8pease/2/lsstsw/build/daf_persistence/python/lsst/daf/persistence/repository.py"", line 88, in __init__      self._initMapper(repositoryCfg)    File ""/Users/n8pease/2/lsstsw/build/daf_persistence/python/lsst/daf/persistence/repository.py"", line 108, in _initMapper      module = __import__(module)  ImportError: No module named CameraMapper  {code}  The module that contains the CameraMapper ({{lsst.daf.persistence}}) should be imported, not the module+class name.",1
"DM-7576","09/08/2016 16:00:24","Set default detection threshold to 5.0-sigma if decorrelation is turned on","The current logic to set detection.thresholdValue to 5.0 if doDecorrelation == True is set in setDefaults(), and so never runs. This block needs to be moved elsewhere (doValidate, perhaps?) so that it runs correctly.",2
"DM-7578","09/08/2016 16:59:02","Create obs_base and make obs_test work with it","To support broad testing of obs packages, with minimal configuration, we will add an {{obs_base}} package that all obs packages will depend on. It will contain a test suite for them to derive from, plus frameworks for other modules (e.g. {{makeSomeCameraRepository.py}}).    {{obs_test}} will be the demo for how this new module works. Implementation details will be worked out along the way.",8
"DM-7584","09/09/2016 13:49:22","Fix missing code lines in doxygen page on using masked image locators","The doxygen page   https://lsst-web.ncsa.illinois.edu/doxygen/xlink_master_2015_11_10_02.53.27/masked_image_locators.html  describes how to use image locators to move around images. Unfortunately, the page is missing the actual code lines that are being discussed. Fix the broken/missing code lines on this page.",0.5
"DM-7586","09/10/2016 08:31:20","add lsst_py3 job to jenkins","The new {{lsst_py3}} metapackage enumerates eups products known to be compatible with python3.  A new dedicated jenkins job has been requested to catch regressions.",0.5
"DM-7587","09/12/2016 09:44:36","Add timesys argument to DateTime constructor to toString (implement RFC-219)","Implement RFC-219 as follows:    - DateTime(string, timescale) with ""Z"" required for UTC and forbidden for TAI. Ideally forbid ""23:59:60"" leap seconds for TAI as well.  - DateTime.toString(timescale) with ""Z"" produced for UTC and not for TAI.    Require the timescale argument, unless it proves to be a major headache by affecting too much code, in which case default to UTC for backwards compatibility.    Additional tests that verify that ""23:59:60"" strings are not produced by toString(TAI) at actual leap seconds.",3
"DM-7588","09/12/2016 09:46:13","Update MPICH","Version 3.2 of MPICH fails multiple tests on OS X and does not integrate well with mpi4py. Presently the newer 3.3a development version performs better and when the stable version 3.3 is released we should upgrade the stack.",2
"DM-7590","09/12/2016 11:35:52","SUIT vision document","SUIT vision document work, review, revision, and submission for approval by TCT. ",2
"DM-7593","09/12/2016 15:43:28","Make use of the phase folded code","Now that DM-7165 is done, use the code in the LC handler to complete the processor added [DM-7162]",1
"DM-7594","09/12/2016 17:46:23","Extra phase column is displayed with too few precision","DM-7165 allow to add an extra column ('phase') to the raw LC table, which result in the phase folded table.    This is done using DataType object.     When the result is shown in table view, the display format is '0.0' which is not enough.    The phase column should be displayed as double precision with at least 8 significant digits    Also, add attribute to the extra phase column so a comment or some description appears in the ipac table header when is saved by the user if possible.",2
"DM-7595","09/12/2016 17:59:21","Extra column doesn't appear in the header description when saving ipac table","Add attribute to the extra phase column so a comment or some description appears in the ipac table header when is saved by the user if possible    NOTE: this ticket is combined with DM-7594.",0
"DM-7604","09/13/2016 13:07:07","file() in lsst.cat should be open()","The lsst.cat package has a reference to file(), which is not supported in Python 3.  Updating this to use open().",1
"DM-7608","09/13/2016 15:33:04","testSelectSdssImages.py broken: ""noConnectionStr"" not defined","A recent change to testSelectSdssImages.py broke the test for those who could connect to the database. The symptom is:  {code}  NameError: name 'noConnectionStr' is not defined  {code}  and is self descriptive. I have a fix ready to go.",0
"DM-7610","09/13/2016 15:49:54","Implement Target Panel resolve option  (ned then simbad or simbad then ned)","We have not implement the target panel resolve option.  When is there in the search panels now are dummy ui components.",1
"DM-7614","09/14/2016 11:13:28","Support pytest in coadd_chisquared tests","Modernize the test file to be compatible with pytest.",0
"DM-7615","09/14/2016 11:15:54","Update tests in shapelet to support pytest","The tests in {{shapelet}} need to be modernized to support pytest and to drop {{suite}}.",1
"DM-7622","09/14/2016 14:55:48","DateTime rejects an acceptable date","The following fails for reasons I do not understand:    {code}  from lsst.daf.base import DateTime  # for most versions of daf_base  DateTime(""1969-12-31T23:59:59Z"")  # if using the latest master of daf_base  DateTime(""1969-12-31T23:59:59Z"", DateTime.UTC)  {code}  The error is:  {code}    File ""src/DateTime.cc"", line 391, in lsst::daf::base::DateTime::DateTime(int, int, int, int, int, int, lsst::daf::base::DateTime::Timescale)      Unconvertible date: 1969-12-31T23:59:59 {0}  lsst::pex::exceptions::DomainError: 'Unconvertible date: 1969-12-31T23:59:59'  {code}  and comes from this code:  {code}  dafBase::DateTime::DateTime(int year, int month, int day,                              int hr, int min, int sec, Timescale scale) {          struct tm tm;      tm.tm_year = year - 1900;      tm.tm_mon = month - 1;      tm.tm_mday = day;      tm.tm_hour = hr;      tm.tm_min = min;      tm.tm_sec = sec;      tm.tm_wday = 0;      tm.tm_yday = 0;      tm.tm_isdst = 0;      tm.tm_gmtoff = 0;        time_t secs = timegm(&tm);            // long long nsecs will blow out beyond sep 21, 1677 0:00:00, and apr 12 2262 00:00:00      // (refering to the values of EPOCH_IN_MJD +/- MAX_DAYS ... exceeds 64 bits.)      // However, a tm struct is only 32 bits, and saturates at:      //    low end - Dec 13 1901, 20:45:52      //    hi end  - Jan 19 2038, 03:14:07            if (secs == -1) {          throw LSST_EXCEPT(                            lsst::pex::exceptions::DomainError,                            (boost::format(""Unconvertible date: %04d-%02d-%02dT%02d:%02d:%02d"")                             % year % month % day % hr % min % sec).str());      }  {code}  ",2
"DM-7626","09/14/2016 16:31:55","Modernize dax_webservcommon tests to work with pytest","Support pytest.",0.5
"DM-7629","09/14/2016 16:35:11","Modernize dax_metaserv to support pytest","Support pytest.",1
"DM-7630","09/14/2016 18:46:19","Dispaly ""null"" when a value ""null"" is in the data","Allow null values to display as ""null"", currently a value of null is left blank. ",2
"DM-7651","09/15/2016 12:14:18","Clarify in dm_dev_guide that SSH keys must be in one-line format","The [instructions for using public-key authentication with lsst-dev|https://developer.lsst.io/services/lsst-dev.html#lsst-dev-ssh-keys] assume the user has OpenSSH. While this does not prevent users from translating the instructions to their own SSH software, it does cause a confusing bug if the user isn't aware that OpenSSH (or, specifically, the {{~/.ssh/authorized_keys}} file) only accepts one-line keys and not the more common multi-line PEM format.    I propose that a note be added to the end of ""1. Generate a key pair"" warning non-OpenSSH users to make sure their public key is in the one-line format.",1
"DM-7652","09/15/2016 13:00:54","Replace usage of execfile in astrometryNetDataConfig.py","{{astrometryNetDataConfig.py}} gained a {{from past.builtins import execfile}} during the python3 conversion. We should replace this with a more modern syntax.",1
"DM-7655","09/16/2016 11:28:34","The region with semicolon inside text property is not displayed","Region description is not parsed well in case there is character ';' contained in 'text', 'tag' (the property which has text string value). ",3
"DM-7662","09/17/2016 13:09:34","Prepare and deliver a documentation/technical writing talk at the invitation of NCSA.","The talk is archived at https://zenodo.org/record/153867#.V92VODvsXPg and was given on September 12, 2016.",1
"DM-7665","09/18/2016 15:46:46","plan to choose the context background image for LSST data","As we search and display the LSST sources/objects, the SUIT should be able to choose a sensible background image to overlay the sources/objects on.     We need to come up with a strategy and the potential images to choose from at different stages:  *  PDAC v1 (end of F16): Current method that Firefly employs to choose. Depending on size needed, it will pick one from 2MASS, WISE, SDSS, IRAS.   * PDAC v2 (end of S17)  * Ready ComCAM  ",2
"DM-7668","09/19/2016 08:08:08","Remove shapelet code from meas_algorithms","Implement RFC-223.",2
"DM-7669","09/19/2016 08:09:16","Remove GPU warping, convolution, and support code","Implement RFC-224.",2
"DM-7674","09/19/2016 11:35:39","New workflow for onboarding DM members into community.lsst.org groups","Previously we made every T/CAM a community.lsst.org admin so that they could add their team members to the LSST and LSSTDM groups to access private community categories.    Discourse now has the concept of Group owners, who can add/remove members from individual groups.    I’d like to limit the number of people with Admin credentials since it poses a security risk; especially considering our responsibility to support private Science Collaboration categories.    This ticket will update community configurationg and Developer Guide documentation to implement this.",0.5
"DM-7676","09/19/2016 14:22:10","lsof used by setup.csh not always in /usr/sbin","The {{setup.csh}} script uses {{/usr/sbin/lsof}} to detect the location of the script if indirectly sourced.  On a stock Ubuntu 14.04 system (which is not an officially supported platform but on which the stack works otherwise), it appears that this utility is installed as {{/usr/bin/lsof}} instead.  Making this usage slightly more generic should help improve portability.",1
"DM-7677","09/19/2016 14:33:24","Create RFC documents for changes to Mapper.paf files","In DM-7049, an set of datasets were moved from the HSC mapper to daf_butlerUtils.  These are in exposures.yaml and datasets.yaml, and are now available to all cameraMapper subclasses (provided that there is an exposures or datasets section in their own Mapper.paf file.)    An RFC will be published for each Mapper describing possible additional changes:    1.  Additional cleanup is required in each of the obs_*/policy/*Mapper.paf files to fully implement this move.  Datasets which are not consistent with the new ""shared"" datasets need to be altered before they can be removed, but carefully, so as not to disturb the code which utilizes the mapper.    2.  Identify datasets in each mappers which have the same function as a shared dataset, but a different name.  If possible, these should be renamed and made consistent with the shared dataset.    3.  Attempt discover datasets which are no longer in use and could be deleted.",3
"DM-7679","09/19/2016 16:56:57","XrdSsiClient library compilation error with gcc -v 6.2 in Ubuntu 16.10","[~speckins] tried building qserv under Ubuntu 16.10, which ships with gcc -v 6.2 . I was able to reproduce the error with cmake on the xrdssi branch as follows:    {code}  Scanning dependencies of target XrdSsiLib  [ 77%] Building CXX object src/CMakeFiles/XrdSsiLib.dir/XrdSsi/XrdSsiClient.cc.o  In file included from /home/qserv/src/xrootd/src/XrdSsi/XrdSsiClient.cc:42:0:  /home/qserv/src/xrootd/src/XrdSsi/XrdSsiClient.cc: In member function 'virtual XrdSsiService* XrdSsiClientProvider::GetService(XrdSsiErrInfo&, const char*, int)':  /home/qserv/src/xrootd/src/./XrdSsi/XrdSsiAtomics.hh:45:34: error: request for member 'load' in 'XrdSsi::initDone', which is of non-class type 'bool'   #define Atomic_GET(x)          x.load(std::memory_order_relaxed)                                    ^  /home/qserv/src/xrootd/src/XrdSsi/XrdSsiClient.cc:122:8: note: in expansion of macro 'Atomic_GET'     if (!Atomic_GET(initDone))          ^~~~~~~~~~  src/CMakeFiles/XrdSsiLib.dir/build.make:62: recipe for target 'src/CMakeFiles/XrdSsiLib.dir/XrdSsi/XrdSsiClient.cc.o' failed  make[2]: *** [src/CMakeFiles/XrdSsiLib.dir/XrdSsi/XrdSsiClient.cc.o] Error 1  CMakeFiles/Makefile2:1224: recipe for target 'src/CMakeFiles/XrdSsiLib.dir/all' failed  make[1]: *** [src/CMakeFiles/XrdSsiLib.dir/all] Error 2  Makefile:127: recipe for target 'all' failed  make: *** [all] Error 2  {code}",1
"DM-7681","09/19/2016 17:42:52","Begin the documentaiton process of visualization","Begin the visualization documentation process.  This is the first step of probably much more documentation.  However, some needs to be put into jsdocs soon so this ticket should be that documentation.",2
"DM-7687","09/20/2016 10:41:49","Split HSC comparison/QA script into separate package","Currently, it lives as all the scripts starting with {{hsc}} in branch {{u/lauren/working}} of {{pipe_tasks:bin.src}}. These should all move to a new package in the lsst-dm organization.",1
"DM-7691","09/20/2016 13:48:14","Shared stack in NFS /lsst4 shows error at initialization","The NFS stack created in DM-6968 today shows an error upon initialization :    {code}  % cd  /lsst4/lsstsw/stack/  % source loadLSST.bash  setup: [Errno 13] Permission denied: '/nfs/lsst4/lsstsw/stack/.lockDir/shared-daues.801175'  You are attempting to run ""setup"" which requires administrative  privileges, but more information is needed in order to do so.  Authenticating as ""root""  Password:  {code}    This may be occurring due to the existence of a lock and .lockDir :    {code}  %  ls -ld /nfs/lsst4/lsstsw/stack/.lockDir/  drwxr-xr-x 2 lsstsw ac 4096 Sep 19 04:49 /nfs/lsst4/lsstsw/stack/.lockDir/  % ls -ld /nfs/lsst4/lsstsw/stack/.lockDir/*  -rwxr-xr-x 1 lsstsw ac 0 Sep 14 00:36 /nfs/lsst4/lsstsw/stack/.lockDir/shared-lsstsw.2531683  {code}    Could this be investigated ?                  Greg   ",0.5
"DM-7693","09/21/2016 10:20:22","fix premature merge of previous ctrl_orca ticket","When I went to look for the pull for ctrl_orca on Monday for ticket DM-7183, I didn't see one;  I made changes to the ""is True"", ""is False"", and list(keys()) issues, and then didn't look again on Tuesday when I saw the ""Review complete"", so I thought it had been done, which is was not.   This is to look at those issue and fix them.",2
"DM-7694","09/21/2016 10:24:04","Restore a couple of datasets from DM-7049","DM-7049 removed at least one dataset which might be in use.    Restore any doubtful datasets to the Mappers and move them to the DM-7677 RFC.  ",1
"DM-7700","09/21/2016 17:10:25","Atlas table filtering doesn't work properly","Atlas result page is using Firefly table component but couple of problems have been detected:    - Filtering the table doesn't work properly.    Step to reproduce:  Go to http://irsawebdev1.ipac.caltech.edu/applications/Atlas, do a search on SEIP.  Then try to filter 'Instruments' column on 'mips'. The result is wrong. I see 'IRAC' rows.    ",2
"DM-7701","09/21/2016 17:18:25","conda-lsst ndarray test failure","The conda-lsst build log for the test failure can be found here:    https://gist.github.com/jmatt/dcb6ce8312aac1ff81cbb923703d8430    {code}  Running tests...  Test project /Users/square/dev/conda-lsst/miniconda/conda-bld/work/build      Start 1: test_ndarray  1/5 Test #1: test_ndarray .....................   Passed    0.01 sec      Start 2: test_views  2/5 Test #2: test_views .......................   Passed    0.01 sec      Start 3: test_ndarray-fft  3/5 Test #3: test_ndarray-fft .................***Exception: Other  0.00 sec      Start 4: test_ndarray_eigen  4/5 Test #4: test_ndarray_eigen ...............   Passed    0.01 sec      Start 5: swig_test  5/5 Test #5: swig_test ........................   Passed    0.11 sec    80% tests passed, 1 tests failed out of 5    Total Test time (real) =   0.14 sec    The following tests FAILED:     3 - test_ndarray-fft (OTHER_FAULT)  Errors while running CTest  make: *** [test] Error 8  {code}",1
"DM-7702","09/21/2016 18:14:13","stretch dropdown feature is currently broken","The stretch feature is broken.    The log shown are related to getRangeValues error:    Uncaught TypeError: (0 , _PlotViewUtil.primePlot)(...).plotState.getRangeValues is not a function    Please investigate and fix. Thanks.",0.5
"DM-7706","09/22/2016 10:04:39","Fix issues related to tomcat8 and ipv6. Also add firefly.war into firefly release page.","- Fix duplicate nom.tam.fits classes during build  - Add preferIPv4Stack to tomcat env.  - Add firefly.war to Firefly Standalone release page.",3
"DM-7710","09/22/2016 11:34:30","afw.display.makeMosaic doesn't understand self.images","The routine makeMosaic doesn't work if you pass in a list of images (as opposed to appending images to self)  ",1
"DM-7716","09/22/2016 16:59:10","Fix dataset lookup in multibandDriver.py","{{MultibandDriver}} builds a patch reference list by checking for the existence of {{self.config.coaddName + ""Coadd""}}.  However, when coadd assembly is performed by running {{coaddDriver}}, this coadd is not persisted in favor of only persisting the updated calexp version.  Thus, currently {{MultibandDriver}} ends up with and empty patch list and exits quietly.  The existence check should be on the {{self.config.coaddName + ""Coadd_calexp""}}.",0.5
"DM-7725","09/23/2016 13:35:35","Fix missing lsst.log update and build dependencies","There was one missed instance of updating to the new {{lsst.log}} system and the remaining {{pex_logging}} dependencies are not reflected in the {{ups/}} table and cfg files (but are being pulled in by {{meas_algorithms}}).",0.5
"DM-7726","09/23/2016 15:05:45","Fix MySQL Incorrect date time value warning in ctrl_stats","in Mysql 5.7, the warning    /home/srp/lsstsw27/lsstsw/stack/Linux64/cat/12.1.rc1-3-g00b8a9c+1/python/lsst/cat/MySQLBase.py:158: Warning: Incorrect datetime value: '0000-00-00 00:00:00' for column 'executionStartTime' at row 1    is emitted.    This is because of a change in MySQL 5.7 about how '0000-00-00 00:00:00"" dates are handled.",1
"DM-7738","09/26/2016 11:51:28","Remove outdated instructions in obs_decam README","{{obs_decam}} README has outdated information, leading to confusion like [this|https://community.lsst.org/t/error-of-using-obs-decam/1181/1]. Remove the outdated instructions to avoid confusion. For example, {{processCcdDecam.py}} is no longer used since DM-4692 (https://community.lsst.org/t/backward-incompatible-changes-to-processccdtask-and-subtasks/581) ",0.5
"DM-7739","09/26/2016 12:32:17","Docstring improvements in Firefly Python API","The docstrings for FireflyClient are very helpful overall. That said, I found myself tripping over a few items that could be clarified for users.    In add_mask:  * It would be helpful to mirror the order of the parameters with the function signature, i.e. in the Parameters section, please list bit_number first and then image_number.  * Please indicate that image_number is zero-based, e.g. for a file of image, mask, variance, the user will want to specify image_number=1.    In show_fits:  * Under additional_params, it would be helpful to note that the user can supply MultiImageIdx to display a particular image extension from the file, and that this is a zero-based index. (The link to parameters does not list MultiImageIdx btw.)",1
"DM-7741","09/26/2016 13:51:27","Missed incompatible logging changes in Python + migrate meas_astrom","Since DM-6999, Python codes using Task/CmdLineTask are converted to use {{lsst.log}} instead of {{lsst.pex.logging}}.  Codes like {{self.log.log(self.log.WARN, ""messages"")}} should be changed to {{self.log.warn(""messages"")}} because that API is no longer supported. Many places need such changes but were missed.  This ticket is to go through the Python codes of pipe_\*, ip_\*, meas_\* and obs_\* and correct them. ",1
"DM-7745","09/26/2016 17:34:26","enum constants in daf_base.DateTime can cause confusion","Whilst looking at DM-7742 I started to wonder why the {{DateTime}} constructor did not complain when it was given an MJD time system instead of a UTC time scale.    It seems we have two issues with {{DateTime}} that can lead to subtle bugs not being caught:    # Both the TimeScale and DateSystem enums share values in Python land. This means that in Python you can pass in an MJD but C++ will treat it as UTC. We should ensure that both these enums convert to different sets of integers in Python so that we can trap system/scale confusion.  # The DateTime constructor does not check all cases. It assumes that if the scale is not  TT or TAI then the user probably meant UTC. The code in DateTime should never use an {{else}} to assume UTC but should always check and trigger and exception if the value is not recognized.    ",1
"DM-7754","09/27/2016 08:54:46","Can't use simple afwImage primitives without explicitly initialising log4cxx","This simple script:  {code}  import lsst.afw.image as afwImage    exp = afwImage.ExposureF(1, 1)  {code}    generates the error:  {quote}  log4cxx: No appender could be found for logger (afw.image.Mask).  log4cxx: Please initialize the log4cxx system properly.  {quote}    I'd expect to be able to use LSST primitives without explicitly setting up the logging.  ",1
"DM-7756","09/27/2016 09:30:43","Port pipe_drivers to Python 3","{{pipe_drivers}} needs to be converted to python 3.",0.5
"DM-7760","09/27/2016 11:38:16","WCS match does not work right when the images are not initially locked","WCS match is not working immediately when the images start out as not locked. It requires a second uncheck and check to make it work.    Bug was found with following description below.  This ticket fixes the WCS matching part of the problem.      The rotation not centering is another problem that I can't quite repeat but is probably not related to WCS-match. We have other work for the rotation (DM-7336) that might help.      -----------------------------------------------------------    1. Go to the Spitzer Enhanced Imaging Products:    http://irsawebdev1.ipac.caltech.edu/data/SPITZER/Enhanced/SEIP/    2. Try the last example, CGCG 036-024, with size = 0.1 and images must cover coordinate checked.    3. In the results, I cannot seem to sort by distance. However, I can now   filter by wavelength. Filter wavelength = 24 and type of data = science.   Only 2 results should remain. Send both to the IRSA Viewer.    4. On the second image, zoom in 6x and center target in display. Click WCS Match.    Nothing happens. (I waited 6 minutes.)    5. Turn WCS Match off and on again. It works.    6. Turn WCS Match off. Rotate the second image using 30 degrees.    The target moves off the display instead of just rotating.    7. Recenter the target.    Works.    8. Click WCS Match on again.    The first image target goes off the screen instead of just rotating.    9. Click on the first image, and recenter target.    Works.",0.5
"DM-7762","09/27/2016 12:02:32","Understand PDAC data base enough to determine the queries we need to do.","This story involves understanding the data we need to access for the PDAC UI.     * Understand the two applications we want to make for the PDAC: general catalog search and light curve viewer  * Understand the data base searches we need to do.  * Analyze the tables and data we need for the two applications.  * Come up with example select statements for each search.  * Try to make the the related DAX URL for each search. (We might need to make a second ticket to really work this last issue)  * Get some example fits files and put the into firefly_test_data    The scope grew and more work in documenting the difference of qserv_* functions and scisql_* functions. ",20
"DM-7765","09/27/2016 12:53:07","Rendering region text on PNG ","The region text defined with  region object (not text region) is not shown on PNG.   ",2
"DM-7769","09/27/2016 15:25:50","duplicate imports in CharacterizeImageTask","A few duplicate imports snuck into CharacterizeImageTask in a recent change.    It's a one-liner to fix. However, pep8 also picked up the fact that lines 565-569 are indented too far. Might as well fix that as well and run flake8 on all python code.",1
"DM-7770","09/28/2016 09:44:32","RingsSkyMap.findAllTracts fails with wcslib error","Johnny Greco (Princeton) points out an exception when using {{RingsSkyMap.findTractPatchList}}:  {code}  RuntimeError                              Traceback (most recent call last)  <ipython-input-95-53f3ab207e06> in <module>()  ----> 1 skymap.findTractPatchList(coordlist)    /Users/protostar/anaconda/envs/lsst/opt/lsst/skymap/python/lsst/skymap/ringsSkyMap.pyc in findTractPatchList(self, coordList)      211         retList = []      212         for coord in coordList:  --> 213             for tractInfo in self.findAllTracts(coord):      214                 patchList = tractInfo.findPatchList(coordList)      215                 if patchList and not (tractInfo, patchList) in retList:    /Users/protostar/anaconda/envs/lsst/opt/lsst/skymap/python/lsst/skymap/ringsSkyMap.pyc in findAllTracts(self, coord)      194         for entry in [0,len(self)-1]:      195             tract = self[entry]  --> 196             if tract.getBBox().contains(afwGeom.Point2I(tract.getWcs().skyToPixel(coord.toIcrs()))):      197                 tractList.append(tract)      198     /Users/protostar/anaconda/envs/lsst/opt/lsst/afw/python/lsst/afw/image/imageLib.pyc in skyToPixel(self, *args)     8691         skyToPixel(Wcs self, Coord coord) -> Point2D     8692         """"""  -> 8693         return _imageLib.Wcs_skyToPixel(self, *args)     8694      8695     def skyToIntermediateWorldCoord(self, *args):    RuntimeError:     File ""src/image/TanWcs.cc"", line 313, in virtual geom::Point2D lsst::afw::image::TanWcs::skyToPixelImpl(geom::Angle, geom::Angle) const      Error: wcslib returned a status code of 9 at sky 30.18, -3.8 deg: One or more of the world coordinates were invalid {0}  lsst::pex::exceptions::RuntimeError: 'Error: wcslib returned a status code of 9 at sky 30.18, -3.8 deg: One or more of the world coordinates were invalid'  {code}    I believe it's because wcslib can fail nastily if coordinates are well off an image (e.g. [here|https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/selectImages.py#L196-L199]).",0.5
"DM-7781","09/28/2016 17:40:46","JUnit Test for Zscale","The Zscale class is used by FitsRead.  Therefore, it is important to make sure it works correctly all the time.  The unit test is needed to identify any bugs introduced by changing this class.",8
"DM-7784","09/29/2016 01:30:38","WISE single exposure image fails sometime to be plotted in LC viewer","in the LC prototype, once the phase folded table is displayed, one can highlight a row and the corresponding single exposure will be plotted based on the 'frame_id'.    For some reason, the plot request fails on 'frame_id' starting with 09xx in the example used.    Step to reproduce:  Go to LC viewer (lc.html) and use raw table from:   http://web.ipac.caltech.edu/staff/ejoliet/demo/OneTarget-27-AllWISE-MEP-m82-2targets-10arsecs.tbl  Then, click for example on the row where 'frame_id' == 09401b071, see plot fails.     Although FITS exists in IBE:  http://irsa.ipac.caltech.edu/ibe/data/wise/merge/merge_p1bm_frm/1b/09401b/071/09401b071-w1-int-1b.fits    Please investigate and fix.",3
"DM-7798","09/29/2016 14:25:09","Add remote launch capabilities to the Time Series viewer","Do the following:     * Add hooks so the time series tool to be able to be launched externally   * Gator or the triview (or something else) should be able to launch the LC with search parameters (see my comment below about initial values for IRSA)",2
"DM-7801","09/29/2016 14:31:32","Wrap afw::cameraGeom with pybind11","The generated wrappers will live parallel to the Swig wrappers. This ticket only covers the C++ wrappers themselves, not the Python layer on top (which will continue to use the old wrappers) all work will stay on a separate branch and will not be merged to master until DM-6168 is complete.    The tests included in this ticket are:  # testDetector.py  # testMakePixelToTanPixel.py  # testExposure.py  # testCamGeomFitsUtils.py  # testCameraGeom.py  # testDistortedTanWcs.py  # testCameraTransformMap.py  # testColor.py  # testOrientation.py  # testCameraSys.py  # testWarper.py (moved to DM-8619)",8
"DM-7805","09/29/2016 16:40:25","Changing column to plot doesn't reset the 'flip' option to unchecked state","When doing a catalog search (or Gator), the x-axis in the xy-plot comes in 'reverse' direction which is wanted for plotting ra vs. dec.    But when the column is changed, the option should be reset to regular x-axis direction ('reverse' checkbox to be unchecked).    Please fix.",1
"DM-7811","09/29/2016 22:13:08","WBS restructure discussion","participate discussion of WBS restructuring,  review and edit",2
"DM-7812","09/29/2016 22:19:32","2D plotting package design discussion","Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",2
"DM-7813","09/29/2016 22:20:18","Organize the 2D plotting package design discussion","Organize in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",3
"DM-7814","09/29/2016 22:22:10"," 2D plotting package design discussion","Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",3
"DM-7815","09/29/2016 22:23:24"," 2D plotting package design discussion","Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",3
"DM-7816","09/29/2016 22:24:02","2D plotting package design discussion","Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",3
"DM-7817","09/29/2016 22:25:29","2D plotting package design discussion","Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",1
"DM-7818","09/29/2016 22:26:10","2D plotting package design discussion","Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",1
"DM-7819","09/29/2016 22:26:45"," 2D plotting package design discussion","Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",1
"DM-7820","09/29/2016 22:27:20"," 2D plotting package design discussion","Participate in the 2-day discussion of 2D plotting package design,  functions supported, and strategy for introducing new 3rd party software packages,",1
"DM-7823","09/30/2016 10:39:43","meeting with SLAC camera team for visualization discussion ","SUIT team members Trey, Tatiana, Loi, and Gregory traveled to SLAC to meet with Tony and Stuart for a 2-day discussion of the relevant issues in using Firefly for camera I&T visualization support. We came up with a rough design which defines the interface between the camera diagnostic cluster and Firefly server, the services that camera team will provide so  Firefly can display the images at different resolutions as needed.    ",8
"DM-7824","09/30/2016 11:20:36","Document the Histogram API and fix some minor bugs","When exposing Histogram React Component to API and writing the demo, I found the following bugs:    - log scale for y axis fails  - tooltip might be misaligned with bin boundaries   ",2
"DM-7825","09/30/2016 11:35:18","Large catalog fail to completely load after filter was applied.","MAJOR: imposing filters on the catalog makes the plot go away.   (1) Gator. Search in WISE. All WISE. M16. 30 arcmin  (2) In the data table, enable filters. Type “>10” in each of the w1snr, w2snr, w3snr, w4snr column filter boxes. do NOT tab across to get them; it messes up the column headings.  (3) the plot never comes back, the overlays never update on the image, and the “circular cyclical dots” indicating “I’m thinking” never stops. As in, I did this, reproduced this, typed this up, got distracted by a text, checked email in different accounts, came back to this, and still no updates.   Reproducibility:  restarted tool both platforms. WISE/AllWISE/M16/30 arcmin cone.  both come back with 17,816 sources.  impose filters.  plot vanishes as soon as first filter imposed. never returns from all four filters being imposed. Firefox is tilling me 695 sources are left (with the “circular cyclical dots” still spinning) and Chrome is telling me 979 sources are left  (with the “circular cyclical dots” still spinning).  another test:   restarted tool both platforms. WISE/AllWISE/M16/30 arcmin cone.  both come back with 17,816 sources.  impose JUST filter on w1snr. >10  Firefox: 3823 sources, plot vanishes, overlay doesn’t update, “circular cyclical dots” still spinning  Chrome: 3609 sources, plot vanishes, overlay doesn’t update, “circular cyclical dots” still spinning  So, this problem exists for one filter AND more than one filter.    *Copied from the pull request by Loi (9/30/2016)*  We were using cookies to pass websocket connection info to the server. This approach does not work in and embedded mode, ie. Gator.  I've converted to use custom http headers instead. This also affects python api. I've modified the code to reflect the changes.    This change fixed the above problem and should not have any regression issue",2
"DM-7830","09/30/2016 12:38:30","display single exposure from raw table as well as the phase folded curve table","Please, add the same feature to the raw-table highlight as the phase folded table.    When the raw-table LC is loaded and a row is highlighted, display the image single exposure.    ",2
"DM-7840","09/30/2016 18:22:29","Change the way the single exposure request is built from LC table to be generic","Right now, the LC viewer knows how to fetch the single exposure by hard-coding a WebPlot request (plus some column to use) which is specific to WISE and has a particular image set that doesn't necessarily correspond to the time of the observation (see problem describe in DM-7784.    The single exposure to be displayed should come from and contained by the table itself. The LC viewer shouldn't be aware about the mission, image set or any other specific logic.     Change the way to fetch the single exposure by reading out the URL from the table (a column for each time identified by the flux as a prefix of the column name will be present in the table) and build a WebPlotRequest based on a URL fetch instead.    This method is generic for any data-set such as PTF, WISE, IRTF, etc. The generator of the table is responsible for giving the unique way to fetch the single exposure for a particular time and band.    For example, in case of WISE MEP table, it will contain 4 columns, each of them will have a name such as 'w1_url', 'w2_url', etc. When the user select the flux column to be used by the periodogram/xy-plot, for example 'w1mpro_ep', the image URL request to be used will come from the column which prefix name matches the user selected flux column, in the example, it will be 'w1_url'. The url will be an IBE call to WISE like:    http://irsa.ipac.caltech.edu/ibe/data/wise/allsky/4band_p1bm_frm/7b/00717b/102/00717b102-w2-int-1b.fits      ",8
"DM-7844","10/01/2016 08:01:48","cmd-line tasks should log the command being executed","When a command-line task is executed it would be very helpful to have the command logged. This helps diagnose issues with packages such as {{ci_hsc}} and {{validate_drp}}.    This is trivial when the task is executed from the command line, e.g. add the following to {{ArgumentParser}}:  {code}  log.info(""Running: {}"".format("" "".join(sys.arvg)))  {code}  It is a bit trickier if the command is executed by code that calls {{CmdLineTask.parseAndRun}} directly; the arguments are readily available but determining the task will require introspection.",1
"DM-7855","10/03/2016 12:59:07","Propose new WBS breakdown for 02C.04","Develop a new WBS describing work to be undertaken in 02C.04.",2
"DM-7858","10/03/2016 13:05:44","Develop DRP planning packages","Develop planning packages suitable for loading into PMCS which reflect the DRP plan.",8
"DM-7860","10/03/2016 13:08:52","Produce straw-man plan for Calibration Products Pipeline ","Using the    * Understanding developed in DM-7560;  * Dependency map developed in DM-7558;  * Responsibility assignments developed in DM-7031;  * Rough resource requirements developed in DM-7032, DM-7034;  * Tooling decided in DM-7559;    develop an outline of a plan for Calibration Products Production.",5
"DM-7862","10/03/2016 15:21:25","Update StarFast functionality","A few components and functions of the StarFast simulator have fallen behind the needs of DM-6245. In particular, the exposures created by the simulator are missing needed metadata. This ticket is to clean up the interface to better support the current uses, and to supply the missing metadata.",2
"DM-7869","10/04/2016 09:52:44","virtualDevice assumes that the display has a .frame member","When running in verbose mode the virtualDevice assumes that it can access {{self.display.frame}}; this isn't always true.  ",1
"DM-7886","10/04/2016 14:42:06","Replace pyfits with astropy.io.fits in all code","We currently use {{pyfits}} in multiple packages: afw, coadd_chisquared, obs_base, meas_astrom, meas_deblender, meas_extensions_psfex, meas_mosaic, obs_cfht, obs_lsstSim, obs_sdss, obs_subaru and obs_test.    Strangely, we only have explicit dependencies on {{pyfits}} listed for afw, obs_base, galsim, healpy and obs_subaru.    galsim can use astropy.io.fits or pyfits. healpy really does seem to not work with astropy.io.fits -- is there a newer version that does?    Please replace {{pyfits}} with {{astropy.io.fits}} where appropriate and update the table files to correctly express the dependency (and removing {{pyfits}} where inappropriate).  ",2
"DM-7889","10/04/2016 17:39:53","Activate HSC afterburner functionality","We have just merged the functionality of the HSC afterburner (DM-6784, DM-6785).  We need to add configuration options in obs_subaru to activate the features in the coadds.",5
"DM-7892","10/04/2016 18:28:38","Transfer relevant C++ doc guidelines from Confluence to Developer Guide","Investigation supporting RFC-225 revealed that some rules in the [Confluence documentation guidelines|https://confluence.lsstcorp.org/display/LDMDG/Documentation+Standards] (e.g., the use of @ for Doxygen tags, or placing all documentation at the point of declaration) are not present in the developer guide. Ensure all relevant rules are present in the final documentation (senior developers have the final word on which rules are relevant).",2
"DM-7893","10/04/2016 18:36:14","Add exception safety tag to Doxygen","To allow easier implementation of DM-7891, all DM projects' doxygen config files should include an alias {{@exceptsafe}} that expands to a paragraph with the heading ""Exception Safety"". The tag can be used to describe any guarantees made by documented code in the event of an exception.    -Yes, this requires touching (almost) every repository in the stack.-",2
"DM-7894","10/04/2016 19:33:06","mapper and butler queryMetadata method badly documented","The {{queryMetadata}} methods in both Mapper and Butler are very badly documented. Their docs both claim to accept a ""key"" parameter, but the methods don't take ""key"" (and Butler's version attempts to use one if ""format"" is not specified), while the ""format"" parameter (which appears non-optional from the code itself) doesn't have any docstring in Mapper. Looks like these docs rotted badly.",1
"DM-7896","10/05/2016 06:54:30","meas_extensions_convolved is undocumented","Please add at least a README file providing a short summary of its functionality and some bare-bones documentation on how to enable and use it.",2
"DM-7899","10/05/2016 12:24:33","Move datatypes for Indexed reference catalogs to daf_butlerUtils","The new indexed reference catalogs (for photometric and astrometric calibration) are stored under the dataset ""cal_ref_cat"". This dataset was only added to obs_lsstSim and obs_test, and did not make it to the rest of the cameras. Since this is independent of camera it makes the most sense to move this into the common datasets in daf_butlerUtils. (The other required datatype, ""IngestIndexedReferenceTask_config"", is already in daf_butlerUtils).    This is currently blocking usage of the new Gaia reference catalogs, so it would be very useful to have this implemented soon instead of waiting for the various dataset RFCs to close. Giving this to [~pgee] per his suggestion.    ",1
"DM-7900","10/05/2016 12:32:52","Add --batch-type None to possibilities, disabling any MPI","It can be desirable to run ctrl_pool enabled commands (such as constructBias.py from pipe_drivers) without any batch system.    Please add an option  bq. --batch-type None  that simply runs the job in the current process.  ",2
"DM-7905","10/05/2016 13:47:54","Finish Convert GWT code to pure JavaScript (F16)","Finish the remaining work for converting GWT code to pure JavaScript and fix the bugs found",100
"DM-7918","10/06/2016 12:09:08","constraints column is not rendered correctly if empty cell is displayed","[~cwang] found a problem in the catalog constraint search panel, the constraint column cell are replaced by disabled input field cell when empty value is entered. This component was working before, it must be a change recently in the TableRenderer.js.  We were talking with [~loi] and [~cwang], seems that [~cwang] was looking at it while doing the catalog search for LSST and could have a fix.    Please fix for release soon.",2
"DM-7921","10/06/2016 13:26:43","Experiment with multiple series histogram display","Multiple series histogram display (not connected to table)    Show how multiple histogram series can be displayed. Display multiple histogram data in an API.    (This is needed to understand how to organize multiple series data)",3
"DM-7938","10/10/2016 01:57:44","Re-install Qserv on PDAC ","Qserv master on PDAC has been re-install from scratch for security reasons:  https://jira.ncsa.illinois.edu/browse/LSST-801    Qserv needs to be re-installed and re-tested here once master node will be available.",2
"DM-7940","10/10/2016 10:27:33","Disable colourisation when not writing to a terminal","There's code in pex.config.history to colour output, but it's en/disabled unconditionally.  Please change it to never colour text that isn't going to the terminal.  ",0.5
"DM-7941","10/10/2016 11:15:24","Fix config for reference object loader","{{meas_mosaic}} if failing to un-persist the source matches for the HSC NB0921 filter with:  {code}   WARNING: Failed to read DataId(initialdata={'taiObs': '2015-03-16', 'pointing': 1170, 'visit': 23046, 'dateObs': '2015-03-16', 'filter': 'NB0921', 'field': 'SSP_UDEEP_COSMOS', 'tract': 0, 'ccd': 41, 'expTime': 900.0}, tag=set([])): Could not find flux field(s) N921_camFlux, N921_flux  {code}    It seems the filterMap is not being loaded properly.  Please fix this.",1
"DM-7947","10/10/2016 14:29:49","Unit test for Circle class","This is one of series unit test ticket for package ""package edu.caltech.ipac.visualize.plot"".",1
"DM-7948","10/10/2016 17:04:42","misc. bug fixes related to Gator/Atlas/irsaviewer - feedbacks from test team","address the bugs reported by the test team",2
"DM-7949","10/10/2016 17:19:59","Delete daf_butlerUtils and move obs_base into lsst","Now that the daf_butlerUtils -> obs_base move is complete, we need to move obs_base to live in the lsst organization (it's currently in lsst-dm). github provides a ""transfer repository"" mechanism:    https://help.github.com/articles/transferring-a-repository-owned-by-your-organization/    but we can't use that until we've deleted daf_butlerUtils since obs_base is a fork (github gives the following error: ""lsst already has a repository in the lsst/daf_butlerUtils network""). Once we're sure there are no more outstanding daf_butlerUtils branches, we can delete that repo (and thus break the fork link) and move obs_base to lsst and update repos.yaml. This shouldn't break Jenkins at all, as github does redirects when a repository is transferred.    I plan to do this at the beginning of November.",0.5
"DM-7951","10/10/2016 17:50:17","Rename daf_butlerUtils component to obs_base","Now that the package move/rename in DM-7915 has been finished, can we please get the daf_butlerUtils component renamed to obs_base to match?",0.5
"DM-7952","10/10/2016 18:48:10","Modifications to project documents to flow down LSR-REQ-0026, re: predefined transient filters, or remove it","In trying to close LIT-101, which was about a reference to ""limited classification"" in the Level 2 object catalog, [~zivezic] and I rediscovered a gap in DM's requirements flowdown.  We discussed this by teleconference today.    The [SRD (LPM-17)|http://ls.st/lpm-17*] contains a ""will"" statement about DM providing ""pre-defined filters optimized for traditionally popular transients, such as supernovae and micro lensed sources"".    This was flowed down nearly verbatim to the [LSR (LSE-29)|http://ls.st/lse-29*] as LSR-REQ-0026, ""Predefined Transient Filters"":    {quote}*Requirement:* Pre-defined filters optimized for traditionally popular transients shall be made available. It shall be possible for the project to add new pre-defined filters as the survey progresses.  *Discussion:* The list of pre-defined filters, by way of example, should include ones for supernovae and microlensed sources.{quote}    This requirement was never flowed down to the [OSS (LSE-30)|http://ls.st/lse-30*], the [DMSR (LSE-61)|http://ls.st/lse-61*], or the [DPDD (LSE-163)|http://ls.st/lse-163*], in the space of system-level controlled documents.    The requirement should either be formally disclaimed, which would require a variance against the SRD and a change request against the LSR, or the proper flowdown should be performed.    The latter would be in two parts: a CCB-level change request for the OSS, DMSR, and DPDD, as well as, within DM, the addition of substantive language to [LDM-151|http://ls.st/ldm-151*] towards fulfilling this requirement.    As the OSS and DMSR are currently completely silent on this, it would be acceptable simply to flow down the LSR requirement verbatim to both of these (as if a <copy> relationship in SysML terms).  However, as noted in LIT-101, the DPDD currently contains language which appears to be in conflict with LSR-REQ-0026, specifically:    {quote}we do not plan to provide any classification (eg., “is the light curve consistent with an RR Lyra?”, or “a Type Ia SN?”).{quote}    This would have to be edited to clarify that while we will _not_ attempt to produce _exclusive_ classifications - that is, assignment of objects to unique categories, but we _will_ provide pre-defined filters, with potentially highly overlapping selections, that provide good completeness but perhaps only very modest purity for a small number of object types of common interest.    It is important to retain the notion that this would be done using _only_ LSST data.    Strictly speaking, as a ""pre-defined filter"" will not be thought of by most of our readers as a ""data product"", it might not need to be mentioned in the DPDD, but because the existing DPDD language suggests a strong conflict, it would be very good to clarify it.    It will also be important to harmonize what we do about this requirement with what we say about the ""mini-broker"" in our requirements flowdown, as this is also not currently very clear.",2
"DM-7955","10/11/2016 10:22:48","Improve the default log configuration","When {{log}}/log4cxx is not configured, it uses the default configuration which is at DEBUG level.   Right now {{log}} is configured explicitly when running CmdLineTasks or python unit tests, but the stack is also used outside of the Task environment or utils unit tests environments.  It should have a more friendly and useful default level and pattern, without the user having to do anything.",3
"DM-7956","10/11/2016 10:48:28","Add the Starlink AST package","As per RFC-193 add the starlink AST package. Name it {{starlink_ast}}.    Note: until we have the new WCS code further along it seems premature to add {{starlink_ast}} to {{lsst_distrib}}",2
"DM-7963","10/11/2016 18:31:36","Add cutout image size option to be exposed and user defined in LC viewer","The LC viewer should get cutouts rather than full images.    We need to decide when the user inputs the desired size of the cutouts and add an input field option in the image viewer to control it.     By default it should be a cutout anyway.",1
"DM-7964","10/11/2016 18:36:06","User should be able to click on a XY-plot point and have the period filled in phase folding panel","If you are looking at the periodogram, the user should be able to click on a point in the XY-plot and have that period (from periodogram) put into the phase folding tool.    Pending cases to be confirmed/agreed:  What about if you click between points?  What if you highlight a row in that table?",2
"DM-7965","10/11/2016 18:45:48","Make consistent highlight colors in the tri-view","In the triview, a point in the XY Plot is highlighted. Simultaneously, a row in the table is highlighted and one of the images is outlined. The linking between these elements is a strength of the tool. The linkage would be stronger and look more professional if these were all the same color. The gold color used for the image highlight would work, as long as it is not too fluorescent, and as long as the highighted point in the XY plot were outlined in black or a darker color.      Use the color of image border for table rows and points on chart. (12/2/2016 XW)",2
"DM-7967","10/11/2016 18:50:01","Magnitudes should be plotted decreasing to the top or to the left by default","Users shouldn't have to manually flip this. Perhaps firefly can detect it by the units? mag or magnitude or mags or magnitudes not case-sensitive should cover it. Yes, this requires some sort of hard-coding but it makes us look like we know what we are doing, so it seems worth it if a more sophisticated solution cannot be found in a reasonable timeframe.",2
"DM-7969","10/11/2016 18:55:41","Add time zero as user defined offset different than default when phase folding","The phase folding tool needs to allow the user to enter a zero point.  By default it should be using the minimum value of the time in the raw table but can be overwritten by the user. The phase folding option panel needs a new input field to let the user define it.",2
"DM-7970","10/11/2016 19:02:05","Image settings specific to LC on by default ","LC image viewer preferences by default should be:    * WCS match should be on by default.  * The image stretch should be locked by default.  * The object position should be centered in the display by default.  * The object should be overlaid as the active target (right now the app is just guessing)",2
"DM-7971","10/11/2016 19:04:54","A highlighted point in the phase-folded light curve, the same point should be highlighted in the raw light curve and vice versa","If the user changes XY Plots from phased to raw, the highlighted/active image displayed should not change.  But the sequence of images displayed will change, because the images sorting is tied to the XY Plot/Table, which will change from {{phase}} to {{mjd}}.    And vice versa, when XY plots changed from raw to phased ...    N.B.: Make more sense when more than 1 image is displayed.",8
"DM-7976","10/12/2016 14:26:53","coadd cannot be loaded directly as afw.image.ExposureF","Loading a {{deepCoadd}} as an afw Exposure gives the following error:     {code:java}  >>> import lsst.afw.image as afwImage  >>> coadd = afwImage.ExposureF(""validation_data_hsc/DATA/rerun/20160805/deepCoadd/HSC-Y/0/7,7.fits"")  31424 [0x7fff74d45000] DEBUG afw.image.Mask null - Number of mask planes: 16  Traceback (most recent call last):    File ""<stdin>"", line 1, in <module>    File ""/opt/sw/lsstsw/stack/DarwinX86/afw/12.1-4-gaba3f16/python/lsst/afw/image/imageLib.py"", line 11623, in __init__      this = _imageLib.new_ExposureF(*args)  lsst.pex.exceptions.wrappers.LogicError:     File ""include/lsst/afw/table/BaseRecord.h"", line 95, in const typename Field<T>::Element *lsst::afw::table::BaseRecord::getElement(const Key<T> &) const [T = int]      Key is not valid (if this is a SourceRecord, make sure slot aliases have been setup). {0}    File ""src/table/io/InputArchive.cc"", line 105, in std::shared_ptr<Persistable> lsst::afw::table::io::InputArchive::Impl::get(int, const lsst::afw::table::io::InputArchive &)      loading object with id=132, name='CoaddPsf' {1}  lsst::pex::exceptions::LogicError: 'Key is not valid (if this is a SourceRecord, make sure slot aliases have been setup). {0}; loading object with id=132, name='CoaddPsf' {1}'  {code}    This used to work, or at least it worked with a stack from Sep 8. ",3
"DM-7979","10/12/2016 18:41:07","write tests and document use of URI or relative path as input and output to butler.","butler can take a URI or relative path for it's inputs and outputs arguments, provided that it has a way to figure out what the details of the repo are, e.g. mapper, and provided the default mode (inputs 'r', outputs 'w') is acceptable.  this needs to be documented in the butler init function. and a formal test written.",2
"DM-7986","10/13/2016 14:29:43","fix circles connecting & add way to reorder image tabs"," fix circles connecting & add way to reorder image tabs.  Most of this ticket has the code to reorder the images tabs in the image select panel  ",2
"DM-7987","10/13/2016 17:06:15","Ambiguous conversion between spherical coordinates and unit vectors","The {{sphgeom::LonLat}} and {{sphgeom::UnitVector3d}} classes provide constructors for converting between the two types. However, the axis convention used for the conversion is left unspecified, limiting the situations where these classes' interoperability can be used.    I propose that the axis convention currently used by the implementations of these classes be made part of their APIs, so that external code knows what results to expect:  {noformat}  (0°, 0°) <--> <1, 0, 0>  (90°, 0°) <--> <0, 1, 0>  (*, 90°) <--> <0, 0, 1>  {noformat}",1
"DM-7992","10/13/2016 22:15:54","Handling of SDSS forced photometry data in the PDACv1 light curve viewer","(Apologies if I'm missing an existing ticket for this; there doesn't seem to be an exactly on-point one, though DM-7990 covers some of it.)    The SUIT portal for PDACv1, the main focus of which is the service of the forced photometry data from SDSS Stripe82 (LSST 2013 processing), needs to be able to deal with multi-band photometry data in a table in which data from all five SDSS filter bands (_u, g, r, i, z_) are mixed together as separate rows in the same table.    It is a minimal requirement to be able to display the data for a single selected filter band, and this should be able to be done in a more natural way than requiring the user to enter a filtering expression in the {{filterId}} column.    As noted in DM-7990, the next step would be the ability to show the light curves for multiple filter bands over-plotted on each other.",1
"DM-8000","10/14/2016 15:52:51","Error instantiating MultiBandDriverTask with LoadIndexedReferenceObjectsTask","Error running {{multiBandDriver}} if {{refObjLoader}} is retargeted to {{LoadIndexedReferenceObjectsTask}} in {{measureCoaddSources}}, e.g. with this config override:    {code:java}  from lsst.meas.algorithms.loadIndexedReferenceObjects import LoadIndexedReferenceObjectsTask  config.match.refObjLoader.retarget(LoadIndexedReferenceObjectsTask)  {code}        {code:java}  Traceback (most recent call last):    File ""/software/lsstsw/stack/Linux64/pipe_drivers/12.1-7-ga5bc178+1/bin/multiBandDriver.py"", line 3, in <module>        MultiBandDriverTask.parseAndSubmit()    File ""/software/lsstsw/stack/Linux64/ctrl_pool/12.1-1-g3e1834e/python/lsst/ctrl/pool/parallel.py"", line 410, in parseAndSubmit        if not cls.RunnerClass(cls, batchArgs.parent).precall(batchArgs.parent):  # Write config, schema    File ""/software/lsstsw/stack/Linux64/pipe_base/12.1-1-g06158e9+2/python/lsst/pipe/base/cmdLineTask.py"", line 300, in precall        task = self.makeTask(parsedCmd=parsedCmd)    File ""/software/lsstsw/stack/Linux64/pipe_drivers/12.1-7-ga5bc178+1/python/lsst/pipe/drivers/multiBandDriver.py"", line 111, in makeTask        return self.TaskClass(config=self.config, log=self.log, butler=butler)    File ""/software/lsstsw/stack/Linux64/pipe_drivers/12.1-7-ga5bc178+1/python/lsst/pipe/drivers/multiBandDriver.py"", line 131, in __init__        peakSchema=afwTable.Schema(self.mergeCoaddDetections.merged.getPeakSchema()))    File ""/software/lsstsw/stack/Linux64/pipe_base/12.1-1-g06158e9+2/python/lsst/pipe/base/task.py"", line 237, in makeSubtask        subtask = taskField.apply(name=name, parentTask=self, **keyArgs)    File ""/software/lsstsw/stack/Linux64/pex_config/12.1+6/python/lsst/pex/config/configurableField.py"", line 83, in apply        return self.target(*args, config=self.value, **kw)    File ""/software/lsstsw/stack/Linux64/pipe_tasks/12.1-3-g35418c8/python/lsst/pipe/tasks/multiBand.py"", line 1039, in __init__        self.makeSubtask(""match"", butler=butler)    File ""/software/lsstsw/stack/Linux64/pipe_base/12.1-1-g06158e9+2/python/lsst/pipe/base/task.py"", line 237, in makeSubtask        subtask = taskField.apply(name=name, parentTask=self, **keyArgs)    File ""/software/lsstsw/stack/Linux64/pex_config/12.1+6/python/lsst/pex/config/configurableField.py"", line 83, in apply        return self.target(*args, config=self.value, **kw)    File ""/software/lsstsw/stack/Linux64/meas_astrom/12.1-2-gf2a177e+2/python/lsst/meas/astrom/directMatch.py"", line 78, in __init__        self.makeSubtask(""refObjLoader"", butler=butler)    File ""/software/lsstsw/stack/Linux64/pipe_base/12.1-1-g06158e9+2/python/lsst/pipe/base/task.py"", line 237, in makeSubtask         subtask = taskField.apply(name=name, parentTask=self, **keyArgs)    File ""/software/lsstsw/stack/Linux64/pex_config/12.1+6/python/lsst/pex/config/configurableField.py"", line 83, in apply          return self.target(*args, config=self.value, **kw)    File ""/software/lsstsw/stack/Linux64/meas_algorithms/12.1-6-g1f798ce+1/python/lsst/meas/algorithms/loadIndexedReferenceObjects.py"", line 48, in __init__                ingest_config = butler.get(self.config.ingest_config_name, immediate=True)                AttributeError: 'NoneType' object has no attribute 'get'  {code}      I included a way to reproduce this without actual data in {{obs_decam}} branch {{u/hfc/DM-8000}}. There I added an empty Butler repo and the measureCoaddSources config override.    With that branch this command reproduces the error:      {{multiBandDriver.py $OBS_DECAM_DIR/repo/ --rerun test --cores 1}}  ",2
"DM-8002","10/17/2016 09:00:22","Fix unguarded display code in SecondMomentStarSelector","Something recent (maybe DM-7848) either broke or uncovered an existing bug in {{SecondMomentStarSelector}}, in which a block of display code (specifically a call to `{{lsst.afw.display.ds9.Buffering()}} is not protected by an {{if display}} guard.    I'm not sure if we expect to require all display code to be guarded like that, but all of the other display code in this file is, so I'm just going to fix that block.  If someone ([~rhl]?) can confirm that unguarded display code is safe, then there's a bug somewhere else and {{SecondMomentStarSelector}} should have a lot of its display guards removed.",1
"DM-8004","10/17/2016 12:53:22","Crash in pipe_supertask.NewExampleCmdLineTask","I try to follow http://dmtn-002.lsst.io/ and run all examples there. {{NewExampleCmdLineTask}} example crashes with an exception:  {noformat}  $ cmdLineActivator NewExampleCmdLineTask --extras $OBS_TEST_DIR/data/input/ --output test --id  lsst.pipe.supertask.examples.NewExampleCmdLineTask.NewExampleCmdLineTask found!    Classes inside module lsst.pipe.supertask.examples.NewExampleCmdLineTask :      NewExampleCmdLineTask.NewExampleCmdLineConfig  NewExampleCmdLineTask.NewExampleCmdLineTask    SuperTask  283 [0x7f1bd2a28740] INFO exampleTask null - exampleTask was initiated  root INFO: Config override file does not exist: '/u2/salnikov/lsstsw/stack/Linux64/obs_test/12.1-3-gf809d79/config/exampleTask.py'  root INFO: Config override file does not exist: u'/u2/salnikov/lsstsw/stack/Linux64/obs_test/12.1-3-gf809d79/config/test/exampleTask.py'  root INFO: input=/u2/salnikov/lsstsw/stack/Linux64/obs_test/12.1-3-gf809d79/data/input  root INFO: calib=None  root INFO: output=/home/salnikov/test  CameraMapper INFO: Loading registry registry from /home/salnikov/test/_parent/registry.sqlite3  Traceback (most recent call last):    File ""/home/salnikov/pipe_supertask/bin/cmdLineActivator"", line 24, in <module>      CmdLineActivator.parse_and_run()    File ""/home/salnikov/pipe_supertask/python/lsst/pipe/supertask/activator.py"", line 556, in parse_and_run      CmdLineClass.activate()  # This is where everything starts    File ""/home/salnikov/pipe_supertask/python/lsst/pipe/supertask/activator.py"", line 234, in activate      if self.precall():    File ""/home/salnikov/pipe_supertask/python/lsst/pipe/supertask/activator.py"", line 222, in precall      self.SuperTask.write_config(self.parsed_cmd.butler, clobber=self.clobber_config, do_backup=self.do_backup)    File ""/home/salnikov/pipe_supertask/python/lsst/pipe/supertask/super_task.py"", line 336, in write_config      elif butler.datasetExists(config_name):    File ""/u2/salnikov/lsstsw/stack/Linux64/daf_persistence/12.1+1/python/lsst/daf/persistence/butler.py"", line 544, in datasetExists      location = repoData.repo.map(datasetType, dataId)    File ""/u2/salnikov/lsstsw/stack/Linux64/daf_persistence/12.1+1/python/lsst/daf/persistence/repository.py"", line 180, in map      loc = self._mapper.map(*args, **kwargs)    File ""/u2/salnikov/lsstsw/stack/Linux64/daf_persistence/12.1+1/python/lsst/daf/persistence/mapper.py"", line 172, in map      func = getattr(self, 'map_' + datasetType)  AttributeError: 'TestMapper' object has no attribute 'map_exampleTask_config'  {noformat}    Need to see what I'm doing wrong.",1
"DM-8006","10/17/2016 13:11:39","Add 2D Chart to client side phase folding dialog","Add 2D Chart to client side phase folding dialog.  The chart should update as the user moves the slider.    copied from the github:  (Nov. 15, 2016 XW)  The implementation mainly,    add 2D chart on the client side phase folding dialog and the plot changes as any of the phase folding parameters (time column, flux column, zero time point, period) changes.  update the parameter items in the parameter setting dialog.  Test:    localhost:8080/firefly/lc.html  open 'lc_raw.tbl' from 'Raw Table', click 'search'  select tab 'Upload/Phase folding' and click button 'Phase Folding'  experiment the setting of all entries and move slider to set 'period', the plot will be updated as any of the parameter (except Flux error column) changes  click button 'Phase Folded Table' to close the dialog and a table with phase column is created (or updated) based on current setting of period, time column and zero point time.  Note:    The time and flux column names are set in default for IRSA cases. Those column names are settable while the phase folding dialog component is used.",8
"DM-8007","10/17/2016 13:22:29","Make firefly_client pip-installable","Make the   {{firefly_client}} Python API installable via pip. The package will be named {{firefly_client}}. The package will exist within the {{firefly}} repository.    copied from the pul request: (XW, 1018/2016)  copied Firefly License.txt  added a simple README  added setup.py and setup.cfg  moved package files to firefly_client directory  ",1
"DM-8011","10/17/2016 18:06:48","Update XRootD from upstream","Merge in latest changes from upstream XRootD (includes gcc 6.2 fixes)",0.5
"DM-8015","10/17/2016 18:53:32","VisitInfo repr() and str() should print a useful summary of contents","The new VisitInfo object is a bit opaque from within Python: you can look at the individual components via e.g. {{visitInfo.darkTime}} and {{visitInfo.boresightAirmass}}, but {{print(visitInfo)}} is not helpful. it would be extremely useful for {{str()}} and {{repr()}} to print either the whole contents of the VisitInfo (it's not that much information), or for {{str()}} to print a useful summary and {{repr()}} the whole thing.",2
"DM-8021","10/18/2016 09:43:41","Deal with large pickles","[~lauren] is running:  {code}  coaddDriver.py /tigress/HSC/HSC --rerun lauren/LSST/DM-6816/cosmos --job DM-6816-cosmos-y-coaddDriver --time 100 --cores 96 --batch-type=slurm --mpiexec='-bind-to socket' --id tract=0 filter=HSC-Y --selectId ccd=0..103 filter=HSC-Y visit=274..302:2^306..334:2^342..370:2^1858..1862:2^1868..1882:2^11718..11742:2^22602..22608:2^22626..22632:2^22642..22648:2^22658..22664:2 --batch-submit '--mem-per-cpu 8000'  {code}  and it is producing:  {code}  OverflowError on tiger-r8c1n12:19889 in map: integer 2155421250 does not fit in 'int'  Traceback (most recent call last):    File ""/tigress/HSC/LSST/stack_20160915/Linux64/ctrl_pool/12.1+5/python/lsst/ctrl/pool/pool.py"", line 99, in wrapper      return func(*args, **kwargs)    File ""/tigress/HSC/LSST/stack_20160915/Linux64/ctrl_pool/12.1+5/python/lsst/ctrl/pool/pool.py"", line 218, in wrapper      return func(*args, **kwargs)    File ""/tigress/HSC/LSST/stack_20160915/Linux64/ctrl_pool/12.1+5/python/lsst/ctrl/pool/pool.py"", line 554, in map      self.comm.scatter(initial, root=self.rank)    File ""MPI/Comm.pyx"", line 1286, in mpi4py.MPI.Comm.scatter (src/mpi4py.MPI.c:109079)    File ""MPI/msgpickle.pxi"", line 707, in mpi4py.MPI.PyMPI_scatter (src/mpi4py.MPI.c:48114)    File ""MPI/msgpickle.pxi"", line 168, in mpi4py.MPI.Pickle.dumpv (src/mpi4py.MPI.c:41672)    File ""MPI/msgbuffer.pxi"", line 35, in mpi4py.MPI.downcast (src/mpi4py.MPI.c:29070)  OverflowError: integer 2155421250 does not fit in 'int'  application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0```  {code}    We need to fix or work around this problem.",2
"DM-8023","10/18/2016 11:15:43","afw interface.py crashes with ""ds9"" backend because there's no ds9.DisplayImpl","Making an afw.display with ""ds9"" backend (and perhaps others!) throws an exception and crashes because ds9 doesn't have DisplayImpl.    As an example:  import lsst.afw.display as afwDisplay  display = afwDisplay.Display(""ds9"")    throws:    AttributeError: 'module' object has no attribute 'DisplayImpl'",2
"DM-8026","10/18/2016 17:05:31","log4cxx gcc 6.2 compatibility fixes","Minor fixes for gcc 6.2 compatibility",0.5
"DM-8027","10/18/2016 17:23:02","Update obs_lsstSim to add VisitInfo to eimages","The eimage dataset type is special to obs_lsstSim so was not updated in the process of implementing the VisitInfo ticket.",2
"DM-8028","10/18/2016 17:33:18","Add a utility class to upload the unit test data file","A utility class is needed to upload the unit testing data file in firefly_test_data tree.",2
"DM-8030","10/18/2016 19:52:47","Identify and correct differing background model between py2 and py3","The background model pixel values differ by 0.00011 ± 0.00035 on average in python2 versus python3. The cause of this discrepancy needs to be tracked down and fixed.",2
"DM-8032","10/18/2016 20:01:05","Tighten testProcessCcd thresholds once background model is fixed","Once the {{meas_algorithms}} background model is being generated correctly, the original {{pipe_tasks}} test that first pointed to this problem in needs to be put back to higher precision. A comment in {{testProcessCcd.py}} indicates the assertions that should have their precisions lowered back to pre-py3-porting values. Presently, this test fails with lower thresholds because the background model is different than expected.",0.5
"DM-8034","10/18/2016 22:09:56","reprocess validation_data* to contain VisitInfo","Please reprocess the validation data sets so that their output data contains the VisitInfo metadata structure (see DM-5503). I will provide a specific weekly to in the reprocessing in a comment.",1
"DM-8035","10/19/2016 09:03:51","daf_persistence test failures with gcc 5.2 on el5","The majority of the {{daf_persistence}} tests fail when attempting to build a conda package via {{conda-lsst}} on el5 with gcc 5.2.    {code:java}  Traceback (most recent call last):    File ""tests/reposInButler.py"", line 36, in <module>      import lsst.daf.persistence as dp    File ""/home/jhoblitt/conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/__init__.py"", line 28, in <module>      from .persistenceLib import *    File ""/home/jhoblitt/conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/persistenceLib.py"", line 27, in <module>      _persistenceLib = swig_import_helper()    File ""/home/jhoblitt/conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/persistenceLib.py"", line 26, in swig_import_helper      return importlib.import_module('_persistenceLib')    File ""/home/jhoblitt/conda-lsst/miniconda/envs/_build/lib/python2.7/importlib/__init__.py"", line 37, in import_module      __import__(name)  ImportError: No module named _persistenceLib  tests/butlerSubset.py  {code}    I'm at a loss as why the tests are failing in this environment.   The library is being successfully built.      {code:java}  $ ls -la /home/jhoblitt/conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/_persistenceLib.so   -rwxrwxr-x 1 jhoblitt jhoblitt 9716560 Oct 19 14:47 /home/jhoblitt/conda-lsst/miniconda/conda-bld/work/python/lsst/daf/persistence/_persistenceLib.so  {code}        {code:java}  $ swig -copyright    SWIG Version 3.0.10  Copyright (c) 1995-1998  University of Utah and the Regents of the University of California  Copyright (c) 1998-2005  University of Chicago  Copyright (c) 2005-2006  Arizona Board of Regents (University of Arizona)  {code}    ",2
"DM-8044","10/20/2016 15:38:02","Default local background subtraction to False for safe coadd clipping","In RFC-212 we adopted turning on the ""junk suppression"" temporary local background subtraction by default: {{SourceDetectionConfig.doTempLocalBackground=True}}.  Since the safe clipping coadd assembly (introduced in DM-2915) also performs object detection, this had the effect of turning on the junk suppression background there too.  The safe clipping algorithm was designed and tuned with no extra background estimations, thus {{doTempLocalBackground}} should default to *False* for that task.",0.5
"DM-8051","10/21/2016 14:27:28","weekly-release/build-build-tag jobs broken","{{weekly-release}} failed on monday because master was broken.  The job seems to failing in odd ways today, seemingly requiring some updates the pipeline dsl syntax.",1
"DM-8063","10/24/2016 12:45:08","Check the Stubbs Am241 gain calibration plan","Review the document detailing Stubb's plan to use Am241 gammas from outside the cryostat to perform absolute gain calibration in the main camera for any gross errors in reasoning.",1
"DM-8074","10/25/2016 03:47:40","Test ""Swarm mode"" high availabilty features","Swarm mode provide high availabilty feature wich will be tested on Openstack.",8
"DM-8076","10/25/2016 13:12:31","Cleanup repository move wording in DMTN-027","There are some lingering incorrect ""you should..."" statements in the ""merging work"" section of DMTN-027, that should be referring to the ""other developer"", and the text needs to be reworded to reflect that dependencies on master need to live in {{lsst}}, and so some extra repository shuffling has to happen.    A few relevant quotes from Slack:    About how to move things without deleting daf_butlerUtils:    {quote}  Tim Jenness [11:50 AM]    move daf_butlerUtils into lsst-sqre, move obs_base into lsst, move daf_butlerUtils into lsst-dm  {quote}    About unclear ""you"":    {quote}  Kian-Tat Lim [11:53 AM]  Oh, except I missed this at the top ""Once your rename has been merged to master,""  One thing about that section that I noticed before but didn't comment on: it's sometimes unclear who ""you/your"" is -- the renamer or the ""other developer""    John Parejko [11:55 AM]  I tried to make it always be “you” as the person doing the rename. The person the whole document is aimed at.  If I got that wrong somewhere, I’m happy to fix it.    Kian-Tat Lim [11:55 AM]  But ""your work-in-progress branch"" is really ""the other developer"", no?  Same for ""Check that the branch in the new clone matches your branch in meas_worst,""  @parejkoj So I think the rest of the document is fine as long as you put in that merging to master also involves moving to lsst and change the remote url in the instructions.  {quote}",0.5
"DM-8078","10/25/2016 14:35:28","Remove int and long from schema aliases","In python 3 all integers are 64 bit ""long"" integers, while in python 2, 32 and 64 bit integers are differentiated by the int and long types respectively. This causes confusion when adding a new field to a schema using addField in afw/table/Base.i in python 3, where the python type is converted into a C++ type using a dictionary    {code:python}  aliases = {      long: ""L"",      int: ""I"",      float: ""D"",      str: ""String"",      futurestr: ""String"",      numpy.uint16: ""U"",      numpy.int32: ""I"",      numpy.int64: ""L"",      numpy.float32: ""F"",      numpy.float64: ""D"",      Angle: ""Angle"",  }  {code}    In RFC-227 it was decided that {{int}} and {{long}} will be removed from {{aliases}}, leaving  {code:python}  aliases = {      str: ""String"",      futurestr: ""String"",      numpy.uint16: ""U"",      numpy.int32: ""I"",      numpy.int64: ""L"",      numpy.float32: ""F"",      numpy.float64: ""D"",      Angle: ""Angle"",  }  {code}    All existing code using `type=long` will need to be updated as part of this ticket.    This will be included in the pybind11 wrapped code, with a deprecation warning for users attempting to add fields using {{int}} or {{long}}.",0
"DM-8080","10/25/2016 15:11:52","start a old-vs-new butler document","[~rhl] requested:  I think it'd be really helpful to have a relatively short document describing (or at least mentioning) the features that are in the butler that the `classic' butler didn't have.    I think we can put a section in LDM-463 for this.",2
"DM-8081","10/25/2016 15:51:38","unable to build release git tag when 3rd party deps change","The fundamental issue is that {{lsstsw/lsst-build}} operate on refs in the git repo.  Due to the way eups generates version strings from git tags, we are unable to tag the repos for 3rd party products.  This means that we are unable to build a last tagged release from source once a 3rd party dependency has been upgraded to an incompatible version.",3
"DM-8084","10/25/2016 19:31:19","Throwing exception TableExistsError with no parameters","Database utility function *createTableFromSchema* doesn't provide parameters to the constructor of the exception class *TableExistsError* when throwing the exceptions. This causes the *wmgr* Web service to fail with the following stack:  {code}  Traceback (most recent call last):    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1817, in wsgi_app      response = self.full_dispatch_request()    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1477, in full_dispatch_request      rv = self.handle_user_exception(e)    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1381, in handle_user_exception      reraise(exc_type, exc_value, tb)    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1475, in full_dispatch_request      rv = self.dispatch_request()    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1461, in dispatch_request      return self.view_functions[rule.endpoint](**req.view_args)    File ""/qserv/stack/Linux64/qserv/12.1.rc1-3-g72e15fd+3/lib/python/lsst/qserv/wmgr/dbMgr.py"", line 321, in createTable      utils.createTableFromSchema(dbConn, schema)    File ""/qserv/stack/Linux64/db/12.1.rc1+1/python/lsst/db/utils.py"", line 288, in createTableFromSchema      raise TableExistsError()  TypeError: __init__() takes at least 4 arguments (1 given)  {code}    And this is the function affected:  {code}  Linux64/db/12.1.rc1/python/lsst/db/utils.py  {code}  {code:python}  def createTableFromSchema(conn, schema):      """"""      Create database table from given schema.        @param conn        Database connection or engine.      @param schema      String containing full schema of the table (it can be a dump                         containing ""CREATE TABLE"", ""DROP TABLE IF EXISTS"", comments, etc.        Raises TableExistsError if the table already exists.      Raises sqlalchemy exceptions.      """"""      if conn.engine.url.get_backend_name() == ""mysql"":          try:              conn.execute(schema)          except OperationalError as exc:              log.error('Exception when creating table: %s', exc)              if exc.orig.args[0] == MySqlErr.ER_TABLE_EXISTS_ERROR:                  raise TableExistsError()              raise      else:          raise NoSuchModuleError(conn.engine.url.get_backend_name())  {code}    The relevant constructor is found in one of the base classes of the thrown exception:  {code}  Linux64/sqlalchemy/1.0.8.lsst3+2/lib/python/SQLAlchemy-1.0.8-py2.7-linux-x86_64.egg/sqlalchemy/exc.py  {code}  {code:python}  class DBAPIError(StatementError):      ...      def __init__(self, statement, params, orig, connection_invalidated=False):  {code}    {code}",2
"DM-8085","10/25/2016 19:40:33","Using misspelled name TableExistError for exception TableExistsError ","The *wmgr* Web services attempts to throw an exception of the non-existing class *TableExistError* instead of *TableExistsError*. This causes the service to fail with with the following stack trace:  {code}  2016-10-25 23:57:01,814 [PID:392] [ERROR] (log_exception() at app.py:1423) wmgr: Exception on /dbs/sdss_stripe82_00/tables/RunDeepForcedSource/chunks [POST] Traceback (most recent call last):    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1817, in wsgi_app      response = self.full_dispatch_request()    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1477, in full_dispatch_request      rv = self.handle_user_exception(e)    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1381, in handle_user_exception      reraise(exc_type, exc_value, tb)    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1475, in full_dispatch_request      rv = self.dispatch_request()    File ""/qserv/stack/Linux64/flask/0.10.1.lsst2+1/lib/python/Flask-0.10.1-py2.7.egg/flask/app.py"", line 1461, in dispatch_request      return self.view_functions[rule.endpoint](**req.view_args)    File ""/qserv/stack/Linux64/qserv/12.1.rc1-3-g72e15fd+3/lib/python/lsst/qserv/wmgr/dbMgr.py"", line 593, in createChunk      except utils.TableExistError as exc:  AttributeError: 'module' object has no attribute 'TableExistError'  2016-10-25 23:57:01,815 [PID:392] [INFO] (_log() at _internal.py:87) werkzeug: 141.142.181.132 - - [25/Oct/2016 23:57:01] ""POST /dbs/sdss_stripe82_00/tables/RunDeepForcedSource/chunks HTTP/1.1"" 500 -  {code}    The relevant code:  {code}  /qserv/stack/Linux64/qserv/12.1.rc1-3-g72e15fd+3/lib/python/lsst/qserv/wmgr/dbMgr.py:593  {code}  {code:python}  @dbService.route('/<dbName>/tables/<tblName>/chunks', methods=['POST'])  def createChunk(dbName, tblName):              ...  593:        except utils.TableExistError as exc:   {code}",2
"DM-8087","10/26/2016 10:57:10","scons should clean the rerun","When you rerun {{ci_hsc}} after changing something it fails as the versions have changed.  The/a natural reaction is to run {{scons -c}} (i.e. clean), but unfortunately that doesn't help.    Please make {{scons -c}} delete the rerun/ci_hsc directory.  ",0.5
"DM-8090","10/26/2016 13:25:22","Update Verification Documentation  2","We update the lsst-dev7 + Verification Cluster documentation to cover the transition / renaming of the head node from lsst-dev7 to lsst-dev01 . ",2
"DM-8091","10/26/2016 13:47:30","Fix cmsd warning message","Andy H. provided the solution:  {quote}Hi Fabrice,    Yes, this is common for configurations that don't expect to be dynamically  written to but yet export the space in R/W mode (which we do). All the  message is saying is you don't have enough space for impromptu writes (which  is true but then you don't expect any). So, there is a configuration  directive to resolve this problem:    cms.space 1k 2k    The directive is documented here:  http://xrootd.org/doc/dev45/cms_config.htm#_Toc454223038    I would add this directive to the config file as soon as you can and restart  he cmsd.    Andy{quote}",2
"DM-8094","10/26/2016 14:28:52","Replace Swarm default test container","version should be 'dev' here, but 'dev' need to be updated to contains DM-7139 code:    {code}  admin/tools/docker/deployment/swarm/manager/env-docker.sh  @@ -2,8 +2,8 @@   # Allow to customize Docker container execution      # VERSION can be a git ticket branch but with _ instead of /  -# example: u_fjammes_DM-4295  -VERSION=dev  +# example: tickets_DM-7139, or dev  +VERSION=tickets_DM-7139  {code}",2
"DM-8095","10/26/2016 14:51:11","Remove integration tests warning message","sock option must be replaced by socket:  {code}  dev@clrinfopc04:/qserv/stack/Linux64/qserv_testdata/12.0+45$ grep -rw sock *  ...  python/lsst/qserv/tests/sql/cmd.py:        self._mysql_cmd.append(""--sock=%s"" % self.config['mysqld']['sock'])  python/lsst/qserv/tests/sql/cmd.py:        self._mysql_cmd.append(""--sock=%s"" % self.config['mysqld']['sock'])  {code}    Maybe sock should also be replaced in Qserv code/config files int the future...",1
"DM-8096","10/26/2016 17:37:27","Make ""immediate=True"" the default for butler.get()","Considering how problematic readProxy objects are to deal with, I feel it might be better to default to {{immediate=True}} for {{butler.get()}} calls. For any test code or interactive code, we want to get an immediately useable object, while inside the pipeline we can be more explicit about {{immediate=False}} when we realize that we're waiting on I/O.",0.5
"DM-8099","10/27/2016 10:37:02","LDM-493 v1 Edits from DMLT feedback","This ticket covers edits to the [v1 integration branch|https://github.com/lsst/LDM-493/pull/4] of LDM-493 based on feedback from the DMLT.",0.5
"DM-8100","10/27/2016 11:38:27","Determine what astropy.io.fits does with FITS header cards for missing data","As part of implementing RFC-239 determine how {{astropy.io.fits}} handles FITS header cards with missing (unknown) values. I am virtually certain it cannot write such cards, but what happens when it tries to read them?    This will inform implementation of RFC-239.",1
"DM-8102","10/27/2016 12:08:33","Introduce Set operations between SpanSets and masks","Currently SpanSets can intersect, intersectNot, and union between a themselves and another SpanSet. This ticket will add the ability to use these methods with masks as ""other"". This work will replace the intersectMask and footprintAndMask methods from the old footprint implementation.",2
"DM-8103","10/27/2016 12:31:30","Add method to find edge pixels to SpanSet","Add a method to SpanSets which returns a new SpanSet that contains the pixels at the boarder of the original SpanSet",2
"DM-8105","10/27/2016 14:24:49","Missing test case for SpherePoint","One of the changes introduced during review of DM-5529 was a mismatch between the behavior of indexing in Python and C++ -- negative indices are allowed in the former but not the latter, consistent with either language's conventions. However, indexing is tested only in the Python test suite against the Python specification.    A test case should be added to {{testSpherePoint.cc}} to verify that {{operator[]}} behaves as specified when called directly.",2
"DM-8106","10/27/2016 14:29:02","SpherePoint does not have move constructors/assignment","The {{SpherePoint}} class explicitly declares that it is using the default copy-constructor and copy assignment operator, but does not do the same with the move constructor and move assignment operator. This is inconsistent with RFC-209.    Since there is no reason to disallow move semantics for {{SpherePoint}}, move constructors and move assignment should be explicitly declared. I am not aware of any way in which this change can be tested.",1
"DM-8115","10/28/2016 07:04:05","jenkins master can't run builds / write temp files","I discovered this morning that there are a bunch of ""dead"" builds (zombies) listed for the build slave that runs on the same node as the jenkins master.  The master was reporting that it is unable to create temp files and that is why the threads for these builds crashed.  Further investigation shows that the master node is out of disk space and this seems to be due to 100's of GiB of console output from a build that has been running since the 25th and it is still running.  ",0.5
"DM-8119","10/28/2016 10:41:07","Incorrect time zone settings within Qserv Docker containers","I have noticed that the timezone is not set correctly within the latest version of the Qserv containers. This is an example:  {code:bash}  % hostname  lsst-qserv-db01    % date  Fri Oct 28 11:33:00 CDT 2016    % docker exec -it qserv date  Fri Oct 28 16:33:10 UTC 2016  {code}    This may have various (some of them would be unpleasant) side effects, such as (to count just a few):  * incorrect timing in the log files (which will make it hard to investigate problems)  * shifted monitoring data (hard to interpret/analyze the monitoring data)  * anything else which is time-dependent within the application (such as scheduling, etc.)    It looks like the timezone configuration within the container is set to:  {code:bash}  % docker exec -it qserv cat /etc/timezone  Etc/UTC  {code}    Should this be solved by running the container with this extra option which would override the default setting of *Etc/UTC* set at a container build time?  {code:bash}  % ls -l /etc/localtime  lrwxrwxrwx 1 root root 35 Sep 23 09:43 /etc/localtime -> /usr/share/zoneinfo/America/Chicago    % docker run ... -v /etc/localtime:/etc/localtime:ro  {code}  This would add the read-only map from the container's configuration file to the hosts's one.",2
"DM-8122","10/28/2016 13:24:29","SpherePoint tests fail on macOS","Building afw on macOS Sierra I get:  {code}  tests/testSpherePoint.py    .........................F....  ======================================================================  FAIL: testStrValue (__main__.SpherePointTestSuite)  Test if __str__ produces output consistent with its spec.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testSpherePoint.py"", line 762, in testStrValue      self.assertRegexpMatches(numbers[1], r'\+|-nan')  AssertionError: Regexp didn't match: '\\+|-nan' not found in 'nan'    ----------------------------------------------------------------------  Ran 30 tests in 2.210s    FAILED (failures=1)  {code}  It is failling for point {{(180.000000, nan)}}.",2
"DM-8138","10/31/2016 08:22:05","Implement changes requested in RFC-247","Move several common datasets to obs_base.  This will not affect the dataset definitions of the individual cameraMappers.",3
"DM-8147","10/31/2016 13:54:23","Fix error while loading deepCoadd fits file.","dax_imgserv experiences an error while loading lsst-qserv-dax01:/datasets/gapon/data/DC_2013/coadd/deepCoadd/r/0/321,4.fits.   This same file should be available as:   lsst-dev01.ncsa.illinois.edu:/datasets/gapon/data/DC_2013/coadd/deepCoadd/r/0/321,4.fits.  The error is:  {code}    MalformedArchiveError:     File ""src/CoaddPsf.cc"", line 358, in virtual std::shared_ptr<lsst::afw::table::io::Persistable> lsst::meas::algorithms::CoaddPsf::Factory::read(const InputArchive&, const CatalogVector&) const      Archive assertion failed: catalogs.size() == 2u {0}    File ""src/table/io/InputArchive.cc"", line 105, in std::shared_ptr<lsst::afw::table::io::Persistable> lsst::afw::table::io::InputArchive::Impl::get(int, const lsst::afw::table::io::InputArchive&)  {code}    On lsst-qserv-dax01.ncsa.illinois.edu this can be reproduced by  {code}  $ source /software/lsstsw/stack/loadLSST.bash  $ setup meas_algorithms  python  >>> import lsst.afw.image  >>> e = lsst.afw.image.ExposureF(""/datasets/gapon/data/DC_2013/coadd/deepCoadd/r/0/321,4/coadd-r-0-321,4.fits"")  {code}",1
"DM-8155","11/01/2016 10:56:14","Update XRootD from upstream (again)","This captures the logging plugin changes from upstream xrootd xrdssi branch. Passed jenkins.",0.5
"DM-8156","11/01/2016 11:14:25","add support for Slurm to allocateNodes.py","Add support for allocation of HTCondor nodes through Slurm via allocateNodes.py command.  This is related to work being done for DM-8154",20
"DM-8157","11/01/2016 11:20:57","Implement an image search processor to access the image from PDAC","This is the working extension for DM-8010.  The DM-8010 implemented MetaData and TableData search processors.  The image search processor is needed as well.  It will also exercise the cutout service that SLAC imgServ API will provide.    The image search processor (LSSTImageSearch) is searching the DAX using either id or a set of ids to locate the image and then displaying them in the tree-view window.  There are two types of search URL:    Coadded image (and cutout) retrieval   curl -o outImageCoadd.fits ""http://lsst-qserv-dax01.ncsa.illinois.edu:5000/image/v0/deepCoadd/id?id=23986176""      For Science CCD, it can also be searched using id.   But the preferred way is using a set of ids, such as:    curl -o outImage3.fits ""http://lsst-qserv-dax01.ncsa.illinois.edu:5000/image/v0/calexp/ids?run=3325&camcol=1&field=171&filter=z""    The image search processor processes the parameters passed from the UI and then build the URL like above to find the image.              ",8
"DM-8169","11/02/2016 11:47:00","Use -isystem (rather than -I) for include files from external packages","See RFC-246 for the reasoning and for an example implementation.",2
"DM-8179","11/03/2016 15:22:03","Preserve background jobs and statuses beyond a browser session.","Currently, background jobs and statuses are kept on the client.  That information is lost after a browser reload.  We need to save that information so that a user can come back at a later time and still get this information presented.    Implementation:  - save data on the server.  - push all statuses to client when connected  - continue to update statuses while client is connected",8
"DM-8182","11/03/2016 17:16:15","Resend email notification when email value changes","A new email notification should be sent for every successfully completed background jobs when a new email is entered.",2
"DM-8187","11/04/2016 09:21:41","Qserv czar crashes itself and mysql-proxy on invalid queries","h1. Problem summary    Qserv *mysql-proxy* service always crashes on queries made on either non-existing databases or made in a lack of any specific database context. The same behavior is obsolved for queries addressed to databases which are present within the MySQL/MariaDB service of the Qserv *master* node while not being registered with Qserv's CSS.    Examples of queries based on the integration test setup:  {code:sql}  SELECT COUNT(*) FROM AnyTable;  SELECT COUNT(*) FROM UnknownDatabase.SomeTable;  SELECT COUNT(*) FROM qservTest_case03_mysql.RunDeepSource;  {code}    h1. Details    Once the crash happens no further details found in the service's log files (the report was made by logging into a running Docker container):  {code:bash}  [gapon@lsst-qserv-master01 ~] docker exec -it qserv bash  qserv@lsst-qserv-master01:/qserv$ ls -al run/var/log/  ..  -rw-r--r-- 1 qserv qserv 2193695372 Nov  4 00:52 mysql-proxy-lua.log  -rw-r----- 1 qserv qserv      11811 Nov  4 00:51 mysql-proxy.log  {code}  The only (and the last) relevant record left in *mysql-proxy-lua.log* is about the query causing the crash. For example:  {code}  % tail  mysql-proxy-lua.log  ..[2016-11-04T01:27:40.883-0500] [LWP:1402] DEBUG ccontrol.UserQuerySelect (core/modules/ccontrol/UserQuerySelect.cc:397) - QI=227: UserQuery registered SELECT * FROM R LIMIT 1  {code}    Another obstacle for investigating the root cause of the problem was that no core file was left by the crashed process. Further investigation has revealed that this was happening because the proxy is usually launched with the *--daemon* option (the report was taken from within a running Docker container):  {code:bash}  qserv@lsst-qserv-master01:/qserv$ ps -ef | grep proxy  qserv     1540     0  0 01:44 ?        00:00:00 mysql-proxy --daemon --proxy-lua-script=…  {code}    In order to get the core dump the following actions were taken. First of all the core configuration file of the container's host machine was modified to prefix core files with the name of the crashed executables:  {code:bash}  sudo -i  echo ""%e.core"" > /proc/sys/kernel/core_pattern  {code}  The next step was to ensure no limit for core dumps is set for user *qserv* within the Docker container of the *Master* image:  {code:bash}  [gapon@lsst-qserv-master01 ~] docker exec -it qserv bash    qserv@lsst-qserv-master01:/qserv$ ulimit -c unlimited  qserv@lsst-qserv-master01:/qserv$ ulimit -a  core file size          (blocks, -c) unlimited  ...  {code}  The next step was to disable option *--daemon* in the service management file:  {code}  /qserv/run/etc/init.d/mysql-proxy  {code}    The new configuration was tested by stopping/starting the service from within the container:  {code:code}  qserv@lsst-qserv-master01:/qserv$ run/etc/init.d/mysql-proxy stop  [ ok ing mysql-proxy.  qserv@lsst-qserv-master01:/qserv$ run/etc/init.d/mysql-proxy start  [ ok ing mysql-proxy..  {code}    The the following query was made to crash the service:  {code:sql}  SELECT * FROM R LIMIT 1  {code}  After the service went down a desired core file was found in the following folder of the running container:  {code:bash}  qserv@lsst-qserv-master01:/qserv$ ls -al   ..  -rw-------  1 qserv qserv 28422144 Nov  4 01:27 mysql-proxy.core.1402  {code}    The dump was analyzed with *gdb* to get the stack of the crash:  {code}  qserv@lsst-qserv-master01:/qserv$ which mysql-proxy            /qserv/stack/Linux64/mysqlproxy/0.8.5+12/bin/mysql-proxy    qserv@lsst-qserv-master01:/qserv$ gdb `which mysql-proxy` mysql-proxy.core.1402  …  Reading symbols from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/bin/mysql-proxy...done.  [New LWP 1402]  [New LWP 1436]  [New LWP 1437]  [New LWP 1438]  [Thread debugging using libthread_db enabled]  Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".  Core was generated by `mysql-proxy --proxy-lua-script=/qserv/stack/Linux64/qserv/12.1.rc1-3-g72e15fd+3'.  Program terminated with signal SIGSEGV, Segmentation fault.  #0  0x00007f4bf1445573 in lsst::qserv::qdisp::Executive::setQueryId (this=0x0, id=227) at core/modules/qdisp/Executive.cc:103  103  core/modules/qdisp/Executive.cc: No such file or directory.      (gdb) where  #0  0x00007f4bf1445573 in lsst::qserv::qdisp::Executive::setQueryId (this=0x0, id=227) at core/modules/qdisp/Executive.cc:103  #1  0x00007f4bf1293bc8 in lsst::qserv::ccontrol::UserQuerySelect::_qMetaRegister (this=0x180ea80) at core/modules/ccontrol/UserQuerySelect.cc:398  #2  0x00007f4bf1290c1c in lsst::qserv::ccontrol::UserQuerySelect::UserQuerySelect (this=0x180ea80, qs=std::shared_ptr (count 2, weak 0) 0x1801870,       messageStore=std::shared_ptr (count 2, weak 0) 0x180efc0, executive=std::shared_ptr (empty) 0x0, infileMergerConfig=std::shared_ptr (empty) 0x0,       secondaryIndex=std::shared_ptr (count 2, weak 0) 0x179fa70, queryMetadata=std::shared_ptr (count 2, weak 0) 0x17c9da0, czarId=2, errorExtra="""")      at core/modules/ccontrol/UserQuerySelect.cc:151  #3  0x00007f4bf12868b6 in __gnu_cxx::new_allocator<lsst::qserv::ccontrol::UserQuerySelect>::construct<lsst::qserv::ccontrol::UserQuerySelect<std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> > (this=0x7ffc21ece13f, __p=0x180ea80)      at /usr/include/c++/4.9/ext/new_allocator.h:120  #4  0x00007f4bf128607a in std::allocator_traits<std::allocator<lsst::qserv::ccontrol::UserQuerySelect> >::_S_construct<lsst::qserv::ccontrol::UserQuerySelect<std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> >(std::allocator<lsst::qserv::ccontrol::UserQuerySelect>&, std::allocator_traits<std::allocator<lsst::qserv::ccontrol::UserQuerySelect> >::__construct_helper*, (lsst::qserv::ccontrol::UserQuerySelect<std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&>&&)...) (__a=...,       __p=0x180ea80) at /usr/include/c++/4.9/bits/alloc_traits.h:253  #5  0x00007f4bf12858d4 in std::allocator_traits<std::allocator<lsst::qserv::ccontrol::UserQuerySelect> >::construct<lsst::qserv::ccontrol::UserQuerySelect<std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> >(std::allocator<lsst::qserv::ccontrol::UserQuerySelect>&, lsst::qserv::ccontrol::UserQuerySelect<std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&>*, (lsst::qserv::ccontrol::UserQuerySelect<std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&>&&)...) (__a=..., __p=0x180ea80) at /usr/include/c++/4.9/bits/alloc_traits.h:399  #6  0x00007f4bf1284c74 in std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> (this=0x180ea70, __a=...) at /usr/include/c++/4.9/bits/shared_ptr_base.h:515  #7  0x00007f4bf1283f40 in __gnu_cxx::new_allocator<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2> >::construct<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2><std::allocator<lsst::qserv::ccontrol::UserQuerySelect> const, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> > (this=0x7ffc21ece387, __p=0x180ea70) at /usr/include/c++/4.9/ext/new_allocator.h:120  #8  0x00007f4bf1283427 in std::allocator_traits<std::allocator<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2> > >::_S_construct<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySele---Type <return> to continue, or q <return> to quit---  ct>, (__gnu_cxx::_Lock_policy)2><std::allocator<lsst::qserv::ccontrol::UserQuerySelect> const, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> >(std::allocator<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2> >&, std::allocator_traits<std::allocator<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2> > >::__construct_helper*, (std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2><std::allocator<lsst::qserv::ccontrol::UserQuerySelect> const, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&>&&)...) (__a=..., __p=0x180ea70)      at /usr/include/c++/4.9/bits/alloc_traits.h:253  #9  0x00007f4bf1282887 in std::allocator_traits<std::allocator<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2> > >::construct<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2><std::allocator<lsst::qserv::ccontrol::UserQuerySelect> const, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> >(std::allocator<std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2> >&, std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2><std::allocator<lsst::qserv::ccontrol::UserQuerySelect> const, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&>*, (std::_Sp_counted_ptr_inplace<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, (__gnu_cxx::_Lock_policy)2><std::allocator<lsst::qserv::ccontrol::UserQuerySelect> const, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&>&&)...) (__a=..., __p=0x180ea70)      at /usr/include/c++/4.9/bits/alloc_traits.h:399  #10 0x00007f4bf1281b27 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> (      this=0x7ffc21ece928, __a=...) at /usr/include/c++/4.9/bits/shared_ptr_base.h:619  #11 0x00007f4bf1280e15 in std::__shared_ptr<lsst::qserv::ccontrol::UserQuerySelect, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> (      this=0x7ffc21ece920, __tag=..., __a=...) at /usr/include/c++/4.9/bits/shared_ptr_base.h:1090  #12 0x00007f4bf1280388 in std::shared_ptr<lsst::qserv::ccontrol::UserQuerySelect>::shared_ptr<std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> (this=0x7ffc21ece920, __tag=..., __a=...)      at /usr/include/c++/4.9/bits/shared_ptr.h:316  #13 0x00007f4bf127f778 in std::allocate_shared<lsst::qserv::ccontrol::UserQuerySelect, std::allocator<lsst::qserv::ccontrol::UserQuerySelect>, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, ---Type <return> to continue, or q <return> to quit---   std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> (__a=...)      at /usr/include/c++/4.9/bits/shared_ptr.h:588  #14 0x00007f4bf127eafb in std::make_shared<lsst::qserv::ccontrol::UserQuerySelect, std::shared_ptr<lsst::qserv::qproc::QuerySession>&, std::shared_ptr<lsst::qserv::qdisp::MessageStore>&, std::shared_ptr<lsst::qserv::qdisp::Executive>&, std::shared_ptr<lsst::qserv::rproc::InfileMergerConfig>&, std::shared_ptr<lsst::qserv::qproc::SecondaryIndex>&, std::shared_ptr<lsst::qserv::qmeta::QMeta>&, unsigned int&, std::string&> () at /usr/include/c++/4.9/bits/shared_ptr.h:604  #15 0x00007f4bf127cf8d in lsst::qserv::ccontrol::UserQueryFactory::newUserQuery (this=0x17c79a0, query=""SELECT * FROM R LIMIT 1"", defaultDb="""")      at core/modules/ccontrol/UserQueryFactory.cc:126  Python Exception <type 'exceptions.ValueError'> Cannot find type const std::map<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_Rep_type:   #16 0x00007f4bf129f3aa in lsst::qserv::czar::Czar::submitQuery (this=0x17c6df0, query=""SELECT * FROM R LIMIT 1"", hints=std::map with 3 elements)      at core/modules/czar/Czar.cc:125  Python Exception <type 'exceptions.ValueError'> Cannot find type const std::map<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_Rep_type:   #17 0x00007f4bf17e778a in lsst::qserv::proxy::submitQuery (query=""SELECT * FROM R LIMIT 1"", hints=std::map with 3 elements) at core/modules/proxy/czarProxy.cc:102  #18 0x00007f4bf17f5556 in _wrap_submitQuery (L=0x17703c0) at build/proxy/czarProxy_wrap.c++:4177  #19 0x00007f4bf38e50c4 in luaD_precall () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #20 0x00007f4bf38e54a4 in luaD_call () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #21 0x00007f4bf38e487b in luaD_rawrunprotected () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #22 0x00007f4bf38e565b in luaD_pcall () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #23 0x00007f4bf38e2ebc in lua_pcall () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #24 0x00007f4bf38f32f8 in luaB_pcall () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #25 0x00007f4bf38e50c4 in luaD_precall () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #26 0x00007f4bf38ee26a in luaV_execute () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #27 0x00007f4bf38e54ed in luaD_call () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #28 0x00007f4bf38e487b in luaD_rawrunprotected () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #29 0x00007f4bf38e565b in luaD_pcall () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #30 0x00007f4bf38e2ebc in lua_pcall () from /qserv/stack/Linux64/mysqlproxy/0.8.5+12/lib/libmysql-chassis.so.0  #31 0x00007f4bf1a08fa6 in proxy_lua_read_query (con=con@entry=0x176c470) at proxy-plugin.c:1227  #32 0x00007f4bf1a09155 in proxy_read_query (chas=chas@entry=0x1758620, con=con@entry=0x176c470) at proxy-plugin.c:1334  #33 0x00007f4bf36b4c4d in plugin_call (srv=0x1758620, con=0x176c470, state=<optimized out>) at network-mysqld.c:892  #34 0x00007f4bf36b6283 in network_mysqld_con_handle (event_fd=11, events=2, user_data=0x176c470) at network-mysqld.c:1617  #35 0x00007f4bf2958ed0 in event_process_active_single_queue (activeq=<optimized out>, base=<optimized out>) at event.c:1325  #36 event_process_active (base=<optimized out>) at event.c:1392  #37 event_base_loop (base=0x1769f40, flags=flags@entry=0) at event.c:1589  #38 0x00007f4bf2959b87 in event_base_dispatch (event_base=<optimized out>) at event.c:1420  #39 0x00007f4bf38dfa0a in chassis_event_thread_loop (event_thread=0x1769e90) at chassis-event-thread.c:466  #40 0x00007f4bf38df496 in chassis_mainloop (_chas=0x1758620) at chassis-mainloop.c:359  #41 0x0000000000402a09 in main_cmdline (argc=1, argv=0x7ffc21ed1d78) at mysql-proxy-cli.c:597  #42 0x00007f4bf20a1b45 in __libc_start_main (main=0x401db0 <main>, argc=6, argv=0x7ffc21ed1d78, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>,       stack_end=0x7ffc21ed1d68) at libc-start.c:287  ---Type <return> to continue, or q <return> to quit---  #43 0x0000000000401dde in _start ()  {code}",1
"DM-8189","11/04/2016 10:30:50","CatalogConstraintsPanel need to handle fetch errors","At the moment, when an error occurs, the panel does not update leaving the loading masks visible indefinitely.  This panel should instead update itself with error message(s) from the fetch.    TODO:  - Fix CatalogConstraintsPanel  - add generic error display into BasicTableView  - move createErrorTbl from TablesCntlr to TableUtil",2
"DM-8206","11/07/2016 16:44:28","Coverage is not shown if active table set before table loads","I think there are flaws in how showImages are set in FireflyViewerManager layoutManager saga. In the first catalog load, coverage is not shown if active table is set before the table is loaded.    At dispatchUpdateLayoutInfo time     When active table is set before table is loaded:                        tableResults.added     showImages=false  tableResults.active     showImages=false  table.loaded                showImages=true  charts.data/chartAdd  showImages=false    When active table is set after table is loaded:    tableResults.added     showImages=false  table.loaded                showImages=true  charts.data/chartAdd  showImages=false  tableResults.active     showImages=true    You can test it by changing line 340 in TablesCntlr.js:      dispatchActiveTableChanged(tbl_id, options.tbl_group);    instead of       dispatchAddSaga(doOnTblLoaded, {tbl_id, callback:() => dispatchActiveTableChanged(tbl_id, options.tbl_group)});",8
"DM-8210","11/08/2016 08:47:00","Revert accidental rename of id->objectId in ForcedPhotCoaddTask","The ""id"" column in coadd forced photometry outputs is being renamed to ""objectId"", due to some code intended to *add* an ""objectId"" column to CCD level forced photometry being accidentally applied.",0.5
"DM-8211","11/08/2016 09:29:46","Add support for reading a catalog schema","It would be helpful to be able to read a catalog schema without reading the entire catalog (e.g., to prepare for concatenating catalogs).  I propose adding {{static}} methods:    {code}  Schema::readFits(std::string const& filename);  Schema::fromFitsMetadata(daf::base::PropertySet & header);  {code}",1
"DM-8212","11/08/2016 09:30:30","Add support for reading metadata, length and schema of a catalog","The {{butler.get(dataset + ""_md"", dataId)}} pattern is only supported for images, not catalogs.  It would be useful to allow this to work on catalogs.  At the same time, it would be useful to add support for getting the length and schema of a catalog.",1
"DM-8215","11/08/2016 11:24:47","Use DAX metaServ to properly handle multiple databases in PDAC","5/15/2017    dbserv v1 (albuquery) is integrated with metaserv v1. As a consequence, column metadata are returned with with the data in the metadata section of the result.    As a part of this ticket, dbserv v1 and metaserv v1 were tested and debugged, tickets were created for outstanding issues, and SUIT code was updated.     * Consolidate PDAC table information, used by SUIT in LSSTMetaInfo.json on the server side. In the future this file might be converted to a table and stored in metaserv under SUIT curation. To facilitate development, we'd like to store it as a resource with the source code.   * Update SUIT code to work with dbserv v1 (albuquery) and metaserv v1  a. Query metaserv to create table constraints table  b. Use column metadata returned by dbserv for data definition info, such as units and description    XW (12/21/2016)   We will be using MetaServ to get the database names, data types served, table names, and other information.    09/18/2017 (Tatiana Goldina)    The information provided by the current [metaserv prototype]([http://lsst-qserv-dax01.ncsa.illinois.edu:5005/meta/v1/db/]) is not yet sufficient to automate search interface creation. There is an ongoing discussion with DAX team on which information is missing. See    [https://confluence.lsstcorp.org/display/DM/Data+Access+meeting+2017-09-11]    At the moment, it is not clear how to replace the information in [LSSTMetaInfo.json]([https://github.com/Caltech-IPAC/firefly/blob/dev/src/firefly/java/edu/caltech/ipac/firefly/resources/LSSTMetaInfo.json]) (used by SUIT to support PDAC) with the information from metaserv.    The current implementation is sufficient to get column name, datatype, unit, description, if we'd like to descope the use of metaserv just for this purpose.    Metaserv prototype:   [http://lsst-qserv-dax01.ncsa.illinois.edu:5005/meta/v1/db]    Metaserv documentation:   [http://dm.lsst.org/dax_metaserv/api.html]    Table definitions in the available database schema:   [http://lsst-qserv-dax01.ncsa.illinois.edu:5005/meta/v1/db/W13_sdss/sdss_stripe82_00/tables/]    Dump of column definitions:   [https://gist.github.com/brianv0/e6cfc4ba36ced2a57eb131210ba16c46#file-dump-yaml-L34]    _______________________________________    This ticket list two issues to be considered later:   * The test version of the two search processors had hard coded database's name. But there will be multiple databases, we need to find a better way to handle it. Should it be passed to the server side or should it be stored in a configuration file? Currently, if the database name is not defined in the property, the hard coded default name is used.  {code} //private static final String DATABASE_NAME =AppProperties.getProperty(""lsst.database"" , ""gapon_sdss_stripe92_patch366_0"");   private static final String DATABASE_NAME =AppProperties.getProperty(""lsst.database"" , """");  {code}     * When the DAX server is down, it takes a long time to return a failed message back. To avoid confusing the users, the timeout control is set in in the getDataToFileUsingPost of the URLDownload class. The default value is 30seconds. The timeout field may be stored in the property file later.  {code}             URLConnection c = makeConnection(url, cookies, requestHeader, false);               c.setConnectTimeout(timeout * 1000);//Sets a specified timeout value, in milliseconds               c.setReadTimeout(timeout * 1000);  {code}",20
"DM-8216","11/08/2016 11:33:56","Pass full available precision, when doing histogram for Long values","This ticket will handle the first part of DM-8180, which will result in correct histogram display for the long values that can be converted into doubles without loosing precision.  (ex. LSST htm index)",2
"DM-8221","11/08/2016 15:04:30","Inconsistency in forced schema catalogs","[~nchotard] has discovered an inconsistency between the {{deepCoadd_forced_src}} catalogs and their accompanying {{_schema}} dataset in some processed Megacam data; one of these (I forget which) has a few additional fields, which mostly seem related to aperture correction.    First step is to add a test for this in pipe_task's {{testCoadds.py}}; if that doesn't fail, we can try to reproduce in ci_hsc, and if that fails I'll ask for more details about the specific dataset.",2
"DM-8222","11/08/2016 17:04:03","further qserv container time offset fix","Improve the original fix proposed by [DM-8119] to make it fully compatible with the Debian Linux distribution used to build Qserv containers. It requires to have the following two files to be properly initialized:  {code}  /etc/localtime  /etc/timezone  {code}  *NOTE*: the second file doesn't exists in the RedHat-based Linux distributions.    Some ideas on how to solve this problem for Docker container can be found in the following documents:  * [https://www.ivankrizsan.se/2015/10/31/time-in-docker-containers/]  * [https://debian-administration.org/article/213/Changing_the_timezone_of_your_Debian_system]",1
"DM-8223","11/08/2016 17:10:05","write a test for getting mapper from _parent in v1 butlers","write a test that shows that butler can find mapper from _parent directories when using v1 butler functionality",1
"DM-8224","11/08/2016 17:27:49","DAX dbserv fails when returning a COUNT() result of zero.","This query:  {{curl -d 'query=SELECT+COUNT\(*)+FROM+RunDeepForcedSource+WHERE+objectId=3219370046129638' http://lsst-qserv-dax01.ncsa.illinois.edu:5000/db/v0/tap/sync}},  which selects no rows, fails with a spectacular stream of HTML containing an error report (see attached file).      The equivalent `SELECT+*` works fine.    I suspect this is a {{dbserv}}-level problem.  Note that the {{<title>}} element in the HTML contains the string ""TypeError: Decimal('0') is not JSON serializable"", which seems very suggestive.    This is a rather high priority to fix, because it is easy to imagine the PDAC SUIT generating queries containing {{COUNT()}} clauses.  I don't _know_ that there already is such code, though.  ",1
"DM-8227","11/08/2016 23:01:40","webserv: don't use single-threaded flask internal server","The flask internal web server is single-threaded, which will cause web services to be blocked e.g. during long running queries.  We will need to place some sort of multithreaded/pooling web server (e.g. nginx + wsgi) ahead of the flask service for acceptable behavior until implementing/adopting disconnected queries. ",2
"DM-8228","11/08/2016 23:19:36","Discuss 20% DR1 dataset strategy with Serge","Talked to Serge about the existing 10% DR1 dataset, and potential strategies for building a 20% DR1 dataset.    The existing dataset was produced from SDSS stripe82 data, scaled up to 10% DR1 by replicating to cover greater sky area.  Serge thinks we may not be able to similarly reach a 20% DR1 dataset by just scaling coverage (there may not be enough unused coverage).    Instead, we may need to increase chunk density by modifying the replicator to map source HTM cells to HTM cells of the next smaller scale.  This would increase density by a factor of four; we could then either manipulate coverage or thin data during replication to converge on the 20% DR1 target.",0.5
"DM-8229","11/09/2016 09:00:50","Add sims_survey_fields to lsstsw","A new package, {{sims_survey_fields}}, will be added to the _repos.yaml_ in {{lsstsw}}.",1
"DM-8230","11/09/2016 12:45:52","forcedPhotCcd.py doesn't work on DECam data","{{forcedPhotCcd.py}} on DECam data with {{tract,visit,ccdnum}} data IDs fails with a registry lookup error.  This is due to an incomplete workaround for a known issue: [incomplete data IDs with skymap keys cannot be filled in by the registry|https://community.lsst.org/t/problem-with-megacam-dataid/1199/8], because the skymap keys are included in the registry queries but don't exist in the registry tables.  Our usual workaround for datasets like {{forced_src}} (which typically require a registry lookup to fill in unspecified data ID keys) is to do registry lookup on a similar type without a skymap key (such as {{src}}) by calling {{Butler.subset}}.    In {{obs_decam}}, however, the {{src}} template requires only {{visit, ccdnum}}, while {{forced_src}} also requires {{filter}}.  Because the data ID we're passing to {{Butler.subset}} is complete for {{src}}, it skips the registry lookup and passes an incomplete (for {{forced_src}}) data ID on to later code, which fails trying to complete a {{forced_src}} data ID that includes {{tract}}.    There are a few ways I can imagine fixing this:     - We could modify the {{obs_decam}} template to remove {{filter}}.  This is really just a band-aid, but it'd be easy, and since we're discovering this bug now I think it's unlikely there's any DECam CCD forced photometry results sitting on disk that this would break.     - We could copy some of the logic in {{ButlerSubset}} into the data ID mangling code for {{forcedPhotCcd.py}}, having it call {{getKeys}} and {{queryMetadata}} directly to fill out the data ID.  We could alternatively add a new {{Butler}} method to do this more flexibly that the {{forcedPhotCcd.py}} code could delegate to.     - We could modify {{ButlerSubset}} or {{queryMetdata}} to explicitly ignore skymap ({{tract}} and {{patch}}) data ID keys when querying the registry.  I think this would be a simple fix and it would avoid painful workarounds of the sort mentioned above when reading {{forced_src}} data.    [~ktl], [~npease], the last of the above solutions is my preferred one, but I wanted to check with you to make sure you weren't bothered by the special-casing of ""tract"" and ""patch"" data ID keys in the butler.  I'm happy to do the work myself (this is to support some DESC work that uncovered the bug, and I'm at DESC hack week).",1
"DM-8235","11/09/2016 15:36:16","make utils.sequencify allow dicts","per  https://community.lsst.org/t/bug-in-daf-persistence-blob-master-python-lsst-daf-persistence-utils-py/1389/2  allow dicts to count as a 'sequence' in sequencify.",1
"DM-8245","11/10/2016 10:46:23","Add Image tables, Science_Ccd_Exposure and DeepCoadd,  into LSST catalogs search panel ","- Add image tables, Science_Ccd_Exposure and DeepCoadd, into the item search list.   - show DD table if any of the image table is selected for LSSTMetaSearch.  - show the image metadata table for LSSTCatalogSearch.  ",1
"DM-8247","11/10/2016 14:18:26","Background job management","When the searches take long time, the packaging of the data to be downloaded takes long time, Firefly will put them in the background so users can continue to interact with the UI. ",40
"DM-8249","11/10/2016 14:25:46","Alert subscription system requirement gathering","to gather requirement and dependencies on other DM teams for Alert subscription system developed by SUIT.    After much discussion with AP team and the re-plan exercise, the requirement for alert subscription has been identified as the following:  *use the API that AP team will provide to*    # provide a UI for user to specify the filters on alerts of their interests, the destination of the alert to be sent  # save the specification in a DB  # provide UI to allow users to make modification of the filters and destinations of alerts  # possibly to annotate the alerts and allow user to access the annotation    This will involve SLAC for DB, NCSA for user management  ",8
"DM-8251","11/10/2016 17:08:17","Unicode string not being interpreted correctly.","The following (running on lsst-qserv-dax01)     {code}  dataRoot=""/datasets/gapon/data/DC_2013/calexps/""  butlerPolicy=""calexp""  ids = id {'filter': u'g', 'field': 694, 'camcol': 4, 'run': 5646}    def getImageByIds(self, ids):      butler = lsst.daf.persistence.Butler(dataRoot)      img = butler.get(butlerPolicy, dataId=ids)      return img, butler  {code}    Results in this error   {code}    File ""/lsst/stack/dax_imgserv/python/lsst/dax/imgserv/locateImage.py"", line 286, in getImageByIds      img = butler.get(self._butlerPolicy, dataId=ids)    File ""/lsst/stack/Linux64/daf_persistence/12.1/python/lsst/daf/persistence/butler.py"", line 586, in get      location = repoData.repo.map(datasetType, dataId)    File ""/lsst/stack/Linux64/daf_persistence/12.1/python/lsst/daf/persistence/repository.py"", line 180, in map      loc = self._mapper.map(*args, **kwargs)    File ""/lsst/stack/Linux64/daf_persistence/12.1/python/lsst/daf/persistence/mapper.py"", line 173, in map      return func(self.validate(dataId), write)    File ""/lsst/stack/Linux64/daf_butlerUtils/12.1+1/python/lsst/daf/butlerUtils/cameraMapper.py"", line 294, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/lsst/stack/Linux64/daf_butlerUtils/12.1+1/python/lsst/daf/butlerUtils/mapping.py"", line 142, in map      return ButlerLocation(self.python, self.persistable, self.storage, path, additionalData, mapper)    File ""/lsst/stack/Linux64/daf_persistence/12.1/python/lsst/daf/persistence/butlerLocation.py"", line 62, in __init__      self.additionalData.set(k, v)    File ""/lsst/stack/Linux64/daf_base/12.1/python/lsst/daf/base/baseLib.py"", line 4136, in _PS_setValue      return _propertyContainerSet(self, name, value, _PS_typeMenu)    File ""/lsst/stack/Linux64/daf_base/12.1/python/lsst/daf/base/baseLib.py"", line 4098, in _propertyContainerSet      raise lsst.pex.exceptions.TypeError(""Unknown value type for %s: %s"" % (name, t))  TypeError: Unknown value type for filter: <type 'unicode'>  {code}",0.5
"DM-8252","11/10/2016 17:26:35","Add and  modify the server side codes to search the LSSTCatalog from Science_Ccd_Exposure and DeepCoadd tables","Two tables are added in the LSSTCatalog's UI.  Thus, the search processors in the server side need to be updated to reflect the changes in the UI.      The following work will be done:  * Find out the ra and dec columns based on the table name passed from the UI  * Process the constraints passed from the UI and add to the SQL's ""where"" clause  * Re-arrange the codes   * Test all four tables to make sure they work for each of the search method except mutli-object.  * Add error handling when the search method is not supported  * Add error handling when the data type defined in the metadata of the DAX's JSON file is not the same as in the actual datatype in the same JSON file.   ",2
"DM-8254","11/11/2016 11:20:15","make butler silently allow compressed files (ending in .gz)","Butler needs to allow repositories to contain a mix of compressed files (that end in .gz) and not compressed files, where the policy template does not specify the compressed file extension (gz)",1
"DM-8255","11/11/2016 11:27:27","Error if CmdLineTask is given an empty rerun folder","CmdLineTask gets confused when it's given a rerun folder that already exists but is empty, and resulting in this error message:   {code:java}  Error: input directory specified by --rerun must have the same mapper as INPUT  {code}    This happens in use cases similar to what's wanted in RFC-249: a user creates a symlink for a rerun before running the task. ",1
"DM-8265","11/14/2016 03:42:24","xrootd master fails to start","Xrootd master crashes with following error message:  {code:bash}  Plugin loaded unreleased XrdSsiLPI unknown from logging libXrdSsiLog-4.so  Config neither ssi.loglib nor ssi.svclib directive specified in /qserv/run/etc/lsp.cf  Config '-l@' requires a logmsg callback function but it was found!  161114 10:34:25 208 XrdConfig: Logging plugin initialization failed.  {code}    This breaks Qserv CI in Travis.",3
"DM-8273","11/15/2016 13:09:13","Missing newlines in ConfigurableField persistence","{{ConfigurableField.save}} uses the Python buffer {{write}} method but doesn't append a newline (it should probably append two for readability).    This is leads to a failure to read persisted configs, first noticed in an lsst_py3 Jenkins run of tickets/DM-8230, in meas_modelfit:testMeasureImage.py:103, in {{copy.deepcopy}}.    My only guess as to how we haven't noticed this before is that it's somehow not causing a problem in Python 2, and we hadn't tested Python 3 until now.",0.5
"DM-8293","11/16/2016 14:39:04","Fix travis build for PR","Fix travis PR build are broken for unknown reason since a few month",2
"DM-8295","11/16/2016 14:41:36","Implement ws4py as third-party package installable by eups","Following the developer instructions for third-party packages, add the {{ws4py}} package as a third-party package that is installable by eups.",2
"DM-8296","11/16/2016 14:44:30","Make fork of firefly_client installable by eups","To support the afwDisplay backend for Firefly, firefly_client is being added to the stack as a third-party package. To more easily incorporate changes as firefly_client is developed, firefly_client has been forked to the LSST Github org from the Caltech-IPAC org. A branch will be created and the eups tables will be added to it.",2
"DM-8297","11/16/2016 15:49:46","Add examples to firefly_client repository","An examples subdirectory can be added to the firefly_client repository at the top level. It can contain sample Jupyter notebooks. The starting point can be notebooks for the Python API from the firefly repository, where the API was first developed.",2
"DM-8303","11/16/2016 19:43:43","Use qserv functions instead of scisql function in catalog search processor","Need to follow https://github.com/lsst/qserv/blob/master/UserManual.md#spatial-constraints-should-be-expressed-through-our-qserv_areaspec_-functions  to use the qserv functions, like     qserv_areaspec_box(      lonMin               DOUBLE PRECISION,  # [deg]    Minimum longitude angle      latMin               DOUBLE PRECISION,  # [deg]    Minimum latitude angle      lonMax               DOUBLE PRECISION,  # [deg]    Maximum longitude angle      latMax               DOUBLE PRECISION   # [deg]    Maximum latitude angle  )    qserv_areaspec_circle(      lon                  DOUBLE PRECISION,  # [deg]    Circle center longitude      lat                  DOUBLE PRECISION,  # [deg]    Circle center latitude      radius               DOUBLE PRECISION   # [deg]    Circle radius  )    qserv_areaspec_ellipse(      lon                  DOUBLE PRECISION,  # [deg]    Ellipse center longitude      lat                  DOUBLE PRECISION,  # [deg]    Ellipse center latitude      semiMajorAxisAngle   DOUBLE PRECISION,  # [arcsec] Semi-major axis length      semiMinorAxisAngle   DOUBLE PRECISION,  # [arcsec] Semi-minor axis length      positionAngle        DOUBLE PRECISION   # [deg]    Ellipse position angle, east of north  )    qserv_areaspec_poly(      v1Lon                DOUBLE PRECISION,  # [deg]    Longitude angle of first polygon vertex      v1Lat                DOUBLE PRECISION,  # [deg]    Latitude angle of first polygon vertex      v2Lon                DOUBLE PRECISION,  # [deg]    Longitude angle of second polygon vertex      v2Lat                DOUBLE PRECISION,  # [deg]    Latitude angle of second polygon vertex   ...  )    See more examples in https://confluence.lsstcorp.org/display/DM/PDAC+sample+queries+and+test+cases    ",1
"DM-8308","11/17/2016 12:44:25","Update LSST search panel to work for both catalog and image search","update the LSST search panel to work on   - catalog table and image table selection  - spatial search for either catalog or image search  - fix DD table operation bugs including     . constraint column entry is not focused  - Update status store for LSST search panel - store the status for each catalog.      ",8
"DM-8309","11/17/2016 12:51:52","Enable histogram in IRSAViewer","The story is about to branch off 'dev' a version of IRSAViewer to have the latest version of Firefly and enable:    - histogram feature (simple UI, no binning method options, only column/expression input)    Test version should be ready by beginning January.     Release plan for end of January.      ",2
"DM-8311","11/17/2016 13:42:29","fix broken jenkins jobs that execute on lsst-dev","The change of hardware for lsst-dev has broken the {{run-rebuild}} and {{run-publish}} jobs. See: https://community.lsst.org/t/official-release-of-lsst-dev01-and-replacement-of-lsst-dev/1386    There seems to be sort of odd problem with git checkouts:    {code:java}    ERROR: Error fetching remote repo 'origin'  hudson.plugins.git.GitException: Failed to fetch from https://github.com/lsst-sqre/buildbot-scripts.git   at hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:797)   at hudson.plugins.git.GitSCM.retrieveChanges(GitSCM.java:1051)   at hudson.plugins.git.GitSCM.checkout(GitSCM.java:1082)   at org.jenkinsci.plugins.multiplescms.MultiSCM.checkout(MultiSCM.java:143)   at hudson.scm.SCM.checkout(SCM.java:495)   at hudson.model.AbstractProject.checkout(AbstractProject.java:1269)   at hudson.model.AbstractBuild$AbstractBuildExecution.defaultCheckout(AbstractBuild.java:604)   at jenkins.scm.SCMCheckoutStrategy.checkout(SCMCheckoutStrategy.java:86)   at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:529)   at hudson.model.Run.execute(Run.java:1741)   at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)   at hudson.model.ResourceController.execute(ResourceController.java:98)   at hudson.model.Executor.run(Executor.java:410)  Caused by: hudson.plugins.git.GitException: Command ""git fetch --tags --progress https://github.com/lsst-sqre/buildbot-scripts.git +refs/heads/*:refs/remotes/origin/* --depth=1"" returned status code 128:  stdout:   stderr: fatal: git fetch-pack: expected shallow list  fatal: The remote end hung up unexpectedly  {code}  ",0.5
"DM-8312","11/17/2016 17:10:13","Values are not kept after switching from one panel tab to another ","Latest LC viewer doesn't keep the value of the form when switching from one option tab to another. Some of the field value are kept, the other are reset and back to their initial values.",1
"DM-8325","11/18/2016 10:20:04","Detailed LSST the Docs product monitoring","Parse the output of keeper.lsst.codes/products in order to retrieve a list of published products, and monitor the health of those endpoints.",5
"DM-8329","11/18/2016 13:35:28","terraform deployment for liveness monitoring","A basic terraform deployment that setups a VPC, ACLs, an EIP, and route53 entries for {{status.lsst.codes}}.",0.5
"DM-8336","11/21/2016 07:01:58","Improve Docker image creation","qserv/qserv:dev image is based on qserv/qserv:latest and may become huge if former one isn't update on a regular basis.    This ticket will allow to create qserv/qserv:dev from scratch to reduce its size.",2
"DM-8350","11/21/2016 13:11:40","PDAC v2, provide access to WISE/NEOWISE data","This epic captures the specific issues for PDAC v2  ",100
"DM-8355","11/22/2016 09:05:26","AssembleCcdTask failure with Python 2","E.g. when running ip_isr's {{examples/runAssembleTask.py}}:    {code}  Traceback (most recent call last):    File ""runAssembleTask.py"", line 36, in <module>      runAssembler()    File ""runAssembleTask.py"", line 21, in runAssembler      assembledExposure = assembleTask.assembleCcd(assemblyInput)    File ""/scratch/swinbank/ip_isr/python/lsst/ip/isr/assembleCcdTask.py"", line 200, in assembleCcd      ccd = next(assembleInput.values()).getDetector()  TypeError: list object is not an iterator  {code}    Looks like a bad conversion to Python 3: it assumes that {{dict.values()}} returns an iterator, but in Python 2 it returns a list. Simple (but ugly) fix:    {code}  --- a/python/lsst/ip/isr/assembleCcdTask.py  +++ b/python/lsst/ip/isr/assembleCcdTask.py  @@ -197,7 +197,7 @@ class AssembleCcdTask(pipeBase.Task):           ccd = None           if hasattr(assembleInput, ""has_key""):               # Get a detector object for this set of amps  -            ccd = next(assembleInput.values()).getDetector()  +            ccd = next(iter(assembleInput.values())).getDetector()               # Sent a dictionary of input exposures, assume one amp per key keyed on amp name                 def getNextExposure(amp):  {code}    Better suggestions welcome.    Rather shocking that this wasn't caught by tests. Sufficiently shocking that I'm wondering if I missed something. I suggest that rather than just committing the above tweak, this ticket should include a test which demonstrates that the task actually runs. An easy way to do that might be to simply migrate a slightly modified version of the code from {{examples/}} to {{tests/}}.",0.5
"DM-8356","11/22/2016 09:35:32","Port ci_hsc to log package","Switch from using {{pex_logging}} to {{log}} in {{ci_hsc}}. ",2
"DM-8357","11/22/2016 09:37:26","Port meas_modelfit to log package","Switch from using {{pex_logging}} to {{log}} in {{meas_modelfit}}. ",2
"DM-8358","11/22/2016 09:39:38","Port meas_mosaic to log package","Switch from using {{pex_logging}} to {{log}} in {{meas_mosaic}}. ",2
"DM-8359","11/22/2016 09:41:25","Port ctrl_pool to log package","Remove {{pex_logging}} dependency in {{ctrl_pool}}. ",2
"DM-8363","11/22/2016 11:32:04","Add macro for Pybind11 wrapping of LSST_CONTROL_FIELD","The {{LSST_CONTROL_FIELD}} macro defines multiple fields and methods in a Control object's C++ interface, most of which are implementation details hidden from the average programmer. These implementation methods must appear in an object's Python interface for {{pex_config}} to work correctly, but should not be wrapped manually as this would break the abstraction provided by the macro.    The simplest solution is to define a macro that adds all elements of an {{LSST_CONTROL_FIELD}} to the appropriate Pybind11 wrapper object. This macro must be defined in {{pex_config}} in a place where it can be included by the wrapper code for any other stack package.",2
"DM-8365","11/22/2016 12:33:47","deprecate pex_logging","With the implementation of RFC-203, {{pex_logging}} is no longer used within {{pipe_tasks}}.  Please check if there are project-supported codes that still use pex_logging and need to be ported, update documentations if necessary, and formally deprecate {{pex_logging}}. ",0.5
"DM-8372","11/22/2016 16:59:06","SUIT and Science Platform requirements definition","This epic is to finalize the requirements  for SUIT and science platform.  ",20
"DM-8373","11/22/2016 17:11:49","SUIT requirement from EE","make a list of the original SUIT requirement from EE  Help with the requirement flowdown  ",2
"DM-8374","11/22/2016 17:17:51","Add function for Pybind11 wrapping of PersistableFacade","Because pybind11 requires explicit wrapping of each template instantiation, the curiously recurring template pattern is difficult to wrap -- details of which methods need to be wrapped should be centralized in one place, but the instantiations naturally belong to the classes that inherit from the template.    The {{PersistableFacade}} interface is an example of the CRTP used by roughly a dozen classes across the stack. It should be wrapped by defining a function in {{afw::table::io}} for creating a pybind11 wrapper, then calling the function when wrapping a class that implements {{PersistableFacade<T>}}.",1
"DM-8375","11/22/2016 18:40:58","publish SUIT docs","We need to publish the documents for JS API, Python API, and Jupyter widgets.",40
"DM-8377","11/23/2016 02:50:34","Remove tests from Mariadb build","mariadb and mariadbclient eups package contains 330 GB of tests binaries/data. This should be removed to have a lighter stack. A mariadb package with test could be added in order to test mariadb install (or this could be performed at build time...).    {code}  qserv@worker3:/qserv/stack/Linux64/mariadb/10.1.18-1-g0e935dc$ du -skh *  20K     COPYING  28K     COPYING.LESSER  88K     COPYING.thirdparty  4.0K    CREDITS  12K     EXCEPTIONS-CLIENT  12K     INSTALL-BINARY  4.0K    README  20K     README-wsrep  285M    bin  12K     data  4.7M    include  66M     lib  712K    man  327M    mysql-test  24K     scripts  3.0M    share  2.8M    sql-bench  {code}",1
"DM-8387","11/23/2016 09:41:14","draft requirement of Level 3 and data space","write the draft requirement for Level 3, and its relationship with data space and JupyterHub. ",5
"DM-8390","11/23/2016 10:18:51","calibrateTask.py example broken","pipe_tasks provides {{examples/calibTask.py}}, which exercises {{CharacterizeImageTask}} and {{CalibrateTask}}.    As [reported by Mandeep Gill|https://community.lsst.org/t/problem-running-calibtask-py-example-script-from-pipe-tasks-dir-examples/1426] this currently fails as follows:    {code}  RuntimeError: No such FITS catalog file: /tigress/HSC/LSST/stack_20160915/Linux64/obs_test/12.1-9-g3e397f1+2/data/input/schema/icSrc.fits  {code}    In addition to that, the code also attempts to pass a {{VisitInfo}} to an {{ExposureF}} constructor, which fails.",1
"DM-8394","11/23/2016 11:32:31","Understanding supertask quanta (round 1)","Summary of our first discussion with Gregory and K-T which covered few simplest use cases.",2
"DM-8395","11/23/2016 11:51:11","Review the draft SUIT requirement document","Review  and give feedback of the draft SUIT requirement document. ",1
"DM-8409","11/28/2016 11:53:38","Feedback issues from firefly API testing","These are task taken from [~rhl] comments in DM-7321 that need to be done the the firefly library. They are the issues that are exclusively on the Firefly client/server side. I am also including some comments:    * -Putting up an image often generates a small image (zoom=1); a small resize of the window causes a resize.- *(#2)* - _implemented in DM-10948_  * -It'd be nice to have an option for a horizontal layout (cf. ds9) – displays tend to be wider than high, but the firefly (and default ds9) display is higher than wide so you lose image area. *(#3)*-  _implemented in DM-10948_  * In the layers display, one layer seems to be the image. Why does it have a colour and a tickbox? *(#5)* DM-13124  * Pixel coords are off by (0.5, 0.5) – the middle of the bottom left pixel should be (0, 0) (i.e. offset by (1, 1) from the fits standard, which assumes fortran-style 1-indexed arrays) *(#7)* DM-13126  * There doesn't seem to be a way to show coordinates allowing for XY0 *(#8)* _I am not sure what this means, i will need more detail_  (DM-13152)  * The nice inset detail of the image doesn't show the mask planes *(#11)* DM-13127  * The inset detail isn't always centred on the cursor. E.g. at the lower left corner, if you put the cursor on the LL pixel, that pixel is shown at the LL of the inset, not the centre *(#12)* _I am not sure this behavior is wrong, I need to be convinced_ DM-13128    * Colours for masks appear to be case sensitive (unlike at least most web standards), and illegal colours such as RED are silently ignored *(#13)*  DM-13132  * Support a rapid stream of visualization actions, some which come before async actions complete (_from slack discussion_). DM-13131          More notes:  * We have talked about make another type of api html entry point that will allow for grid type layout display. We might do this as part of the QA efforts. _*#2* and *#3* implemented in DM-10948_  * *#4, #9, #10* are probably on the python api side or need to be worked on both sides and need other tickets. I will works this out with [~shupe], they need separate tickets  * *#6* -  -I don't think this is an issue-, _update_ - will solve *#6* as I documented in the comment below.  *#1, #14* - already fixed as part of DM-7321      More issues:    Fast zooming + restore failed:  How to repeat:  Catalog search Gaia at m81: Got an coverage image.  Image search WISE at m81: The WISE image swapped the Gaia coverage map.  When zoomed in fast and all the way till the zoom option panel shows, clicked Restore before closed the zoom option panel, the image got lost and the error message showed:  ""ZoomOptionsPopup.jsx:52 Uncaught TypeError: Cannot read property 'zoomFactor' of null  at getInitialPlotState (ZoomOptionsPopup.jsx:52)  at Array. (ZoomOptionsPopup.jsx:87)  at dispatch (createStore.js:186)  at middleware.js:86  at index.js:85  at index.js:14  at dispatch (applyMiddleware.js:45)  at PlotImageTask.js:178  at Object.dispatch (index.js:11)  at process (ReduxFlux.js:269)  ""  ",2
"DM-8413","11/28/2016 14:41:17","Add DPDD requirements to LSE-61","As part of requirements tracing, go through DPDD, determine where requirements are missing and create new requirements in LSE-61 as required. This will also involve updating some of the existing requirements to match DPDD.",20
"DM-8417","11/28/2016 16:29:36","Wrap tests that depend on both image and table with pybind11","The following tasks depend on both {{afw::image}} and {{afw::table}}. Wrap them.    # testApCorrMap.py (enable one skipped test)  # testBox.py (add a test for the copy constructor)  # testExposureTable.py  # testPolygon.py  # testTableIO.py  # testTableUtils.py  # testValidPolygon.py  # testVisitInfo.py  # testWeather.py  ",8
"DM-8420","11/28/2016 16:56:39","Wrap coadd_utils with pybind11","May not require any wrapping, if so then this ticket is just for tracking. Otherwise, adjust SP.",2
"DM-8421","11/28/2016 16:58:48","Wrap pipe_base with pybind11","May not require any wrapping, if so then this ticket is just for tracking. Otherwise, adjust SP.",0
"DM-8422","11/28/2016 16:59:46","Wrap obs_test with pybind11","May not require any wrapping, if so then this ticket is just for tracking. Otherwise, adjust SP.",1
"DM-8424","11/28/2016 17:02:22","Wrap skypix with pybind11","May not require any wrapping, if so then this ticket is just for tracking. Otherwise, adjust SP.",0
"DM-8428","11/29/2016 04:44:41","Add support for container timezone for Debian host.","Linux does not provide a standard way to get container timezone",1
"DM-8439","11/29/2016 13:46:51","Add wrapper on astshim to take point lists","Once the new point classes are implemented in DM-1987 this will provide the wrapper on top of astshim to pass them in and get them out both as lists and as single points.",20
"DM-8440","11/29/2016 13:49:04","Create new Wcs class","Create the new flexible AST-backed WCS class.    This will be called {{lsst::afw:;geom::SkyWcs}} to indicate that it is a celestial WCS. It will implement most of the methods of the existing {{lsst::afw::image::Wcs}}. It will subclass from {{lsst::afw::geom::Transform<Point2Endpoint, SpherePointEndpoint>}}, because that class already performs the necessary pixel-to-sky transformations. It will include various standard frames to support the LSST pixel convention.    Note that this ticket is purely to create the new WCS class. It is not intended to replace usage of the existing WCS class.",20
"DM-8449","11/29/2016 17:28:21","LSST catalog search panel related bugs ","fix and update the following LSST catalog search panel issues:     - Catalog polygon search doesn't work (wrong parameters passed)  - Can not switch to catalog search after image search fails on creating the plot.   - shorten the decimal number display on the title of table tab.    ",1
"DM-8467","11/29/2016 18:26:45","Wrap lsst_distrib with pybind11","This is the new (taking over from DM-6168) top level ticket for gathering all pybind11 wrappers. When this ticket is merged into master the port is complete.",0
"DM-8491","11/30/2016 10:29:02","Add Psf-matched CTEs and Coadds as independent data products in DRP ","This ticket will incorporate psf-matched coadds into DRP and include the following:    1) Develop plan for processing Psf-Matched coadds as part of DRP including interface in pipe_tasks commandline tasks, and butler data product naming convention and mapped filesystem paths: https://community.lsst.org/t/who-are-the-stakeholders-for-psf-matched-coadds/1439  2) RFC plan.  3) Add new data products to appropriate mapper files, including getting input on ones with unique file paths like obs_subaru and obs_sdss.  4) Edit command line tasks (currently WarpAndPsfMatch, MakeCoaddTempExp, AssembleCoadd) to treat psf-matched and non-psf-matched coadds as separate entities.  ",8
"DM-8496","11/30/2016 11:27:43","DRP LDM-151 updates from Ivezic review","Fixes to LDM-151 DRP section in response to [~zivezic]'s comments.  ",2
"DM-8514","11/30/2016 15:22:51","Prepare ASTERICS demo","Demo will run a set of SQL queries on IN2P3 cluster.",5
"DM-8528","12/01/2016 00:54:13","Port jointcal to python 3","Now that the rest of the stack works with python3 (particularly the obs packages), it's time to bring jointcal into the fold.",1
"DM-8536","12/01/2016 11:29:50","Add postage stamp transmission/collection to alert_stream","DM-7452 extend the sample alert schema to include a postage stamp cutout file.  Need to  * modify alert_stream repo and Docker images to grab the new alert schema and postage stamp files  * modify producers to add ability to encode and transmit stamps (can only be done with encoding)  * add a consumer to decode and write stamp (to the local filesystem?)  ",5
"DM-8537","12/01/2016 11:44:03","Update container networking in alert_stream to use non-host network","Current version of the alert_stream producer/consumer + Kafka broker system (DM-7453 and DM-7454) uses Docker compose and just exposes all ports to the host network.  That won't work for scaling up to multiple hosts.  Need to replace this with better networking that will work for multiple hosts.",5
"DM-8547","12/01/2016 16:06:46","Replace print/cout in jointcal with lsst::log","Jointcal has a many {{print()}} and {{cout<<}} that clutter up the output. These should be converted to lsst.log levels as appropriate (e.g. most of the python print are probably .info, while many of the cout are .debug)/",8
"DM-8548","12/01/2016 16:11:12","Add more info in the image title in the triview","Currently the image title in the triview has filter name, like u, g, i, ...    It would be nice to add more information about the image, say deepCoaddId or scienceCcdExposureId.     Trey added:  see LsstSdssRequestList.js    [LZ 1-4-2017]  # For DeepCoadd,           Get the baseId from the selected row:                 baseId = deepCoaddId - deepCoaddId % 8            For each filter, the title is:                id = baseId + filterId  ( numerical addition here)                title = id+""-""+filterName  # For science CCD, its id structure is different than deepCoaddId      scienceCcdExposureId = run+filterId + field (  string concatenation)                      ",1
"DM-8549","12/01/2016 16:14:31","jenkins job(s) for sqre-git-snapshot","Resulted in two new jenkins jobs:    * {{backup/nightly-sqre-github-snapshot}} - a ""cron"" job that runs nightly, executing the snapshot process from inside a container  * {{backup/build-sqre-github-snapshot}} - builds the docker container used by {{backup/nightly-sqre-github-snapshot}}.    Containers are pushed to https://hub.docker.com/r/lsstsqre/sqre-github-snapshot/",2
"DM-8553","12/01/2016 17:38:34","Write jointcal photometry test for cfht","Once I have the tests in place for twinkles, write a similar jointcal photometry test for cfht. This would be the time to replace the currently-broken validation_data_jointcal/cfht data with one updated to have VisitInfo.",8
"DM-8556","12/01/2016 17:59:56","Rename validation_data_jointcal to testdata_jointcal","""validation_data_jointcal"" is really ""testdata_jointcal"" and should be named accordingly, before jointcal becomes a stack dependency. Then all the test data I'm using can live there and we won't need optional dependencies on really large data sets (which are always installed by lsst distrib and lsstsw).",1
"DM-8561","12/02/2016 09:49:35","Filter null measurements in post-qa","{{validate_base}} allows measurements to be {{None}} if they failed, though SQUASH doesn't accept null measurement values.    # have post-qa filter out null measurements before posting to SQUASH.  # move the configuration that decides what metrics to post to the command line, making it easier to change in the future (and disable AM3 for now).  # when handling errors, don't print JSON since django actually seems to return an HTML error page.",1
"DM-8566","12/02/2016 12:03:15","Some fixes to build process","This is an action item from [SLAC-IPAC meeting|https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=~tjohnson&title=Camera+Image+Visualization+meeting+with+IPAC]:    - generate pure war (without embedded tomcat)  - workaround duplicate nom.tam.fits classes, etc   - Add preferIPv4Stack to tomcat env    preferIPv4Stack=true is added to setenv.sh file in tomcat.  This is done in ife-evn repo.    duplicate of ticket https://jira.lsstcorp.org/browse/DM-7706  closing DM-7706.",3
"DM-8571","12/02/2016 12:30:16","The current search result table tab should be shown immediately","In catalog or image search, if the query takes a long time to get the result, the table tab is not shown until the result comes,  user still sees the previous search result during waiting.    It would be nice to show the current table tab immediately, so user can see that the search is going on.  How to repeat the case:  Make the first query: target (1,1), cone, 10 arcsec.  Then make the second query: same target, cone 1000 arcsec.  The second table won't show until the result comes out.  ",2
"DM-8579","12/02/2016 16:50:50","Clean up the server side visualization code","clean up including:    - moved some files our of server/visualize  - created a ...server.visaulize.imageretrieve package  - remove unused code  - clean up some caching to simplify  - removed aborting a plot  - cleaned up managing plot context  - moved the *Commands.java files to .../server/rpc  - general cleanup including some java 7 syntax updates and javadocs",3
"DM-8581","12/03/2016 11:22:26","add python unit test of Citizen","As part of tickets/DM-8417 I added a Python unit test of Citizen. [~pschella] would like me to add the same unit test to master.",1
"DM-8586","12/05/2016 12:56:44","Document policies for making DM talks, publications, etc available on Zenodo (or elsewhere)","It's (fairly) well known that DM has a policy of making materials deriving from talks given, papers published, etc by DM members available on [Zenodo|https://zenodo.org/communities/lsst-dm]. But we need some documentation specifying at least:    - The details of the policy (what should be made available, who is responsible, etc);  - The mechanism (how does one upload to Zenodo, what collections should the upload be added to, etc);  - Requirements regarding licensing.",2
"DM-8588","12/05/2016 13:52:56","Create DOIs for DMTN-006 and DMTN-021","PUB-39 requires DOIs for DMTN-006 and DMTN-021. For now, these DOIs are prepared manually since we haven't engineering a technote release process yet.",0.5
"DM-8590","12/05/2016 14:26:57","Remove cat dependency in ctrl_stats","Remove dependency on the ""cat"" package in ctrl_stats",8
"DM-8591","12/05/2016 15:01:58","Follow-up pybind11 behavior with numpy.int64s as indices in Python 3","DM-8557 introduced a workaround for an issue seen in afw's multiMatch module where a numpy.int64 could not be used as an index by SWIG under Python 3. This ticket is to see if pybind11 has a similar issue under Python 3, and introduce a fix if so.    Suggested procedure to diagnose the numpy.int64 as index issue is:    # Revert commit in afw from DM-8557 called ""Cast catalog indices to int as SWIG Py3 workaround""  # See if the afw test {{tests/testTableMultiMatch.py}} runs.",2
"DM-8594","12/05/2016 17:43:30","Attend Camera visualization weekly meeting","Attend the Camera visualization weekly meeting, act as the point of contact the UIUC team, bring back the requests and bug reports to Firefly team. ",2
"DM-8598","12/06/2016 16:16:21","Dispatch TBL_RESULTS_ACTIVE when table is removed","If i have 2 or more tables and i switch from one to another, the xy plot gets updated correctly.  Now, if i close one table, the next table gets active but the plot doesn't refresh/update and fails to show the xy plot of the active table.  Please check and fix.    --------------------  TG 12/7/2016    The bug is fixed in dev by adding TABLE_REMOVE to syncChartViewer saga in ChartsSync.js  However, Loi thinks TBL_RESULTS_ACTIVE should be dispatched when a table is removed.     To test this ticket, delete TABLE_REMOVE from ChartsSync.js:syncChartViewer saga - the charts should be still in sync with the table when a table is removed.",2
"DM-8604","12/07/2016 13:02:17","document the issues rising out of SUIT but needs DM attention","There are several issues rising out of SUIT requirement and design discussions. Those issues affect SUIT but need DM management attention. This ticket is an attempt to make a list of those issues and ask DM management/architecture team to address them. ",5
"DM-8605","12/07/2016 15:14:05","problems setting log level in Python unit tests","Example: meas_algorithms/tests/testMeasure.py  In the past, I could simply change [this line|https://github.com/lsst/meas_algorithms/blob/c33f9c998fe5757b14474aa91ccf447ead665f6f/tests/testMeasure.py#L45] to  {{Log.getLogger(""measurement"").setLevel(Log.DEBUG)}}  and then running the test would show the DEBUG log messages. But that no longer works since DM-7955.     I think this is because:    DM-7150 added a hard-coded log configuration, using {{lsst.log.configure_prop()}}, to the initialization code of Python unit tests ({{init()}} in {{utils/tests.py}} ).  It was done because the default log configuration at that time was disliked.     Later, DM-7955 improved the default config as well as the configuration process in {{log}}, and {{lsst.log.configure_prop()}} now would reset configuration (including resetting all loggers). Customizing the log level of a specific logger on the top of the test file no longer does what users want, because init() resets it (if I understand correctly)    So, I'm thinking to remove the log re-configuring in utils; in other words. revert DM-7150.  ",1
"DM-8606","12/07/2016 16:38:19","Improve call signature for makeCameraPoint","{{Detector::makeCameraPoint}} accepts arguments {{(geom::Point2D point, CameraSys cameraSys)}} instead of {{(geom::Point2D point const &, CameraSys cameraSys const &)}} and similarly for the {{CameraSysPrefix}} overload. This should be fixed. Note that it will require a matching change in the pybind11 wrapper.    Also {{Detector::makeCameraSys}} returns a {{CameraSys const}} but there is no need for the {{const}} because {{CameraSys}} is immutable.    This work should not be done until after we switch to pybind11. It's too minor to require changing sooner and would needlessly complicated the pybind11 transition.",0.5
"DM-8610","12/08/2016 14:37:11","Use more secure way to pass password to mysql","Implementation of `db.utils.loadSqlScript()` runs mysql command and passes  password as command line argument which is not very secure. We want better way to pass credentials to mysql, special protected defaults file is probably the best choice.",2
"DM-8619","12/09/2016 12:56:36","Enable testWarper.py with pybind11","Wrap as much code as needed to make {{testWarper.py}} and {{testWarpExposure.py}} work and enable that test.",3
"DM-8626","12/09/2016 17:42:27","ConfigDictField cannot handle unicode keys in py2","Trying to use a unicode string as a key in a ConfigDictField causes a type error in Python 2. Unfortunately unicode is difficult to avoid with pybind11. I suggest trying to auto-cast such keys to str if the key is string-like and the type doesn't match the expected key type.",1
"DM-8629","12/10/2016 15:42:20","Remove cmsd log message","On cc-in2p3 cluster (ccqserv123) next message appears in xrootd-console.log:  {code}  [2016-12-10T23:37:28.298+0100] [LWP:275] INFO  xrdssi.msgs (cmsd:0) - Meter: Insufficient space;  7GB available < 11GB high watermark  {code}  Maybe next lst.cf directive should be moved to global:  {code}      # Specify that no significant free space is required on servers      # Indeed current configuration doesn't expect to be dynamically      # written to, but export the space in R/W mode      cms.space 1k 2k  {code}",2
"DM-8630","12/11/2016 14:39:59","/ssd/lsstsw/stack not updating on lsst-dev01","Last weekly available is {{w_2016_48}}. Judging by the logs, builds have been failing since 29 November. Error is:    {code}  ***** error: from /ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/build.log:  buildConfig([""doc/doxygen.conf""], [""doc/doxygen.conf.in""])  doxygen /ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/daf_persistence-12.1-15-gaf6b168/doc/doxygen.conf  /ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/daf_persistence-12.1-15-gaf6b168/src/DbStorageImpl.cc:192: warning: argument 'am' of command @param is not found in the argument list of lsst::daf::persistence::DbStorageImpl::startSession(std::string const &location)  DirectoryInstaller([""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/tests""], [""tests""])  Install file: ""ups/eupspkg"" as ""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/eupspkg""  Install file: ""ups/daf_persistence.cfg"" as ""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.cfg""  Install file: ""ups/daf_persistence.build"" as ""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.build""  DirectoryInstaller([""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/include""], [""include""])  DirectoryInstaller([""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/lib""], [""lib""])  DirectoryInstaller([""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/policy""], [""policy""])DirectoryInstaller([""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/python""], [""python""])Install file: ""ups/daf_persistence.table"" as ""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.table""  DirectoryInstaller([""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/src""], [""src""])  eups expandbuild -i --version 12.1-15-gaf6b168 /ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.build  DirectoryInstaller([""/ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/doc""], [""doc""])  eups expandtable -i -W '^(?!LOCAL:)' /ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.table  eups expandtable: Processing /ssd/lsstsw/stack/Linux64/daf_persistence/12.1-15-gaf6b168/ups/daf_persistence.table: Product miniconda2 is a dependency for log 12.1-4-gad3b865+2, but is not setup  scons: *** [table] Error 9  scons: building terminated because of errors.  + exit -5  eups distrib: Failed to build daf_persistence-12.1-15-gaf6b168.eupspkg: Command:          source /ssd/lsstsw/stack/eups/bin/setups.sh; export EUPS_PATH=/ssd/lsstsw/stack; (/ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/build.sh) >> /ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/build.log 2>&1 4>/ssd/lsstsw/stack/EupsBuildDir/Linux64/daf_persistence-12.1-15-gaf6b168/build.msg  exited with code 251  {code}",1
"DM-8642","12/12/2016 14:48:05","Understanding supertask quanta (round 2)","Few random thoughts on quanta to keep record of my misunderstanding.",2
"DM-8645","12/12/2016 16:37:59","Implement new matcherSourceSelector object for use in matchOptimisticB code. This links to DM-6824.","astrometrySourceSelector can not be used alone in matchOptimisticB as it tests for two different criteria depending on if it's matching objects(needs good centroid, is a parent object, has a minimum S/N, valid flux) or returning those objects to astrometry.py(all the previous plus is not saturated, is not interpolated, and is not edge). The new source selector matcherSourceSelector will make former selection, astrometrySourceSelector will make the second test. This ticket extends the work of DM-6824.",2
"DM-8646","12/12/2016 17:00:59","Update eups build tables in display_firefly","To make display_firefly be installable via eups, and with the implementation of its dependencies in DM-8295 and DM-8296, some updates are needed:    * Add the firefly_client packages as a dependency in ups/display_firefly.table  * Remove the xpa, pex_exceptions and swig parts from ups/display_firefly.cfg  * Add the display_firefly repository to etc/repos.yaml in the lsstsw repository",1
"DM-8647","12/12/2016 17:49:36","Enable autobrief in Doxygen","{{AUTOBRIEF}} is turned off by default in our Doxygen configuration. The developers guide   https://developer.lsst.io/docs/cpp_docs.html suggests using @brief but the very first example after that omits it and much of our code omits it. I think it should be optional and certainly much of our code reflects that.    The proposed fix is set the following to {{YES}} in in `doc/base.inc`:  - {{JAVADOC_AUTOBRIEF}}  - {{QT_AUTOBRIEF}}    We mostly use javadoc style, but there is no harm in supporting both, and some of our code may already use QT style.",0.5
"DM-8648","12/12/2016 19:11:10","Image viewer incorrectly displays flux unit when loading multiextension FITS file","Preparing next release of Herschel image set, Justin is using the IRSAViewer in {{OPS}} to load FITS file _(see attached an example of it that show the problem)_ and has found a problem with the image viewer (readout flux unit is incorrect but other value might be also affected).     When the multiextension FITS file attached is loaded, the 3 images are correctly loaded but the unit shown is the one from the last image ({{coverage}}). From the code, debug shows that {code:java}edu.caltech.ipac.firefly.server.visualize.ImagePlotCreator#makeAllNoBand{code} uses NO_BAND as a key, so the {{WebFitsData}} corresponding to each image is overwritten with the last one read.    The consequence is FITS values such as CVAL, NAXIS, and others of the ImagePlot might not be correct. In particular, the flux unit is '1' for all images when it should display 'Jy/beam' in the readout.    [LZ 1-3-2017]   The bug is located at the makeAllNoBand in ImagePlotCreator.java.  When the FITS file has multi-extension and the image is not three color image, each image 's band is NO-BAND.  The WebFitsData containing flux unit, dataMin, dataMax etc, is read in from the FitsRead and is stored in wfDataMap that is a  Map<Band, WebFitsData>.   However, the wfDataMap was created outside the for loop.  When each WebFitsData  is calculated, since the map key=NO_BAND is the same for all the extensions, the value in wfDataMap is replaced by the new value calculated in the loop.   When the loop is over, the wfDataMap contains the last   calculated wfData, ie., the last extension. Thus, the piArray contains three identical elements.   Except the flux unit is wrong, the dataMin, dataMax etc. are wrong as well.    {code}    ImagePlotInfo piAry[]= new ImagePlotInfo[readAry.length];           FileReadInfo readInfo;           Map<Band,WebFitsData> wfDataMap= new LinkedHashMap<Band,WebFitsData>(5);           for(int i= 0; (i<readAry.length); i++)  {                 WebFitsData wfData= makeWebFitsData(plot,frGroup, readInfo.getBand(),readInfo.getOriginalFile());                ...               wfDataMap.put(Band.NO_BAND,wfData);               ...               piAry[i]= new ImagePlotInfo(stateAry[i],plot, frGroup,readInfo.getDataDesc(), wfDataMap,fileWriterMap);           }             return piAry;  {code}    This block shows the correct wfData calculation for the same input (1250p4106_1342188754_SpirePhoto_L20_PMP500_SPG14.0.fits).  {code}  _dataMin = -0.11674876511096954  _dataMax = 0.7930733561515808  _fitsFileSize = 1198080  _fluxUnits = ""Jy/beam""  _beta = 0.03296741879503815      _dataMin = 7.146336138248444E-4  _dataMax = 0.11852526664733887  _fitsFileSize = 1198080  _fluxUnits = ""Jy/beam""  _beta = 0.0044617014275964454    _dataMin = 0.0  _dataMax = 156.0  _fitsFileSize = 1198080  _fluxUnits = ""1""  _beta = 1.0  {code}      This block shows what the value actually received.   {code}  _dataMin = 0.0  _dataMax = 156.0  _fitsFileSize = 1198080  _fluxUnits = ""1""  _beta = 1.0  {code}    Since the wfDataMa is to store each WebFitsData, it should be initialized inside the for loop.    ",8
"DM-8652","12/13/2016 12:01:51","Wrap afw ds9 test with pybind11","Wrap {{testDs9.py}} with pybind11.  This was left over from DM-7799 because it didn't really fit there.",1
"DM-8654","12/13/2016 17:15:39","Policy::names(bool) ignores its argument","The method {{names(bool topLevelOnly = false)}} in {{pex::policy::Policy}} does not use its argument, always behaving as if {{topLevelOnly}} were {{true}}. It appears that this bug went undetected because the swig wrapper for {{names(list<string> &, bool, bool)}} masked {{names(bool)}}, preventing it from ever being called from Python code.    I propose that the implementation of {{names(bool)}} be changed to share code with {{names(list<string> &, bool, bool)}}, both fixing the immediate problem and preventing inconsistent behavior from calling the wrong overload in the future.",1
"DM-8656","12/13/2016 18:38:54","meas_astrom tests depend on PyQt4 and Qt4 and break with PyQt5 and Qt5","When building meas_astrom with lsstsw using -b option on CentOS7 with gcc 4.8.5 tests break through a dependency on matplotlib.    {code}  *** error building product meas_astrom.  *** exit code = 2  *** log is in /opt/lsstsw/build/meas_astrom/_build.log  *** last few lines:  :::::  [2016-12-13T22:08:07.868579Z]   File ""/opt/lsstsw/miniconda/lib/python2  .7/site-packages/matplotlib/backends/backend_qt5.py"", line 31, in <module>  :::::  [2016-12-13T22:08:07.868617Z]     from .qt_compat import QtCore, QtGui,   QtWidgets, _getSaveFileName, __version__  :::::  [2016-12-13T22:08:07.868716Z]   File ""/opt/lsstsw/miniconda/lib/python2  .7/site-packages/matplotlib/backends/qt_compat.py"", line 137, in <module>  :::::  [2016-12-13T22:08:07.868749Z]     from PyQt4 import QtCore, QtGui  :::::  [2016-12-13T22:08:07.868763Z] ImportError: No module named PyQt4  :::::  [2016-12-13T22:08:07.868775Z] The following tests failed:  :::::  [2016-12-13T22:08:07.869806Z] /opt/lsstsw/build/meas_astrom/tests/.test  s/testFitTanSipWcsHighOrder.py.failed  :::::  [2016-12-13T22:08:07.869939Z] 1 tests failed  :::::  [2016-12-13T22:08:07.870705Z] scons: *** [checkTestStatus] Error 1  :::::  [2016-12-13T22:08:07.875630Z] scons: building terminated because of err  ors.  {code}",1
"DM-8660","12/14/2016 13:00:50","Create a Unit Test for Geom class","The unit test is needed for Geom class. This class is used for making images.  Therefore it is important to make sure it works as it should.    Analysis by Lijun:  * Add unit test for its public method  * Add end to end test for its seven cases so that all methods can be run.  * Prepare different input and output data set to test different combinaitons",8
"DM-8661","12/14/2016 13:04:34","Create a Unit Test for ImageData class","To ensure the Firefly works correctly, the unit test for ImageData is needed.",8
"DM-8662","12/14/2016 15:57:31","Incorrect PYTHONPATH in firefly_client repo","The implementation of firefly_client as a third-party package in DM-8296 does not allow the module to be imported. A small change is needed to the PYTHONPATH in the eups table.     The problem was missed in development because the module importing was tested in the repository's base directory. It is necessary to change directories before testing that the import works.",0.5
"DM-8667","12/15/2016 11:04:39","Changes to lsstsw to add ctrl_platform_lsstvc remove old ctrl_platform_* references","This work removes the following:    ctrl_platform_lsst  ctrl_platform_gordon    from etc/repos.yaml    adds:    ctrl_platform_lsstvc    to etc/repos.yaml    and moves the repos to    lsst-dm/legacy_ctrl_platform_lsst  lsst-dm/legacy_ctrl_platform_gordon  ",2
"DM-8676","12/15/2016 14:09:23","Hide matplotlib imports in meas_algorithms","meas_algorithms has several ""naked"" matplotlib imports that happen at the top level, and thus prevent anything that imports meas_algorithms from subsequently setting the backend. A simple fix is to move those imports into the functions where matplotlib is used, so they only occur if matplotlib is run, and preferably wrap them in try:except and log.warn on any errors.    This is a stop-gap until 5790 is properly dealt with.",1
"DM-8678","12/16/2016 08:56:28","update jenkins qserv container generation job","Request from [~fritzm] via slack: ""update CI to generate it using following command `./1_build-image.sh -CD`"".    As jenkins is currently using a fork of the qserv docker scripts with some minor modifications, testing will be required to migrate to the qserv master branch.  ",1
"DM-8686","12/16/2016 17:51:06","Change Child Repo Access to Parent Registries","Child Repositories need to be able to perform lookups in the sqlite Registry of one or more parent repositories.    Old Butler repositories used to find the registry by looking in the current repo, then following the _parent symlink until a registry was found.    New Butler repositories do not access their parent repositories directly.    The shorter term fix is for butler to pass all of a repo’s parent sqlite registries to a repo (via an ordered list of registry objects, each of which is of course a reference). The butler will use that list, whose order will be 1. a local sqlite registry (if present), 2. all the parent registries in order, and 3. use a PosixRegisry on the local repository. In the case of no results found when searching sqlite registries, butler will verify that the tables specified for the policy are in the registry. If they are, butler will return ‘no results’. If the tables are not in that registry, butler will look in the next registry.    (The better/longer term fix is “output registries”, where butler updates a repo-local registry when writing a dataset to a repo, and never does lookups in parent registries.)  ",8
"DM-8688","12/16/2016 18:14:54","testPsfSelectTest fails when run with via ""pytest *.py""","testPsfSelectTest fails when run via {{pytest *.py}} in {{meas_algorithms/tests}}, but passes if run with {{python testPsfSelectTest.py}} or {{pytest testPsfSelectTest.py}}. Sample failing run shown below. I'm betting this is a badly-initialized random seed (e.g. the one on line 45).    Including [~vpk24] since it looks like he did the pytest port, and [~sullivan] because he added the new-style test boilerplate at the bottom.    {code}  $ pytest *.py  ============================= test session starts ==============================  platform linux2 -- Python 2.7.12, pytest-3.0.4, py-1.4.31, pluggy-0.4.0  rootdir: /home/parejkoj/lsst/lsstsw/build/meas_algorithms/tests, inifile:   plugins: cov-2.4.0  collected 147 items     testAstrometrySourceSelector.py ..........  testBinnedWcs.py ...  testCoaddApCorrMap.py ...  testCoaddBoundedField.py ....  testCoaddPsf.py ...........  testCr.py ....  testDetection.py ...  testExecutables.py .....  testGaussianPsf.py ...........  testGaussianPsfFactory.py .....  testHtmIndex.py .........  testInstallGaussianPsf.py ......  testInterp.py ......  testLoadReferenceObjects.py ....  testMeasure.py .....  testMeasureApCorr.py ..........  testNegative.py ...  testPsfAttributes.py ...  testPsfCandidate.py .....  testPsfDetermination.py .........  testPsfIO.py .......  testPsfSelectTest.py F...  testReadFitsCatalog.py ........  testReadTextCatalog.py ......  testTicket-2986.py ...    =================================== FAILURES ===================================  ___________________ PsfSelectionTestCase.testDistortedImage ____________________    self = <testPsfSelectTest.PsfSelectionTestCase testMethod=testDistortedImage>        def testDistortedImage(self):                detector = self.detector                psfSigma = 1.5          stars = plantSources(self.x0, self.y0, self.nx, self.ny, self.sky, self.nObj, psfSigma, detector)          expos, starXy = stars[0], stars[1]                # add some faint round galaxies ... only slightly bigger than the psf          gxy = plantSources(self.x0, self.y0, self.nx, self.ny, self.sky, 10, 1.07*psfSigma, detector)          mi = expos.getMaskedImage()          mi += gxy[0].getMaskedImage()          gxyXy = gxy[1]                kwid = 15  # int(10*psfSigma) + 1          psf = measAlg.SingleGaussianPsf(kwid, kwid, psfSigma)          expos.setPsf(psf)                expos.setDetector(detector)                ########################          # try without distorter          expos.setDetector(self.flatDetector)          print(""Testing PSF selection *without* distortion"")          sourceList = self.detectAndMeasure(expos)          psfCandidateList = self.starSelector.run(expos, sourceList).psfCandidates                ########################          # try with distorter          expos.setDetector(self.detector)          print(""Testing PSF selection *with* distortion"")          sourceList = self.detectAndMeasure(expos)          psfCandidateListCorrected = self.starSelector.run(expos, sourceList).psfCandidates                def countObjects(candList):              nStar, nGxy = 0, 0              for c in candList:                  s = c.getSource()                  x, y = s.getX(), s.getY()                  for xs, ys in starXy:                      if abs(x-xs) < 2.0 and abs(y-ys) < 2.0:                          nStar += 1                  for xg, yg in gxyXy:                      if abs(x-xg) < 2.0 and abs(y-yg) < 2.0:                          nGxy += 1              return nStar, nGxy                nstar, ngxy = countObjects(psfCandidateList)          nstarC, ngxyC = countObjects(psfCandidateListCorrected)                print(""uncorrected nStar, nGxy: "", nstar, ""/"", len(starXy), ""   "", ngxy, '/', len(gxyXy))          print(""dist-corrected nStar, nGxy: "", nstarC, '/', len(starXy), ""   "", ngxyC, '/', len(gxyXy))                ########################          # display          if display:              iDisp = 1              ds9.mtv(expos, frame=iDisp)              size = 40              for c in psfCandidateList:                  s = c.getSource()                  ixx, iyy, ixy = size*s.getIxx(), size*s.getIyy(), size*s.getIxy()                  ds9.dot(""@:%g,%g,%g"" % (ixx, ixy, iyy), s.getX(), s.getY(),                          frame=iDisp, ctype=ds9.RED)              size *= 2.0              for c in psfCandidateListCorrected:                  s = c.getSource()                  ixx, iyy, ixy = size*s.getIxx(), size*s.getIyy(), size*s.getIxy()                  ds9.dot(""@:%g,%g,%g"" % (ixx, ixy, iyy), s.getX(), s.getY(),                          frame=iDisp, ctype=ds9.GREEN)                # we shouldn't expect to get all available stars without distortion correcting  >       self.assertLess(nstar, len(starXy))  E       AssertionError: 64 not less than 64    testPsfSelectTest.py:452: AssertionError  ----------------------------- Captured stdout call -----------------------------  Testing PSF selection *without* distortion  Testing PSF selection *with* distortion  uncorrected nStar, nGxy:  64 / 64     0 / 9  dist-corrected nStar, nGxy:  64 / 64     0 / 9  ----------------------------- Captured stderr call -----------------------------  sourceDetection INFO: Detected 70 positive sources to 5 sigma.  sourceDetection INFO: Resubtracting the background after object detection  measurement INFO: Measuring 70 sources (70 parents, 0 children)   measurement INFO: Measuring 1 sources (1 parents, 0 children)   sourceDetection INFO: Detected 70 positive sources to 5 sigma.  sourceDetection INFO: Resubtracting the background after object detection  measurement INFO: Measuring 70 sources (70 parents, 0 children)   measurement INFO: Measuring 1 sources (1 parents, 0 children)   ============================ pytest-warning summary ============================  WC1 /home/parejkoj/lsst/lsstsw/build/meas_algorithms/tests/testGaussianPsfFactory.py cannot collect test class 'TestConfig' because it has a __new__ constructor  =========== 1 failed, 146 passed, 1 pytest-warnings in 28.33 seconds ===========  {code}",1
"DM-8689","12/16/2016 18:32:52","In one place code calls log.warning instead of log.warn","In one place code from DM-8656 calls {{log.warning}} instead of {{log.warn}}",0
"DM-8691","12/19/2016 08:52:48","ConvolvedFluxPlugin should measure even if seeing is small","The {{ConvolvedFluxPlugin}} currently refuses to measure a source if the target seeing is smaller than the actual seeing.  This was a bad choice, as having a potentially-incorrect measurement is more useful than having nothing at all (so long as there's a flag indicating the problem).",1
"DM-8695","12/19/2016 13:37:11","Fix dataRef list creation bug introduced in DM-8230","A bug was introduced in DM-8230 such that the {{dataRef}} was not being appended to the refList in the {{PerTractCcdDataIdContainer}} found in *forcedPhotCcd.py* (resulting in an empty refList).  Please fix.",1
"DM-8696","12/19/2016 16:43:28","Wrap testTicketDM-433 with pybind11","Moved here from DM-6297 because it depends on DM-8674 to be merged.",1
"DM-8698","12/20/2016 09:12:54","Jenkins build failure in meas_modelfit","{code}  tests/testMeasureImage.py    /home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py2/lsstsw/miniconda/lib/python2.7/site-packages/astropy/config/configuration.py:687: ConfigurationMissingWarning: Configuration defaults will be used due to OSError:Could not find unix home directory to search for astropy config dir on None    warn(ConfigurationMissingWarning(msg))  Traceback (most recent call last):    File ""tests/testMeasureImage.py"", line 38, in <module>      import lsst.meas.modelfit.display    File ""/home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py2/lsstsw/build/meas_modelfit/python/lsst/meas/modelfit/display/__init__.py"", line 1, in <module>      from .densityPlot import *    File ""/home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py2/lsstsw/build/meas_modelfit/python/lsst/meas/modelfit/display/densityPlot.py"", line 63, in <module>      class HistogramLayer(object):    File ""/home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py2/lsstsw/build/meas_modelfit/python/lsst/meas/modelfit/display/densityPlot.py"", line 80, in HistogramLayer      defaults2d = dict(cmap=matplotlib.cm.Blues, vmin=0.0, interpolation='nearest')  AttributeError: 'module' object has no attribute 'cm'  -----------------------------------------------------  {code}",1
"DM-8701","12/20/2016 14:24:53","Copy and ingest HSC Cosmos data","Once RFC-266 is accepted, {{/gpfs/fs0/scratch/pprice/UH-Cosmos/*.fits}} needs to be copied into {{/datasets/hsc/raw/cosmos}} and ingested into {{/datasets/hsc/repo}}.",1
"DM-8714","12/21/2016 13:21:23","Fix position of psf computation for base_SdssShape_psf","There is an error in the psf computation of *base_SdssShape_psf* in {{meas_base}}'s *src/SdssShape.cc* in that it does not provide the position of the source, so is just getting the measurement at the position returned by {{afw}}'s {{getAveragePosition()}} in *src/detection/Psf.cc* for all sources.  Please fix.",1
"DM-8729","12/22/2016 16:32:47","Publish display_firefly to eups","This ticket captures work done to publish the display_firefly backend for afw.display to eups, including researching and learning the steps to do it.    * tag the ws4py and firefly_client dependencies appropriately  * rebuild display_firefly using Jenkins  * publish display_firefly using Jenkins    enabling:  {code}  export EUPS_PKGROOT=https://sw.lsstcorp.org/eupspkg  eups distrib list display_firefly # returns version to use in next line  eups distrib install display_firefly master-g0a33da8b30+2  {code}",2
"DM-8741","12/22/2016 19:15:17","Bug fixes, improvements, and code refactoring in SUIT S19","This epic includes the tickets for improvements, code refactoring,  and bug fixes needed for LSP portal.",100
"DM-8742","12/22/2016 19:18:12","Firefly code refactor and bug fixes (S19)","This version of pipelineQA portal could be used for ComCam commissioning.",100
"DM-8750","12/22/2016 22:25:28","eliminate jointcal compile warnings","Jointcal produces a number of compile-time warnings (which one can miss without the scons fix described in RFC-246). We should clean these up, which might be helped by compiling with both gcc and clang, and comparing their messages. It may be best to do this after we've dealt with other problems (like the boost pointer memory management), as that may take care of some of the warnings along the way. On the other hand, there's quite a bit of low-hanging fruit in the form of variables that are defined but never used.",2
"DM-8751","12/23/2016 09:17:53","Fix skipped testPickle in testSourceTable.py","There is one skipped test in {{testSourceTable.py}}: {{testPickle}}. This segfaults, likely when a source catalog is pickled. Fix this.     Should all kinds of catalog be pickleable? If so, make sure all are being tested.",2
"DM-8757","12/23/2016 12:17:12","monthly test of tickets related to UI changes and bug fixes (Jan. 2017)","Test all the tickets affecting UI when merged into dev. File Jira tickets  with the steps to duplicate the bugs.      Jan. 3 2017 Tested https://jira.lsstcorp.org/browse/DM-8548 and found the bug. See comment in the ticket.  Jan. 3 2017 Reviewed DM-8648 in git hub.   Jan. 4, 2017 Tested/reviewed DM-8548.   Jan. 9, 2017 Found a bug and issued ticket DM-8957:Original plot Inconsistent does not abort  Jan. 12, 2017 Reviewed DM-8957: Original plot does not abort and over writes active plot  Jan. 12, 2017 Reviewed DM-8049: Crop related classes need to be refactored  Jan. 17, 2017 Reviewed DM-7827: Create LSST File group processor for packaging (download the LSST images)  Jan. 20, 23, 2017 Reviewed DM-8668: LC period finder  Jan. 31, 2017 Tested DM-8985: mask",5
"DM-8758","12/23/2016 12:18:30","monthly test of tickets related to UI changes and bug fixes (Feb. 2017)","monthly test of tickets related to UI changes and bug fixes.  file tickets with steps to reproduce the bug.      2/1/2017 Reviewed DM-8660: Unit test for Geom.  2/2/2017 Reviewed DM-8670: Light Curve UI (wise data).  2/3/2017 Reviewed DM-7780: FlipXYTest  2/7/2017 Reviewed DM-9247: All Sky catalog query  2/8/2017 Reviewed DM-9248: All Sky mode  2/9/2017 Reviewed DM-8661: Unit Test for ImageData  2/12/2017 Reviewed DM-7946: UnitTestForCentralPoint  2/15/2017 Reviewed DM-7780: FlipXYTest  2/16/2017 Reviewed DM-8839: Image header unit test  2/17/2017 Reviewed DM-9470: Compass layout  2/21/2017 Reviewed DM-8845: UnitTest for ImagePlot class  ",5
"DM-8759","12/23/2016 12:19:25","monthly test of tickets related to UI changes and bug fixes (Mar. 2017)","monthly test of tickets related to UI changes and bug fixes  File tickets with steps to reproduce the bugs    3/1/2017: Tested DM-8578.  ",0.5
"DM-8760","12/23/2016 12:20:10","monthly test of tickets related to UI changes and bug fixes (Apr. 2017)","monthly test of tickets related to UI changes and bug fixes   file tickets with steps to reproduce the bugs    DM-10065 needs to be thoroughly tested. ",5
"DM-8761","12/23/2016 12:21:13","monthly test of tickets related to UI changes and bug fixes (May 2017)","monthly test of tickets related to UI changes and bug fixes   file tickets with steps to reproduce the bugs",5
"DM-8791","12/29/2016 10:22:47"," lsst-dm-mac.lsst.org is inaccessible","From my office desktop:  {code:java}  $ ping -i0.2 -c 10 -q lsst-dm-mac.lsst.org  PING lsst-dm-mac.lsst.org (140.252.32.108) 56(84) bytes of data.    --- lsst-dm-mac.lsst.org ping statistics ---  10 packets transmitted, 0 received, +10 errors, 100% packet loss, time 1869ms  pipe 10  $ ssh -vvv root@lsst-dm-mac.lsst.org  OpenSSH_7.2p2, OpenSSL 1.0.2j-fips  26 Sep 2016  debug1: Reading configuration data /home/jhoblitt/.ssh/config  debug1: Reading configuration data /etc/ssh/ssh_config  debug1: /etc/ssh/ssh_config line 3: Applying options for *  debug1: auto-mux: Trying existing master  debug1: Control socket ""/home/jhoblitt/.ssh/master/root@lsst-dm-mac.lsst.org:22"" does not exist  debug2: resolving ""lsst-dm-mac.lsst.org"" port 22  debug2: ssh_connect_direct: needpriv 0  debug1: Connecting to lsst-dm-mac.lsst.org [140.252.32.108] port 22.  debug2: fd 3 setting O_NONBLOCK  debug1: connect to address 140.252.32.108 port 22: No route to host  ...  {code}    From the production jenkins instance:  {code:java}  $ ssh -i /var/lib/jenkins/.ssh/id_rsa root@lsst-dm-mac.lsst.org -vvv  OpenSSH_6.6.1, OpenSSL 1.0.1e-fips 11 Feb 2013  debug1: Reading configuration data /etc/ssh/ssh_config  debug1: /etc/ssh/ssh_config line 56: Applying options for *  debug2: ssh_connect: needpriv 0  debug1: Connecting to lsst-dm-mac.lsst.org [140.252.32.108] port 22.  debug1: connect to address 140.252.32.108 port 22: Connection timed out  ssh: connect to host lsst-dm-mac.lsst.org port 22: Connection timed out  {code}",0.5
"DM-8817","01/03/2017 13:31:04","Port DMTN-017 LaTeX document to technote platform","DMTN-017 was written as a LaTeX doc before technotes existed. This story is to port DMTN-017 to a PDF viewer template (DM-4602) for PDF documents that can be published with LSST the Docs.",1
"DM-8822","01/03/2017 14:19:48","Fix octal umask handling in ctrl_pool","While reprocessing some hsc data, I received the error given below. This is because python3 does not implicitly convert integers with leading zeros into octal. We should make the ctrl_pool code use an explicit octal int for the umask defined at the top of parallel.py, and then use a better formatting string where necessary.    {code}  SyntaxError: invalid token    File ""<string>"", line 1      import os; os.umask(002); import lsst.ctrl.pool.log; lsst.ctrl.pool.log.jobLog(""singleFrame""); import lsst.pipe.drivers.singleFrameDriver; lsst.pipe.drivers.singleFrameDriver.SingleFrameDriverTask.parseAndRun();                            ^  SyntaxError: invalid token  Tue Jan  3 12:38:28 PST 2017  Done.  {code}",1
"DM-8825","01/03/2017 17:31:51","cannot run singleFrameDriver on hsc data with python3 due to ""__builtin__.str"" in *Mapper.paf","When attempting to reprocess some hsc data, I ran into a number of problems (one fixed in DM-8822). In this case, some of the python type definition in the obs_subaru {{Mapper.paf}} files are not valid: {{\_\_builtin\_\_.str}} doesn't exist on python3. Fortunately because of our use of futurize, we have {{builtins.str}}.    Although I don't plan to write it as part of this ticket, having a python2+3 validation system for the policy files would be good.",1
"DM-8830","01/04/2017 11:09:57","Fix accounting for fraction of successful measurements","The final accounting for success/fail of the KPMs in `validateDrp.py` appears to be off:    {code}  =================================================================  design level summary  =================================================================  FAILED (37/21 measurements)  =================================================================  {code}    1. Investigate why this is happening (likely a counter not being reset, or the denominator not being updated).  2. Decide on correct accounting and implement.",1
"DM-8832","01/04/2017 12:12:17","Butler mapper issue in validate_drp since Jenkins build 663","Since [build 663 on Jenkins|https://ci.lsst.codes/job/validate_drp/dataset=cfht,label=centos-7,python=py2/663/console] we're seeing a new Butler-related issue in {{validate_drp}} even with the CFHT dataset. Could there be a butler change that is triggering this issue? This would have happened January 3-4 2017.      {noformat}  [py2] $ /bin/bash -e /tmp/hudson8266472982640135803.sh  notice: lsstsw tools have been set up.  Ingesting Raw data  root INFO: Loading config overrride file '/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/obs_cfht/12.1-14-gddeedbd+20/config/ingest.py'  CameraMapper INFO: Unable to locate registry registry in root: /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/validate_drp/Cfht/input/registry.sqlite3  CameraMapper INFO: Unable to locate registry registry in current dir: ./registry.sqlite3  CameraMapper INFO: Loading Posix registry from /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/validate_drp/Cfht/input  CameraMapper INFO: Unable to locate calibRegistry registry in root: /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/validate_drp/Cfht/input/calibRegistry.sqlite3  CameraMapper INFO: Unable to locate calibRegistry registry in current dir: ./calibRegistry.sqlite3  CameraMapper INFO: Loading Posix registry from /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/validate_drp/Cfht/input  /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_cfht/master-g2016f8e221+2/raw/849375p.fits.fz --<link>--> /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/validate_drp/Cfht/input/raw/06AL01/D3/2006-05-20/r/849375p.fits.fz  /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_cfht/master-g2016f8e221+2/raw/850587p.fits.fz --<link>--> /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/validate_drp/Cfht/input/raw/06AL01/D3/2006-06-02/r/850587p.fits.fz  running processCcd  validating  Traceback (most recent call last):    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-gf3b529ce90+2/bin/validateDrp.py"", line 95, in <module>      validate.run(args.repo, **kwargs)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-gf3b529ce90+2/python/lsst/validate/drp/validate.py"", line 104, in run      **kwargs)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-gf3b529ce90+2/python/lsst/validate/drp/validate.py"", line 204, in runOneFilter      verbose=verbose)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-gf3b529ce90+2/python/lsst/validate/drp/matchreduce.py"", line 147, in __init__      repo, dataIds, matchRadius)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-gf3b529ce90+2/python/lsst/validate/drp/matchreduce.py"", line 175, in _loadAndMatchCatalogs      butler = dafPersist.Butler(repo)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/daf_persistence/12.1-17-g9654cba+1/python/lsst/daf/persistence/butler.py"", line 304, in __init__      self._addRepo(args, inout='out', defaultMapper=defaultMapper, butlerIOParents=butlerIOParents)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/daf_persistence/12.1-17-g9654cba+1/python/lsst/daf/persistence/butler.py"", line 399, in _addRepo      ""Could not infer mapper and one not specified in repositoryArgs:%s"" % args)  RuntimeError: Could not infer mapper and one not specified in repositoryArgs:RepositoryArgs(root='Cfht/output', cfgRoot=None, mapper=None, mapperArgs={}, tags=set([]), mode='rw', policy=None)  Validation failed  Build step 'Execute shell' marked build as failure  [PostBuildScript] - Execution post build scripts.  [py2] $ /bin/sh -xe /tmp/hudson1544818061461777296.sh  ++ lsof -d 200 -t  + Z=  + [[ ! -z '' ]]  + rm -rf /home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/.lockDir  Archiving artifacts  [INFO] HipChat notification sent to the following rooms: Bot: Jenkins  Finished: FAILURE  {noformat}",1
"DM-8837","01/04/2017 17:38:15","Reference catalog proper motions, parallaxes and errors","We will need to apply proper motion corrections when matching reference catalogs to measured catalogs.  This epic will implement that feature.  This will require several steps:  # Implement standardized schema (per RFC-271)  # Implement correction code  # Write tests and verify performance    The construction of reference catalogs is briefly discussed in LDM-151 v4.1 §6.1, but we defer to RFC-271 in terms of implementation detail. A deliverable for this epic will be documentation describing the format and contents of reference catalogs.    Per the comment below, this epic also covers implementing RFC-368, including adding parallaxes to the reference catalogs.",40
"DM-8841","01/04/2017 19:35:38","ci_hsc is broken","[~Parejkoj] in DM-8825 points out that ci_hsc is broken.  It appears to have broken between build [#932 (Jan 3, 2017 5:57:59 PM)|https://ci.lsst.codes/job/ci_hsc/932/] and [#933 (Jan 4, 2017 1:57:00 AM)|https://ci.lsst.codes/job/ci_hsc/933/].  Diagnose and fix.",1
"DM-8842","01/05/2017 08:57:31","LeastSqFitter1d(..., unsigned int order) should be signed","LeastSqFitter1d's constructor's {{order}} parameter is {{unsigned int}}. However it is stored as {{int}} and {{LeastSqFitter2d}} uses {{int}} in both places.    I suggest switching to {{int}}. The existing code always supplies a small positive integer, so it's safe and trivial to fix. However, it will require the same change to the pybind11 interface file.    I further suggest not changing this until after the pybind11 transition. Then change it and make sure the whole stack builds.",0.5
"DM-8866","01/05/2017 12:43:11","Wrap datarel with pybind11","May not have any work associated with it, but is an {{lsst_distrib}} dependency. Investigate and update SP.",0.5
"DM-8867","01/05/2017 12:43:51","Wrap lsst_apps with pybind11","May not have any work associated with it, but is an {{lsst_distrib}} dependency. Investigate and update SP.",0
"DM-8872","01/05/2017 12:52:20","Disable locking in shared-stack.py","[~price] points out that we can (and likely should) disable EUPS locking globally by adding    {code}  hooks.config.site.lockDirectoryBase = None  {code}    to the EUPS configuration.",1
"DM-8874","01/05/2017 12:54:24","Wrap display_ds9 with pybind11","May not have any work associated with it, but is an {{lsst_distrib}} dependency. Investigate and update SP.",3
"DM-8877","01/05/2017 12:56:13","Wrap skymap with pybind11","May not have any work associated with it, but is an {{lsst_distrib}} dependency. Investigate and update SP.",2
"DM-8890","01/05/2017 15:41:46","Investigate if  MariaDB 10.1+ DynamicField() can be used for storing JSON blobs","During deployment of SQuaSH we realized that JSONField() as implemented in DM-8414 works only with MySQL and that the corresponding field type for MariaDB is the DynamicField()    This ticket is to make sure we can use MariaDB features and stick with it in production.",2
"DM-8914","01/06/2017 06:04:40","Improve container build in Jenkins","Build script is here:  https://github.com/lsst-sqre/jenkins-dm-jobs/blob/master/pipelines/qserv/docker/build.groovy#L25-L26    dev containers should be created at each build, whereas release should only be created once a month.",5
"DM-8923","01/06/2017 09:54:12","remove jenkins hipchat notifications","The slack migration appears to be a success and HC is essentially unused.  Hipchat notifications from jenkins jobs should be safe to remove at this point and it is an opportunity to prune another plugin.",1
"DM-8933","01/06/2017 11:14:02","Fix formatting in validateDrp.py --help message","Fix formatting in validateDrp.py --help message    The ""description"" currently reads as    {code}  Calculate and plot validation Key Project Metrics from the LSST SRD.  http://ls.st/LPM-17\n Produces results to: STDOUT Summary of key metrics  REPONAME*.png Plots of key metrics. Generated in current working directory.  REPONAME*.json JSON serialization of each KPM. where REPONAME is based on the  repository name but with path separators replaced with underscores. E.g.,  ""Cfht/output"" -> ""Cfht_output_""  {code}    But it should read (as written in the string):    {code}      description = """"""      Calculate and plot validation Key Project Metrics from the LSST SRD.      http://ls.st/LPM-17        Produces results to:      STDOUT          Summary of key metrics      REPONAME*.png          Plots of key metrics.  Generated in current working directory.      REPONAME*.json          JSON serialization of each KPM.        where REPONAME is based on the repository name but with path separators      replaced with underscores.  E.g., ""Cfht/output"" -> ""Cfht_output_""      """"""  {code}    * I think this is just a matter of passing {{formatter_class=argparse.RawDescriptionHelpFormatter}} to {{argparse.ArgumentParser}}",0
"DM-8934","01/06/2017 11:28:28","Service Management & Emergent Work  (December) ","This story captures service management / emergent work  for December actives related to the LSST development servers and Nebula OpenStack. This include issues and project communications related to the Home Directory transition to NFS to GPFS, network settings maintenance in NPCF,  tuning of HTCondor settings & installation on lsst-dev01,  Nebula instance shutdowns / migrations to help support live migration functionality, and similar issues. ",5
"DM-8944","01/09/2017 10:02:02","migrate squash production DB to MariaDB 10.1.x","Migrate the production RDS instance from MariaDB 10.0 -> 10.1 in preparation for the next squash release.",0.5
"DM-8948","01/09/2017 12:14:15","ctrl_stats fails tests in 2017","{{lsst_py3}} is no longer building because {{ctrl_stats}} is getting the year wrong in tests.  {code}  tests/testTerminated.py    ...F.F..  ======================================================================  FAIL: test4 (__main__.TestTerminated)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testTerminated.py"", line 79, in test4      self.assertEqual(rec.timestamp, self.year+""-08-21 10:27:31"")  AssertionError: '2016-08-21 10:27:31' != '2017-08-21 10:27:31'    ======================================================================  FAIL: test6 (__main__.TestTerminated)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testTerminated.py"", line 102, in test6      self.assertEqual(rec.timestamp, self.year+""-08-21 10:29:43"")  AssertionError: '2016-08-21 10:29:43' != '2017-08-21 10:29:43'    ----------------------------------------------------------------------  Ran 8 tests in 0.040s    FAILED (failures=2)  {code}  3 other test files fail.  ",1
"DM-8949","01/09/2017 13:01:39","ctrl_stats doesn't calculate year to next year progress properly","An issue in DM-8948 brought up a problem in how the time are calculated within ctrl_stats.  It made an assumption and used a files creation date as the year to start with;  this worked throughout the year, but failed if the files were pulled in one year, and run in the next year. (time stamps of 2016 vs the year being 2017).   This assumption was fixed in DM-8948.    There is still a problem that exists in year to year calculations, for jobs that start in one year and end in the next.  There will be negatives times calculated.   This ticket will address this issue.",8
"DM-8957","01/09/2017 17:45:41","Original plot does not abort and over writes active plot","Sometimes in the irsa tri-view, the images don't match the table and the xy-plot.    For example:  In http://localhost:8080/firefly/lsst-pdac-triview.html;a=layout.showDropDown,  select ""Images"" and ""Science Ccd Exposure"",  target: ra = 9.6, dec = -1.1;    Then in the tri-view, five images have been downloaded (one of the scienceCcdExposureId is 1755410440). Now click any row which never downloaded images (for example, with the scienceCcdExposureId=4203410469), now this new set of images start to be downloaded. BEFORE (!) the downloading is finished, click back the very first row (1755410440) and wait for the downloading to finish. When the downloading is done, you will see the inconsistent tri-view:  The first row is selected in the table, the dot of the first row in the xy-plot is highlighted, but the images are the new set. If user keeps clicking on the first row, the tri-view won't change and the inconsistency stays. Only when the user clicks any other row, the inconsistency will be gone.    Bug?",5
"DM-8963","01/10/2017 12:41:07","FITS download (save) does not work for 3 color image if the type is FITS","To reproduce the problem:    # Start the IRSA viewer in the browser (/ocalhost:8080/firefly/)  # Select ""Create a New Plot 3-color""  # Select Red panel and Wise data  # Enter ""m31""  # Click ""Search""  # Click the file save icon in the toolbar  # Click ""Download""   # But nothing happens after clicking download    NOTE: If the type of file is PNG, it works.  ",0
"DM-8964","01/10/2017 13:14:57","Update singleFrameDriver following changes to reference catalogs","The signature of {{ProcessCcdTask.\_\_init\_\_}} has changed (in DM-8232 to support distinct reference catalogs).  {{SingleFrameDriverTask.\_\_init\_\_}} needs to be changed to match.",0.5
"DM-8965","01/10/2017 14:08:18","Extend Alert Production prototype with new index type","I want to try to improve indexing in the DiaObject table to make both search and insertion faster.",8
"DM-8967","01/10/2017 14:53:19","Documenting using emoji reactions in GitHub code reviews","After a discussion in Slack (a while ago in Dec 2016) we agreed that we're getting too many emails during code reviews when a developer replies ""Done"" to each line comment. A way around this is to use GitHub's emoji reactions: the dev can check off a comment, but an email is not sent out.    This ticket documents that process in the [Developer Workflow page|https://developer.lsst.io/processes/workflow.html] of developer.lsst.io.",0.5
"DM-8972","01/11/2017 11:15:00","obs_cfht table file uses envAppend","obs_cfht.table includes:  {code}  envAppend(DYLD_LIBRARY_PATH, ${PRODUCT_DIR}/lib)  envAppend(PYTHONPATH, ${PRODUCT_DIR}/python)  envAppend(PATH, ${PRODUCT_DIR}/bin)  {code}    These {{envAppend}} calls should be {{envPrepend}}.",1
"DM-8973","01/11/2017 12:47:31","Wrap new Footprints with pybind11 and create python unit test","Wrap the new Footprints class with pybind11. A success criteria for this is successfully porting and updating the Footprints python unit test to successfully run.",8
"DM-8978","01/11/2017 16:58:39","writing jointcal output is slow due to dataRef lookup","[~boutigny] noticed that jointcal is now very slow to write its output. This is at least in part due to the way I rewrote the output code to work with decam, which does not have ""ccd"" in its dataRefs. I think new slowdown is coming from the repeated calls to {{get(""calexp"").getDetector().getId()}}.    [~price] suggested making a dictionary to map visit and ccd name to each dataRef:    {code}  visit_ccd_to_dataRef = {(dataRef.dataId['visit'], dataRef.get('calexp').getDetector.getId()): dataRef for dataRef in dataRefs}  {code}    and then replacing the for loop and if statement with a lookup in the dict.    Note to Butler people: this problem is related to the problem of having no standard set identifiers in the dataIds: if we can guarantee that ""visit"" and ""ccd"" are always there (and, I'd argue, some other things), this code would be quite a bit simpler.",2
"DM-8980","01/12/2017 09:36:54","Revise Python Style Guide for RFC-107 (79 character docstring lengths)","This ticket will implement RFC-107, which states that all Python docstrings and comments must have a maximum line length of 79 characters.  ",1
"DM-8982","01/12/2017 10:37:10","Incorrect binning in overscan spline interpolation","The ordinates for the overscan spline interpolation can violate the requirement of monotonic increasing in the presence of masked rows, causing GSL to reject it and we end up raising an exception.  For example (notice the third element):    {code}  #1  0x00002aaadf6ccae4 in lsst::afw::math::InterpolateGsl::InterpolateGsl (      this=0x6f4ef0, x=std::vector of length 30, capacity 32 = {...},       y=std::vector of length 30, capacity 32 = {...},       style=lsst::afw::math::Interpolate::AKIMA_SPLINE)      at src/math/Interpolate.cc:211  211         int const status = ::gsl_interp_init(_interp, &x[0], &y[0], _y.size());  (gdb) p x  $19 = std::vector of length 30, capacity 32 = {-4.6668978729026289,     -1.0006934865900383, -2.2712418300653598, -0.76676245210727956,     -0.70019157088122619, -0.63362068965517238, -0.5668103448275863, -0.5,     -0.43342911877394619, -0.36685823754789276, -0.30028735632183906,     -0.23371647509578544, -0.1669061302681992, -0.10009578544061301,     -0.033524904214559385, 0.033045977011494247, 0.099616858237547928,     0.16618773946360157, 0.23299808429118776, 0.29980842911877403,     0.36637931034482762, 0.43295019157088127, 0.49952107279693497,     0.56609195402298851, 0.6329022988505747, 0.69971264367816088,     0.7662835249042147, 0.83285440613026807, 0.89942528735632199,     0.96623563218390807}  {code}    This is because the code to generate these ordinates (binning the overscan data vector) is incorrect.",0.5
"DM-8990","01/13/2017 00:22:03","wmgr database connection leak","Igor reported mysql error when loading lots of data into qserv cluster at in2p3:  {noformat}  I ran into an interesting problem when doing bulk loading of the KPM 20% data into the second Qserv cluster at *IN2P3*. At some point my loaders (at some point I had ~20 parallel loaders per each worker node) began to fail with the following complain:    [DEBUG] lsst.qserv.wmgr.client: Response body: {""exception"": ""OperationalError"", ""message"": ""(_mysql_exceptions.OperationalError) (1040, 'Too many connections')""}  [CRITICAL] Loader: Failed to create chunk 10772 for table 'ForcedSource'   {noformat}    I think it happens due to wmgr creating new connection on every data load request (this was verified) because it creates new sqlalchemy engine and:  - every new sqlalchemy engine has its separate connection pool  - engines are not destroyed by sqlalchemy (and not reused)    To avoid this issue we should reuse engine instances and avoid creating new ones. May need to verify first whether my guesses above are true.",2
"DM-9004","01/16/2017 11:14:41","Check uses of darktime for NAN","The darktime can be {{NaN}} if not set explicitly in the obs package's {{makeRawVisitInfo}}.  Any scaling of an exposure by the darktime can therefore result in a useless image full of {{NaN}} values.  We therefore need to catch the case {{isnan(darktime)}} wherever we use it: pipe_drivers for construction of the dark, and ip_isr for application of the dark.",1
"DM-9005","01/16/2017 11:55:17","Shared stack build failures on lsst-dev01","lsst-dev01 shared stack builds are having exactly the same problem with w_2017_2 & _3 as recorded for w_2017_1 in DM-8803:    {code}  ***** error: from /software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/build.log:  tests/testPsfCaching.cc(32): last checkpoint    *** 1 failure is detected in the test module ""PsfCaching""  tests/testWarpedPsf    Running 2 test cases...  unknown location(0): fatal error: in ""warpedPsf"": signal: SIGSEGV, si_code: 0 (memory access violation at address: 0x00000080)  tests/testWarpedPsf.cc(260): last checkpoint: ""warpedPsf"" entry.    *** 1 failure is detected in the test module ""DISTORTION""  The following tests failed:  /software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/meas_algorithms-12.1-15-g09aec8f/tests/.tests/testExecutables.py.failed  /software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/meas_algorithms-12.1-15-g09aec8f/tests/.tests/testImagePsf.failed  /software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/meas_algorithms-12.1-15-g09aec8f/tests/.tests/testPsfAttributes.failed  /software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/meas_algorithms-12.1-15-g09aec8f/tests/.tests/testPsfCaching.failed  /software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/meas_algorithms-12.1-15-g09aec8f/tests/.tests/testWarpedPsf.failed  5 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  + exit -4  eups distrib: Failed to build meas_algorithms-12.1-15-g09aec8f.eupspkg: Command:          source /software/lsstsw/stack/eups/bin/setups.sh; export EUPS_PATH=/software/lsstsw/stack; (/software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/build.sh) >> /software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/build.log 2>&1 4>/software/lsstsw/stack/EupsBuildDir/Linux64/meas_algorithms-12.1-15-g09aec8f/build.msg  exited with code 252  {code}",1
"DM-9011","01/17/2017 12:13:38","Make simpleShape less chatty","simpleShape can be verbose due to throwing {{pex::exceptions::RuntimeError}}.  Throwing {{meas::base::MeasurementError}} would make it quieter.",0.5
"DM-9012","01/17/2017 12:33:12","Refactor FileInfo and FileData classes into one class","We have 2 FileData classes and 1 FileInfo class which are all how information about a file.  Refactor these into one class - FileInfo.",3
"DM-9013","01/17/2017 12:33:55","Re-enable MKL/OpenBLAS","Use of MKL/OpenBLAS was disabled (in commit {{6fe95ec}}) while adapting meas_mosaic to work with the LSST pipeline. It needs to be re-enabled so we're not limited to the slow matrix inversion using Eigen.    Reverting that single commit is sufficient to get the threaded matrix inversion:  {code}    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND             24920 pprice    32  12 14.7g  13g  45m R 717.0 14.3 506:09.03 python              {code}",1
"DM-9014","01/17/2017 12:44:26","Add 2-d version of cppIndex","Add a 2-d version of cppindex to pybind11.h in utils in order to give better error messages when used on 2-d arrays.    Also fix the casting in the old cppIndex to avoid compiler warnings.    Note that this is pybind11-related so the work should branch from and merge to DM-8467",0.5
"DM-9016","01/17/2017 14:14:08","Drivers should be able to be made less verbose about eups","{{singleFrameDriver.py}} (and likely other pipe_drivers scripts) echos a bunch of eups commands and their output on startup. Those commands aren't part of the standard logging system and it should be possible to turn them on/off with an argument. I suggest either:     * {{--verbose-eups-dump}} to turn them on  or   * {{--quiet-eups-dump}} to turn them off.    I'm not sure whether on or off is the better default, and I'd be happy with either.    One can turn them off currently via {{--batch-type none}}, but that changes the actual type of processing, which is not desired.",1
"DM-9024","01/18/2017 09:11:51","Amend Python test naming guidelines in Developer Guide Following RFC-229","Amend the Developer Guide to require that test modules be prefixed with {{test_}} to enable automatic pytest discovery:    {code}  tests/test_example.py  {code}    See RFC-229.",1
"DM-9026","01/18/2017 10:35:38","Firefly IPAC table reader should handle data type ""real""","IRSA IPAC table reader handles real number as double. See detail in [http://irsa.ipac.caltech.edu/applications/DDGEN/Doc/ipac_tbl.html].    Firefly IPAC table reader currently treats real as char (data type show description) and String (the data type class). See the 4th column in the attached table.    It could be confirmed by saving the table after upload. The type for column 'V' chnaged form ""Real"" to ""Char"".    We need to follow IRSA's IPAC table definition to treat real as double.",1
"DM-9041","01/18/2017 18:55:45","Make indexed reference loader agnostic to ingest name","The indexed reference loader reads the dataset name out of the ingest config, but that's ridiculous because you need to give it the dataset name to find the config.  I'm removing the bit that loads the config and gets the name out of it, so that the loader will be agnostic to what name it was called at ingest time.",2
"DM-9045","01/19/2017 09:07:45","Remove or revive bitrotted code in meas_modelfit","meas_modelfit contains a lot of code that was used for FDR-era prototyping of modelfitting and has not been used since.  Some of this is worth keeping and reviving, because it's hard-won algorithmic code we may want to use in the future, while some of it should be removed.  In many cases, the unwanted code is being kept around because it's the only way the code we want is still tested; in other cases a small, easy-to-replace fraction of it is used by the bitrotted code we want to keep or code in active use (i.e. CModel).    This ticket is for cleaning up that mess.  Roughly, that means:   - Remove the custom table/record/catalog classes.   - Remove the custom CmdLineTasks.   - Remove the Sampler and Interpreter classes.    Some of this work may be best spawned done on other tickets, which should be linked here.",2
"DM-9049","01/19/2017 13:57:00","Enable autolinking in Doxygen","During review of DM-7891, [~jsick] said he intends for the final LSST documentation to link all mentions of API components, and requested that a style rule concerning manual links ({{@ref}}) be removed from the draft style guidelines. Without such a rule, however, current documentation will have no links at all.    Since it would be simpler to not introduce the rule now than to request/approve its removal later, the best solution is to enable Doxygen's autolinking, removing the need for {{@ref}} tags. This change should not cause compatibility problems for existing documentation that uses {{@ref}}.",1
"DM-9050","01/19/2017 14:14:02","Add flags for sources used in astrometric and photometric calibration","The PSF modeling tasks conveniently create and set flags indicating which sources were used to determine the PSF model.  The single-frame astrometric and photometric calibration tasks should do the same, indicating at least (for each procedure):   - Which sources were selected for potential matching.   - Which sources were actually matched.   - Which sources were actually used for calibration.    As we have done in PSF modeling, it would also be good to have the ability to reserve a set of candidate sources for validation purposes.    [~Parejkoj] and/or [~cmorrison] may have opinions on how this interacts with the new source selection stuff they've been working on.",5
"DM-9055","01/19/2017 15:39:22","DarkCombineTask broken","DM-8913 changed {{DarkCombineTask}} to use {{VisitInfo}}, but this assumed that the {{combined}} variable is an {{Exposure}}, but it's actually a {{DecoratedImage}}.  That means we need to use a different means of getting the metadata in.",0.5
"DM-9060","01/19/2017 19:46:27","Add metadata access to get wcs, visitInfo, and calib from a calexp dataset","This is regarding the recent discussion in the science-pipelines room where there has been discussion about using composites to get components of a calexp.     In {{CameraMapper}}  we will add metadata readers with object constructors for wcs, calib, and visitInfo.    The API to get the components and entire calexp would be     {code:}  wcs = butler.get(‘calexp_wcs’, dataId={…})  calib = butler.get(‘calexp_calib’, dataId={…})  visitInfo = butler.get(‘calexp_visitInfo’, dataId={…})  calexp = butler.get(‘calexp’, dataId={…})  {code}    (note: we were originally going to use butler composites for this, but decided this is a better way to go)  ",8
"DM-9064","01/20/2017 09:20:17","Fix memory leak in SpanSets Persistence","A schema used in the persistence layer of SpanSets needs to be marked as persistent or it causes a memory leak. Mark the schema as such.",0.5
"DM-9072","01/20/2017 16:24:51","Chart container tracking an active table in a table group","Currently, ChartsContainer is only displaying the default chart viewer. In future, we want to add a tbl_group property and logic that ties a chart container to a given group.    When table group property is provided, the Chart Container would create a chart viewer for the given table group (if does not yet exist), and display the charts related to the active table in that table group.   ",8
"DM-9076","01/23/2017 12:08:33","Keep LS as only periodogram calculation option","Update the periodogram panel option so is only reflect LS algorithm and keep as single option for now.   Others doesn't make sense with the current API and there won;t be time to make progress on that until March release.",2
"DM-9080","01/23/2017 18:22:48","labels for tab were cutoff a little at the bottom","The bottom of letter 'g' was cutoff in the labels.   See attached image.",2
"DM-9081","01/23/2017 18:39:29","testExposure.testGetWcs docstring is wrong, and tests should be assertIsNone","The docstring for {{testExposure.testGetWcs}} does not match the implemented tests in the method, and at least some of those tests appear to be incorrect, given the current (SWIGed) behavior of {{lsst.afw.image.Wcs}}. I've listed several obvious problems below:    * The docstring claims exceptions should be raised, but none of the tests check for exceptions.  * In addition, {{exposure.getWcs()}} returns None if the exposure was initialized without a Wcs, not False. None is ""falsey"", but if the API really wants None returned, we should test for that explicitly.  * The two unadorned {{getWcs()}} calls should have an {{assertEqual}} against self.wcs, since that's what those exposures were initialized with.  *  I also noticed testSetMembers, which catches a pex.Exception, prints a message, and then continues on its merry way. This should also be fixed.    This whole test suite needs to be looked at. That might be beyond scope for this ticket, but all of the above points are very worrying.",2
"DM-9096","01/24/2017 15:50:38","Wrap geom with pybind11","Wrap package {{geom}} with pybind11 instead of Swig.",0
"DM-9097","01/24/2017 17:19:42","Fix lsst-sphinx-bootstrap-theme deployment","https://github.com/lsst-sqre/lsst-sphinx-bootstrap-theme is on PyPI, but the current v0.1.0 distribution is missing the {{templates/}} directory. Could have been human error in setting up the package, or a problem with wheels.    This ticket will fix the PyPI distribution, and possibly set up automated continuous delivery with Travis to PyPI.",0.5
"DM-9098","01/24/2017 18:41:49","Update mariadb eups packages","Experimentation with latest MariaDB containers shows indications of significant performance improvements.  We'd like to update MariaDB and client to latest versions before undertaking our upcoming qserv KPMs  ",1
"DM-9102","01/25/2017 12:54:42","Update the import package in LSSTFileGroupProcessor","The FileInfo class was updated and moved to a new location.  It triggered LSSTFileGroupProcessor failing.  The LSSTFileGroupProcessor should be modified to import from the new location.",0.5
"DM-9105","01/25/2017 14:22:43","Make SpanSet operator templates more generic","Expand the flatten and unflatten methods of SpanSets such that they can operate on multi-dimensional ndarrays. This work involves making the template parameters for these functions, and the getter classes more generic.",2
"DM-9109","01/25/2017 16:40:15","Create ellipticity residuals quiver plots","Include ellipticity residual comparison and quiver plots in the analysis script, including:    *Intra-stack:*  scatter/histogram/sky plots for *psfUsed* stars of  > residual e1 & e2 ellipticities between the source and model psf at the position of the source (for both SDSS and HSM measurements), where:  {code}  e1 = (shape_xx - shape_yy)/(shape_xx + shape_yy)  e2 = 2*shape_xy/(shape_xx + shape_yy)  {code}  > residual ellipticity, δe, between source and model psf at source position quiver plot where:  {code}  δe = sqrt((e1_src - e1_psf)^2 + (e2_src - e2_psf)^2))  {code}    *Inter-stack:*  scatter/histogram/sky comparison plots for;  > comparison of the trace radii of the sf models at the position of *psfUsed* stars matched between the two stack catalogs, where:  {code}  traceRadius = sqrt(0.5*(shape_xx + shape_yy))  {code}",5
"DM-9113","01/26/2017 08:26:02","the bias ingestion should not care about the filter ","In the CP MaterCal bias products, the filter in the header can be anything.  Currently the ingest code updates the validity range for each filter, which can result in more than one bias at one time. ",1
"DM-9120","01/26/2017 19:09:57","matcherSourceSelector incorrectly uses nChild and footprints in isMultiple test.","This bug was found by dm-square as a drop in match rms quality. The replacement for the SourceInfo class in matchOptimisticB.py has a test for isMultiple but this test was not used in the subsequent isGood or isUsable tests. The current implementation of the new matcherSourceSelector incorrectly uses this test and to parrot the performance of SourceInfo (which was the goal of DM-6824) this test should be removed. ",1
"DM-9126","01/27/2017 11:42:10","qserv_distrib does not setup qserv_testdata","[~jammes] reported on slack that the {{qserv:dev}} container is unable to setup the {{qserv}}.    Demonstration of issue:    {code:java}  [master] ~ $ docker run -ti qserv/qserv:dev  qserv@f405e3798cb4:~$ . /qserv/stack/loadLSST.bash   qserv@f405e3798cb4:~$ setup qserv_distrib -t qserv-dev  qserv@f405e3798cb4:~$ eups list -s | egrep ^qserv  qserv_distrib         1.0.0+664   b2666 qserv-dev setup  {code}    Expected result:    {code:java}  [master] ~ $ docker run -ti qserv/qserv:dev  qserv@ab194158b222:~$ . /qserv/stack/loadLSST.bash   qserv@ab194158b222:~$ setup qserv_distrib   qserv@ab194158b222:~$ eups list -s | egrep ^qserv  qserv                 12.1-27-gc42959c  b2531 current b2541 w_2016_51 setup  qserv_distrib         1.0.0+652   b2531 current b2541 w_2016_51 setup  qserv_testdata        12.0+79     current b2541 qserv-dev w_2016_51 b2531 setup  {code}    This appears to be caused by this changed commited on Dec 19th as part of DM-8256:    https://github.com/lsst/lsstsw/commit/49acd3e33364d0b4b67a7900d910fe121a0ac8fb  ",0.5
"DM-9131","01/27/2017 12:52:21","Apply second round of Robert's LDM-151 comments","Having applied Robert's first round of corrections, a second marked-up pdf with many few comments exists which should be applied during this iteration on LDM-151.",8
"DM-9135","01/27/2017 13:57:00","bulk rename of jointcal variables","Jointcal currently uses very non-standard, non-stylistic, and/or non-helpful variable and method names (e.g. {{void FittedStar::SetRefStar(const RefStar *R)}}). I've been cleaning them up piecemeal as I go, but that results in confusing commits and can be a pain. The clang format package provides ""clang-rename"" which might help take care of most/all of these in one foul swoop. Other suggestions are welcome (SublimeText's smart selection isn't quite smart enough, particularly for single-character variable names).",2
"DM-9136","01/27/2017 14:30:51","Include CBP coordinate transformation system in LDM-151","Per [LCR-581|https://docushare.lsstcorp.org/docushare/dsweb/Get/Version-37829/LCR-581CalibrationHardwareRequirementsUpdateApproved.pdf]:    {quote}  To be added to LSE-30 (OSS):    Beam Projector Coordinate Relationship    Specification: Coordinate system transformations shall be measured  and/or computed relating the collimated beam projector position and  telescope pupil position to the illumination position on the telescope  optical elements and focal plane, and a software interface shall be  developed to represent these relationships, including their possible  evolution in time.    Justification:    This is necessary to facilitate the data acquisition and reduction. The  user shall be able to specify an LSST pupil and focal plane position for  a given spot, then have the CBP and telescope offset accordingly.  Similarily, the spot positions should be predictable based on the CBP  and telescope position.    This requirement will need to be flowed down appropriately to the Data  Management and Observatory Control System requirements documents. It is  assumed that Data Management will develop a Python interface that  represents the relationships, and that the Observatory Control System  will use those in the construction of the control system functions  relating to the collimated beam projector.  {quote}    Note the requirements on DM above. These should be reflected in LDM-151.",2
"DM-9162","01/30/2017 17:41:15","magnitude shoud be plotted in decrease order in period finding layout","The preview phase folded curve in the period finding layout is showing the magnitude axis in increase order but it should be in decrease order as the result layout to be consistent.",1
"DM-9163","01/30/2017 18:47:33","newinstall.sh broken by conda package removal from public channels","{code:java}  Installing Miniconda2 Python Distribution ...     [  1/1  ]  miniconda2 4.2.12 ...     ***** error: from /newinstall/EupsBuildDir/Linux64/miniconda2-4.2.12/build.log:  Fetching package metadata .......  .Solving package specifications: .      PackageNotFoundError: Package not found: '' Packages missing in current linux-64 channels:     - fontconfig 2.12.1 0    - glib 2.50.2 0    {code}    ",1
"DM-9165","01/30/2017 18:57:49","Add feature to overlay the searched position on the coverage image.","The old firefly's API 'addCoveragePlot' will overlay the coverage image with the searched position given by the 'OverlayPosition' parameter.  This feature is missing from the new showCoverage function.  Also, catalog search results should also have the searched position overlaid as well.  Please confirm.",3
"DM-9166","01/31/2017 07:19:35","Help IN2P3 scientist to load stack processed data inside Qserv","Help [~nchotard] to load LSST processed data inside Qserv.  ",5
"DM-9182","01/31/2017 14:46:13","Cleanup pybind11 code in afw","Following the review of DM-9063, I've put together a list of fixes to make for most of afw and its dependencies, which I have attached to the ticket.  I have *not* looked at afw::table, as that will be reviewed and fixed independently on DM-8716.    First, a list of *generic* issues that weren't worth capturing on a file-by-file basis (though I did start by doing that).  This is essentially a checklist that should be used by all pybind11 cleanup issues:  - Move trivial Python extensions to C++.  - Use continueClass decorator in remaining Python files.  - Reorganize/rename files as per RFC.  - Address any TODOs  - Remove commented-out headers.  - Make sure pybind11 header is included first (because Python.h needs to be included first).  - Remove (or otherwise address) commented-out code  - Remove any use of ndarray/converters.h (redundant with ndarray/pybind11.h)  - Make sure all functions that should have kwargs do.  - Replace py::arg(""foo"") with ""foo""_a (with using pybind11::literals)  - Make sure shared_ptr holder type is used for all but trivial classes  - Look for comment headings that are unused or disrupt readability  - Make sure {{py::is_operator}} is used on binary, non-in-place operators, and is not used anywhere else.  - Look for lambdas with non-const reference arguments; could they be const references?  - Look for getters that should be using reference_internal  - Remove spurious (empty) wrapper files.  - Determine whether enums are used as enums or integer constants, and adjust wrappers accordingly.  - Check for anonymous namespace and `static` usage.  - Check for worthless module docstrings.  - Define typedefs for py::class_ instantiations, or otherwise ensure they're not repeated.  - Don't use ::Ptr (here, or anywhere).  - Run clang-format?  Some of indentation is really terrible at definitely not conformant, and clang-format improves it.  - Delete trailing whitespace.    ",3
"DM-9186","01/31/2017 16:30:51","Create hscIsr.py script","I have some unusual data (HSC pinhole filter images) for which I'd like to run only very basic Isr tasks (overscan subtraction and bias subtraction), which is tricky to manage with processCcd.py.  This ticket is to create a subaruIsr.py command line executable to accomplish this.",2
"DM-9187","01/31/2017 16:35:29","port jointcal to pybind11","Jointcal's python interface is currently SWIG-based. Now that most (all?) of the dependencies are converted, it's time to convert jointcal to pybind11.",8
"DM-9196","01/31/2017 18:26:33","cutout size parameter is not passed in the request by the download dialog","The download dialog has an option 'cutout_size' but is not passed to the server.  Please fix.  ",1
"DM-9206","02/01/2017 07:50:18","Precision of expression columns","This is a bug introduced by in DM-8367 line chart. The precision of the expression columns is not set when the values are saved to an IPAC table. When the table is read back and the first row happens to be 0.0, all values will be passed to the client with 1 decimal digit.    Test: load sample table, change y column to -count/time. Notice that all values in the point tooltip have 1 digit after decimal point and that highlighted points do not match the points on the line. (Highlighted values are calculated on client from the values of the table, plot points - on the server.)    https://github.com/Caltech-IPAC/firefly/pull/279",2
"DM-9233","02/01/2017 13:25:17","Add default constructor to new Footprints","Having a default constructor to Footprints would be useful in many cases. Add one which takes only a PeakSchema as a default argument, and creates a null SpanSet",1
"DM-9242","02/02/2017 09:47:02","squash KPM plots should label the time axis with a timezone","At present, the KPM time series plots do not label the time axis with the timezone.",0.5
"DM-9245","02/02/2017 10:22:13","Replacing JS package manager npm with yarn","Yarn is a new package manager for JS.  It increases performance and is more reliable.    You do need to install yarn. From your command prompt:  npm install yarn -g  Depending on where you install node, you may need sudo for this to work.  I've updated our docs to reflect this new requirement.    In order to get consistent installs across machines, yarn.lock is used.  If you updated package.json, you need to run yarn install to create a new yarn.lock file.  You must commit the new yarn.lock file with your updated package.json for build to work.",1
"DM-9249","02/02/2017 11:26:09","Modify FlagHandler C++ and flagDecorator.py to make flag identification robust","As discovered by DM-6561, the FlagHandler mechanism for connecting the enumeration of flags in C++ and the order in which the flags appear in the schema and internal FlagHandler structures is not robust.  Fix this, problem, so that the identifier used identify a particular flag and the lookup of the Flag Key are guaranteed to match.    Then fix the flagDecorator (it will be simpler) and all of the algorithms to match the new FlagHandler scheme.",8
"DM-9253","02/02/2017 16:12:44","Prepare for IRSA Time series viewer release","The story is to collect the ticket and track them in order to prepare and release Time Series viewer (old LC) in March.    Links to issues should be added.    The RC is expected to happen around mid February. ",8
"DM-9254","02/02/2017 17:41:36","Firefly is not working in Windows browser IE 11","I've tested Firefly in Windows browser IE11 and it doesn't render.    We should try to figure out the issues first. Make new tickets for large bugs. ",8
"DM-9261","02/03/2017 11:34:20","Update git-lfs repositories to address deprecations.","Update all git-lfs repositories to be compliant with current git-lfs best practices.    1. Remove {{batch = false}} configurations.  2. Ensure {{.lfsconfig}} files exist.",2
"DM-9274","02/03/2017 18:15:11","Build Python doc using Sphinx","Jonathon Sick has a boiler plate to bild docs for Numpy doc docstring.   This is a ticket to use that for SUIT Python doc build.  https://community.lsst.org/t/what-packages-have-numpydoc-docstrings-so-far/1612      Please add the cheat sheet for Python doc generation in https://confluence.lsstcorp.org/display/DM/Python+document+generation+cheat+sheet.    ",8
"DM-9275","02/04/2017 08:35:19","Port obs_monocam to pybind11","Port the obs_monocam package to pybind11    Note that obs_monocam is part of lsst_distrib",0.5
"DM-9294","02/06/2017 13:16:10","makeCamera.py has undefined variables","Function {{makeAmp}} in {{makeCamera.py}} has two undefined variables: {{nExtended}} and {{nOverclock}}.",1
"DM-9298","02/06/2017 15:54:41","The stripMetadata argument of makeWcs doesn't work reliably","The function {{afw::image::makeWcs(metadata, stripMetadata}} with its second argument true does not strip metadata if certain values are present because those values induce a deep copy of the metadata, and the keywords are stripped from the copy.  ",0.5
"DM-9301","02/07/2017 01:07:05","Implement parallel processing in L1DB prototype","Next step in understanding alert production performance is to see if paralellising processing can help with reducing database access overhead.",8
"DM-9313","02/07/2017 10:47:25","obs_decam should not call stripWcsKeywords","{{DecamMapper}} presently calls {{stripWcsKeywords}} in several places. This function is not part of afw's public interface (it is in namespace {{lsst::afw::image::detail}} and the call is unnecessary because {{makeWcs}} will strip the metadata if told to do so (by setting the second argument true).    Fixing this will help the pybind11 conversion effort because the pybind11 wrapped version of afw does not make {{stripWcsKeywords}} available, and we'd rather not change that.",0.5
"DM-9314","02/07/2017 11:34:39","weekly tag/release build of w_2017_6 failed: flask","The new flask is not building on master and this is causing the weekly tag/release build to fail. It looks like this is being triggered by DM-9268.",0.5
"DM-9317","02/07/2017 13:14:20","Creates online help for SUIT","Migrate Firefly's onlinehelp system so that it can be used by SUIT.  - created github repository: https://github.com/lsst/suit-onlinehelp/  - copy content from existing irsaviewer online help.  - change build/config script for SUIT.  - setup build/install environment for pdac    To build and install online help on pdac:  - log into sui-tomcat01  - switch to suiadmin  - source /hydra/cm/env/env.sh  - cd /hydra/cm/suit-onlinehelp  - git pull  - gradle install",2
"DM-9326","02/07/2017 18:11:03","Clean up time series viewer","Please, to be consistent with the new name of Time Series viewer:    - Rename {{lc.html}} to {{ts.html}} (Time Series replace Light-Curve)  - {{wise}} should be upper case    Thanks.",1
"DM-9327","02/07/2017 18:50:35","Create IFE app for Time series viewer","Need a Time Series viewer app for IRSA, under IFE repository.    Please, create app with no logo (for now - for later probably we will need a new icon).    ",2
"DM-9334","02/08/2017 11:52:58","monocamIngestImages.py is vestigial","obs_monocam provides a [{{monocamIngestImages.py}}|https://github.com/lsst/obs_monocam/blob/786219475ffd0e022c98234d5b00a5c30ca668c5/bin.src/monocamIngestImages.py] which attempts to import {{lsst.obs.monocam.ingest.MonocamIngestTask}}. {{MonocamIngestTask}} [was removed in {{7862194}}|https://github.com/lsst/obs_monocam/commit/786219475ffd0e022c98234d5b00a5c30ca668c5#diff-16d2f190c33eaa0d3ef05279458bd775L95]. I conclude that {{monocamIngestImages.py}} is a confusing remnant which should be removed.",1
"DM-9336","02/08/2017 13:23:41","Complete LCR-836 (typographic correction to LSE-69)","LCR-836 (already created) is for a long-pending typographic correction to LSE-69 to get it to match LSE-130's section headings.    The work on this ticket is to prepare the new docgen and submit it to the CCB. ",2
"DM-9342","02/08/2017 15:36:46","histogram option update (input dialog and server support)","For uniform binning (fixed bin size), there are two different ways to give the input:  1. number of bins (the current option only)  OR  2. bin width    Label may needs to be changed as well.    [March-2-2017]  To have a choice of the number of bins or bin width, two radio buttons and one text box are added.  By default, the number of bins is selected.  When the radio button is selected, a new value either number of bins or bin width should be entered and validated.   Then the histogram will be updated accordingly. Since no data range information is available before sending the request, the bin width can not be validated correction.  Thus, an exception is thrown if the bin width is large than the data range.     [March-16-17]  Based on the modified requirements, the UI is  # Add two radio buttons and two input text boxes next to the two radio buttons, one of which is number of bins and the other is bin width.  #  Add two input text boxes below the two set radio/text boxes.  One of which is for min and the other is max    After several discussions and iterations, we added the following:  # For single column histogram, since the data min/max is stored in the column variable array, they can be used to pre-fill the min and max.  # The bin width is calculated based on the number of bins and min and max and then the bin width is pre-filled.  # The bin width is recalculated each time when number of bins or min or max changes.  # When number of bin is selected, the bin width text box is disabled, when the bin width is selected, number of bins text box is disabled    ",8
"DM-9346","02/08/2017 16:31:38","label change requests for Firefly/IRSA viewer","The following change will be in Firefly:  * histogram  Y axis label: Number  * Charts scatter/histogram option dialog, ""search"" button:  ""OK""  * Choose columns dialog ""Set column or expression"" button:  ""OK""    The following changes will be in IRSA Viewer:  ",2
"DM-9348","02/08/2017 17:09:38","LSST - time series viewer results (part 2, UI work)","This is the second part of changes to support LSST time series, which should handle band selection in UI.    We need to support mission specific input, from which the x and y columns and automatic filtering of the raw table depending on the input parameters (band selection).    https://github.com/Caltech-IPAC/firefly/pull/313    Attached to this ticket, please find the sample LSST multi-band table for object id 2990683841366709.",8
"DM-9349","02/08/2017 19:01:00","Filtering selected row (ROWID filter) gives wrong results (dev version)","In Firfefly (+IRSAVIewer) development version, when the user select table rows and filter them out, the result table is not matching the selected rows before applying the filter.  OPS is fine though.    Step to reproduce:  do a catalog search i.e., 2mass, 100"". From the result table, select a row and apply filter. Then check that the resulting row is not the one you selected.    TG 02/10/2017 The row returned is actually the previous row.   The related issue is filtering a point from an image. A previous row is returned as a result. ",2
"DM-9350","02/08/2017 22:38:50","Possible to provide a drawing of the Stripe 82 sky region for context?","I am wondering if it would be straightforward to provide a ""footprint"" outline of the Stripe 82 area on the sky for use in highly zoomed-out context images in PDAC.    If this is an easy task, in what format would we need to specify the coordinates of the online?    I think we can do a calculation of four corners of all the coadded images and produce a sd9 region file for the outline.       ==============================================    How to generate the region files attached here:    1)Get four corners data for all the coadds FOVs for SDSS:  curl -o sdssFourCorners.json -d 'query=SELECT+corner1Ra,corner1Decl,corner2Ra,corner2Decl,corner3Ra,corner3Decl,corner4Ra,corner4Decl+FROM+sdss_stripe82_00.DeepCoadd;' http://lsst-qserv-dax01.ncsa.illinois.edu:5000/db/v0/tap/sync  2)Convert the json file to csv using a tool on line:  http://www.convertcsv.com/json-to-csv.htm  3)Sort the csv file by corner1Ra  4)Remove ""^M""  5)Convert the csv file to a region file by adding ""polygon("" and  "") #color=green"" and a header ""fk5""  6)For only plot 1% or 10% FOVs:  awk 'NR == 1 || NR % 100 == 0' input.reg > output_1pct.reg  awk 'NR == 1 || NR % 10 == 0' input.reg > output_10pct.reg    The region file can be loaded to Firefly.",2
"DM-9353","02/09/2017 11:03:26","Update configuration for HSC calib construction","DM-9186 changed how the ISR configuration is set, but the configuration for calib construction wasn't updated to match.",0.5
"DM-9356","02/09/2017 12:08:42","Summarize plans & questions for Calib Telescope work","Summarize the plans for processing data from the Calibration Telescope and any open questions that remain about the approach to be taken in bullet-point form. Provide them to [~mfisherlevine] for incorporation into LDM-151.",2
"DM-9358","02/09/2017 14:35:34","Fix setting of calib_psf_candidate flag to match docstring description","The docstring for {{calib_psf_candidate}} reads:  {code}            self.candidateKey = schema.addField(                  ""calib_psf_candidate"", type=""Flag"",                  doc=(""Flag set if the source was a candidate for PSF determination, ""                       ""as determined by the star selector."")              )  {code}  However, if the reserve fraction is set to a non-zero value, an object that was selected as a PSF candidate by the star selector but then flagged as reserved will have the {{calib_psf_candidate}} flag set to *False*. This is inconsistent with the docstring (and erroneously implies the object was not deemed a suitable candidate). Please update the setting of the flag to reflect its docstring description.",3
"DM-9359","02/09/2017 14:57:18","Restoring coverage image after cropping it shows an empty tab entitled 'FITS data'","I've found a something wierd after doing a catalog search and i think is not expected.    When doing a catalog search, the result tri-view shows image, table and xy-plot.  The image is called 'Coverage'.   Using the selection tool and cropping part of the image, the result is a smaller image which is correct.    Then when trying to go back and restore to default image (click the ""Restore"" button on the toolbar), the result is 2 tabs, one with the coverage restored correctly and another one called 'FITS data' empty and active.    I think the second tab shouldn't be displayed.  ",1
"DM-9361","02/09/2017 15:04:52","Update Calib Telescope data processing section in LDM-151","Based on the input provided by [~aguyonnet] in DM-9356, update LDM-151 to provide a complete description of the plans for processing data from the Calibration Telescope.",2
"DM-9363","02/09/2017 15:06:43","Construct obs package for 0.9m at CTIO","Data from the 0.9m telescope at CTIO is being used to prototype the processing pipeline for the Calibration Telescope. In order to ingest it into the stack, we'll need an appropriate obs package (""obs_ctio0m9""). Please create one.",8
"DM-9364","02/09/2017 15:09:31","wcs creation is mandatory","Line 1044 of cameraMapper.py tries to attach a wcs, and crashes if the necessary keywords are not found in the metadata, and this shouldn't be the case.    One could replace this with a try block to give every exposure a dummy wcs, but this is probably not the correct course of action - on failure the wcs should _probably_ just not be set, to allow a None wcs to exist.    Whatever the fix, this should log a warning.",2
"DM-9371","02/09/2017 17:13:10","Display image cutout in LSST PDAC","We need to provide option to display the image cutout, using DAX API.",2
"DM-9379","02/10/2017 07:26:31","Changes to lsstsw to rename ctrl_events package to legacy-ctrl_events","This work removes:  {code}  ctrl_events  {code}  from {{etc/repos.yaml}}    and moves the repo to  {code}  lsst-dm/legacy_ctrl_events  {code}",0.5
"DM-9381","02/10/2017 08:58:05","Add ability to highlight data subsets in analysis plots","Highlighting objects based on a ""third"" parameter, e.g. a flag setting, (other than spatial, for which we are already producing diagnostic figures) can be very useful in diagnosing pathologies (see, e.g. DM-9252).  This ticket is to add this ability to the analysis plotting scripts.",2
"DM-9382","02/10/2017 10:32:17","Phase folded table content fix","fix Phase folded table created  for time series viewer (DM-8670)   - fix the time column value (mjd) on the expanded cycle part. (the phase folded table is made to contain the data rows for two phase cycles): the value of 'mjd' column in the expanded part is duplicated from the original raw table.   - make the creation of phase folded to be derived from full set of raw table, not just partial raw table shown on the page.    ",2
"DM-9383","02/10/2017 10:47:17","Investigate propagation of visit flags for certain patches in HSC RC processing","For certain diagnostics plots, it is useful to select on specific subsets of the data.  An example subset would be the objects that were used actually in the PSF modeling, i.e. those with flag *calib_psfUsed=True*.  When plotting this subset for the coadds of the LSST stack processing of the HSC RC dataset (DM-6816), a large number of patches have no objects for which this flag is set.  While the list is not expected to be identical to the list from the visit processing (the flag is propagated based on the fraction of visits overlapping that object and contributing to the coadd that had the flag set for said object), ending up with a list of zero objects used for PSF modeling should be very unlikely.  The pattern is quite delineated, with sharp truncation lines beyond which no objects used in PSF modeling are found:    !plot-t0-HSC-I-footNpix_calib_psfUsed-sky-stars_LSST.png|width=500!    Of note, this does not happen for the same dataset processed through the HSC 4.0.5 stack (DM-9028):    !plot-t0-HSC-I-footNpix_calib_psfUsed-sky-stars_HSC.png|width=500!",3
"DM-9384","02/10/2017 10:49:59","Fix wrapped constructor for StatisticsControl","The constructor for {{lsst::afw::math::StatisticsControl}} has some arguments with default values, but the pybind11 wrapper omits them and only provides a default constructor. This breaks some existing code. I propose to wrap the constructor as written (which will be quite pleasant to use with named arguments) rather than fix the existing Python code that relies on being able to specify arguments to the constructor.",0.5
"DM-9386","02/10/2017 11:42:44","IpacTableIpacTableFromSource returns one row less if there is no terminating new line in the table","IpacTableFromSource processor does not return the last row of the attached table. (This table does not have the newline after the last row.)     To test, put the attached a.tbl to /hydra/workarea/firefly/temp_files and the attached test.html into /hydra/server/tomcat/webapps/firefly/demo. Use http://localhost:8080/firefly/demo/test.html to see the table.   - Notice the number of rows is 157, the last row seems to be lost.   - Sort on any column. Notice the number of rows is 158.    We noticed this problem in LC phase folded table (the one that is being uploaded to the server). The attached file is a copy of the uploaded phase-folded table.",2
"DM-9387","02/10/2017 12:51:25","lsst_build git fetch/clone retrying","We occasionally observe a ""wave"" of github fetch/clone failures from {{lsstsw / lsst_build}} in the jenkins env. Where a wave is several random failures over the course of a day or two and then there are no failures for weeks.  I am convinced that these are on the github end as I have experienced clone failures when running {{lsstsw}} outside the the jenkins env.    I am loath to retry the entire jenkins build upon any failure as this might result in a legitimate build failure unnecessarily tying up build slaves.  There are two  solutions that occur to me:    1) propagate errors up from {{lsst_build}} in such a way that the CI driver can determine the reason of failure and retry a set of failure modes    2) add git fetch/clone retrying support into {{lsst_build}}    I am leaning towards #2 as the implementation is straight forward and contained within a single component.",3
"DM-9394","02/10/2017 17:17:31","Add meas_extensions_convolved to lsst_distrib","This package is in active use and should benefit from regular CI. Adding it to lsst_distrib will require an RFC.",1
"DM-9400","02/13/2017 07:02:07","Construct obs package for test stand 3","Create an obs package that provides basic access to data from test stand 3 (ie, individual CCDs). This should include loading the data from disk and providing all the standard LSST functionality, but does not need to integrate with Camera Team systems (e.g. you don't need to ingest metadata directly from eTraveller, or similar).",8
"DM-9416","02/13/2017 14:40:50","Cleanup dead read/write StarList c++ code","While converting to use lsst::log in DM-8547, I started cleaning up all of the dead read/write code for StarLists. Since we're using LSST catalogs and refcats now, we shouldn't need it for anything, and we'll eventually be able to interrogate those lists from python (once we deal with DM-4043), if such functionality is desired.    I'm filing this as a separate ticket so it doesn't clutter DM-8547 with a bunch of deleted lines.",1
"DM-9423","02/14/2017 09:53:47","Port HSC patch to allow multiple filters in mosaic","[HSC-1398|https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1398] allows multiple filters to be used in the mosaic solution.  This is helpful because there are multiple r-band and i-band filters, and it's helpful to combine them.    I'll also update the {{MosaicTask}} to allow use of the new LSST format reference catalogs.",0.5
"DM-9428","02/14/2017 11:26:46","Update exceptions tutorial-level documentation for pybind11","pex_exceptions contains a tutorial in Doxygen that includes Swig-specific instructions for making sure C++ exceptions are propagated correctly to C++.  This should be updated.",1
"DM-9429","02/14/2017 12:36:03","Update lsst_dm_stack_demo for pybind11","Update lsst_dm_stack_demo for pybind11 and fix anything needed in the dependent packages",0.5
"DM-9433","02/14/2017 13:42:54","ds9.py error code not working as intended","The following code in afw's display/ds9.py appears to not function as intended:  {code}      except Exception as e:          # No usable version of display_ds9.          # Let's define a version of getDisplay() which will throw an exception.          e.args = [""%s (is display_ds9 setup?)"" % e]            def getDisplay(*args, **kwargs):              raise e            class DisplayImpl(object):              def __init__(self, *args, **kwargs):                  raise e  {code}    To see this run examples/estimateBackground.py with display_ds9 not set up. What I see is:  {code}  Traceback (most recent call last):    File ""examples/estimateBackground.py"", line 109, in <module>      main()    File ""examples/estimateBackground.py"", line 89, in main      ds9.mtv(image, frame=0)    File ""/Users/rowen/UW/LSST/lsstsw3/build/afw/python/lsst/afw/display/ds9.py"", line 84, in mtv      return getDisplay(frame, create=True).mtv(data, title, wcs, *args, **kwargs)    File ""/Users/rowen/UW/LSST/lsstsw3/build/afw/python/lsst/afw/display/ds9.py"", line 46, in getDisplay      raise e  NameError: name 'e' is not defined  {code}    I confess to some surprise. I am not an expert on Python's binding rules, but I expected the code to work.",0.5
"DM-9434","02/14/2017 13:56:25","Fix database creation error in testTimeFuncs.py","An error is occurring in testTimeFuncs.py:    {noformat}      Unable to execute query: CREATE DATABASE test_14871027723 - * Access denied for user 'srp'@'%' to database 'test_14871027723' {0}  lsst::pex::exceptions::RuntimeError: 'Unable to execute query: CREATE DATABASE test_14871027723 - * Access denied for user 'srp'@'%' to database 'test_14871027723  {noformat}    The error is occurring because users don't have database creation permissions to create   {noformat}test_%{noformat} databases.    This only occurs on machines in the ncsa.illinois.edu domain, because the test is skipped if not run on that domain.",1
"DM-9437","02/14/2017 14:55:14","automate jenkins workspace cleanup","All jenkins jobs that use {{lsstsw}} consume couscous amounts of disk space and require periodic manual workspace purging.  This should either be moved to a jenkins plugin or an automated script.",3
"DM-9438","02/14/2017 16:44:48","Switch default reference catalog for HSC to PS1 in LSST format","With the availability of PS1 and Gaia catalogs in the LSST format, we can remove our dependence on astrometry.net.  This involves changing configuration files to use the PS1 catalog in LSST format and disable the use of astrometry.net.",1
"DM-9439","02/14/2017 17:26:08","Package version checking is non-deterministic","When a command-line task runs, it [writes the versions of packages currently set up to the repository|https://github.com/lsst/pipe_base/blob/54f122d8ff9696ff2b1de8f72d33eff08773e0a3/python/lsst/pipe/base/cmdLineTask.py#L285]. We're then [unable to run the same task again without matching versions|https://github.com/lsst/pipe_base/blob/54f122d8ff9696ff2b1de8f72d33eff08773e0a3/python/lsst/pipe/base/cmdLineTask.py#L614]. This helps with reproducibility.    However, when I repeatedly try to run the same task with exactly the same task with exactly the same packages set up, this version checking fails intermittently. The task exits, complaining:    {code}  Traceback (most recent call last):    File ""/Users/jds/Projects/Astronomy/LSST/src/pipe_drivers/bin/constructBias.py"", line 4, in <module>      BiasTask.parseAndSubmit()    File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/ctrl_pool/12.1-7-gb57f33e/python/lsst/ctrl/pool/parallel.py"", line 422, in parseAndSubmit      if not cls.RunnerClass(cls, batchArgs.parent).precall(batchArgs.parent):  # Write config, schema    File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/pipe_base/12.1-5-g06c326c+6/python/lsst/pipe/base/cmdLineTask.py"", line 303, in precall      self._precallImpl(task, parsedCmd)    File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/pipe_base/12.1-5-g06c326c+6/python/lsst/pipe/base/cmdLineTask.py"", line 285, in _precallImpl      task.writePackageVersions(parsedCmd.butler, clobber=parsedCmd.clobberVersions)    File ""/Users/jds/Projects/Astronomy/LSST/stack/DarwinX86/pipe_base/12.1-5-g06c326c+6/python/lsst/pipe/base/cmdLineTask.py"", line 618, in writePackageVersions      ""); consider using --clobber-versions or --no-versions"")  lsst.pipe.base.task.TaskError: Version mismatch (meas_algorithms: 12.1-17-g13cfda1+6 with boost=1.60.lsst1+1 eigen=3.2.5.lsst2 vs 12.1-17-g13cfda1+6 with eigen=3.2.5.lsst2 boost=1.60.lsst1+1; coadd_utils: 12.1-1-g5961e7a+70 with boost=1.60.lsst1+1 eigen=3.2.5.lsst2 vs 12.1-1-g5961e7a+70 with eigen=3.2.5.lsst2 boost=1.60.lsst1+1; afw: 12.1-31-gb5bd9ab+1 with boost=1.60.lsst1+1 eigen=3.2.5.lsst2 vs 12.1-31-gb5bd9ab+1 with eigen=3.2.5.lsst2 boost=1.60.lsst1+1); consider using --clobber-versions or --no-versions  {code}    Note that the versions *are* the same, but are being reported in a different order ({{12.1-17-g13cfda1+6 with boost=1.60.lsst1+1 eigen=3.2.5.lsst2}} as compared to {{12.1-17-g13cfda1+6 with eigen=3.2.5.lsst2 boost=1.60.lsst1+1}}).    Please fix this so that version checking works reliably.",0.5
"DM-9456","02/15/2017 12:00:03","Rename FileLoader in test/util to UnitTestDataIO","While working on ImageHeaderTest, I found there are some methods can be utility methods.  To add those methods there, the FileLoader is no longer meaningful.  I rename it as UnitTestDataIO which contains all kind I/O including file, json file etc.",0
"DM-9457","02/15/2017 13:23:20","test failure due to esutil/numpy problem","I'm seeing a test failure in meas_algorithms due to what looks like a failure of {{esutil}} compiled code to import the NumPy C API:  {code}  ======================================================================  ERROR: setUpClass (__main__.HtmIndexTestCase)  ----------------------------------------------------------------------  ImportError: numpy.core.multiarray failed to import    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""tests/testHtmIndex.py"", line 125, in setUpClass      cls.indexer = IndexerRegistry['HTM'](config)    File ""/home/jbosch/LSST/sw/build/meas_algorithms/python/lsst/meas/algorithms/indexerRegistry.py"", line 44, in makeHtmIndexer      return HtmIndexer(depth=config.depth)    File ""/home/jbosch/LSST/sw/build/meas_algorithms/python/lsst/meas/algorithms/htmIndexer.py"", line 34, in __init__      self.htm = esutil.htm.HTM(depth)    File ""/home/jbosch/LSST/sw/stack/Linux64/esutil/0.6.0+1/lib/python/esutil/htm/htmc.py"", line 152, in __init__      this = _htmc.new_HTMC(depth)  SystemError: <built-in function new_HTMC> returned a result with an error set  {code}    This is Python 3.6 and Numpy 1.12, so it's likely I'm seeing the problem because I'm pushing to newer versions than anyone else.",2
"DM-9462","02/15/2017 16:46:29","Allow disabling adding underscores to pybind11 library names","Our final pybind11 coding conventions dictate that the wrapper library name should match the source code name. However, all of our existing pybind11 wrappers assume that the library name will always start with an underscore (whether the source file name does or not).    I will add a new flag {{addUnderscore=True}} to the {{pybind11}} function. New code should specify this flag as False. Old code will continue to work unchanged.",0.5
"DM-9464","02/15/2017 17:06:47","Link SublimeText clang-format setup instructions in docs","The current [clang-format docs|https://developer.lsst.io/tools/clang_format.html] link to setup instructions for vim and emacs, but not SublimeText. We should have a similar link on that page to how to integrate clang-format (including the best choice of Sublime package for it) with SublimeText.",1
"DM-9465","02/15/2017 17:13:37","bug in utility class method objetArrayToJsonSring for Projection unit test ","The bug is FitsHeaderToJson which is a utility class for Projection's unit test.  It is under java/test.  It has nothing to do with java/src.    Bug description:    The objetArrayToJsonSring converts a 2-dimension or 1-dimension array to a Json string.  When it does the conversion, it shares a listObject and this object is cleared after each use.  How since this list is added to another 2-dimension list, the values added in the 2-dimension were accidentally reset to 0.0.    This bug was introduced when create unit test for Projection.  But this bug does not affect the Projection's unit test since the array values are not relevant to the Projection and were not used.    Analysis:  The FitsHeaderToJson was created to produce the testing files for Projection.  Since now there is util/ package under test.  This file should move to util/ directory.",1
"DM-9472","02/16/2017 09:25:45","Update pipelines.lsst.io installation docs for 12.1 release","bq. [somebody] installing the stack following the Conda instructions at pipelines.lsst.io is actually getting v12.1, rather than 12.0?    bq. If so, we have a docs problem since https://pipelines.lsst.io/install/demo.html is pointing them at the lsst_dm_stack_demo for 12.0. And so it fails.",0.5
"DM-9474","02/16/2017 10:02:17","validate_drp example/runDecamTest.sh broken on decam dataset","The {{validate_drp decam}} dataset was working in a test env several days ago.  This dataset was added to the production {{validate_drp}} yesterday and is failing.  It is also failing in the {{validate_drp hsc}} test env.    {code:java}  [py2] $ /bin/bash -e /tmp/hudson5838991375736374023.sh  notice: lsstsw tools have been set up.  Ingesting Raw data  root INFO: Loading config overrride file '/home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/obs_decam/12.1-18-g15f9154+5/config/ingest.py'  CameraMapper INFO: Unable to locate registry registry in root: /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/registry.sqlite3  CameraMapper INFO: Unable to locate registry registry in current dir: ./registry.sqlite3  CameraMapper INFO: Loading Posix registry from /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input  CameraMapper INFO: Unable to locate calibRegistry registry in root: /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/calibRegistry.sqlite3  CameraMapper INFO: Unable to locate calibRegistry registry in current dir: ./calibRegistry.sqlite3  CameraMapper INFO: Loading Posix registry from /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input  ingest.parse WARN: Unable to find value for ccdnum (derived from CCDNUM)  ingest.parse WARN: Unable to find value for ccd (derived from CCDNUM)  ingest.parse WARN: Unable to find value for ccdnum (derived from CCDNUM)  ingest.parse WARN: Unable to find value for ccd (derived from CCDNUM)  /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/instcal/instcal0176837.fits.fz --<link>--> /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176837/instcal0176837.fits.fz  /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/dqmask/dqmask0176837.fits.fz --<link>--> /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176837/dqmask0176837.fits.fz  /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/wtmap/wtmap0176837.fits.fz --<link>--> /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176837/wtmap0176837.fits.fz  /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/instcal/instcal0176846.fits.fz --<link>--> /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176846/instcal0176846.fits.fz  /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/dqmask/dqmask0176846.fits.fz --<link>--> /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176846/dqmask0176846.fits.fz  /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/lsstsw/stack/Linux64/validation_data_decam/master-g52ac2b0d78/wtmap/wtmap0176846.fits.fz --<link>--> /home/jenkins-slave/workspace/validate_drp/dataset/decam/label/centos-7/python/py2/validate_drp/Decam/input/0176846/wtmap0176846.fits.fz  running processCcd  Build step 'Execute shell' marked build as failure  {code}  ",0.5
"DM-9476","02/16/2017 10:43:24","ISR fails in overscan for HSC visit=90738 ccd=33","{code}  pprice@perseus:/tigress/pprice/greco $ processCcd.py /tigress/HSC/HSC --rerun price/test --id visit=90738 ccd=33 --clobber-config --no-versions  root INFO: Loading config overrride file '/tigress/pprice/greco/obs_subaru/config/processCcd.py'  /tigress/HSC/LSST/stack_perseus_20170207/Linux64/miniconda2/3.19.0.lsst4/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.    warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')  Cannot import lsst.meas.extensions.convolved (No module named convolved): disabling convolved flux measurements  root INFO: Loading config overrride file '/tigress/pprice/greco/obs_subaru/config/hsc/processCcd.py'  root INFO: Running: /tigress/HSC/LSST/stack_perseus_20170207/Linux64/pipe_tasks/12.1-22-g7df19e7+1/bin/processCcd.py /tigress/HSC/HSC --rerun price/test --id visit=90738 ccd=33 --clobber-config --no-versions  processCcd INFO: Processing {'taiObs': '2016-11-25', 'pointing': 1790, 'visit': 90738, 'dateObs': '2016-11-25', 'filter': 'HSC-G', 'field': 'SSP_WIDE', 'ccd': 33, 'expTime': 150.0}  processCcd.isr INFO: Performing ISR on sensor {'taiObs': '2016-11-25', 'pointing': 1790, 'visit': 90738, 'dateObs': '2016-11-25', 'filter': 'HSC-G', 'field': 'SSP_WIDE', 'ccd': 33, 'expTime': 150.0}  WARNING: Couldn't write lextab module u'angle_lextab'. [Errno 13] Permission denied: u'/tigress/HSC/LSST/stack_perseus_20170207/Linux64/miniconda2/3.19.0.lsst4/lib/python2.7/site-packages/astropy/coordinates/angle_lextab.py'  WARNING: Couldn't create u'angle_parsetab'. [Errno 13] Permission denied: u'/tigress/HSC/LSST/stack_perseus_20170207/Linux64/miniconda2/3.19.0.lsst4/lib/python2.7/site-packages/astropy/coordinates/angle_parsetab.py'  /tigress/HSC/LSST/stack_perseus_20170207/Linux64/ip_isr/12.1-6-gba867dc/python/lsst/ip/isr/isr.py:389: RuntimeWarning: invalid value encountered in divide    weights=collapsed.data*~collapsedMask)[0]/numPerBin  /tigress/HSC/LSST/stack_perseus_20170207/Linux64/ip_isr/12.1-6-gba867dc/python/lsst/ip/isr/isr.py:391: RuntimeWarning: invalid value encountered in divide    weights=indices*~collapsedMask)[0]/numPerBin  processCcd FATAL: Failed on dataId={'taiObs': '2016-11-25', 'pointing': 1790, 'visit': 90738, 'dateObs': '2016-11-25', 'filter': 'HSC-G', 'field': 'SSP_WIDE', 'ccd': 33, 'expTime': 150.0}:     File ""src/math/Interpolate.cc"", line 204, in lsst::afw::math::InterpolateGsl::InterpolateGsl(const std::vector<double>&, const std::vector<double>&, lsst::afw::math::Interpolate::Style)      Failed to initialise spline for type akima, length 0 {0}  lsst::pex::exceptions::OutOfRangeError: 'Failed to initialise spline for type akima, length 0'    Traceback (most recent call last):    File ""/tigress/HSC/LSST/stack_perseus_20170207/Linux64/pipe_base/12.1-5-g06c326c+10/python/lsst/pipe/base/cmdLineTask.py"", line 347, in __call__      result = task.run(dataRef, **kwargs)    File ""/tigress/HSC/LSST/stack_perseus_20170207/Linux64/pipe_base/12.1-5-g06c326c+10/python/lsst/pipe/base/timer.py"", line 121, in wrapper      res = func(self, *args, **keyArgs)    File ""/tigress/HSC/LSST/stack_perseus_20170207/Linux64/pipe_tasks/12.1-22-g7df19e7+1/python/lsst/pipe/tasks/processCcd.py"", line 181, in run      exposure = self.isr.runDataRef(sensorRef).exposure    File ""/tigress/pprice/greco/obs_subaru/python/lsst/obs/subaru/isr.py"", line 266, in runDataRef      statControl=statControl,    File ""/tigress/HSC/LSST/stack_perseus_20170207/Linux64/ip_isr/12.1-6-gba867dc/python/lsst/ip/isr/isr.py"", line 394, in overscanCorrection      afwMath.stringToInterpStyle(fitType))    File ""/tigress/HSC/LSST/stack_perseus_20170207/Linux64/afw/12.1-32-gb99f2ce+2/python/lsst/afw/math/mathLib.py"", line 6325, in makeInterpolate      return _mathLib.makeInterpolate(*args)  OutOfRangeError:     File ""src/math/Interpolate.cc"", line 204, in lsst::afw::math::InterpolateGsl::InterpolateGsl(const std::vector<double>&, const std::vector<double>&, lsst::afw::math::Interpolate::Style)      Failed to initialise spline for type akima, length 0 {0}  lsst::pex::exceptions::OutOfRangeError: 'Failed to initialise spline for type akima, length 0'    {code}",1
"DM-9490","02/16/2017 15:27:57","Remote api issues","Do the following:  * make point selection work if extension is added before image * point selection is not always showing point * mask is not working when zoom is quickly changed before new mask is added * make table active row table extension * remotely turning on target match ",8
"DM-9495","02/16/2017 17:12:03","Fix all jointcal header multiple-inclusion #defines","As pointed out in another review by [~krzys], jointcal isn't following the {{#define LSST_BLAH}} standard for multiple inclusion prevention in its header files. Should fix this with a quick pass over the current headers. Might also be a good time to look over the existing headers and see if any can disappear or be merged elsewhere.",1
"DM-9500","02/17/2017 12:42:00","FitsDownloadDialog.js has bugs","The FitsDownloadDialog only works for NO_BAND.  When a 3-color image is created, it has the following issues:    # it only shows a ""red"" color band  # Saving file does not work  because the null FITS file name is in the URL  # It does throws the exception, for example, saving a 2mass 3-color image, the exception is:    ""GET http://localhost:8080/firefly//servlet/Download?file=null&return=twomass-j.fits&log=true 404 (Not Found)  download @ WebUtil.js:274  resultsSuccess @ FitsDownloadDialog.jsx:356  onSuccess @ FitsDownloadDialog.jsx:282  validUpdate @ CompleteButton.jsx:25  (anonymous) @ CompleteButton.jsx:34""    1. when its s a color image,  all 3 bands should be available for FITS download  2. for PNG and region file, there i son need for band choice  3. the title of the dialog should be ""File Download"", not ""Fits download""  ",3
"DM-9502","02/17/2017 16:45:17","SpherePoint throws wrong exception for invalid arguments","Several methods in {{SpherePoint}} throw {{pex::exceptions::OutOfRangeError}} when fed invalid arguments. Assuming the pex exceptions are intended to mimic the standard C++ exceptions of the same name (see DM-9435), this is inappropriate -- {{OutOfRangeError}} should refer to invalid indices, not other cases where an argument falls outside some interval.    These methods should be changed to throw either {{pex::exceptions::DomainError}} or {{pex::exceptions::InvalidParameterError}}, which apply to generic numerical arguments.",1
"DM-9503","02/17/2017 16:54:16","afw catalog asAstropy fails due to multiple columns of same name","When using {{catalog.asAstropy()}}, we experience an error when converting certain tables:    {code}  >>> catalog.asAstropy()  Traceback (most recent call last):    File ""<stdin>"", line 1, in <module>    File ""/slac/lsst/stack/DarwinX86/afw/12.1+1/python/lsst/afw/table/_syntax.py"", line 272, in BaseCatalog_asAstropy      return cls(columns, meta=meta, copy=False)    File ""/slac/lsst/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/site-packages/astropy/table/table.py"", line 360, in __init__      init_func(data, names, dtype, n_cols, copy)    File ""/slac/lsst/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/site-packages/astropy/table/table.py"", line 624, in _init_from_list      self._init_from_cols(cols)    File ""/slac/lsst/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/site-packages/astropy/table/table.py"", line 697, in _init_from_cols      self._make_table_from_cols(self, newcols)    File ""/slac/lsst/stack/DarwinX86/miniconda2/3.19.0.lsst4/lib/python2.7/site-packages/astropy/table/table.py"", line 729, in _make_table_from_cols      raise ValueError('Duplicate column names')  ValueError: Duplicate column names  {code}",1
"DM-9504","02/17/2017 17:33:27","lsst_py3 CI failure due to meas_extensions_ngmix","[~npease] points out that the lsst_py3 Jenkins build is currently failing and has been since [build #454|https://ci.lsst.codes/job/stack-os-matrix/label=centos-7,python=py3/21317/console] (regardless of the misleading red circles on Jenkins). Reported error is:    {code}  ======================================================================  FAIL: testLeaks (__main__.TestMemory)  !Check for memory leaks in the preceding tests  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py3/lsstsw/stack/Linux64/utils/12.1-5-g648ee80+2/python/lsst/utils/tests.py"", line 161, in testLeaks      self.fail(""Leaked %d block%s"" % (nleak, plural))  AssertionError: Leaked 13 blocks    ----------------------------------------------------------------------  Ran 10 tests in 0.688s    FAILED (failures=1, skipped=2)       Number of calls to function has reached maxfev = 10.       Number of calls to function has reached maxfev = 10.       Number of calls to function has reached maxfev = 10.       Number of calls to function has reached maxfev = 10.  {code}",2
"DM-9505","02/17/2017 18:38:04","Please serve the ndarray tutorial somewhere","At one time the ndarray tutorial was part of the Doxygen documentation, but that is no longer the case. It really should be served somewhere -- preferably linked from the docs that are already on github.",1
"DM-9517","02/20/2017 08:01:50","Migrate meas_algorithms to modern afwDisplay","meas_algorithms contains several (e.g. [#1|https://github.com/lsst/meas_algorithms/blob/0bf8251ccb5c79be6e181817359729fdb2425311/python/lsst/meas/algorithms/objectSizeStarSelector.py#L43], [#2|https://github.com/lsst/meas_algorithms/blob/d421edbfcf2fc993cbad9211c1498767479d069a/python/lsst/meas/algorithms/pcaPsfDeterminer.py#L35], there are more) explicit uses of {{lsst.afw.display.ds9}}. These should be replaced by calls to the generic (backend-independent) {{lsst.afw.display}} system.",1
"DM-9518","02/20/2017 08:22:53","Move activemq packages to legacy status","With the move of ctrl_events to legacy status the activemqcpp and ctrl_activemq packages is no longer in use,      Moving lsst/activemqpp to lsst-dm/legacy-activemqcpp  (this is a move and a change in etc/repos.yaml)    and    Moving lsst/ctrl_activemq to lsst-dm/legacy-ctrl_activemq (this is just a move)",0.5
"DM-9524","02/20/2017 11:54:19","mangle OSX EUPS tarball shebang paths","A method of mangling the shebangs on OSX to a valid path is needed.",2
"DM-9528","02/20/2017 14:10:03","Cleanup pybind11 code in meas_deblender","Use the checklist from DM-9182 to clean up code in meas_deblender.",1
"DM-9531","02/20/2017 16:53:15","Fix override warnings in afw","Compiling afw on modern clang results in this warning:  {code}  include/lsst/afw/geom/SpanSet.h:662:10: warning: 'isPersistable' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]      bool isPersistable() const { return true; }           ^  {code}  Please fix by marking the method {{override}}",0.5
"DM-9534","02/21/2017 11:17:23","Output jointcal metrics via a metrics logger","Until we have butler metrics persistence system, we can use a dedicated logger to output each product's metrics. Since jointcal is the testbed for the new metrics system, we'll use it as an example for how to produce those logs.",5
"DM-9535","02/21/2017 11:25:47","Assess whether differences in Brighter-Fatter implementations are contributing to the trace radii differences: LSST vs. HSC","As noted in DM-6817 and highlighted/further explored in DM-9411, there is a trend of increasing difference in trace radii with increasing magnitude when directly comparing outputs from the HSC vs. LSST stacks.  A likely culprit is slight differences in the implementations of the Brighter-Fatter corrections between the stacks.  This ticket is to assess whether this is contributing to the trace radii differences.",5
"DM-9539","02/21/2017 12:03:36","jointcal validation framework integration (continued from S17)","Jointcal will be the testbed for the new metrics validation system. This epic captures the work on the jointcal side, including integrating jointcal's output into the system, and writing user documentation to allow others to plug their products into validation.",20
"DM-9543","02/21/2017 17:10:56","calling showXYPlot API without xcol/ycol name fails","The API call 'showXYPlot' fails when xCol, yCol is not specified.    Before migration, the function was used without specifying the column name, the xy plot was plotting the first 2 numerical columns if no column is passed in.  That is no longer the case, the function seems to be missing. If not, then documentation should be updated on how to call the same function as before.    Please check and fix.",1
"DM-9549","02/22/2017 12:13:22","ctrl_stats fails on mac os","The Linux build for ctrl_stats succeeds, but the mac OS X build for ctrl_stats fails in the tests/testYearWrap.py.    Additionally, a change in Python 3.6:  {code}  Unknown escapes consisting of '/' and an ASCII letter in regular expressions will now cause an error.  {code}  now makes some code in terminated.py fail (and possibly in other places).",3
"DM-9552","02/22/2017 14:45:53","Reported mouse position on mouse click is a couple of pixels off","1. The symbol was drawn a few pixels off the center of the mouse when ""lock by click"" checkbox is selected.   2. Other drawing functions, like distance tool, the area selection tool are also affected.   ",2
"DM-9553","02/22/2017 14:53:14","Investigate the best algorithm to compute derivatives for the Brighter-Fatter correction","As noted in DM-9535, the method used to compute second derivatives in the Brighter-Fatter correction implementation can lead to significant (up to 1%, which is particularly significant for weak lensing considerations) differences in the trace radii of sources, with the differences becoming larger with brighter magnitudes (see figures in DM-9535).  This ticket is to investigate the optimal algorithm to use for the Brighter-Fatter correction.",1
"DM-9556","02/22/2017 15:53:20","All NaNs in coord_ra and coord_dec columns in deepCoadd forced src tables","In recent runs of the stack through *multiBandDriver.py*, the persisted forced src tables for the coadds are not getting ra and dec set properly (all entries for the {{coord_ra}} and {{coord_dec}} columns are NaN).  Looking back at a run in mid-Nov, 2016, these numbers were indeed set properly in the forced tables.  Assuming this was not intentional, track down the cause and fix it such that these values get set properly for the persisted forced src tables.",1
"DM-9561","02/23/2017 09:23:53","Improve Monotonicity Operator","Preliminary testing of the NMF deblender shows that the radial monotonicity operator as designed does not work as expected. [~pmelchior] and I have verified that the monotonicity operator itself is built properly based on our earlier design, which uses a single reference pixel (the one that lies closest to a radial line from the peak to the current pixel) for each pixel.    Based on a pixels position from the peak, it lies in one of 8 octants that determines which pixel it will use as a reference, but it appears that the transition between reference pixel positions is causing the monotonicity operator to generate weird streaks in the deblended objects that are unphysical.    We are redesigning the monotonicity operator to use the weight of all three neighboring pixels that are closer to the peak as a reference, which we hope will fix this issue.",8
"DM-9564","02/23/2017 10:33:16","Set assembled Coadd Psf to modelPsf with auto-computed dimensions ","DM-8088 changed makeCoaddTempExp so that the user no longer has to specify the pixel dimensions of the model PSF to match to. The model Psf dimensions are updated at runtime to match those of the warped calexp PSFs (which have dimensions impossible for a user to know ahead of time).     AssembleCoadd currently attaches the PSF corresponding to the *user-specified model PSF* dimensions rather than the updated PSF-dimensions.  This is bad because the user-specified dimensions could be way off and it's incongruous to tell users that they don't have to pay attention to this in makeCoaddTempExp, but they do in assembleCoadd.     This ticket will ensure that the modelPsf dimension information flows down from the coaddTempExps to the assembled deeepCoadd in a sensible way. Most likely using the maximum dimensions of the input coaddTempExps.    ",2
"DM-9578","02/24/2017 14:35:20","warning message does not disappear even with valid data if the mouse is covering the warning icon","if user enters invalid data and keeps his mouse over the warning icon while entering valid data into a field then the warning message does not disappear even with valid data. It stays up for the life time of the firefly webapp.    ",1
"DM-9579","02/24/2017 15:17:03","nondeterministic random number seeds in MeasurePsf candidate reservation","{{MeasurePsfTask}} randomly reserves a fraction of its candidates for validation, in a way that is supposed to be deterministic.  This seems to be broken; I've identified at least two problems:   - {{CharacterizeImageTask}} does not pass the {{expId}} argument to {{MeasurePsfTask.run}}, letting it default to zero.   - {{MeasurePsfTask}} uses Python's built-in {{random}} module instead of {{afw.math.Random}}.  Contrary to its own documentation, calling {{random.seed(0)}} does not always produce deterministic results (though I've only been able to trigger this the first time I tried it):  {code}  In [3]: random.seed(0)    In [4]: random.random()  Out[4]: 0.7579544029403025    In [5]: random.seed(0)    In [6]: random.random()  Out[6]: 0.8444218515250481    In [7]: random.seed(0)    In [8]: random.random()  Out[8]: 0.8444218515250481    In [9]: random.seed(0)    In [10]: random.random()  Out[10]: 0.8444218515250481  {code}    I have no idea what could be going on in the built-in {{random}} module (and it's hard to report upstream since I was only able to trigger it once), but we should switch to our own random number generator regardless.",1
"DM-9581","02/24/2017 18:24:26","MSX image rotation not computed correctly.","Update From Trey - 2/28:    This problem has nothing to do with WCS match.  It is a problem with rotation of MSX images. They are galactic based which confuses our computation.  To reproduce: Read in MSX image with target m16 and the rotate 180.  The problem is obvious. In this case the relative rotation is computed wrong. The problem begins at FitsRead.createFitsReadRotated.     ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  ORIGINAL TITLE:  WCS match behavior is odd    ORIGINAL DECRIPTION:  Luisa reported 10 days ago on TT (#9353) on the OPS version of Firefly/IRSAViewer:    WCS align works in some odd ways in IRSA Viewer when MSX images are included; it doesn't actually align the MSX image to other images properly. If you align to MSX images, though, it works fine.  I wonder if whatever is going on is somehow tied to the galactic coordinates that the MSX tile should have, though of course the tool should only be paying attention to RA/Dec.    Step to reproduce:  OS 10.11.6  Firefox 51.0.1    search on M16, WISE ch1 (default), default size.  again, new image, MSX A band (default), all other defaults  again, new image, DSS poss2uk red (default), all other defaults.    It returns three images of the sky.  physically what is going on here is that the wise image and the DSS image that are returned should already have north up. the MSX image, however, is in galactic coordinates by default but carries the RA/Dec with it, of course. I just mean the tile is aligned with galactic coordinates by default.    At this point, the DSS image should be “selected” (e.g., outlined in orange).  click WCS match.  WISE doesn’t change, but MSX rotates about 45 degrees. (see first attachment.) here the features in the WISE and POSS images are aligned, so they physically make sense. The MSX one isn’t right.  Click on the MSX image so that it is outlined in orange.  click on WCS match to turn it off. All three images move slightly for me but don’t change orientation.  click on WCS match to turn it on. Now all three images are rotated.  (see second attachment.)  the MSX image didn’t change, but the nebulosity is now aligned properly, so this is really the correct alignment (though note that North isn't up anymore). The tool knows North isn't up; coordinates overlaid and a compass rose overlaid are aligned properly.     (From TT9353)    [March-1-2017 LZ]  I reproduced the bug according to Trey's instruction.  I checked the codes where the rotation is calculated and found that the rotation coordinate somehow is predefined as CoordinateSys.EQ_J2000.  Thus, if the image is not in CoordinateSys.EQ_J2000, the calculation is wrong.  My suggestion is to use the incoming coordinate instead.    {code}            WorldPt worldPt1 = projection.getWorldCoords(centerX, centerY - 1);              WorldPt worldPt2 = projection.getWorldCoords(centerX, centerY);              double positionAngle = VisUtil.getPositionAngle(worldPt1.getX(),                      worldPt1.getY(), worldPt2.getX(), worldPt2.getY());              if (fromNorth) {                  long angleToRotate= Math.round((180+ rotationAngle) % 360);                  if (angleToRotate==Math.round(positionAngle)) {                      return fitsReader;                  }                  else {                      return createFitsReadPositionAngle(fitsReader, -angleToRotate, inCoordinateSys);//CoordinateSys.EQ_J2000);                  }              }              else {                  return createFitsReadPositionAngle(fitsReader, -positionAngle+ rotationAngle, inCoordinateSys);//CoordinateSys.EQ_J2000);    {code}  ",2
"DM-9588","02/27/2017 13:41:36","validate_drp broken on cfht/hsc datasets","{{validate_drp}} has been failing since the 22nd. This is suspiciously coincidental with the merger of https://github.com/lsst-sqre/jenkins-dm-jobs/pull/58 .  The first build failure appears to be shell script related but the most recent failures for both the {{cfht}} and {{hsc}} data set look like they may be the result of a change in the stack.    *HSC*    https://ci.lsst.codes/job/validate_drp/835/dataset=hsc,label=centos-7,python=py2/console    {code:java}  Traceback (most recent call last):    File ""/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/bin/validateDrp.py"", line 97, in <module>      validate.run(args.repo, **kwargs)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/validate.py"", line 104, in run      **kwargs)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/validate.py"", line 217, in runOneFilter      job=job, linkedBlobs=linkedBlobs, verbose=verbose)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/calcsrd/amx.py"", line 159, in __init__      verbose=verbose)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/calcsrd/amx.py"", line 242, in calcRmsDistances      visit[obj2], ra[obj2], dec[obj2])    File ""/home/jenkins-slave/workspace/validate_drp/dataset/hsc/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/calcsrd/amx.py"", line 326, in matchVisitComputeDistance      j = visit_obj2_idx[j_raw]  IndexError: index 3 is out of bounds for axis 0 with size 3  Build step 'Execute shell' marked build as failure  [PostBuildScript] - Execution post build scripts.  {code}    *CFHT*    https://ci.lsst.codes/job/validate_drp/835/dataset=cfht,label=centos-7,python=py2/console      {code:java}  Traceback (most recent call last):    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/bin/validateDrp.py"", line 97, in <module>      validate.run(args.repo, **kwargs)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/validate.py"", line 104, in run      **kwargs)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/validate.py"", line 204, in runOneFilter      verbose=verbose)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/matchreduce.py"", line 147, in __init__      repo, dataIds, matchRadius)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/validate_drp/master-g3511a1277e+1/python/lsst/validate/drp/matchreduce.py"", line 229, in _loadAndMatchCatalogs      oldSrc = butler.get('src', vId, immediate=True, flags=SOURCE_IO_NO_FOOTPRINTS)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/daf_persistence/12.1-19-gd507bfc/python/lsst/daf/persistence/butler.py"", line 845, in get      location = self._locate(datasetType, dataId, write=False)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/daf_persistence/12.1-19-gd507bfc/python/lsst/daf/persistence/butler.py"", line 795, in _locate      location = repoData.repo.map(datasetType, dataId, write=write)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/daf_persistence/12.1-19-gd507bfc/python/lsst/daf/persistence/repository.py"", line 198, in map      loc = self._mapper.map(*args, **kwargs)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/daf_persistence/12.1-19-gd507bfc/python/lsst/daf/persistence/mapper.py"", line 144, in map      return func(self.validate(dataId), write)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/obs_base/12.1-21-gbdb6c2a+2/python/lsst/obs/base/cameraMapper.py"", line 379, in mapClosure      return mapping.map(mapper, dataId, write)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/obs_base/12.1-21-gbdb6c2a+2/python/lsst/obs/base/mapping.py"", line 124, in map      actualId = self.need(iter(self.keyDict.keys()), dataId)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/obs_base/12.1-21-gbdb6c2a+2/python/lsst/obs/base/mapping.py"", line 257, in need      lookups = self.lookup(newProps, newId)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/obs_base/12.1-21-gbdb6c2a+2/python/lsst/obs/base/mapping.py"", line 221, in lookup      result = self.registry.lookup(properties, self.tables, lookupDataId, template=self.template)    File ""/home/jenkins-slave/workspace/validate_drp/dataset/cfht/label/centos-7/python/py2/lsstsw/stack/Linux64/daf_persistence/12.1-19-gd507bfc/python/lsst/daf/persistence/registries.py"", line 330, in lookup      c = self.conn.execute(cmd, valueList)  sqlite3.OperationalError: no such column: flags  {code}  ",1
"DM-9590","02/27/2017 14:52:39","XY plot is unrecoverable after it fails because of column name doesn't exist","Sometimes, Gator or LC viewer shows an xy plot by settings xCol, yCol which doesn't exist in the table.   That produce an exception and plot is not displayed, error message with the java exception is shown (*at least message should be user-friendly and not code oriented*).    From there, there is no way to recover the plot, even if the user change the column name to an existing one, it won't respond and display any data.  ",8
"DM-9594","02/27/2017 17:30:56","The default radio button (point)  in scatter plot  is not selected after line style changes.","Steps to reproduce:  - switch between different styles  - reset to default  - the selected radio button but shown.    Emmanuel reported that the default radio button is no longer selected after the line style changes.  I tested in the DM-9343 and saw it.  Then I tested it in the dev and saw it there as well.  It seems an existing problem.      [March-8-2017]  When the setOption is called by clicking reset button, the defaultParams which contains only x and y is passed to the setOption.  Since the plotStyle is not in the defaultParams, its default value is used.  How the default value never triggers the listener.  To fix it, the plotStyle was added to the default parameter.  Thus, the plotStyle is stayed after resetting the other fields.    [March-10-2017]  According to Tatiana, each plot has a set of default values.   When the reset button is pressed, the option parameters should be back to the default.  Therefore, the above implementation is against the design.     Tatiana did research and found out that the there was an implementation issue in RadioGroupInputView, see http://react.tips/radio-buttons-in-reactjs/.  The  RadioGroupInputView became an uncontrolled component when the ""name"" is used  along with value and check fields. The uncontrolled component can not be handled by the React Component.   Thus, the checked=true is not  displayed.     To fix this issue:  # Remove the name attribute from the ""input"" tag   # Since the fieldKey is not needed, fieldKey is remove as well.     {code}    function makeOptions(options,alignment ,value,onChange,tooltip) {        const labelStyle= alignment==='vertical' ? vStyle : hStyle;      return options.map((option) => (          <span key={option.value}>              <div style={{display:'inline-block'}} title={tooltip}>                  <input type='radio'                         title={tooltip}                         value={option.value}                         checked={value===option.value}                         onChange={onChange}                  /> <span style={labelStyle}>{option.label}</span>              </div>              {alignment==='vertical' ? <br/> : ''}           </span>      ));  }    {code}  ",1
"DM-9604","02/28/2017 09:16:33","Write up data/parallelization axis transformation examples for SuperTask WG","Produce slides or a short, informal document showing examples from DRP where we need to change data units and/or parallelization axis at a scale we expect to be handled by SuperTask.",2
"DM-9608","02/28/2017 11:52:08","Fix LTD Dasher title processing (handle removal)","The Dasher rendering code that cleans up titles tries to remove handle prefixes. So that    {{code}}  SQR-016: Stack release playbook  {{code}}    becomes    {{code}}  Stack release playbook  {{code}}    There's a bug in the filter that turns this input:    {{code}}  Stack release playbook  {{code}}    into    {{code}}  tack release playbook  {{code}}    This ticket fixes that.",0.5
"DM-9615","02/28/2017 13:35:29","Convert DCR code to use Tasks","The DCR code should be written so that the classes inherit from Tasks, and make use of those features.",8
"DM-9616","02/28/2017 13:36:56","Make DCR command line task","It should be possible to run the new DCR template generation code from the command line.",3
"DM-9617","02/28/2017 13:40:16","Use logging in DCR code","The prototype DCR code currently uses print statements to issue warnings and informative messages. These should be converted to use the standard logging system.",3
"DM-9623","02/28/2017 14:58:37","Investigate posible matcher improvements","This issue is a catch all for investigative work relating to DM-7366 that has blocked DM-8113.    Optimistic pattern matcher B appears to preform poorly for dense stellar fields (6-10k reference stars per visit). This issue involves finding solutions, work arounds, and possible matcher improvements for both the current stack matcher and the pure Python implementation from a previous issue in this Epic. This involves finding ways to mitigate false positive matches reliably and discovering why the stack matcher fails immediately when running on dense stellar fields.",20
"DM-9638","03/01/2017 01:10:25","empty image tab appears unexpectedly after selecting an histogram column and cancel","Is a very weird bug but i ran into it when i hit the 'cancel' button of the 'chart' drop down dialog.  An empty white image tab titled {{Image Meta Data}} suddenly appears next to the {{coverage} tab.    To test: Open tri-view, do a catalog search, then click 'charts', then 'histogram' radio button. Then select column (this is the part that make the bug apparent), then 'ok', then 'cancel'. The empty tsab will appear next to 'coverage' tab.    From Trey 3/1:  This bug is probably in FireflyViewerManager.js    From Tatiana 3/2    Another test case:  1. Load Firefly.  2. Do catalog search (m31, default settings) - tri-view table, xyplot, coverage shows up  3. Click on FITS Header icon - empty ""Image Meta Data"" tab comes up    Looks like the empty white image tab titled {{Image Meta Data}} appears whenever we add a new table, which is not coverage or catalog. In the scenarios above, the tables are column table or fits header table - both of them appear to FireflyViewerManager as ""image metadata"" tables. The bug is in the last line of converterUtils.js:isMetaDataTable function - Boolean(converter) is always true, because we have ""UNKNOWN"" converter. So the question is when this UNKNOWN converter is relevant and how can we separate image metadata tables with UNKNOWN converter from other tables.   ",2
"DM-9639","03/01/2017 06:15:38","Install v13 release into shared stack","v13 has now been tagged. Update the shared-stack build script on lsst-dev to install it, and make sure that happens successfully.",0.5
"DM-9646","03/01/2017 08:03:26","Update DRP release notes for v13","The v13 release [has been tagged|https://sw.lsstcorp.org/eupspkg/tags/v13_0.list], but the [DRP release notes|https://confluence.lsstcorp.org/display/DM/Data+Release+Production+WIP+F16+Release+Notes] are a couple of months out of date. Update them and provide them to SQuaRE to accompany the release.",2
"DM-9669","03/01/2017 13:19:11","Butler(root=""foo"") should not warn about mapper class instance","when initializing butler as Butler(""foo""), butler warns   daf.persistence.butler WARN: mapper ought to be an importable string or a class object (not a mapper class instance)    it should not do this - a mapper instance was not passed in.",1
"DM-9675","03/01/2017 17:05:02","Soften symmetry operator requirements","The current symmetry operator enforces strict symmetry, meaning that all pixels that do not have a symmetric partner in the footprint are penalized. To ease this requirement, which may be necessary if some of the flux for one (or more) of the objects lies outside of the footprint, we allow the user to use a less stringent penalty value.",1
"DM-9681","03/01/2017 22:08:46","Write function to apportion flux based on NMF template weights","The main assumption of the current deblender is that galaxies have an identical profile in each band. This is not strictly true, as the color near the center of galaxies is more white.    It is worth attempting to use the NMF deblender output as a template, similar to the symmetric templates generated by the SDSS deblender, and re-apportion flux based on the ratio of the templates in each pixel. This may help us recover more accurate colors while still performing better in the wings of blended galaxies than the current deblender.    This ticket will refactor a version of {{lsst.meas.deblender.apportionFlux}} to re-apportion flux for NMF templates.",1
"DM-9682","03/02/2017 06:14:17","Update git-lfs in lsst-dev shared stack","The {{git-lfs}} installed in the shared stack on {{lsst-dev}} is pretty elderly. Please provide a newer version.",0.5
"DM-9686","03/02/2017 10:21:22","Add LSST branding to pipelines.lsst.io","Similar to DM-6120, add LSST branding to pipelines.lsst.io theme.    Add:    - Edit on GitHub button  - LSST branding  - Edit on GitHub button  - LTD edition dashboard links",1
"DM-9710","03/03/2017 14:37:13","Update the FitsReadTest.java due to the changes in rotation's calculation in FitsRead","After DM-9581 was implemented, the FitsRead's unit test is failed because the testing data prepared is no longer correct. The FITS file used in the unit test for rotation was created by running FitsRead.createRotationAngles.  Since this method is modified, the old FITS file is no longer correct.    The following needs to be done:    * Generate the new testing data and store in the firefly_test_data  * Re-run the unit test to make sure everything work    ",1
"DM-9711","03/03/2017 14:41:50","Clean up meas_base pybind11 wrappers","Rebasing meas_base and other packages with `meas.base.Algorithm` subclasses for the DM-9249 changes.    This will require new/different pybind11 wrappers for changed C++ interfaces.",2
"DM-9715","03/03/2017 16:02:05","cppIndex should raise Python's built-in IndexError","In order to synthesis {{\_\_iter\_\_}} from {{\_\_getitem\_\_}} Python apparently requires *exactly* {{IndexError}} to be thrown, so that's what we should throw in these functions.    That will require rewriting the testPybind11.cc unit test in a way that has access to Python symbols (which should be done anyway).    There will likely be workaround code in afw/table/python/catalog.h that can be cleaned up after this change (I'll just catch OutOfRangeError there and re-throw as IndexError).",0.5
"DM-9740","03/08/2017 10:27:38","Make lsst_ci run obs_cfht, obs_decam example in its scons test.","Make {{lsst_ci}} run {{obs_cfht}}, {{obs_decam}} examples in its {{scons}} test.    The {{test}} step of {{lsst_ci}} should verify that the simple test example runs of {{obs_}} packages succeed.    Monitoring the results using {{validate_drp}} will be delayed for a later ticket.  Just making sure things run successfully through {{processCcd.py}} or the equivalent is the goal here.    1. [x] Add {{validation_data_cfht}}, {{validation_data_decam}} to required dependencies.  2. [x]  Run command-line based test scripts.  3. [x] Wrap these scripts in scons test to report pass/fail based on just successfully completing.",2
"DM-9743","03/08/2017 16:03:53","Field of time column name handling for Time Series Viewer","- The raw data table should be updated and a light curve calculation restarted (the existing phase folded table is removed) if the time column name in TS viewer changes because the table is sorted on the new column -- currently nothing happens when user changes the input value for the time column name field to be another valid one.   - add time column name field for LSST   ",2
"DM-9745","03/09/2017 11:36:21","Update text relating to baseline CentOS versions for building stack","Following RFC-293 we have agree that we can specify a minimum CentOS7 version for building the stack. Locate relevant documentation and update it with this information.",0.5
"DM-9752","03/09/2017 15:19:13","Add jointcal to lsst_distrib","This ticket implements RFC-300.    Once jointcal has been ported to pybind11 (DM-9187), it will be ready to join the stack in lsst_distrib. This includes the {{jointcal_cholmod}} dependency, and the optional {{testdata_jointcal}} package (which will be excluded from most installs due to size).    Note to [~jhoblitt]: We'll need to add {{testdata_jointcal}} to the various exclude files once this is ready to go. I'll ping you at that time.",1
"DM-9754","03/10/2017 12:40:36","re-separate python Storage & cpp Storage","due to a naming collision, there are separate classes both called Storage in cpp and python. They got combined during the pybind11 conversion and are re-separated here.  ",1
"DM-9757","03/10/2017 13:02:44","Add stat table usage options to mysql config file","For mariadb query optimization (DM-9175) in qserv, the relevant parameters must be set persistently in the mysql configuration file so that the generated stats tables are used to run queries on table chunks. The options to be added are:    {code}  set use_stat_tables='PREFERABLY';  set optimizer_use_condition_selectivity=3;  {code}    Their default values are {{'NEVER'}} and {{1}}.",1
"DM-9758","03/10/2017 13:17:17","astshim fails to build on linux","astshim fails to build on linux using Jenkins. Here is a build log  https://ci.lsst.codes/job/stack-os-matrix/22127/label=centos-7,python=py2/console    Also fix build warnings",2
"DM-9761","03/10/2017 14:15:16","butlerProxy test does not clean up after itself properly.","the tearDown function needs to be updated to reflect recent changes to the path in the setUp function.",1
"DM-9764","03/10/2017 15:33:38","SOURCE_IO_NO_FOOTPRINTS and related enums should be properly wrapped in pybind11","In order to use {{lsst.afw.table.SOURCE_IO_NO_FOOTPRINTS}} in python, it has to be explicitly cast to {{int()}}. This is a bug in the pybind11 wrapper.",0.5
"DM-9765","03/10/2017 16:02:45","Suspicious numerical precision code in Angle","The implementation of {{Angle::wrapNear}} includes several corrections for possible truncation error, the last of which is to rescale the wrapped angle by (1 - 2ε) if it is too far below {{refAng}}. This requirement is odd, for example because it is sensitive to the absolute values of {{refAng}} and the angle to be wrapped.    However, I have not been able to create a test case that exposes a problem in the wrapping, either by trying to carefully tune the value to give zero wrapped angle or by trying to work with very large angles like 10000π. Attempts to reproduce the suspected bug should continue, and if successful the bug should be fixed.",1
"DM-9766","03/10/2017 16:24:59","FITS ""can opener""","It would be useful to expose in Firefly applications the ability to read an arbitrary FITS file, browse its headers and any extensions, and be able to visualize whatever subset of data types Firefly can natively handle (e.g., image, basic table).    Mutatis mutandi for other file types we support.    This would be a useful debugging tool, but it would also be useful in the JupyterLab environment where plugins can be associated with specific ""file"" types in the file browser component.    Then, if users have FITS files in their workspace storage, they can open them in the JupyterLab environment with simple UI actions.    It may also be of interest in other Firefly application contexts, e.g., in IRSA, to offer this capability, but of course there are security issues associated with providing a general ""upload a FITS file and we'll tell you all about it"" capability.",0
"DM-9769","03/10/2017 17:37:25","Provide SLAC information on image cutout, reference IRSA existing practice","SLAC has delivered imageServ APIs to retrieve the image cutouts but they have questions about how to provide the cutouts for coadds (per Kenny Lo). They would like our team to provide how-to.     Thanks to Tatiana who points out that IRSA has been using the cutouts algorithm in http://irsatest.ipac.caltech.edu//ibe/cutouts.html. Basically users provide the center coordinates and the size of the cutout. ""The size parameter consists of one or two (comma separated) values followed by an optional units specification. Units can be pixels (px, pix, pixels) or angular (arcsec, arcmin, deg, rad); the default is degrees. The first size value (x) is taken to be the full-width of the desired cutout along the first image axis (NAXIS1), and the second (y) is taken to be the full-height along the second axis (NAXIS2). If only one size value is specified, it is used as both the full-width and full-height. Negative sizes are illegal.""     Should we suggest SLAC team to consider to adapt the same algorithm for LSST image cutouts, both single epoch and coadd? Need David C. and Gregory to consider this and approve it.    Please see the full document in the link below.  ",3
"DM-9775","03/13/2017 06:26:49","Create NMF deblender presentation","Create a presentation for the Princeton Monday Meeting to outline NMF, proximal operators, and the new deblender.",3
"DM-9784","03/13/2017 11:42:05","Implement test for overlapping galaxies","Once DM-9644 is completed we need to create a test that calculates the amount of overlap between galaxies, basically    (S{~}i~ S{~}j~)^2^ / ((∑S{~}i~^2^) (∑S{~}j~^2^))    for both the measured intensities and simulated intensities.",5
"DM-9789","03/13/2017 16:37:14","pipelines.lsst.io v13 needs some fixes","After perusing the latest version of pipelines.lsst.io, I made the following comments on the merge commit:  * {{metrics/v13_0.rst}} line 9: Should we explain that we are not trying to compare to the ""glide path"" in LDM-240 anymore and that the targets are hence goals for much later rather than thresholds we are supposed to meet with this release?  * {{releases/note_source/v13_0.rst}} line 659: There's no release note that says that {{daf_butlerUtils}} was moved to {{obs_base}}? And I think the title of this would be a bit better if it had ""definitions"" appended.  * {{releases/note_source/v13_0.rst}} line 786: If there were no documentation improvements, this section should be omitted.  But I think there were some, at least to pipelines.lsst.io itself...",1
"DM-9790","03/13/2017 17:07:43","Items missing from the pybind11 wrapping guide","I've been using the [pybind11 wrapping guide|https://developer.lsst.io/coding/python_wrappers_for_cpp_with_pybind11.html] to wrap jointcal, and it's generally been quite helpful, but there were a few questions it didn't answer for me.     * what to change/add in {{python/SConscript}} and {{__init__.py}}.  * how many of the wrapped modules to include in {{__init__}}: should they all be included as a general rule, or just those that one definitely wants to be accessible from python?  * How to choose between {{clsBlah}} and {{cls}} when naming the wrapped code (it's discussed in the style guide, but it would be useful to say something about it in the wrapping guide, with a link to the style guide sections).  * how to import new pybind11 modules in python (no more blahLib).    It might be useful to also have a list of ""how to remove all your SWIG stuff"", for those dealing with packages that were previously SWIGed.",1
"DM-9792","03/13/2017 18:11:10","Mapping::getInverse not exception-safe","The current implementation of {{Mapping::getInverse}} allocates a new {{AstMapping}}, but does not free it if AST's internal error flag triggers an exception via {{assertOK}}. -In addition, {{Object::fromAstObject}} does not free {{rawObjCopy}} if there is a dynamic casting error.-    The leaks in these methods should be fixed using either {{assertOK}} or a {{catch}} or {{finally}} block. Unfortunately, as I understand it astshim's object and type juggling would interfere with using a {{unique_ptr}}.    In addition, both methods should document the exceptions they throw and their (intended) exception safety.",2
"DM-9794","03/13/2017 18:20:36","Pass both LSST_LIBRARY_PATH and DYLD_LIBRARY_PATH in scons on Mac OS","To allow both Python code and shell scripts to inherit the necessary env settings, I'd like {{sconsUtils.utils.libraryLoaderEnvironment}} to pass both {{LSST_LIBRARY_PATH}} and {{DYLD_LIBRARY_PATH}} to things it calls on Mac OS.    Currently {{libraryLoaderEnvironment}} is used within {{sconsUtils}} only by {{sconsUtils.tests.Control.run}} to ensure that, on Mac OS, that {{DYLD_LIBRARY_PATH}} is passed with either {{DYLD_LIBRARY_PATH}} or {{LSST_LIBRARY_PATH}}, whichever is defined first in that search order.    But a shell script will be called out of {{/bin}} or an equivalently system-protected directory and needs {{LSST_LIBRARY_PATH}} passed (which will not be stripped) so that that shell script can then reset {{DYLD_LIBRARY_PATH}} (which will be stripped by SIP).    The particularly motivating usage is to enable Python test code to call a shell script as part of DM-9740.",2
"DM-9795","03/13/2017 20:02:27","CModel priors are weighted incorrectly relative to likelihood","When per-pixel variances are turned off, CModel likelihoods are computed without using the variance at all.  This would not matter in a pure likelihood fit, but it means the prior and likelihood are not given the appropriate relative weights - and the relative weighting is not even consistent; it depends on the noise level of the image.    Since the typical effect of this is to make the prior much more informative, there is some danger that fixing this bug will cause other problems due to poorly-constrained fits.  To avoid this, I'll add a configuration option to tune the relative weighting of the prior via a constant (which we could set to the typical variance level of the images to get behavior like what we have now without the inconsistency).",2
"DM-9797","03/14/2017 01:59:19","Investigate HSC-Y background configs","Default background configs fail to remove a rotating ellipse-shaped ring of scattered light in the  y-band HSC data:         !https://paper.dropbox.com/ep/redirect/image?url=https%3A%2F%2Fd2mxuefqeaa7sj.cloudfront.net%2Fs_CE3712A3B91055C667598BDAAFD85C792A8BC38908F5D92E32C19E3223FF4694_1489481658883_Screen%2BShot%2B2017-03-14%2Bat%2B5.23.22%2BPM.png&hmac=8V%2F3Mr6bDlLZbvSguW8pJDZRR9CzFz1%2Fr85NKoWsCjc%3D&width=600!    This ticket will capture work to identify appropriate background configs for removing. ",2
"DM-9798","03/14/2017 08:09:17","Remove superfluous print message","During the pybind11 port someone (meaning me, probably) left a print statement that print ""STRAY FLUX"". This should be removed.",1
"DM-9802","03/14/2017 10:30:16","Update base docker images for alert stream","The base kafka image in the docker-compose file for the current alert_stream repo is deprecated already.  :(  Need to swap this image out with the current stable version of kafka, go through new release docs to update deprecated environment variables, swap out the zookeeper image, and upgrade the python base image of the main Dockerfile.  We want to do this before testing because the currently used version of kafka and also the python base image lack features/performance we will want later, so we should test the versions we want to use.",2
"DM-9808","03/14/2017 15:59:44","astshim does not build on Ubuntu","The stack cannot be built on Ubuntu because the build of {{astshim}} complains about a missing symbol:  bq. lsstsw2/stack/Linux64/starlink_ast/lsst-dev-g4bd2b55bd7/lib/libast_pass2.so: undefined reference to 'astPutErr_'",1
"DM-9810","03/14/2017 17:11:05","Make PSFEx oversampling configurable","Add a configuration option that lets PSFEx decide when to oversample PSF images.    ",1
"DM-9811","03/14/2017 20:06:23","Add 1.3 arcsec target seeing for convolved flux measurement","Masayuki Tanaka asks that we also include a 1.3 arcsec target seeing in the {{ConvolvedFluxPlugin}}.  This target seeing is larger than any expected seeing in the coadd, and so we'll always get a useful result.",1
"DM-9812","03/14/2017 22:41:03","Clean up outputs from CharacterizeImageTask and CalibrateTask","We're writing the {{icExp}} files in {{CharacterizeImageTask}}, which we don't use, but greatly increases our disk usage.     We want the denormalized match catalogs from {{CalibrateTask}} and from multiband processing.",2
"DM-9816","03/15/2017 10:54:19","Evaluate Plotly.js as a replacement to Highcharts","Plotly is an open source plotting package that is more oriented toward science than Highcharts.    Study the following:  * Can we control tooltips?  * What control do we have during selection?  * In general, can monitor or have hooks into their events?  * How hard to port histograms, scatter and density?  * How hard to create a heat map to replace density?  * Do they support optimized updates and not complete redraws?    Overall we want a sense if we should go to the next step of replacing Highcharts.  We need to understand how much efforts would be involved.",8
"DM-9817","03/15/2017 11:30:58","Doxygen tries to parse pybind11 wrappers","Doxygen currently reads {{.cc}} files in the {{python/}} directory, leading to silly documentation like https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/namespacelsst_1_1afw_1_1camera_geom.html#a2a868ed61ae08543a5cc793c35f5674e.    I propose that any C++ files in {{python/}} be excluded using an {{EXCLUDE}} or {{EXCLUDE_PATTERNS}} configuration. I'm not sure whether this only needs to be changed in {{base}} or in all packages.",2
"DM-9818","03/15/2017 11:41:17","Design dochub-prototype (www.lsst.io builder)","Write up a lightweight specification+design for the {{dochub-prototype}} project (the build infrastructure for www.lsst.io) to help [~athornton] make the initial implementation.    {{dochub-prototype}} is itself a prototype for what will become DocHub (https://sqr-013.lsst.io).",1
"DM-9824","03/15/2017 14:04:01","Fail to upload additional catalog after initial upload","Select Catalogs -> Load Catalog  Upload the sample catalog file below.  Click Search.  Everything works as intended.    Now, try to upload another catalog.  Clicking Search does nothing.  No error indications.  You'll find uncaught errors in the console.    sample catalog file:  {noformat}  | ra        | dec       |  | double    | double    |    66.76957    26.10453   194.25975    22.03131    {noformat}  ",0
"DM-9826","03/15/2017 17:07:31","Add hasTranForward and hasTranInverse to Transform","Like the {{Mappings}} they adapt, a {{Transform}} may or may not have a forward transformation, and may or may not have an inverse transformation. Test methods should be added to {{Transform}} to let clients defend against calling a missing transformation.",2
"DM-9828","03/16/2017 00:25:39","Enable rectangular binSizes in SubtractBackgroundTask","While validating on HSC data, we encountered a use case for rectangular bin sizes in {{SubtractBackgroundTask}}.  We propose two new config fields:    * binSizeX  * binSizeY    By default they will be None and populated with the value in binSize. This change will be 100% backwards compatible. Everyone's config files will produce the same results.   ",2
"DM-9846","03/16/2017 13:41:50","Improve handling of error messages","astshim presently prints AST error messages to stderr. Improve this so that the messages and up as the text of exceptions, instead.    This work requires the new AST capability of registering an error handler which has just been released.",1
"DM-9848","03/16/2017 14:20:30","obs_subaru test failures possibly related to daf_persistence","Both [~wmwood-vasey] and myself have seen {{obs_subaru}} test failures today.  In my case, the test failure was non-reproducible.  I am concerned that this might be a concurrent butler access issue.    https://ci.lsst.codes/job/release/job/run-rebuild/472/consoleFull#console-section-13    {code:java}  [2017-03-16T18:08:21.499615Z] Traceback (most recent call last):  [2017-03-16T18:08:21.499681Z] Traceback (most recent call last):  [2017-03-16T18:08:21.499703Z]   File ""bin.src/genDefectFits.py"", line 86, in <module>  [2017-03-16T18:08:21.499727Z]   File ""bin.src/genDefectFits.py"", line 86, in <module>  [2017-03-16T18:08:21.499748Z]     genDefectFits(args.cameraName, args.defectsFile, args.targetDir)  [2017-03-16T18:08:21.499771Z]     genDefectFits(args.cameraName, args.defectsFile, args.targetDir)  [2017-03-16T18:08:21.499789Z]   File ""bin.src/genDefectFits.py"", line 20, in genDefectFits  [2017-03-16T18:08:21.499808Z]   File ""bin.src/genDefectFits.py"", line 20, in genDefectFits  [2017-03-16T18:08:21.499825Z]     mapper = mapperMap[cameraName.lower()](root=""."")  [2017-03-16T18:08:21.499844Z]     mapper = mapperMap[cameraName.lower()](root=""."")  [2017-03-16T18:08:21.499875Z]   File ""/home/lsstsw/jenkins/release/lsstsw/build/obs_subaru/python/lsst/obs/hsc/hscMapper.py"", line 31, in __init__  [2017-03-16T18:08:21.499912Z]   File ""/home/lsstsw/jenkins/release/lsstsw/build/obs_subaru/python/lsst/obs/hsc/hscMapper.py"", line 31, in __init__  [2017-03-16T18:08:21.499936Z]     super(HscMapper, self).__init__(policy, policyFile.getRepositoryPath(), **kwargs)  [2017-03-16T18:08:21.499974Z]       File ""/home/lsstsw/jenkins/release/lsstsw/stack/Linux64/obs_base/13.0-5-g36cc3d4+1/python/lsst/obs/base/cameraMapper.py"", line 223, in __init__  [2017-03-16T18:08:21.500009Z] super(HscMapper, self).__init__(policy, policyFile.getRepositoryPath(), **kwargs)  [2017-03-16T18:08:21.500050Z]   File ""/home/lsstsw/jenkins/release/lsstsw/stack/Linux64/obs_base/13.0-5-g36cc3d4+1/python/lsst/obs/base/cameraMapper.py"", line 223, in __init__  [2017-03-16T18:08:21.500392Z]     calibStorage = dafPersist.Storage.makeFromURI(uri=calibRoot)  [2017-03-16T18:08:21.500448Z]   File ""/home/lsstsw/jenkins/release/lsstsw/stack/Linux64/daf_persistence/13.0-4-g886c4ba/python/lsst/daf/persistence/storage/storageContinued.py"", line 144, in makeFromURI  [2017-03-16T18:08:21.500470Z]     calibStorage = dafPersist.Storage.makeFromURI(uri=calibRoot)  [2017-03-16T18:08:21.500516Z]   File ""/home/lsstsw/jenkins/release/lsstsw/stack/Linux64/daf_persistence/13.0-4-g886c4ba/python/lsst/daf/persistence/storage/storageContinued.py"", line 144, in makeFromURI  [2017-03-16T18:08:21.500969Z] CameraMapper INFO: Unable to locate registry registry in root: registry.sqlite3  [2017-03-16T18:08:21.501428Z] CameraMapper INFO: Unable to locate registry registry in current dir: ./registry.sqlite3  [2017-03-16T18:08:21.501771Z] CameraMapper INFO: Loading Posix registry from .  [2017-03-16T18:08:21.502249Z] CameraMapper INFO: Unable to locate calibRegistry registry in root: calibRegistry.sqlite3  [2017-03-16T18:08:21.502676Z] CameraMapper INFO: Unable to locate calibRegistry registry in current dir: ./calibRegistry.sqlite3  [2017-03-16T18:08:21.503010Z] CameraMapper INFO: Loading Posix registry from ./CALIB  [2017-03-16T18:08:21.506297Z] CameraMapper INFO: Unable to locate registry registry in root: registry.sqlite3  [2017-03-16T18:08:21.507001Z] CameraMapper INFO: Unable to locate registry registry in current dir: ./registry.sqlite3  [2017-03-16T18:08:21.507448Z] CameraMapper INFO: Loading Posix registry from .  [2017-03-16T18:08:21.507975Z] CameraMapper INFO: Unable to locate calibRegistry registry in root: calibRegistry.sqlite3  [2017-03-16T18:08:21.508473Z] CameraMapper INFO: Unable to locate calibRegistry registry in current dir: ./calibRegistry.sqlite3  [2017-03-16T18:08:21.508875Z] CameraMapper INFO: Loading Posix registry from ./CALIB  [2017-03-16T18:08:21.513985Z]     ret = theClass(uri=uri)  [2017-03-16T18:08:21.514008Z]     ret = theClass(uri=uri)  [2017-03-16T18:08:21.514060Z]   File ""/home/lsstsw/jenkins/release/lsstsw/stack/Linux64/daf_persistence/13.0-4-g886c4ba/python/lsst/daf/persistence/posixStorage.py"", line 55, in __init__  [2017-03-16T18:08:21.514114Z]   File ""/home/lsstsw/jenkins/release/lsstsw/stack/Linux64/daf_persistence/13.0-4-g886c4ba/python/lsst/daf/persistence/posixStorage.py"", line 55, in __init__  [2017-03-16T18:08:21.520395Z]     os.makedirs(self.root)  [2017-03-16T18:08:21.520437Z]   File ""/home/lsstsw/jenkins/release/lsstsw/miniconda/lib/python2.7/os.py"", line 157, in makedirs  [2017-03-16T18:08:21.520468Z]     os.makedirs(self.root)  [2017-03-16T18:08:21.520512Z]   File ""/home/lsstsw/jenkins/release/lsstsw/miniconda/lib/python2.7/os.py"", line 157, in makedirs  [2017-03-16T18:08:21.536157Z]     mkdir(name, mode)  [2017-03-16T18:08:21.536186Z]     mkdir(name, mode)  [2017-03-16T18:08:21.536237Z] OSError: [Errno 17] File exists: './CALIB'  [2017-03-16T18:08:21.536264Z] OSError: [Errno 17] File exists: './CALIB'  [2017-03-16T18:08:21.619152Z] scons: *** [hsc/defects/2014-06-01/defects.dat-fits] Error 1  [2017-03-16T18:08:21.623222Z] scons: *** [hsc/defects/2013-01-31/defects.dat-fits] Error 1  {code}  ",1
"DM-9851","03/16/2017 15:59:46","Improve DCR code use of the butler","The current DCR template generation code requires the user to supply the name of the telescope, and has telescope-specific code that interfaces with the butler. This code should be re-written to be less fragile and to remove the lines that depend on the telescope.",2
"DM-9857","03/17/2017 08:53:35","Research Support of Evaluation of Plotly as a replacement to Highchartts","Plotly is an open source plotting package that is more oriented toward science than Highcharts. Provide a second set of eyes in evaluation of Plotly.js and support DM-9816.    Study some of the following:  Can we control tooltips?  What control do we have during selection?  In general, can monitor or have hooks into their events?  How hard to port histograms, scatter and density?  Do they support optimized updates and not complete redraws?  How  would a Plotly react component work?  Overall we want a sense if we should go to the next step of replacing Highcharts. We need to understand how much efforts would be involved.",8
"DM-9862","03/17/2017 13:18:15","Update meas_mosaic's wcs/fcr output files to reflect LSST coordinate system","When importing {{meas_mosaic}}, the coordinate system for writing out the wcs/fcr files was not adapted to that expected from LSST (which always associates the detector origin with the electronics, whereas HSC's is such that a given detector's origin, pixel (0, 0) is associated with its LLC w.r.t. to the focal plane), but rather the tasks in {{meas_mosaic}}'s *updateExposure.py* were adapted to account for the rotated CCDs.  It was assumed that this was the only place those corrections were every used.  This turns out not to be the case since the wcs used to create the coadd gets attached to it in *coaddInputs*.  If {{meas_mosaic}} was run and *doApplyUberCal=True* (which are both the case for our HSC data processing), the wcs’s that are getting attached to the coaddInputs are not in the coordinate system appropriate for LSST for any nQuarter != 0 ccds.    Since this is already causing an issue in {{pipe_tasks}}'s *propogateVisitFlags.py*, see issue highlighted in DM-9383, (and could very well be causing issues elsewhere as yet undiscovered), we have decided these outputs need to be written out in the coordinate system expected by LSST.",3
"DM-9863","03/17/2017 13:34:45","Replace use of makeVisitInfo(... with VisitInfo(...)","{{lsst.afw.image.makeVisitInfo(...)}} is superseded by {{lsst.afw.image.VisitInfo(...)}}. Remove existing usage of {{makeVisitInfo}}.    {{makeVisitInfo}} is used in one or two places in most obs_ packages and a few other packages as well.    Note that this can be done on a package by package basis as time permits. Once all usage is gone we can get rid of {{makeVisitInfo}}.",0.5
"DM-9866","03/17/2017 15:49:42","Make change to remove flagDecorator (RFC-302)","This is a follow up to the FlagHandler modification DM-9249.  Posted this last deletion to flagDecorator.py in case anyone unknown was using it.",1
"DM-9871","03/20/2017 07:43:03","Move wcs and fcr datasets out of {pointing} directory in obs_subaru","We should change the templates for {{fcr}} and {{wcs}} to the following:  {code}  jointcal-results/%(tract)d/wcs-%(visit)07d-%(ccd)03d.fits  jointcal-results/%(tract)d/fcr-%(visit)07d-%(ccd)03d.fits  {code}    This will avoid problems in copying/updating directories at NAOJ during incremental releases.  ",0.5
"DM-9873","03/20/2017 09:16:42","PropertySet does not support values of None","In DM-5466, we needed to pass the results from {{ParseTask.getInfo()}} to the butler as a dataId. This is normally valid, since both are dictionaries, and even though {{getInfo()}} often contains extraneous entries that aren't relevant, the butler will usually ignore them. However, when parsing calibration files this dictionary contains some values that are set to {{None}}, since they will be filled in later. These extraneous keys are then placed in {{ButlerLocation.additionalData}} ([butlerLocation.py:221|https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/butlerLocation.py#L221]), which throws an exception as it is a PropertySet and does not support python {{None}} as a value.    DM-5466 has a work-around that strips these None values from the dictionary, but this is inelegant. The main driver for excluding None from PropertySet seems to compatibility with FITS headers. This seems like an unwarranted mixing of data model and persistence formats. Unless there is some advantage to not being able to store None in our dictionary-like objects, it seems preferable to shift the burden of accommodating FITS's peculiarities onto the persistence layer rather than PropertySet.   ",3
"DM-9882","03/20/2017 17:17:59","Add integrate interface to BoundedField/ChebyshevBoundedField","In order to implement the ""mean zero point"" functionality of PhotoCalib, I need to compute the integral over the bounding box. {{BoundedField}} does not include facilities for integration/differentiation. Fortunately, my primary usage is {{ChebyshevBoundedField}}, which has a relatively straight-forward recurrence relation for integration.    I suggest an interface like the following on {{BoundedField}}, with a virtual ""not implemented"" default:    {code}  // Compute the integral of this function over its bounding-box.  double integrate() const;  {code}",8
"DM-9885","03/21/2017 03:39:40","Rename deepCoadd_srcMatch as deepCoadd_measMatch","Implement RFC-306.",1
"DM-9893","03/21/2017 11:40:32","Write SQuaRE User Stories for SuperTask","Write a set of user stories and consequent SuperTask design implications that address SQuaRE needs.",1
"DM-9894","03/21/2017 12:09:39","SQuaRE SuperTask Collaboration Week of 2017-03-20","Bucket ticket to cover non-specific SuperTask collaboration activities (see linked meeting notes).",1
"DM-9895","03/21/2017 12:17:01","FrameSet frames not preserved by Transform(frameSet) constructor","When an {{lsst.afw.geom.Transform}} is constructed from an {{astshim.FrameSet}} the internal frames in the frame set are lost. I strongly suspect what is happening is that the mapping constructor of {{Transform}} is being called, instead of the {{FrameSet}} constructor. Reversing the order in the pybind11 wrapper file should fix the problem. A unit test is required.",2
"DM-9897","03/21/2017 13:39:39","conda channel errors causing lsstsw/bin/deploy to fail","We see errors installing both osx and linux packages like the following:    {code:java}     CondaRuntimeError: Runtime error: RuntimeError: Runtime error: Could not open u'/Users/square/jenkins/workspace/release/tarball/DarwinX86/miniconda2/4.2.12.lsst1/pkgs/mkl-11.3.3-0.tar.bz2.part' for writing (HTTPSConnectionPool(host='repo.continuum.io', port=443): Read timed out.).  {code}    I believe the best resolution is to mirror the public conda channels internally.  Another, less desirable, option would be build retrying into the conda command.",3
"DM-9900","03/21/2017 16:08:00","Recreate DCR simulation images for testing","The existing simulation data used for testing the DCR template building code is missing some important metadata. Those simulations should be brought up to date, and re-created with all the required metadata.",2
"DM-9916","03/22/2017 15:06:03","Re-write internal DCR template wcs-matching","In the DCR template generation code, when exposures are warped that operation is currently performed in-place, over-writing the array values and wcs info. This could potentially lead to bugs down the road, so either the implementation should be made more robust or checks should be put in place to verify that it is safe.",3
"DM-9917","03/22/2017 18:49:40","Add a callback to cameraGeom.showCamera","It is sometimes very convenient to modify the data that {{cameraGeom.utils.showCamera}} displays;  the classic case if trimming, bias subtracting, or gain-correcting raw data.      Please port old RHL HSC code that registers a callback to do the work (much better than adding more and more special-case code to {{showCamera}})  ",1
"DM-9920","03/23/2017 11:20:59","The catalog search in IRSAViewer does not work for multi-object search ","While working on LSST multi-object search, I found that the multi-object search in IRSA viewer does not work.      After spending hours to look into the reason, I found that the uploaded table in CatalogSelectViewPanel.jsx is not proper defined.   {code}    if (spatial === SpatialMethod.get('Multi-Object').value) {          var filename = get(catPart, 'fileUpload');  {code}  The correct way should be:  {code}  if (spatial === SpatialMethod.get('Multi-Object').value) {          var filename = get(spacPart, 'fileUpload');  {code}  ",2
"DM-9921","03/23/2017 11:24:41","Uploader for dochub-prototype HTML to LSST the Docs","Publish dochub-prototype HTML to {{www.lsst.io}} via LSST the Docs.",2
"DM-9925","03/23/2017 17:49:26","PolyTran should not provide an iterative inverse by default","{{PolyTran}}'s constructor that takes only forward coefficients provides an iterative inverse by default (as does its other constructor if the forward coefficient array is empty). This seems like a bad idea for several reasons:  - Most polynomials do not have unique inverses. That said, the iterative inverse will do sensible things in that situation (nan if no value exists, else a valid value if multiple choices exist).  - The iterative inverse is much less efficient to  compute than the fit inverse provided by `PolyMap.polyTran`.    I propose to disable the iterative inverse by default, using the philosophy of ""when in doubt, refuse to guess"".",1
"DM-9931","03/24/2017 10:27:04","Add dependency of ""refcat"" on ""preSfm"" in ci_hsc","{{ci_hsc}} fails in my lsst-dev/lsstvc/slurm env (using 1 node) although it passes on Jenkins.    The problem arose when {{preSfm}} ran before  {{refcat}}.     I think this is because {{preSfm}}, effectively {{processCcd.py DATA --rerun ci_hsc --doraise}}, needs the {{ref_cats}} link already in the repo. Running the command in a repo without the {{ref_cats}} gives this error:     {code:java}  root INFO: Running: /home/hchiang2/lsstsw/stack/Linux64/pipe_tasks/13.0-8-g148fcaf+1/bin/processCcd.py /home/hchiang2/ciHscTest/asMaster/ci_hsc/DATA --rerun test2 --doraise  Traceback (most recent call last):    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_tasks/13.0-8-g148fcaf+1/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_base/13.0-1-g92030ba+10/python/lsst/pipe/base/cmdLineTask.py"", line 482, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_base/13.0-1-g92030ba+10/python/lsst/pipe/base/cmdLineTask.py"", line 202, in run      if self.precall(parsedCmd):    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_base/13.0-1-g92030ba+10/python/lsst/pipe/base/cmdLineTask.py"", line 300, in precall      task = self.makeTask(parsedCmd=parsedCmd)    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_base/13.0-1-g92030ba+10/python/lsst/pipe/base/cmdLineTask.py"", line 391, in makeTask      return self.TaskClass(config=self.config, log=self.log, butler=butler)    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_tasks/13.0-8-g148fcaf+1/python/lsst/pipe/tasks/processCcd.py"", line 160, in __init__      astromRefObjLoader=astromRefObjLoader, photoRefObjLoader=photoRefObjLoader)    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_base/13.0-1-g92030ba+10/python/lsst/pipe/base/task.py"", line 237, in makeSubtask      subtask = taskField.apply(name=name, parentTask=self, **keyArgs)    File ""/home/hchiang2/lsstsw/stack/Linux64/pex_config/13.0-1-g41367f3+1/python/lsst/pex/config/configurableField.py"", line 83, in apply      return self.target(*args, config=self.value, **kw)    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_tasks/13.0-8-g148fcaf+1/python/lsst/pipe/tasks/calibrate.py"", line 322, in __init__      self.makeSubtask('astromRefObjLoader', butler=butler)    File ""/home/hchiang2/lsstsw/stack/Linux64/pipe_base/13.0-1-g92030ba+10/python/lsst/pipe/base/task.py"", line 237, in makeSubtask      subtask = taskField.apply(name=name, parentTask=self, **keyArgs)    File ""/home/hchiang2/lsstsw/stack/Linux64/pex_config/13.0-1-g41367f3+1/python/lsst/pex/config/configurableField.py"", line 83, in apply      return self.target(*args, config=self.value, **kw)    File ""/home/hchiang2/lsstsw/stack/Linux64/meas_algorithms/13.0-6-g6f8a914+2/python/lsst/meas/algorithms/loadIndexedReferenceObjects.py"", line 50, in __init__      dataset_config = butler.get(""ref_cat_config"", name=self.config.ref_dataset_name, immediate=True)    File ""/home/hchiang2/lsstsw/stack/Linux64/daf_persistence/13.0-5-g87674b4/python/lsst/daf/persistence/butler.py"", line 945, in get      raise NoResults(""No locations for get:"", datasetType, dataId)  lsst.daf.persistence.butlerExceptions.NoResults: No locations for get: datasetType:ref_cat_config dataId:DataId(initialdata={'name': 'ps1_pv3_3pi_20170110'}, tag=set([]))    {code}  ",1
"DM-9937","03/24/2017 12:37:15","Add noexcept specifiers to applicable methods in afw","C++11 methods that are declared {{noexcept}} enable compilers to streamline code that calls them, in some cases (e.g., STL code) unlocking more efficient algorithms that would be unsafe with throwing methods. These changes are most useful for low-level types, whose methods are also the most likely to be provably non-throwing.    This ticket shall add the {{noexcept}} specifier to any methods that are guaranteed not to throw and are unlikely to be modified to throw exceptions in the future (e.g., trivial getters will usually qualify, but methods for which some inputs are invalid will not even if the existing code performs no input validation). Particular attention shall be paid to constructors, assignment operators, and {{std::swap}} implementations, as these are the methods where {{noexcept}} provides the biggest gains.    The classes covered by this ticket are (based on assumed simplicity, and therefore subject to change):  * {{afw::cameraGeom::CameraPoint}}  * {{afw::cameraGeom::CameraSys}}  * {{afw::cameraGeom::CameraSysPrefix}}  * {{afw::cameraGeom::Orientation}}  * {{afw::coord::Coord}} and its subclasses  * {{afw::coord::Observatory}}  * {{afw::coord::Weather}}  * {{afw::detection::Threshold}}  * {{afw::geom::Angle}}  * {{afw::geom::AngleUnit}}  * {{afw::geom::Box}}  * {{afw::geom::CoordinateBase}} and its subclasses  * {{afw::geom::CoordinateExpr}}  * {{afw::geom::Span}}  * {{afw::geom::SpherePoint}}  * {{afw::geom::polygon::Polygon}}  * {{afw::image::Calib}}  * {{afw::image::Color}}  * {{afw::image::DefectBase}}  * {{afw::image::Filter}}  * {{afw::image::FilterProperty}}  * {{afw::math::FitResults}}  * {{afw::math::Function}} and its subclasses  * {{afw::math::MaskedVector}}  * {{afw::math::Statistics}}  * {{afw::table::BaseRecord}} and its subclasses  * {{afw::table::ConstFunctorKey}}  * {{afw::table::FieldBase}} and its subclasses  * {{afw::table::InputFunctorKey}} and its subclasses  * {{afw::table::KeyBase}} and its subclasses  * {{afw::table::Match}}  * {{afw::table::OutputFunctorKey}} and its subclasses  * {{afw::table::ReferenceFunctorKey}}  * {{afw::table::SchemaItem}}  * {{afw::table::io::Persistable}}  * {{afw::table::io::PersistableFacade}}",3
"DM-9940","03/24/2017 14:11:06","Simplify package dependencies of template generation","The DCR matched template code has dependencies on sims packages that might be replaced with functionality from core software. Once DM-9615 is completed, some of these (such as the bandpass) may be obtained from the relevant obs package.",3
"DM-9942","03/24/2017 15:09:43","Plotly Change: Create Plotly loading infrastructure and React wrapper ","Setup plotly environment:    * Set up how we are going the load plotly using a deferred load  * Create a plotly react wrapper  * set up Firefly environment to  contain a property to switch between plotting packages",3
"DM-9946","03/24/2017 16:45:49","Remove debugging example refcount.cc","The file {{examples/refcount.cc}} was intended for debugging, not distribution. Remove it.",0
"DM-9948","03/24/2017 16:52:50","LSST catalog search processor","make sure the current catalog search processor handles WISE catalog data search properly",8
"DM-9949","03/24/2017 16:56:00","Update the PDAC sample queries and test cases page","Update the PDAC sample queries and test cases page [https://confluence.lsstcorp.org/display/DM/PDAC+sample+queries+and+test+cases] to include WISE data",2
"DM-9954","03/27/2017 09:36:39","Optimize PSF convolution to use sparse matrices","Currently the PSF convolution operator is a collection of 2D sparse arrays, which are different for each band, that converted are into full arrays in order to use [~pmelchior]'s proximal operator algorithm. Since the current simulated data is PSF matched, the processing time can be significantly reduced (by several orders of magnitude) by using the PSF operator as a sparse matrix in each band.    This fix is temporary so that we can profile and test PSF convolution and other features of the new deblender but  it is expected that this section of the code will be re-written in C++ in the final deblender before it is merged into the stack in a future sprint.",2
"DM-9955","03/27/2017 09:53:50","Investigate building stack on CentOS7 with devtoolset-6","RFC-303 approved the principle that we can mandate a CentOS7 baseline operating system that requires a devtoolset to build the stack. This ticket is to demonstrate that a stack can build and run using devtoolset-6 on CentOS7 without running in to problems. A successful outcome should result in an RFC requesting a new compiler baseline.",0.5
"DM-9958","03/27/2017 10:43:26","SQuaRE SuperTask Collaboration Week of 2017-03-13","Bucket ticket to cover non-specific SuperTask collaboration activities (see linked meeting notes).",1
"DM-9959","03/27/2017 11:04:30","SQuaRE SuperTask Collaboration Week of 2017-03-27","Bucket ticket to cover non-specific SuperTask collaboration activities.",1
"DM-9961","03/27/2017 11:25:16","Add treecorr to validate_drp and lsst_ci dependencies","Create a package for {{treecorr}}  to allow for calculation of correlation functions in a fast, robust, and tested code.    This will not add dependencies to {{lsst_apps}} or {{lsst_distrib}}.     Implementation is likely  1. Create wrapper install packages for {{treecorr}}.  2. Figure out how to avoid adding the FITS reader dependency in {{treecorr}}.  3. Add treecorr dependence to {{validate_drp}} table file.    TreeCorr is a  ""[c]ode for efficiently computing 2-point and 3-point correlation functions"" and is being proposed for use in {{validate_drp}} to calculate the correlation function of the PSF residual ellipticity (DM-8951).  The GitHub repo is here:    https://github.com/rmjarvis/TreeCorr    TreeCorr depends on the following Python packages.    numpy  future  fitsio  pandas  pyyaml  cffi  ",3
"DM-9962","03/27/2017 11:28:59","Write proposal for storing coadd HeavyFootprints","Deblending results in the form of HeavyFootprints should be available to users, which means they must be stored.    This proposal should include an estimate of how much storage this will require for a given area of sky, which can probably be derived from the average HeavyFootprint size in HSC data.  This will be a function of depth, so it will not be the same at the beginning of the survey as it will be at the end (but we should have HSC data that spans those depths).    This will be a conservative estimate - it is possible that deblender outputs may take a form that allows them to be highly compressed and quickly reconstituted given coadd data.    We will not consider the possibility that it will be necessary to store per-epoch HeavyFootprints for ForcedSources.",2
"DM-9964","03/27/2017 12:14:16","Change LSSTCatalogSearch 's behavior to the same as IRSA's catalog search","When the search result in IRSA catalog search is empty, the IRSA catalog search shows the table with empty row data.  The LSSTCatalogSearch returns the message with ""no data found"".   I think that the behaviors in two catalog searches should be consistent.       Since the meta server is not available, the data definitions are obtained from the search result.  When there no data is available, there is no data type definition either.  The current implementation threw an exception when the empty table is returned from the PDA search.  But for multi-object search, the exception will stop further searches for the other object.  For now,  it is modified as below:  # Let getDataFromUR return null if the result is empty  #  When the PDA search returns empty, the exception in LSSTCatalogSearch is deferred at the loadFile  # In LSSTMutliObjectSearch, when one of the callable returns empty result, it will be skipped. Thus, all targets in the input file will be searched.    NOTE:  When the meta server is working, the above implementation will be changed accordingly.   So is the meta definition in LSSTQuery.             ",1
"DM-9965","03/27/2017 12:39:33","Fix bug in monotonicity operator","There appears to be a bug in the monotonicity operator that causes some edge pixels to wrap monotonicity around the edge to the next (or previous) line. The cause of this needs to be found and fixed.",1
"DM-9966","03/27/2017 12:49:55","Recommend that TODO comments link to Jira","Following discussion on RFC-307, this ticket adds the following rule to the Python style guide:    h3. To-do comments SHOULD include a Jira issue key    If the commented code is a workaround for a known issue, this rule makes it easier to find and remove the workaround once the issue has been resolved. If the commented code itself is the problem, this rule ensures the issue will be reported on Jira, making it more likely to be fixed in a timely manner.    {code}  # TODO: DM-12345 is triggered by this line  {code}    {code}  # TODO: workaround for DM-6789  {code}    ----    and the following rule to the C++ style guide:    h4. 6-26. To-do comments SHOULD include a Jira issue key    If the commented code is a workaround for a known issue, this rule makes it easier to find and remove the workaround once the issue has been resolved. If the commented code itself is the problem, this rule ensures the issue will be reported on Jira, making it more likely to be fixed in a timely manner.    {code}  // TODO: DM-12345 is triggered by this line  {code}    {code}  // TODO: workaround for DM-6789  {code}",1
"DM-9967","03/27/2017 13:16:05","ctrl_pool should not accept a default for --time on real batch systems","ctrl_pool's {{--time}} is easily confused with the inherited {{-t}} from CmdLineTask, which makes it easy to use the wrong one and get your batch jobs killed too early.  We should always be forced to provide a time estimate when submitting to a real batch system.",1
"DM-9968","03/27/2017 14:25:25","Add eupspkg build for igprof","The profiler we recommend using in the developer guide isn't provided as part of the stack, and isn't entirely trivial to build on most operating systems (though it's not hard).    This ticket will provide eupspkg builds for igprof and its dependencies.  Hopefully that will make it easy to also make it eups-distrib-installable in the near future, though it shouldn't ever be a dependency of any stack packages (or metapackages).  ",1
"DM-9970","03/27/2017 14:56:10","Fix Composite Policy Docs","in this block:  {quote}  2.16.9   Composite Policy  To indicate that a dataset should be serialized/deserialized from components, the policy’s dataset definition has a keyword composite. The structure is:  {code}  <dataset type name>: {      composite: {          <component name>: {              datasetType: <dataset type>              setter: <method name of setter>              getter: <method name of getter>              assembler: <importable function to do custom deserialization>              disassembler: <importable function to do custom serialization>              subset: bool              inputOnly: bool          }          ...      }  {code}  {quote}  assembler & disassembler are in the wrong spot, the need to be below ‘composite’ not ‘componentName’",1
"DM-9975","03/27/2017 17:46:20","Port LDM-151 to new latex style","LDM-151 is currently using a default Latex style. It needs to be ported to use the standard look and feel {{lsstdoc.cls}}.",0.5
"DM-9989","03/28/2017 18:46:17","Incorporate ZOGY and A&L decorrelation option into imageDifference.py","Incorporate ZOGY and A&L decorrelation option into imageDifference.py. This is specifically regarding the spatially-varying variants of each, which rely upon the ImageMapReduce tasks. This epic includes fixing various bugs discovered after incorporation into imageDifference.py and running them on real images.",40
"DM-9991","03/29/2017 06:50:26","Provide functional obs_ctio0m9","Building on the work done by [~aguyonnet] in DM-9363, provide an obs_ package that can be used with data from the 0.9m at CTIO.",2
"DM-9992","03/29/2017 06:54:29","Provide CTIO 0.9m data in NCSA","Please transfer appropriate CTIO 0.9m datasets to NCSA in order to facilitate the construction and test of obs_ctio0m9.",1
"DM-9993","03/29/2017 06:59:58","Prepare presentation on crowded field processing","Audience is the PST-SC Chairs telecon scheduled for April 18th.    Should be able to mostly reuse slides from last P&CW, with some updates from LDM-151.",2
"DM-9999","03/29/2017 09:48:45","Review literature on CModel","Where ""literature"" likely means the relevant section of the HSC pipeline paper, provided by [~jbosch].",2
"DM-10008","03/29/2017 15:04:38","MapBox.maxOutCoord not set to nout if specified as 0 during construction","MapBox claims that maxOutCoord will be set to nout if specified as 0 in the constructor, but that does not happen. Fix it and add a unit test.    Also consider making some or all fields const. (Making the computed vectors const is almost certainly more work than it is worth).",0.5
"DM-10009","03/29/2017 16:18:25","Ensure masks are valid from ImageMapReduceTask","Ensure that the reducer subtask correctly sets the mask to invalid for pixels where the reduced exposure is infinite or NaN.",2
"DM-10023","03/30/2017 13:45:07","Build prototype alert filtering system","This is to do the research and coding necessary to produce a prototype alert filtering mechanism (simple broker) for the LSST.",40
"DM-10035","03/30/2017 16:52:33","Rotate class needs some minor refactoring","While working on DM-8854, the unit test for Rotate class, I noticed that it needs some minor refactoring.    After investigating how this class is used, I found that the class just needs some document.  I added the algorithm and   brief description what it is doing.      This ticket can be closed.  ",1
"DM-10039","03/31/2017 08:19:37","Incorrect docs for CatalogCalculationConfig","meas_base's [{{CatalogCalculationConfig}}|https://github.com/lsst/meas_base/blob/0b6c81b6fe40f3d0e39237a9a83394b219cce41a/python/lsst/meas/base/catalogCalculation.py#L104] claims that it is:    {quote}  Default CatalogCalculationConfig. Currently this is an empty list, meaning that there are no default plugins run. The config object for each plugin must use this variable to specify the names of all plugins to be run  {quote}    This is wrong in a number of ways — it's not a list, it's not empty, and it doesn't make sense for each plugin to have to specify the names of all other plugins in use.    Please fix it.",1
"DM-10042","03/31/2017 09:00:06","Update mariadb statistics on 35TB dataset","Join query has a very strange query execution plan on 35TB dataset.  Statistics have to be updated in order to fix that.",5
"DM-10046","03/31/2017 11:10:48","Remove deprecated upload URL from firefly_client","In spring 2017, a change was made to the URL for uploading data to a Firefly server. The Python API handles this change in DM-9843 by checking the old and the new URL. After some months, the old URL can be considered deprecated and can be removed from the upload functions in {{firefly_client}}.",2
"DM-10047","03/31/2017 11:30:01","Add LSST refs to LDM-151","LDM-151 doesn't cite LSST documents so they don't appear in references. They need to be added.",0.5
"DM-10051","03/31/2017 12:37:51","Add core documents to shared bibliography","The lsstdoc.cls file defines standard macros for referring to core documents but not all of these are found in the shared bibliography in lsst-texmf. Add these missing documents. Also redefine the {{\ds}} macro to use a citation rather than a direct docushare link.",0.5
"DM-10066","03/31/2017 16:15:26","Provide utility function for wrapping operator<<","We frequently want to implement a class's {{\_\_str\_\_}} or {{\_\_repr\_\_}} methods in terms of its {{operator<<}}. The current approach has led to a fair amount of code duplication throughout the stack.    Once a function template that takes a printable object and returns a string is available, explicit wrappers for {{operator<<}} should be rewritten to use it.",3
"DM-10069","03/31/2017 18:18:31","Remove boost_thread Qserv dependency","The sole remaining boost_thread dependency in Qserv seems to be some thread-local storage used by the mysql qserv module.  Since the baseline minimal compiler is gcc 4.8, we can use C++11 thread_local instead.",0.5
"DM-10089","04/03/2017 16:36:38","Simplify validate_base Blob into a concrete collection type","validate_base's BlobBase type is currently intended to be an abstract base class that apps create subclasses for. The original intention was that blob data would be computed within the Blob subclass's code.    With the new validate_base we're going towards a composition model with container types. This ticket is to convert BlobBase into a Blob class, and permit applications to store data into the Blob instance through standard mapping idioms.",1
"DM-10091","04/03/2017 20:31:02","Fix problems left over from DM-9952","DM-9952 is merged to master, but I typoed the Jenkins run {{DM-9952}} not {{tickets/DM-9952}} and didn't notice failing tests from afwdata; afwdata is not present on tiger in Princeton where I did my local testing.    This ticket is to capture the errors turned up when I *did* run the tests.    ",1
"DM-10093","04/04/2017 09:54:04","Revert disabling of meas_modelfit dependency in lsst_apps","In DM-8467, the meas_modelfit dependency of lsst_apps was temporarily removed:    {code}  commit 041ae975d9c60a41b24488ec0ba1768431330fd8  Author: Pim Schellart <P.Schellart@princeton.edu>  Date:   Tue Feb 14 15:51:46 2017 -0500        Disable meas_modelfit until pybind11 wrapped    diff --git a/ups/lsst_apps.table b/ups/lsst_apps.table  index dce8fd4..b239e74 100644  --- a/ups/lsst_apps.table  +++ b/ups/lsst_apps.table  @@ -1,5 +1,6 @@   setupRequired(meas_deblender)  -setupRequired(meas_modelfit)  +# temporarily disable meas_modelfit until pybind11 wrapped  +#setupRequired(meas_modelfit)   setupRequired(pipe_tasks)   setupRequired(obs_lsstSim)   setupRequired(obs_sdss)  {code}    meas_modelfit is now pybind wrapped (DM-8465), so this should be reverted.",1
"DM-10094","04/04/2017 10:19:04","Copy CTIO 0.9m data to /datasets/ctio0m9/raw","See RFC-313 and DM-9992  ",1
"DM-10096","04/04/2017 11:48:10","Add unit test asserts for SpherePoint, SpherePointList and PointList","Add unit test asserts to lsst.afw.geom.utils:  - assertSpherePointsAlmosttEqual  - assertSpherePointListsAlmostEqual  - assertPointListsAlmostEqual    and unit tests for them. This work is needed (or at least very useful) for the new WCS classes.",2
"DM-10101","04/04/2017 19:04:56","bad exception handling in afw for python3","Looks like a missed 2to3 compatibility check:      {code}  *** error building product afw.  *** exit code = 2  *** log is in /Users/josh/src/lsstsw3/build/afw/_build.log  *** last few lines:  :::::  [2017-04-05T00:57:00.101207Z]     import lsst.afw.cameraGeom.utils as cameraGeomUtils  :::::  [2017-04-05T00:57:00.101210Z]   File ""/Users/josh/src/lsstsw3/build/afw/python/lsst/afw/cameraGeom/utils.py"", line 397  :::::  [2017-04-05T00:57:00.101214Z]     except Exception, e:  {code}  ",1
"DM-10102","04/05/2017 07:05:15","Reset lsst-dev01:/ssd shared stack","Implement RFC-317 by wiping out the shared stack in {{lsst-dev01:/ssd/lsstsw/stack}} (or, perhaps, moving it aside) and creating a new stack in its place.    Take precautions to make sure that the toolchain used to build the new stack is always consistent.    Consider bumping to newer versions of EUPS, Anaconda etc than come by default in the {{shared-stack.py}} script. Also install AstroPy through {{conda}}.",1
"DM-10103","04/05/2017 07:08:20","Investigate segfaults in lsst-dev01:/ssd/lsstsw/stack shared stack","The {{w_2017_14}} weekly fails to build as part of the shared stack in {{lsst-dev01:/ssd/lsstsw/stack}}. This may be related to segfaults previously reported by [~tmorton].",1
"DM-10105","04/05/2017 07:19:31","Inconsistency in meas/forced wcs leads to CModel failure","From Sogo Mineo:    {code}  During processing a dataset with multiBandDriver.py in hscPipe 5.0-beta1, I ran into an error:    Traceback (most recent call last):    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/ctrl_pool/13.0-1-g0a48345/python/lsst/ctrl/pool/pool.py"", line 113, in wrapper      return func(*args, **kwargs)    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/ctrl_pool/13.0-1-g0a48345/python/lsst/ctrl/pool/pool.py"", line 1067, in run      while not menu[command]():    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/ctrl_pool/13.0-1-g0a48345/python/lsst/ctrl/pool/pool.py"", line 237, in wrapper      return func(*args, **kwargs)    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/ctrl_pool/13.0-1-g0a48345/python/lsst/ctrl/pool/pool.py"", line 1085, in reduce      result = self._processQueue(context, func, [(index, data)], *args, **kwargs)[0]    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/ctrl_pool/13.0-1-g0a48345/python/lsst/ctrl/pool/pool.py"", line 544, in _processQueue      return self._reduceQueue(context, None, func, queue, *args, **kwargs)    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/ctrl_pool/13.0-1-g0a48345/python/lsst/ctrl/pool/pool.py"", line 570, in _reduceQueue      resultList = [func(self._getCache(context, i), data, *args, **kwargs) for i, data in queue]    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/pipe_drivers/13.0-1-g07ace30+4/python/lsst/pipe/drivers/multiBandDriver.py"", line 388, in runForcedPhot      self.forcedPhotCoadd.run(dataRef)    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/meas_base/13.0-3-gc489882+1/python/lsst/meas/base/forcedPhotImage.py"", line 147, in run      self.measurement.run(measCat, exposure, refCat, refWcs, exposureId=self.getExposureId(dataRef))    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/meas_base/13.0-3-gc489882+1/python/lsst/meas/base/forcedMeasurement.py"", line 330, in run      beginOrder=beginOrder, endOrder=endOrder)    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/meas_base/13.0-3-gc489882+1/python/lsst/meas/base/baseMeasurement.py"", line 268, in callMeasure      self.doMeasurement(plugin, measRecord, *args, **kwds)    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/meas_base/13.0-3-gc489882+1/python/lsst/meas/base/baseMeasurement.py"", line 287, in doMeasurement      plugin.measure(measRecord, *args, **kwds)    File ""/data/mineo/opt/hscpipe/5/stack_5.0-beta1/Linux64/meas_modelfit/13.0-3-g177c5a2+1/python/lsst/meas/modelfit/cmodel.py"", line 98, in measure      ""CModel forced measurement currently requires the measurement image to have the same""  FatalAlgorithmError: CModel forced measurement currently requires the measurement image to have the same Wcs as the reference catalog (this is a temporary limitation).  application called MPI_Abort(MPI_COMM_WORLD, 1) - process 10    The exception was thrown from  meas_modelfit/13.0-3-g177c5a2+1/python/lsst/meas/modelfit/cmodel.py:        def measure(self, measRecord, exposure, refRecord, refWcs):          if refWcs != exposure.getWcs(): ### Condition (A) ###              raise lsst.meas.base.FatalAlgorithmError(                  ""CModel forced measurement currently requires the measurement image to have the same""                  "" Wcs as the reference catalog (this is a temporary limitation).""              )    `refWcs` and `exposure` in Condition (A) was from  meas_base/13.0-3-gc489882+1/python/lsst/meas/base/forcedPhotImage.py:139            refWcs = self.references.getWcs(dataRef)          exposure = self.getExposure(dataRef)    `self.references.getWcs` was probably defined in  meas_base/13.0-3-gc489882+1/python/lsst/meas/base/references.py:202:        def getWcs(self, dataRef):          """"""Return the WCS for reference sources.  The given dataRef must include the tract in its dataId.          """"""          skyMap = dataRef.get(self.config.coaddName + ""Coadd_skyMap"", immediate=True)          return skyMap[dataRef.dataId[""tract""]].getWcs()    Therefore, in Condition (A), one of the compared WCS was from skyMap,  and the other was from a FITS file.    I suspected that WCS had been changed when saved in the FITS file,  due to limited number of digits written in FITS headers.    To prove it, I wrote the following script. It actually proved  that there are some tracts where Condition (A) is violated.    ======================  import lsst.afw.image as afwImage  import cPickle as pickle    def main():      skyMap = pickle.load(open(""skyMap.pickle"", ""rb""))        for tract, tractInfo in enumerate(skyMap):          testWcs(tract, tractInfo.getWcs())    def testWcs(tract, wcs):      exp = afwImage.ExposureF(0, 0)      exp.setWcs(wcs)        exp.writeFits(""/dev/shm/foo.fits"")      exp = afwImage.ExposureF(""/dev/shm/foo.fits"")        # This message must always be ""{tract}, True""      print tract, exp.getWcs() == wcs    main()  {code}",1
"DM-10111","04/05/2017 11:19:11","Add overscan exclusion regions as an ISR config parameter","As per the discussion in RFC-315, two config parameters need to be added to overscanCorrection() to allow excluding the first and last n rows.    Both parameters should default to 0, so that the default configuration reproduces current functionality. ",1
"DM-10112","04/05/2017 11:37:15","Visualization algorithm research (f17)","This epic captures the algorithm related research for visualization. ",20
"DM-10118","04/05/2017 18:56:57","SQuaRE SuperTask Collaboration Week of 2017-04-03","Bucket ticket to cover non-specific SuperTask collaboration activities.",1
"DM-10132","04/06/2017 16:31:20","Make DCR template unit test data robust","The current unit tests for the DCR template generation prototype uses `numpy.save()` and `numpy.load()` to persist and depersist test data. This format is fragile and has already been broken once with the conversion from swig to pybind11, so a more robust test data format should be used.",2
"DM-10138","04/07/2017 09:54:17","Compare the convergence of ADMM, SDMM, and GLM algorithms","The convergence of our new algorithms, SDMM and GLMM are unknown as of now, so we should study the convergence rates of both algorithms (and compare them to the ordinary ADMM) to understand convergence in all three algorithms.",3
"DM-10142","04/07/2017 11:17:23","Change HDU used in obs_ctio0m9 from 1 to 0","obs_ctio0m9 looks in the wrong HDU for data. Config parameter just needs to be changed from 1 to 0.",1
"DM-10144","04/07/2017 12:54:41","Document lsst-texmf","People are starting to use lsst-texmf for document writing but there is some confusion. This ticket is to write a document explaining how to use the lsst latex classes.    The consensus is that this document should match the LSST ""software guide"" standard and be written using sphinx in a subdirectory of lsst-texmf (rather than writing it in Latex). The documentation will be published to https://lsst-texmf.lsst.io and will include links to the rendered example PDFs.    This ticket covers the writing of the guide. Publishing will be handled separately.",2
"DM-10146","04/07/2017 16:09:07","Fix minor doc typos","Object.h and WcsMap.h both refer to {{astshim::}} when they should refer to {{ast::}}. I'll fix that for now, but I am becoming increasingly aware that {{astshim::}} would be less confusing. Still...changing that would be another (simple) ticket.",0
"DM-10149","04/07/2017 17:54:10","The range of the phase value and period value for periodogram in time series are not correct","The range of phase value should be [0, 2)    plotly based phase folding chart doesn't react properly while an invalid period value is selected from the periodogram table. The phase folded chart should show empty content when an invalid period value is selected. ",2
"DM-10151","04/07/2017 20:43:15","Investigate a better symmetry operator","The current symmetry operator work by comparing each pixel to its symmetric partner and projecting the difference to zero using a proximal operator, forcing the solution to be symmetric. This works relatively well, but not as well as the current deblender in certain cases. For example, in a blend with only two sources, with one source much brighter than the other, the faint source using the NMF deblender attempts to steal a small amount of flux from its brighter neighbor. We had hoped that the near zero flux opposite the bright source would be enough to limit this effect (when combined with symmetry) but the algorithm breaks when the fainter object wants to steal flux near the same level as the noise. Sparsity is not much help, as l0 sparsity limits the size of brighter objects more than the smaller ones.    This blending issue is accounted for in the current deblender (adapted from SDSS) because it doesn't just force a symmetric solution, it forces the symmetric solution to use the minimum of each pixel and its symmetric partner. This is a much stronger constraint (and more useful) constraint.    It would be useful to update the NMF deblender to have the same minimum pixel value constraint. One way to do this is to use a proximal operator that projects each template onto a space that uses the minimum of each symmetric pair. This will no longer be a linear operation, as choosing the minimum pixel value is non-linear, so it is unclear how this will affect convergence.    If we can make this work, combining the new symmetry operator with monotonicity might eliminate the need to use a sparsity constraint, which is advantageous as [~pmelchior] and I are realizing that sparsity is a wilder beast than it first appeared to be, and taming it is not a simple task. This is because the sparsity of an image is dependent on the total flux in the image, and the brightness of each source, so it might be that using sparsity would require each source in a blend to have a different sparsity requirement dependent on it's brightness, which could be a difficult scheme to implement. Taking care of this directly using a better symmetry constraint is preferable.",3
"DM-10152","04/08/2017 12:16:19","Fix bug in Box Python stringification","The implementation of {{Box2I.\_\_repr\_\_}} in the pybind11 wrappers incorrectly labels it as a {{Box2D}} which is very confusing.  ",0.5
"DM-10161","04/10/2017 11:06:35","Remove maxtasksperchild=1 during pool initialization in meas_mosaic","Testing for DM-10043 has revealed that having {{maxtasksperchild=1}} in the pool initialization for {{mosaic.py}} (the pool being used to read input catalogs in parallel) causes a bug where {{mosaic.py}} hangs indefinitely.  Curiously, this happens only in with the LSST stack (pybind11 version), not with the HSC stack.  It has been demonstrated that removing this constraint removes this hanging behavior.",1
"DM-10173","04/10/2017 13:34:45","Update bokeh to version 0.12.4","New version of bokeh released, updating from 0.12.3 to 0.12.4.   ",0.5
"DM-10183","04/11/2017 09:58:55","Investigate why maxtasksperchild=1 causes mosaic.py to hang on pybind11 stack","{{mosaic.py}} uses {{multiprocessing.Pool}} to read catalogs with multiple cores.  When this pool is initialized with {{maxtasksperchild=1}}, {{mosaic.py}} hangs indefinitely at a consistent point in the running---that is, running with the same arguments multiple times will freeze up in the same place.  This is only a problem with the pybind11 version of the stack, as this behavior does not occur in the HSC stack, which is currently still wrapped with swig.  The underlying cause of this should be investigated to make sure that there is not some deeper issue that might cause problems with parallelization elsewhere.  ",2
"DM-10187","04/11/2017 11:45:49","Properly handle decimal.Decimal types in dbserv","The WISE dataset uses fixed precision decimals and dbserv doens't properly handle them when serializing to JSON.      {code}  [2017-04-11 16:52:06,003] ERROR in app: Exception on /db/v0/tap/sync [POST]  Traceback (most recent call last):    File ""/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py"", line 1982, in wsgi_app      response = self.full_dispatch_request()    File ""/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py"", line 1614, in full_dispatch_request      rv = self.handle_user_exception(e)    File ""/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py"", line 1517, in handle_user_exception      reraise(exc_type, exc_value, tb)    File ""/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py"", line 1612, in full_dispatch_request      rv = self.dispatch_request()    File ""/lsst/stack/Linux64/flask/0.12/lib/python/Flask-0.12-py2.7.egg/flask/app.py"", line 1598, in dispatch_request      return self.view_functions[rule.endpoint](**req.view_args)    File ""/lsst/stack/Linux64/dax_dbserv/12.1-2-gb29d23c+10/python/lsst/dax/dbserv/dbREST_v0.py"", line 98, in sync_query      return _response(response, status_code)    File ""/lsst/stack/Linux64/dax_dbserv/12.1-2-gb29d23c+10/python/lsst/dax/dbserv/dbREST_v0.py"", line 138, in _response      response = json.dumps(response)    File ""/lsst/stack/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/json/__init__.py"", line 244, in dumps      return _default_encoder.encode(obj)    File ""/lsst/stack/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/json/encoder.py"", line 207, in encode      chunks = self.iterencode(o, _one_shot=True)    File ""/lsst/stack/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/json/encoder.py"", line 270, in iterencode      return _iterencode(o, 0)    File ""/lsst/stack/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/json/encoder.py"", line 184, in default      raise TypeError(repr(o) + "" is not JSON serializable"")  TypeError: Decimal('0.000') is not JSON serializable  {code}",1
"DM-10188","04/11/2017 13:57:42","Fix two drawing related issues with the new rotation scheme  and check regions","Fix two drawing related issues with the new rotation scheme      - marker resize handles off when rotated   - footprint rotate handles off when rotated    Check both wit flip as well.     Also check if the region code is working the flip and rotate.      This ticket should be done as a pull request against branch dm-10065-canvas instead of dev.",8
"DM-10195","04/11/2017 18:36:22","Improve comparison handling in Name and SpecificationSet classes of verify framework","I realized that not {{__ne__}} is not automatically implemented when {{__eq__}} is implemented. This can cause unexpected behavior with the {{!=}} operator.    This ticket implements {{__ne__}} in existing classes in verify.    I also implements {{__lt__}}, {{__le__}}, {{__gt__}}, and {{__ge__}} operators to the Name class to support Name sorting.",0.5
"DM-10197","04/12/2017 09:40:53","Clarify wording relating to flat-fielding in LDM-151","On reading LDM-151, I asked [~mfisherlevine] a question about how flat-fields were being applied. He's provided some updated text. On this ticket, we'll include it in the document.",1
"DM-10206","04/12/2017 17:43:57","Fix obs_decam compatibility with 0-indexed HDUs","The recent implementation of DM-9952 caused the ingestCalibs config in obs_decam to break when ingesting defects (and possibly other calibration products too). A quick fix should adjust the default HDU indexing to begin at 0 instead of 1.",0.5
"DM-10212","04/13/2017 02:54:16","Check memory locking in containers","Memory locking should be controlled inside containers",3
"DM-10217","04/13/2017 17:42:34","Locate LSE-63 source and modernize","In order to annotate LSE-63 with requirements I need to locate the source from [~tony], add it to github and make it work with the new Latex look and feel.",1
"DM-10220","04/14/2017 10:58:48","Miscellaneous corrections to C++ Doxygen guidelines","The following minor changes should be made to the C++ Documentation style guide:  * The guide should note that multi-line brief descriptions must be prefixed by {{@brief}}.  * The guidelines for covering multiple parameters in a single {{@tparam}} or {{@param}} tag claim that Doxygen can't handle parameters separated by both a comma and a space. Testing has shown that this is not the case; Doxygen correctly identifies what's a parameter and what's the first word of the description.  * Likewise, parameters can be documented as {{\[in, out]}} rather than {{\[in,out]}}.  * The guidelines for the {{@throws}} tag should say how to namespace-qualify exceptions: the rules given for {{@see}} work, but namespace abbreviations must not be used (Doxygen won't resolve the links correctly).    This ticket *should not* be started before July 2017 to give time for more errors to be identified.",1
"DM-10221","04/14/2017 11:23:07","Allow --id to use any key in the registry","As described in DM-5902 it is very useful to be able to specify arbitrary registry keys in the {{--id}} of e.g. {{constructBias.py}}.  Unfortunately only keys that are returned by {{butler.getKeys}} for the appropriate dataset type are currently accepted, even if other keys would be sufficient to define the desired data.    Please fix this.  ",1
"DM-10225","04/14/2017 13:04:04","Ingest IMGTYPE along with other header keys","Ingest {{IMGTYPE}} along with other header keys. Add a translator so that these end up with useful/sane values.",1
"DM-10229","04/14/2017 13:55:08","pipe_base tests try to write to obs_test","And then fail when they don't have permission. That is:    {code}  ======================================================================  ERROR: testOutputs (__main__.ArgumentParserTestCase)  Test output directories, specified in different ways  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""testArgumentParser.py"", line 502, in testOutputs      args = parser.parse_args(config=self.config, args=[DataPath, ""--rerun"", ""foo""])    File ""/ssd/swinbank/pipe_base/python/lsst/pipe/base/argumentParser.py"", line 509, in parse_args      namespace.butler = dafPersist.Butler(inputs=inputs, outputs=outputs)    File ""/ssd/lsstsw/stack_20170409/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/butler.py"", line 351, in __init__      self._createRepoDatas(inputs, outputs)    File ""/ssd/lsstsw/stack_20170409/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/butler.py"", line 572, in _createRepoDatas      parents = self._getParentsList(inputs, outputs)    File ""/ssd/lsstsw/stack_20170409/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/butler.py"", line 538, in _getParentsList      cfg = Storage.getRepositoryCfg(args.cfgRoot)    File ""/ssd/lsstsw/stack_20170409/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/storage/storageContinued.py"", line 82, in getRepositoryCfg      ret = Storage.storages[parseRes.scheme].getRepositoryCfg(uri)    File ""/ssd/lsstsw/stack_20170409/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/posixStorage.py"", line 150, in getRepositoryCfg      repositoryCfg = PosixStorage._getRepositoryCfg(uri)    File ""/ssd/lsstsw/stack_20170409/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/posixStorage.py"", line 131, in _getRepositoryCfg      with open(loc, 'r') as f:  IOError: [Errno 13] Permission denied: u'/ssd/lsstsw/stack_20170409/Linux64/obs_test/13.0-6-g7b63e3f/data/input/rerun/foo/repositoryCfg.yaml'  {code}",1
"DM-10231","04/14/2017 14:25:56","FileForWriteOnceCompareSame does not respect umask","Viz:    {code}  $ umask  0022    $ python  Python 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15)  [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2  Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.  Anaconda is brought to you by Continuum Analytics.  Please check out: http://continuum.io/thanks and https://anaconda.org  >>> import lsst.daf.persistence.safeFileIo as safeFileIo  >>> with safeFileIo.FileForWriteOnceCompareSame(""bar"") as f:  ...     f.write(""foo"")  ...  >>>    $ ls -l bar  -rw------- 1 swinbank lsst_users 3 Apr 14 15:24 bar  {code}    This means in particular that {{repositoryCfg.yml}} files will be written with restrictive permissions. See also DM-10229.",1
"DM-10233","04/14/2017 18:50:24","getInfoFromMetadata() throws away errors without warning.","getInfoFromMetadata() throws away errors without warning. This makes debugging writing translator functions very hard.",1
"DM-10234","04/17/2017 06:24:26","Submit lsstDebug notes to Developer Guide","A while back, I wrote some (brief!) notes on {{lsstDebug}}, which I think are more helpful than https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/base_debug.html. Rather than having them get lost, let's see if we can add them to the Dev Guide.    (Ultimately, they should be in a ""framework"" section of the pipelines docs, à la DMTN-030. But that doesn't exist yet.)",1
"DM-10235","04/17/2017 07:23:13","Bug in coaddDriver when selecting images by PSF quality.","In DM-9855, the option to select input images based on PSF quality was added and made the default for assembleCoadd in obs_subaru.  However, coaddDriver has its own function to select images and sets the one in assemble coadd to null.  The config in obs_subaru was overriding this and the selection of images was being run twice and having strange results.    The resolution is to set the image selector in coaddDriver to the PSF quality selector and set the assembleCoadd selector to null.",1
"DM-10236","04/17/2017 07:29:57","Properly apply the meas_mosaic solution","As stated in DM-9862, meas_mosaic assumes the 0,0 for each CCD to be the lower left hand corner.  LSST uses a different coordinate system so meas_mosaic rotates the wcs into the LSST frame when writing to disk.  However the photometric correction is still in the HSC frame, so when applying the meas_mosaic correction we need to rotate the wcs back to the HSC frame.  We can then create the photometric correction, rotate it into the LSST frame and apply it to the image.  We then rotate the wcs back to the LSST frame.",2
"DM-10241","04/17/2017 16:53:20","Need to fix the East arrow in the North/East compass when close to the polar region","In http://irsawebdev1.ipac.caltech.edu/firefly/;a=layout.showDropDown?visible=false, search for image: RA = 45, Dec = 89, select IRAS with 5 degree cutout; Add grid, add the North/East compass. The East arrow line should be perpendicular to the North arrow line. ",1
"DM-10242","04/17/2017 17:44:34","Stop using astrometry_net by default","There is code in {{pipe_tasks}} that uses {{astrometry_net}} to load catalogs and fit astrometric solutions. This ticket is to move to the new reference catalog format and newer astrometry fitter.    Note that when this is done in {{pipe_tasks}} that we can remove {{meas_extensions_astrometryNet}} as a dependency.    I suggest this wait on DM-2186, though that is not strictly necessary.",1
"DM-10251","04/18/2017 12:12:26","weekly release w_2017_16 failed due to lsstsw@lsst-dev breakage","The jenkins node running as {{lsstsw@lsst-dev}} used for publishing eups distrib packages is unable to build from current master, thus preventing the weekly release process.  Speculation is that some installed products were built from a mixture of the system gcc and devtoolset-3.",0.5
"DM-10252","04/18/2017 13:34:08","getOutputId() assumes keys will exist, and doesn't use butler to retrieve them","In constructCalibs.py, getOutputId() assumes certain keys will exist exist within a dataId. Whilst this information will likely exist in the registry, it _shouldn't_ already exist in the dataId (that's not what they're for), and these should be retrieved at this point using the butler.",1
"DM-10265","04/19/2017 08:22:05","Include table persistence docs in Doxygen listing for afw","There's some documentation at https://github.com/lsst/afw/blob/1d3521f81d348fdc49b708342eba0be1caf5295b/doc/tablePersistence.dox which seems useful, but is basically impossible to find in Doxygen. Let's include it on the list at https://lsst-web.ncsa.illinois.edu/doxygen/x_masterDoxyDoc/afw.html.",1
"DM-10267","04/19/2017 08:35:05","Port HSC support for PostgreSQL registries to LSST","At least some of the implementation was in    $OBS_SUBARU_DIR/python/lsst/obs/subaru/ingest.py    on the old HSC fork.",2
"DM-10268","04/19/2017 09:20:33","Butler cannot read a repo using the realpath when it was created with a link ","With the recent Butler changes (likely when {{_parent}} link was removed), Butler can no longer read an output repo if the repo path was from a symlink.   For example, on lsst-dev, I have a symlink   {{/datasets/hsc/repo/rerun/private/hchiang2/}} -> {{/scratch/hchiang2/hscRerun/}}   (therefore {{/datasets/hsc/repo/rerun/private/hchiang2/xxx}} ==  {{/scratch/hchiang2/hscRerun/xxx}})    so I processed some hsc data and the outputs went to my scratch space   (e.g. {{processCcd.py /datasets/hsc/repo --id visit=18194 ccd=9 --rerun private/hchiang2/xxx}})     Then I tried to read the output repo. I can only read it through {{dafPersist.Butler(""/datasets/hsc/repo/rerun/private/hchiang2/xxx/"")}}, but not {{dafPersist.Butler(""/scratch/hchiang2/hscRerun/xxx/"")}}.  The latter had errors finding its parent:       {code:java}  Traceback (most recent call last):    File ""<stdin>"", line 1, in <module>    File ""/software/lsstsw/stack/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/butler.py"", line 353, in __init__      self._repos._buildLookupLists(inputs, outputs)    File ""/software/lsstsw/stack/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/butler.py"", line 219, in _buildLookupLists      addRepoDataToLists(repoData, 'out')    File ""/software/lsstsw/stack/Linux64/daf_persistence/13.0-6-gf146911/python/lsst/daf/persistence/butler.py"", line 211, in addRepoDataToLists      addRepoDataToLists(self.byRepoRoot[parent], addParentAs)  KeyError: '/'  {code}    Above was with the  {{w_2017_14}} stack. The {{repositoryCfg.yaml}} file in the repo has    {{\_parents: \[../../../..\]}}",2
"DM-10270","04/19/2017 09:38:50","isrTask does not provide config option for defects","Most things in isrTask are configurable, but whether defects masked is not currently protected by a config option, and furthermore, the default behaviour when a defect file isn't found is a fail.    A config parameter should be added to allow not doing defects.",1
"DM-10271","04/19/2017 10:41:36","Fix order of operations when using temporary local backgrounds in detection","Our current implementation of the temporary local background approach to avoiding spurious detections near bright objects simply subtracts a local background from the full image before performing any detection steps.  That can result in missed isolated-object detections and incorrect Footprints for large objects.    Instead, we should:  1. Detect Footprints and Peaks.  2. Subtract the local background.  3. Detect Peaks within each Footprint again, and use the new set of Peaks instead of the old set if and only if there is at least one Peak in the new set.    This really ought to be fixed before the HSC internal release or major HSC processing at NCSA.",3
"DM-10272","04/19/2017 10:57:14","scons build fails under miniconda3 on el6","We are unable to build on el6/py3 as the {{scons}} product's {{eupspkg.cfg.sh}} references {{python2.7}}.  The system python interpreter on el6 is {{python2.6}}.  If scons is still compatible with 2.6, and we want to support the combination of el6/py3 (???), this could be fixed by changing the interpreter to be {{python2}}.    {code:java}  [el6] ***** error: from /build/EupsBuildDir/Linux64/scons-2.5.0.lsst2+1/build.log:  [el6] ++ debug 'CC='\''cc'\'''  [el6] ++ [[ 3 -ge 2 ]]  [el6] ++ echo 'eupspkg.dumpvar (debug): CC='\''cc'\'''  [el6] eupspkg.dumpvar (debug): CC='cc'  [el6] + for _VAR in '""$@""'  [el6] + eval 'debug ""CXX='\''$CXX'\''""'  [el6] ++ debug 'CXX='\''c++'\'''  [el6] ++ [[ 3 -ge 2 ]]  [el6] ++ echo 'eupspkg.dumpvar (debug): CXX='\''c++'\'''  [el6] eupspkg.dumpvar (debug): CXX='c++'  [el6] + for _VAR in '""$@""'  [el6] + eval 'debug ""SCONSFLAGS='\''$SCONSFLAGS'\''""'  [el6] ++ debug 'SCONSFLAGS='\''opt=3'\'''  [el6] ++ [[ 3 -ge 2 ]]  [el6] ++ echo 'eupspkg.dumpvar (debug): SCONSFLAGS='\''opt=3'\'''  [el6] eupspkg.dumpvar (debug): SCONSFLAGS='opt=3'  [el6] + build  [el6] + python2.7 setup.py build  [el6] /build/EupsBuildDir/Linux64/scons-2.5.0.lsst2+1/scons-2.5.0.lsst2+1/ups/eupspkg.cfg.sh: line 5: python2.7: command not found  [el6] + exit -4  [el6] eups distrib: Failed to build scons-2.5.0.lsst2+1.eupspkg: Command:  [el6]  source ""/build/eups/bin/setups.sh""; export EUPS_PATH=""/build""; (/build/EupsBuildDir/Linux64/scons-2.5.0.lsst2+1/build.sh) >> /build/EupsBuildDir/Linux64/scons-2.5.0.lsst2+1/build.log 2>&1 4>/build/EupsBuildDir/Linux64/scons-2.5.0.lsst2+1/build.msg   [el6] exited with code 252  [el6] Removing lockfile /build/.lockDir/exclusive-root.631  {code}  ",1
"DM-10275","04/19/2017 11:23:04","Add save image button for Plotly charts","When a chart is plotly chart, image download button should be present on it's toolbar.  Start with png and jpeg (which require no payment plan).",5
"DM-10281","04/19/2017 14:54:41","compiler warnings in astshim","I'm seeing several variants of the following errors in afw builds against astshim:  {code}  include/astshim/Object.h:426:80: warning: format not a string literal and no format arguments [-Wformat-security]       void set(std::string const & setting) { astSet(getRawPtr(), setting.c_str()); }  {code}  This happens virtually anywhere there's a call to an AST function with the results of {{.c_str()}}.    I also see two instances of signed/unsigned comparison:  {code}  /home/jbosch/LSST/py2/stack/Linux64/astshim/master-gb637561bb9/include/astshim/detail/utils.h:43:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]       if (val1 != val2) {  {code}    This is with gcc 5.4.0 on Ubuntu 16.04.  ",2
"DM-10287","04/20/2017 07:10:08","Add measurement plugin to store footprint area","This is a trivial feature request that would have also been very useful on DM-9962.  In terms of effort it should be ""in the noise"" of everything else blocking DM-10266, so I'm going to try to get it in.",0.5
"DM-10288","04/20/2017 08:03:33","afwImage.TanWcs.cast() not supported anymore in jointcalCoadd","The cast at : https://github.com/lsst/jointcal/blob/master/python/lsst/jointcal/jointcalCoadd.py#L54 is not supported anymore and is probably useless since the pybind11 migration",1
"DM-10289","04/20/2017 08:08:46","record.setValidPolygon(xxx) does not accept None as a valid input anymore ","makeCoaddTempExp is crashing at https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/coaddInputRecorder.py#L148 because None is not accepted anymore as a valid input for record.setValidPolygon.    Just skipping this instruction when there is no valid polygon set in the exposure seems to be a possible workaround",1
"DM-10292","04/20/2017 11:56:22","The FrameSet returned by Transform.getFrameSet can change the contained FrameSet in Python","The {{ast::FrameSet}} returned by {{Transform.getFrameSet}} is not immutable in Python, and modifying it modifies the frame set owned by the transform. In C++ the returned FrameSet is immutable, but that is impossible to enforce in Python, so the obvious fix is for the Python wrapper to return a copy.",0.5
"DM-10295","04/20/2017 14:51:29","Fix metaserv v0 table listing and deploy docker containers","metaserv table listing uses {{information_schema}}. It should utilize DBRepo and DDT_Table in metaserv.",1
"DM-10302","04/20/2017 17:16:01","Rename ""*_flux"" fields to ""*_instFlux"" in SourceCatalogs","This is the implementation ticket for RFC-322. The implementation is as follows:    * Rename all of our {{*Flux_flux}}/{{*Flux_fluxSigma}} table fields to {{*Flux_instFlux}}\{{*Flux_instFluxSigma}} to hold the post-ISR counts.  * Add {{*Flux_flux}} and {{*Flux_mag}} fields for the post-calibrated flux (in Maggies) and magnitudes.  *Rename the {{InstFlux}} slot to {{GaussianFlux}}, and remove the {{InstMag}} slot.  * Add associated documentation to the above.  * Pass these changes on to the relevant database groups to update e.g. {{cat}}.    Implementing this will wait until Calib is removed from the stack and replaced by PhotoCalib (not yet scheduled, but likely within the next couple months).",8
"DM-10304","04/21/2017 08:41:07","Create tech note describing options for DM software releases","Following RFC-188 and discussion the 2017-04-17 DMLT meeting, discuss options for the DM release process with stakeholders and create a technote summarising important considerations.",3
"DM-10305","04/21/2017 10:31:28","Update firefly_widgets for changes in external dependencies","Update firefly_widgets to account for some changes in external dependencies:    * Generalize the connection parameter, for working with Firefly servers now being deployed with {{suit}} or {{irsaviewer}} in the URL, as was done for firefly_client in DM-9843  * Make some minor syntax changes to work with ipywidgets 6.0  * Update example notebooks accordingly",2
"DM-10329","04/24/2017 12:56:18","Write up Science Pipelines perspective on SuperTask interfaces and concepts","Describe (my) perception of the current state of the SuperTask interface from the Science Pipelines perspective to gather feedback from other Science Pipelines stakeholders and clarify any problems to the rest of the SuperTask working group.",2
"DM-10332","04/24/2017 15:14:16","Test deblender with exact positions","[~pmelchior] and I believe that the main failure of the current deblender is due to the difference between a sources peak position and its integer position in the pixel grid, which causes the symmetry operator to fail. In DM-10189 we created a translation operator that can shift the position of the peak to a fractional position and before we implement a solution to fit this position (DM-10310) it will be advantageous to check use the exact position (known in a simulated image) and see if this resolves the problem.    There is no reason to proceed with DM-10310 until we are sure that better positions and the translation operator will improve the deblender performance.    Since this ticket requires re-writing the functions that calculate the likelihood, this will also include updating the intensity matrix ({S}} in the NMF code) so that each source is in the center pixel. This suggestion by [~jbosch] allows each source to use the same monotonicity and symmetry operators, whose calculation were one of the more time consuming parts of the new deblender. It should also make optimization easier once parts of the code are ported to C++.",5
"DM-10333","04/24/2017 15:58:24","duplicate keys in obs_base/policy/datasets.yaml","For example: 'processCcd_config' and `characterizeImage_config` both occur twice.  Maybe others?    Not a problem right now since the values are the same for a given key (at least for the two examples above), but could lead to errors in the future if, for example, second entry overrides the first.    Should probably use a yaml loader that would raise an exception on duplicate keys, e.g.,  https://gist.github.com/pypt/94d747fe5180851196eb  ",1
"DM-10336","04/24/2017 17:53:15","DM-10271 seems to have broken afw","As of DM-10271 I can no longer build afw on my Mac. I see errors such as:  {code}  In file included from python/lsst/afw/detection/peak.cc:33:  include/lsst/afw/table/python/catalog.h:170:13: error: no matching function for call to 'PySlice_GetIndicesEx'          if (PySlice_GetIndicesEx((PySliceObject*)s.ptr(), self.size(), &start, &stop, &step, &length) != 0) {              ^~~~~~~~~~~~~~~~~~~~  python/lsst/afw/detection/peak.cc:114:42: note: in instantiation of function template specialization 'lsst::afw::table::python::declareCatalog<lsst::afw::detection::PeakRecord>' requested        here      auto clsPeakCatalog = table::python::declareCatalog<PeakRecord>(mod, ""Peak"");                                           ^  /Users/rowen/UW/LSST/lsstsw3/miniconda/include/python3.5m/sliceobject.h:43:17: note: candidate function not viable: no known conversion from 'PySliceObject *' to 'PyObject *'        (aka '_object *') for 1st argument  PyAPI_FUNC(int) PySlice_GetIndicesEx(PyObject *r, Py_ssize_t length,                  ^  {code}  I am using a Python 3 lsstsw stack on macOS 10.12.4 with the current clang: Apple LLVM version 8.1.0 (clang-802.0.42)",0.5
"DM-10338","04/24/2017 18:28:43","Mix of tabs and spaces in breaks meas_base builds","Seems to have been introduced in DM-430; [~pgee], please make sure your editor is configured to use spaces instead of tabs.",0.5
"DM-10343","04/25/2017 10:14:15","Update lsst-dev shared stacks to use devtoolset-6","Update both shared stacks on {{lsst-dev01}} to build against devtoolset-6.    Since this won't be ABI compatible with the earlier (no-devtoolset) stacks, we'll need to create entirely new stacks, which should probably build over a weekend.",1
"DM-10344","04/25/2017 12:17:38","The Validate failed to validate the integer range","In HistogramOption, the numBins field used the validator: ""Validate.intRange.bind(null, 1, 500, 'numBins')"" to validate its value.  However, when the empty string is entered in to the  numBins, the validation still shows it is valid.  The empty string is not in the range of 1-500, by definition, it is not valid. ",0.5
"DM-10349","04/25/2017 22:20:14","Chart expression logarithm to be the same as other languages","In Firefly  chart column expression, we use log() as 10-based logarithm, ln() as natural logarithm.     I checked Python, Perl, C, C++, Java, the convention is to use log() as natural logarithm, and log10() as the 10-based logarithm.     We need to make the change and document it. ",2
"DM-10360","04/26/2017 14:46:09","Documentation for generating statistics for L2/catalog data","Statistics were generated at IN2P3 for correcting {{JOIN}} queries. The general process needs to be documented so we can optimize it in the future (see DM-9757 for reference).",2
"DM-10370","04/26/2017 16:21:13","Refactor server side image code so that we cut memory size in half","Moving some of the image processing to the client side allows us not to use as much memory on the server. We currently keep to versions of the FITS data a 2D array and a 1D array.  Remove the need to hold on to the 2D array.",8
"DM-10379","04/27/2017 10:44:53","After a catalog search with a globe coverage map, the catalog search doesn't work anymore.","In today's build, either http://irsawebdev1.ipac.caltech.edu/firefly/ or http://irsawebdev1.ipac.caltech.edu/irsaviewer/, a bug is found:    Catalogs, WISE, m81, search; then Catalogs, WISE, m31, search; Now the coverage map is shown as a whole globe. Now select Catalogs, it won't work. If select Images, this search panel can't proceed to search or being cancelled.    The error message:  CatalogSearchMethodType.jsx:179 Uncaught TypeError: Cannot read property 'x' of null      at a (CatalogSearchMethodType.jsx:179)      at Object.s [as reducerFunc] (CatalogSearchMethodType.jsx:395)      at J (FieldGroupCntlr.js:384)      at Z (FieldGroupCntlr.js:363)      at p (FieldGroupCntlr.js:274)      at combineReducers.js:132      at d (createStore.js:179)      at middleware.js:52      at index.js:75      at Object.dispatch (index.js:14)    ",2
"DM-10381","04/27/2017 11:40:43","Enhance test for meas_deblender's clipFootprintToNonzeroImpl","In DM-10361, [~nlust] added a quick fix for a problem in this function which was breaking the stack demo.    That fix has done the trick, but it would have been nice to be able to catch the problem before it propagated as far as it did. There's already a fine unit test for this function; extend it to catch the problematic case.",1
"DM-10386","04/27/2017 15:54:53","Add Constructor documentation to Footprints","Somehow in my many rebasings I lost the doxygen documentation to the Footprint constructors. Add that documentation back in.",1
"DM-10391","04/27/2017 23:54:05","jupyterlab technote","Work involved in SQR-018.",5
"DM-10392","04/28/2017 06:27:45","Upgrade kubernetes/docker on cc-in2p3 cluster","Work performed in DM-10314, DM-10212 and DM-10042 has to be integrated on cc-IN2P3 nodes ccqserv100->124.",5
"DM-10393","04/28/2017 09:59:14","correct variable name in sites.xml template","Need to fix a typo in a variable name.",0
"DM-10398","04/28/2017 14:57:24","Analyze run metadata from validation runs","Analyze and make summary plots for the metadata collected in DM-12440.  This may involve investigating further any failure modes that may have cropped up.",8
"DM-10401","04/29/2017 20:16:52","getPackageDir raises RuntimeError instead of pex::exceptions::NotFoundError","The documentation for {{utils.getPackageDir}} claims {{@throw lsst::pex::exceptions::NotFoundError if desired version can't be found}}, but it actually raises {{RuntimeError}}:    {code}  In [1]: import lsst.utils    In [2]: lsst.utils.getPackageDir('dajfsfsa')  ---------------------------------------------------------------------------  RuntimeError                              Traceback (most recent call last)  <ipython-input-2-bac2a7aa8ca6> in <module>()  ----> 1 lsst.utils.getPackageDir('dajfsfsa')    RuntimeError: Package dajfsfsa not found  {code}    We should either fix the docstring, or fix what is raised (likely at the pybind11 layer).    We also need to fix the {{GetPackageDirTestCase}} unittest so that it tests against the correct exception being raised (it currently tests {{Exception}}, which is unhelpful).",1
"DM-10416","05/01/2017 17:14:15","Make lsst.afw.geom.Transform and SkyWcs pickleable","The new `lsst.afw.geom.Transform` classes and the subclass `SkyWcs` should be pickleable.    One possibility is to pickle the string representation for the contained FrameSet.",0
"DM-10421","05/01/2017 22:40:13","Publish LDM-294 draft to LSST the Docs","Publish [LDM-294|https://github.com/lsst/LDM-294] to LSST the Docs, with a landing page pointing to PDFs. This landing page is an MVP that will be improved and made generally available for all LaTeX-based DM documents.",1
"DM-10426","05/02/2017 07:34:41","Identify stable version of kubernetes and docker on openstack","Kubernetes and Docker have provided quick update which have broken both openstack and cc-in2p3 setup. Stable version and configuration have to be identified.",2
"DM-10427","05/02/2017 10:05:22","Establish sha256 hashing framework for VO API and content signatures","The idea of introducing SHA256 hashing in VO context is for the quick identification and verification of signatures for API method calls, as well as the returned content such as generated FITS images, which will become handy for caching and asynchronous operation.",20
"DM-10430","05/02/2017 10:51:18","Add time stamps to the standard outputs to BatchCmdLineTask","Add time stamps to the logs by default in {{ctrl_pool}} jobs.  In slurm jobs, this changes the log format in the JOBNAME.oJOBID log files.",1
"DM-10433","05/02/2017 11:51:47","Build date information at wrong place in small browser window","Usually the application build date is at the bottom of the browser window on the dark bark. But when the browser window is small, the build date information flows up at the bottom of the visible portion of browser window.     We need to fix it. ",1
"DM-10438","05/02/2017 13:20:08","Add DCR model data types","To use existing code in the stack for multi-band photometry, forced photometry, etc.. on the models generated with the DCR algorithm, many data types need to be added to obs_base. These will be added using the current name 'dcrModel' throughout, though it is expected that name might change following the eventual RFC to add the data type.",2
"DM-10439","05/02/2017 13:43:26","bug in WCS match or compass overlay ","Access PDAC http://lsst-sui-proxy01.ncsa.illinois.edu/suit, follow the steps and see the bug  appear:    # search CCD exposure images at position (0, 0)  # filter on run = 5895  # turn on WCS match  # turn on compass overlay  # r-band image has east up while all other bands have east down  # click on North up icon, r-band compass has North down    Attachment is the portal after WCS match and north up actions.  ",2
"DM-10441","05/02/2017 15:51:28","Image cutout is off the center when size specified in Ang degrees.","Following update from DM-10364, David Shupe reported seeing cutout images (calexp) is off the center.  Further diagnosis revealed it's only the case when the cutout size is specified in Ang Degrees, not by 'pixel'.",2
"DM-10452","05/03/2017 10:59:44","Create bboxFromIraf function in obs_base utils","Create (move) utility function to create a bbox from from an IRAF box from obs package to obs_base so others can use it.",1
"DM-10453","05/03/2017 12:16:49","Fix bugs in matchPessimisticB","Bugs have been found in the latest refactor of matchPessimsiticB that effect performance of the matcher.    failedPatternList persists between runs of MatchPessimisticBTask when running a range of ccds in the same processCcd run.  failedPatternList is not currently used properly in the matcher loop.  The pair_id lookup table is not filled with the correct data for half of the matrix.",2
"DM-10455","05/04/2017 10:50:08","Use pool_recycle=3600 for long-lived database connection pools","We get ""mysql server has gone away"" errors when dbserv hasn't been used in a while. This might not be a problem in the future, but it's a problem with pooled connections too.      The fix is to use pool_recycle parameter when creating sqlalchemy:    http://docs.sqlalchemy.org/en/latest/core/pooling.html    3600 seconds is probably appropriate.",1
"DM-10461","05/04/2017 14:58:01","WCS match can be a little off for plots that are nearly north","There is a roundoff error for computing rotation for plots that are nearly north",0.5
"DM-10465","05/04/2017 17:55:03","Better error messages for position input","When user input a position like (-34, 23) or (123, -91), Firefly treated it as object name and gave the error message ""Could not resolve Object: Enter valid object"". It is hard for user to understand exactly what was wrong with the input. In reality it is the position out of range error. Firefly should provide better error message for situation.    Another issue: ""12.3, 23.4"" is valid input, but ""12.3, 23.4 gal"" is not. And we have an example for the latter in the input pane.    input ""18.0, -23.0""  has been interpreted into ""18.0, -23.0  Equ J2000 "". but the latter is invalid    input ""12 34 56.89 -23 45 16.56""  has been interpreted into 12h00m00.00s, -23d00m00.0s  Equ J2000",8
"DM-10477","05/05/2017 14:42:09","""WCS match""  checkbox on/off behavior improvements ","Current behavior:  When there are more than one images displayed,  turning ""WCS match"" checkbox on will zoom and rotate all the images to the same pixel size and direction as the active image (selected, with orange color highlight), turning the checkbox off does nothing.     Improvement:  Turning off the ""WCS match"" should reset the images to a zero rotation.    Also include two bug fixes:    - A rotation issue: clicking rotate north on north up images fliped them south  - When north is up, clicking north up makes the image rotate south.    ",2
"DM-10483","05/08/2017 09:37:32","Create testdata_deblender Repo","Create a repository to store data for testing {{meas_deblender}}. At present this is the simulated data generated by [~rearmstr] in DM-9644 but later is likely to include public HSC data of particularly difficult blends.",1
"DM-10486","05/08/2017 18:44:04","warpExposure and warpImage do not test correctly for dest = src","warpExposure and warpImage are supposed to throw an exception if destImage = srcImage. However, the unit test for this is mis-written, due to checking for Exception being raised, instead of a more specific exception, hiding other problems. The test case in question is {{testWarpIntoSelf}}.    Worse, on my Mac, when I correct the test I find that it is possible to warp one image or exposure into itself, even though this results in altering the supposedly const input image and produces an incorrect destination image. Thus the C++ code that attempts to check for dest == src is not working.",3
"DM-10490","05/09/2017 09:38:20","Cache camera in HscMapper","The lion's share of the time in instantiating a butler goes to building the camera object, because that involves parsing a {{Config}} with many lines, and a {{stat}} call for each line to get the traceback.  Multiple butler instantiations (e.g., for multiprocessing or pipe_drivers) could benefit from caching the camera once it's built.",1
"DM-10495","05/09/2017 13:14:03","turn on travis and flake8 protections in jointcal","The branch protection+travisCI+flake8 system will soon be made available for activation on LSST repositories. I've volunteered be a non-SQuaRE person to test the process, using the jointcal repo (which has a very small list of commit users right now).    Once the ""final draft"" instructions are available, I'll follow them for jointcal and provide feedback to SQuaRE.",1
"DM-10496","05/09/2017 15:24:58","test_chebyMap.py sometimes segfaults","test_chebyMap.py occasionally segfaults when run on my Mac (something on the order of 1 in 10 times). When this occurs it happens almost immediately and no output is printed.    One thing to try is updating to the latest AST since a known bug in the Chebyshev handling of AstPolyTran has been fixed. If that doesn't solve the problem then more digging will be required (e.g. valgrind, but that would be much easier if I could write a pure C++ program that exhibited the behavior).",1
"DM-10502","05/10/2017 09:27:47","Update NMF deblender to use new footprints","Development on the new deblender has been using an older version of the stack before the new footprints were pushed. In order to complete DM-9784 it is useful to have access to the new {{Footprint}}/{{SpanSet}} API, so this ticket will update the user branch {{u/fred3m/deblender}} that the holds the new deblender work to the current version of the stack.",1
"DM-10505","05/10/2017 11:51:31","Robustify validate_drp fitting and catching errors.","First attempt to run `validate_drp` on HSC data failed with the following:    {code}  [...]  818 sources in ccd 0  visit 29352  1428 sources in ccd 1  visit 29352  1322 sources in ccd 2  visit 29352  1202 sources in ccd 3  visit 29352  Photometric scatter (median) - SNR > 100.0 : 18.6 mmag  Traceback (most recent call last):    File ""/home/wmwv/local/lsst/validate_drp/bin/validateDrp.py"", line 98, in <module>      validate.run(args.repo, **kwargs)    File ""/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/validate.py"", line 104, in run      **kwargs)    File ""/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/validate.py"", line 205, in runOneFilter      photomModel = PhotometricErrorModel(matchedDataset)    File ""/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/photerrmodel.py"", line 235, in __init__      matchRef)    File ""/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/photerrmodel.py"", line 246, in _compute      fit_params = fitPhotErrModel(mag[bright], magErr[bright])    File ""/home/wmwv/local/lsst/validate_drp/python/lsst/validate/drp/photerrmodel.py"", line 132, in fitPhotErrModel      photErrModel, mag, mag_err, p0=p0)    File ""/ssd/lsstsw/stack_20170409/Linux64/miniconda2/4.2.12.lsst2/lib/python2.7/site-packages/scipy/optimize/minpack.py"", line 740, in curve_fit      raise RuntimeError(""Optimal parameters not found: "" + errmsg)  RuntimeError: Optimal parameters not found: Number of calls to function has reached maxfev = 800.  {code}    1. [X] Catch errors in computing things, recover, and do something reasonable.  2. [X] Make the fitting more robust.",2
"DM-10508","05/10/2017 13:41:19","Remove writing of warped template added in DM-8145","I accidentally committed a line added for debugging, which saved the warped template in ImagePsfMatchTask. Delete this line.",1
"DM-10511","05/10/2017 15:46:42","Apply Tim's comments to LDM-151","Apply comments",1
"DM-10512","05/10/2017 16:34:47","Upgrade IPython on the shared stack to >=5.2","The current version of IPython on the shared stack on lsst-dev is 5.1.0    Unfortunately, 5.1 introduced a nasty bug that makes it ~impossible to quit ipdb in any remotely sane manner. All versions >=5.2 fix this.    For details, see the first bullet point in the changelog for the 5.2 release on  http://ipython.readthedocs.io/en/stable/whatsnew/version5.html#ipython-5-0  ",1
"DM-10514","05/11/2017 10:03:41","Check qserv/qserv:dev works correctly","This container was broken, it will be re-generated and tested on Galactica and ccqservxxx.",2
"DM-10516","05/11/2017 11:57:11","Default Null type for Avro schema may be incorrect","The current Avro schemas in lsst-dm/sample-avro-alert (and also in ZTF's schemas) can give a warning when decoded in Spark using that says (e.g., shows up I think for all nullable fields):    {code}  [WARNING] Avro: Invalid default for field cutoutScience: null not a [{""type"":""record"",""name"":""cutout"",""namespace"":""ztf.alert"",""fields"":[{""name"":""fileName"",""type"":""string""},{""name"":""stampData"",""type"":""bytes"",""doc"":""jpeg""}]},""null""]  [WARNING] Avro: Invalid default for field cutoutTemplate: null not a [{""type"":""record"",""name"":""cutout"",""namespace"":""ztf.alert"",""fields"":[{""name"":""fileName"",""type"":""string""},{""name"":""stampData"",""type"":""bytes"",""doc"":""jpeg""}]},""null""]  {code}    This happens when I use Spark package com.databricks:spark-avro_2.11:3.2.0, create a SparkSession, and read data this way:    {code}  data = (sparkSession          .read          .format(""com.databricks.spark.avro"")          .load(""ztf/with-schema/ztf_avro_packets""))  {code}    This might be because null needs to be in quotes and it isn't?  I don't know.      This story is try quotes around ""null"" type in default type field and see if     1) the Spark warning goes away  2) data still serializes/deserializes with plain Python modules and nullable fields   ",1
"DM-10520","05/11/2017 14:43:12","""Restore to the defaults"" icon doesn't work properly","Build firefly from today's dev (May 11, 2017, the last commit is dd74beba73c8ab99bc8751b329bcf7e36627e591).  Open firefly, search image, target = m81, WISE and SDSS. Select WISE image. Change to single image mode. Zoom out the WISE image. Click the ""Restore to the defaults"" key: The WiSE image is restored to the original size but right after that the SDSS image pops up.   The restore key should only do the restoring job, not changing images.   ",3
"DM-10521","05/11/2017 16:07:28","Create script to produce release performance table","Take the JSON output of a {{validate_drp}} run and produce a report suitable for a Characterization Metrics Report.    See, e.g.,     https://pipelines.lsst.io/metrics/v13_0.html#metrics-v13-0    https://github.com/lsst/pipelines_lsst_io/blob/master/metrics/v13_0.rst    and the attachment for an example to how to format things.      ",3
"DM-10522","05/11/2017 16:32:28","Make dcrCoadds proper coadds","The dcrCoadd dataset type needs to be compatible with existing tools and functions that process coadds. ",3
"DM-10530","05/12/2017 14:29:28","don't set filter if the filter ID is not UNKNOWN (instead of testing if filter is None)","the filter is never None, but it can be not-set. In that case the Id is Filter::UNKNOWN.",1
"DM-10531","05/15/2017 11:08:16","Add DM-level policy on use of secure Web protocols to LDM-148","Implement the consensus decision on RFC-326 to state, as a DM policy, that all services facing the public Internet will use secure protocols (https:, in particular, for Web services), unless a specific technical justification for not doing so is accepted by the DM-CCB and the project ISO.",1
"DM-10532","05/15/2017 11:15:54","Pursue project-level policy on secure external protocols","We would like to recommend that the DM-level policy adopted under RFC-326 be promoted to a project-level policy.  This action is to begin work on that, which might take the form of submitting an LCR advocating adopting this as an OSS-level requirement, or as a separate policy document.    The basic policy proposed is that all services facing the public Internet should use secure protocols, unless a specific, technically justified exception is adopted via formal change control and with sign-off from the ISO.",2
"DM-10541","05/16/2017 09:36:29","Add properties to image classes","This is the most obvious and useful application of RFC-279:   - Add {{image}}, {{mask}}, and {{variance}} properties to {{MaskedImage}} (and {{Exposure}}, for convenience).   - Add a {{maskedImage}} property to {{Exposure}}.   - Add {{array}} properties to {{Image}} and {{Mask}}.    These will cut down on unnecessary characters and save us from having to make temporary variables in order to assign to an image component.  ",1
"DM-10542","05/16/2017 15:03:35","Replace XYTransform::linearizeTransform","Many clients of {{XYTransform}} use its methods {{linearizeForwardTransform}} and {{linearizeReverseTransform}}. We do not yet have analogous code for {{Transform}}. This functionality can be implemented as either a method on {{Transform}} or a separate function; the latter would provide better encapsulation.",2
"DM-10546","05/16/2017 23:28:28","meas.mosaic.updateExposure.applyMosaicResultsExposure will not if there is no mosaic solution","Furusawa-san pointed out that coaddDriver can produce outputs even if mosaicking has not been run though {{doApplyUberCal=True}} (which is the default value for HSC). This appears to be due to allowing null values of the {{wcs}} and {{ffp}} in {{updateExposure.py}}.    We should allow the functions in there to fail if the appropriate values are not available, or the user can be deceived as to what corrections have actually been applied.",1
"DM-10549","05/17/2017 10:02:38","Use all simulated peaks","One of the shortcomings of the NMF deblender is that undetected sources wreak havoc on detected sources, where the likelihood causes detected sources to account for the extra flux with no local peak to attribute it to.    [~pmelchior] and I have discussed methods to work around this problem but in the short term it will be easier to just use the exact locations of all of the objects from the simulated catalog, as opposed to only fitting the peaks detected by the pipeline.    This ticket will give the user the option of using a set of simulated peaks instead of the detected peaks.",3
"DM-10550","05/17/2017 10:10:24","Convert comparison text to plots","A lot of the data comparisons, like {{compareMeasToSim}} display a lot of information in text that would be easier to digest (and more compact) in the form of plots, which need to be created.",2
"DM-10551","05/17/2017 10:14:04","Display templates together","It will be useful to have a script that can compare peaks using several different deblender methods side-by-side.",2
"DM-10552","05/17/2017 10:28:51","Upgrade display_firefly to work with more servers","Implement changes to display_firefly to enable it to work with more servers, and fix some issues found in testing.    * Add a {{basedir}} parameter, to make use of the capability added in DM-9843, to allow the firefly backend to be used with the PDAC server and the public IRSA server.  * Pass keyword arguments to the FireflyClient constructor.  * Update the image and mask handling, in part for changes due to the pybind11 conversion, and to restore mask labeling.    In afw.display:    * Fix the \_\_getattr\_\_ method of the display interface  * Reorder tests so that displays are not closed prematurely",2
"DM-10553","05/17/2017 14:36:15","Correct simulation positions","There are two issues with the simulation positions, both of which are likely related.    The images created in DM-9644 are split into 4 quadrants, each of which is generated separately. The intensity values in the simulated tables are only the size of the quadrants, not the entire images,  so the x,y positions in the table do not correspond to the position of a source in it's intensity image.    The other issue is that the x,y positions in the catalog can be off by 1/2 pixel, which might depend on the quadrant chosen. In this case all of the positions will need to be adjusted accordingly.    This ticket will use the position of the source to determine the quadrant it is located in to adjust the x,y positions to the correct positions in the quadrant, and make any adjustments needed to properly align the sources with the image.",1
"DM-10557","05/18/2017 12:15:42","Use SingleFrameMeasurementTask to test deblender results","Because the simulated galaxies we are using to configure the deblender are extended with very large wings, comparing the deblended flux with the simulated flux is deceptively difficult. The problem is that a large percentage (as much as 50%) of the simulated flux can lie below the noise level, where it is truncated in the deblender, making it difficult to compare the total flux of each object.    It was recommended by [~jbosch] that running {{SingleFrameMeasurementTask}} on both the deblender output and simulated images will allow us to make a better comparison of the results.     This ticket will implement a set of tests using {{SingleFrameMeasurementTask}}.",2
"DM-10558","05/18/2017 13:08:47","disable or remove butler caching","the reused object aspect of caching (documented in LDM-463 draft) is causing confusion because shared objects can be mutated, and this is not what some developers would expect. Disable it and/or remove the code for now. If it's wanted, we can have an RFC or RFD about how to handle mutable objects.    Also update LDM-463 (remove the bit about caching)  ",1
"DM-10559","05/18/2017 15:05:49","afw.image.makeWcs() returns null pointer without warning","Line 49 of https://github.com/lsst/afw/blob/master/src/image/makeWcs.cc should log a warning before returning a None wcs, as it makes it hard to track things down without it.",1
"DM-10563","05/19/2017 08:31:26","Migrate to non-LSST deblender packages","Much of the new deblender's design was written by [~pmelchior] as part of a similar effort for WFIRST. This has included the creation of two new packages, one that uses proximal operators to solve minimization problems and another that implements an NMF deblender. Currently development is occurring on both the stack and the new packages, but the structure of the external packages is changing as we prepare for publication and it is becoming increasingly difficult to co-develop them.    Eventually all of the stack code will be refactored to optimize the NMF deblender, so to make it easier to share changes with [~pmelchior], {{meas_deblender}} should be modified _slightly_ to call the two external packages when applicable.  This amounts to removing {{proximal_nmf.py}} and instead making a call to the new deblender package, which has a nearly identical API and with some performance improvements, including an angular symmetry operator.    This ticket will strip out the unnecessary modules in {{meas_deblender}} and make calls to the new {{deblender}} and {{proxmin}} packages.",1
"DM-10574","05/22/2017 12:41:02","Hit AssertionError in deblender","In a validation run for hscPipe 5.0-beta5:  {code}  pprice@tiger-sumire:/tigress/pprice/hscPipe-5.0-beta5 $ measureCoaddSources.py /tigress/HSC/HSC --rerun hscPipe-5.0-beta5/20170517/wide --id tract=8764 patch=0,0 filter=HSC-I  root INFO: Loading config overrride file '/tigress/HSC/stack/hsc-tiger-20170322/Linux64/obs_subaru/13.0-18-gbe0b5c0/config/measureCoaddSources.py'  root INFO: Loading config overrride file '/tigress/HSC/stack/hsc-tiger-20170322/Linux64/obs_subaru/13.0-18-gbe0b5c0/config/hsc/measureCoaddSources.py'  root INFO: Running: /tigress/pprice/hscPipe-5.0-beta5/pipe_tasks/bin/measureCoaddSources.py /tigress/HSC/HSC --rerun hscPipe-5.0-beta5/20170517/wide --id tract=8764 patch=0,0 filter=HSC-I  /tigress/HSC/stack/hsc-tiger-20170322/Linux64/miniconda2/4.2.12.lsst1/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.    warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')  measureCoaddSources INFO: Read 7020 detections: DataId(initialdata={'filter': 'HSC-I', 'patch': '0,0', 'tract': 8764}, tag=set([]))  measureCoaddSources.deblend INFO: Deblending 7020 sources  measureCoaddSources FATAL: Failed on dataId=DataId(initialdata={'filter': 'HSC-I', 'patch': '0,0', 'tract': 8764}, tag=set([])):   Traceback (most recent call last):    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/pipe_base/13.0+10/python/lsst/pipe/base/cmdLineTask.py"", line 347, in __call__      result = task.run(dataRef, **kwargs)    File ""/tigress/pprice/hscPipe-5.0-beta5/pipe_tasks/python/lsst/pipe/tasks/multiBand.py"", line 1086, in run      self.deblend.run(exposure, sources)    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/pipe_base/13.0+10/python/lsst/pipe/base/timer.py"", line 121, in wrapper      res = func(self, *args, **keyArgs)    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/deblend.py"", line 238, in run      self.deblend(exposure, sources, psf)    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/pipe_base/13.0+10/python/lsst/pipe/base/timer.py"", line 121, in wrapper      res = func(self, *args, **keyArgs)    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/deblend.py"", line 323, in deblend      medianSmoothTemplate=self.config.medianSmoothTemplate    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/baseline.py"", line 667, in deblend      debResult = newDeblend(debPlugins, footprint, maskedImage, psf, psffwhm, filters, log, verbose, avgNoise)    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/baseline.py"", line 739, in newDeblend      reset = debPlugins[step].run(debResult, log)    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/plugins.py"", line 76, in run      reset = self.func(debResult, log, **self.kwargs)    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/plugins.py"", line 152, in fitPsfs      dp.img, dp.varimg, psfChisqCut1, psfChisqCut2, psfChisqCut2b, tinyFootprintSize)    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/plugins.py"", line 371, in _fitPsf      sx1, sx2, sx3, sx4 = _overlap(xlo, xhi, obb.getMinX(), obb.getMaxX())    File ""/tigress/HSC/stack/hsc-tiger-20170322/Linux64/meas_deblender/13.0-3-gdadfd13+1/python/lsst/meas/deblender/plugins.py"", line 319, in _overlap      (xlo <= xhi) and (xmin <= xmax))  AssertionError  {code}    After a bit of poking, I believe it's due to the use of {{LOCAL}} [here|https://github.com/lsst/meas_deblender/blob/master/python/lsst/meas/deblender/plugins.py#L282].",1
"DM-10575","05/22/2017 13:59:05","obs_decam build takes 10 minutes","Running under Jenkins:  {code}             obs_decam: 13.0-12-ga2ce546+35 ...........................................................................................................................................................................................................................................................................................................ok (638.1 sec).  {code}    I believe this is due to running {{ProcessCcdTask}} within the {{TestCase.setUp}}; it should be in the {{TestCase.\_\_init\_\_}} so it only gets run once.",1
"DM-10576","05/22/2017 14:19:00","Propagate information from qserv exceptions in dbserv","QServ throws non-standard MySQL exceptions. mysqlclient/MySQLdb doesn't propagate errors higher than a predefined number, so it returns an error code of -1 with message ""Totally whack"" The goal of this issue is to propagate the errno and error in that exception.",1
"DM-10594","05/23/2017 13:52:35","Investigate apparent slow-down of ci_hsc","ci_hsc is now taking over an hour to run, when it used to take around half an hour. Why?",2
"DM-10597","05/23/2017 17:36:14","firefly_client upload_data test fails with Python 2","The FireflyClient upload_data  and upload_file methods includes a test upload of a small number of bytes using io.StringIO. StringIO must be given a Unicode string, which is not the default for strings in Python 2.    The fix is to explicitly specify Unicode, e.g. io.StringIO(u'test'). ",1
"DM-10601","05/24/2017 10:11:25","Add reStructuredText documentation to display_firefly","Add documentation in reStructuredText format to the display_firefly package, in a doc/ subdirectory. The documentation will include setup of the afw.display interface, including the host, port, etc. parameters that are specific to display_firefly. Other points (like known issues) unique to display_firefly will be covered.    display_firefly is not yet included with lsst_apps or lsst_distrib. Initially, an Installation section will be included. It is expected to replace the contents with links to centralized pages on how to install additional packages against a core stack installation.    Copied for pull request  https://github.com/lsst/display_firefly/pull/7  * Add doc/_static placeholder README  * setup initial files following validate_base as template  * add installation, start tutorial  * ignore eupspkg directory  * add setup of display_firefly for Python API ref  * add introduction, installation content  * fill out rest of Using... section  * Pin Sphinx<1.6.0  * Various reStructuredText formatting fixes  * reformat Makefile  * Merge branch 'tickets/DM-11017' into tickets/DM-10601",5
"DM-10603","05/24/2017 16:42:23","Outline for Science Platform design","Prepare an outline for Science Platform design document",2
"DM-10612","05/25/2017 07:28:52","Implement strict monotonicity","The current deblender uses an update scheme similar to ADMM, where constraints are not strictly enforced in any given step but instead the solution converges to meet the constraint. It should be possible to make monotonicity a proximal operator applied to the likelihood gradient, as opposed to a linear constraint, which will force a strict monotonic solution.    This ticket will be an attempt to add the option of using strict monotonicity in the deblender.",3
"DM-10613","05/25/2017 07:35:56","Implement smoothness constraint","When using images not convolved with the PSF, we've long known that ""snowflakes"" or spiky fractal-like structures are created due to the monotonicity operator. It is likely that a smoothness operator, which is used in compressive sensing applications, will be able to fix (or at least suppress) this problem.",3
"DM-10615","05/25/2017 07:59:03","Convolve to common PSF","The current PSF operator in the new deblender uses the PSF for the image in each band to attempt to deconvolve the image to a single pixel. This appears to be causing issues with deblending the wings of faint extended objects that are not seen when using PSF-matched images and no PSF operator (see attachments for an example).    It is thought that convolving the image in each band to a common PSF (such as the best seeing)  will remove the artifacts that the current PSF convolution operator produces.",8
"DM-10617","05/25/2017 09:36:30","Fix ndarray type casters to work with nullptr (i.e. None) arguments","In contrast to the regular pybind11 type casters the ndarray ones don't work well with {{nullptr}}.  E.g. the function {{void testOptionalArray(ndarray::Array<double, 1, 1> * arr = nullptr)}} should accept {{None}} as a default argument, but it does not.",2
"DM-10621","05/25/2017 11:57:55","ArgumentParser's butler doesn't output calibs in the calib storage","Following DM-8686 (between tagged weeklies {{w_2017_11}} and {{w_2017_12}}), the {{constructBias.py}} (etc.) scripts in pipe_drivers write their outputs in the output storage rather than in the calib storage (e.g., {{/tigress/pprice/ci_hsc/DATA/rerun/ci_hsc/BIAS/2017-12-34/NONE/BIAS-2017-12-34-012.fits}} instead of {{/tigress/pprice/ci_hsc/DATA/rerun/ci_hsc/CALIB/BIAS/2017-12-34/NONE/BIAS-2017-12-34-012.fits}}).    This appears to be due to the output repo being unaware of calibs because it's not being told about them by the {{ArgumentParser}} in pipe_base.    This was originally reported by Furusawa-san at NAOJ.",1
"DM-10623","05/25/2017 13:09:48","Mismatching dataId in logger output","When running {{processCcd.py}} on multiple visits, self-contradictory outputs are generated by the logger. For example:    {noformat}  INFO  2017-05-25T00:07:04.004 processCcd (DataId(initialdata={'visit': 242618039}, tag=set([])))(processCcd.py:181)- Processing DataId(initialdata={'visit': 242619360}, tag=set([]))  {noformat}    and    {noformat}  FATAL 2017-05-25T00:07:04.004 processCcd (DataId(initialdata={'visit': 242713680}, tag=set([])))(cmdLineTask.py:351)- Failed on dataId=DataId(initialdata={'visit': 242715702}, tag=set([])): Unable to match sources  {noformat}    The problem above is that the visit numbers don't match, but there should be no way for this to ever be the case, as far as I can see. FWIW, the second number is the true visit number, at least in my case.    This is only noticeable when {{--longlog}} is used, to allow disentangling of parallel processed data, but I don't know whether that means the logger is also wrong when sequentially running multiple visits on a single core (I suspect it's OK in that instance, but I'm not certain).    This bug is _bad_ for tracking down which visits are failing and why.",1
"DM-10633","05/26/2017 09:06:55","Increase CmdLineTask multiprocessing timeout","The timeout in {{CmdLineTask}} catches a lot of people unawares, and causes lots of confusion and annoyance. It's necessary (because of that python {{multiprocessing}} bug) to have _some_ value, but as soon as we stick a value in people manage to exceed it. [~mfisherlevine] had a good idea: we make the value a number _per target_, which will make it harder to exceed when people throw lots of data at {{-j}}. That just means calling {{getTargetList}} a bit earlier.",1
"DM-10686","05/30/2017 07:47:40","RingsSkyMap.findAllTracts() behaves oddly at poles","Sogo Mineo writes:  {quote}  I found that RingsSkyMap.findAllTracts() behaves oddly at poles.    The docstring of RingsSkyMap.findAllTracts(self, coord) says it returns a list:  https://github.com/lsst/skymap/blob/master/python/lsst/skymap/ringsSkyMap.py    When coord is IcrsCoord((0, -90), degrees), however, findAllTracts() returns not a list but a TractInfo. (line 163).    When coord is IcrsCoord((0, 90), degrees), it raises an error because line 166 ""return self[-1]"" is wrong. It has to be ""return self[len(self)-1]"".  {quote}",1
"DM-10690","05/30/2017 11:28:06","SQuaRE SuperTask Collaboration Week of 2017-04-10","Bucket ticket to cover non-specific SuperTask collaboration activities",1
"DM-10691","05/30/2017 11:30:34","SQuaRE SuperTask Collaboration Week of 2017-04-17","Bucket ticket to cover non-specific SuperTask collaboration activities",1
"DM-10737","05/30/2017 15:06:48","Make meas_mosaic use new reference catalogs by default","DM-2186 broke meas_mosaic (not surprising since it isn't in CI), and it looks like the right fix is to move the configuration that makes us use the new reference catalogs instead of astrometry.net into meas_mosaic itself, since there's no point having an override in obs_subaru when only obs_subaru uses meas_mosaic.  ",0.5
"DM-10748","05/31/2017 12:45:00","Test meas_mosaic pybind11-related fix by RC data processing  ","Test the corrections made in {{meas_mosaic}} in DM-10688 by running a full HSC-RC run with the current stack to see if the ellipticity residuals are better.",2
"DM-10760","06/01/2017 10:03:51","Switch warpType from enum to strings","DM-8491 added some code that enumerated warpTypes as enum.Enums.  This third party package is not an official dependency. Backing that out. ",0.5
"DM-10764","06/01/2017 14:18:20","Rename Transform::of and Mapping::of to ::then","astshim.Mapping and afw.geom.Transform both support concatenation using {{of}}. However, we normally specify transforms as ""aToB"" (e.g. pixelsToSky), resulting in this awkward notation:  {code}  aToC = bToC.of(aToB)  {code}  To make code easier to read I propose renaming ""of"" to ""then"" and swapping the order of concatenation, resulting in the following:  {code}  aToC = aToB.then(bToC)  {code}  This also follows the constructor order for SeriesMapping:  {code}  aToC = SeriesMapping(aToB, bToC)  {code}    I propose to do this soon, while Transform is still not used in the DM stack.",2
"DM-10769","06/01/2017 15:15:34","Astropy integration for F17","It has been deemed useful to integrate with astropy where possible.  In particular, we should integrate with NDData.",40
"DM-10777","06/01/2017 17:33:14","Create TransformBoundedField","Jointcal's improved photometric fit is done on the full focal plane, and thus we need its persisted output to be able to convert pixel coordinates on one CCD to the focal plane. Probably the easiest way to implement this is to create an {{astBoundedField}} that we can put into a {{PhotoCalib}}.    We may eventually want to replace {{PhotoCalib}}'s use of {{BoundedField}} internally with AST, but for consistency with [~jbosch]'s {{meas_mosaic}} work, keeping the {{PhotoCalib}} interface the same is probably best.    For reference, jointcal's photometric output looks something like {{f(g(x,y))*h(x,y)}}, where {{(x,y)}} are in pixel coordinates, {{g}} is the pixel to focal plane transformation, {{f}} is a Chebyshev polynomial, and {{h}} is some chip-level transform (currently a constant).    Tagging this PairCoding, as it might be nice for me to sit with [~rowen] as he works on it, to aid my familiarity with AST.",8
"DM-10778","06/01/2017 17:50:29","Add metadata access for Filter","Please add metadata access for {{Filter}} objects via the same pattern used to implement {{DM-9060}}.  I hope this is essentially trivial.",5
"DM-10779","06/01/2017 18:06:56","Implement running time metric(s)","By the All Hands Meeting, we should be able to demonstrate verification metrics by reporting running times (wall-clock) for AP pipeline stages. This ticket covers writing code for applying the {{verify}} framework to inline tests, profiling each stage, and supporting SQuaSH export (though the export itself is the responsibility of {{verify_ap}}).    See DM-11118 for a list of pipeline stages. This ticket may report metrics at a finer-grained level than is presented there.",8
"DM-10782","06/02/2017 08:36:57","Add bright star masks to ci_hsc","We're running production with bright star masks, so those should also be in the integration test. Otherwise there's the possibility that production fails when ci_hsc passes.",1
"DM-10785","06/02/2017 09:19:38","setBrightObjectMasks does not properly construct footprint. ","This causes coaddDriver to fail when bright object masks are turned on, e.g.     {noformat}  Traceback (most recent call last):    File ""/software/lsstsw/stack/Linux64/ctrl_pool/13.0-4-g98e8074+2/python/lsst/ctrl/pool/parallel.py"", line 495, in logOperation      yield    File ""/software/lsstsw/stack/Linux64/pipe_drivers/13.0-5-g7e5c55c/python/lsst/pipe/drivers/coaddDriver.py"", line 261, in coadd      coaddResults = self.assembleCoadd.run(patchRef, selectDataList)    File ""/software/lsstsw/stack/Linux64/pipe_base/13.0-5-g0e05785/python/lsst/pipe/base/timer.py"", line 121, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack/Linux64/pipe_tasks/13.0-31-gbcfe521/python/lsst/pipe/tasks/assembleCoadd.py"", line 357, in run      self.setBrightObjectMasks(coaddExp, dataRef.dataId, brightObjectMasks)    File ""/software/lsstsw/stack/Linux64/pipe_tasks/13.0-31-gbcfe521/python/lsst/pipe/tasks/assembleCoadd.py"", line 788, in setBrightObjectMasks      foot = afwDetect.Footprint(center, radius, exposure.getBBox())  TypeError: __init__(): incompatible constructor arguments. The following argument types are supported:      1. lsst.afw.detection._footprint.Footprint(inputSpans: lsst.afw.geom.spanSet.SpanSet, region: lsst.afw.geom.box.Box2I=Box2D(minimum=Point2I(0, 0), dimensions=Extent2I(0, 0)))      2. lsst.afw.detection._footprint.Footprint(inputSpans: lsst.afw.geom.spanSet.SpanSet, peakSchema: lsst.afw.table.schema.schema.Schema, region: lsst.afw.geom.box.Box2I=Box2D(minimum=Point2I(0, 0), dimensions=Extent2I(0, 0)))      3. lsst.afw.detection._footprint.Footprint(arg0: lsst.afw.detection._footprint.Footprint)      4. lsst.afw.detection._footprint.Footprint()    Invoked with: Point2I(30655, 33041), 51.792857142857095, Box2D(minimum=Point2I(27900, 31900), dimensions=Extent2I(4200, 4100))  {noformat}    This should be covered in a unit test as well.  Probably a good idea to do a quick scan for similar issues elsewhere too.",1
"DM-10795","06/02/2017 11:31:14","Outline L1 Minimum Viable System","Outline and prioritize components required for end-to-end testing of L1 alert generation--most immediately to enable verify_ap development (DM-9676 and others).",3
"DM-10799","06/02/2017 15:49:42","Rename Transform::tranForward to applyForward","The {{tranForward}} and {{tranInverse}} methods of {{Transform}} go against the LSST convention discouraging abbreviations. Rename them to {{applyForward}} and {{applyInverse}}, respectively.",0.5
"DM-10800","06/02/2017 16:12:11","Merge matcherSourceSelector and matcherPessimisticSourceSelector","DM-9751 created a new source selector that inherits from matcherSourceSelector called matcherPessimisticSourceSelector. This source selector is a stop gap measure to preserve the behavior of matcherSourceSelector for matchOptimisticB. If the new matchPessimisticB is adopted from a future RFC as the default matcher this ticket will merge the two source selectors and matcherSourceSelector with take on the behavior of matcherPessimisticSourceSelector.",1
"DM-10805","06/02/2017 18:33:25","Spatially-varying ZOGY option","Use ImageMapReduce to implement a spatially-varying A&L decorrelation task. Integrate that task as an option in imageDifference.py.    Will possibly want to implement it as a subclass of ImagePsfMatchTask, because that task includes wrapper code for warping, which will be necessary here.",8
"DM-10807","06/02/2017 19:10:01","Create command line interface (CLI) for ImgServ","This ticket will de-couple the REST interface, and potentially the VO API layer as well, from ImgServ Core, which will then become a library, thus enabling greater flexibility for change the web front ends while providing scalability to handle service requests. After completion, be able to perform a simple VO operation:  is service available? ",13
"DM-10809","06/05/2017 07:54:13","Release LTD Mason with pinned ruamel.yaml version 0.14","Create a new release of LTD Mason with the version of ruamel.yaml pinned to prevent future version incompatibilities.    See https://github.com/lsst-sqre/ltd-mason/pull/5 :    bq. There will be API changes in the 0.15+ versions, that might lead to warnings that your users could see. Therefore please release a version of your package with this change, so that it will not automatically take the latest ruamel.yaml release.",0.5
"DM-10819","06/05/2017 13:16:09","Define Endpoint equality","To allow for future development, {{BaseEndpoint}} should have an equality operator that accepts another {{BaseEndpoint}}, possibly with different template parameters, and returns true iff the two {{Endpoints}} have the same implementation class and the same (implementation-specific) state.    Implementations and test cases must be carefully designed to avoid substitution-principle-related issues. At worst, this may require imposing a convention that all concrete subclasses of {{BaseEndpoint}} must be final.",2
"DM-10828","06/06/2017 10:22:15","Generate comparison plots for the simulated deblender data","Run the deblender on all of the simulated images and compare the results to the truth, and to the results from the old deblender.",5
"DM-10831","06/06/2017 11:27:08","Histogram in multi-trace chart","We need to migrate Firefly Histogram (with the bind calculated on the server) to multi-trace chart architecture. This includes options and table connections.     Since histogram parameters do not map exactly to Plotly bar chart attributes (we use variable width bars to represent bins), this would be a good addition to the multi-trace charts proof-of concept.    Other items that should be resolved by this ticket   - Separating firefly specific things from plotly trace data  - Not storing fetchData function in tablesource, it's preferable to store a string id that can be resolved into this function  ",8
"DM-10837","06/06/2017 16:37:58","Fix eimageIsr import","`eimageIsr.py` in `obs_lsstSim` imports functions from `ip_isr` with `from lsst.ip.isr import isr`, but this only imports functions inside `isr.cc`. This import statement should be changed to `import lsst.ip.isr as isr` for the rest of the code in the module to work.",0.5
"DM-10838","06/06/2017 17:30:20","Expose channel and connid from WebsocketClient","A {{firefly_client.FireflyClient}} instance can be connected to a Firefly widget when the {{channel}} and {{connID}} are known. The {{channel}} and {{connID}} are logged to the browser Javascript console when the {{connect}} method from the {{firefly_widgets}} package is invoked in a notebook. The requested improvement is to expose the function that returns {{channel}} and {{connID}} as an API.     The benefit would be that the functionality of the Python API could be used on widget instances.    ",1
"DM-10839","06/06/2017 19:09:39","Implement missing features of command line SuperTask activator","Missing features include:  - specifying multiple SuperTasks on command line  - loading Pipeline from serialized format  - support for ""show"" command    Multi-task support may need rethinking of UI for command line.",8
"DM-10842","06/07/2017 08:50:12","RFC new design for afw::math::Statistics","Make sure there are no overwhelming community objections to the design & requirements outlined in DMTN-043",3
"DM-10853","06/07/2017 11:26:50","SUIT portal integration with PDAC v2 Science Platform ","We will start integrate the SUIT portal into the science platform.   Major features will be:  * Enable use login  * accessing user workspace (storage space) from portal   *  ",40
"DM-10854","06/07/2017 11:36:46","User workspace access API ","We need to decide the API to use to access user workspace (storage).  DAX team will implement a WedDAV API first, VOSpace API will be implemented later.    Work with DAX team during their implementation phase on the issues coming up.         [https://jira.ipac.caltech.edu/browse/FIREFLY-94]",8
"DM-10871","06/08/2017 04:34:56","Add unit test for MDC overwriting","DM-10623 does not add unit test for the piece of functionality that it altered, would be nice to add one. ",1
"DM-10903","06/09/2017 09:23:21","Update DMTN-23 tutorial to work with recent versions of the stack","I'm planning to publicize this document at the Lyon meeting next week as a part of a tutorial session on how to use the stack, but it has some instructions that no longer work.    I know [~jsick] has been working through this and fixing things (and probably putting it in a new, more permanent location), but I don't want to assume anything about that yielding results by next week.",1
"DM-10908","06/09/2017 18:49:45","Refactor Butler instantiation code in ImgServ","Pull Butler instantiation code further up in imgserv, so it can be done externally to the ImgServ library when the ImgServ library is split into its own module (will enable use to cache Butlers in the Flask/CLI layers for improved performance)",5
"DM-10919","06/13/2017 10:08:34","Add WBS breakdown to LDM-294","Update LDM-294 so that, given a textual description of the WBS, it:    - Automatically includes that in the document  - Associates each WBS element with a set of products from {{productlist.csv}}",1
"DM-10920","06/13/2017 10:39:25","Improve metasrc's parsing of latex for metadata","DM's LaTeX authors make use of whitespace in command invocations that weren't anticipated by metasrc's regular expressions. For example:    {code}  \title    [Test Plan]  { \product ~Test Plan}  {code}    This ticket adds flexibility to the regular expressions to work with this whitespace.    *Eventually we really need to replace the regular expressions with a fully-fledged TeX tokenizer.*",0.5
"DM-10921","06/13/2017 10:40:53","Deploy LDM-503 to LSST the Docs","Deploy LDM-503 to ldm-503.lsst.io using Lander.",0.5
"DM-10922","06/13/2017 11:12:24","Outline necessary metrics for verify_ap","Outline range of metrics needed in verify_ap.",3
"DM-10926","06/13/2017 17:14:27","Incompatibility with NumPy 1.13","Per [this report on CLO|https://community.lsst.org/t/difficulty-installing-on-python-3-6/1961], our test suite fails when run against NumPy 1.13.0:    {code}  File ""tests/testSimpleTable.py"", line 346, in testExtract  self.assertFloatsEqual(d[""a_b_c2""], catalog.get(""a_b_c2"")[idx])  File ""/home/mnewsome/envs/lsst_maf/lsst/Linux64/utils/13.0/python/lsst/utils/tests.py"", line 652, in assertFloatsEqual  return assertFloatsAlmostEqual(testCase, lhs, rhs, rtol=0, atol=0, **kwargs)  File ""/home/mnewsome/envs/lsst_maf/lsst/Linux64/utils/13.0/python/lsst/utils/tests.py"", line 568, in assertFloatsAlmostEqual  diff = lhs - rhs  TypeError: numpy boolean subtract, the - operator, is deprecated, use the bitwise_xor, the ^ operator, or the logical_xor function instead.  {code}    This has been promoted from a {{DeprecationWarning}} to a {{TypeError}} as of [NumPy 1.13.0|https://github.com/numpy/numpy/releases/tag/v1.13.0], which was recently released.",1
"DM-10931","06/14/2017 17:00:10","Fix variable name bug and remove print statements in matchPessimisticB.","A print statement that was inserted for debugging should be removed from the stack version of the code.    The empty struct returned by the PessimisticPattern matcher class has an incorrect name for distances_rad which was introduced in DM-9751.",1
"DM-10934","06/14/2017 17:30:28","Fix ""Sigma"" and ""Err"" in change controlled DM docs","Find all uses of {{Err}} and {{Sigma}} in the version controlled DM documents (hopefully just LDMs?), determine if they are correct or not, and fix the ones that are incorrect.    How many LDMs are there (of order 20 on the page below), and where do they all live? This document is a good start at finding them:    https://confluence.lsstcorp.org/display/DM/Requirements+and+Design+Hierarchy    This git repo should encompass all the important ones:    https://github.com/lsst-dm/dm-docs.git",1
"DM-10937","06/14/2017 18:04:36","Update ICD tracking sheet for DM","Based on a request from Brian Selvy to all subsystems, update the ICD status tracking sheet Document-19424 to reflect appropriate need dates, issues, etc.",2
"DM-10943","06/15/2017 12:49:06","References to TranForward and TranInverse are confusing","Various doc strings reference the {{Mapping}} attributes {{TranForward}} and {{TranInverse}}. However, these are obtained using {{hasForward}} and {{hasInverse}}, so much of this documentation is confusing. Clean this up.",0
"DM-10945","06/15/2017 14:11:30","update eups to 2.1.3 to improve tarball package installation","{{newinstall.sh}} should be updated to use eups 2.1.3 to reduce the time required to install binary tarballs.  {{lsstsw}} should be updated at the same time to keep our build envs in sync.",2
"DM-10947","06/15/2017 14:49:02","Allow linearizeTransform and affineTransform to simplify their mappings","The current implementation of {{geom::linearizeTransform}}, and the initial implementation for an AffineTransform -> Transform conversion factory, use unsimplified mappings internally to work around AST bugs. This makes them unneccessarily slow, particularly {{linearizeTransform}}, which composes three mappings at present. Simplification should be added once the bugs in question are fixed.",1
"DM-10953","06/16/2017 12:04:04","Give  ModelPsfMatchTask ablilty to match to all PSF types","Ticket named "" 'Psf' object has no attribute 'resized'"" as filed.     {{ModelPsfMatchTask}} calls {{Psf.resized}}, but that method is only defined for certain specialised {{Psf}} classes ({{SingleGaussianPsf}} and {{DoubleGaussianPsf}}). It should have a default implementation, and be defined for {{CoaddPsf}}.    {code}   > python modelPsfMatchTask.py --template 'data/SDSSJ0920+0034/calexp-HSC-I-9564-7,3.fits' --science 'data/SDSSJ0920+0034/calexp-HSC-R-9564-7,3.fits'    psfMatch INFO: compute Psf-matching kernel  psfMatch INFO: Adjusting dimensions of reference PSF model from (43, 43) to (41, 41)  Traceback (most recent call last):    File ""modelPsfMatchTask.py"", line 160, in <module>      run(args)    File ""modelPsfMatchTask.py"", line 130, in run      result = psfMatchTask.run(templateExp, scienceExp)    File ""modelPsfMatchTask.py"", line 45, in run      return ModelPsfMatchTask.run(self, scienceExp, templateExp.getPsf())    File ""/Users/aisun/anaconda/envs/lsst/opt/lsst/pipe_base/python/lsst/pipe/base/timer.py"", line 121, in wrapper      res = func(self, *args, **keyArgs)    File ""/Users/aisun/anaconda/envs/lsst/opt/lsst/ip_diffim/python/lsst/ip/diffim/modelPsfMatch.py"", line 274, in run      result = self._buildCellSet(exposure, referencePsfModel)    File ""/Users/aisun/anaconda/envs/lsst/opt/lsst/ip_diffim/python/lsst/ip/diffim/modelPsfMatch.py"", line 388, in _buildCellSet      referencePsfModel = referencePsfModel.resized(lenPsfScience, lenPsfScience)    File ""/Users/aisun/anaconda/envs/lsst/opt/lsst/afw/python/lsst/afw/detection/detectionLib.py"", line 3745, in <lambda>      __getattr__ = lambda self, name: _swig_getattr(self, Psf, name)    File ""/Users/aisun/anaconda/envs/lsst/opt/lsst/afw/python/lsst/afw/detection/detectionLib.py"", line 89, in _swig_getattr      raise AttributeError(""'%s' object has no attribute '%s'"" % (class_type.__name__, name))  AttributeError: 'Psf' object has no attribute 'resized'  {code}    Bug reported by [Ai-Lei Sun|mailto:alsun@asiaa.sinica.edu.tw].",8
"DM-10965","06/19/2017 10:24:07","FootprintSet setter unable to accept results from getter","A bug in afw FootprintSets was reported where the returned value from getFootprints could not be used in the function call setFootprints as shown below. Success of this ticket will be when the output can be used as an input.      {code:python}  (Pdb) footprintSet.setFootprints(footprintSet.getFootprints())  *** TypeError: setFootprints(): incompatible function arguments. The following argument types are supported:      1. (self: lsst.afw.detection._footprintSet.FootprintSet, arg0: std::vector<std::shared_ptr<lsst::afw::detection::Footprint>, std::allocator<std::shared_ptr<lsst::afw::detection::Footprint> > >) -> None    Invoked with: <lsst.afw.detection._footprintSet.FootprintSet object at 0x7fdc226da928>, [<lsst.afw.detection._footprint.Footprint object at 0x7fdc226dac38>, <lsst.afw.detection._footprint.Footprint object at 0x7fdc226dadf8>, <lsst.afw.detection._footprint.Footprint object at 0x7fdc226dac70>]  {code}  ",1
"DM-10973","06/19/2017 15:28:22","Make SkyWcs transform to IcrsCoord instead of SpherePoint","At present {{SkyWcs}} is a subclass of {{Transform<Point2Endpoint, SpherePointEndpoint>}} and this means that {{pixelToSky}} transforms to {{SpherePoint}}. However {{SkyWcs}} is always normalized to ICRS and so {{pixelToSky}} should probably produce {{IcrsCoord}}.    The simplest way to change this is to change {{SpherePointEndpoint}} to {{IcrsCoordEndpoint}}. One obvious alternative is to keep both and change the endpoint class {{SkyWcs}} uses. However, we have no identified need for {{Transform}} to transform to {{SpherePoint}} (nor {{IcrsCoord}} except as a base class for {{SkyWcs}}). Furthermore if we have N {{Endpoint}} classes then we instantiate N^2 {{Transform}} classes. So I propose to simply rename for now.",5
"DM-10974","06/19/2017 15:31:30","Ensure linearity is being applied in ISR to decam","Linearization was implemented in DM-6356. Make sure that linearity is still applied given the full decam ISR task.",2
"DM-10975","06/19/2017 15:48:29","Update input repositories and config for Gaia","* Update lsst-dev butler repository to include Gaia ref cat (my be a no-op).  * Update DMTN instructions, if necessary.  * Update configs used by the prototype to use Gaia as the astrometric ref and PS1 (or SDSS) as the photometric ref",2
"DM-10976","06/19/2017 16:03:55","Get HiTS 2014 data","* Get HiTS data for 2014 fields matching 2015 fields for the prototype AP system (DM-7925), to be used for template generation  * Once the dataset exists, make a git-lfs repo for it and/or add it to /datasets on lsst-dev (along with the 2015 field data to be used as ""science"").  This should adhere to the layout defined in DM-11116.",5
"DM-10977","06/19/2017 16:10:57","Share ImageSelector from Twinkles","Bring the good seeing ImageSelector from the Twinkles branch into the {{decam_hits}} repository.  Add a {{goodSeeingCoadd}} config parameter to use it.",2
"DM-10981","06/19/2017 16:47:58","Schedule and implement a conversation with DAX about L1 DB","We need some sort of mocked up interface to the level 1 database.  Specifically, this requires the ability to retrieve DIAObjects based on spatial position and the ability to put potentially updated versions of those DIAObjects back from multiple threads.    Ideas were to either use a sqlite database and interact directly through SQL, but it might be informative to have a shimmed butler interface to do this interaction.    In either case, the schema are defined and reside in the {{cat}} package.",3
"DM-10982","06/19/2017 16:49:09","Ensure that Firefly package's JSDoc content is updated on firefly.lsst.io","https://firefly.lsst.io/ and https://firefly.lsst.io/builds/ seem to exist, but they have not received any new content since March.  We need to be clear about what updates we expect to go to this site - all updates to the {{dev}} branch?    Decision: the JSDoc is in sync with github master branch.",2
"DM-10983","06/19/2017 16:58:42","Get Python docstring content from firefly_client published to firefly-client.lsst.io","At the ""Firefly documentation mini-workshop"" it was agreed that the Firefly Python API in the firefly_client package should be published on a {{firefly-client.lsst.io}} mini-site using [~jsick]'s standard technology.    Docstring content is available in that package but the site doesn't seem to be active.  This ticket is about getting the site up and running.  ",3
"DM-10984","06/19/2017 17:02:51","Write ""user guide"" documentation for firefly_client","In addition to the existing Python docstring documentation in {{firefly_client}}, it would be desirable to have ""user guide""-type documentation, including examples, available.",5
"DM-10998","06/20/2017 17:01:49","buildbot-scripts cleanups","There are a couple of small issues with {{lsst-sqre/buildbot-scripts}} that could be improved, including:    * Inconsistent use of true/false, 0/1, and ""yes""/""no"" for boolean values.  * printing of variables that are no longer in use (DEMO_ROOT & DEMO_TGZ).  * inconsistent use of posix and bash style conditional expressions",0.5
"DM-11000","06/20/2017 23:26:17","Fix pyyaml build problem in Qserv base container","The recent pyyaml change to build on top of libyaml busted the Qserv base container builds.      It looks like a necessary -L option for libyaml is missing of the CFLAGS additions in the build() override in the eupspkg.cfg.sh; in miniconda installs this goes unnoticed since the lib is picked up from miniconda.  Since the Qserv base containers are non-miniconda builds, they broke.",1
"DM-11002","06/21/2017 09:34:38","Simplify Git LFS documentation in the Developer Guide","With the Git LFS updates, the Git LFS documentation (https://developer.lsst.io/tools/git_lfs.html) as written seems to be unclear. This ticket will clean that up.",0.5
"DM-11009","06/21/2017 13:51:59","Automatic backtrace printing is unhelpful","All in all, I think this could be more useful:     {code}  [swinbank@lsst-dev01 ~]$ . /ssd/lsstsw/stack/loadLSST.bash  Note: do not use devtoolset-3 in conjunction with this stack.  [swinbank@lsst-dev01 ~]$ setup lsst_apps  [swinbank@lsst-dev01 ~]$ eups list -s lsst_apps     13.0-2-gcc7d151+16  w_2017_24 current setup  [swinbank@lsst-dev01 ~]$ cat > backtrace.py  import lsst.afw.image as afwImage  afwImage.ImageF(None)  [swinbank@lsst-dev01 ~]$ python backtrace.py   Caught signal 11, backtrace follows:  terminate called after throwing an instance of 'std::regex_error'    what():  regex_error  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Caught signal 6, backtrace follows:  terminate called recursively  Segmentation fault  {code}",1
"DM-11010","06/21/2017 17:27:57","Footprint.transform may be transforming the wrong position","{{Footprint.transform}} contains the following code:  {code}  std::shared_ptr<Footprint> Footprint::transform(geom::XYTransform const& t, ...      ...      // now populate the new Footprint with transformed Peaks      for (auto const& peak : getPeaks()) {          // Transform the x y Point          auto newPoint = t.forwardTransform(geom::Point2D(peak.getFx(), peak.getFx()));          newFootprint->addPeak(newPoint.getX(), newPoint.getY(), peak.getPeakValue());      }  {code}    I strongly suspect it should be transforming Fx, Fy instead of Fx, Fx. If so, the code would be simpler and less error-prone as:  {code}  auto newPoint = t.forwardTransform(peak.getF());  {code}    In the same vein: a variant of {{addPeak}} that took a {{Point2D}} for the position would be a useful addition.",1
"DM-11013","06/22/2017 14:06:04","SUIT needs to throw out a more meaningful error message when DAX has connection issue","When DAX has problem as today (6/22/17), select the table ""Forced photometry based on i-band coadds"" table from ""SDSS Stripe82"" in http://localhost:8080/firefly/lsst-pdac-triview.html;a=layout.showDropDown, in the metadata table area SUIT throws out this error message:    ""Catalog Fetch Error: edu.caltech.ipac.firefly.server.query.DataAccessException: DataAccessException:ERROR:DAX Error: OperationalError from:unknown""    We need to improve the error message, following exception convention implemented by Trey in ticket DM-10560, for our users.",2
"DM-11016","06/22/2017 18:20:46","Add standard library tag file to Doxygen","cppreference.com provides a [tag file|http://en.cppreference.com/w/Cppreference:Archives#Doxygen_tag_file] that allows Doxygen builds to link to its documentation on standard library types ({{vector}}, {{string}}, standard exception classes, etc.). Using this file in {{lsst.base}} would save developers working with Stack code from having to look up these classes by hand. A minor feature, but a convenient one.    -One question that would need to be addressed is whether the tag file should be stored in {{base}} or downloaded from the website each time. The former is easier (and allows documentation to be built without an internet connection), while the latter would prevent the tag file from going out of date.-",1
"DM-11018","06/23/2017 09:27:50","check Firefly display of mask after DM-7477 implementation","RFC-25 triggered DM-7477 to increase the mask plane from 16 bits to 32 bits. Firefly has the function to display mask plane when it is part of teh extension. We need to double check to make sure the display still works. ",1
"DM-11033","06/26/2017 10:47:57","Problem with exists/getStorage() with composite datatypes","When using composite data types (e.g. with obs_comCam), seemingly all cmdLineTasks (certainly {{processCcd.py}} and {{constructCalibs.py}}) fail with    {noformat}  File ""/ssd/lsstsw/stack_20170409/Linux64/daf_persistence/13.0-17-gd5d205a+2/python/lsst/daf/persistence/repository.py"", line 303, in exists      butlerLocationStorage = location.getStorage()  AttributeError: 'ButlerComposite' object has no attribute 'getStorage'  {noformat}    It seems that these composite objects don't have the function defined.    Steps to reproduce can be found by setting up a stack on lsst-dev with {{. /home/mfl/nate.sh}} and the repo can be remade and the failing command run with {{. /home/mfl/rerun_nate.sh}}.",2
"DM-11051","06/27/2017 12:13:42","Change logger level for wcs warning","DM-10559 introduced a warning when makeWcs() fails to construct a wcs, but the level was too high. It should be reduced DEBUG level warning.",1
"DM-11091","06/28/2017 15:13:20","Fix multibandDriver attempting to run detection when no data present","MultibandDriver attempts to run detection if no calibrated exposures are found. It should skip trying to do this if there is no data present at all.",1
"DM-11094","06/28/2017 16:00:04","The time series page should not be named as "": Viewer""","localhost:8080/suit/  SDSS Stripe82, Object table, RA=9.6, Dec=-1.1, radius=10 as  Click any row with coadd_filter_id=3  Click ""View Time Series: <id>"" button  The time series will popup. At the upper left corner, "":Viewer"" is there.     I think the leading title was missing.    Fix: Use a more meaningful name, like ""Time Series Tool: Viewer""  ",1
"DM-11097","06/29/2017 06:29:26","Document required entry points for Tasks","Per RFC-352, all {{Task}}​s (modulo approved exceptions; see the RFC for details) are expected to provide a {{run}} method as their primary point of entry, and may also provide {{runDataRef}} for operating on dataRefs. Please update the documentation in pipe_base to explain this requirement (and the procedure for obtaining an exemption).",1
"DM-11104","06/29/2017 12:06:16","generate py3/lsst_distrib docker containers from tarballs","At present, the docker containers that are published as part of the weekly pipelines/apps tag/release process are using python 2.7.  In addition, they are from scratch builds which should no longer be necessary as weekly binary ""tarball"" builds are being published.",1
"DM-11137","06/30/2017 09:45:39","testSafeFileIO fails with too many open files on Python 3 and Terminal","testSafeFileIO.py fails on my mac (iMac and laptop) from Terminal using Python3. It passes on Python2 and using iTerm2 on Python3. The open file limit seems to be on the edge and might have to be calculated dynamically.",0.5
"DM-11138","06/30/2017 12:04:36","Convert meas_mosaic wcs output to a format directly readable by the butler","{{meas_mosaic}} writes its WCS as FITS header with no attached image, which requires loading code to use the following pattern:  {code}  md = butler.get(""wcs_md"", ...)  wcs = lsst.afw.image.makeWcs(md)  {code}  instead of simply  {code}  wcs = butler.get(""wcs"", ...)  {code}  Using a header-only format also limits us to FITS-standard WCS mappings.    Because {{Wcs}} inherits from {{afw::table::io::Persistable}} it already has {{writeFits}} and {{readFits}} methods that utilize our FITS binary table format, which will be able to save more complex WCS solutions.  It's also compatible (or will be soon, on DM-10728) with the ""FitsCatalogStorage"" butler storage type, so we should be able to fix this by:   - Redefining ""wcs"" to be a ""FitsCatalogStorage"" dataset, instead of a ""FitsStorage"" exposure in all mappers;   - modifying meas_mosaic (and possibly jointcal, if needed) to use {{butler.put}} directly.   - modifying any code that consumes the {{wcs}} dataset to use {{butler.get}} directly.    In addition, this issue should include creating a simple command-line script that can be used to convert a data repository from the old format to the new one.",2
"DM-11147","06/30/2017 13:48:00","Reconfigure obs_ctio0m9 to work after changes to meas_astrom","After DM-10253 and DM-10565 things were working well with obs_ctio0m9, but changes to some defaults in meas_astrom meant that the {{processCcd.py}} config merged there is no longer correct. This ticket exists to make those changes.",2
"DM-18276","06/30/2017 14:17:12","Make SAL message timestamp visible to Python","All SAL messages contain a timestamp, but this fields is not visible to Python. Please make this field available to Python code, along with any other similar topic metadata that might be useful.",2
"DM-11158","07/02/2017 20:37:24","Doing a stack install using Anaconda 4.4.0 (Python 3.6.1) results in 3 tests failing during the build in meas_base.","Both at NERSC and on a Linux machine at Duke I see the following identical behavior.  The Duke machine is one of our reference platforms so I believe the behavior should be reproducible anywhere.    {code}  cosmology-01:~ $ cat /etc/redhat-release   CentOS Linux release 7.3.1611 (Core)   {code}    I installed Anaconda 4.4.0 (Python 3.6 version) and used it for the stack install (instead of the 3.5 based miniconda).    I used the newinstall from:    {{https://raw.githubusercontent.com/lsst/lsst/master/scripts/newinstall.sh}}    I issued the following command after the eups bootstrap:    {{eups distrib install -t w_2017_25 lsst_apps}}    The build runs until meas_base where 3 of the tests fail.  The three resulting failures are:    {code}  cosmology-01:.tests $ ls *.failed  testApertureFlux.py.failed    testScaledApertureFlux.py.failed  testMeasureSources.py.failed  cosmology-01:.tests $   {code}    For example:    {code}  cosmology-01:.tests $ cat testMeasureSources.py.failed  tests/testMeasureSources.py    F....  ======================================================================  FAIL: testCircularApertureMeasure (__main__.MeasureSourcesTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/testMeasureSources.py"", line 102, in testCircularApertureMeasure      self.assertAlmostEqual(10.0*math.pi*r*r/currentFlux, 1.0, places=4)  AssertionError: 2.3123809147074006 != 1.0 within 4 places    ----------------------------------------------------------------------  Ran 5 tests in 2.487s    FAILED (failures=1)  {code}",1
"DM-11160","07/03/2017 06:40:25","Add metadata to LDM-534 draft","It should have a change record, note of repository URL, etc.",1
"DM-11162","07/03/2017 12:04:49","Replace all use of Coord and subclasses with SpherePoint","Implement RFC-353 by replacing all use of {{Coord}} and its subclasses with {{SpherePoint}}.    Things to keep in mind:  - afw will no longer support coordinate conversions. We do not rely on this now (except for a very small number of cases of use of FK5 J2000 instead if ICRS that is likely a mistake).  - {{SpherePoint}} and {{Coord}} have slightly different APIs (for instance {{SpherePoint}} is immutable) so some usage will need to change.  - {{Coord}} has some methods that {{SpherePoint}} does not. These may have to be added to {{SpherePoint}} or implemented as free functions.    This would also be a good time to eliminate the {{SpherePoint(double raDecRad[2])}} constructor. This was added to support {{Transform}} but turned out to be unnecessary.",20
"DM-11163","07/03/2017 14:33:46","Always create VisitInfo from metadata when available","Please create {{VisitInfo}}  whenever we read an {{Exposure}} from disk if suitable metadata is available.    Currently this is done in the {{std_raw}} method, but it's also needed when e.g. reading master dark calibration frames from {{DecoratedImages}}.  ",1
"DM-11164","07/03/2017 14:35:33","Write suitable metadata for VisitInfo when writing calibrations","We need to be able to get darktime from {{VisitInfo}} when using e.g. master darks, so please add code to write suitable metadata.  ",1
"DM-11166","07/03/2017 14:56:54","Fix typo in log message","Please fix the typo for ""metadata"" in makeRawVisitInfo.py:  {code}  Key=\""{}\"" not in medata  {code}  ",0
"DM-11183","07/05/2017 14:24:24","PEP8 and Python 3 Improvements for imgserv","Rewrite more of imgserv according to PEP8 and Python 3 improvements",2
"DM-11192","07/05/2017 16:03:38","Status monitoring for qserv disconnected queries","Revisit qserv status monitoring (SHOW PROCESSLIST etc.) and ensure that it will be usable for monitoring the status of qserv disconnected queries",0.5
"DM-11196","07/06/2017 10:26:56","Move yaml camera model to obs_base","[~rhl] designed a nice replacement for the camera model using yaml description.    This ticket is to, as an experiment, move it to {{obs_base}} without activating it, so that it can be tested and integrated where people so choose.    An RFC will be filed regarding switching to it as our default camera description once the move is complete.",0.5
"DM-11215","07/06/2017 14:35:52","Enable validateDrp.py to run from JSON file.","Enable validateDrp.py to run from JSON file.    Currently, the basic running is:  {code}  validateDrp.py repodir  {code}    This produces an output JSON file, along with STDOUT of the results, and plots of KPMs and related metrics.    Behavior will be the (slightly magic):  {code}  validateDrp.py output.json  {code}  where the code will just check to see if it was being passed a JSON file, and, if so, then load those results and produce requested output solely based on the JSON file.",5
"DM-11217","07/06/2017 15:08:44","Butler+CmdLineTask cannot output to a (non-repo) folder where some files already exist","[~sthrush] noticed that tasks fail if the output is a non-empty non-repo folder.  To reproduce with {{w_2017_26}}:     {code:java}  $ mkdir out  $ echo 1234 > out/a  $ processCcd.py /datasets/hsc/repo --id visit=444 ccd=4  --output out  {code}    This gives errors:   {code:java}  Traceback (most recent call last):    File ""/software/lsstsw/stack/Linux64/pipe_tasks/13.0-38-gf73ba12/bin/processCcd.py"", line 25, in <module>      ProcessCcdTask.parseAndRun()    File ""/software/lsstsw/stack/Linux64/pipe_base/13.0-9-g1c7d9c5+5/python/lsst/pipe/base/cmdLineTask.py"", line 509, in parseAndRun      parsedCmd = argumentParser.parse_args(config=config, args=args, log=log, override=cls.applyOverrides)    File ""/software/lsstsw/stack/Linux64/pipe_base/13.0-9-g1c7d9c5+5/python/lsst/pipe/base/argumentParser.py"", line 512, in parse_args      namespace.butler = dafPersist.Butler(inputs=inputs, outputs=outputs)    File ""/software/lsstsw/stack/Linux64/daf_persistence/13.0-20-g8cd6840/python/lsst/daf/persistence/butler.py"", line 531, in __init__      self._setAndVerifyParentsLists(repoDataList)    File ""/software/lsstsw/stack/Linux64/daf_persistence/13.0-20-g8cd6840/python/lsst/daf/persistence/butler.py"", line 905, in _setAndVerifyParentsLists      repoData.cfg.extendParents(parents)    File ""/software/lsstsw/stack/Linux64/daf_persistence/13.0-20-g8cd6840/python/lsst/daf/persistence/repositoryCfg.py"", line 167, in extendParents      if all(x == y for (x, y) in zip(self._parents, newParents)):  TypeError: zip argument #1 must support iteration  {code}      If the folder is empty (i.e. skipping {{$ echo 1234 > out/a}} in the example) then the task runs just fine.",2
"DM-11229","07/07/2017 12:52:04","Investigate GLMM convergence","While testing the proximal gradient methods (ADMM, SDMM, and GLMM) on more simplistic functions, [~pmelchior] and I have been noticing strange behavior in the algorithms, noticeably that under certain circumstances the solutions diverge.    This ticket is meant to be a placeholder for sub-tickets that will dive into specific cases where we have noticed that the algorithms fail to understand and correct the behavior.",5
"DM-11230","07/07/2017 12:57:15","Investigate divergence in spectral normalization","In hyperspectral mixing, a similar but easier to solve problem then a deblender, we normalize the spectra in exactly the same way that we do in the deblender, namely requiring that the values of each component's spectrum are positive and sum to one.    What we notice is that if we perform the normalization as a linear operator, the algorithm quickly diverges. Understanding why this happens is likely to improve our understanding of deblender failures, so this ticket aims to understand the reason for this divergence.",5
"DM-11237","07/08/2017 12:07:47","Please turn down verbosity of sipterms.cc test","It's very hard to see what tests are failing as the sipterms test spits many *many* lines of output all over the screen.  The most recent changeset is 3d57adbe, hence the assignment of this issue.  It's not your fault, but we might as well change the tests to {{#if 0}} or some CPP symbol at the same time.    {code}  3d57adbe (Krzysztof Findeisen    2017-04-19  90)             if (1) {  742ec6fa (fergal                 2010-06-15  91)                 printf(""\n%.1f %.1f : %.3f\n"", u, v, distortX);  dcfba3df (dstn                   2011-05-05  92)                 printf(""%.7f %.7f \n"", lin.getRa().asDegrees(), lin.getDec().asDegrees());  dcfba3df (dstn                   2011-05-05  93)                 printf(""%.7f %.7f \n"", sip.getRa().asDegrees(), sip.getDec().asDegrees());  3d57adbe (Krzysztof Findeisen    2017-04-19  94)                 printf(""Diff: %.7f %.7f \n"", sip.getRa().asDegrees() - lin.getRa().asDegrees(),  3d57adbe (Krzysztof Findeisen    2017-04-19  95)                        sip.getDec().asDegrees() - lin.getDec().asDegrees());  742ec6fa (fergal                 2010-06-15  96)             }  {code}",1
"DM-11245","07/10/2017 12:35:10","Improve Lander's logic for displaying 'draft' notice and docushare links","Make the following changes to Lander:    - Always include the ""View on DocuShare"" button for controlled document types (LDM, LSE, LPM, and so on). Implement as a whitelist, and use the ls.st short link form. This makes the {{--docushare-url}} command line optional have a good automatic default.  - Do not show the DRAFT notice for builds on {{master}} and {{docushare-vN}} branch/tag names.",1
"DM-11247","07/10/2017 17:16:29","Add additional alert packet contents to DPDD","Per https://jira.lsstcorp.org/browse/RFC-348, add (approximate) history of nondetections to the alert packet.    Also clarify (per https://jira.lsstcorp.org/browse/RFC-350) that DIASource records will include filterName.",2
"DM-11251","07/11/2017 14:58:48","add support for the standardize function for composite datasets","composite objects should get passed to the std_<datasetType> function of the mapper, if there is one.",2
"DM-11256","07/12/2017 12:17:19","Update beamer to LSST2017 templates","A new set of templates are provided, which we should use starting with the July NSF/DOE review.  Please change lsst-texmf to use them.  ",1
"DM-11269","07/13/2017 09:10:38","Please move ""Empty WCS extension, using FITS header"" from INFO to DEBUG","When reading raw or rawish files you often have a WCS in the header but not one of the custom LSST WCS extensions; this is not interesting to the user, but it's currently logged as INFO (and it's pretty common -- e.g. when assembling biases you get it for every bbox read by {{constructCalib}} scripts):  {code}  afw.image.ExposureInfo INFO: Empty WCS extension, using FITS header  {code}    Please move this to DEBUG.  ",1
"DM-11271","07/13/2017 09:39:27","DocTree is incorrect about LDM-151","Our DocTree currently describes LDM-151 as the ""L2 Pipeline Design"". It does not provide a design for L1 (or calibration products) pipelines.    In fact, LDM-151 covers all of these pipelines, and we should make this clear in the diagram.    (I don't think it's critical to fix this before the review, though.)",1
"DM-11280","07/14/2017 10:38:13","Update configs missed in DM-10469","There were a few {{obs_subaru}} config updates missed in DM-10469 in support of running the {{pipe_analysis}} QA scripts.  These will be addressed here.",0.5
"DM-11289","07/17/2017 07:23:06","RepositoryArgs needs to check that mapperArgs is dict-like","{{RepositoryArgs.mapperArgs}} is assumed to be dict-like (e.g. {{if 'root' not in mapperArgs}}), but it can be initialised to {{None}}.    The init function needs to guarantee that {{mapperArgs}} is dict-like. I think that we should also fix the default value ({{RepositoryArgs.\_\_init\_\_()}} has a default {{mapperArgs=None}}), although we could just add the test;  but I think it's clearer to set the default to a valid value too.    N.b. mapperArgs is omitted from the class docstring, which otherwise duplicates most of the {{\_\_init\_\_}} docstring.  ",1
"DM-11299","07/17/2017 17:53:32","WISE coadded images displayed ignored cutout size","For WISE images PDAC in  PDAC v2, the input cutout size was ignored for the coadded images. steps to repeat:  # select WISE images in LSST search panel  # user target m51, input cutout size 600 arcsec  # then use the same target, do NOT specify size  # Compare the images returned, they are the same    The 600arcsec images should be much smaller, and show a pretty spiral galaxy  ",8
"DM-11322","07/19/2017 15:33:05","Add calibUsed-only qa plots for astrometry and photometry","For qa analysis it is useful to have plots and statistics based solely on the objects that were used for any particular calibration.  We have so far been making *calib_psfUsed*-only plots as the PSF modeling was the only provided such a flag in the output catalogs.  As of DM-9050, we now also have *calib_astrometryUsed* & *calib_photometryUsed* flags for the astrometry and photometry fitting.  This issue is to add the relevant/useful plots to the analysis scripts based on these objects.",1
"DM-11324","07/19/2017 17:51:39","rename decam_hits repository ap_pipe","As we generalize the prototype AP pipeline to non-DECam, non-HITS datasets we should have a more general name than `decam_hits`.  I suggest `ap_pipe`.",2
"DM-11328","07/20/2017 07:43:26","afwImage doesn't roundtrip through numpy","{code}  im = afwImage.ImageF(10, 20)  ima = im.array    afwImage.ImageF(ima)  afwImage.Image(ima, dtype=ima.dtype)  {code}  fails with  {code}  ----> 5 afwImage.Image(ima, dtype=ima.dtype)    /Users/lsst/products/DarwinX86/miniconda/latest/envs/lsst-13_0/var/opt/eups/DarwinX86/utils/13.0-7-gd7d2357/python/lsst/utils/wrappers.pyc in __call__(self, *args, **kwds)      272         if cls is None:      273             d = {k: v for k, v in zip(self.TEMPLATE_PARAMS, key)}  --> 274             raise TypeError(""No registered subclass for {}."".format(d))      275         return cls(*args, **kwds)      276     TypeError: No registered subclass for {'dtype': dtype('float32')}.  {code}  ",2
"DM-11334","07/20/2017 13:44:18","Update documenteer dependency for lsst-texmf, developer.lsst.io sites, and technotes","Pin documenteer >0.1,<0.2 in these projects to preserve existing assumptions about dependencies.    For technotes, the right approach might be to update to documenteer 0.2 and make the necessary changes to their conf.py files.",3
"DM-11335","07/20/2017 14:37:11","obs_cfht tests fall over due to -9999.9 for TELEAZ for calibs","After DM-11163 (and its related tickets), visitInfos are created for calibs (we need darkTimes and expTimes etc).    However, some of the metadata in the {{testdata_cfht}} contains values which make {{makeRawVisitInfo}} unhappy. Luckily, the {{megacamMapper}} had a {{_standardizeDetrend}} method for stripping out this which upset things downstream.    This ticket just adds the {{TELEAZ}} and {{TELEALT}} keywords to those removed. This should only be called for biases, and therefore not upset anything else.",1
"DM-11336","07/20/2017 16:12:16","Switch default reference catalog for DECam to PS1 in LSST format","Following {{obs_subaru}}, it would be nice to change the configuration files of {{obs_decam}} so to also use the PS1 catalog in LSST format and remove dependence on astrometry.net.",1
"DM-11348","07/21/2017 16:46:26","Generate PDF report from stack-driven eotest outputs","We now have eotest callable from within the DM Stack. Fix its driver script for producing PDF reports to work with the data products as stored by stack execution, and use this to demonstrate that it's doing the same thing as the stand-alone eotest.  ",2
"DM-11368","07/25/2017 11:10:43","Update EUPS_PKGROOT in shared stack","Following DM-11355, the {{EUPS_PKGROOT}} used to distribute the stack has changed to {{https://eups.lsst.codes/stack/src/}}. Update the shared stack on {{lsst-dev01}} (and Tiger/Perseus, for Princeton users) to pull from that.",1
"DM-11376","07/25/2017 13:53:08","backgrounding job bug","In Firefly catalog searches,  do a position search on catalog ""Gaia G-band Time Series of Variable Sources"".    it runs until put in background monitor, then the ""Unexpected error"" appears.    Since this search is illegal, the error message should appear right away, not until in background.",2
"DM-11377","07/25/2017 13:54:02","Prepare for AP Verification Metrics session at LSST 2017 Meeting","There is a 90 minute session at LSST 2017 about AP Verification Metrics.  This ticket is to plan and prepare for the session, which includes    * determining how to structure the session; meeting organizers sent suggestions  * inviting relevant non-AP staff to attend (esp. SQUARE)  * preparing relevant talks and/or demos  * aggregating and summarizing feedback after the session",5
"DM-11378","07/25/2017 14:32:59","Remove config option to make PSF-matched warps with old and wrong order of operations","In DM-8289 we switched the order of operations when PSF-matching warps to warp first and PSF-match second (so that we don't undo the matching by warping).  I add a config parameter matchThenWarp  to invoke the old behavior while while the epic was in progress.  Now, that that it has been tested, it is time to remove the matchThenWarp branch. ",1
"DM-11385","07/26/2017 10:51:47","shebangtron breaks scons","The current behavior of {{shebangtron}} is mangle all {{#!}} statements.  This breaks {{scons}} in an otherwise py3 based installation.    Steps to reproduce:    {code:java}  git clone https://github.com/lsst/base.git  cd base/  setup -k -r .    $ scons  scons: *** SCons version 2.5.0 does not run under Python version 3.5.2.  Python 3 is not yet supported.  {code}    ",1
"DM-11395","07/26/2017 16:29:15","Update testdata_jointcal refcats to new Indexed format","Many of the reference catalogs provided with {{testdata_jointcal}} are in the old astrometry.net format, which is now deprecated. We should update those provided refcats to the new style format, which will also simplify the test configuration.",2
"DM-11397","07/26/2017 16:30:39","Remove twinkles1 jointcal testdata and tests","The reference catalogs supplied with the twinkles data I have are in the a.net format, and do not contain flux errors. When we update them to the new style format, we should also add flux errors of some sort, so that they can be used with jointcal's new photometry system.         Update: after much thought and discussion, we decided that the twinkles1 jointcal testdata no longer serves a useful purpose, and is holding us up from removing a.net refcats and {{obs_lsstSim}}. Instead of trying to update the included refcat (the HTM indexed version that [~krughoff] found was not a drop in replacement), I'm going to delete the twinkles data and tests all together. We'll be getting some DC2 data built on obs_lsst soon enough, and we will have to write entirely new tests for that anyway.",0.5
"DM-11398","07/26/2017 16:51:19","Time Series image display location issue ","1. Search WISE source from PDAC, m81  2. Highlight one source, click on the ""View Time Series ..."" button for that source  3. change to the time series tab    change the number of images displayed to 1, use arrow key to go down the table, the displayed image jumps around, sometimes is in the middle of the image display area, sometimes is at the left side, sometimes at the right side.  ",2
"DM-11404","07/27/2017 11:27:53","Identify cause of new API inconsistencies","The new deblender API appears to have large residuals in most blends and completely fails in certain cases, even when using identical constraints. This ticket will attempt to identify and fix these issues.",5
"DM-11410","07/27/2017 13:40:42","validate_drp incorrectly outputs filenames as '_<filter>.json'","After the merge of DM-11300, validate_drp has dropped the repo from the output filename.  Thus, {{<repo>_<filter>.json}} has become {{_<filter>.json}}.",1
"DM-11415","07/28/2017 07:56:25","Add additional DocuShare link roles to Documenteer","Add additional DocuShare roles to Documenteer. Make it so that any {{ls.st}} functionality is available from reStructuredText (support LTS, LEP, LCA, DMTN, SQR, Document, etc.)",0.5
"DM-11421","07/28/2017 13:44:08","Fix unicode support in obs_lsstSim","obs_lsstSim tests that ""raft"", ""sensor"", and ""channel"" are of type `str`, but this fails in python 2.7 since these may be unicode.",0.5
"DM-11422","07/28/2017 14:06:06","Adapt prototype pipeline to use coadds as templates","Once we have useable coadds from the HiTS 2014 fields matching our HiTS 2015 dataset, these should be used as templates for the AP prototype pipeline (soon to be {{ap_pipe}}). The pipeline will still accept another visit as a template, but the default will be to use the coadds from a specific location in the dataset repo.",3
"DM-11424","07/28/2017 16:22:26","After filtering a plot or on a sorted table, i can't filter selected rows anymore","In dev, after filtering chart points, i tried to filter out couple row selected but nothing happen. Error in the console log is thrown:  {code:java} :8080/irsaviewer/timeseries;a=layout.showDropDown?visible=false:1 Uncaught (in promise) SyntaxError: Unexpected token   in JSON at position 159 {code}  It happen in tri-view and timeSeries tool.  If i sort a table, it also fail to filter the selected row.",2
"DM-11429","07/31/2017 08:20:48","tests/testPhotoCal.py fails on 2017-07-31 ""master""","Starting this morning, (2017-07-31 ~ 03:00am EDT), {{pipe_tasks}} on master fails {{tests/testPhotoCal.py}}.  See first attached file ({{consoleText.txt}} for full log from Jenkins job.  second attached file {{pipe_tasks_failure_build.log}} for the same error with a local build on my desktop.",2
"DM-11432","07/31/2017 11:02:43","Implement Robust Coaddition using PSF-matched Warps for Artifact Removal","Follow up to DM-10005 (Prototype). Implement Robust Coaddition task and merge into stack. ",8
"DM-11444","07/31/2017 14:15:15","newinstall.sh fails to install Miniconda","{code}  $ bash newinstall.sh     LSST Software Stack Builder  =======================================================================    ######################################################################## 100.0%  Detected git version 2.13.3. OK.      In addition to Python 2 (>=2.7) or 3 (>=3.5), some LSST packages depend on  recent versions of numpy, matplotlib, and scipy.  If you do not have all of  these, the installation may fail.  Using the Miniconda Python distribution  will ensure all these are set up.    Miniconda Python installed by this installer will be managed by LSST\'s EUPS  package manager, and will not replace or modify your system python.    Would you like us to install the Miniconda Python distribution (if  unsure, say yes)? yes    Configured EUPS_PKGROOT: https://eups.lsst.codes/stack/src    ::: Deploying Miniconda2-4.2.12-MacOSX-x86_64.sh  mktemp: too few X's in template ‘XXXXXXXX.Miniconda2-4.2.12-MacOSX-x86_64.sh’  {code}  ",1
"DM-11454","08/01/2017 08:47:15","Modify UnitNormMap to round trip zero-length vectors","In AST {{UnitNormMap}} transforms a zero-length vector into {{(NaN, Nan, ..., NaN, 0)}} in the forward direction, as one would expect (since the components of the unit vector are not well defined and the norm is 0), but the inverse transform of that is {{(NaN, NaN, .., NaN)}}. Enhance {{UnitNormMap}} so the inverse transform of {{(NaN, NaN, ..., NaN, 0)}} is {{(0, 0, ..., 0)}}, instead, so zero-length vectors round-trip correctly.    Also document the behavior at zero and update the unit test.    Be sure to do this on a branch of master, rather than LSST's branch.    Finally, update astshim documentation and unit test accordingly.",1
"DM-11458","08/01/2017 15:31:29","Periodogram table rows are no longer selectable and highlighted","In time series tools, on a periodogram table, the first row can be selected but not the others. The highlight is also gone. But i think is still updating the period value so somehow the table panel is not updated.   If you click on the peak table and back, you will see the row selected.    in Dev branch  ",2
"DM-11459","08/01/2017 15:54:49","Fix Butler compatibility issues","Recent changes to the Butler have broken the DCR template generation code. This ticket is to update the DCR code to work with the latest version of the Butler.",2
"DM-11461","08/01/2017 18:33:11","Add defect and refcat support to Dataset","The {{ap.verify.dataset.Dataset}} class currently recognizes three types of input data: raw science images, calibration images, and templates. Depending on future policy decisions, up to two more data types -- reference catalogs and defects -- may have their own locations.    For forward compatibility (i.e., to avoid having to change hardwired assumptions about where these files are located) the {{Dataset}} class should have properties giving the location of these two data types. At present, they may be defined to be equal to or subdirectories of {{Dataset.calib_location}}.",1
"DM-11467","08/02/2017 14:13:40","Fix docker image name generated with travis-ci","+underlined text+Travis will then automatically generate docker images on each gitbuh commit. This will avoid developer to build docker image on their own.  Image will be named qserv/qser:$SHORT_COMMIT_ID (example qserv/qserv:b78189f) and their alias for branch tip will be qserv/qserv:travis_$BRANCH_NAME (example qserv/qserv:travis_DM-11467)",2
"DM-11473","08/03/2017 13:51:19","Add SpherePoint(long, lat, unit) constructor","Implement RFC-367 by adding a {{SpherePoint(double longitude, double latitude, AngleUnit unit)}} constructor.    This will simplify DM-11162: Replace all use of {{Coord}} and subclasses with {{SpherePoint}}.",1
"DM-11475","08/03/2017 15:34:15","Grid Layout: access plotly support and make another demo","continue working on the grid layout for firefly, access the plot.ly support, and make more demo/test code",8
"DM-11487","08/04/2017 17:53:29","Fix OSX flakiness in qhttp unit test","Qhttp unit test fails frequently under OSX.  Investigate and fix.",1
"DM-11488","08/05/2017 08:30:25","weekly release w_2017_31 failed","The {{w_2017_31}} weekly release failed building master due to what appears to be a repeatable git-lfs error, possibly triggered by DM-11095.    {code:java}     testdata_jointcal: Traceback (most recent call last):    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/bin/lsst-build"", line 51, in <module>      args.func(args)    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 762, in run      manifest = p.construct(args.products)    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 725, in construct      self._add_product_tree(products, name)    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 713, in _add_product_tree      dependencies.append(self._add_product_tree(products, dprod.name))    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 713, in _add_product_tree      dependencies.append(self._add_product_tree(products, dprod.name))    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 696, in _add_product_tree      ref, sha1 = self.product_fetcher.fetch(product_name)    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/python/lsst/ci/prepare.py"", line 333, in fetch      git.checkout(""--force"", ref)    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/python/lsst/ci/git.py"", line 49, in checkout      return self('checkout', *args, **kwargs)    File ""/home/lsstsw/jenkins/release/lsstsw/lsst_build/python/lsst/ci/git.py"", line 44, in __call__      raise GitError(retcode, cmd, stdout, stderr)  lsst.ci.git.GitError: Command '['git', 'checkout', '--force', 'master']' returned non-zero exit status 128.  stdout:    stderr:  Downloading cfht_minimal/src/06AL01/D3/2006-05-20/r/SRC-849375-12.fits (75.94 KB)  Error downloading object: cfht_minimal/src/06AL01/D3/2006-05-20/r/SRC-849375-12.fits (5071e4548d33b407c0438b3b7fcbf4fab390674f30ccfd5f7492e21b689e6653)    Errors logged to /home/lsstsw/jenkins/release/lsstsw/build/testdata_jointcal/.git/lfs/objects/logs/20170805T020850.70624876.log  Use `git lfs logs last` to view the log.  error: external filter git-lfs smudge %f failed 2  error: external filter git-lfs smudge %f failed  fatal: cfht_minimal/src/06AL01/D3/2006-05-20/r/SRC-849375-12.fits: smudge filter lfs failed  {code}  ",0.5
"DM-11507","08/07/2017 12:32:05","Lua build fails if GNU sed is the default on Darwin","See:    https://community.lsst.org/t/sed-e-expression-1-char-1-unknown-command/2147/4    for the description of the problem. Proposed resolution is to update lua's eupspkg scripts to test for gnu vs. BSD sed.",1
"DM-11512","08/07/2017 15:07:17","Create pytest eups package","Create an EUPS package for pytest PyPI distribution.",0.5
"DM-11514","08/07/2017 15:31:08","Modify sconsUtils to use pytest for test execution","This ticket is for modifying sconsUtils such that it uses pytest for test execution rather than python.  This is part of improving the test execution environment such that we can capture output from tests in a JUnit format compatible with Jenkins, and so that we can report on skipped tests as well as test execution times.",8
"DM-11516","08/07/2017 17:35:20","Let Doxygen report inherited methods inline","[~mrawls] showed me that the new Sphinx-based documentation system reports inherited methods as full documentation rather than as links to the base class documentation.    If the plan for C++ code is still for Doxygen to produce XML output which is then fed to Sphinx ([~jsick], is this still the case?), it would be helpful to set {{INLINE_INHERITED_MEMB}} to {{YES}} so that it has the same behavior.",1
"DM-11517","08/07/2017 17:47:24","Define Science Pipelines milestones in LDM-564 (draft)","The current LDM-564 draft (on {{tickets/DM-11468}}) uses short names (""DRP-MS-IMCHAR-6"", etc) for SciPi milestones. Turn them into something readable.",2
"DM-11518","08/07/2017 18:40:14","Modify db tests to support pytest","This ticket is for the work of migrating the DB tests such that they run with the py.test test runner.",1
"DM-11528","08/08/2017 17:23:08","Publish Firefly to npm in support of JupyterLab Widgets development","JupyterLab Widget requires JavaScript libraries to be npm installable.  We need to package Firefly as an npm package and publish it.  ",8
"DM-11529","08/08/2017 17:38:02","Stack tutorial - notebook content","Adapt Jim's notebook and dataset for the stack tutorial. ",1
"DM-11538","08/08/2017 19:42:24","fix a few C++ compiler warns","These small fixes address some warns that I have encountered in more recent gcc and clang releases, and some things flagged by the eclipse c++ code analyzer ",0.5
"DM-11540","08/09/2017 09:30:21","Experiment with pytest-flake8 eups package","The {{lsst.verify}} package uses flake8 pytest configurations meaning that by default {{verify}} fails when DM-11514 is active because the flake8 plugin is not installed. Add this package as a dependency of sconsUtils. We may have to install flake8 as well, although that might be in the conda installations already.",1
"DM-11542","08/09/2017 11:05:07","Adapt qa analysis script to matplotlib versions on shared stacks","These scripts were being developed on an old version of matplotlib (which was being provided by the shared stack on {{tiger}} at Princeton -- this has recently been updated).  The shared stacks on {{lsst-dev}} and {{tiger}} (at Princeton) provide a newer version which results in deprecation warnings and badly formatted plots (to the point where axis labels fall off the side of the figures).  This makes several tweaks to adapt to the currently supplied shared stack versions:  {code}  Current on lsst-dev is matplotlib.__version__ '2.0.0'  Current on tiger at Princeton is: matplotlib.__version__ '2.0.2'  {code}",3
"DM-11545","08/09/2017 11:19:33","Copyedit LDM-542 in preparation for DM Review","In preparation for the DM Review, make an editing pass over the entire document.",2
"DM-11547","08/09/2017 15:08:07","work with NCSA in deploying a Firefly app in lsst-dev or Nebula","Work out all the issues in deploying a Firefly server in lss-dev or Nebula environment. ",3
"DM-11548","08/09/2017 15:10:55","Update text for CCB/SE group in LDM-294","The text for membership of the DM SysEng and DM CCB groups is inconsistent. Make them identical.",1
"DM-11556","08/10/2017 11:09:18","Update chart content of grid-view sample, ffapi-slate-test2.html ","Update the chart content of ffapi-slate-test2.html to be in sync with the change of the relevant sample in firefly_client.     The content for each chart on WISE search is updated as follows,   2D scatter: w1-w2 and w2-w3 for x and y  heatmap: 3 sets, w1 vs. w2, w1 vs. w3, and w1 vs. w34.  histogram: 2 sets, w1, w2.  3D scatter: w1, w2, w3 for x, y, z.    ",1
"DM-11563","08/10/2017 15:27:08","firefly grid display bugs found during testing ","Fix the following:    - http code that starts with ""file://"" now goes through LocalFileRetriever, better security this way.  - fix layout issues with charts  - fix issues with initial plot returning the wrong RangeValues  - reinit now hides all dialogs",2
"DM-11566","08/10/2017 15:40:29","automatic purging of daily eups distrib tags","Short lived tags (daily, etc.) should be automatically cleaned up once they have ""expired"". This should include eupspkg and tarball repos.",8
"DM-11569","08/10/2017 17:53:22","Plotly 3d bug fixes","scatter3d bugs:   - 2d axes should not show  - options should be basic (not scatter) and should not include options for 2d XY  - drag mode buttons should be those that are supported for 3d  - selection button should not show unless chart supports selection  - zoom to original should work",5
"DM-11574","08/11/2017 09:37:14","Make testDistortion test the distortion","The unit test testDistortion.py tests a few things but those do not include the actual distortion computation (instead it prints a series of values that includes an error. The test contains outdated data (the camera geometry has changed) so some error values are unreasonably large.    I propose to do the following:  - Update the test to optionally write the expected results to a file (e.g. by setting a constant).  - Have the test read the data from a file instead of as text in the file.  - Instead of printing output it will test on the difference.    Unfortunately this assumes that the current distortion function operates correctly, so it's not quite as thorough a test as we might like, but it will be an improvement and will greatly help DM-5922 which changes to a new distortion function.",1
"DM-11577","08/11/2017 13:20:14","newinstall.sh fails if a pre-exisitng python is not in the path","{code:java}  [vagrant@jhoblitt-skc-f25 ~]$ which python  /usr/bin/which: no python in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/home/vagrant/.local/bin:/home/vagrant/bin)  [vagrant@jhoblitt-skc-f25 ~]$ /opt/lsst/software/stack/newinstall.sh -b    LSST Software Stack Builder  =======================================================================    ######################################################################## 100.0%  Detected git version 2.9.4. OK.    /opt/lsst/software/stack/newinstall.sh: line 569: python: command not found    {code}  ",2
"DM-11578","08/11/2017 16:09:55","Cleanup public:private: ordering in jointcal headers","Many of jointcal's classes have a {{public:protected:private:}} ordering that doesn't match our standard. It'd be good to clean this up, as a separate single commit.",0.5
"DM-11580","08/11/2017 18:15:31","Add a default .pythonrc to the JupyterLab environment","It would be nice if the images spawned by the JupyterLab environment had a set of Python configs that would make using the interpreter nice.  This is likely just a `.pythonrc` file in the home directory with at least tab completion (it's possible this comes for free when using ipython instead of python, but I haven't checked).",2
"DM-11584","08/14/2017 09:27:04","jupyterlabdemo w_2017_32 build failed","The jenkins {{build-jupyterlabdemo}} job failed with package dependency errors.",2
"DM-11590","08/15/2017 10:09:42","community.lsst.org TLS cert expires on 2017-10-14","Per nagios:      {code:java}  community/TLS cert expiration is CRITICAL:  CheckHttpCert CRITICAL: TLS/SSL certificate expires on 2017-10-14 15:42:17 UTC - 59 days left.  {code}  ",0.5
"DM-11594","08/15/2017 14:37:08","test_distortion fails when run from pytest","test_distortion.py fails when run from pytest while unpickling:  {code}  >           savedData = pickle.load(dataFile)  E           AttributeError: Can't get attribute 'CcdData' on <module '__main__' from '/Users/rowen/UW/LSST/lsstsw3/miniconda/bin/pytest'>  {code}",1
"DM-11595","08/15/2017 15:33:12","daf_persistence tests fail with pytest-xdist","With DM-11514, we get tests run in parallel using pytest-xdist. This seems to cause individual tests to be parallelized in multiple processes, leading to the daf_persistence tests failing because they assume that a single temp directory can be used for all tests in a single class.    We need to change this so that {{setUp}} allocates a proper temp directory with a unique name.",3
"DM-11599","08/16/2017 15:19:04","Migrate qserv LUA wrapper from SWIG to native C API","Last step in getting rid of SWIG dependency is to replace our SWIG-generated LUA wrapper for proxy with hand-crafted one. The interface is not too complicated, there are just four methods in C++ API that need wrapping.",2
"DM-11602","08/16/2017 17:08:08","change syntax for table column data identification ","Details:    * tbl_id in data  * syntax- ""table::column"" or ""table::expression""",3
"DM-11604","08/16/2017 17:29:02","Support showing the active chart traces","When the active trace is changed then move it to the top. This changes the order of the trace and replots,    This might needs to be discuss with [~tatianag] and [~loi] before the work begins.",8
"DM-11608","08/18/2017 10:54:45","Update scisql version to 0.3.8 in lsst repo","New scisql version 0.3.8 is out, it fixes issues in running deployment scripts with Python3 (and hence it's needed for Qserv Python3 migration).  ",1
"DM-11609","08/18/2017 14:31:24","File upload URL input field is not editable","While testing the new upload feature search, i realize that the URL input field is not editable. If a user try to write or edit the URL, it looses focus and can no longer write.   Please, allow a user to enter manually a URL.  ",2
"DM-11610","08/18/2017 14:36:22","Panel size is not fixed in catalog search when adding a new tab","The VO panel in catalog search changes its size when accessed the first time.    If you go to Catalog search and switch to ""VO"", the panel shrink automatically to fit the space.    The size should be fixed in advance.",0
"DM-11613","08/18/2017 22:49:50","ctrl_execute fails with pytest-xdist","Testing ctrl_execute with pytest-xdist (eg using ticket branches {{tickets/DM-11514}} and {{tickets/DM-11514-base}}, results in the following error:  {code}  ### ctrl_execute - pytest-ctrl_execute.xml.failed  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  ============================= test session starts ==============================  platform linux -- Python 3.5.2, pytest-3.2.0, py-1.4.34, pluggy-0.4.0  rootdir: /home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py3/lsstsw/build/ctrl_execute, inifile:  plugins: session2file-0.1.9, xdist-1.19.2.dev0+g459d52e.d20170817, forked-0.3.dev0+g1dd93f6.d20170817, flake8-0.8.1  gw0 I / gw1 I / gw2 I / gw3 I / gw4 I / gw5 I / gw6 I / gw7 I  gw0 [37] / gw1 [37] / gw2 [37] / gw3 [37] / gw4 [37] / gw5 [37] / gw6 [37] / gw7 [37]    scheduling tests via LoadScheduling  .......................F.............   generated xml file: /home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py3/lsstsw/build/ctrl_execute/tests/.tests/pytest-ctrl_execute.xml   =================================== FAILURES ===================================  _____________________________ TestAllocator.test3 ______________________________  [gw7] linux -- Python 3.5.2 /home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py3/lsstsw/miniconda/bin/python    self = <test_allocator.TestAllocator testMethod=test3>        def test3(self):          sys.argv = self.regularArgs()          al = self.subSetup(""config_condor.py"")                fileName = os.path.join(""tests"", ""testfiles"", ""config_allocation.py"")  >       al.loadPbs(fileName)    tests/test_allocator.py:116:   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   python/lsst/ctrl/execute/pbsPlugin.py:109: in loadPbs      configuration = self.loadAllocationConfig(name, ""pbs"")  python/lsst/ctrl/execute/allocator.py:202: in loadAllocationConfig      os.makedirs(self.configDir)  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     name = './tests/condor_scratch/configs', mode = 511, exist_ok = False        def makedirs(name, mode=0o777, exist_ok=False):          """"""makedirs(name [, mode=0o777][, exist_ok=False])                Super-mkdir; create a leaf directory and all intermediate ones.  Works like          mkdir, except that any intermediate path segment (not just the rightmost)          will be created if it does not exist. If the target directory already          exists, raise an OSError if exist_ok is False. Otherwise no exception is          raised.  This is recursive.                """"""          head, tail = path.split(name)          if not tail:              head, tail = path.split(head)          if head and tail and not path.exists(head):              try:                  makedirs(head, mode, exist_ok)              except FileExistsError:                  # Defeats race condition when another thread created the path                  pass              cdir = curdir              if isinstance(tail, bytes):                  cdir = bytes(curdir, 'ASCII')              if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists                  return          try:  >           mkdir(name, mode)  E           FileExistsError: [Errno 17] File exists: './tests/condor_scratch/configs'    ../../miniconda/lib/python3.5/os.py:241: FileExistsError  =============================== warnings summary ===============================  tests/test_configurator.py::TestConfigurator::test4    /home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py3/lsstsw/eups/2.1.3/python/eups/hooks.py:253: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/jenkins-slave/workspace/stack-os-matrix/label/centos-7/python/py3/lsstsw/stack/site/startup.py' mode='r' encoding='ANSI_X3.4-1968'>      exec(compile(open(startupFile).read(), startupFile, 'exec'), _globals, locals())    -- Docs: http://doc.pytest.org/en/latest/warnings.html  =============== 1 failed, 36 passed, 1 warnings in 3.60 seconds ================  {code}  The problem seems to be that tests are assuming they are running sequentially but with {{pytest -n 8}} individual tests can run in different processes and since the tests use the same config directory they all race with each other.    [~spietrowicz] before I look deeply at the code, is there an obvious way for the tests to use different temp directories or config files?",0.5
"DM-11615","08/19/2017 13:38:53","Fix flaky qhttp ajax unit test","The qhttp ajax unit test still fails intermittently when run under Jenkins CI.  Logs indicate a timing sensitivity.  Rework this unit test to make it more robust.",1
"DM-11618","08/21/2017 14:34:39","Bug where a test directory is being used twice caused a race condition and failure","In fixing DM-11613, I missed an test where a non-unique directory was being created through code that was being exercised.  When this test was run in parallel, a race-condition occurred and a test failed.",0.5
"DM-11625","08/22/2017 13:51:17","Bug in calculation of number-of-detected-in-bands in peak culling","HSC internal release processing uncovered a bug in our peak culling in MergeCoaddDetectionsTask, due to a schema string with periods instead of underscores.  The result is that when testing how many bands a source appears to have been detected in, the result is always zero.    This has apparently been present on the LSST for a long time (possibly since the code was ported there), and has gone unnoticed because this test is only operative in the very largest blends, which are moderately rare in the Wide survey.  We certainly should have spotted it in the UltraDeep RC tests, however; we need to make sure our RC tests include checking for changes in the number or density of detected sources.",1
"DM-11628","08/23/2017 08:57:50","Fix minor bugs in peak culling","Sogo Mineo reports another bug in peak culling that makes it hard to disable via configuration:  {quote}  The docstring of class CullPeaksConfig says:        To disable peak culling, simply set nBandsSafe=1.    If I assume `nBandsSafe` is a typo for `nBandsSufficient`  and set nBandsSufficient = 1,  then the condition for a peak to be kept:        if ((rank < self.config.cullPeaks.rankSufficient) or          (self.config.cullPeaks.nBandsSufficient > 1 and           sum([peak.get(k) for k in keys]) >= self.config.cullPeaks.nBandsSufficient) or          (rank < self.config.cullPeaks.rankConsidered and           rank < self.config.cullPeaks.rankNormalizedConsidered * familySize)):    will be equivalent to:        if ((rank < self.config.cullPeaks.rankSufficient) or          (rank < self.config.cullPeaks.rankConsidered and           rank < self.config.cullPeaks.rankNormalizedConsidered * familySize)):    and peak culling will be still active in spite of the instruction of the docstring.  {quote}  ",1
"DM-11630","08/23/2017 15:15:40","ltd-mason bug uploading Science Pipelines docs with lsst.verify API info","The ltd-mason-travis build is showing:    {code}  Traceback (most recent call last):    File ""/Users/jsick/.virtualenvs/ltd-mason/bin/ltd-mason-travis"", line 11, in <module>      load_entry_point('ltd-mason', 'console_scripts', 'ltd-mason-travis')()    File ""/Users/jsick/lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/traviscli.py"", line 58, in run      upload(manifest, product)    File ""/Users/jsick/lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/uploader.py"", line 31, in upload      aws_credentials=aws_credentials)    File ""/Users/jsick/lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/uploader.py"", line 136, in upload_via_keeper      **aws_credentials)    File ""/Users/jsick/lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/s3upload.py"", line 110, in upload      manager.delete_directory(bucket_dirname)    File ""/Users/jsick/lsst/_pipelines_lsst_io_build/ltd-mason/ltdmason/s3upload.py"", line 353, in delete_directory      assert len(key_objects) > 0  AssertionError  {code}    Turns out the problem is that the {{ObjectManger.list_dirnames_in_directory}} method is suddenly including {{'..'}} as one of the returned directory names, which is an empty directory that can't exist on S3.    I'm already filtering {{'.'}}, so I just need to filter {{'..'}} too.",0.5
"DM-11650","08/25/2017 11:59:53","Clean up file-based unit tests in ap_verify","The current unit tests for {{Dataset}} and argument parsing use fixed filenames. They should be rewritten to use randomly generated names as recommended on [Community|https://community.lsst.org/t/pytest-is-now-being-used-for-test-execution/2201].",1
"DM-11677","08/28/2017 15:19:25","Fix broken tar file in eups scisql 0.3.8","Tar file in eups scisql 0.3.8 was created with an incorrect prefix.  Builds and ""installs"" without error, but fails subsequently becuase installed files are in the wrong places.",0.5
"DM-11691","08/29/2017 12:47:22","Test failure with butler in obs_sdss","In a Jenkins run last night the Mac failed with an error in obs_sdss:  {code}  RuntimeError: The RepositoryArgs and RepositoryCfg must match for writable repositories, RepositoryCfg:RepositoryCfg(root='/Users/square/jenkins/workspace/stack-os-matrix/osx.py3/lsstsw/build/obs_sdss', mapper=<lsst.obs.sdss.sdssMapper.SdssMapper object at 0x141d71320>, mapperArgs={}, parents=[], policy=None), RepositoryArgs:RepositoryArgs(root='/Users/square/jenkins/workspace/stack-os-matrix/osx.py3/lsstsw/build/obs_sdss', cfgRoot=None, mapper=<lsst.obs.sdss.sdssMapper.SdssMapper object at 0x141d4d320>, mapperArgs=None, tags=set(), mode='rw', policy=None)  Stacktrace  self = <testGetId.GetIdTestCase testMethod=testId>      def setUp(self):          self.bf = dafPersist.ButlerFactory(mapper=SdssMapper(root="".""))  >       self.butler = self.bf.create()  tests/testGetId.py:37:   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   ../../stack/DarwinX86/daf_persistence/13.0-32-g7b14ddd/python/lsst/daf/persistence/butlerFactory.py:79: in create      return Butler(root=root, mapper=self.mapper)  ../../stack/DarwinX86/daf_persistence/13.0-32-g7b14ddd/python/lsst/daf/persistence/butler.py:521: in __init__      self._getCfgs(repoDataList)  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   self = <[AttributeError(""'Butler' object has no attribute '_repos'"") raised in repr()] Butler object at 0x141d4d6d8>  repoDataList = [RepoData(id=5399557680,repoArgs=RepositoryArgs(root='/Users/square/jenkins/workspace/stack-os-matrix/osx.py3/lsstsw/b...fg=None,cfgOrigin=None,cfgRoot=None,repo=None,parentRepoDatas=[],isV1Repository=False,role=output,parentRegistry=None)]      def _getCfgs(self, repoDataList):          """"""Get or make a RepositoryCfg for each RepoData, and add the cfg to the RepoData.              If the cfg exists, compare values. If values match then use the cfg as an ""existing"" cfg. If the              values do not match, use the cfg as a ""nested"" cfg.              If the cfg does not exist, the RepositoryArgs must be for a writable repository.                    Parameters              ----------              repoDataList : list of RepoData                  The RepoData that are output and inputs of this Butler                    Raises              ------              RuntimeError                  If the passed-in RepositoryArgs indicate an existing repository but other cfg parameters in those                  RepositoryArgs don't                  match the existing repository's cfg a RuntimeError will be raised.              """"""          def cfgMatchesArgs(args, cfg):              """"""Test if there are any values in an RepositoryArgs that conflict with the values in a cfg""""""              if args.mapper is not None and cfg.mapper != args.mapper:                  return False              if args.mapperArgs is not None and cfg.mapperArgs != args.mapperArgs:                  return False              if args.policy is not None and cfg.policy != args.policy:                  return False              return True                for repoData in repoDataList:              cfg, isOldButlerRepository = self._getRepositoryCfg(repoData.repoArgs)              if cfg is None:                  if 'w' not in repoData.repoArgs.mode:                      raise RuntimeError(                          ""No cfg found for read-only input repository at {}"".format(repoData.repoArgs.cfgRoot))                  repoData.setCfg(cfg=RepositoryCfg.makeFromArgs(repoData.repoArgs),                                  origin='new',                                  root=repoData.repoArgs.cfgRoot,                                  isV1Repository=isOldButlerRepository)              else:                  if 'w' in repoData.repoArgs.mode:                      # if it's an output repository, the RepositoryArgs must match the existing cfg.                      if not cfgMatchesArgs(repoData.repoArgs, cfg):                          raise RuntimeError((""The RepositoryArgs and RepositoryCfg must match for writable "" +                                              ""repositories, RepositoryCfg:{}, RepositoryArgs:{}"").format(  >                                               cfg, repoData.repoArgs))  E                       RuntimeError: The RepositoryArgs and RepositoryCfg must match for writable repositories, RepositoryCfg:RepositoryCfg(root='/Users/square/jenkins/workspace/stack-os-matrix/osx.py3/lsstsw/build/obs_sdss', mapper=<lsst.obs.sdss.sdssMapper.SdssMapper object at 0x141d71320>, mapperArgs={}, parents=[], policy=None), RepositoryArgs:RepositoryArgs(root='/Users/square/jenkins/workspace/stack-os-matrix/osx.py3/lsstsw/build/obs_sdss', cfgRoot=None, mapper=<lsst.obs.sdss.sdssMapper.SdssMapper object at 0x141d4d320>, mapperArgs=None, tags=set(), mode='rw', policy=None)  ../../stack/DarwinX86/daf_persistence/13.0-32-g7b14ddd/python/lsst/daf/persistence/butler.py:799: RuntimeError  {code}",1
"DM-11692","08/29/2017 14:00:20","artifact stack-os-matrix build .log files","Post merger of DM-11612, junit xml files have been recorded with a non-zero failure count even though the build completed successfully.  There is concern that there is a failure mode in which {{pytest}} observes a test failure but exists {{0}} (confirmed in DM-11693).    As the build is successful, {{lsstswBuild.sh}}'s log printing is not triggered.  It would be useful for debugging purposes to unconditionally artifact all of the build's {{*.log}} files.  Tangentially, saving all of the log files would mean that {{lsstswBuild.sh}}'s could be invoked without the {{--print-fail}}, possibly removing the need to address DM-5887.",2
"DM-11698","08/29/2017 15:39:57","Let ap_verify support multiple --dataIdString arguments","Tasks take multiple {{\-\-id}} arguments to allow data satisfying one or more complex conditions. {{ap_verify}} does not currently have this capability; if the user passes multiple {{\-\-dataIdString}} values, only the last will be used. This behavior should be changed to let users have more of the flexibility they expect from running {{CmdLineTasks}} directly.    Note that this ticket cannot be satisfied by using {{pipe.base.ArgumentParser}}, because that class assumes its program only needs repository (i.e., previously ingested) input.    This ticket may require changes to {{ap_pipe}} to forward multiple IDs to relevant tasks.",2
"DM-11718","08/30/2017 08:37:15","Select bounding box for each source","For the deblender to be useful in the long term, which includes possibly deblending entire images as a single blend, we need to be able to define a bounding box for each source that is a small fraction of the total image size (see the issue created by [~pmelchior]    at https://github.com/fred3m/deblender/issues/14).    This ticket will attempt to identify the best algorithm to create an initial bounding box large enough to fit all of the flux for _most_  individual objects, with the possibility of growing the box during optimization if the source appears to need a larger area.",1
"DM-11720","08/30/2017 08:43:56","Investigate feasibility to use colors to estimate source boundary boxes","For the deblender to be optimal in the long term survey, which includes possibly deblending entire images as a single blend, we need to be able to define a bounding box for each source that is a small fraction of the total image size (see the issue created by [~pmelchior]    at https://github.com/fred3m/deblender/issues/14).    Issue DM-11718 has been created to develop an effective algorithm to do this, but will not likely be added to the stack until after testing has been run on a large patch of real HSC images.    This ticket is created to track work that has already been done to test whether or not such an algorithm might be possible, and resulted in [this|https://github.com/lsst-dm/deblender_examples/blob/master/test_bbox.ipynb] ipython notebook, which shows that such a procedure is likely to exist.    It also shows the possibility of performing multi-band object detection in the future, although discussion with [~pmelchior] indicate that this might be difficult (and/or time consuming) in practice.",2
"DM-11725","08/30/2017 09:26:08","Add coverage testing of unittest","The {{pytest-cov}} plugin should be enabled to write coverage reports from unit testing. This ticket covers the work to enable per-product coverage testing solely from the unittests associated with that product. This can be enabled as part of normal test execution. It is possible to write coverage results to html, the terminal and XML. XML is needed for Jenkins support -- it may be desirable to default to HTML and terminal reporting but allow Jenkins to enable XML output via an environment variable.",3
"DM-11728","08/30/2017 11:35:37","lsst-build output not visible in jenkins console","This may only (always?) happen with the osx / py3 combination. The symptoms are that the console shows:    {code:java}  ### build  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Rebuild is commencing....stand by; using  -r tickets/DM-11663  {code}    with no further output for a considerable portion of the build but inspecting the node directly shows that the build is progressing normally.  Speculation on possible causes is:    * python stdout buffering  * jenkins agent buffering  * jdk issues -- of note is that the version of java on the osx nodes is at a different patch level",0.5
"DM-11731","08/30/2017 13:39:59","default min/max values for histogram need more significant digits","# Upload the attached file  # add a histogram for glon, all look good  # add a histogram for glat, an empty histogram shows up (see attached PNG file)  # open the option panel, the min/max vales are the same    We need to add more significant digits to those values so the min and the max values are different.  There is no point to get histogram if there is no spread in values. ",0
"DM-11735","08/30/2017 14:35:48","fix tearDown race conditions in obs_test","Because the test does not use a temp dir for each test method, rmdir in tearDown can collide:  https://ci.lsst.codes/blue/organizations/jenkins/stack-os-matrix/detail/stack-os-matrix/26515/tests/",0.5
"DM-11744","08/30/2017 17:48:29","Move ci_ctio0m9 to use LSST backed LFS","Currently the {{ci_ctio0m9}} package is using the github backed LFS server.  The typical policy is to use the LSST git-lfs server.  This package should be converted to use that backend.",1
"DM-11745","08/30/2017 17:55:47","Investigate wrapping external function calls in ap_verify","As discussed in https://jira.lsstcorp.org/browse/DM-10779, it may be possible to minimize code duplication in {{ap_verify}} by using a wrapper function to perform appropriate metrics handling on the library calls to {{ap_pipe}}.  This ticket is to investigate the utility of such an approach.",2
"DM-11758","08/31/2017 12:56:51","Make  WarpType configurable in GetCoaddAsTemplateTask","Now that there are multiple types of coadds for the different warp types, we should allow specifically requesting PSF-matched templates for image subtraction.   ",1
"DM-11771","08/31/2017 18:32:29","obs_base fails to build on NFS-mounted systems","When I tried to build {{obs_base}} on an NFS-mounted machine, I got the attached failure.  This is after the upgrade to {{pytest-xdist}}.",0.5
"DM-11772","08/31/2017 18:42:23","Study how to integrate metrics into Tasks","At the moment, we do not have a clear plan or framework for how we will make measurements of {{verify}} metrics, particularly those that are specific to a single Task's implementation details. Future development of {{ap_verify}} and feature requests for {{verify}} depend on how we intend to handle these metrics.    This ticket is for researching one or more designs for supporting metrics within the Task framework. The result shall be a tech note describing my understanding of the requirements for metrics usage, a summary of what {{verify}} currently supports, and specific proposals for how to incorporate metric measurements into Tasks and/or {{ap_verify}}. The note shall be sent for review to the stakeholders for {{verify}}, {{ap_verify}}, and {{pipe_base}}.",8
"DM-11784","09/01/2017 14:26:11","Find all the necessary data for jointcal/meas_mosaic comparison","The data that was selected for the jointca/meas_mosaic acceptance test is listed on this website:    https://confluence.lsstcorp.org/display/DM/JointCal+Acceptance+Tests    We need to confirm that it is all available and processed with a recent enough version of the stack, including necessary reference catalogs.",1
"DM-11785","09/01/2017 14:27:32","Run jointcal on acceptance data","Run jointcal on the acceptance data, and produce catalogs that are ready to be validated via validate_drp.",8
"DM-11790","09/01/2017 17:03:58","Move jointcal doc to DMTN-36","In order to make the jointcal document more readily accessible, I will port the current LaTeX into DMTN-36, using the lsst-tex system.",2
"DM-11791","09/02/2017 10:53:10","weekly release w_2017_35 failed","{{_35}} failed in the canonical build on lsst-dev failed due to a full disk or quota limit.    {code:java}                   afw: 13.0-94-ga1c4440+6 ....................................................................................:::::  [2017-09-02T07:30:59.486911Z] /home/lsstsw/jenkins/release/lsstsw/miniconda/lib/python2.7/config/libpython2.7.a(posixmodule.o): In function `posix_tempnam':  :::::  [2017-09-02T07:30:59.486972Z] /home/ilan/minonda/conda-bld/work/Python-2.7.12/./Modules/posixmodule.c:7578: warning: the use of `tempnam' is dangerous, better use `mkstemp'  :::::  [2017-09-02T07:30:59.874220Z] /home/lsstsw/jenkins/release/lsstsw/miniconda/lib/python2.7/config/libpython2.7.a(posixmodule.o): In function `posix_tmpnam':  :::::  [2017-09-02T07:30:59.874323Z] /home/ilan/minonda/conda-bld/work/Python-2.7.12/./Modules/posixmodule.c:7631: warning: the use of `tmpnam_r' is dangerous, better use `mkstemp'  :::::  [2017-09-02T07:30:59.874362Z] /home/lsstsw/jenkins/release/lsstsw/miniconda/lib/python2.7/config/libpython2.7.a(posixmodule.o): In function `posix_tempnam':  :::::  [2017-09-02T07:30:59.874401Z] /home/ilan/minonda/conda-bld/work/Python-2.7.12/./Modules/posixmodule.c:7578: warning: the use of `tempnam' is dangerous, better use `mkstemp'  :::::  [2017-09-02T07:31:10.858727Z] /tmp/cck8pdsX.s: Assembler messages:  :::::  [2017-09-02T07:31:10.858909Z] /tmp/cck8pdsX.s: Fatal error: can't close tests/ellipse.o: No space left on device  :::::  [2017-09-02T07:31:10.935937Z] scons: *** [tests/ellipse.o] Error 1  :::::  [2017-09-02T07:31:10.941966Z] scons: building terminated because of errors.  ERROR (206 sec).  *** error building product afw.  *** exit code = 2  *** log is in /home/lsstsw/jenkins/release/lsstsw/build/afw/_build.log  *** last few lines:  {code}  ",0.5
"DM-11800","09/05/2017 10:59:06","Update meas_mosaic to new transform-based API for afw::cameraGeom","With the merge of DM-5922, {{meas_mosaic}} is currently broken with errors like:  {code}  src/mosaicfit.cc:2500:38: error: 'lsst::afw::cameraGeom::CameraTransformMap' has not been declared                       afw::cameraGeom::CameraTransformMap::Transforms newTr;                                        ^  {code}    Please update {{meas_mosaic}} to the new {{afw::cameraGeom}} API.",1
"DM-11802","09/05/2017 11:49:55","bump eups version to 2.1.4","Update the eups version to {{2.1.3}} for an important py3 compatibility fix.",0.5
"DM-11809","09/05/2017 15:49:25","Explicitly enable pep8-naming tests","Currently, {{flake8}} is installed with minimal plugins and does not include {{pep8-naming}}. This is problematic on two counts:    # The developer guide explicitly specifies naming tests with a set of ignored codes. These codes are not currently checked.  # If a developer happens to have {{pep8-naming}} installed their build can break because the flake8 testing in CI and in default builds is not testing what it should be.    I am going to add {{pep8-naming}} eups package and enable name testing. This will also require fixing any flake8 problems in the packages that have automated flake8 testing.    I do not believe an RFC is required to add this package as the developer guide already implies we are using it.",1
"DM-11862","09/07/2017 17:50:11","writeFitsCatalogStorage calls obj.writeFits incorrectly","{{writeFitsCatalogStorage}} calls {{obj.writeFits(logLoc.locString(), flags=flags)}}, where {{obj}} is ""the object to be written"". If {{obj}} is an {{afw.Persistable}}, this is an invalid call (Persistable.writeFits doesn't take {{flags}}), so I cannot persist PhotoCalibs in jointcal:    {code}      dataRef.put(photoCalib, 'photoCalib')    File ""/home/parejkoj/lsst/lsstsw/stack/Linux64/daf_persistence/13.0-34-g674f0d6/python/lsst/daf/persistence/butlerSubset.py"", line 216, in put      self.butlerSubset.butler.put(obj, datasetType, self.dataId, doBackup=doBackup, **rest)    File ""/home/parejkoj/lsst/lsstsw/stack/Linux64/daf_persistence/13.0-34-g674f0d6/python/lsst/daf/persistence/butler.py"", line 1429, in put      location.getRepository().write(location, obj)    File ""/home/parejkoj/lsst/lsstsw/stack/Linux64/daf_persistence/13.0-34-g674f0d6/python/lsst/daf/persistence/repository.py"", line 189, in write      return butlerLocationStorage.write(butlerLocation, obj)    File ""/home/parejkoj/lsst/lsstsw/stack/Linux64/daf_persistence/13.0-34-g674f0d6/python/lsst/daf/persistence/posixStorage.py"", line 264, in write      writeFormatter(butlerLocation, obj)    File ""/home/parejkoj/lsst/lsstsw/stack/Linux64/daf_persistence/13.0-34-g674f0d6/python/lsst/daf/persistence/posixStorage.py"", line 713, in writeFitsCatalogStorage      obj.writeFits(logLoc.locString(), flags=flags)  TypeError: writeFits(): incompatible function arguments. The following argument types are supported:      1. (self: lsst.afw.table.io.persistable.Persistable, fileName: str, mode: str='w') -> None      2. (self: lsst.afw.table.io.persistable.Persistable, manager: lsst.afw.fits._fits.MemFileManager, mode: str='w') -> None    Invoked with: PhotoCalib(spatially constant with mean: 7.30334e+12 err: 2.15887e+09), '/home/parejkoj/lsst/jointcal/jointcal/tests/.test/JointcalTestCFHTMinimal/run/jointcal-results/06AL01/D3/2006-05-20/r/0/photoCalib-849375-12.fits_mcaoaru'; kwargs: flags=0  {code}",1
"DM-11863","09/07/2017 19:52:14","Fix bug in strict monotonicity","There is a bug in the strict monotonicity algorithm that occurs when a faint object is close enough to a bright object that its initial SED is a poor choice, and is closer to the SED of its brighter neighbor. In this case the algorithm completely fails.    To test the projection, I ran a deblend using only the symmetry operator, which resulted in the first attached image. I then ran the projection, which resulted in the 2nd image, which is not strictly monotonic. This ticket is to investigate the origin of this strange behavior.",1
"DM-11867","09/08/2017 15:25:22","Update developer guide with flake8 testing instructions","Now that flake8 testing can be enabled as part of normal tests, we should document how to do that.",1
"DM-11870","09/09/2017 11:06:54","weekly release w_2017_36 failed","The build / eups distrib tag / tarball completed but the pipeline died in the container ""build from eups distrib"" before building the jupyterlabdemo image. Presumably, the failure was because {{lsst_distrib}} can no longer be built in the docker default 10GiB root volume size.    The easiest solution would be to replace the eupspkg build with a tarball install.    {code:java}      docker-centos-7: [ 72/111 ]  afw 13.0-97-g247b2c20e ... scons: building terminated because of errors.      docker-centos-7: IOError: [Errno 28] No space left on device:      docker-centos-7: File ""/opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/scons/2.5.0.lsst2+1/lib/scons/SCons/Script/Main.py"", line 1346:      docker-centos-7: _exec_main(parser, values)      docker-centos-7: File ""/opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/scons/2.5.0.lsst2+1/lib/scons/SCons/Script/Main.py"", line 1309:      docker-centos-7: _main(parser)      docker-centos-7: File ""/opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/scons/2.5.0.lsst2+1/lib/scons/SCons/Script/Main.py"", line 1091:      docker-centos-7: nodes = _build_targets(fs, options, targets, target_top)      docker-centos-7: File ""/opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/scons/2.5.0.lsst2+1/lib/scons/SCons/Script/Main.py"", line 1283:      docker-centos-7: jobs.run(postfunc = jobs_postfunc)      docker-centos-7: File ""/opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/scons/2.5.0.lsst2+1/lib/scons/SCons/Job.py"", line 113:      docker-centos-7: postfunc()      docker-centos-7: File ""/opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/scons/2.5.0.lsst2+1/lib/scons/SCons/Script/Main.py"", line 1280:      docker-centos-7: SCons.SConsign.write()      docker-centos-7: File ""/opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/scons/2.5.0.lsst2+1/lib/scons/SCons/SConsign.py"", line 109:      docker-centos-7: syncmethod()      docker-centos-7: File ""/opt/lsst/software/stack/stack/miniconda3-4.2.12-7c8e67/Linux64/scons/2.5.0.lsst2+1/lib/scons/SCons/dblite.py"", line 119:      docker-centos-7: self._pickle_dump(self._dict, f, 1)      docker-centos-7: Exception IOError: (28, 'No space left on device') in <bound method dblite.__del__ of <SCons.dblite.dblite object at 0x350e490>> ignored      docker-centos-7: + exit -5  {code}  ",2
"DM-11871","09/10/2017 08:38:04","afw.table column access is slow via__getitem__ (as compared to get)","See https://community.lsst.org/t/afwtable-performance-degradation-using-brackets-vs-explicit-get-for-fields.",1
"DM-11873","09/11/2017 11:55:01","Optimize QA code for speed in accessing/assigning afw table columns","The QA scripts in {{pipe_analysis}} do a lot of column accessing & assigning of afw tables.  A recent [community post|https://community.lsst.org/t/afwtable-performance-degradation-using-brackets-vs-explicit-get-for-fields/2236] highlighted the inefficient nature of accessing catalog elements with {{catalog\[fieldName\]}} vs {{catalog.get(fieldName)}} (or, even better, {{catalog.get(filedNameKey)}}).  Please update the code for speed efficiency according to the findings and recommendations of [this community post|https://community.lsst.org/t/afwtable-performance-degradation-using-brackets-vs-explicit-get-for-fields/2236].",2
"DM-11895","09/12/2017 11:21:21","Support getting data for current card of FitsChan","AST {{astGetFits<X>}} functions ({{astGetFitsS}}, {{astGetFitsF}}, etc.) take a name to specify which FITS card to read; if the name (a {{char *}}) is {{NULL}} then the current card is read. astshim does not yet support the ability to read the current card, and it is very useful; in particular it would be a big help for DM-10765. I propose to add this functionality as follows:  - if {{name}} is """" then read the current card  - {{name}} will default to """"  - In the pybind11 wrapper allow name=None, which will be the default    In addition, the most recent version of AST enhances {{astTestFits}} to support {{name=NULL}} for the current card. Add similar support to astshim, as above.    An obvious alternative is to add an overload for each {{FitsChan.getFIts<X>}} that has no {{name}} argument. That is arguably a bit more C++-like, but in my opinion it adds too many new member functions to be worthwhile. Plus overloads make the Python interface a bit harder to understand.",3
"DM-11902","09/13/2017 16:58:52","LSST Eigen installs are not discoverable by cmake","Our Eigen package has an all-too-clever eupspkg.cfg.sh that ""emulates"" the CMake build that comes with Eigen - and fails to install files that other CMake packages can use to find it.    This was discovered while testing an external PR on ndarray that improves and cleans up its CMake build.    My planned solution is to simply remove the eupspkg.sh and let Eigen's CMake build do its thing.",1
"DM-11904","09/13/2017 17:17:59","""namespace"" all newinstall.sh functions","Virtually all {{newinstall.sh}} functions should be namespaced to facilitate using the script as a ""library"" from other scripts.  At present, only the most recently modified functions have been namespaced under {{n8l::}}.",2
"DM-11909","09/14/2017 09:43:33","Test functionality of deblender task ","DM-11329 involved creating a deblender task to run the new deblender on HSC data. This ticket is designed to run that task on a few HSC fields to test its performance before the larger task of running the deblender on a large patch of data and analyzing the results (DM-11330). ",5
"DM-11916","09/14/2017 12:49:22","Warnings ""Extent2I object has no attribute getWidth"" when running meas_mosaic, jointcal","Running  {{MosaicTask}} with {{w_2017_36}} and current {{meas_mosaic}}  gives many warnings like this:    {code:java}  root WARN: Failed to read {...}: 'lsst.afw.geom.coordinates.coordinates.Extent2I' object has no attribute 'getWidth'  {code}      ",1
"DM-11921","09/14/2017 18:53:01","Change match method in DIAObjectCollection to return DIAObject ids","The match method of DIAObject Collection currently returns the indices of the DIAObjects in the collection that are updated rather than their unique ids. This ticket is to change this return from the former to the latter.",2
"DM-11931","09/15/2017 16:23:29","Fix jointcal exit status and doRaise handling","jointcal is giving these warnings since DM-4141 was implemented:    {code}  Unable to retrieve exit status ('Struct' object has no attribute 'exitStatus')   {code}    To fix this, I think I need to do the following:    * add this to the start of {{run}}: {{exitStatus = 0  # exit status for shell}}  * add {{exitStatus}} to the returned Struct  * wrap most of {{run}} in {{try: except:}}  *  set {{exitStatus=1}} (or some other number) if {{doRaise}} is False and something went wrong.    Anything else I'm missing?",2
"DM-11942","09/19/2017 07:27:34","Create new JIRA teams","Per RFC-385, please create the following new JIRA teams:    - System Management  - DM Science  - Data Facility",1
"DM-11943","09/19/2017 07:33:00","Map tickets from old teams to new teams","Per RFC-385, please ensure that all tickets currently assigned to ""old"" teams are assigned to one of the ""new"" teams.    The following direct mappings apply:    - Process Middleware → Data Facility  - Site Infrastructure → Data Facility    The following will need manual intervention to map tickets to whichever team is paying for the work:    - DMLT / Management  - Project Science  - T/CAMs",2
"DM-11944","09/19/2017 08:11:59","Delete old JIRA teams","Please verify that there are no tickets currently assigned to the following teams (this should be the case after DM-11943 is completed), then delete them:    - DMLT / Management  - T/CAMs  - Project Science  - Process Middleware  - Site Infrastructure",1
"DM-11945","09/19/2017 08:19:40","Document usage of groups and labels on JIRA","Per RFC-385, we have defined a number of teams and standard labels for use on JIRA. Please ensure these are described in the Developer Guide.",1
"DM-11948","09/19/2017 09:53:44","Add option to force detections in coaddDriver","If detections exist somewhere in the butler hierarchy the detection task is not being run. This is problematic if the input repository is a reprocessing of data. Add option to force rerunning detections.",1
"DM-11949","09/19/2017 11:35:36","newinstall.sh should warn about env vars known to cause problems","https://pipelines.lsst.io/v/14-0/install/newinstall.html#running-newinstall-sh-in-an-already-set-up-shell",1
"DM-11954","09/19/2017 12:36:46","add `w_latest` and `d_latest` docker image tags","We are publishing weekly and daily tags to the docker.io registry but it would be convent to also have {{7-stack-lsst_distrib-[dw]_latest}} tags.",0.5
"DM-11957","09/19/2017 18:40:59","Cannot round-trip >7th degree Chebyshev photometry models","If I set {{photometryVisitDegree > 7}} in jointcal's {{ConstrainedPhotometryModel}}, I receive {{RuntimeError: Could not read an AST object from this channel}} when trying to unpersist the {{PhotoCalib}} object containing the {{TransformPoint2ToGeneric}} that I had written via {{butler.put('photoCalib', photoCalib)}}. I will attach example files produced with degree=7 and 11, to help with debugging.",3
"DM-11966","09/20/2017 18:06:58","Update DCR and simulation tools to Python 3","Fix python 3 compatibility issues in the prototype DCR code and StarFast simulator before stackification.",1
"DM-11971","09/21/2017 10:33:44","memory leak in astshim isSeries function","Running the attached script results in the failure    {code}  ERROR: RuntimeError: AST: Error at line 51 in file src/detail/utils.cc.AssocId(WinMap): There are too many AST Objects in use at once. [__main__]  Traceback (most recent call last):    File ""cause_failure_afwCameraGeom.py"", line 13, in <module>      pt = getCenterPixel('R:2,2 S:1,1')    File ""cause_failure_afwCameraGeom.py"", line 7, in getCenterPixel      centerPoint = camera[name].getCenter(FOCAL_PLANE)  RuntimeError: AST: Error at line 51 in file src/detail/utils.cc.AssocId(WinMap): There are too many AST Objects in use at once.  {code}    It is possible that this is expected/desirable behavior, given that I am needlessly calling {{makeCameraSys}} over and over again on the same inputs.  I was under the impression, however, that these would get deleted upon exiting the {{getCenterPixel}} method defined in the script.",3
"DM-11973","09/21/2017 11:01:21","run-rebuild job does not artifact build logs","The jenkins {{run-rebuild}} job does to artifact the build logs, other than the {{manifest.txt}} as is done by {{stack-os-matrix}}.  This makes debugging build problems difficult.",0.5
"DM-11974","09/21/2017 12:12:04","Add calibUsed-only qa plots to CoaddAnalysis","Add plots similar to the visit-level *calibUsed*-only versions added in DM-11322 but at the coadd level. This cannot be done until the flags are propagated to the coadd catalogs when DM-11866 lands.",1
"DM-11987","09/21/2017 18:34:07","ap_verify should allow output to non-empty repos","At present, {{ap_verify}} will not run if the destination (output repo) has anything in it. Ideally, it would not care and use the checks in {{ap_pipe}} to be able to pick up where it left off. For example, if it previously completed successfully through raw image ingestion only, I don't want to have to blow that away in order to resume running the next step (calib ingestion).",1
"DM-12003","09/22/2017 15:07:14","Add default slot for PSF shape","{{validate_drp}} needs to access various columns from source catalogs to do its work.  Not all of these have slots defined.  I am going to add the slot that's missing so the validation scripts are more future proof.",2
"DM-12006","09/23/2017 11:07:36","Improve testing of copyPolymorphic","While working on DM-11971 I found that {{CmpMap}} was missing an override of {{copyPolymorphic}}. Improve the unit tests so this would be caught.    Consider removing the definition of {{copyPolymporphic}} and {{copy}} from {{Mapping}}, since that class is abstract. This may make it easier to detect omission in subclasses of {{Mapping}}, since such code will not compile. However, that still leaves deeper classes that could forget to override it, so unit testing is more important.    Finally, remove the {{virtual}} since it is implied by {{override}}.    Consider adding explicit @copydoc to {{copyPolymorphic}}, though Doxygen already shows the documentation.",0.5
"DM-12007","09/25/2017 10:26:12","Add tests to display_firefly","Following discussions at the 2017 Project & Community Workshop, add a minimal test suite to the display_firefly package. Since a Firefly server is needed to use this package, a minimal test suite can implement an import test. Interactive user tests are already included in the afw.display package.",2
"DM-12008","09/25/2017 10:28:23","Add a test suite to firefly_client","Following discussion at the 2017 Project & Community Workshop, implement a minimal test suite for firefly_client. A minimum automated test is to import the package. ",2
"DM-12015","09/25/2017 17:04:08","Derive SQuaRE, CI and commissioning butler requirements","Go through CI, commissioning, and SQuaRE use cases and derive requirements for them.",2
"DM-12019","09/26/2017 09:11:08","Document use of k8s setup","Explain to [~jgates] how to use k8s on ccqserv100 to ccqserv124 and improve related documentation.",8
"DM-12028","09/26/2017 13:58:16","newinstall.sh should not set PATH unless miniconda is bootstraped","{code:java}  Craig Loomis [10:40 AM]   A fresh `https://raw.githubusercontent.com/lsst/lsst/master/scripts/newinstall.sh`, run without installing a miniconda, generates a loadLSST.bash which contains a troublesome `export PATH=""/bin:${PATH}""`. Not entirely sure what is supposed to be happening with the `miniconda_path`, etc. variables in the use-user-anaconda case. MacOS 10.12, anaconda 4.4.0.      Tim Jenness  [10:42 AM]   @josh  I think you need to protect `export PATH=""${miniconda_path}/bin:${PATH}""` so that it only happens if we installed miniconda. (edited)      [10:43]   actually it’s this line sorry:  ```local cmd_setup_miniconda=""export PATH=\""${miniconda_path}/bin:\${PATH}\""""```  {code}  ",0.5
"DM-12029","09/26/2017 15:15:16","recompress jointcal's testdata zeroed images with fpack","Now that the stack supports tile compression of FITS files, and a number of bugs related to handling of FITS image metadata were fixed (DM-11332), I can recompress the images (really just holders of the metadata) I zeroed out in {{testdata_jointcal}} so that they aren't just gzipped files. Once done I can use the new butler calls (DM-9060, DM-9153) to directly access that metadata. This should drastically speed up file ingestion for jointcal.    The compression step is probably best done via a short bash script that runs gunzip and then fpack, with some carefully chosen parameters (the files are zeroed, so it probably doesn't matter much which compression scheme is selected).    If this works, we can close DM-6911 as well, as that probably got fixed ""for free"" as part of DM-11332.",2
"DM-12030","09/27/2017 09:44:44","Persist parquet tables from pipe_analysis scripts","In order to make interactive QA plots such as DM-11682, we need to persist parquet files containing the compiled data that are used to make the static matplotlib plots.  Admittedly, this should be a temporary step until all the source info is easily loadable from a column-store database, but for now it is necessary.  At the visit level it might be less so, but I'll do it anyway.  This is probably technically part of DM-10859, but I thought it might be useful to get it in its own ticket so it can be merged quickly.",1
"DM-12031","09/27/2017 10:39:16","Write presentation for DESC Blending Task Force meeting","I've been asked to outline the DRP pipelines to kick off DESC's Blending Task Force on Monday, October 2 at 12pm.  This will take some preparation.  ",2
"DM-12038","09/27/2017 14:41:05","Update pipe_analysis to provide inputs for Bokeh based QA system","Together with [~lauren], ensure that the pipe_analysis scripts efficiently & effectively dump all the information that Tim's plotting scripts need to Parquet files without wasting time generating redundant plots or stripping out useful data.",2
"DM-12040","09/27/2017 17:33:47","Errors in test_transformFactory.py","test_transformFactory.py has a few errors that flake8 catches in the method {{testLinearize}} including:  - rng is undefined  - nDelta is undefined    The reason the test passes is that the code in question never runs: {{invertible}} is False for the one time the code in question runs.    This code looks like a potentially useful check for several different transforms (e.g. a radial polynomial and a normal polynomial). Perhaps it should be moved into a new ""check<something>"" method and called in more than one place?",2
"DM-12052","09/28/2017 15:24:23","Upgrade ws4py to 0.4.2","A {{pip install firefly_client}} pulls in the latest released version of ws4py for web sockets, as version 0.4.2, released on 2017 March 29. This package is included in the LSST stack as a TaP package, version 0.3.5 released on 2014 April 1. This ticket is for upgrading the TaP package to version 0.4.2.",2
"DM-12053","09/28/2017 15:29:20","Package v1.2 of firefly_client for PyPI and sync the LSST fork","Recent improvements to the {{firefly_client}} package need to be gathered into a v1.2 release due to some API changes. At an appropriate point, tag this version, publish to PyPI (for {{pip install firefly_client}}, and synchronize the fork in the lsst Github org.",3
"DM-12061","09/29/2017 12:59:42","Eliminate test warnings in test_methods.py","test_methods.py issues {{DeprecationWarnings}} while testing deprecated assert methods. As of Python 3.2 unittest offers {{assertWarns}} which would allow us to make sure the deprecated methods actually do issue the appropriate warning, and stop pytest from complaining.    The trick will be making sure the tests run under older versions of Python (even if pytest warns in that situation).  ",1
"DM-12068","10/02/2017 06:45:29","Create Presentation for STSci PSF Photometry Workshop","Create a 20 minute presentation on the deblender and the LSST's current plan for photometry in crowded fields.",2
"DM-12069","10/02/2017 09:54:59","In imageREST_v1 _file_response(): investigate writing lsst.afw.Image straight to data vs temp FITS file first","There is existing comment inside imageREST_v1.py mentioning the desirability of writing Image object to data object in memory instead of to a temporary FITS file, then reading it into memory, to improve image response performance and avoidance of using temp files in the local file system.     Since afw.Image does not provide an existing/ready method to do so, some investigation of alternative ways of doing this is warranted.      ",5
"DM-12070","10/02/2017 10:17:38","Include obs_ctio0m9 and obs_comCam in lsst_distrib","Implementation ticket for RFC-391.    Get {{obs_ctio0m9}} and {{obs_comCam}} included in {{lsst_obs}} and thus {{lsst_distrib}}",1
"DM-12080","10/02/2017 13:28:45","Update lsst_repos to include obs_comCam","Add {{obs_comCam}} to https://github.com/lsst/repos",1
"DM-12085","10/02/2017 14:06:21","Camera geometry incorrect and outdated in obs_test","The optical distortion model is backwards.",1
"DM-12089","10/03/2017 12:03:02","Plotly chart with 1M points","While it's generally a bad idea to plot more than 100,000 points, Firefly should be still producing the requested plot, however slow.    Several issues are covered by this Story, the attached test comes from David Shupe.    1. Firefly produces “Maximum call stack size exceeded”, when mapping large table columns into points to plot.    2. hoverinfo = 'skip' is not honored. When no hover layer is present, tooltips are not displayed and point click and select are not supported. The chart should be displayed with no highlight or select layer and no point select option.    Also as a part of this ticket remove fireflyScatter type. Regular scatter plot should behave as fireflyScatter (with the improved tooltips and axis labels).     ",5
"DM-12102","10/04/2017 10:46:19","add input validation for SpherePoint(double, double, AngleUnits) constructor","Add input validation for the new {{SpherePoint(double, double, AngleUnits)}} constructor",1
"DM-12104","10/04/2017 11:57:41","Add JSON Schema validation package for ImageServ","The specification for the data model of imageREST_v1 is based on the standard JSON schema specification: http://json-schema.org.    The schema validator package for JSON-based queries is available on Github: https://github.com/Julian/jsonschema.  ",5
"DM-12107","10/04/2017 12:49:42","add CI (jenkins) to developers docs","Create dev/user documentation for the DM Jenkins instance.  Presumably, to supplant or be linked to from:    https://developer.lsst.io/processes/workflow.html?highlight=jenkins#testing-with-jenkins",0.5
"DM-12108","10/04/2017 12:55:54","Add fake sources after wcs update","As it stands now, fake sources that are added according to wcs positions are using the wcs provided by the telescope which can be off. If adding fake sources this should happen after wcs has been updated. This also means that detection and measurements will need to be re-run such that the fake sources are inserted,",1
"DM-12117","10/05/2017 12:37:56","repositoryCfg.yaml input root not backwards compatible","The following butler command fails  on a dataset on lsst-dev that was written with {{w_2017_17}}, when using a more recent stack (in this case, {{w_2017_39}}). It also fails if {{root=PATH}} is used instead of just passing the path as the first arg. See discussion beginning here on slack: https://lsstc.slack.com/archives/C3UCAEW3D/p1507226925000049    {code}  lsst.daf.persistence.Butler(""/datasets/hsc/repo/rerun/DM-10404/SFM/"")  {code}    On the other hand, if the input is specified as ""inputs=\{'root':PATH\}"" it succeeds. Either {{RepositoryCfg_v1}} should have its version number bumped, or the newer Butler should transparently read the older .yaml.    Full stack trace of the failure:    {code}  In [7]: lsst.daf.persistence.Butler(root=""/datasets/hsc/repo/rerun/DM-10404/SFM/     ...: "")  ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)  <ipython-input-7-04365c25b492> in <module>()  ----> 1 lsst.daf.persistence.Butler(root=""/datasets/hsc/repo/rerun/DM-10404/SFM/"")    /software/lsstsw/stack/Linux64/daf_persistence/13.0-40-g460649b+5/python/lsst/daf/persistence/butler.pyc in __init__(self, root, mapper, inputs, outputs, **mapperArgs)      523         self._addParents(repoDataList)      524  --> 525         self._setAndVerifyParentsLists(repoDataList)      526      527         self._setDefaultMapper(repoDataList)    /software/lsstsw/stack/Linux64/daf_persistence/13.0-40-g460649b+5/python/lsst/daf/persistence/butler.pyc in _setAndVerifyParentsLists(self, repoDataList)      900                 repoData.cfg.addParents(parents)      901             elif repoData.cfgOrigin in ('existing', 'nested'):  --> 902                 if repoData.cfg.parents != parents:      903                     try:      904                         repoData.cfg.extendParents(parents)    /software/lsstsw/stack/Linux64/daf_persistence/13.0-40-g460649b+5/python/lsst/daf/persistence/repositoryCfg.pyc in __eq__(self, other)       95         if not other:       96             return False  ---> 97         return self.root == other.root and \       98             self.mapper == other.mapper and \       99             self.mapperArgs == other.mapperArgs and \    AttributeError: 'str' object has no attribute 'root'  {code}",3
"DM-12121","10/05/2017 14:38:38","build-jupyterlabdemo failing","https://ci.lsst.codes/job/sqre/job/infrastructure/job/build-jupyterlabdemo/67/consoleFull    {code:java}  > npm pack /usr/share/git/jupyterlab-hub  npm WARN lifecycle @jupyterlab/hub-extension@0.4.1~prepublish: cannot run in wd %s %s (wd=%s) @jupyterlab/hub-extension@0.4.1 npm run build /usr/share/git/jupyterlab-hub  jupyterlab-hub-extension-0.4.1.tgz  > node node-version-check.js  Uninstalling @jupyterlab/hub-extension from /usr/share/jupyter/lab/extensions  Traceback (most recent call last):    File ""/usr/bin/jupyter-labextension"", line 11, in <module>      load_entry_point('jupyterlab==0.28.0.dev0', 'console_scripts', 'jupyter-labextension')()    File ""/usr/lib/python3.4/site-packages/jupyter_core/application.py"", line 267, in launch_instance      return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)    File ""/usr/lib/python3.4/site-packages/traitlets/config/application.py"", line 658, in launch_instance      app.start()    File ""/usr/lib/python3.4/site-packages/jupyterlab/labextensions.py"", line 168, in start      super(LabExtensionApp, self).start()    File ""/usr/lib/python3.4/site-packages/jupyter_core/application.py"", line 256, in start      self.subapp.start()    File ""/usr/lib/python3.4/site-packages/jupyterlab/labextensions.py"", line 90, in start      raise e    File ""/usr/lib/python3.4/site-packages/jupyterlab/labextensions.py"", line 86, in start      build(self.app_dir, logger=self.log)    File ""/usr/lib/python3.4/site-packages/jupyterlab/commands.py"", line 525, in build      return IOLoop.instance().run_sync(func)    File ""/usr/lib64/python3.4/site-packages/tornado/ioloop.py"", line 458, in run_sync      return future_cell[0].result()    File ""/usr/lib64/python3.4/site-packages/tornado/concurrent.py"", line 238, in result      raise_exc_info(self._exc_info)    File ""<string>"", line 4, in raise_exc_info    File ""/usr/lib64/python3.4/site-packages/tornado/gen.py"", line 1069, in run      yielded = self.gen.send(value)    File ""/usr/lib/python3.4/site-packages/jupyterlab/commands.py"", line 541, in build_async      _ensure_package(app_dir, name=name, version=version, logger=logger)    File ""/usr/lib/python3.4/site-packages/jupyterlab/commands.py"", line 823, in _ensure_package      shutil.copy(src, dest)    File ""/usr/lib64/python3.4/shutil.py"", line 229, in copy      copyfile(src, dst, follow_symlinks=follow_symlinks)    File ""/usr/lib64/python3.4/shutil.py"", line 108, in copyfile      with open(src, 'rb') as fsrc:  FileNotFoundError: [Errno 2] No such file or directory: '/usr/lib/python3.4/site-packages/packages/shortcuts-extension/schema/plugin.json'  /bin/sh: line 0: cd: jupyterlab-savequit: No such file or directory  The command '/bin/sh -c mkdir -p /usr/share/git &&      cd /usr/share/git &&      npm install -g webpack &&      git clone https://github.com/jupyterhub/jupyterlab-hub.git &&      git clone https://github.com/lsst-sqre/jupyterlab-savequit &&      for i in hub savequit; do        cd jupyterlab-${i} &&          npm install --unsafe-perm &&          python3 /usr/bin/jupyter labextension link . &&          npm run build &&          cd .. ;     done' returned a non-zero code: 1  Docker build failed.  {code}  ",3
"DM-12128","10/05/2017 17:11:15","Fix y-band for HSC in validate_drp","The filter maps are not quite right in validate_drp for HSC any more.  Fix up the filtermaps so that y-band data can run to completion.",2
"DM-12132","10/06/2017 08:18:27","Finish high-level Butler design (J)","Turn DMTN-056 into a polished, internally-consistent proposal for a new Butler design.    This should include:   - Python and SQL interfaces at and above the Registry and Datastore level   - walkthroughs of how the design supports various use cases   - a preliminary list of the concrete Registry and Datastore implementations needed.    This is the ticket for [~jbosch]'s piece of the work; a similar ticket will follow for [~pschella].",8
"DM-12135","10/06/2017 08:19:45","Finish high-level Butler design (P)","Turn DMTN-056 into a polished, internally-consistent proposal for a new Butler design.    This should include:   - Python and SQL interfaces at and above the Registry and Datastore level   - walkthroughs of how the design supports various use cases   - a preliminary list of the concrete Registry and Datastore implementations needed.    This is the ticket for [~pschella]'s piece of the work; DM-12132 captures [~jbosch]'s.  ",8
"DM-12153","10/06/2017 13:55:57","Replication master/worker comms improvements","Refined and improved communication model providing master-side connection pull and logical connection multiplexing",8
"DM-12155","10/06/2017 13:57:27","Replication state schema design","Database schema to support persistent states for requests, complex replication operations, replicas statuses (including file sizes, checksums), and persistent logging.",8
"DM-12159","10/06/2017 14:06:57","Replication DB implementation","Implementing database support for requests and jobs",8
"DM-12170","10/06/2017 16:24:46","Create LDM for Butler Use Cases","Convert the Butler Use Cases developed by the Working Group into an LDM document ready for DM internal review.",8
"DM-12174","10/08/2017 16:09:51","community.lsst.org backups not being sent to S3","The daily backups on community.lsst.org _should_ be uploaded to S3 (builtin Discourse functionality), but they aren't. We are doing weekly DigitalOcean snapshots that mitigate some of the data loss risk, but it would be better if we copied those daily backups.    Check if it's a permissions issue that can be fixed. Alternatively, set up a {{cron}} service that {{scp}}s the backup to S3.",1
"DM-12184","10/09/2017 16:57:07","Coaddition Tasks cannot assume that N masks can fit in memory","[~pgee] and [~tjohnson] have recently reported high memory usage by {{SafeClipAssembleCoadd}} for examples with either large N (number of warps) or large patch sizes.  For simulated data processed by DESC,   [~tjohnson] reported 70GB processes coadding 6kX6k patches and unknown N. (Fill this in if you remember)   For DES data, [~pgee] reported memory usage >32GB  with 10kX10k patches and unknown N.  (Note: patches this large are not recommended). Masks were still uint16 at the time.     Profiling reveals that this memory usage was likely due to holding N masks in memory. In this example, patches were 4200X4200, mask pixels = int32, and N=33. 4200*4200*4*33/2**20 = 2220MiB. Full profiling print out attached.     {code}  Line #    Mem usage    Increment   Line Contents  ================================================    1294    556.6 MiB      0.0 MiB       @profile    1295                                 def detectClip(self, exp, tempExpRefList):    1296                                     """"""!    1297                                     \brief Detect clipped regions on an exposure and set the mask on the individual tempExp masks    ...                               1326    586.2 MiB      0.0 MiB           clipFootprints = []    1327    586.2 MiB      0.0 MiB           clipIndices = []    1328                                 1329                                     # build a list with a mask for each visit which can be modified with clipping information    1330    586.2 MiB      0.0 MiB           tempExpClipList = [tmpExpRef.get(self.getTempExpDatasetName(self.warpType),    1331                                                                      immediate=True).getMaskedImage().getMask() for    1332   2806.8 MiB   2220.7 MiB                              tmpExpRef in tempExpRefList]    1333                                 1334   2813.1 MiB      6.3 MiB           for footprint in footprints.getFootprints():    1335   2813.1 MiB      0.0 MiB               nPixel = footprint.getArea()  {code}    For the simulated data example, N=~1000, patch size = 6KX6K, mask pixel size = uint16 (May 2016) , would have yielded a theoretical usage of 6e3*6e3*2*1000/2**30 = 67GiB.   For the DES, example with the 10kX10k patches any more than 150 visits would have not fit on the 32G system.    Now that we've switched to int32 masks this is even more important, as the memory usage doubles for examples with  large N or large patches. Even with 4k X 4k patches, it only takes N=180 to get memory usage > 10GB.    This ticket will  remove the assumption that N masks can fit in memory from the implementation of {{CompareWarpAssembleCoadd}}. Because the solution will likely be the same for SafeClipAssemble too it will likely be implemented at the same time. ",8
"DM-12189","10/10/2017 12:19:48","Firefly should not longer use path segment parameters, it should use query string","Firefly has been using path segment parameters to specify meta parameters {{;}} such as channel and action type.  This does not work exactly has expected on all servers. Therefore we are going to go back to using classic query string {{?}}. {{wsch}} (channel) parameter will now be {{__wsch}}, {{a}} (action) will now be {{__a}}    Fix should be put into rc.",2
"DM-12200","10/11/2017 11:48:09","jointcal tests fail when optional testdata_jointcal is not present","The most recently merged PR is https://github.com/lsst/jointcal/pull/45    {{jointcal}} from master is not buiding via {{eups distrib install}}.  I suspect this is because unit tests we introduced that are dependent on the {{setupOptiona}} {{testdata_jointcall}} product.  Test that rely on optional packages should bail out gracefully without failing.    {code:java}    [build]         lsst.afw.image.utils.resetFilters()                # Load or fake the necessary metadata for each CcdImage (using ccd=12)  >       data_dir = lsst.utils.getPackageDir('testdata_jointcal')  E       lsst.pex.exceptions.wrappers.NotFoundError:   E         File ""src/Utils.cc"", line 42, in std::string lsst::utils::getPackageDir(const string&)  E           Package testdata_jointcal not found {0}  E       lsst::pex::exceptions::NotFoundError: 'Package testdata_jointcal not found'    tests/test_photometryModel.py:26: NotFoundError  =============== 4 failed, 37 passed, 19 skipped in 6.84 seconds ================  Global pytest run: failed  {code}  ",0.5
"DM-12208","10/12/2017 10:42:52","Firefly viewer loading race condition - should wait for document load","Firefly sprint can attempt to access the DOM before it is ready.  It needs to check {{window.document.readyState}} and wait for onload if necessary.",2
"DM-12215","10/12/2017 15:55:47","matchedVisitMetricsTask outputPrefix results in hidden files","{{matchedVisitMetricsTask.py}} has a default {{outputPrefix="".""}}, which results in files like {{.\_PA1.png}} and {{..json}} in the directory it was run from. The default should probably be {{""""}}, or {{""./""}}, as the plotting code uses {{%s_%s}} with the first substitution being outputPrefix.    An alternate solution would be to use {{--output}} for this purpose, since this is a {{Task}}, and does appear to require an explicit output. Edit to add: the {{outputPrefix}} that gets passed on to {{validate}} should maybe be {{output/outputPrefix}}, though that still doesn't work with the substitution that's used when making the plot files, if outputPrefix is {{""""}}",0.5
"DM-12216","10/12/2017 19:10:48","Not all plot files append outputPrefix with an underscore","Some of validate_drp's plot filenames are created as {{outputPrefix+""something.png""}} instead of {{""%s_%s""%(outputPrefix, ""something.png"")}}. This can make the output filenames less readble. As an example showing the differences:    {code}  matchedVisit_HSC-IAM1_D_5_arcmin_17.0_21.5_mag.png  matchedVisit_HSC-IAM2_D_20_arcmin_17.0_21.5_mag.png  matchedVisit_HSC-ITE1_D_1_arcmin.png  matchedVisit_HSC-ITE2_D_5_arcmin.png  matchedVisit_HSC-I_PA1.png  matchedVisit_HSC-I_check_astrometry.png  matchedVisit_HSC-Icheck_photometry.png  {code}    Should be an easy fix, unless there's other code that expects the files to be handled that way?",0.5
"DM-12230","10/13/2017 17:10:53","Mapping.applyForward and applyInverse fail on empty arrays","Mapping.applyForward and Mapping.applyInverse fail on empty arrays: it triggers an AST error.    This showed up as a mysterious error in DM-10765 in afw test_footprint1.py    The fix is trivial: have Mapping._tran skip the call to astTranN if the input array has 0 points.",1
"DM-12232","10/16/2017 03:11:26","Create presentation for DESC blending group","The LSST-DESC blending group has asked Peter and I to give a presentation on the new deblender. Most of the talk was already created for LSST-2017, however some work is needed to update it with the latest results using multiple components and the current status.",1
"DM-12233","10/16/2017 07:24:57","Improve interface for multiple components","The current interface for the deblender takes constraints and components as arguments but doesn't utilize that information when building the constraints for each object (for example having default constraints for different component types). It also doesn't pass those parameters to the main NMF algorithm without being explicitly given as function arguments, so improving this interface would make constraints and components easier to use and more robust against inconsistencies (for example this weekend I had forgotten to pass the constraints to the function to build the monotonicity operator, causing half of the components to not have monotonicity applied).",3
"DM-12254","10/17/2017 15:30:27","Switch jointcal to default to IndexedRefObj instead of a.net","Jointcal currently defaults to using {{LoadAstrometryNetObjectsTask}} for its {{refObjLoader}}. This can be dangerous in that if one doesn't retarget in the config, loading the skyCircle will fail and getting to that point can take a long time with lots of files.    We also plan to phase out A.Net catalogs over time, so this is a good idea long term as well. It does require some reconfiguration of the unittests however.",1
"DM-12256","10/17/2017 17:26:31","Incorporate AssociationTask into ap_pipe","Let {{ap_pipe}} call {{AssociationTask}} and verify that the code runs to completion on the standard {{ap_pipe}} dataset ({{verify_ap_hits2015}}, {{visit=410985 ccdnum=25}}).",2
"DM-12257","10/17/2017 17:28:14","Implement association step in ap_verify","Let {{ap_verify}} call the association step in {{ap_pipe}} and verify that the code runs to completion on the standard {{ap_pipe}} dataset ({{verify_ap_hits2015}}, {{visit=410985 ccdnum=25}}).",1
"DM-12270","10/19/2017 10:54:37","AST persistence is not exact","AST persistence by writing to a string is not exact because double precision values are not written to sufficient precision. The result is tiny differences in computations from FrameSets that have been persisted and unpersisted. David Berry is working on a fix. This ticket provides a place to record the issue and mark temporary workarounds in our tests.",1
"DM-12272","10/19/2017 12:04:58","Fix bug in arrayFromVector","arrayFromVector has a bug: internally the strides vector is wrong, which can lead to memory issues. Fix this, wrap it for use in Python, and provide a unit test.    Also, coefficient array arguments for {{ChebyMap}}, {{PolyMap}} and {{MatrixMap}} do not have const elements. Fix this.",0.5
"DM-12305","10/19/2017 14:55:21","Research HiPS viewing in Firefly","Begin researching HiPS viewing in Firefly. Do the following:    * Learn about Healpix  * Learn about HiPS  * Determine how a viewer would fit into Firefly  * Look at Aladin Lite code and tool  * Look at Aladin Lite API  * If straightforward, add a proof of concept HiPS viewing into Firefly.",20
"DM-12306","10/19/2017 15:08:00","Make datasets optional for ap_verify","According to the [developer guide|https://developer.lsst.io/build-ci/new_package.html#handling-git-lfs-backed-repos], git-lfs repos cannot be a required dependency of any Stack package. {{ap_verify}} currently requires {{ap_verify_hits2015}} for testing purposes. This behavior must be changed before {{ap_verify}} can be included in {{lsst_apps}}.    Note that {{Dataset}} cannot be tested at all without a dataset package, and command-line parsing can only be partially tested.",2
"DM-12307","10/19/2017 15:48:02","update blue-ocean to 1.3.0","Blueocean 1.3.0 has been released and the changelog lists a fix for non-working http links in console output.  Perhaps this will also fix linking to triggered jobs?    changelog: https://plugins.jenkins.io/blueocean",1
"DM-12315","10/20/2017 12:15:21","Generalize ap_pipe to non-HiTS data","Currently, {{ap_pipe}} has some hardcoded assumptions that it is working on DECam data, and the ingestion step is specific to HiTS. Both of these assumptions should be lifted (likely delegating much of the work to a standards-compliant {{obs_*}} package) to allow {{ap_pipe}} to be run on arbitrary datasets.",8
"DM-12359","10/23/2017 11:38:55","send Task log output to stdout","Send Task {{lsst.log}} output to stdout instead of stderr.    This is the implementation ticket for RFC-402.",1
"DM-12361","10/23/2017 13:11:39","Qserv needs more changes after xrootd shared object renaming","In DM-12308 we changed the names of some shared objects that come from xrootd. There are still references to other shared object in xrootd config file that needs similar change.",0.5
"DM-12362","10/23/2017 14:54:45","SCons 3 rebuilds qserv binaries every time","For some reason new SCons 3 wants to rebuild all binaries (shared objects included) on every scons invocation.",0.5
"DM-12367","10/24/2017 09:14:43","Add an abstract base class for spatially-varying transmission curves","Add a C++ abstract base class (with Python wrappers) for classes that represent wavelength-dependent transmission (e.g. filter curves) as a function of position.    These should make it possible to use an implementation in one coordinate system (e.g. focal plane coordinates) to provide a transmission curve looked up via another set of coordinates (e.g. pixels) via a proxy subclass of the ABC.    Design probably deserves an RFC.  Many conversations on this the appropriate design for this functionality have been started but none have converged; some prior art exists in Sims.  Should defer discussions about functionality beyond what's needed for HSC filter curve coaddition as much as possible.    These objects will need to be persistable with afw::table::io.",2
"DM-12368","10/24/2017 09:16:23","Add a trivial spatially-constant transmission curve implementation","Implement the interface of DM-12367 for non-spatially varying transmission curves.",0.5
"DM-12369","10/24/2017 09:18:20","Add a coordinate-transform proxy transmission curve","Implement the interface of DM-12367 for a proxy that changes the coordinate system of a different transmission curve object, using the new Transform objects.    Should support at least focal-plane to CCD pixels and coadd pixels to CCD pixels.  Does not obviously need to support sky coordinates.",1
"DM-12370","10/24/2017 09:19:49","Add a coadded transmission curve implementation","Implement the interface of DM-12367 with a proxy that combines other concrete transmission curves as a function of position.",2
"DM-12372","10/24/2017 09:21:07","Add transmission curves for HSC filter traces","Implement the interface of DM-12367 using [~erykoff]'s approach to interpolating HSC filter traces.",2
"DM-12373","10/24/2017 09:25:48","Add spatially-varying transmission curves to Exposure/ExposureRecord","Attach an instance of the ABC defined in DM-12367 to Exposure and ExposureRecord.    While we could add these to Filter rather than add it to Exposure directly, my preliminary feeling is that it'll be better to let this represent the full throughput rather than associating it strictly with the filter.  It may *also* make sense to make Filter hold one of these objects to represent its contribution only, but that isn't what's needed for DM-12366.",3
"DM-12374","10/24/2017 09:26:49","Add transmission curve coaddition to coadd Task code","Add high-level code to combine spatially-varying transmission curves during coaddition.",2
"DM-12375","10/24/2017 09:31:22","Attach transmission curves during ISR or std_raw","Make sure we have spatially-varying transmission curves attached to Exposures by the time we write the calexp dataset.    Will need to decide how to store filter curves - best bet is probably as a butler dataset keyed only by filter for now.",2
"DM-12384","10/24/2017 18:53:48","exit/raise when data is less than parameters","Jointcal should exit or raise when the number of data points is less than the number of fitting parameters (e.g., constrained 4th order polynomial) with only 3 stars). I thought there was once such a check for astrometry, but I can't find it now.",2
"DM-12390","10/25/2017 11:34:44","Butler doesn't raise when failing to write data","If you supply insufficient data in the dataId for a butler to {{put()}} the data, it will just pretend to have done it (_i.e._ it doesn't raise or warn, it just silently returns as if all was fine).    If one does  {noformat}  butler.put(img, 'postISRCCD', dataId={'visit':123,'filter':'r'})   {noformat}  where the {{postISRCCD}} template looks like  {{template: ""postISRCCD/postISRCCD_v%(visit)d_f%(filter)s.fits""}}  then this will work. However, if you provide an empty dataId dict, it will not complain, but nothing will get written (of course).    This seems like a moderately serious problem, as it is easy to accidentally under-specify or mis-specify a dataId.    (FWIW, if I change 'postISRCCD' to an undefined dataset type I _do_ get an error).",2
"DM-12391","10/25/2017 13:22:12","Improve error message for case when warpCompare can't find any psf-Matched warps","This message means that no psfMatched warps were found, and a static sky model could not be built but a user might not deduce that from:    ```Traceback (most recent call last):    File ""/software/lsstsw/stack/Linux64/ctrl_pool/13.0-6-gf96f8ec+54/python/lsst/ctrl/pool/parallel.py"", line 496, in logOperation      yield    File ""/software/lsstsw/stack/Linux64/pipe_drivers/13.0-21-g61c0bd4+9/python/lsst/pipe/drivers/coaddDriver.py"", line 261, in coadd      coaddResults = self.assembleCoadd.run(patchRef, selectDataList)    File ""/software/lsstsw/stack/Linux64/pipe_base/13.0-14-g8b3bf66+40/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/home/yusra/lsst_devel/LSST/DMS/pipe_tasks/python/lsst/pipe/tasks/assembleCoadd.py"", line 388, in run      supplementaryData = self.makeSupplementaryData(dataRef, selectDataList)    File ""/home/yusra/lsst_devel/LSST/DMS/pipe_tasks/python/lsst/pipe/tasks/assembleCoadd.py"", line 1629, in makeSupplementaryData      templateCoadd = self.assembleStaticSkyModel.run(dataRef, selectDataList).coaddExposure  AttributeError: 'NoneType' object has no attribute 'coaddExposure'```",1
"DM-12417","10/26/2017 11:26:44","yaml dump prepends !!python/unicode to everything","Calling {{dumpToFile}} on a daf_persistence.policy causes !!python/unicode to get prepended to everything. As per https://stackoverflow.com/questions/1950306/pyyaml-dumping-without-tags it seems that changing {{yaml.dump()}} to {{yaml.safe_dump()}} should fix this.",0.5
"DM-12432","10/26/2017 13:21:33","Fix timing measurement construction","At present, {{ap_verify}} produces few timing measurements because I was not aware of DM-11935 when I wrote the code (and the pipeline was not yet in a state where it could be tested). The code should be changed so that measurements are reliably generated, either by fixing DM-11935 or by working around it.",2
"DM-12436","10/26/2017 14:52:53","remove jenkins py2 builds - post v15 release","Remove py2 support from:    - jenkins ({{stack-os-matrix}} + assorted publishing/release related jobs)  - {{newinstall.sh}}  - {{lsstsw}}",1
"DM-12445","10/26/2017 20:03:54","Set appropriate default configs for CompareWarp Coadds","Set default configs for makeCoaddTempExp, assembleCoadd, and coaddDriver in pipe_tasks and obs_subaru based on testing results.    These include the usual obs_subaru defaults for AssembleTask and SafeClipAssembleTask and more.  For example, these will probably include these configs:    {code}  config.makeCoaddTempExp.makePsfMatched=True  config.makeCoaddTempExp.warpAndPsfMatch.psfMatch.kernel['AL'].kernelSize=29  config.makeCoaddTempExp.warpAndPsfMatch.psfMatch.kernel['AL'].alardSigGauss=[1.0, 2.0, 4.5]  config.makeCoaddTempExp.modelPsf.defaultFwhm=6.1    from lsst.pipe.tasks.assembleCoadd import CompareWarpAssembleCoaddTask  config.assembleCoadd.retarget(CompareWarpAssembleCoaddTask)  config.assembleCoadd.badMaskPlanes = (""BAD"", ""EDGE"", ""SAT"", ""INTRP"", ""NO_DATA"",)  config.assembleCoadd.doSigmaClip = False  config.assembleCoadd.subregionSize = (10000, 200) # 200 rows (since patch width is typically < 10k pixels)  config.assembleCoadd.doMaskBrightObjects = True  config.assembleCoadd.sigmaClip = 1.5  config.assembleCoadd.clipIter = 3  config.assembleCoadd.assembleStaticSkyModel.sigmaClip = 1.5  config.assembleCoadd.assembleStaticSkyModel.clipIter = 3  config.assembleCoadd.assembleStaticSkyModel.statistic = 'MEANCLIP'  config.assembleCoadd.doNImage=True  config.assembleCoadd.doWrite=True  {code}",3
"DM-12447","10/27/2017 14:18:31","Make Detector.transform and Camera.transform support lists of points","Implement RFC-392: make Detector.transform and Camera.transform support lists of points (or single points) and delete the CameraPoint class. Removing the CameraPoint class also requires a minor change to Camera.findDetectors and findDetectorsList.",5
"DM-12450","10/28/2017 10:58:39","Implement RFC-407: improve interface for clobbering vs. reusing outputs","See RFC for description.    IMO, this has bitten too many DRP team members recently to just put it on the backlog; I think it's really costing us time and money.  Even if the RFC needs to stew for a while before merging, I'd like to get at least a partial implementation on a branch while it's fresh in my mind.  ",2
"DM-12452","10/30/2017 09:04:48","Add FrameDict class","Add a new class FrameDict to astshim that is a FrameSet with a table for looking up frame index by domain. This can greatly simplify much of the code in afw that manipulates FrameSets.",2
"DM-12455","10/30/2017 11:33:25","Please install v14.0 in the shared stacks on lsst-dev ","The stack release v14.0 just came out. It would be useful if v14.0 is available in the shared stack on lsst-dev. ",1
"DM-12463","10/31/2017 10:29:13","Create initial design for how DCR will fit in the stack","Now that the prototype DCR code is finished it needs to be refactored and brought in to the stack. Since there is a substantial amount of code involved I will write an initial design first. The result of this ticket will be a brief report describing the code that will be added to existing packages and any anticipated changes.",8
"DM-12464","10/31/2017 10:45:43","DCR correction design review","Review the design of the DCR algorithm in the stack, and gather feedback from stakeholders.",2
"DM-12465","10/31/2017 10:47:38","Revise DCR technote","Revise the DCR technote DMTN-037 based on feedback received on the initial full draft.",3
"DM-12467","10/31/2017 11:03:48","Prototype DCR code maintenance","While working on the design of how the DCR algorithm will fit in the stack, the existing prototype code is expected to change to reflect new functionality and limitations. ",2
"DM-12473","10/31/2017 14:50:15","Add getParallacticAngle() to visitInfo","[~sullivan] has done a bunch of math for calculating things related to refraction. Some of these should live in VisitInfo, since it knows the necessary values for the calculations. In particular, {{visitInfo::getParallacticAngle()}} would be very helpful. I suspect that there are some others in his DCR code that we'd like to have more broadly available.    Ian suggests that he could do this as a pair coding exercise with [~rowen] or [~krzys], since he doesn't know C++ that well.",2
"DM-12475","10/31/2017 15:37:16","Simplify PSF calculation in prototype DCR code","In order to simplify the transition of the prototype DCR code to the stack, several components should be re-written. This includes the PSF model, which currently depends on a lot of old and complicated code that deconvolves DCR from measured PSFs.",2
"DM-12476","10/31/2017 15:41:33","Re-write Bandpass class and usage in prototype DCR code","The current bandpass implementation leads to an inaccurate effective wavelength calculation for DCR. That implementation should change, and the way the bandpass is used throughout the prototype code should be updated",5
"DM-12480","11/01/2017 11:44:00","Update obs_comCam policy file","Following review comments on DM-12429, the policy for obs_comCam is not the minimal required to produce the same functionality, as a lot of things are redefined identically to how they are defined for the default in obs_base. This ticket is to make a best-effort at improving that situation, _and_ to fix a number of places in the path templates that are not currently correct (this is a newish obs_package and they were born wrong).",5
"DM-12491","11/02/2017 16:18:37","Reduce compareWarp I/O","We already assume that a list of spanSets representing the artifacts can fit in memory, so we can assume that the list of artifact CANDIDATES can also fit in memory. Lets loop over the warps once, building the count map and candidates at once  and then filter them afterwards.",2
"DM-12524","11/07/2017 08:33:03","AST should still write in FITS-WCS format after offsetting CRPIX","If I offset CRPIX by inserting a shift at the beginning of the GRID to IWC mapping, the resulting WCS cannot be written in FITS-WCS format. David Berry has a fix in AST.    This will require a new unit test in astshim and enables an improvement in ast: the shift can be added after GRID instead of between PIXEL0 and GRID (breaking the desired rule that PIXEL0 to GRID is always a shift of +1).",1
"DM-12527","11/07/2017 10:41:56","base build failure on Ubuntu due to gcc non-detection","sconsUtils is not correctly detecting the default gcc (5.4) on Ubuntu 16.04. This results in {{-fno-lto}} not being included in the compiler options, resulting in a build failure (first failing package is {{base}}).    Here's what the gcc non-detection looks like in the build log:    {code}  [2017-11-07T09:20:29.476932Z] Checking who built the CC compiler...unknown  [2017-11-07T09:20:29.476948Z] CC is unknown version unknown  {code}",2
"DM-12529","11/07/2017 11:08:52","Enum comparison should use == not is","The *showVisitSkyMap.py* script in the {{examples}} sub-directory of the {{skymap}} repo uses the comparison:  {code}  ccd.getType() is cameraGeom.SCIENCE  {code}  This no longer evaluates to *True* when appropriate.  The condition here should be `==`, not `is` (it is likely the behavior changed along with the pybind11 wrapping and has simply gone unnoticed since).",0.5
"DM-12531","11/07/2017 15:18:21","Implement RFC-409: only check configurations/schemas/versions in output repos","Implements RFC-409.    If this is as easy as I'm guessing, I should be able to get this done before the RFC is adopted, and then merge it at the same time as DM-12450 to allow me to group the announcements about these changes in behavior.",1
"DM-12547","11/08/2017 13:13:04","SUIT and Firefly integration and test","This epic will capture the effort for integration and test in S18 of SUIT, including PDAC portal update/test,  new features in portal an dFirefly test,  packaging and deployment issues. ",40
"DM-12555","11/08/2017 15:23:37","HiPS: query a HiPS server to see what is available.","This issue needs research. It is currently possible from CDS if there is a standard. Research and implement the ability to create a list of HiPS repositories.    The service is documented in section 5.2 of the attach HiPS pdf.    The development includes,   * send query to HiPS server to get public HiPS  per information from [http://aladin.u-strasbg.fr/hips/list|https://github.com/Caltech-IPAC/firefly/pull/url]  the available HiPS images can be retrieved by sending the query like  [http://alasky.unistra.fr/MocServer/query?hips_service_url=*&dataproduct_type=!catalog&dataproduct_type=!cube&get=record|https://github.com/Caltech-IPAC/firefly/pull/url]   * show the list of the found public HiPS in a table",8
"DM-12556","11/08/2017 15:28:41","reportPerformance.py can't handle new metrics","Now that TE1 is calculated on the master branch of {{validate_drp}} but not included in the release metrics.  This causes a {{KeyError}} when I run {{reportPerformance.py}} on JSON files produced with a master version.",1
"DM-12561","11/09/2017 08:02:05","jupyterlabdemo build failing","{code:java}  > npm pack @jupyter-widgets/jupyterlab-manager  jupyter-widgets-jupyterlab-manager-0.29.3.tgz  Traceback (most recent call last):    File ""/usr/bin/jupyter-labextension"", line 11, in <module>      load_entry_point('jupyterlab==0.29.0rc0', 'console_scripts', 'jupyter-labextension')()    File ""/usr/lib/python3.4/site-packages/jupyter_core/application.py"", line 266, in launch_instance      return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)    File ""/usr/lib/python3.4/site-packages/traitlets/config/application.py"", line 658, in launch_instance      app.start()    File ""/usr/lib/python3.4/site-packages/jupyterlab/labextensions.py"", line 174, in start      super(LabExtensionApp, self).start()    File ""/usr/lib/python3.4/site-packages/jupyter_core/application.py"", line 255, in start      self.subapp.start()    File ""/usr/lib/python3.4/site-packages/jupyterlab/labextensions.py"", line 64, in start      for arg in self.extra_args]    File ""/usr/lib/python3.4/site-packages/jupyterlab/labextensions.py"", line 64, in <listcomp>      for arg in self.extra_args]    File ""/usr/lib/python3.4/site-packages/jupyterlab/commands.py"", line 137, in install_extension      return IOLoop.instance().run_sync(func)    File ""/usr/lib64/python3.4/site-packages/tornado/ioloop.py"", line 458, in run_sync      return future_cell[0].result()    File ""/usr/lib64/python3.4/site-packages/tornado/concurrent.py"", line 238, in result      raise_exc_info(self._exc_info)    File ""<string>"", line 4, in raise_exc_info    File ""/usr/lib64/python3.4/site-packages/tornado/gen.py"", line 1069, in run      yielded = self.gen.send(value)    File ""/usr/lib/python3.4/site-packages/jupyterlab/commands.py"", line 208, in install_extension_async      raise ValueError(msg)  ValueError:   ""@jupyter-widgets/jupyterlab-manager@0.29.3"" is not compatible with the current JupyterLab  Conflicting Dependencies:  JupyterLab              Extension            Package  >=0.51.0-0 <0.52.0-0    >=0.50.2-0 <0.51.0-0 @jupyterlab/services  >=0.12.0-0 <0.13.0-0    >=0.11.1-0 <0.12.0-0 @jupyterlab/application  >=0.12.0-0 <0.13.0-0    >=0.11.1-0 <0.12.0-0 @jupyterlab/notebook  >=0.12.0-0 <0.13.0-0    >=0.11.1-0 <0.12.0-0 @jupyterlab/rendermime  >=0.12.0-0 <0.13.0-0    >=0.11.1-0 <0.12.0-0 @jupyterlab/coreutils    The command '/bin/sh -c jupyter labextension install @jupyter-widgets/jupyterlab-manager' returned a non-zero code: 1  Docker build failed.    {code}  ",1
"DM-12571","11/10/2017 00:26:23","Update dev guide to allow super()","This ticket is to implement the final change from RFC-397. I've tweaked the wording slightly here to clarify the single/multiple inheritance point.    This is the replacement version of the section of the python style guide about {{super()}}:    {code}  `super` MAY be used with care to call inherited methods    Python provides `super()` so that each base class’s method is only called once. Using `super()` ensures a consistent Method Resolution Order, and prevents inherited methods from being called multiple times.    In Python 3, `super()` does not require naming the class that it is part of, making its use simpler and removing a maintenance issue.    The trickiest issue with the use of `super()` is that, in the presence of multiple inheritance, there is no way for a class to know for certain which inherited method will be called.  In particular, this means that the calling signature (arguments) for all versions of a method must be compatible.    As a result, there are a few argument-related caveats about the use of `super()` in multiple inheritance hierarchies:    * Only pass `super()` the exact arguments you received.  * When you use it on methods whose acceptable arguments can be altered on a subclass via addition of more optional arguments, always accept `*args`, `**kw`, and call `super()` like `super().currentmethod(alltheargsideclared, *args, **kwargs)`. If you don’t do this, forbid addition of optional arguments in subclasses.  * Never use positional arguments in `__init__` or `__new__`.  Always use keyword args, and always call them as keywords, and always pass all keywords on to `super()`.    To use `super()` with multiple inheritance, all base classes in Python's Method Resolution Order need to use `super()`; otherwise the calling chain gets interrupted.  If your class may be used in multiple inheritance, ensure that all relevant classes use `super()`, including documenting requirements for subclasses.    Following these guidelines for single inheritance hierarchies, will let them be extended to multiple inheritance with minimal difficulty.  {code}",1
"DM-18249","11/10/2017 08:59:40","SAL features - add authList support to XML","Detailed design",2
"DM-12587","11/10/2017 09:22:23","Use minikube for travis-ci","* Improve Qserv container version management: use tag for Qserv container (produced with git describe (--parent?))   * Build run-kubectl container at runtime with a given version or install kubectl   * What about qserv/qserv:dev container?   * add qserv_deploy to qserv_distrib?   * Try to speed up the build... (reduce container image size)",8
"DM-12588","11/10/2017 10:49:40","Port validate_drp documenattion","The code in {{validate_drp}} was ported to use {{verify}} in DM-12253, but that did not include porting the documentation.    This ticket is to update documentation in {{validate_drp}} for API changes as well as general docstring cleanup.",2
"DM-12593","11/13/2017 08:40:42","Use HSC S17A Junk List to validate CompareWarp","Use Junk list from Matsuoka-san to validate CompareWarpAssemble",2
"DM-12596","11/13/2017 10:55:54","AstrometryTask.distort broken","The changes in DM-5922 were not propagated to {{AstrometryTask.distort}} in {{meas_extensions_astrometryNet}} so that method is broken. The fix is trivial.  ",1
"DM-12610","11/13/2017 16:00:15","Document lsst-dev filesystems","{{lsst-dev01}} has access to all the GPFS filesystems available to the Verification Cluster (ie, as documented at https://developer.lsst.io/services/verification.html#verification-gpfs). However, if you didn't already know that, you wouldn't discover it by reading https://developer.lsst.io/services/lsst-dev.html. Please cross-link this documentation appropriately.",1
"DM-12611","11/13/2017 21:13:38","FrameDict(FrameSet const &) broken","FrameDict(FrameSet const &) results in an unusable FrameDict in C++ because AST has already reclaimed the raw pointer.    pybind11 avoids the issue, so it is difficult to test in Python. Add a simple C++ function that Python can call to induce the problem, call it from {{test_frameDict.py}} to demonstrate the problem, then fix the problem.",1
"DM-12615","11/14/2017 11:00:03","Add copy-constructors to astshim objects","Most astshim objects do not provide a copy-constructor (motivated by the discussion in DM-9174?). This means that to copy an object, one needs complex statements like {{Object(std::move(*object.copy()))}}. Since astshim objects can be copied anyway through {{copy}} or a temporary object, adding a copy-constructor would allow for more idiomatic C++. The copy-constructor could be implemented either by indirect constructions like the one above, or by something more efficient.",1
"DM-12616","11/14/2017 12:25:29","BUG: partial decimated data returned on repeated request","Using slate-demo-explicit2.ipynb from firefly_client examples.    If the request is reissued (last cell reloaded) before the first one completes, partial results (100 pts instead of ~1000) are returned for heatmap. (See attached.)    Controlling pageSize in DecimationProcessor.java fetchData does not work when the cached version of the data is returned. We need to pass pageSize from the client.    ",2
"DM-12652","11/14/2017 16:17:59","code change to deal with 32bit mask layer in LSST image","Update the code to auto-generate the mask layers for all 32 bits when the mask extension is detected in the image. ",2
"DM-12658","11/15/2017 10:20:51","base_PixelFlags_flag_clipped not getting set on measurements on CompareWarp Coadds","Catalogs measured from CompareWarpCoadds don't have {{base_PixelFlags_flag_clipped}} set if clipped.   The only reason SafeClipped coadds do is because the coadd masks are manually set before writing out as a workaround for: DM-9953.    {code}          # Set the coadd CLIPPED mask from the footprints since currently pixels that are masked          # do not get propagated          maskExp = retStruct.coaddExposure.getMaskedImage().getMask()          maskExp |= maskClip  {code}     I""m going to add this to CompareWarp, but we should remember to remove it from both when DM-9953 is finished. ",2
"DM-12660","11/15/2017 14:16:33","Install test and study a local k8s installer","this local installer could be used for running integration tests on travis-ci.",5
"DM-12667","11/16/2017 09:40:12","Implement minimal Butler POSIX Datastore prototype","Implement a minimal Butler Datastore (as defined in DMTN-056) backed by a POSIX filesystem.    The intent of this ticket is to build a minimum prototype for testing purposes to be used with either the Registry from DM-12613 (or DM-12371).  This prototype is to be as simple as possible but should be useable to inform decisions about pluggability, chaining, caching and API layering for later versions.",8
"DM-12675","11/16/2017 15:21:23","Developer guide inaccurately summarises issue semantics","https://developer.lsst.io/processes/workflow.html#tickets makes various claims about the association of different types of issue (story/bug/improvement) with epics. In particular, it claims that ""bug"" and ""improvement"" type issues are not associated with epics.    This is simply incorrect: bugs and improvements should be associated with epics in just the same way as stories, and are treated identically for the purposes of earned value accounting.    (I think this bad documentation is based on a misunderstanding of the outcome of RFC-43.)",1
"DM-12690","11/16/2017 17:25:02","Make ConstrainedPolyModel actually support initFromWCS","While sorting out the best approach for getting jointcal to ingest a SkyWcs, we discovered that the {{ConstrainedPolyModel}} was ignoring the {{initFromWCS}} constructor boolean flag. Looking at the history, it appears it was never used: the chip transfo was always being initialized with the input WCS.    This is an easy fix, and I'll also make it a configurable, so we can test how larger datasets behave when it is {{False}}; the testdata all appears to be fine (initial chi2 is different, but it quickly settles down to the same value).",0.5
"DM-12692","11/17/2017 08:30:22","Improve  temporal threshold for CompareWarp","An integer maxNumEpochs is too crude. The original fraction temporalThreshold of the number warps entering an image is too crude.     Evaluate and test a fraction with a floor based on the local N_epochs. ",2
"DM-12694","11/17/2017 09:52:11","Add queue option for allocateNodes.py","Following IHS-612, there will be multiple slurm queues on LSSTVC.   It would be great to be able to tell {{allocateNodes.py}} which queue to send jobs to. ",1
"DM-12697","11/17/2017 13:12:15","Fix hollowed out cores of saturated stars in CompareWarp","Thus far I've been clipping any any region for which we don't have information from the PSF-matched warp. This includes more area than direct warp because of  matching kernel smears out the bad pixels and edges of calexps.  Test allowing saturated pixels through. Merge if it works. ",3
"DM-12700","11/17/2017 14:28:01","Flip CompareWarpAssembleCoaddTask on by default for RC and ci_hsc","Switch defaults everywhere (not just obs_subaru coaddDriver).   Includes running ci_hsc with CompareWarpAssembleCoadd. ",2
"DM-12701","11/17/2017 15:51:37","Update showVisitSkyMap.py for unique visit and tract identification","Update the color mapping of the visit outlines such that each visit gets a unique color.  Also add a legend with the visit numbers such that they can be identified in the plot.  Additionally, allow for multiple tracts to be specified and have their outlines drawn on the plot with unique gray-scaling and added to the legend.  ",1
"DM-12712","11/17/2017 17:15:21","Update LDM-148 for Commissioning Review","Adjust text and diagrams to match service designs.  Submit RFC to baseline new version.",20
"DM-12718","11/17/2017 17:18:51","Investigate alternatives for packaging/build infrastructure","Investigate and propose an alternative to current product/package structure and packaging and build tools.  Submit an RFC or determine that one is not timely.",40
"DM-12720","11/17/2017 17:21:20","Prepare and execute 15.0 release","Ensure that all of the following steps are executed, performing as many as possible directly and training on each:  * Identify any pre-release blockers (“must-have features”)  * Track blockers to closure  * Eups publish rc1 candidate  * Git Tag v15.0-rc1  * Github release lsst_demo 15.0  * Branch 15.0 of newinstall.sh (lsst)  * Track any latent bugs to closure  * Repeat with additional -rcN candidates  * Confirm DM Externals are at stable tags  * Tag DM Auxiliary (non-lsst_distrib) repos  * Perform full OS testing (see https://ls.st/faq2 )  * Git Tag 15.0, rebuild, eups publish  * Produce factory binaries  * Test factory binaries  * Gather contributed binaries  * Update Prereqs/Install  * Update Known Issues  * Gather Release notes  * Gather Metrics report  * Email announcement",40
"DM-12721","11/17/2017 17:34:41","SUIT and Firefly deployment in docker","deployment of SUIT, PDAC using docker technology",0
"DM-12764","11/20/2017 11:07:54","Overhaul SkyWcs","As part of DM-10765 it became apparently that SkyWcs needed many changes, including:  - Do not inherit from Transform  - Add FITS binary table persistence  - Add missing functionality as methods or free functions    In addition, it is possible to greatly simplify the code using the new {{ast::FrameDict}} class.    [~jbosch] suggested, and I agree, that it is better to do the SkyWcs overhaul on a ticket separate than DM-10765, making DM-10765 itself smaller and more focused.",20
"DM-12765","11/20/2017 11:11:10","Record filter ratios in HSC coadds","This is the most minimal version of DM-12366, requested by NAOJ.  It makes sense to do it on a separate ticket.    This will mostly be highly HSC-specific code, as it's a lot harder to come up with a way to report fractions of different filters used when you can have more than two filters and arbitrary names for them; for HSC we know exactly which pairs of filters we need to take ratios of.",1
"DM-12766","11/20/2017 11:14:16","coaddDriver  with --cores > 1 produces MPI_Abort(MPI_COMM_WORLD, 1) ","To reproduce, the first one with --cores=1 works, but second with --cores=2 does not:      {code}  source /software/lsstsw/stack/loadLSST.bash  setup lsst_distrib  export wideVisitsG=9852^9856^9860^9864^9868^9870^9888^9890^9898^9900^9904^9906^9912^11568^11572^11576^11582^11588^11590^11596^11598  coaddDriver.py  /datasets/hsc/repo --rerun RC/w_2017_46/DM-12545:private/yusra/psfMatching/newStack --time 500 --cores=1  --id tract=8766  patch=5,5 filter=HSC-G --selectId ccd=0..8^10..103 visit=$wideVisitsG   coaddDriver.py  /datasets/hsc/repo --rerun RC/w_2017_46/DM-12545:private/yusra/psfMatching/newStack --time 500 --cores=2  --id tract=8766  patch=5,5 filter=HSC-G --selectId ccd=0..8^10..103 visit=$wideVisitsG  {code}    Note to get any useful output you'd need to either switch:  makeCoaddTempExp.doApplyUbercal=False  AssembleCoadd.doApplyUbercal=False    OR     setup meas_mosaic (which I currently can't build with the python3 stack)",1
"DM-12771","11/20/2017 13:23:18","Support the new FitsChan SipReplace attribute","Add support for the new {{FitsChan}} attribute {{SipReplace}}. This makes AST believe the inverse SIP coefficients and will be usable in DM-12764 to improve the accuracy of SkyWcs.skyToPixels for WCS with SIP terms.",1
"DM-12780","11/21/2017 11:00:16","prune eups.lsst.code s3 backups","The daily backups for {{s3://eups.lsst.codes}} were growing unbounded.  Backups have been prune down to retain only the 1st day of each month prior to the current month. Eg.    {code:java}  #!/bin/bash    #set -e  set -o xtrace    for m in {4..10}; do    mon=$(printf ""%02d"" $m)    for d in {2..30}; do      day=$(printf ""%02d"" $d)      aws s3 rm --recursive ""s3://eups.lsst.codes-backups/2017/${mon}/${day}"" &    done  done  {code}    Either a rotation script is needed or s3 object expiration needs to be set.  Ie., backups on every day but the first of the month have a 30 day expiration set on the objects.",2
"DM-12783","11/21/2017 12:11:22","Normalize weights differently for morphology and SED updates","It is believed that the problems with bad colors in HSC-Z and Y that we observed in DM-12776 are due to the way we are using inverse variance maps to weight gradient steps in the deblender. When two bands have substantially different mean variance, certain colors converge more quickly than other colors, and at all times are capable of taking larger steps toward the minimum, which we believe results in poor colors in the bands with higher variance. Similarly, images with widely varying pixel variances could also see some pixels reaching convergence more quickly than others (although we have not observed this directly).    To combat this [~pmelchior] and I propose to normalize the weights for SED updates along each band, so that only pixel by pixel variations are weighted for each SED update, and normalize the weights for morphologies along each single band image. This should give us more uniform convergence and hopefully solve DM-12776 as well.",1
"DM-12794","11/21/2017 16:05:23","Fix Qserv dev container (and Travis CI) builds","The recent upgrade of the python3 baseline after stack release 14.0 seems to have busted the Qserv container builds (and thus Travis CI for Qserv)",1
"DM-12802","11/22/2017 14:07:00","Add shape measurements to coadd comparison.","Add ext_shapeHSM_HsmShapeRegauss_e1, ext_shapeHSM_HsmShapeRegauss_e2, ext_shapeHSM_HsmShapeRegauss_resolution to the coadd comparison script to aid with assessing changes relevant to shear measurements.",1
"DM-12815","11/27/2017 14:56:35","convert jenkins jenkins-ebs-snapshot job to pipeline","This trivial job needs to be converted to pipeline in order to implement timeouts.",0.5
"DM-12818","11/27/2017 16:44:10","CSS needs new parameter for match table separation.","Currently qana uses database-global ""ovelap"" value as a match table ""separation"" parameter which is wrong. Database-global ""ovelap"" is supposed to be a default overlap value for tables that don't define their own overlap. Match table separation is not really related to overlap, so to reduce confusion we need to add new parameter to match table configuration in CSS and use is to set qana parameters.  ",3
"DM-12819","11/27/2017 19:22:09","Explore possible reduction of memory footprint needed for image cutouts","Currently, ImageServ is passing data id to butler to retrieve the FULL image from the underlying fits file, before performing the cutout per user-specified dimensions, in a 2-step process.    It may be possible to combine the two steps into a single one, in the following manner:               image = butler.get(dataset_type, bbox=bbox, immediate=True,                              tract=tract, patch=patch, filter=filterName)    Note: bbox dimensions always in pixels.",5
"DM-12848","11/28/2017 12:16:15","ap_verify unit tests fail on lsst-dev","One of the unit tests for {{ap_verify}} tests whether its running-time metrics return a sensible time for a dummy Task execution. Because the task is a dummy, it almost immediately raises an exception, allowing the timing code to run.    However, on {{lsst-dev}} (and, so far, only {{lsst-dev}}), the running time returned is equal to {{0.0 * astropy.units.second}}, failing a test for a positive value. This result should be investigated and, if necessary, the test modified to return a nontrivial result. The assertion should not be removed if at all possible, because a measurement of zero could result from incorrect propagation of {{None}} or other invalid values.",2
"DM-12854","11/28/2017 16:28:20","ongoing jenkins dockerd/xfs errors","Jenkins jobs that use docker are frequently hanging and {{dockerd}} / {{xfs}} errors are appearing in the {{dmesg}}:    {code:java}  [279961.166094] INFO: task dockerd:902 blocked for more than 120 seconds.  [279961.169742] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.  [279961.173508] dockerd         D ffff8800a188e010     0   902      1 0x00000080  [279961.176932]  ffff880033883d78 0000000000000086 ffff8803bb2e8fd0 ffff880033883fd8  [279961.180601]  ffff880033883fd8 ffff880033883fd8 ffff8803bb2e8fd0 ffff8800a188e000  [279961.184682]  ffff8800a188e040 ffff88036f27c140 ffff8800a188e068 ffff8800a188e010  [279961.188403] Call Trace:  [279961.190270]  [<ffffffff816a94e9>] schedule+0x29/0x70  [279961.193155]  [<ffffffffc01732ea>] xfs_ail_push_all_sync+0xba/0x110 [xfs]  [279961.196618]  [<ffffffff810b1910>] ? wake_up_atomic_t+0x30/0x30  [279961.199554]  [<ffffffffc015c2e1>] xfs_unmountfs+0x71/0x1c0 [xfs]  [279961.202526]  [<ffffffffc015cded>] ? xfs_mru_cache_destroy+0x6d/0xa0 [xfs]  [279961.205747]  [<ffffffffc015ee92>] xfs_fs_put_super+0x32/0x90 [xfs]  [279961.209261]  [<ffffffff81203722>] generic_shutdown_super+0x72/0x100  [279961.212640]  [<ffffffff81203b67>] kill_block_super+0x27/0x70  [279961.215738]  [<ffffffff81203ea9>] deactivate_locked_super+0x49/0x60  [279961.218801]  [<ffffffff81204616>] deactivate_super+0x46/0x60  [279961.221628]  [<ffffffff8122184f>] cleanup_mnt+0x3f/0x80  [279961.224301]  [<ffffffff812218e2>] __cleanup_mnt+0x12/0x20  [279961.227135]  [<ffffffff810ad247>] task_work_run+0xa7/0xf0  [279961.230449]  [<ffffffff8102ab62>] do_notify_resume+0x92/0xb0  [279961.233768]  [<ffffffff816b52bd>] int_signal+0x12/0x17  {code}    These problems seem to go away, at least for awhile, after the node is restarted (they will not soft reboot).  It isn't yet known if this is a kernel bug or some of random EBS I/O timeouts.  As it seems to be only (?) triggered by dockerd, it seems much more likely to be a kernel issue.  What is unusual is that this doesn't seem to have been an issue until ~ a week ago and this doesn't correlate with a kernel or dockerd version change.",5
"DM-12861","11/29/2017 12:49:08","jenkins s3backup-eups job is failing","The last several {{s3backup-eups}} builds have failed. Eg.,    https://ci.lsst.codes/job/sqre/job/backup/job/s3backup-eups/215/consoleFull    {code:java}  copy failed: s3://****/stack/redhat/el6/devtoolset-3/miniconda3-4.3.21-10a4fa6/utils-14.0-6-g4f52afe@Linux64.tar.gz to s3://****/2017/11/29/2017-11-29T12:12:10Z/stack/redhat/el6/devtoolset-3/miniconda3-4.3.21-10a4fa6/utils-14.0-6-g4f52afe@Linux64.tar.gz An error occurred (InvalidArgument) when calling the UploadPartCopy operation: Range specified is not valid for source object of size: 27092590    {code}  ",0.5
"DM-12873","11/30/2017 12:31:28","Run pipe_analysis scripts on w_46-processed RC data at NCSA","In support of the LDM-534 QA milestone, run {{coaddAnalysis.py}}, {{colorAnalysis.py}} and {{visitAnalysis.py}} on all coadds and visits from the weekly 46 RC processing run (/datasets/hsc/repo/rerun/RC/w_2017_46/DM-12545/).  ",2
"DM-12881","12/01/2017 00:30:12","coaddDriver tries to unset doMatchBackgrounds, which does not exist","See https://hsc-jira.astro.princeton.edu/jira/browse/HSC-1426  ",0.5
"DM-12882","12/01/2017 07:59:35","Frequent catastrophic misalignment in coadd inputs","Catastrophically bad WCSs that print through to the coadd seem to have much more common between _42 (where they were basically nonexistent) and _46.    This manifests both as duplicated sources and bad clipping.",2
"DM-12890","12/01/2017 10:15:30","Remove imageREST_v0 (deprecated API) wrapper from code base.","As is imageREST_v0 API (URL pattern/style) is already supported on top of v1 code base.    Once we transition over to imageREST_v1, which can be seen as v0++ from functionality standpoint,  there is really no need for the old v0 code (isolated to a few files) to remain in the code base.    ",1
"DM-12894","12/01/2017 10:49:54","Comments on ""beta"" HiPS viewing in Firefly","Congratulations on getting HiPS viewing going in Firefly!    In general it looks really good.  I have mostly minor comments of a wide variety, which I will post as a document attached to this ticket.",2
"DM-12909","12/01/2017 18:10:06","Test failure in meas_mosaic","Carelessness in the merge of DM-12882 led to a failing unit test.  Fixing it now.",0.5
"DM-12924","12/05/2017 16:34:04","SpherePoint.offset should work at the poles and for negative offsets","In replacing Coord with SpherePoint (DM-11162) I discovered a problem in the skymap package: skymap makes heavy use of Coord.offset, including for points at a pole. However, SpherePoint rejects such offsets (as well as negative offsets).    I believe the behavior of {{offset}} for a coord at the pole is unambiguous and unsurprising if the coord is constructed from longitude, latitude. However, I acknowledge that this is less so for a coord constructed from a 3-vector. Nonetheless, I propose to change the behavior of SpherePoint to match Coord and document the behavior at the pole.    I also propose to allow negative offsets, as the intent is unambiguous for those and Coord supports it.    Also make sure the sign convention for the orientation of offset is the same for {{SpherePoint}} as {{Coord}} and document it carefully.",1
"DM-12937","12/06/2017 15:33:22","Image retrieval via v0 API for 'i' band deep coadd returning index out of range","The error response should have been along the line of 'image not found'.    Just for record, deepcoadd cutouts for bands other than r result in the `IndexError: list index out of range`.  Example: `http://lsst-qserv-dax01.ncsa.illinois.edu:5003/image/v0/deepCoadd?ra=9.6&dec=-1.1&filter=i`",5
"DM-12938","12/06/2017 15:53:44","ap_pipe crashes if --output is absolute path","If {{ap_pipe}} is called with an absolute path as its {{--output}} directory, it will crash soon after attempting to ingest defects with a message like:  {noformat}  ap.pipe.defectIngest INFO: Ingesting defects...  usage: ap_pipe.py input [options]  ap_pipe.py: error: Error: input='[partialdir]/[outputdir]/ingested' not found  {noformat}  A similar error appears if {{--output}} is relative and the working directory is anything other than the parent of {{\[outputdir]}}.    I suspect the problem is because {{defectIngest}} assumes {{repo}} must be a path relative to the output directory's parent, but have not tested to make sure.",2
"DM-12940","12/06/2017 16:58:21","Catch the error when adding marker or footprint to a plot without image (due to image search failure)","For the plot with no image due to an image search failure, some running error occurs while adding a marker or a footprint to that plot. The marker/footprint adding should skip the undefined plot or the plot with no image.   ",1
"DM-12943","12/06/2017 17:59:37","Investigate coadd covariance impact on detection kernels","Determine the correlation matrix empirically from blank patches of sky on a coadd, then see if we can construct a detection kernel that corrects for it.",2
"DM-12949","12/07/2017 12:12:13","Filtering table from scatter chart fails when column expressions are involved","The error is:    ""...from data where (""w1mpro-w2mpro"" > -1.3191681767193026) and (""w1mpro-w2mpro"" < -0.7958581413409266) and (""w3mpro"" > 8.643434512115258) and (""w3mpro"" < 13.130081532416503) ) as b) WITH DATA]; nested exception is java.sql.SQLSyntaxErrorException: user lacks privilege or object not found: w1mpro-w2mpro from:null""    Here ""w1mpro-w2mpro"" should probably be ""w1mpro""-""w2mpro""    12/14/2017 TG  https://github.com/Caltech-IPAC/firefly/pull/516    - now using db to calculate column expression values for general charts (we used expression lib before)  - fixed filter from chart when column expressions are used  - allow same columns to be used for x and y  - fixed filtering issues for expressions with alpha-numeric columns. Unfortunately, if a column name contains space, filter will not work the second time, because filter parser is using space as a delimiter. Created DM-13037 for this issue.  - updated histogram request to get the expression calculation in the database.  - fixed selection bug in heatmap I have introduced when checking that user is selecting active trace.  - created tickets for outstanding column expression issues:    DM-13036 Use db to calculate expressions for decimation (heatmap data)    DM-13039 Use custom database functions for column expressions functions not supported by DB    DM-13038 Support column expressions for column names that are not alpha-numeric    ",8
"DM-12968","12/11/2017 14:08:33","Include INTERP+CR pixels in coadds","I think we agreed at the 2018-12-11 Monday Meeting that it's better to included interpolated CR pixels in the coadd (since the interpolation is pretty good) rather than leave them out and get lots more pixels masked with INEXACT_PSF.    [~price], if you're in a position to do this quickly before the next HSC release, please steal it.  Otherwise I'll try to get to it later.",1
"DM-12969","12/11/2017 14:10:30","Configure LDM/DMTR docs on Travis for LSST the Docs","This ticket covers effort associated with configuring LDM and DMTR (LaTeX documents) for deployment on LSST the Docs.",2
"DM-12972","12/11/2017 14:48:06","Write paper on deblender","[~pmelchior] has been writing up a paper based on the new deblender. For my part I will help write some sections of the paper, including results from tests on both HSC and Sim data.",8
"DM-12980","12/12/2017 10:01:52","Add display_firefly to lsst_distrib","This ticket implements RFC-421.    {{display_firefly}} will be added as {{setupRequired}} to the table file in {{lsst_distrib}}.",1
"DM-12992","12/12/2017 17:08:30","Confirm access to the newly loaded NEOWISE data table","confirm  that the newly loaded NEOWISE data table is accessible through current DAX service",1
"DM-13037","12/14/2017 11:14:38","Second filter fails on column names with spaces (NED)","If a column name contains space, filter will not work the second time, because filter parser is using space as a delimiter. (see parse in FilterInfo.js)    Test case:  NED catalog search.    Using table filters, filter on ""Redshift Points"" in table, then filter on ""Distance"".    Alternatively, create a chart with the columns that contain spaces, filter from the chart. The second filter is producing a db error.",3
"DM-13041","12/14/2017 14:25:10","Attend Galaxy and Cosmology Class in Dec","Attend Michael Strauss and Jenny Greene's observational galactic astronomy class to better understand the scientific applications of the deblender.    There are only 2 weeks of class this month.",2
"DM-13045","12/15/2017 09:38:14","Test new deblender on simulated data","Before updating the stack API (DM-12404) and testing the deblender on HSC data (DM-11330), we should make sure that the new version works on the same simulated data as well or better than the previous version.",5
"DM-13052","12/15/2017 16:53:15","AssociationTask does not call update on associated DIAObjects","After running ap_pipe and ap_verify the L1DB produced by AssociationTask does not update the values of the stored DIAObjects. This causes the appearance that each DIAObject is associated with only one DIASource. This ticket will fix this case.",2
"DM-13054","12/15/2017 18:53:24","Add colorterm config support to jointcal","[~boutigny]'s work on color terms in u/fix_outliers should help improve the photometry models.    This ticket is to port over [~boutigny]'s work in the u/fix_outliers branch (specifically this commit: https://github.com/lsst/jointcal/commit/e1d0292c71275315a6b17b67ecaf47acd2ffbee8 ) to use {{ReferenceSourceSelectorTask}}, add a unittest to demonstrate that the color terms do affect the fit (for both the simple, and constrained models), and update the other tests to reflect any changes in the fit metrics.",8
"DM-13055","12/17/2017 18:11:22","reject NaN centroid sigmas in astrometrySourceSelector","Jointcal is still getting NaNs in some of the HSC PDR1 data. It appears that finite errors are not guaranteed by the centroid_flag, so we have to reject non-finite Sigmas during source selection. For now, we'll add it to astrometrySourceSelector, but we'll have to think of a better solution since this will affect all source selectors.",1
"DM-13073","12/18/2017 16:22:38","Add CCOB Milestone test in LDM-503","Update LDM-503 to include approriate tests for handling data from Camera Calibration Optical Bench (CCOB). This may only be possible after the Feb meeting which sets the CCOB schedule. ",1
"DM-13080","12/19/2017 12:45:29","Add reference band flags used for forced coadd measurements to persisted parquet tables","It is very useful for qa/debugging to know which band was used as the reference band for the measurements in forced photometry on the coadds.  These are currently only persisted in the *deepCoadd_ref* output file, so will need to read these in and add the appropriate columns to the parquet tables.",2
"DM-13082","12/19/2017 13:44:19","test failure due to DM-12968 config move","It was not in fact safe to move the badMaskPlanes config option from CoaddBaseTask to AssembleCoaddTask.  Master is broken.",0.5
"DM-13084","12/19/2017 14:16:39","Be smarter about combining metadata from FITS headers","Our afw code that reads FITS headers for images and exposures works as follows:  - It reads the metadata (as a PropertySet or PropertyList) from the first HDU  - It reads the metadata from the next HDU (the image plane)  - It combines them using PropertySet.combine    If the same keyword is present in both sets of metadata, this can cause problems. For example:  - In some files LTV[12] is specified as 0 in the first HDU and -XY0 as a float in the image HDU. This causes a failure with a complaint about mismatched types. Even if the type is int in the second HDU we end up with an array of values which is not what we want.  - In some files NAXIS[12] is specified as 0 in the first HDU and typical values in the next HDU. This results in an array value for NAXIS[12], which is not what we want.    My suggestion is to write a free function that combines metadata in a FITS-friendly manner:  - For a select few FITS-specific keywords, including COMMENT and HISTORY, use the current behavior: every time the keyword is found, add its value to an array. Require that the types match.  - For all other keywords, when a name is re-used, keep only the most recent version. Do not require that the types match.",2
"DM-13096","12/20/2017 15:35:39","Add refraction calculation to the stack","Add a calculation of atmospheric refraction and utilities for differential refraction to the stack. These calculations are needed for the DCR-corrected templates that are being written in DM-9615, but they can be made more general and separated from the rest of the DCR code.",2
"DM-13099","12/20/2017 18:24:32","Can't set only one axis binning value on decimated plot","In the decimated plots, I can't change the number of bins in the x-direction only.  I enter a number, hit apply, and nothing happens.  Seems that i have to set both axis. Can we use a default value for the one that is not filled in?",3
"DM-13116","12/21/2017 23:13:57","Implement weighted strict monotonicity","The strict monotonicity projection using only the nearest neighbor creates fractal-like unphysical features and affects the accuracy of the deblender. This ticket is to implement a strict, weighted monotonicity projection similar to the weighted monotonicity operator that calculates the radial gradient.    For speed, this will have to be written in C++.",2
"DM-13123","12/22/2017 12:56:14","Doxygen search broken","When I visit http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/index.html, type something into the search box, and hit enter, I get a {{search.php}} file to download in response.    Obviously, I should get search results.    This was discussed on DM-10544, but doesn't seem to have actually been fixed there (or, if it was, it has subsequently broken again).",1
"DM-13126","12/22/2017 13:31:08","possible bug in pixel coord readout","From Robert's input #7 in DM-8409:    Pixel coords are off by (0.5, 0.5) – the middle of the bottom left pixel should be (0, 0) (i.e. offset by (1, 1) from the fits standard, which assumes fortran-style 1-indexed arrays) *(#7)*  ",0
"DM-13129","12/22/2017 13:47:46","Warnings in test_camGeomFitsUtils.py ","test_camGeomFitsUtils.py issues warnings that appear to be bugs we should fix. Examples:  {code}    /Users/rowen/UW/LSST/lsstsw3/build/afw/python/lsst/afw/cameraGeom/fitsUtils.py:76: UserWarning: WARNING: Failed to set setName attribute with None value: setName(): incompatible function arguments. The following argument types are supported:        1. (self: lsst.afw.table.ampInfo.ampInfo.AmpInfoRecord, name: str) -> None  ...    /Users/rowen/UW/LSST/lsstsw3/build/afw/python/lsst/afw/cameraGeom/fitsUtils.py:76: UserWarning: WARNING: Failed to set setBBox attribute with None value: setBBox(): incompatible function arguments. The following argument types are supported:        1. (self: lsst.afw.table.ampInfo.ampInfo.AmpInfoRecord, bbox: lsst.afw.geom.box.Box2I) -> None  {code}  See attached log for more info",0.5
"DM-13134","12/22/2017 14:26:19","Firefly Python API: disp.pan(0,0) is ignored","Robert's input #9, DM-7321    disp.pan(0, 0) is ignored (I think anything below (0.5, 0.5) fails). I'd expect to be able to pan any point to the centre of the display, but I'd certainly expect that it'd clip to a valid coord not silently ignore my request)",3
"DM-13135","12/22/2017 14:28:00","Firefly Python API: need to deal with killing Firefly display window","Robert's input #10, DM-7321    Killing the firefly display window seems to leave the notebook that's talking to it perfectly happy, but it doesn't respawn the window even if I call the constructor again.",0
"DM-13136","12/22/2017 16:23:11","Investigate processing failure of HiTS visit 411657, ccd 47","During the DM-503-3 processing, we encountered an error for HiTS dataId visit 411657, ccd 47, filter='g':    {code}  File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/  Linux64/meas_algorithms/14.0-7-g23fdbe95+15/python/lsst/meas/algorithms/  measureApCorr.py"", line 251, in run      (name, len(subset2), self.config.minDegreesOfFreedom+1))  RuntimeError: Unable to measure aperture correction for required algorithm  'base_PsfFlux': only 0 sources, but require at least 2.  {code}    Further investigation shows that 6 PSF sources are selected, but all have self.refFluxKeys.flag set, so they raise the error above in {{measureApCorr}}.     This ticket is to investigate the root cause of the failure to select usable PSF sources.",2
"DM-13144","12/28/2017 02:47:22","Launch S15 large scale test @ cc-in2p3","Non regression on Qserv performances",3
"DM-13145","12/28/2017 10:25:02","Fix LTD Dasher's title truncation","LTD Dasher trims document handles from titles to protect from cases where the document handle was accidentally inserted into the document title string. But for test reports, the title legitimately includes a document handle (but for a different document). See https://dmtr-53.lsst.io/v/index.html The fix is to only trim the document's own handle from a title (and only if it occurs as a distinct word unit).",0.5
"DM-13162","01/03/2018 14:11:03","Fix warning in processEimage","`processEimage.py` currently loads the metadata of a simulated observation twice and runs the same code both times, except it only provides a mapper the second time. This generates a warning the first time the code is run, and is unnecessary because the second call provides the correct exposure with VisitInfo.",0.5
"DM-13166","01/04/2018 09:02:39","Update AST to add support for writing SIP terms","Update AST to the latest version, which adds support for writing SIP terms to FITS headers.",0.5
"DM-13169","01/04/2018 11:18:54","jenkins security update - spectre/meltdown","Update the running kernel on all jenkins related linux nodes as a partial mitigation of the    https://spectreattack.com/    Centos 7/EL7 related links:    https://access.redhat.com/security/vulnerabilities/speculativeexecution  https://lwn.net/Articles/742919/  https://access.redhat.com/errata/RHSA-2018:0007    EC2 instances may need to be restarted regardless due to AWS patching hypervisors, so it makes sense to update the kernel at the same time.    Presumably, the jenkins OSX nodes will also need to be updated but this may be split into a separate ticket.",1
"DM-13172","01/04/2018 13:09:18","Make HiPS popular list configurable and update HiPS list content","- Create a method for the application to configure the list containing popular HiPS data.   - Research what information to report for the HiPS data list in addition to data's type, title and url.     ",8
"DM-13181","01/05/2018 16:33:30","Flowdown LCR-1024 OSS changes to LSE-61","In LCR-1024 requirements language was added to LSE-30 to connect it to LSE-163. This ticket is for adding the derived requirements relationships from the relevant DM requirements back to LSE-30.",1
"DM-13187","01/08/2018 20:28:14","jointcal selected_*_refStars is not correctly computed","Jointcal reports the {{collected\_\*\_refStars}} and the {{selected\_\*\_refStars}}, which should represent the total number of available refStars from the input refcat, and the number that were associated to fittedStars. The latter is the number that are important for the actual fit. Thanks to [~rowen]'s investigation about a separate issue, I realized that the {{selected_*_refStars}} metrics are incorrect: jointcal does not alter the {{associations.refStarList}} during selection, but rather the pointers between fittedStars and refStars.         To fix this, we need to traverse the fittedStarList and count the number of fittedStars that have a non-nullptr \{\{refStar}}. Once done, we'll have to update all of the {{selected_*_refStars}} metrics.    Fixing this will help debug the jointcal test failures in DM-10765.",2
"DM-13189","01/09/2018 09:38:16","Add FunctorKey for Boxes","We often store Box2Is and Box2Ds in tables.  We should have a FunctorKey to do this to reduce code duplication.    Unfortunately we may not be able to use this actually reduce existing code duplication when code has used a different convention for naming fields (as we'd need to maintain backwards compatibility with already-persisted objects), but this should at least reduce duplication going forward.    I'm planning to do this now for DM-12370 so I can use it there.",1
"DM-13193","01/09/2018 12:04:22","weekly release w_2018_01 failed","{{w_2018_01}} failed over the weekend due to git-lfs being down due to the nebula maintenance period.  (Jira was down for maintenance most of yesterday, so there may not be a tickets for the git-lfs problems)    The build was restarted yesterday, which failed attempting to publish eupspkgs.    https://ci.lsst.codes/blue/organizations/jenkins/release%2Fweekly-release/detail/weekly-release/137/pipeline",0.5
"DM-13201","01/09/2018 15:23:09","calexp have TPV and SIP terms","The {{calexp}} exposures in {{validation_data_decam}} have TPV distortion terms in HDU 0, which should have been stripped (and probably are when using the current master of the DM stack). In addition they have the expected SIP distortion terms in HDU 1. Thus it is possible that some FITS readers will read the WCS as having both TPV and SIP distortion terms.    Please reprocess the calexp (and all other processed files, for consistency). The current DM stack properly strips the TPV terms when generating {{calexp}} exposures, so that should take care of the problem.",1
"DM-13204","01/10/2018 07:20:38","Create v14.0 versions of validation_data_(cfht|decam|hsc)","Create v14.0 versions of validation_data_(cfht|decam|hsc).         1. Update these data reference sets with a processing by v14.0 of the stack    2. Add a v14.0     ",2
"DM-13212","01/10/2018 17:28:19","dbserv on PDAC returning 502 Bad Gateway when querying image metadata","Tatiana of SUIT reporting this bug when executing the following query on PDAC:    curl -o imagesContainTarget.json -d 'query=SELECT+*+FROM+sdss_stripe82_00.DeepCoadd+WHERE+scisql_s2PtInCPoly(9.462, -1.152, corner1Ra, corner1Decl, corner2Ra, corner2Decl, corner3Ra, corner3Decl, corner4Ra, corner4Decl)=1;'  [http://lsst-qserv-dax01.ncsa.illinois.edu:5000/db/v0/tap/sync]         Brian's investigation:  Python 3 issue with base64encoding of binary data          ",2
"DM-13213","01/11/2018 08:50:28","Cannot build packages against galsim binaries distributed by eups distrib","I have installed my stack using the eups distrib binaries. Today I went to rebuild the meas_extensions_hsm package which links to the galsim package, and the build failed. It complained about not being able to find libgalsim.dylib in the galsim package. Upon looking in the directory for galsim I see:    {code}  _-rwxr-xr-x  1 nate  staff  3289320 Dec 13 10:12_ _libgalsim.1.5.dylib_  _lrwxr-xr-x  1 nate  staff      190 Dec 13 10:12_ _libgalsim.dylib_ _-> /Users/square/jenkins/workspace/release/tarball/osx/10.9/clang-800.0.42.1/miniconda3-4.3.21-10a4fa6/build/stack/miniconda3-4.3.21-10a4fa6/DarwinX86/galsim/1.5.1.lsst1/lib/libgalsim.1.5.dylib_  {code}    Which shows the simlink for the dylib is still pointing to the a directory on the build system. I propose that if this cant be fixed on the packaging side that we need a program like the shebangtron that will rewrite all the simlinks",1
"DM-13214","01/11/2018 08:56:26","Simultaneously recenter all sources in a blend","When the deblender was updated to use different size boxes for all objects, the {{recenter_sources}} method was modified to fit each sources position separately. This might be the reason behind some faint sources that we see drifting during the fit, so this ticket will update the {{recenter_sources}} method to project each source onto the full model so that the positions can be updated simultaneously again.",1
"DM-13215","01/11/2018 10:54:26","Install 2018-vintage shared stacks","The shared stacks on lsst-dev01 (and Tiger & Perseus, for the Princeton crowd) should be updated to install 2018-vintage weeklies. This should just be a matter of tweaking the regexp.",1
"DM-13225","01/12/2018 13:20:59","Edit SuperTask requirements for order-independence","MagicDraw enforces alphabetic order for requirements (within a package).  This makes it impossible to define the order of the requirements as they appear.    The existing SuperTask requirements in LDM-556 were written without realizing this and contain references to previous requirements as ""above"".    It is desirable for requirements to be meaningful in isolation in any event, so this task is to clean up the language and avoid positional references.",2
"DM-13231","01/12/2018 17:07:01","Make photoCalib outField write to _flux instead of _calFlux","DM-10729 implemented one way of writing back out to a catalog prior to RFC-322 being implemented, that introduced a {{_calFlux}} field for the {{outField}}. {{_calFlux}} isn't used elsewhere in the stack, and is inconsistent with the documentation.    This ticket is to change it to write to {{outField+""_flux""}}, which allows writing the calibrated fluxes back to the same field (by having {{inField==outField}}), which is consistent with what meas_mosaic is currently doing.",1
"DM-13232","01/12/2018 18:22:25","Python PropertySet.set mis-handles array of bool","PropertySet and PropertyList both mis-handle set(name, array-of-bool). The call succeeds, but the item is not correctly saved. Consider the following example:  {code}  from lsst.daf.base import PropertySet, PropertyList  ps = PropertySet()  # or PropertyList()  ps.set(""A"", [False, True])  ps.get(""A"")  # throws *** lsst.pex.exceptions.wrappers.TypeError: Unknown PropertySet value type for A  ps.set(""B"", False)  ps.add(""B"", True)  ps.get(""B"")  # returns [False, True]  {code}    Note that it is possible to set an array of bool using add, so it seems to be something about PropertySet.set.",2
"DM-13234","01/15/2018 01:50:42","Use k8s headless service","Headless service will set up a constant DNS name for all Qserv nodes. This will allow to ease configuration.",8
"DM-13240","01/16/2018 10:36:43","Fixes in dbserv for better handling with reporting SQL errors and int datatype","Tatiana G. at SUIT reported this:  {code}  SELECT * FROM sdss_stripe82_01.RunDeepSource1 WHERE qserv_areaspec_ellipse(9.469,-1.152,58,36,0);   [DAX] Bad Gateway  {code}  Instead, it'll be more informative to redirect the SQL error back up through dbserv to query originator.    The {{int}} datatype should be set to {{long}}, removing the implicit assumption that clients of dbserv must be Python-based.          ",2
"DM-13242","01/16/2018 11:40:05","Leftover result tables must produce errors","There was an incident reported by [~vaikunth] on slack:     {quote}  I did a count(*) on LSST20 and it gave me back real data and columns for the result …      [3:31]    ```[vthukral@ccosvms0070 in2p3]$ time mysql --host ccqserv125 --port 4040 --user qsmaster LSST20 -e ""SELECT COUNT(*) FROM Object;"";  +--------------------+---------------------+---------------------------+----------------------------+-----------------------------------+------------------------+------------------------+------------------------+  | ra                 | decl                | raVar                     | declVar                    | radeclCov                         | u_psfFlux              | u_psfFluxSigma         | u_apFlux               |  +--------------------+---------------------+---------------------------+----------------------------+-----------------------------------+------------------------+------------------------+------------------------+  | 284.98083849415934 |  -63.32859963197394 |       0.03588015008077164 |        0.04575243321809053 |       0.0000000012152185681231455 |  7.396697501646931e-30 |  9.604600362557029e-31 |  9.647062881997884e-30 |  ```  (and many more rows)  {quote}    and Igor dug out this from the log:  {noformat}  [2018-01-13T00:27:12.724+0100] [LWP:503] DEBUG rproc.InfileMerger (core/modules/rproc/InfileMerger.cc:264) - Merging w/CREATE TABLE result_364646 ENGINE=MyISAM SELECT SUM(QS1_COUNT) FROM result_364646_m  [2018-01-13T00:27:12.746+0100] [LWP:503] ERROR util.Error (core/modules/util/Error.cc:50) - Error [1050] Error applying sql: Error 1050: Table 'result_364646' already exists Unable to execute query: CREATE TABLE result_364646 ENGINE=MyISAM SELECT SUM(QS1_COUNT) FROM result_364646_m  [2018-01-13T00:27:12.746+0100] [LWP:503] ERROR rproc.InfileMerger (core/modules/rproc/InfileMerger.cc:342) - InfileMerger error: Error applying sql: Error 1050: Table 'result_364646' already exists Unable to execute query: CREATE TABLE result_364646 ENGINE=MyISAM SELECT SUM(QS1_COUNT) FROM result_364646_m  {noformat}    This looks like there was a leftover result table whose name was reused for new query (this could happen when someone resets QMeta autoincrement ID).    Qserv should handle this situation better, there should be an error returned to user in that case, not data from old result table (or leftover table should be deleted).",2
"DM-13250","01/16/2018 14:38:38","Write simple filter for sims alerts","Experiment with a simple way to filter alert data in Python using sims data.",8
"DM-13258","01/17/2018 11:17:09","upgrade blueocean to 1.5.x","blueocean 1.4.0 was released today:    https://plugins.jenkins.io/blueocean    The changelog needs to be inspected to see if it addresses any of the open user requests.",1
"DM-13264","01/18/2018 12:06:15","Add use case for jobs as composite datasets","Add a use case and requirement that describes the need for the {{Butler}} to have the ability to persist the data blobs associated with a {{Job}} and the JSON file that describes the {{Job}} in different datastores.  For example the blobs may be placed in an object store with the {{Job}} could be ingested in a JSON aware SQL database.",1
"DM-13265","01/18/2018 12:08:42","Saga error handling","Firefly is using sagas for action side-effects.    An error in masterSaga causes problems in later application behavior, like the following table loads not completing on the client unless they are explicitly backgrounded.    To test, modify getCovColumnsForQuery in CoverageWatcher.js to produce an error:    ```  {noformat}function getCovColumnsForQuery(options, table) {    const cAry= [...options.getCornersColumns(table), null];    const base = cAry.map( (c)=> `""${c.lonCol}"",""${c.latCol}""`).join();     return base+',""ROW_IDX""';  }{noformat}       The function above produces the exception and console output in the attachment, when a catalog search is requested in firefly.    Notice that the following catalog searches will not complete on the client (trackFetch in TableCntlr.js will not be called).         Implementation done:   * Use spawn when using dispatchAddSaga. This prevents the unhandled exceptions in one saga to cancel all the siblings.   * Use fork with dispatchAddActionWatcher, because we are catching the unhandled exceptions. If an exception occurs in the callback, it won't cancel the saga.   * Added documentation for actionWatcherCallback   * Fixed a bug preventing remote charts on the test page   * converted some dispatchAddSaga to dispatchAddActionWatcher, including coverage and image metadata watchers and firfely.utl.addActionListener API method.    Test case: [http://localhost:8080/firefly/demo/ffapi-highlevel-test.html] Open console to view the output, ""Start selection extensions"", ""Track mouth readout"". Before, you would get an error when switching readout from one image to the other, which would cancel both listeners. Now, a single error in user defined readout listener does not prevent the following calls to succeed.",5
"DM-13267","01/18/2018 12:26:33","Create presentation for Princeton Monday Meeting","Create a presentation to give the Princeton software group an update on the new deblender, which is likely to be implemented in the stack as part of the current epic.",2
"DM-13269","01/18/2018 17:14:17","Improve jointcal debugging output","The fitter chi2 contributions debug output files need to be cleaned up and improved so we can do direct comparisons of jointcal's internal model, pre- and post-fit. Also have to make some plots to test that the output is good.",8
"DM-13270","01/18/2018 17:25:26","cherry pick ccdImage method cleanups from DM-9071","I made a number of cleanups of method names in DM-9071, in this commit:    https://github.com/lsst/jointcal/commit/9efc5d23808b5e6f33d69e7ccd9bcf6c0bc844cb    These should be picked out of that ticket and pushed to master.",1
"DM-13274","01/19/2018 09:17:50","Deblender sometimes fails to model second object, or crashes","This issue was reported by Erin Sheldon's student. The problem was that some blends failed during initialization with a {{ValueError:}}  {code:java}/Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/operators.py:37: RuntimeWarning: invalid value encountered in true_divide  /Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/source.py:149: RuntimeWarning: invalid value encountered in less    morph[morph<0] = 0  /Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/source.py:161: RuntimeWarning: invalid value encountered in greater    ypix, xpix = np.where(morph>blend._bg_rms[band]/2)  /Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/source.py:163: RuntimeWarning: invalid value encountered in greater    ypix, xpix = np.where(morph>0)  Traceback (most recent call last):    File ""galsim_deblend.py"", line 194, in <module>      model,mod2 = make_model(img,bg_rms,B,coords)    File ""galsim_deblend.py"", line 80, in make_model      blend = scarlet.Blend(sources, img, bg_rms=bg_rms)    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/blend.py"", line 76, in __init__      self.init_sources()    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/blend.py"", line 255, in init_sources      self.sources[m].init_source(self, self._img)    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/source.py"", line 579, in init_source      self.init_func(self, blend, img)    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/source.py"", line 164, in init_templates      Ny = np.max(ypix)-np.min(ypix)    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 2272, in amax      out=out, **kwargs)    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py"", line 26, in _amax      return umr_maximum(a, axis, None, out, keepdims)  ValueError: zero-size array to reduction operation maximum which has no identity  {code}  And other times failed during the fit with a {{LinAlgError}}:  {code:java}/Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/utils.py:340: RuntimeWarning: divide by zero encountered in double_scalars  /Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/utils.py:340: RuntimeWarning: invalid value encountered in multiply  /Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/utils.py:310: RuntimeWarning: divide by zero encountered in double_scalars  /Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/utils.py:310: RuntimeWarning: invalid value encountered in multiply  /Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/utils.py:358: RuntimeWarning: divide by zero encountered in double_scalars  /Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/utils.py:360: RuntimeWarning: divide by zero encountered in double_scalars  /Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/utils.py:360: RuntimeWarning: invalid value encountered in true_divide  Traceback (most recent call last):    File ""galsim_deblend.py"", line 194, in <module>      model,mod2 = make_model(img,bg_rms,B,coords)    File ""galsim_deblend.py"", line 92, in make_model      blend.fit(200)#, e_rel=1e-1)    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/blend.py"", line 229, in fit      e_rel=self._e_rel, e_abs=self._e_abs, accelerated=accelerated, traceback=traceback)    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/algorithms.py"", line 487, in bsdmm    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/blend.py"", line 541, in _steps_f      self._stepAS = [self._cbAS[block](block) for block in [0,1]]    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/blend.py"", line 541, in <listcomp>      self._stepAS = [self._cbAS[block](block) for block in [0,1]]    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/proxmin-0.4.3-py3.6.egg/proxmin/utils.py"", line 222, in __call__    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/scarlet-0.1.d2e9b44-py3.6-macosx-10.7-x86_64.egg/scarlet/blend.py"", line 486, in _one_over_lipschitz      LA = np.real(np.linalg.eigvals(SSigma_1S).max())    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py"", line 889, in eigvals      _assertFinite(a)    File ""/Users/lorena/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py"", line 217, in _assertFinite      raise LinAlgError(""Array must not contain infs or NaNs"")  numpy.linalg.linalg.LinAlgError: Array must not contain infs or NaNs{code}       The root cause of both issues is that the new deblender takes coordinates as (y,x) while the user was passing (x,y) (see the discussion on [github|https://github.com/fred3m/scarlet/issues/26] for more).         The first error was thrown because the incorrect position given had no flux above the noise level at the peak, so the source couldn't be initialized. We should implement a check for this to warn the user and have some fallback initialization.         The second error is caused by a problem with fitting positions for sources that have no flux, which [~pmelchior] has fixed in a soon to be merged branch.         I'll make the change to the initialization error and close this ticket once [~pmelchior]'s branch has been merged.",2
"DM-13289","01/19/2018 14:17:27","Change SsiSession to be deleted by shared pointer.","SsiSession is currently deleted by a call to Finished, and this can happen before tasks using the object are all done with it, causing a segfault and worker crash. Moving the the UnBindRequest call to the destructor and using a shared pointer for the SsiSesion object should make it safe. Also, it looks like eliminating the SsiSession::ReplyChannel class will make the code simpler, but some changes need to be made to expose the functions ReplyChannel calls in xrdssi.",5
"DM-13301","01/19/2018 19:39:32","Configurable worker identity and chunk availability map in Qserv","This ticket proposes two improvements to be made in a scope of the Qserv worker services:  # replacing the current mechanism of deriving a list of available chunk numbers by parsing table names within the MySQL's _information schema_ with a new one based on an explicit persistent configuration stored within a database. The present mechanism has one major limitation preventing from integrating Qserv with the dynamic chunk replication system - it runs only one time when the worker services are being started. In the proposed model a list of chunk numbers (along with the corresponding table and database names) will be stored in a dedicated table seen by the relevant worker services. This will also allow a consistent view onto a stable collection of chunks. It will also lay a foundation for implementing a safe and efficient coordination mechanism between the replication system and Qserv worker services.  # adding a persistent support for the unique identity of the worker services. The idea here is to store some unique (in a scope of a particular Qserv cluster) name within databases used by the workers. These identifiers will be used as a foundation for setting up worker-specific resources and by the replication system. Extend the implementation and the API of the Qserv's module *wpublish* to support the proposed improvement.    For both tasks make proper changes to the Qserv management, integration and data loading scripts to populate the newly added table with a valid set of chunks and the worker identity strings, so that these resources would be available to the corresponding worker services.    Make sure the unit tests were updated accordingly.  ",3
"DM-13302","01/19/2018 19:57:27","Adding support for worker-specific resources in Qserv","Extend the present implementation of Qserv worker plugin (xrootd SSI) to recognize (and route to the corresponding handler) requests to the worker-specific resources. The proposed extension is planned to be used for the following tasks:  * integrating Qserv with the replication system which requires an interface for interacting with the worker services to notify the ones about changes in chunk availability on the corresponding nodes  * allowing to pull worker-specific stats and counters into the custom service monitoring applications",3
"DM-13307","01/22/2018 12:20:13","Add final conclusions to DMTN-028","Add summary and conclusions regarding suitability of Kafka to alert distribution system needs to DMTN-028.",3
"DM-13317","01/23/2018 14:12:34","Filtered table display in image overlay fails to apply the filter","This is in the test build [~tatianag] is running in response to DM-13099.    I've queried two tables on PDAC: AllWISE catalog sources and WISE All-Sky (4 band) single-epoch sources.  I've identified a cluster of sources in the single-epoch data that I suspect come from a single object, and I'm trying to identify that object in the AllWISE catalog.    I drew a box around the cluster in the single-epoch data and used it as a filter.    I then tried to copy the filter specification from the ""filter panel"" dialog from the single-epoch table:     {quote}""ra"" > 0.9909882575279574;""ra""  < 0.992205622144179;""decl"" > 0.0072994581062348135;""decl""  < 0.009400119782442063""{quote}    to the filter panel on the AllWISE table.  That didn't work at all - a separate problem? - so instead I copied the two expressions from the column headers from the one table to the other: ""> 0.9909882575279574; < 0.992205622144179"" for {{ra}} and ""> 0.0072994581062348135; < 0.009400119782442063"" for {{decl}}.    That worked, and the row I wanted ended up selected in the table.  However, the selection was not successfully applied to the coverage image for the AllWISE table, only for the single-epoch table.  See screenshots.",1
"DM-13322","01/23/2018 15:07:10","memory mishandled inside UnitNormMap","AST UnitNormMap mis-handles the memory for the center parameter when unpersisting: it may allocate one too many doubles and fill the final one with garbage.",0.5
"DM-13325","01/23/2018 16:20:01","warpExposure does not propogate visitInfo","[~sullivan] discovered that warpExposure is not propagating the visitInfo of its parent exposure. This is a bug, and it should be a trivial fix on line 250 of {{afw/math/warpExposure.cc}} (doing the same as the wcs and calib), plus a bit of unittests.",2
"DM-13337","01/24/2018 16:24:51","selection of overlaid catalog does not work after filtering with a region selection (either circle or rectangle)","When I have a catalog overlay and an elliptical or rectangle region, I can successfully filter the catalog on that region. However, I cannot then click on any of the overlay points to select them. I have to get rid of the elliptical region by clicking on the image toolbar. And then I can't get that region back.     ",1
"DM-13342","01/25/2018 07:26:30","Refactor Datastore prototype to improve orthogonality","In order to facilitate work on the Datastore prototype the existing code should be refactored a bit.",0.5
"DM-13345","01/25/2018 09:43:26","Improve template and warp variance for Warp Compare","Template: The PSF-matched sigma-clipped coadd variance is computed from the variance planes of the inputs and not the empirical dispersion of the images as expected. Intuitively if we're looking for outliers, we'd want the template variance to use the dispersion of the images.          Warp: As pointed out by [~price], we'd also want to scale the variance in the warps for the same reason we scale the variance on coadds before detection on coadds. ",2
"DM-13361","01/25/2018 12:26:32","Minimal S3 backed Datastore","This would exercise {{Datastore}} and {{Formatter}} by requiring:    * Pass-through of credentials  * SHA instead of filename  * Serializing into memory rather than file  * Write to file then upload.    Only implement for one or possibly two {{DatasetType}}s.",20
"DM-13363","01/25/2018 12:36:41","Minimal in-memory caching Datastore","This ticket is for creating an in-memory datastore and adjusting the tests to run with it and the existing posix datastore.  ",8
"DM-13371","01/25/2018 15:28:15","Enable flake8 testing in daf_butler","Now that the butler prototype is becoming the new butler, flake8 tests should be enabled.",0.5
"DM-13374","01/25/2018 15:34:58","Deconstruct Butler prototype for redesign","Remove all elements (and associated tests) in current prototype of Butler and Registry that will be redesigned (as opposed to keeping everything working during redesign).",2
"DM-13379","01/26/2018 12:59:05","Determine whether the LSST LaTeX documentclass document type codes can be retired","Currently, LSST documents generated from LaTeX source use a template that has a ""document type"" parameter.  This parameter has documented values of ""DM"", ""MN"", and ""CP"" at this time, and a commonly-used but undocumented value of ""SE"" as well (for LSE- and LPM- documents).    As the documentation at https://lsst-texmf.lsst.io/lsstdoc.html says:  {quote}DM defines the document type to be a “Data Management” document. Other options include MN for minutes and CP for conference proceedings but these are holdovers from the original Gaia class file and currently have no effect on the document output. They are considered optional, but descriptive, at this time.{quote}    I suggest that we either add ""SE"" to the list above or, recognizing that the document type is in fact unused in the LaTeX template (it relies on the DocuShare handle, instead), remove the mention of the document type from the {{lsst-texmf}} documentation and, opportunistically, remove the document types from existing documents when other editing actions occur.",0.5
"DM-13381","01/26/2018 15:43:44","Rewrite updateRefCentroids and updateSourceCoords to convert all positions at once","The current updateRefCentroids and updateSourceCoords functions are pure Python and compute one position at a time. SkyWcs performs much better if it can convert a batch of values at once.    I could rewrite the Python code to extract all the positions, convert them, then loop through the catalog again. But I worry this will be needlessly slow (the looping is already slow, and this will only make it worse), so I propose to rewrite the functions in C++ instead.",2
"DM-13384","01/26/2018 18:41:51","Fix Qserv docker image build","A recent merge has broken docker image build scripts used by developers and Travis and Jenkins CI (undefined variable $DOCKER_RUN_DIR in several Dockerfiles)  ",1
"DM-13388","01/27/2018 10:36:27","Enable visit-level sky subtraction for HSC by default","Turn on visit-level sky subtraction by default for HSC, which I believe means updating the coaddition configuration to expect its outputs to be available.",0.5
"DM-13389","01/27/2018 10:38:14","Enable transmission curve attachment for HSC by default","Set  {code:java}  self.doAttachTransmissionCurve = True{code}  in {{SubaruIsrTask}}.",0.5
"DM-13396","01/29/2018 10:45:43","Fix coadd mask propagation","DM-9953 created the SENSOR_EDGE mask to mark coadd pixels that were on or near the boundary of an input CCD (and hence should have INEXACT_PSF set as well).  It also started the propagation of those EDGE regions to the coadd.    Propagating the EDGE regions to the coadd caused problems, however, because many of them were affected by otherwise-unmasked bad pixels (leaks from amps or somesuch), so that was disabled on DM-12931.  Contrary to comments on that ticket, this seems to have broken the propagation of SENSOR_EDGE, or perhaps something more recent broke it.    We should also consider whether to split the current CLIPPED flag into both CLIPPED and REJECTED, with the latter being used for pixels rejected due to mask values from the calexps rather than an explicit smart-clip algorithm (e.g. SafeClip or CompareWarps).  ",2
"DM-13403","01/29/2018 14:15:55","numpy types fail in butler dataIds","The {{np.int64}} type appears to be treated differently from the built-in python {{int}} type in butler dataIds. As an example:    {code}  In [9]: butler.datasetExists('src',dataId={'visit':13376, 'ccd':15})  Out[9]: True    In [10]: butler.datasetExists('src',dataId={'visit':13376, 'ccd':np.int64(15)})  Out[10]: False  {code}    However, if one attempts to use that `np.int64` type in a {{butler.get}}, it raises an exception with the value appearing without its type ({{str(15)}} and {{str(np.int64(15))}} are the same), which is extremely confusing to the user.    {code}  NoResults: No locations for get: datasetType:src dataId:DataId(initialdata={'visit': 13376, 'ccd': 15}, tag=set())  {code}    Note also that {{hash(15) == hash(np.int64(15))}}, so there is a general expectation that these will behave similarly.",1
"DM-13410","01/29/2018 15:06:53","Shrink input bboxes in inputRecorder per psfMatched Warp in WarpCompare","WarpCompare has no temporal information for the pixels that are outside the boundary of the psfMatched Warps. These pixels are marked NO_DATA. The BBoxes in the CoaddInputRecorder should be shrunk so that we can have an exact CoaddPsf for sources that fall in the border of calexp. ",5
"DM-13412","01/29/2018 16:41:37","camera mapper should specify DecoratedImageU instead of ImageU","Here's pointer to the camera mapper in question:    lsst.obs.sdss.sdssMapper.SdssMapper    Problem with the return image object type (via butler) for the following data:          /datasets/sdss/preprocessed/dr7/runs/2708/40/corr/3/fpC-002708-r3-0103.fit.gz    Per [~ktl], the correct fix should be following (and new RFC for caution):     If you want the metadata and WCS, I think it will be sufficient to change these lines [https://github.com/lsst/obs_sdss/blob/master/policy/SdssMapper.yaml#L37-L38]   GitHub lsst/obs_sdss obs_sdss - SDSS-specific configuration and tasks for the LSST Data Management Stack    [2:31 PM]   to say `DecoratedImageU` instead of `ImageU`",3
"DM-13413","01/30/2018 10:31:37","blueocean 1.4.0 pipeline view ignoring clicks","After a jenkins core + blueocean update to {{1.4.0}}, clicks in the build pipeline view to change between branches are frequently ignored.  It appears that this is only happening for builds that are actively running.    A search of the upstream jenkins Jira did not find an existing issues thats even in the ballpark.  A screencast should be made to report upstream.  Downgrading BO to {{1.3.x}} may be the best option.",1
"DM-13416","01/30/2018 16:39:16","Firefly should reconnect periodically to the server when the connection fails","Firefly requires a persistent websocket connection to the server in order to function properly.  In the case when the client is disconnected from the server, Firefly should periodically attempt to reconnect.  This can happen when one closes his/her laptop or switching from one network to another.    When Firefly is no connected to the server, there should be an indication showing that it's no longer connected.     ",2
"DM-13417","01/30/2018 18:22:01","Cleanup error reporting and docstrings in cameraGeom.utils","The docstring for {{cameraGeom.utils.makeImageFromCamera}} could be cleaned up (return type specified, binSize and other parameters clarified), and the ""Unable to fit image for detector"" warning message should include the exact exception message that was raised (figuring out the nature of the problem is difficult otherwise). That {{print()}} should probably also be turned into a log message.",0.5
"DM-13420","01/31/2018 12:57:14","error message handling for failed table filtering","From DM-13203 item 1, 4, 5.   # Generate a meaningful error message when a column filter expression applied to a table is invalid.  This may not require any extra parsing effort for the filter expressions - the system knows the name of the column and the text typed in the filter field, and can just wrap the underlying error message with something like ""The filter expression 'xyzzy' applied to column 'foobar' is invalid."".  For the purposes of debugging it may be useful to provide a UI action (e.g., a disclosure triangle) that makes it possible to see the full underlying exception text.   #    #    # Ensure that, when an invalid filter expression is entered, after dismissing the error message the user is returned to the _same state of their data and previously-specified filters as before the failed attempt to construct a filter_.  This does not require arbitrary undos, but only that the displayed state of the table is not changed before the filter has been determined to be valid.   # Do not use the word ""Reload"" for the button that dismisses the error dialog.  ""Back"", ""Close"", ""Cancel"" would all be better choices.  Consider asking for feedback from others, e.g., Vandana, for this choice.  ""Reload"" _strongly_ suggests - to me - that the underlying data retrieval operation would be repeated rather than just abandoning the filter attempt.",5
"DM-13438","02/01/2018 12:47:24","Create simulations with real COSMOS galaxies","LSST internal reviewers suggested using a set of simulations using real galaxy data as a more robust test of the new deblender. This ticket involves creating another 10k simulated blends (this time with real galaxies) and running both the new and old deblenders on it.",8
"DM-13439","02/01/2018 12:48:23","jenkins update-cmirror job broken","This job fails in the current stripped down jenkins env and probably needs to build in a container:    {code:java}  [linux-64] Running shell script  + wget https://repo.continuum.io/pkgs/free/linux-64/repodata.json  /home/jenkins-slave/workspace/sqre/infrastructure/update-cmirror/repodata/linux-64_tmp/durable-716fc7f8/script.sh: line 2: wget: command not found  {code}    https://ci.lsst.codes/job/sqre/job/infrastructure/job/update-cmirror/267/console    This job is also not running periodically as it should be.  The cron trigger must have been lost during a re-factoring.",0.5
"DM-13441","02/01/2018 13:11:50","Fix nominal gains and readNoise values","Some gains are zero, that's bad (leads to inf variance, and therefore NaNs).    The other values are meaningless as this is currently being used for any given raft, so lets set them all to 1 for now. Also set all the readNoise values to 10 for now.",2
"DM-13451","02/01/2018 16:32:39","Make ap_verify responsible for ingestion","Currently, {{ap_pipe}} ingests and processes data. For its ongoing conversion to a {{CmdLineTask}} (DM-13163) and forward-compatibility with {{Pipeline}} classes, {{ap_pipe}} should work on an externally provided repository.    For the time being, this functionality should be moved to {{ap_verify}} (which currently requires uningested data), and the final responsibility for ingestion will be determined later.",8
"DM-13452","02/01/2018 16:47:18","Extend QuantumGraph implementation","Current implementation of QuantumGraph (with different name) is rather minimalist and is optimized mostly for efficient storage. We want to extend it to make usable for other cases as well. ",8
"DM-13453","02/01/2018 16:50:52","Upgrade psutil to version 5","psutil has had some major updates since RFC-176 with some significant speed ups.    This also seems to fix a DeprecationWarning triggered by the current version (which is my main motivation for updating this).    psutil is only used in utils.tests at this time.",0.5
"DM-13456","02/01/2018 17:53:37","Finalize and cleanup data for KPM30","Prepare and finalize data readiness, generate appropriate ObjectIDs and queries for KPM30 run.",8
"DM-13459","02/01/2018 18:10:15","Parse single example query statement with antlr4 and build query objects as antlr2 would"," Parse a query like ""SELECT objectId, ra_PS FROM Object WHERE objectId=1234"" with the new antlr4 parser.",13
"DM-13460","02/01/2018 18:10:47","Extend antlr4 parser abilities","Extend antlr4 parser beyond simple statement, for example:    {{SELECT COUNT ( * ) as OBJ_COUNT FROM Object WHERE qserv_areaspec_box ( 0.1 , - 6 , 4 , 6 ) AND scisql_fluxToAbMag ( zFlux_PS ) BETWEEN 20 AND 24 AND scisql_fluxToAbMag ( gFlux_PS ) - scisql_fluxToAbMag ( rFlux_PS ) BETWEEN 0.1 AND 0.9 AND scisql_fluxToAbMag ( iFlux_PS ) - scisql_fluxToAbMag ( zFlux_PS ) BETWEEN 0.1 AND 1.0)))));}}",13
"DM-13477","02/02/2018 16:41:54","Move association math from DIAObjectCollection into AssociationTask","Removes the score and match methods from DIAObjectCollection and puts them, for now, into AssociationTask. Once AssociationTask moves into lsst-distrib we can move the further into meas_algorithms.",5
"DM-13485","02/05/2018 08:22:46","Fix NB filter transmission curve dataset filenames","Some narrow-band filter names used by {{installTransmissionCurves.py}} are missing leading zeros (e.g. should be NB0921 rather than NB921).",0.5
"DM-13492","02/05/2018 14:00:00","Remove --rerun argument for ap_verify","Nobody remembers why {{\-\-rerun}} was originally added to {{ap_verify}}'s command-line interface; see discussion on DM-12853. In the absence of a compelling case for it (it cannot behave exactly like the {{\-\-rerun}} argument for command line tasks, because {{ap_verify}} does not have an input repository), and given its currently confusing behavior, we should remove {{\-\-rerun}} from the {{ap_verify}} API. It can be added back once we have a clear expectation of how it should behave.",1
"DM-13493","02/05/2018 14:38:33","BaseSourceSelectorConfig should not filter on ""interpolated""","Given the recent change to how {{interpolated}} is used in the stack, it appears that {{BaseSourceSelectorConfig}} should not include {{interpolated}} in its list of default {{badFlags}}. We may want to add any new ""why interplated"" flags instead.",1
"DM-13498","02/06/2018 09:40:23","Add config to make WarpCompare very conservative","Currently a drawback of WarpCompare is any epochs where a source doesn't look like the other epochs gets clipped. This leads to some loss of signal in bright sources. While future work on the image comparison will improve this (background matching, image-to-image psf matching, etc....) for now, we probably want a nuclear option config:    If an artifact candidate fits entirely within a template coadd source, then don't clip it. This means that unfortunate CRs or supernovae won't be clipped, but will improve the photometry. For the HSC release,  coadd photometry seems to be a higher priority.   ",3
"DM-13501","02/06/2018 13:19:20","Add obs_decam to validation_data_decam/ups","Add    setupRequired(obs_decam)    to a    ups/validation_data_decam.table    file.",0
"DM-13506","02/06/2018 20:25:16","Review and fixups for antlr4 eups package","Assist Nate in packaging antlr4 as a third-party TaP eups package",2
"DM-13507","02/07/2018 09:10:22","Add stable hash to SkyMap objects","In the Gen3 butler, the tracts and patches defined by a SkyMap will be loaded into a database, and that will make it much more important to recognize when the same SkyMap has already been loaded.  While SkyMap objects already support equality comparison, it'd be nice if they could also produce a stable hash that can be used to uniquely label them.    -Since that basically amounts to being able to hash the SkyMap's configuration, I think it makes the most sense to actually add this hashing support directly to {{pex_config}}.  Being able to compare hashes to check for config equality seems like it'd be generally useful to.-    I'm currently planning to do this with {{hashlib.sha1}}, rather than just the {{hash}} builtin, because I want something that's guaranteed to be stable between Python versions.    Note that these in-memory hashes will not be equivalent to hashes of the files in which these objects are stored.",2
"DM-13509","02/07/2018 13:33:44","Some pure python packages add to LD_LIBRARY_PATH","The pure python packages listed in Components have ups tables that add themselves to LD_LIBRARY_PATH and two related library paths. They should not do this. Remove the following from their ups table files:  {code}  envPrepend(LD_LIBRARY_PATH, ${PRODUCT_DIR}/lib)  envPrepend(DYLD_LIBRARY_PATH, ${PRODUCT_DIR}/lib)  envPrepend(LSST_LIBRARY_PATH, ${PRODUCT_DIR}/lib)  {code}",0.5
"DM-13510","02/07/2018 13:45:11","Correct inconsistencies in LDM-503 text and tables and improve auto-generation process","The existing LDM-503 package has traces of a strategy to autogenerate substantial sections of the document from a milestone table maintained as the {{dmtestmilstones.csv}} file (as well as other portions from other .csv files).  In particular, from this file a {{dmtestmilstones.tex}} file is generated, for use in {{schedtab.tex}} in Section 6 of the document, as well as a file {{testsections.tex}} used as the body of Section 7 of the document.    There are signs that both of those {{.tex}} files were hand-edited after the last time the autogeneration was performed, and these edits are at variance with each other, with the outcome that there are three versions, slightly differing, of the associated text.    The auto-generation script is not part of the {{Makefile}}, and its outputs are part of the Github _acquis_ for the package, or this would have turned out differently.    Note also that the autogeneration script has a provision to substitute a longer description of a milestone into {{testsections.tex}} if one is available.  Currently this is only the case for LDM-503-2, for which there is an {{LDM-503-2.tex}} file in the package (and a much more extensive {{f17_drp.tex}} file which it in turn includes).  This longer description appears to predate the fuller, separate test specification for DRP and may be redundant or even in conflict with it; I have not checked that.",3
"DM-13519","02/08/2018 14:53:01","Implement per-object Galactic Extinction correction in color analysis QA plots","Implement a per-object Galactic Extinction correction for use with the color-analysis QA plots to replace the per-field placeholder included in DM-13154.  It looks like there is code in \{\{sims_photUtils}} (and dependencies) to do this, so this will be an attempt to get that working with the analysis scripts.         Note that this requires the A_filter/E(B-V) extinction coefficients for the HSC filters (awaiting a response from the HSC team, the placeholder noted above is just using SDSS filter values).",5
"DM-13520","02/08/2018 16:47:44","Add readme to obs_subaru","[~yusra] pointed out the diagram of the HSC focal plane in obs_subaru, which I wouldn't have thought to check. To make it more obvious for future me, I've created a readme and added a note about the diagram to it.",0.5
"DM-13524","02/09/2018 15:31:43","Add unit tests for ingestion","As a self-contained module within {{ap_verify}}, {{ingestion}} should have unit tests of its functions. It may be possible to implement a unit test by specifically ingesting only 1-2 files of each type.",2
"DM-13526","02/09/2018 18:54:44","Fixed a bug in the schema migration tool for worker databases","The original implementation of the schema migration tool deployed for worker side databases *qservw_worker* as per [DM-13301] won't work for tables which have '_' in their base names. The original tool was designed and tested for the LSST-style table names:  {code}Object_<chunk>{code}  However, it won't work for tables like:  {code}  Sart_More_<chunk>  {code}  The first symbol '_' will confuse the stored procedure implemented in the original version of the tool.    A goal of this ticket is to fix this problem by making the stored procedure to look for the last '_' when separating the base names of tables from the trailing chunk numbers.",0.5
"DM-13534","02/12/2018 12:53:08","Upgrade ndarray to upstream 1.4.2","If all goes well, this should allow us to start removing our dependence on the NumPy C API (left for another ticket).    This will probably require modifying our pybind11 build to install its CMake config files (unless we're doing that already, which I doubt), since ndarray now uses those to find pybind11.",1
"DM-13535","02/12/2018 17:16:03","Accept idiomatic input repositories","Currently, {{ap_pipe}} requires a single input repository with two directories, {{ingested}} and {{calibingested}}, each a repository in its own right. This will be a problem for DM-13163 because most command-line tasks allow the URIs of the data and calib repositories to be independent, and in any case the Stack convention is different from our current usage.    This ticket will change the command line to accept a separate calib repository (the argument should behave the same as {{processCcd.py --calib}}?). The API to {{ap_verify}} will not be changed, as it will almost certainly change when DM-13163 is implemented.",1
"DM-13536","02/12/2018 17:16:13","Use repositories more idiomatically","For forward-compatibility with DM-13163, {{ap_verify}} should create separate repositories for ingestion and calibration. In effect, the current ""output repository"" should be a convenient ""workspace"" directory but not a repository.    In addition, the interface module {{pipeline_driver}} should make a distinction between input and output repositories, choosing the location of the latter instead of deferring the choice to {{ap_pipe}}. Neither {{CmdLineTasks}}, nor in the future {{Pipelines}}, are responsible for output paths.    Because the API will change again as part of DM-13163, changes to the interface between {{ap_pipe}} and {{ap_verify}} should be kept minimal; most likely, this will entail removing the ill-advised wrappers added in DM-12257 and calling the existing functions from {{pipeline_driver}} again.",3
"DM-13538","02/13/2018 09:46:08","Add a Jenkins job to build and deploy Firefly's app to a local k8s cluster","During a pull request or an evaluation of an added feature, it's useful to have a specific version of a Firefly app up and running.      Add a job to Jenkins to build and deploy a Firefly app to a local k8s cluster.    This job should take as input; branch, env, and a label used to reference the instance running in k8s.",2
"DM-13539","02/13/2018 09:58:28","astshim fails to preserve SIP terms for some TAN SIP when writing FITS metadata","[~lauren] has found some cases where a TAN-SIP SkyWcs is not written as TAN-SIP to FITS metadata (instead a local TAN approximation is used). The attached file is a program showing an example. I have reported the issue to David Berry in hopes he can fix it or suggest a workaround.    This issue actually consists of two parts:  - Normal TAN-SIP WCS cannot be written as FITS-WCS header cards. David Berry has implement a fix for that problem.  - WCS rotated by rotateWcsPixelsBy90 cannot be written as FITS-WCS. I have split that into a separate ticket: DM-13564.",0.5
"DM-13546","02/13/2018 14:08:08","Build robust Kafka 3-broker cluster","Current alert_stream Kafka prototype uses a single broker Kafka instance.  Build a cluster of 3 brokers and ensure that if one is inaccessible the others can receive and emit alerts.  Also, the current alert_stream Kafka cluster loses all data if a broker container goes down because the Kafka container houses all the data.  Figure out how and where to mount external volumes for Kafka and Zookeeper so that data is recovered and so that consumer offsets are recovered (consumers unaffected) if one or all brokers go down.",8
"DM-13550","02/13/2018 18:39:47","Fix Qserv docker image build","Qserv docker image builds broken after merge of DM-13458 (because libuuid prerequisite is not captured in Debian prereq install script)",0.5
"DM-13554","02/14/2018 11:52:14","Build starlink_ast with opt=3","At present stalrink_ast is built using optimization level 2. I propose to build it with our standard optimization level of 3 in hope of increasing performance.  ",0.5
"DM-13556","02/14/2018 12:04:13","Upgrade sqlalchemy to 1.2","This is to upgrade lsst/alchemy from 1.08 to 1.2.2, with two years worth of improvements and bug fixes, which will facilitate dax_v1 implementation at retrieving/mapping data from the SQL database.     ",5
"DM-13557","02/14/2018 12:54:13","Minor config doc fixes for SourceDetectionTask","These are being generated in real-time when answering questions about detection in the DRP team meeting, but I want to get everyone to review my improvements instead of just merging them (even though that's formally permitted).     ",0.5
"DM-13562","02/14/2018 15:53:49","Migrate base docker containers to Centos7 + Devtoolset6","The new antlr4 package requires gcc 5.0 or greater, but Debian Jessie, on which the Qserv base containers are based, only supports gcc 4.9.  Upgrading the base containers to Centos7 + Devtoolset6 seems the best option.    This should also address currently busted Travis CI runs, and buildability of containers off latest master (sphgeom pybind11 compiler compatibility issue).",1
"DM-13565","02/15/2018 00:42:54","Put correct copyright/license headers in all jointcal files","Jointcal got caught during the RFC-45 copyright/license uncertainty and many of its files don't follow any of the proposals there. We should get them cleaned up, following whatever standard was finalized in that RFC.",1
"DM-13568","02/15/2018 11:54:56","nightly-release d_2018_02_15 -- some eups tarball builds failing with pip install error","https://ci.lsst.codes/blue/organizations/jenkins/release%2Ftarball/detail/tarball/1978/pipeline      {code:java}  + pip install awscli    Collecting awscli      Downloading awscli-1.14.39-py2.py3-none-any.whl (1.2MB)    Collecting rsa<=3.5.0,>=3.1.2 (from awscli)      Using cached rsa-3.4.2-py2.py3-none-any.whl    Collecting PyYAML<=3.12,>=3.10 (from awscli)    Collecting botocore==1.8.43 (from awscli)      Could not find a version that satisfies the requirement botocore==1.8.43 (from awscli) (from versions: 0.4.1, 0.4.2, 0.5.0, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.6.0, 0.7.0, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.9.0, 0.9.1, 0.9.2, 0.10.0, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.15.0, 0.15.1, 0.16.0, 0.17.0, 0.18.0, 0.19.0, 0.20.0, 0.21.0, 0.22.0, 0.23.0, 0.24.0, 0.25.0, 0.26.0, 0.27.0, 0.28.0, 0.29.0, 0.30.0, 0.31.0, 0.32.0, 0.33.0, 0.34.0, 0.35.0, 0.36.0, 0.37.0, 0.38.0, 0.39.0, 0.40.0, 0.41.0, 0.42.0, 0.43.0, 0.44.0, 0.45.0, 0.46.0, 0.47.0, 0.48.0, 0.49.0, 0.50.0, 0.51.0, 0.52.0, 0.53.0, 0.54.0, 0.55.0, 0.56.0, 0.57.0, 0.58.0, 0.59.0, 0.60.0, 0.61.0, 0.62.0, 0.63.0, 0.64.0, 0.65.0, 0.66.0, 0.67.0, 0.68.0, 0.69.0, 0.70.0, 0.71.0, 0.72.0, 0.73.0, 0.74.0, 0.75.0, 0.76.0, 0.77.0, 0.78.0, 0.79.0, 0.80.0, 0.81.0, 0.82.0, 0.83.0, 0.84.0, 0.85.0, 0.86.0, 0.87.0, 0.88.0, 0.89.0, 0.90.0, 0.91.0, 0.92.0, 0.93.0, 0.94.0, 0.95.0, 0.96.0, 0.97.0, 0.98.0, 0.99.0, 0.100.0, 0.101.0, 0.102.0, 0.103.0, 0.104.0, 0.105.0, 0.106.0, 0.107.0, 0.108.0, 0.109.0, 1.0.0a1, 1.0.0a2, 1.0.0a3, 1.0.0b1, 1.0.0b2, 1.0.0b3, 1.0.0rc1, 1.0.0, 1.0.1, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.1.9, 1.1.10, 1.1.11, 1.1.12, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.2.6, 1.2.7, 1.2.8, 1.2.9, 1.2.10, 1.2.11, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.3.8, 1.3.9, 1.3.10, 1.3.11, 1.3.12, 1.3.13, 1.3.14, 1.3.15, 1.3.16, 1.3.17, 1.3.18, 1.3.19, 1.3.20, 1.3.21, 1.3.22, 1.3.23, 1.3.24, 1.3.25, 1.3.26, 1.3.27, 1.3.28, 1.3.29, 1.3.30, 1.4.0, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.4.6, 1.4.7, 1.4.8, 1.4.9, 1.4.10, 1.4.11, 1.4.12, 1.4.13, 1.4.14, 1.4.15, 1.4.16, 1.4.17, 1.4.18, 1.4.19, 1.4.20, 1.4.21, 1.4.22, 1.4.23, 1.4.24, 1.4.25, 1.4.26, 1.4.27, 1.4.28, 1.4.29, 1.4.30, 1.4.31, 1.4.32, 1.4.33, 1.4.34, 1.4.35, 1.4.36, 1.4.37, 1.4.38, 1.4.39, 1.4.40, 1.4.41, 1.4.42, 1.4.43, 1.4.44, 1.4.46, 1.4.47, 1.4.48, 1.4.49, 1.4.50, 1.4.51, 1.4.52, 1.4.53, 1.4.54, 1.4.55, 1.4.56, 1.4.57, 1.4.58, 1.4.59, 1.4.60, 1.4.61, 1.4.62, 1.4.63, 1.4.64, 1.4.65, 1.4.66, 1.4.67, 1.4.68, 1.4.69, 1.4.70, 1.4.71, 1.4.72, 1.4.73, 1.4.74, 1.4.75, 1.4.76, 1.4.77, 1.4.78, 1.4.79, 1.4.80, 1.4.81, 1.4.82, 1.4.83, 1.4.84, 1.4.85, 1.4.86, 1.4.87, 1.4.88, 1.4.89, 1.4.90, 1.4.91, 1.4.92, 1.4.93, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.5.8, 1.5.9, 1.5.10, 1.5.11, 1.5.12, 1.5.13, 1.5.14, 1.5.15, 1.5.16, 1.5.17, 1.5.18, 1.5.19, 1.5.20, 1.5.21, 1.5.22, 1.5.23, 1.5.24, 1.5.25, 1.5.26, 1.5.27, 1.5.28, 1.5.29, 1.5.30, 1.5.31, 1.5.32, 1.5.33, 1.5.34, 1.5.35, 1.5.36, 1.5.37, 1.5.38, 1.5.39, 1.5.40, 1.5.41, 1.5.42, 1.5.43, 1.5.44, 1.5.45, 1.5.46, 1.5.47, 1.5.48, 1.5.49, 1.5.50, 1.5.51, 1.5.52, 1.5.53, 1.5.54, 1.5.55, 1.5.56, 1.5.57, 1.5.58, 1.5.59, 1.5.60, 1.5.61, 1.5.62, 1.5.63, 1.5.64, 1.5.65, 1.5.66, 1.5.67, 1.5.68, 1.5.69, 1.5.70, 1.5.71, 1.5.72, 1.5.73, 1.5.74, 1.5.75, 1.5.76, 1.5.77, 1.5.78, 1.5.79, 1.5.80, 1.5.81, 1.5.82, 1.5.83, 1.5.84, 1.5.85, 1.5.86, 1.5.87, 1.5.88, 1.5.89, 1.5.90, 1.5.91, 1.5.92, 1.5.93, 1.5.94, 1.5.95, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.6.6, 1.6.7, 1.6.8, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 1.7.6, 1.7.7, 1.7.8, 1.7.9, 1.7.10, 1.7.11, 1.7.12, 1.7.13, 1.7.14, 1.7.15, 1.7.16, 1.7.17, 1.7.18, 1.7.19, 1.7.20, 1.7.21, 1.7.22, 1.7.23, 1.7.24, 1.7.25, 1.7.26, 1.7.27, 1.7.28, 1.7.29, 1.7.30, 1.7.31, 1.7.32, 1.7.33, 1.7.34, 1.7.35, 1.7.36, 1.7.37, 1.7.38, 1.7.39, 1.7.40, 1.7.41, 1.7.42, 1.7.43, 1.7.44, 1.7.45, 1.7.46, 1.7.47, 1.7.48, 1.8.0, 1.8.1, 1.8.2, 1.8.3, 1.8.4, 1.8.5, 1.8.6, 1.8.7, 1.8.8, 1.8.9, 1.8.10, 1.8.11, 1.8.12, 1.8.13, 1.8.14, 1.8.15, 1.8.16, 1.8.17, 1.8.18, 1.8.19, 1.8.20, 1.8.21, 1.8.22, 1.8.23, 1.8.24, 1.8.25, 1.8.26, 1.8.27, 1.8.28, 1.8.29, 1.8.30, 1.8.31, 1.8.32, 1.8.33, 1.8.34, 1.8.35, 1.8.36, 1.8.37, 1.8.38, 1.8.39, 1.8.40, 1.8.41, 1.8.42)    No matching distribution found for botocore==1.8.43 (from awscli)    script returned exit code 1  {code}  ",1
"DM-13571","02/15/2018 12:48:45","fix plot_photoCalib bounds","[~jbosch] identified the bug that caused {{plot_photoCalib.py}} to not be able to plot meas_mosaic output. The fix is to change the linspace bounds.",0.5
"DM-13574","02/15/2018 16:43:22","Adding support for building executables of C++ applications","The current build system for the *qserv* package lacks a support for building executables from the C++ applications. A goal behind this ticket is to fix this.",1
"DM-13575","02/15/2018 18:30:09","fix minor bug in photometry ipynb","While looking at HSC data, I noticed that my on-sky dependence plots were not handling the initial dataset correctly. I've fixed that, but it should be committed separately from other open tickets.",0.5
"DM-13583","02/19/2018 08:56:51","Strategize DMTN upload to Docushare","Propose plan for handling DM tech notes in docushare.    There are a number of issues:    * Generating a PDF from rst tech note  * Allocating numbers in docushare.  ",5
"DM-13588","02/19/2018 22:52:50","Document API for current L1DB prototype","Pieces of API that I have now in L1DB prototype could be useful for AP group, need to make sure that it has reasonable documentation.     ",2
"DM-13599","02/20/2018 13:28:57","Update copyright info following RFC-45","Now that RFC-45 is official, update the daf_butler package to be compliant.",0.5
"DM-13600","02/20/2018 14:14:17","Add YAML formatter","Add a YAML formatter to daf_butler.",1
"DM-13608","02/21/2018 09:01:13","newinstall should force LANG=C","[~sogo.mineo] notes that:  {code:java}  the call to `curl` in `n8l::up2date_check` (in `newinstall.sh`) to be prefixed with `env LANG=C` . The message shown by curl when files differ is not always ""differ"" due to i18n.{code}  The easiest way to make sure that the diffs are always legible is to force {{LANG=C}} everywhere.     ",2
"DM-13609","02/21/2018 12:13:52","Undo EXTRACT_PRIVATE override in ip_diffim","{{ip_diffim}}'s Doxygen settings were modified in 2014 to generate HTML documentation for both public and private members. Because of how the build system handles Doxygen overrides, this change had the (presumably unintended) consequence that the Stack-wide documentation also exposed private class members, whether or not they had documentation comments.    Per RFC-451, the override will be removed, so that both {{ip_diffim}}'s and the Stack's documentation includes only API members.",1
"DM-13612","02/21/2018 16:46:52","Upgrading SQLAlchemy from 1.0.8 to 1.2.2","The third-party SQLAlchemy is being used in DAX and Sims.          FYI, release 1.2.2 is a 2+ year newer than 1.0.8, with lots of improvements and new features, as documented in this link:          [http://docs.sqlalchemy.org/en/latest/changelog/migration_12.html]         which would greatly aid the future work related to DB access in DAX services.               ",2
"DM-13623","02/22/2018 12:36:58","Make the Ipac Jenkins send build messages to the IPAC slack","The Ipac Jenkins currently send out email on build failures.  Make it send message to a IPAC slack channel.         The message could contain at least (if possible):   * URL and date of the build/console   * URL of the PR to test if build need to be tested   * Author of the last commit that trigger the build   * Last changes or git log",2
"DM-13630","02/22/2018 18:35:19","Image distance tool fails in particular case where some images searched were not found","When searching images, sometimes there are images that are not found or the area searched is not covered by the dataset chosen. Example: search for m34 on SEIP, see last image (MIPS24 is missing because no image is covering this target).    The result can be a mix of rendered images and failed images. When this happen, the following operations on images failed (maybe others): distance tool offset calculation and flipping around axis.    It happens in Firefly tri-view and IRSA apps (FC/IV).",2
"DM-13632","02/23/2018 11:17:42","Add additional conda packages to the lsst-dev shared stack","On Slack, [~tmorton] writes:    {quote}  {code}  conda install dask distributed datashader  conda install bokeh=0.12.14  conda install -c conda-forge holoviews  conda install -c ioam parambokeh  {code}  Oh, and fastparquet  {quote}    [~hchiang2] says:    {quote}  Is it possible to install fastparquent in the shared stack at {{/software}}? It's used in {{pipe_analysis}}.  {quote}",1
"DM-13635","02/23/2018 12:15:32","Test kafka mirrormaker service","Set up a Kafka MirrorMaker service that is capable of making a copy of all or a subset of data and topics in another Kafka cluster.  This is useful for copying a Kafka service to external downstream brokers and potentially for our mini-broker if we have multiple Kafka clusters.",3
"DM-13637","02/23/2018 15:12:50","WarpCompare: Bad amps eat up temporal budget","Overlapping bad amps eat up the temporalThreshold budget.     For example if a local region has 10 visits, and 3 of those visits are not included because of bad amps, then no candidates are clipped.    CLIPPED mask for {{/datasets/hsc/repo/rerun/private/yusra/RC/DM-13553+DM-134110}}: 9813, HSC-Z   !mask_DM-13553+DM-134110_9813HSC-Z.png|thumbnail!     Seen since DM-12692: Bad amps can be seen in this epochCountImage:    !Screen Shot 2018-01-25 at 12.20.55 PM.png|thumbnail!     If any detections the warpDiffs are entirely covered with the badPixelMask, then don't contribute to the rolling epochCountImage or clip. ",5
"DM-13638","02/23/2018 15:30:07","Allsky visualization in Firefly phase 2","Phase two work for allsky visualization. ",100
"DM-13644","02/26/2018 08:29:32","weekly-release w_2018_08 failed","{{w_2018_08}} failed building the py3 osx ""tarballs"".",0.5
"DM-13649","02/26/2018 10:59:19","deployment  of SUIT using Kubernetes in LSP ","Deploy   SUIT in LSP kubernetes cluster.  ",8
"DM-13653","02/26/2018 12:20:19","Dynamic reconfiguration of the worker services when loading new catalogs","In the present implementation of the catalog loading scripts Qserv worker services need to be restarted each time a database is added or removed to/from a worker node. The restart is made by the *wmgr* service upon a request from the loader script. This scheme causes inconveniences in certain deployments, including Kubernetes-based multi-node integration tests. Hence a goal of this ticket is to replace the hard restart with a request to dynamically reload databases sent from *wmgr* to the worker services. The implementation will be based on a mechanism added as per [DM-13303].    Additional requirements to the proposed change:  * it should not affect the single-node integration test  * it should not require any changes to the catalog loading procedure (script: *qserv-data-loader.py*)",2
"DM-13654","02/26/2018 13:54:52","Set SENSOR_EDGE in coadds","These nice before and after plots shown in DM-13410 were from a the pre-review test run. The version that got merged doesn't set SENSOR_EDGE (and consequently INEXACT_PSF) of the new EDGE pixels.     This has the effect that fewer sources measured on the coadd will have the inexactPsf_flag set. However, most (80-98%) of the EDGE pixels are also BAD or INTRP which are then  ""REJECTED"" which then propagates to INEXACT_PSF:    !DM-13410_vs_08_SENSOR_EDGE.png|thumbnail!    !DM-13410_vs_08_REJECTED.png|thumbnail!    !DM-13410_vs_08_INEXACT.png|thumbnail!",1
"DM-13656","02/27/2018 00:06:18","Implement Developer Guide Topic Reorganization","Implement RFC-453. by reorganizing the topics in the Developer Guide. Redirection pages will also be implemented.    The basic structure proposed in RFC-453 (and given in attached map) is acceptable. There is some concern about the “workflow” topics, and where JIRA and GitHub fit. We’ll have to continue thinking about this as the reorganization shakes out.",5
"DM-13669","02/27/2018 11:00:34","Track and log measurement/reference outliers separately","jointcal currently tracks the total number of outliers per minimization step (""INFO: Total number of outliers 2578""). We should change that to separately track measurement and reference outliers so that we can compare them independently.    With this, we can plot the number of each type of outlier that was removed per fitting step, which can help tell us something about how stable the fit is. (see the final plot in Dominique's {{Astrometry.ipynb}})",2
"DM-13680","02/28/2018 09:19:39","SkyWcs(FrameDict) is not adequately tested","The {{SkyWcs(FrameDict const&)}} constructor is not adequately unit-tested. It is exercised as part of a test, but not explicitly tested. Enhance test_skyWcs.py to explicitly test this constructor.",2
"DM-13686","02/28/2018 15:31:08","Saving a particular FrameSet as FITS-WCS causes a segfault","The attached FrameSet (string representation is in ""skyWcsFrameSet2.txt"") causes a segfault in astshim when writing to an astshim FitsChan as FITS-WCS using the attached code ""example.txt"".    I suspect the bug is in AST and have reported it to David Berry.",1
"DM-13688","03/01/2018 09:28:20","Write up design proposal for Registry/Datastore boundary","This is a retroactive ticket to capture work already done: writing up    [https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73570480]    to guide a discussion on 2018-02-14.",2
"DM-13690","03/01/2018 09:32:58","Write up Gen3 Butler / obs_* package interface sketch","Output will be a confluence page that describes what Gen3 Butler will expect from obs_* packages.  Intended audience is [~krughoff] and the obs_* package working group.         Page is at https://confluence.lsstcorp.org/display/DM/Gen3+Middleware+Camera+Specialization+Interfaces",2
"DM-13693","03/01/2018 10:59:11","Use overload_cast in pybind11 wrappers to simplify wrapping overloaded functions","pybind11's overload_cast can be used to simplify the wrapping of overloaded functions. It requires C+14 so we have not used it in the past. Convert astshim as a test case.",1
"DM-13698","03/02/2018 09:40:24","Update webpack/babel to use env for pollyfills and add webpack-visualizer-plugin","Do following:   * for webpack/babel to use env for pollyfills   * add webpack-visualizer-plugin: Visualize and analyze the Webpack bundle to see which modules are taking up space and which might be duplicates.   * experiment with using babel-plugin-lodash to treeshake lodash (_experiment failed, we can't use it_)   * remove fftools (we not longer us it)     ",3
"DM-13734","03/08/2018 08:25:53","status.lsst.codes ssllabs check broken","The nagios {{check-ssl-qualys.rb}} plugin has been broken for 63 days now.  It appears that the v2 api is gone and v3 has replaced it. Eg.,    https://api.ssllabs.com/api/v3/info  https://github.com/ssllabs/ssllabs-scan/blob/master/ssllabs-api-docs-v3.md",1
"DM-13739","03/08/2018 14:10:13","Document github repo configuration","In RFC-121 we adopted a policy of protecting master. This ticket is to add that documentation to the developer guide and include the settings needed to ensure that the pull requests are up to date with the base branch (which is our policy).",1
"DM-13746","03/09/2018 10:02:23","Modernize use of ndarray in astshim pybind11 wrappers","Update the usage of ndarray in astshim to simplify the pybind11 wrappers and eliminate build warnings.",0.5
"DM-13747","03/09/2018 10:25:50","Fix LSST's ndarray .gitignore to ignore build products","The current .gitignore in our tarball release of ndarray is wildly inaccurate. Clean it up.",0.5
"DM-13749","03/09/2018 11:23:18","Include error handling for antlr4 parser","Error handling for the new antlr4 parser – specifically for conditions where tree-navigation does not have an error but the query is not sufficiently populated.         Hook up a disable able (ifdef?) code that allows the antlr4 caught exception's message to be returned to the user.",13
"DM-13750","03/09/2018 12:17:54","Move Record printing to C++","Current implementation is apparently super slow, and looking at what it's doing (constructing a new dictionary of all fields via {{extract}} just to get one element from it, for every field), I can see why.    That could be cleaned up in Python but a lot of it is more naturally and definitely more efficient in C+++,+ and doing that will give us a C++ {{operator<<}}, too, which is nice.",0.5
"DM-13753","03/09/2018 13:18:30","Enable sphinx doc building in daf_butler","Turn on building of sphinx documentation. We already use numpydoc for docstrings but some may need tidying as a result of the documentation actually being built.",2
"DM-13755","03/09/2018 15:13:11","Make pex_config generate reStructuredText-compatible docstrings","We discovered that {{pexConfig}} objects create {{__doc__}} (docstring) attributes automatically based on information passed to configuration constructors. This means that configuration fields are documented in the attributes of configuration classes inside the regular API documentation.    This is great, but can also fail if the automatically-generated config field docstring is not reStructuredText/Numpydoc compatible.    This ticket is to change the {{__doc__}} generators of {{pexConfig}} fields so that they output Numpydoc/reStructuredText docstrings.",1
"DM-13756","03/09/2018 15:25:11","Add descriptions to datasets","We have no descriptions for the existing butler datasets/exposure types, so nobody knows what they are.",1
"DM-13757","03/09/2018 15:44:00","enable jointcal config writing","There is a block of code in jointcal that disables the persisting of the config and metadata. We should remove that code and add a dataset description for jointcal configs.    This is the block in question:  {code:java}  # We don't need to persist config and metadata at this stage.  # In this way, we don't need to put a specific entry in the camera mapper policy file  def _getConfigName(self):  return None    def _getMetadataName(self):  return None  {code}   ",2
"DM-13760","03/11/2018 19:22:24","pipe_analysis needs updates for the wcs dataset name changes ","Running the visit-level scripts using the current master of {{pipe_analysis}} ({{6b5727d}})  with the {{w_2018_10}} stack gave errors in getting the {{wcs}} dataset.       {noformat}  Traceback (most recent call last):    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/14.0-6-ge2c9487+51/python/lsst/pipe/base/cmdLineTask.py"", line 408, in __call__      result = task.run(dataRef, **kwargs)    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/visitAnalysis.py"", line 215, in run      raise RuntimeError(""No datasets found for datasetType = {:s}"".format(repoInfo.dataset))  RuntimeError: No datasets found for datasetType = wcs  {noformat}      It needs updates for the dataset name changes of DM-11138.   ",1
"DM-13762","03/12/2018 08:18:18","Write up description of DRP current and future test datasets","As per this request from [~zivezic]:  {quote}We need to understand how much HSC data is processed   regularly, is that enough/too much, etc., and where to go from where we  are now. Please can you send half a page to a page summary of your   DRP-driven recommendations within a few days?{quote}   ",0.5
"DM-13768","03/12/2018 11:29:18","Fix firefly_client uploads to work with server on https","The Jellybean environment includes a Firefly server with an https url. Uploads are unsuccessful with the current firefly_client.        * Fix the upload functions to use the base url in the client   * Tag as a bug fix release   * Deploy to PyPI for pip installation   * Sync to the firefly_client repo on the LSST Github org   * In the {{lsst.display.firefly}} backend, remove the {{port}} argument   * Change docs and examples for {{lsst.display.firefly}} to remove the {{port}} argument   * Substantially shorten the Installation portion of the docs, now that {{lsst.display.firefly}} is included in {{lsst_distrib}}.",2
"DM-13779","03/12/2018 18:53:17","the indication box for marker on image separates from marker as the size increases","* Add a marker on an image, it is annotated as marker #1   * click on the marker, position the cursor at the corner of the indication box to increase the marker size   * The distance between the box and the marker increases as the marker size increases   * the annotation ends up pretty far from the marker, not very useful any more, especially if there are more than one markers.    * See the attachment, ""Marker #3"" label is closer to the marker circle #2 than to #3.    Suggestion:    Keep the distance between the marker and the indication box at a fixed number, e.g. 3-5 pixels.    Trey added: This issue stands out even more on HiPS displays.",1
"DM-13786","03/13/2018 09:59:13","'FireflyClient' object has no attribute 'headers'","I'm using commit {{bdd25c9}} on the {{DM-13768-https-upload}} branch.    I occasionally get the following exception:    !image-2018-03-13-09-55-24-833.png!    This is with the following code:  {code:java}  import lsst.afw.display as afw_display  afw_display.setDefaultBackend('firefly')  my_channel = 'my_test_channel'  my_server = 'https://sysengdemo.lsst.codes'  display = afw_display.getDisplay(frame=1,                                    host=my_server, port=443,                                   name=my_channel){code}  It's not clear to me how to reproduce this.  It seems like it is more likely to happen when I've left for a while and come back.",0.5
"DM-13790","03/13/2018 15:19:34","Remove all use of the geom package","Our stack has a very few bits of code that still use the {{geom}} package. In particular they use the {{convexHull}} function. {{geom}} has been superseded by {{sphgeom}} (which is written in C++ instead of python) and contains {{ConvexPolygon.convexHull}}. Update the code, paying careful attention to API differences, if any.    This is driven by a desire to repurpose the {{geom}} package for afw geometry primitives (RFC-460) and to retire obsolete and unmaintained code.",3
"DM-13796","03/14/2018 09:21:02","Fix minor issues in cat schema files","Working on DM-13781 I tried to convert baseline schem to YAML and noticed some minor issues in that schema file:  * in some comments it uses {{</desc>}} instead of {{</descr>}}  * DiaSource index has a name IDX_DiaObject_htmId20   * DiaSource index IDX_DiaSource_filterName refers to non-existing field name    Also I need to verify that type of {{flags}} field in DiaForcedSource is correct (TINYINT)",1
"DM-13797","03/14/2018 10:19:13","Remove the HiPS Grid icon, Always add the HiPS grid layer with checkbox off","Remove the HiPS grid icon from the toolbar.  Then when a HiPS is added always add the HiPS grid layer with the checkbox turned off so that it is hidden by default.  This way a user has access to it without it being on the toolbar.    Add support for calling the IRSA hipslist.    this involves:     - app.config & app.prop     - uniq creator_did     - if title is blank use creator_did, last parts     - irsa.hips.method and irsa.hips.url         See also:    [https://caltech-ipac.atlassian.net/browse/IRSA-1556#add-comment]",3
"DM-13799","03/14/2018 10:34:01","Make l1dbprototype work with sqlite","For simple tests it's useful to run prototype against local sqlite file. There is some code in prototype that cares about engine type, need to extend it to understand sqlite.",2
"DM-13807","03/14/2018 17:25:55","Plan API for Shape Algorithm Objects","Plan out how best to implement the c++ classes which correspond to equations in [DM-13406|https://jira.lsstcorp.org/browse/DM-13406]",2
"DM-13809","03/14/2018 17:35:33","Test the implemented Shape optimization algorithm","Test the shape optimization algorithm by:  * Running the unit tests built during the coding of the algorithm  * Optimizing against a known elliptical Gaussian of differing S/N",5
"DM-13821","03/15/2018 12:07:04","Coordinate sims builds with DM weekly releases","The sims team would like to be building their stack on top of a weekly release.  Meet with the sims team (Scott Daniel) and iron out the actual needs.",2
"DM-13822","03/15/2018 12:15:42","Remove python_mysqlclient dependency from obs_lsstSim and obs_sdss","The dependency on {{python_mysqlclient}} in {{obs_lsstSim}} is cargo culted from {{obs_sdss}} and should be removed.",1
"DM-13823","03/15/2018 13:12:10","Remove lsst.utils.multithreading","Implementation ticket for RFC-461.    Remove {{python/lsst/utils/multithreading/}} and the associated unittests, {{test_sharedData.py}} and {{test_lockProtection.py}}. ",1
"DM-13827","03/15/2018 15:07:11","ScienceSourceSelectorTask is slowly appending to a table when it can simply do the selection","ScienceSourceSelectorTask is slowly appending to a table when it can simply do the selection.  The loop at https://github.com/lsst/meas_algorithms/blob/f7dca96402cda034104c615be7ef821ab3aa9ea9/python/lsst/meas/algorithms/sourceSelector.py#L451 is unnecessary.",0.5
"DM-13833","03/16/2018 14:54:00","Update LDM-503 to use lsst-dm/milestones","The [lsst-dm/milestones|https://github.com/lsst-dm/milestones] system was created to enable automatic generation of LDM-564 from a single PMCS export. Apply it to LDM-503 as well.",1
"DM-13834","03/16/2018 16:23:11","Add minimum and maximum lambda to filter properties","The DCR correction code needs to know the range of wavelengths permitted through a filter, not just the effective wavelength of the full band. I will add additional properties to `Filter` and define those properties for `obs_lsstSim` and `obs_decam`. Other packages should continue working as normal without being modified.",3
"DM-13835","03/16/2018 16:51:25","Cannot ingest empty data","The {{_doIngest}} and {{_flatBiasIngest}} methods cannot handle empty lists of files to ingest, if such a file is passed in, the program crashes. This is clearly undesirable behavior; it would make more sense for these functions to silently do nothing.    Possibly obsolete after DM-13530, since the bug appears to be specific to the placeholder implementation.",2
"DM-13855","03/20/2018 14:21:07","Make proposal for summary information on HiPS maps to list in selection table","Determine which fields from the HiPS {{properties}} files should be reflected in the HiPS-selection table in Firefly.",1
"DM-13861","03/21/2018 09:54:02","Make L1db spatial indexing more flexible.","Current L1DB implementation assumes HTM-20 indexing for DiaObject/DiaSource tables. There is code in L1db class that depends on that when doing region-based selection. It would be better to make this more flexible and client-controlled so that other indexing models could be tested/employed.",1
"DM-13864","03/21/2018 11:00:13","Submit LSP Test Specification document LDM-540 to DM-CCB for approval","Apply final polish and submit.",2
"DM-13865","03/21/2018 11:32:41","Configure L1DB implementation from pex.config","Presently L1db configures itself from INI-style file using Python \{\{ConfigParser}}. For better integration with pipeline tools {{pex.config}} should be used instead.",1
"DM-13870","03/22/2018 09:51:38","Code clean up - remove xml xstream and old serialization ","Code clean up - remove xml xstream and old serialization     Remove old, unused code.   * At one point during the the conversion we had 3 serialization techniques.    * We no longer use the XML based xstream code.",2
"DM-13872","03/22/2018 11:27:07","Title of layer-selection dialog does not change with change of image","When the (non-modal) layer-control dialog is visible in Firefly and the selected image is changed ""under"" that dialog, by clicking in a different image, the _contents_ of the dialog seem to change appropriately, but the image title displayed in the title bar of the dialog is not updated.    Observed on {{http://irsadev.ipac.caltech.edu/irsaviewer/}} just now.",2
"DM-13875","03/22/2018 12:22:11","Assist with L1db integration into AP pipeline","l1dbproto should be more or less usable now for actual AP prototyping work. Integration issues related to that will be covered in this ticket.",2
"DM-13878","03/22/2018 13:51:50","Confirm that ap_verify's documentation is sphinx-buildable","{{ap_verify}} added Sphinx documentation before the framework was fully finalized, and used a workaround script to test builds. Its documentation should be reviewed to confirm that it can be built as described in the [documentation hack day instructions|https://community.lsst.org/t/pipelines-package-docs-hack-day-instructions/2742], and any necessary changes made. In particular, {{doc/single-package.sh}} should be removed as obsolete.",1
"DM-13879","03/22/2018 14:50:22","Include meas_mosaic in lsst_distrib","Implement RFC-462.    Also update meas_mosaic to optionally depend on obs_subaru.",1
"DM-13886","03/23/2018 13:50:25","Simplify Transform to contain a Mapping instead of a FrameSet","At present {{afw::geom::Transform<FromEndpoint, ToEndpoint>}} contains an {{ast::FrameSet}}, although all it does is transform points, which is what {{ast::Mapping}} is for. As DM-13847 demonstrated, using a FrameSet slows down Transform. The workaround in that ticket was to hold onto both a FrameSet and a Mapping. This ticket is to intended implement a more logical but slightly more invasive solution.    I will file an RFC with more explanation.",2
"DM-13897","03/26/2018 11:52:58","Write up development workflow for jellybean","Document the notional development workflow in the JupyterLab environment.",3
"DM-13904","03/27/2018 10:10:34","Specify Eigen directory in ndarray build","An oversight our build script for ndarray can cause it to prefer a non-EUPS Eigen installed in a system include directory over a EUPS-provided Eigen.",0.5
"DM-13907","03/27/2018 16:45:04","sandbox-stackbuild provisioning fails on DO","The DO plugin is able to create droplets but the provisioning fails completely in the same way it used to with the aws plugin.  I suspect this is due to changes in the vagrant core over the last 8 months or so.    {code:java}  $ vagrant up el7  Bringing machine 'el7' up with 'digital_ocean' provider...  ==> el7: Using existing SSH key: sqre  ==> el7: Creating a new droplet...  ==> el7: Assigned IP address: 104.131.24.185  ==> el7: Modifying sudoers file to remove tty requirement...  ==> el7: Creating user account: vagrant...  No host IP was given to the Vagrant core NFS helper. This is  an internal error that should be reported as a bug.{code}    There is an upstream issue but this will be easier as a configuration fix.    https://github.com/devopsgroup-io/vagrant-digitalocean/issues/271",1
"DM-13919","03/28/2018 17:54:02","Add arcminute-scale range to HiPS image FoV display","In the current irsadev version of the HiPS viewer, it appears that the FOV display is either in degrees or in arcseconds, represented by the '°' and '""' symbols, respectively.  It does not have an intermediate display of arcminutes.  Before we go on to apply this to all image viewers, not just HiPS, we should agree on what we want.    Personally I would go for adding the intermediate layer.  I find it a bit odd to see "" 2711"" "" as a field of view, instead of "" 45' "".  Also in other Firefly and IRSA application contexts, such as the _selection_ of a field of view, the arcminute option is presented.    The same logic should apply consistently throughout the Firefly world, I think.    Specifically: when the value is between 1 and 59 arcmin, display it as arcmin (with the ' symbol) with two digits of precision, i.e, as 1.0' through 9.9' or 10' through 59'. But if there is existing practice for something slightly different elsewhere in Firefly, it would be reasonable to re-use existing behavior.    Derives from IRSA-1606 and IRSA-1628.    Aimed at the May 2018 release.",2
"DM-13945","03/29/2018 19:03:44","newinstall.sh could do a better job of reporting missing commands","Notably, {{newintall.sh}} will without informative messages to the end user if certain commands are not available in the path. Eg.,    * the miniconda installer internal uses {{bzip2}}  * the eups build needs {{make}} and a c compiler",0.5
"DM-13950","03/30/2018 12:14:00","Convert assembleCoadd.py to numpydoc","Finish porting pipe_tasks docs from doxygen to numpydocs following Jonathan Sick's hack day instructions.",2
"DM-13961","03/30/2018 18:33:22","Run KPM30 tests at IN2P3","Run KPM30 tests on LSST30 data on the lower half of IN2P3.",13
"DM-13969","04/02/2018 15:53:41","Resolve implied dependency of cp_pipe on eotest","It turns out that {{cp_pipe}} has an implicit dependence on {{eotest}}.  Please resolve the dependence by including it in the table file.  This may also involve moving it into either {{lsst-dm}} or {{lsst}}.",0.5
"DM-13976","04/02/2018 17:43:07","Rename jointcal.gtransfo","jointcal's {{Gtransfo}} object (a mutable 2->2 transform designed for astrometry and optimized for being fit) should have a name that better matches our standards. I created {{PhotometryTransform}}, so I suppose an appropriate new name might be {{AstrometryTransform}}?",2
"DM-13979","04/03/2018 07:04:44","Remove dependency on qserv-run-dir","qserv-run-dir and qserv-configure will no more be used in k8s setup.",8
"DM-13989","04/03/2018 16:59:45","fixed hips cube issues and honor some HiPS properties","_Do the following:_   # Cubes: fixed the bugs related to moving between HiPS cubes and regular hips   # Cubes: display the correct current plane number and the total plane number  when switching between HiPS cube   # Cubes: honor the data_cube_* properties and show information about the 3rd dimension   ** Use: {{value = crval3 + (cubeIdx - crpix3 ) * cdelt3}}   # Make field of view and target optional and then, if they are not specified by the user, default to the HiPS properties {{hips_initial_ra}}, {{hips_initial_dec}}, and {{hips_initial_fov}}.  If neither are available show 180 degrees at 0,0.",2
"DM-13997","04/04/2018 13:16:57","Enable numpydoc support for ip_isr","This ticket is to enable the numpydoc conversion of ip_isr.",0.5
"DM-13998","04/04/2018 14:04:22","Enable automatic flake8 testing in skymap","Enable flake8 testing in skymap",0.5
"DM-14001","04/04/2018 15:01:06","Make afw PEP8 compliant and enable auto testing","Fix PEP8 warnings and enable automatic testing by scons and by Travis on github.    I also fixed the Doxygen warnings, as the warnings about duplicate section names were showing up when building other packages, which was very annoying. One change was to stop Doxygen from scanning any C++ source files in {{src/...}}. I moved the Doxygen documentation from files to the headers long ago, and scanning the source was a frequent cause of spurious Doxygen warnings.",1
"DM-14004","04/04/2018 16:46:16","Make astshim compliant with the -pedantic compiler flag","astshim has a few C++ violations that are exposed using the {{-pedantic}} flag with the C++ compiler. Fix them.",0.5
"DM-14007","04/04/2018 21:34:07","Fix py2 linter error in skymap","Fix the following python 2 error:  {code}  /home/jenkins-slave/workspace/stack-os-matrix/centos-7.py2/lsstsw/build/skymap/examples/plotSkyMap.py:189:69: F812 list comprehension redefines 'x' from line 180 /home/jenkins-slave/workspace/stack-os-matrix/centos-7.py2/lsstsw/build/skymap/examples/plotSkyMap.py:189:72: F812 list comprehension redefines 'y' from line 180  {code}",0
"DM-14008","04/05/2018 09:40:04","Enable TransmissionCurve coaddition for HSC","This should be disabled for most cameras, but we should definitely be running it for HSC.",0.5
"DM-14012","04/05/2018 12:35:44","Add helper container class supporting topologically sorted iteration over elements","Add a {{ConnectedSet}} helper (name optional) that allows for insertion of elements and connections and iteration over them in topologically sorted order.",1
"DM-14014","04/05/2018 13:02:47","Remove ""Keyword assignment operators SHOULD be surrounded by a space..."" from the Python style guide","Implement RFC-471 by removing  this [rule|https://developer.lsst.io/python/style.html#keyword-assignment-operators-should-be-surrounded-by-a-space-when-statements-appear-on-multiple-lines] from the Python style guide:    Keyword assignment operators SHOULD be surrounded by a space when statements appear on multiple lines    we are using PEP8 instead and flake8 will enforce it    Also remove E251 from any list that suggests it as a rule to ignore",0.5
"DM-14019","04/05/2018 16:47:15","Allow ap_pipe to skip association","Once {{ap_association}} supports it, {{ap_pipe}} should query the association database to see if a dataId has already been processed. This will allow {{ap_pipe}} to skip all processing steps that have already been performed.",1
"DM-14024","04/06/2018 09:01:46","Study current cluster setup scripts","Study the current OpenStack cluster setup scripts to plan refactoring   * Use shade instead of nova,cinder and other low-level openstack python clients   * Remove deprecated bare metal commands",2
"DM-14029","04/06/2018 10:44:12","Improve table parsing and memory usage","Refactor DataGroup   * DataGroup was written a long time ago as a table model to represent and manipulate tabular data.  Over the years is has gotten bulky and complicated.  Refactor it with emphasis on memory footprint and larger data set.    Table parsing   * Look into ways to improve table parsing, especially IPAC tables.",5
"DM-14046","04/09/2018 13:12:48","Cache HiPS list and properties search result","In order to optimize the search of HiPS list and properties, create a method to cache the search result. The search result will be re-used repeatedly for any coming query and only be updated when the server have made any modification.           ",1
"DM-14064","04/11/2018 09:20:56","lsst_dm_stack_demo failing with: ./bin/demo.sh: No such file or directory","There are multiple reports on #dm-square this morning of the stack demo failing on same {{stack-os-matrix}} configurations with errors similar to:    {code:java}  /home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/ci-scripts/runManifestDemo.sh: line 210: ./bin/demo.sh: No such file or directory  {code}    The clean build of of {{lsst_distrib}} failed all configurations but had been passing for several days prior:    https://ci.lsst.codes/blue/organizations/jenkins/science-pipelines%2Flsst_distrib/detail/lsst_distrib/211/pipeline    There have been no changes merged to either https://github.com/lsst/lsst_dm_stack_demo or https://github.com/lsst-sqre/ci-scripts/blob/master/runManifestDemo.sh in the last couple of days.",1
"DM-14069","04/11/2018 13:11:59","Update igprof on LSST (and Princeton) systems","Per DM-14060, please install the new version of igprof on:    - All four lsst-dev stacks;  - Both Princeton Tiger stacks;  - Both Princeton Perseus stacks.",1
"DM-14072","04/11/2018 17:15:32","Add getCutout method to Exposure","There are many instances when it would be helpful to have a simple way to return a postage stamp, or small cutout, of an exposure with some user-specified center and dimensions. This ticket is to create such a method, ideally in C++.    A preliminary python implementation lives in {{u/mrawls/exposure-cutout}}.",1
"DM-14074","04/11/2018 20:00:37","Check Firefly access to the amiga.iaa.es HiPS maps","Determine, on {{irsadev}} (which is why I can't do this now, because I'm offsite), whether the current Firefly HiPS code displays the http://amiga.iaa.es/hipslist maps in its index, and (whether or not they are in the index) whether it can display them.",1
"DM-14075","04/11/2018 20:32:39","Exclude bad mask plane in nImage for filtering artifact candidates","Symptom: Artifact candidates are being clipped when there not sufficient epochs remaining to do so.     Cause:  A crude N-Image is passed to filterArtifacts to compute the maximum number of epochs a candidate can appear in and be clipped. If there are < 3 epochs it's zero. (i.e nothing should be clipped). This N-Image is too crude.    The actual N-Image can be lower due to recorded chip defects (everything in the bad mask plane).    NOTE: This only affects coadds with small numbers of epochs",1
"DM-14079","04/12/2018 09:25:03","Add file logging to albuquery, aka dax db/metserv v1","Need to add file logging to persist logs for debugging and tracking, as the default is going to console.",3
"DM-14082","04/12/2018 15:06:20","Update alert_stream for sims data","Current alert_stream repo uses only a single template alert.  We want to change this to be able to send and receive sims data, for use in filtering and for the end-to-end prototype.",0.5
"DM-14103","04/13/2018 16:02:19","Cleanup code related to the coordinate grid that slow down the application","i was testing and trying the feature ‘match image to hips’ so i had 1 image and 1 hips map displayed. When i tried to overlay a grid on FITS image, the browser slow down and sometimes i have to reload the page. It looks to me that there is still a link between applying grid to hips that is not didabled once i removed but button form the toolbar for hips.",2
"DM-14115","04/16/2018 12:58:07","ap_verify's metrics-file command-line argument not documented","Following DM-13042, {{ap_verify}} takes the {{\-\-metrics-file}} command-line argument to override metrics output behavior. However, the ticket neglected to add documentation for this argument. Add a section to {{ap_verify}}'s command-line argument reference describing {{\-\-metrics-file}} and its intended use.",1
"DM-14116","04/16/2018 14:58:12","Learn how to rerun command-line drivers on lsst-dev","Yusra showed me how to source the latest weekly build of the pipeline and running command-line tasks on HSC data on lsst-dev, e.g.:    source scl_source enable devtoolset-6; source /software/lsstsw/stack3/loadLSST.bash    setup lsst_distrib -t w_2018_14 # For that week    mkdir /project/dtaranu/cmodelconfigs/    multiBandDriver.py /datasets/hsc/repo --calib /datasets/hsc/repo/CALIB/ --rerun RC/w_2018_14/DM-13890:private/dtaranu/cmodelconfigs/w_2018_14 --id tract=9813 patch=3,4 filter=HSC-G^HSC-R^HSC-I^HSC-Z^HSC-Y^NB0921",2
"DM-14118","04/16/2018 16:06:55","Create Jupyter notebook summarizing cModel config results","This notebook is to summarize the effects of changing the number of Gaussians in the initial fitting in the cModel task.    This includes running through the Jupyter workflow on lsst-dev, using the data butler, remembering how to process/plot data with Python and forgetting about R.    The notebook is currently at lsst-dev:/home/dtaranu/src/mine/cModelConfigs.ipynb and I will likely add it to a sensibly-named repository on lsst-dm.",2
"DM-14125","04/17/2018 15:06:22","Remove pre-multitrace chart code and server-side expressions","Remove chart code supporting unused pre-multitrace architecture and server-side expressions.  Update the documentation for showChart parameters.",5
"DM-14128","04/18/2018 01:22:03","Fix Qserv container builds","Recent change to admin/tools/docker/1_build-image.sh seems to have broken automated Qserv container builds",0.5
"DM-14129","04/18/2018 05:56:48","Fix AuxDevice multiple forwarder issue discovered at April 4th Pathfinder","This recent Pathfinder activity was the first activity where the AuxDevice was incorporated into the DM L1 code. The AuxDevice specifically handles tasks on the Auxiliary telescope. During the activity, it was discovered that the component was not handling forwarder fully qualified names properly. The fix during the exercise was to configure the device to use only one forwarder with no spare. This fix allows an arbitrary number to be used - which is how all of the commandable devices behave.",2
"DM-14133","04/18/2018 13:18:46","Enable Sphinx support for meas_astrom","Add Sphinx support to meas_astrom package.",5
"DM-14134","04/18/2018 13:31:26","Enable Sphinx support for ip_diffim","Enables Sphinx support by uncommenting `automodapi` and fixing the errors and warnings.",1
"DM-14135","04/18/2018 13:33:44","Convert afw.geom to numpydoc","Convert {{lsst.afw.geom}}'s Python to Numpydoc, following the guidelines on [community|https://community.lsst.org/t/2760].",1
"DM-14152","04/19/2018 15:29:59","Investigate the spatial distribution of failed cModel fits","A significant fraction of sources in HSC-R tract 9813 patch 3,4 have failed initial cModel fits (2.3% nan flux/objective, 10.2% negative flux), while 5.1% of dev/exp fits have nan flux/objective. After learning how to use the butler to display in ds9, I found that these failures come from a combination of sources near the edge of a patch, in/near the halos/diffraction spikes of bright stars, or from difficult/failed deblends, in roughly that order. I did not find any surprising failures - all of the failures also set a seemingly sensible failure flag.    The attached plot can be generated by the notebook created in DM-14118 (lsst-dev:/home/dtaranu/src/mine/taranu_lsst/cModelConfigs.ipynb).",1
"DM-14153","04/19/2018 15:32:02","Add warn messages for too few meas/ref sources per ccd","After every outer outlier rejection loop, we should check the number of reference and measured sources per ccd, and issue a warning if they get too small (where ""small"" is defined by a config parameter).",2
"DM-14154","04/19/2018 16:34:54","Setting up a Qserv and Replication cluster at IN2P3","Set up a testing environment combining Qserv + the Replication system in the upper part of the IN2P3 cluster. This included the following nodes:  * *ccqserv125*: Qserv _czar_ and the Replication System's _controller_  * *ccqserv126* - *ccqserv149*: Qserv _worker_ servers and the Replication System's _worker agents_    Configure Qserv to use a separate set of ports and to use a separate data directories to avoid direct conflicts with the main stream (other) Qserv installation.    Preload the cluster with 1/6th of the following catalogs which are (as of now) being deployed at the NCSA's PDAC Qserv Cluster:  * *sdss_stripe82_01*  * *wise_00*    The data will be copied from the first 5 _worker_ nodes of that (source) cluster:  * *lsst-qserv-db01* - *lsst-qserv-db05*    The total amount of data to be transgferred from those 5 nodes is: *3.7 TB*    Extra actions to be taken in a context of this ticket will also include:  * building and deploying a specially configured version of the Qserv Docker container (to allow separate ports)  * packaging tools of the Replication systems into a separate container to be deployed at teh new cluster  * develop simple set of the management tools for starting/stopping the Replication System's services    Port number modifications in container image *qserv/qserv:tickets_DM-10424*:  {code:bash}  % cat admin/templates/installation/qserv-meta.conf    # Port number for worker management service  port = 25012    # Port number for cmsd server (not used in mono setup)  cmsd_manager_port = 22131    # Port number for xrootd server  xrootd_port = 21094    # Port number for mysql-proxy, this is the primary interface for qserv clients  port = 24040    # Port number for mysql server  port = 23306  {code}    Data directories for the Qserv instance:  {code}  /qserv/replication/  /qserv/replication/data/  /qserv/replication/log/  /qserv/replication/tmp/  {code}",8
"DM-14155","04/19/2018 17:30:37","Experiment with other source selectors for photometry","The photometry failures may be related to not having""appropriate"" reference stars. I should try a few other source selectors, starting with the {{flaggedStarSelector}} (which uses bright PSF sources), to see if they are a better match than the astrometrySourceSelector.    I'll want to be able to count measuredStars (i.e. DM-14153) in order to tell whether the new source selector is picking enough sources to be useful.",2
"DM-14159","04/20/2018 08:10:50","Wrap matplotlib use in meas_mosaic","{{meas.mosaic.utils}} does a module-level import of matplotlib and sets the backend to {{Agg}}.    a) Specifically the [{{matplotlib.use(""Agg"")}}|https://github.com/lsst/meas_mosaic/blob/master/python/lsst/meas/mosaic/utils.py#L30] line should be removed.    It has no effect if the user has already loaded a backend, and spews a big chunk of distracting warnings.    b) The overall import of matplotlib should not happen at the module level, because that code gets run when importing the module.  Importing the module happens even just opening a Butler object view to the repo.  Specifically, meas_mosaic gets imported by just the following code.    {code}  import os  import lsst.afw.display as afwDisplay  from lsst.daf.persistence import Butler    repo = os.getenv('REPO')  butler = Butler(repo)  {code}    Philosophically I don't believe that opening a Butler should trigger setting up matplotlib.    Editorial: I believe that all plotting imports should be protected and separated from the data analysis.  But this implies a refactoring that may not be justified given the imminent replacement by jointcal",0.5
"DM-14162","04/20/2018 09:45:47","Iterative inverse of radial transform not acceptably accurate","A radial transform made with {{afw.geom.makeRadialTransform}} has a very inaccurate iterative inverse in some cases. Here is an example:    {code}  import numpy as np  import lsst.afw.geom as afwGeom  plateScaleRad = 9.69627362219072e-05  radialCoeff = np.array([0.0, 1.0, 0.0, 0.925]) / plateScaleRad  fieldAngleToFocalPlane = afwGeom.makeRadialTransform(radialCoeff)  print(fieldAngleToFocalPlane.applyForward(fieldAngleToFocalPlane.applyInverse(afwGeom.Point2D(0, 850))))  # prints Point2D(0, 850.2)  {code}  The error is most likely in AST's {{PolyMap}}. Note: when inverse coefficients are not provided (as in this case) {{afw.geom.makeRadialTransform}} constructs a {{PolyMap}} using options {{""IterInverse=1, TolInverse=1e-8, NIterInverse=20""}}.    When this is fixed, {{cbp}} test {{testSetFocalPlanePos}} in {{test_coordinateConverter.py}} can be simplified.",1
"DM-14168","04/20/2018 13:02:20","Compare running times of cModel fits to individual objects with different parameters","I examined the distribution of cModel running times for HSC-R 9813 3,4. Two things are apparent - with default settings, some faint sources (mag_psf > 26) take a long time to fit (>1 min); also, changing parameters like the number of initial components or the fit algorithm can exacerbate this problem.    The attached plots can be generated by the notebook created in DM-14118 (lsst-dev:/home/dtaranu/src/mine/taranu_lsst/cModelConfigs.ipynb). I have also put it up on [https://github.com/lsst-dm/modelling_research] (see test_lsst_cmodel.py and jupyternotebooks/lsst_cmodel_configs.py/.ipynb), which supersedes the local copy on lsst-dev.",2
"DM-14170","04/20/2018 15:59:24","Add descriptions for dcr datasets","There are a number of dcr-related datasets in obs_base's {{policy/datasets.yaml}} and {{policy/exposures.yaml}} that need descriptions. See the descriptions that were added in DM-13756 for examples: a sentence or two describing what the dataset is and what might read/write it.",0.5
"DM-14171","04/20/2018 16:00:36","Add descriptions for fgcm and transmission datasets","There are a number of fgcm and transmission curve-related datasets in obs_base's {{policy/datasets.yaml}} and {{policy/exposures.yaml}} that need descriptions. See the descriptions that were added in DM-13756 for examples: a sentence or two describing what the dataset is and what might read/write it.",0.5
"DM-14175","04/22/2018 09:32:20","lsst_ci failing","The weekly, nightly, and {{lsst_distrib}} clean build are failing due to the same error message from {{lsst_ci}}.    {code:java}               lsst_ci: 15.0+15 .......................................................................................................................................................................:::::  [2018-04-22T06:02:20.924442Z] ValueError: Instrument name and input dataset URL must be set in config file  :::::  [2018-04-22T06:02:20.924450Z] ===================== 1 failed, 2 passed in 327.27 seconds =====================  :::::  [2018-04-22T06:02:20.955850Z] Global pytest run: failed  :::::  [2018-04-22T06:02:20.976613Z] Failed test output:  :::::  [2018-04-22T06:02:20.979598Z] Global pytest output is in /home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/lsst_ci/tests/.tests/pytest-lsst_ci.xml.failed  :::::  [2018-04-22T06:02:20.979622Z] The following tests failed:  :::::  [2018-04-22T06:02:20.982067Z] /home/jenkins-slave/workspace/science-pipelines/lsst_distrib/centos-6.py3/lsstsw/build/lsst_ci/tests/.tests/pytest-lsst_ci.xml.failed  :::::  [2018-04-22T06:02:20.982180Z] 1 tests failed  :::::  [2018-04-22T06:02:20.982695Z] scons: *** [checkTestStatus] Error 1  :::::  [2018-04-22T06:02:20.983341Z] scons: building terminated because of errors.  ERROR (349 sec).  {code}  ",2
"DM-14197","04/24/2018 15:11:50","Make obs_test data ingestible","The data in {{obs_test/data/input}} are provided in the form of a Butler v1 repository, which is sufficient for most testing purposes. However, the files cannot be used to test ingestion code: the files have minimal (and generic) FITS headers, and cannot be ingested using default configurations for {{IngestTask}} and {{IngestCalibsTask}}.    Known issues:  * The flats and biases do not have an {{OBSTYPE}} header keyword, so they cannot be ingested without {{\-\-calibType}} (or with it, given DM-13975).  * The default config for {{IngestCalibsTask}} does not list ""defect"" as one of the data types to register  * Calibration files (or just defects?) cannot use the default columns in {{register.unique}}",2
"DM-14208","04/25/2018 10:30:38","Update AuxDevice unit test","Addition of Fault system added some messages that must be verified in the unit test.",2
"DM-14210","04/25/2018 10:42:20","Fine tune AuxDev ACK consumption and make more efficient.","Implement a version of the progressive ACK timer that checks strictly for the one ATS forwarder response; these must be blocking ACKs, so proper ACKs should be identified quickly if received.",5
"DM-14211","04/25/2018 10:45:14","ATS report telemetry design and implementation","Build means to publish reports as telemetry. Reports may be work done, system warnings, etc.",8
"DM-14212","04/25/2018 10:48:20","recompile and re-link DAQ code with new library version.","The utility apps (such as catalog listing, image triggering, etc.) must be rebuilt as well as the DAQ Forwarder code.",1
"DM-14213","04/25/2018 10:49:51","Discuss ATS system release plan with DM release engineer","SSIA.",1
"DM-14217","04/25/2018 15:48:02","SHOW PROCESSLIST is broken in Qserv czar","The master branch of package qserv seems to have a bug in an implementation of the 'SHOW PROCESSLIST' operation in Qserv czar. This statement always return an empty result set, while the underlying database table has multiple entries. Also there are messages in the proxy's log file *mysql-proxy-lua.log*:  {code}  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG sql.SqlConnection (core/modules/sql/SqlConnection.cc:142) - connectToDb trying to connect  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryType (core/modules/ccontrol/UserQueryType.cc:179) - isSubmit: SHOW PROCESSLIST  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryType (core/modules/ccontrol/UserQueryType.cc:130) - isSelect: SHOW PROCESSLIST  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryType (core/modules/ccontrol/UserQueryType.cc:192) - isSelectResult: SHOW PROCESSLIST  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryType (core/modules/ccontrol/UserQueryType.cc:116) - isDropTable: SHOW PROCESSLIST  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryType (core/modules/ccontrol/UserQueryType.cc:103) - isDropDb: SHOW PROCESSLIST  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryType (core/modules/ccontrol/UserQueryType.cc:146) - isFlushChunksCache: SHOW PROCESSLIST  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryType (core/modules/ccontrol/UserQueryType.cc:159) - isShowProcessList: SHOW PROCESSLIST  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryType (core/modules/ccontrol/UserQueryType.cc:164) - isShowProcessList: full: n  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG ccontrol.UserQueryFactory (core/modules/ccontrol/UserQueryFactory.cc:237) - make UserQueryProcessList: full=n  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG czar.Czar (core/modules/czar/Czar.cc:179) - QI=?: starting finalizer thread for query  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG czar.Czar (core/modules/czar/Czar.cc:332) - QI=?: Remembering query: (10.158.37.125:24040, 85) (new map size: 2)  [2018-04-25T23:19:22.312+0200] [LWP:308] DEBUG czar.Czar (core/modules/czar/Czar.cc:216) - QI=?: returning result to proxy: resultTable=qservResult.qserv_result_processlist_10898384329 messageTable=qservResult.message_10898384329 orderBy=ORDER BY Submitted  [2018-04-25T23:19:22.312+0200] [LWP:1594] DEBUG czar.Czar (core/modules/czar/Czar.cc:166) - QI=?: submitting new query  [2018-04-25T23:19:22.312+0200] [LWP:308] INFO  mysql-proxy (qserv/mysqlProxy.lua:343) - Czar response: [result: qservResult.qserv_result_processlist_10898384329, message: qservResult.message_10898384329, order_by: ""ORDER BY Submitted""]  [2018-04-25T23:19:22.312+0200] [LWP:1594] DEBUG qmeta.QMetaSelect (core/modules/qmeta/QMetaSelect.cc:62) - Executing query: SELECT Id, User, Host, db, Command, Time, State, SUBSTRING(Info FROM 1 FOR 100) Info, CzarId, Submitted, Completed, ResultLocation FROM ShowProcessList WHERE CzarId = 2 AND (Completed IS NULL OR Completed > NOW() - INTERVAL 3 DAY)  [2018-04-25T23:19:22.312+0200] [LWP:308] INFO  mysql-proxy (qserv/mysqlProxy.lua:508) - Sendresult 0  [2018-04-25T23:19:22.312+0200] [LWP:1594] DEBUG ccontrol.UserQueryProcessList (core/modules/ccontrol/UserQueryProcessList.cc:176) - creating result table: CREATE TABLE qserv_result_processlist_10898384329(`Id` BIGINT(20),`User` CHAR(63),`Host` BINARY(0),`db` TEXT,`Command` CHAR(5),`Time` BINARY(0),`State` CHAR(9),`Info` VARCHAR(100),`CzarId` INT(11),`Submitted` TIMESTAMP NULL,`Completed` TIMESTAMP NULL,`ResultLocation` TEXT)  [2018-04-25T23:19:22.312+0200] [LWP:1594] ERROR ccontrol.UserQueryProcessList (core/modules/ccontrol/UserQueryProcessList.cc:220) - error updating result table: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '02:59:27,2018-04-25 02:59:54,'table:result_1'),(2,'anonymous',NULL,'sdss_stripe8' at line 1 Unable to execute query: INSERT INTO qserv_result_processlist_10898384329(`Id`,`User`,`Host`,`db`,`Command`,`Time`,`State`,`Info`,`CzarId`,`Submitted`,`Completed`,`ResultLocation`) VALUES (1,'anonymous',NULL,'sdss_stripe82_01','SYNC',NULL,'FAILED','SELECT COUNT(*) FROM sdss_stripe82_01.RunDeepSource',2,2018-04-25 02:59:27,2018-04-25 02:59:54,'table:result_1'),(2,'anonymous',NULL,'sdss_stripe82_01','SYNC',NULL,'COMPLETED','SELECT COUNT(*) FROM sdss_stripe82_01.RunDeepSource',2,2018-04-25 05:26:51,2018-04-25 05:26:54,'table:result_2'),(3,'anonymous',NULL,'wise_00','SYNC',NULL,'COMPLETED','SELECT COUNT(*) FROM wise_00.allwise_p3as_psd',2,2018-04-25 05:27:40,2018-04-25 05:28:46,'table:result_3'),(4,'anonymous',NULL,'wise_00','SYNC',NULL,'COMPLETED','SELECT COUNT(*) FROM wise_00.allwise_p3as_psd',2,2018-04-25 05:28:58,2018-04-25 05:30:03,'table:result_4'),(5,'anonymous',NULL,'sdss_stripe82_01','SYNC',NULL,'COMPLETED','SELECT COUNT(*) FROM sdss_stripe82_01.RunDeepForcedSource',2,2018-04-25 05:30:20,2018-04-25 05:30:22,'table:result_5'),(6,'anonymous',NULL,'wise_00','SYNC',NULL,'ABORTED','SELECT COUNT(*) FROM wi...  {code}",1
"DM-14223","04/26/2018 03:36:54","Set terraform in travis integration tests","Use terraform in the CI build for creating the test cluster instead of installing it localy.",2
"DM-14233","04/27/2018 11:25:41","Remove secondMomentStarSelector","Remove secondMomentStarselector, per RFC-475, and clean up any tests, etc. that refer to it.",0.5
"DM-14236","04/27/2018 14:35:22","Incorporate l1dbproto into AssociationTask","Take the existing AssociationTask functionality and get it running using l1dbproto (rather than SQLite) as a back-end.    (This can run on a standalone system; no need to deploy at LDF.)",8
"DM-14237","04/27/2018 18:12:11","Change DecamIngestTask --filetype default from instcal to raw","As discussed in RFC-478, {{DecamIngestTask}} requires a {{\-\-filetype}} argument to ingest raw data, something its supertask {{IngestTask}} does by default. Change the default of {{\-\-filetype}} to {{raw}} so that {{DecamIngestTask}} behaves like {{IngestTask}} when given the same arguments.    This work includes updating {{obs_decam}} documentation and updating any calls to {{DecamIngestTask}} that assume the old default.",1
"DM-14244","04/30/2018 09:18:36","eups.lsst.codes backups not pruning promptly?","Daily backups are supposed to be expire out of the s3 bucket at 8 days.  This appears to not be happening promptly.  This might be because s3 isn't doing this promptly or it might be because it is a versioned bucket.  This was noticed as the s3 usage has grown a bit faster than expected.    {code:java}  $ aws s3 ls s3://eups.lsst.codes-backups/daily/2018/04/                             PRE 22/                             PRE 23/                             PRE 24/                             PRE 25/                             PRE 26/                             PRE 27/                             PRE 28/                             PRE 29/                             PRE 30/  {code}    The metadata on the object itself seems to suggest that it will expire in ~9 hours:      {code:java}   aws s3api head-object --bucket eups.lsst.codes-backups --key daily/2018/04/22/2018-04-22T11:52:06Z/stack/src/tags/w_latest.list  {      ""AcceptRanges"": ""bytes"",      ""Expiration"": ""expiry-date=\""Tue, 01 May 2018 00:00:00 GMT\"", rule-id=\""daily\"""",      ""LastModified"": ""Sun, 22 Apr 2018 13:55:01 GMT"",      ""ContentLength"": 6857,      ""ETag"": ""\""09b6761b4c732b4396521e78bc0dad4c\"""",      ""VersionId"": ""B19u2Pq35xxGQRX06FUfY1SVEFEEKRvh"",      ""ContentType"": ""binary/octet-stream"",      ""Metadata"": {}  }  {code}        ",1
"DM-14255","05/01/2018 10:39:10","Add ConvexPolygon.intersects and related methods","Add {{ConvexPolygon.intersects}} and the related methods {{contains}}, {{isDisjoint}} and {{isWithin}}. The region overloads can probably be implemented using {{ConvexPolygon.relates}} and the point overloads using {{ConvexPolygon.contains}}",0.5
"DM-14275","05/01/2018 17:09:02","The distortion in test_wcsUtils.py testDistortion is unreasonable","The distortion model used in test_wcsUtils.py's testDistortion method is not reasonable (the distortion is far too much to be reasonable).    I noticed this when that test failed testing a fix to starlink_ast on DM-14162",1
"DM-14279","05/02/2018 09:01:29","upgrade blueocean to 1.5.0 (final) / broken links to triggered builds","Blueocean {{1.5.0-beta-2}}, which was deployed on DM-13258, does add links to triggered builds.  However, these links are not properly URL encoded and are broken when linking to the build of any job that is nested in a cloudbees-folder.  This was fixed in the final release of {{1.5.0}}",1
"DM-14280","05/02/2018 09:46:13","nightly-release d_2018_05_02 failed","{{d_2018_05_02}} failed after 3 tries to build tarballs for {{centos-6.devtoolset-6.miniconda3-4.3.21-10a4fa6}}. The other two tarball configs were successful.    It looks like this may this may be some sort of cached state / pip 10.x upgrade problem but it also appears that the system python is incorrectly being used in the virtualenv.    {code:java}  [miniconda3-4.3.21-10a4fa6] Running shell script    + pip install virtualenv    Requirement already satisfied (use --upgrade to upgrade): virtualenv in /usr/lib/python2.7/site-packages    You are using pip version 8.1.2, however version 10.0.1 is available.    You should consider upgrading via the 'pip install --upgrade pip' command.    + virtualenv venv    New python executable in venv/bin/python    Installing Setuptools.............................................................................................done.    Installing Pip................................................................................................................................................................................................................................................................................................................................done.    + . venv/bin/activate    ++ deactivate nondestructive    ++ unset pydoc    ++ '[' -n '' ']'    ++ '[' -n '' ']'    ++ '[' -n /bin/bash -o -n '' ']'    ++ hash -r    ++ '[' -n '' ']'    ++ unset VIRTUAL_ENV    ++ '[' '!' nondestructive = nondestructive ']'    ++ VIRTUAL_ENV=/home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv    ++ export VIRTUAL_ENV    ++ _OLD_VIRTUAL_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin    ++ PATH=/home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin    ++ export PATH    ++ '[' -n '' ']'    ++ '[' -z '' ']'    ++ _OLD_VIRTUAL_PS1=    ++ '[' x '!=' x ']'    +++ basename /home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv    ++ '[' venv = __ ']'    +++ basename /home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv    ++ PS1='(venv)'    ++ export PS1    ++ alias 'pydoc=python -m pydoc'    ++ '[' -n /bin/bash -o -n '' ']'    ++ hash -r    + pip install --upgrade pip    Traceback (most recent call last):      File ""/home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv/bin/pip"", line 9, in <module>        load_entry_point('pip==1.4.1', 'console_scripts', 'pip')()      File ""/home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv/lib/python2.7/site-packages/pkg_resources.py"", line 378, in load_entry_point        return get_distribution(dist).load_entry_point(group, name)      File ""/home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv/lib/python2.7/site-packages/pkg_resources.py"", line 2566, in load_entry_point        return ep.load()      File ""/home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv/lib/python2.7/site-packages/pkg_resources.py"", line 2265, in load        raise ImportError(""%r has no %r attribute"" % (entry,attr))    ImportError: <module 'pip' from '/home/jenkins-slave/workspace/release/tarball/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6/venv/lib/python2.7/site-packages/pip/__init__.pyc'> has no 'main' attribute    script returned exit code 1  {code}  ",0.5
"DM-14290","05/02/2018 16:36:59","Do not raise generic Exceptions","We have several bits of code that raise either {{Exception}} or {{lsst.pex.exceptions.Exception}}/{{pexExcept.Exception}}. We should replace all of those with an appropriately specific exception. The generic Exception should almost never be raised (or caught, but that's a different issue).",2
"DM-14291","05/02/2018 17:21:42","PolyMap.polyTran does not clear IterInverse","{{PolyMap.polyTran}} replaces the forward or reverse coefficients with a fit. But if it is used to fit the inverse (as is typical) and the original {{PolyMap}} has an iterative inverse, the returned mapping will use that iterative inverse and ignore the fit coefficients.    I have asked David Berry if {{astPolyTran}} is behaving as it should by copying the value of {{IterInverse}} instead of clearing it. I am guessing it is intentional, in which case I can see three solutions:  - Have astshim clear {{IterInverse}} in the returned mapping when appropriate.  - Document the problem and allow {{PolyMap}} to be modified by setting {{IterInverse}} and, presumably, the two related properties, thus breaking the rule that mappings are immutable (other than Id and Ident). Allowing these properties to be set might be useful for tweaking behavior.  - Document the problem and be done with it. I feel this is too surprising to consider.    Of these I feel the first, clearing {{IterInverse}} when appropriate, is most appropriate and least surprising. However, it will require a bit of care, as the original mapping may be inverted and polyTran can fit either direction. I believe the logic is as follows:    If the mapping being fit has IterInverse set:      If the mapping being fit is not inverted and polyTran is fitting the inverse:          unset IterInverse in the returned mapping      if the mapping being fit is inverted and polyTran is fitting the forward direction:          unset IterInverse in the returned mapping    Which simplifies to:  If polyMap IterInverse is set and polyTran forward == PolyMap.isInverted():      unset IterInverse in the returned mapping    Independently it is worth considering allowing modifying the iterative inverse parameters but I'd like a clearer need before doing so.  ",0.5
"DM-14292","05/02/2018 19:42:50","Deploy replication service at PDAC","Major milestones of this effort:  # upgrade Qserv installation in PDAC with the latest version of Docker containers based on MariaDB 10.2.14. The new containers are also needed to support the extended management protocol needed for cooperation between Qserv workers and the Replication system's Controllers when changes to replica disposition are made.  # Docker-based install and preliminary tests of the Replication system's tools on the cluster. At this stage a proper configuration of the Replication system will be devised and tested.  # Scalability tests of the main replication operations, including operations with persistent state of the operations (Replication _jobs_ and _requests_). Making improvements to the implementation of the system as needed.  # Implement the _Health Monitoring Algorithm_ for workers of both kinds (Qserv and the Replication system). Integrate this algorithm into the above mentioned _fixed logic Controller_. The initial version of the algorithm will be sending _probe_ requests to both kinds of workers and measure their responses (which must arrive within a reasonable period of time). This may also require to make some adjustments to the Replication system's Messaging Network to respect priorities of these requests. The current implementation has a plain queue. The new one should get a priority queue.  # Finalize and test the fixed-logic replication Controller  # when confident with the functionality, performance and robustness of the Replication system integrate the system with the existing Kubernetes infrastructure",40
"DM-14299","05/03/2018 10:29:17","Remove Python2-isms from developer guide","Now that Python 2 is not supported, we can remove all the items in the Python style guide talking about Python 2 and {{__future__}}/{{futurize}}.",1
"DM-14302","05/03/2018 11:03:28","verify fails on master, possibly with unexpected Quantity repr","In building {{verify}} package on my Mac with numpy 1.14.2 and astropy3 I get the following failure:  {code}  ============================= test session starts ==============================  platform darwin -- Python 3.6.3, pytest-3.2.0, py-1.4.34, pluggy-0.4.0  rootdir: /Users/timj/work/lsstsw3/build/verify, inifile: setup.cfg  plugins: session2file-0.1.9, forked-0.2, xdist-1.20.1, flake8-0.9.1, remotedata-0.2.1, openfiles-0.3.0, doctestplus-0.1.3, arraydiff-0.2  collected 539 items   run-last-failure: rerun previous 2 failures    tests/test_threshold_specification.py FF     generated xml file: /Users/timj/work/lsstsw3/build/verify/tests/.tests/pytest-verify.xml   =================================== FAILURES ===================================  ___________________ ThresholdSpecificationTestCase.test_spec ___________________    self = <test_threshold_specification.ThresholdSpecificationTestCase testMethod=test_spec>        def test_spec(self):          """"""Test creating and accessing a specification from a quantity.""""""          s = ThresholdSpecification('design', 5 * u.mag, '<')          self.assertEqual(s.name, Name(spec='design'))          self.assertEqual(s.type, 'threshold')          self.assertEqual(s.threshold.value, 5.)          self.assertEqual(s.threshold.unit, u.mag)          self.assertEqual(s.operator_str, '<')          self.assertEqual(              repr(s),  >           ""ThresholdSpecification(""              ""Name(spec='design'), <Quantity 5.0 mag>, '<')"")  E       AssertionError: ""ThresholdSpecification(Name(spec='design'), <Quantity 5. mag>, '<')"" != ""ThresholdSpecification(Name(spec='design'), <Quantity 5.0 mag>, '<')""  E       - ThresholdSpecification(Name(spec='design'), <Quantity 5. mag>, '<')  E       + ThresholdSpecification(Name(spec='design'), <Quantity 5.0 mag>, '<')  E       ?                                                         +    tests/test_threshold_specification.py:118: AssertionError  ___________________ ThresholdSpecificationTestCase.test_spec ___________________    self = <test_threshold_specification.ThresholdSpecificationTestCase testMethod=test_spec>        def test_spec(self):          """"""Test creating and accessing a specification from a quantity.""""""          s = ThresholdSpecification('design', 5 * u.mag, '<')          self.assertEqual(s.name, Name(spec='design'))          self.assertEqual(s.type, 'threshold')          self.assertEqual(s.threshold.value, 5.)          self.assertEqual(s.threshold.unit, u.mag)          self.assertEqual(s.operator_str, '<')          self.assertEqual(              repr(s),  >           ""ThresholdSpecification(""              ""Name(spec='design'), <Quantity 5.0 mag>, '<')"")  E       AssertionError: ""ThresholdSpecification(Name(spec='design'), <Quantity 5. mag>, '<')"" != ""ThresholdSpecification(Name(spec='design'), <Quantity 5.0 mag>, '<')""  E       - ThresholdSpecification(Name(spec='design'), <Quantity 5. mag>, '<')  E       + ThresholdSpecification(Name(spec='design'), <Quantity 5.0 mag>, '<')  E       ?                                                         +    tests/test_threshold_specification.py:118: AssertionError  ============================= 537 tests deselected =============================  =================== 2 failed, 537 deselected in 3.40 seconds ===================  Global pytest run: failed  scons: Nothing to be done for `examples'.  scons: Nothing to be done for `doc'.  Failed test output:  Global pytest output is in /Users/timj/work/lsstsw3/build/verify/tests/.tests/pytest-verify.xml.failed  The following tests failed:  /Users/timj/work/lsstsw3/build/verify/tests/.tests/pytest-verify.xml.failed  1 tests failed  scons: *** [checkTestStatus] Error 1  scons: building terminated because of errors.  {code}",0.5
"DM-14305","05/03/2018 12:05:20","Upgrade Eigen to 3.3.4","Implementation of RFC-479.    For now, this is exploratory work, to see just how bad the changes to ndarray are going to be.",2
"DM-14307","05/03/2018 12:26:36","Review cModel logging setup","I read the developer guide's logging docs and reviewed and tested cModel's logging. In short, the optimizer has detailed debug statements, but there is very little output from cModel itself describing the control flow. For example, one galaxy was running the double shapelet PSF fitting tasks but skipped the cModel routine, as modelfit_CModel_flag_region_maxBadPixelFraction was set. I also spent some time (re)discovering that some ipython output gets sent to jupyter's stdout for reasons I don't understand (possibly this bug: https://github.com/gotcha/ipdb/issues/52).",2
"DM-14311","05/03/2018 16:08:41","Add subtractAlgorithmRegistry to __all__ in imagePsfMatch.py","When DM-14134 added {{\_\_all\_\_}} to {{imagePsfMatch.py}}, {{subtractAlgorithmRegistry}} was not included. This breaks {{imageDifference.py}} in {{pipe_tasks}} which expects to be able to import it.",0.5
"DM-14313","05/03/2018 17:11:25","Add pyarrow to the jellybean install.","Some of the QA tools now need pyarrow.  Please install pyarrow with the other third party modules.",0.5
"DM-14314","05/03/2018 17:58:42","Metaserv should return the metadata for WISE Tables","Per [~tatianag], dax_metaserv should be populated for everything we are serving through PDAC.     http://lsst-qserv-dax01:5000/meta/v1/db/ should have one logical database entry for WISE with multiple schemas corresponding to each of the PDAC database described here:    https://confluence.lsstcorp.org/display/DM/PDAC+v2+data+list     (Similarly to how we have sdss_stripe82_01 in the schemas section returned by sst-qserv-dax01:5000/meta/v1/db/W13_sdss_v2/)    We should only populate metaserv with the data readily available from dbschema (""name"", ""datatype"", ""nullable)"". All other fields like ""ucd"", etc. can be empty.    This would make it possible for SUIT to refer to WISE tables using logical database name rather than internal name like `""//lsst-qserv-master01:4040"".wise_00.allwise_p3as_mep` and make sure that WISE data can be discovered through TAP like services.    5/10/2018 tgoldina  Since there is no optimized way to get table column metadata via dbserv, this issue is a blocker for moving PDAC to dbserv v1 (albuquery). Please consider it when deciding on the priority of this ticket.      ",20
"DM-14320","05/04/2018 10:25:21","dbserv (or albuquery) should include description column in the metadata result set","Per [~tatianag],    The new version of dbserv, integrated with metaserv returns metadata for all columns in the result set. Currently, the ""columns"" metadata fields are ""name"", ""datatype"", ""ucd"", ""unit"", and ""tableName"".    Ideally, column metadata should contain all column metadata fields available via metaserv, including ""nullable"" and ""description"". (See http://dm.lsst.org/dax_metaserv/api.html#get--meta-v1-db-(string-db_id)-tables-(table_id)- for column metadata fields stored in metaserv.)     For WISE datasets, the 'description', 'unit', etc. can be empty (returned as empty strings) in metaserv.  Expected columns, empty or not, should be propagated to the UI.",1
"DM-14325","05/04/2018 15:06:34","deepDiff datasets not supported by HSC","Trying to run {{imageDifference.py}} on an HSC dataset crashes with an error saying that the {{deepDiff_diaSrc}} dataset type does not have a template. Manual inspection of {{obs_subaru/policy/HscMapper.yaml}} confirms that there are no {{diaSrc}} or {{deepDiff}} datasets mentioned.    Please add the appropriate datasets so that we can do image differencing on HSC data.",1
"DM-14333","05/04/2018 18:20:39","Support Oracle dialect in AP prototype script ","While Oracle RAC at NCSA is getting ready for testing I could spend some time to try running my prototype against Oracle in some other non-production setup. ap_proto uses sqlalchemy but it also has dialect-specific SQL handling for optimization purpose. I need to extend that backend-specific code to support Oracle as well.     I can think of few potential things that need different implementation for Oracle:   * multi-row INSERT, Oracle does not support syntax that is supported by other backends (\{\{INSERT INTO TABLE (columns) VALUES (data), (data), (data), ...}}) so I'll have to find some other mechanism for bulk row insert   * ""UPSERT"" functionality should also be implemented differently in Oracle   * Case-sensitivity issue (probably minor)   * Usual DATETIME stuff is not very portable   * Anything else...",5
"DM-14334","05/04/2018 18:26:03","Start implementing QuantumGraph building","Logic is described by Jim in [https://dmtn-056.lsst.io/operations.html#supertask-pre-flight-and-execution.] There are many complications of course, e.g. what is Registry responsibility vs. Pre-flight responsibility. First implementation will likely be sub-optimal and ugly.     ",8
"DM-14336","05/07/2018 01:12:38","Unify k8s proxy manifest for worker and master pods","Unify the k8s definition of the proxy container for worker and master pods.",5
"DM-14345","05/07/2018 13:26:27","Add LSP requirements to model and submit to CCB","In DM-14335 the final list of requirements was submitted. They need to be transferred to the model and converted to a docgen for review by DM CCB.",2
"DM-14347","05/07/2018 14:43:05","Extend antlr4 integration test query parity ","Start to go through the FIXME integration test queries and determine if they are FIXME because they fail to parse, or if they fail to run. Take notes on which is which. Make some effort to figure out why they don't run (with the acknowledgment that it may take 20 story points to chase down even 1 non-running query for a qserv novice).    The output of this will be compared with [query pages in TRAC|https://dev.lsstcorp.org/trac/wiki/db/queries] (being ported to Confluence by [~fritzm]), and stories to fix these stories will be created as appropriate.",20
"DM-14356","05/08/2018 09:39:11","Implement putting of matplotlib figures","Make matplotlib figures {{butler.put()}}able.    [~jbosch] says on Slack:   > Basically, that involves grepping {{daf_persistence}} for {{FitsCatalogStorage}}, copying what you see and calling it {{MatplotlibStorage}}, and then adjusting it appropriately (i.e. call {{savefig}} instead of {{writeFits}}, raise an exception when trying to read).    [~price] notes that there are some gotchas involving the fact that the backend has to be set immediately after import, and this could prove tricky, especially if people want to combine this with pop-up style debug plots.. Some relevant info on this might be found in DM-14159.      ",1
"DM-14359","05/08/2018 13:08:18","Fix data ID handling in ap_*","During investigation of DM-12672, I discovered several bugs in how {{ap_pipe}} and {{ap_verify}} handled data IDs:  * Most command-line tasks implicitly assume their {{run}} or {{runDataRef}} methods take a fully expanded data reference. This is provided by {{pipe.base.ArgumentParser}}, but {{ap_pipe}}'s task runner inadvertently bypassed the data ID expansion. In practice, this meant {{ImageDifferenceTask}} didn't have all the information it expected.  * The same bypass would have prevented {{ap_pipe}} from processing multiple datasets specified as {{visit=#}}, though {{visit=# ccd=1..62}} would have still worked.  * {{ap_verify}} always provided partial data references, because it assumed the run method (or, more precisely, its butler calls) would expand any unambiguous reference.  * The tests in {{test_ingestion.py}} were based on a naive view of how data IDs work, and most of the outstanding issues with the tests can now be fixed.",2
"DM-14360","05/08/2018 14:20:06","Pin pybtex/sphinxcontrib-bibtex dependencies in documenteer 0.2.x & dev guide update","The Dev Guide suggests we use the following:    {code}  .. bibliography:: local.bib lsstbib/books.bib lsstbib/lsst.bib lsstbib/lsst-dm.bib lsstbib/refs.bib lsstbib/refs_ads.bib     :encoding: latex+latin     :style: lsst_aa  {code}    However, when I tried this with a recent sphinxcontrib-bibtex, I get:    {code}  /Users/jds/Projects/LSST/docs/dmtn/031/index.rst:508: ERROR: Error in ""bibliography"" directive:  invalid option value: (option: ""encoding""; value: 'latex+latin')  unknown encoding: ""latex+latin"".    .. bibliography:: lsst-texmf/texmf/bibtex/bib/refs_ads.bib     :style: lsst_aa     :encoding: latex+latin  {code}    I think this is due to changes upstream (I didn't track it down fully, but I note that both sphixcontrib-bibtex and pybtex have been making changes to the way they handle encodings in recent releases).    Simply removing the {{:encoding:}} line works fine for me.",0.5
"DM-14363","05/08/2018 14:50:31","Make afw::cameraGeom::Detector table-persistable","In Gen3, we're planning to just persist Detectors with Exposures to make each Exposure more self-contained and avoid complex modify-on-load code (which would have had to get more complex than what we have now to handle camera versioning).  This means we need a way to save a Detector inside an Exposure, and at least at present, that means making it inherit from {{afw::table::io::Persistable}}.     ",8
"DM-14366","05/08/2018 18:22:55","Make pipe_base and pipe_tasks pep8 compliant","Fix pep8 warnings and errors in pipe_base and pipe_tasks and enable automatic flake8 checking",1
"DM-14367","05/09/2018 07:16:07","Remove cfitio headers and lib from git repo and system install these items.","SSIA    For the Tucson turnkey system, but will be the same for overall system.",1
"DM-14369","05/09/2018 07:26:48","nightly-release d_2018_05_09 failed","The nightly release failed due to fallout from DM-14138 merged yesterday. This is a simple id10t error in that I did not rebuild the {{lsstsqre/codekit}} image after making a new release of {{sqre-codekit}} to pypi, but updated the docker tag string in jenkins.    https://ci.lsst.codes/blue/organizations/jenkins/release%2Fnightly-release/detail/nightly-release/285/pipeline    {code:java}  ++ id -un  ++ id -u  ++ id -gn  ++ id -g  + docker build -t lsstsqre/codekit:5.0.4-local --build-arg USER=jenkins-slave --build-arg UID=996 --build-arg GROUP=jenkins-slave --build-arg GID=991 --build-arg HOME=/home/jenkins-slave .  Sending build context to Docker daemon  2.048kB  Step 1/12 : FROM    lsstsqre/codekit:5.0.4  manifest for lsstsqre/codekit:5.0.4 not found  script returned exit code 1  {code}  ",1
"DM-14370","05/09/2018 07:29:54","Modify SAL message emulator for Tucson turnkey system","Before shipping the turnkey system to Tucson, a burn-in test must be run for 48 hours. This will require modification of the simple SAL message emulator used for in-house testing. The new emulator should include occasional start-ups and shut-downs of the ATS DM system, as well as randomly draw image_ids/catalog names from a set of 100 pre-triggered in the DAQ (triggering the images is not necessary for this test, and is only realistically done when Tony's CCS system is running as well.    As well as proper logging, the system should issue a 'result set' log report every time it is shut down.",2
"DM-14378","05/09/2018 11:23:21","Add Gen3 conversion scripting and tests to ci_hsc","Add gen3 conversion to the ci_hsc SCons build and test that we can use a Gen3 Butler to {{get}} Datasets {{put}} with a Gen2 Butler.",2
"DM-14385","05/09/2018 19:08:48","typo in metaserv return key","metaserv return object for {{http://lsst-qserv-dax01:5000/meta/v1/db/W13_sdss_v2/tables/RunDeepForcedSource/}} has a key {{result:}} instead of {{result}}. Appears like a typo.  ",2
"DM-14392","05/11/2018 01:21:56","Add userfriendliness for provision script","Test will be run on NCSA openstack using 25 nodes and more...",8
"DM-14393","05/11/2018 09:05:23","column datatypes returned by metaserv and dbserv should be consistent","Column datatype returned in the metadata section of the result by dbserv in absence of metaserv data should be consistent with the column datatype in metaserv.    Currently, the dbserv returns ""string"" datatype, while metaserv returns ""text"".   Also for 'BIT', dbserv returns ""binary"", while metaserv returns ""boolean"", for TINYINT, dbserv returns ""int"", metaserv returns ""short"".    To facilitate debugging, please consider returning both dbserv and metaserv datatypes in the metadata section of the dbserv return.    Per [~kennylo],    column datatypes returned by metaserv  come from the conversion table in lsst/dax/metaserv/schema_utils.py:    {code:python}  MYSQL_TYPE_MAP = {      'VARCHAR': ""text"",      'TIMESTAMP': ""timestamp"",      'BINARY': ""binary"",      'TINYINT': ""short"",      'BIGINT': ""long"",      'BIT': ""boolean"",      'FLOAT': ""float"",      'INTEGER': ""int"",      'DOUBLE': ""double"",      'CHAR': ""text""      }  {code}    column datatypes returned by dbserv are defined by the following conversion table from /lsst/dax/albuquery/Results.kt:    {code:kotlin}  fun jdbcToLsstType(jdbcType: JDBCType): String {      return when (jdbcType) {          JDBCType.INTEGER -> ""int""          JDBCType.SMALLINT -> ""int""          JDBCType.TINYINT -> ""int""          JDBCType.BIGINT -> ""long""          JDBCType.FLOAT -> ""float""          JDBCType.DOUBLE -> ""double""          JDBCType.DECIMAL -> ""double"" // FIXME          JDBCType.NUMERIC -> ""double"" // FIXME          JDBCType.ARRAY -> ""binary""          JDBCType.BINARY -> ""binary""          JDBCType.BIT -> ""binary""          JDBCType.BLOB -> ""binary""          JDBCType.CHAR -> ""string""          JDBCType.VARCHAR -> ""string""          JDBCType.NVARCHAR -> ""string""          JDBCType.CLOB -> ""string""          JDBCType.BOOLEAN -> ""boolean""          JDBCType.DATE -> ""timestamp""          JDBCType.TIMESTAMP -> ""timestamp""          JDBCType.TIMESTAMP_WITH_TIMEZONE -> ""timestamp""          JDBCType.TIME -> ""time""          else -> ""UNKNOWN""      }  }  {code}  ",8
"DM-14433","05/14/2018 17:07:29","Perform forced photometry on visit images within the AP pipeline","Perform forced photometry on the corresponding PVI for every DIASource produced by the Alert Generation pipeline. Ensure this design is captured in LDM-151.",40
"DM-14435","05/14/2018 17:18:36","Implement MOC overlay","Implement the visual display of MOC data in Firefly    Initially, display the HEALPixels actually identified in the MOC (which is just a list of HEALPixel IDs in NUNIQ format) as polygons overlaid on a sky display in Firefly. Include this in the usual Firefly layer control behavior.   * We would like multiple MOCs to be able to be displayed at once, with selectable colors used.   * It may be useful to enable the control of both polygon boundary colors, transparency, etc. _as well as polygon fill_ with adjustable transparency.   * Be able to switch between outline (wireframe) and transparent color fill   * MOC show as a HiPS layer with checkbox off, should lazy load with any HiPS         Implementation technical Notes:   * We should read the table using our normal fits table reading and put all the rows (only one column) into the store.   * Each number (NUNIQ) can be translated to a HealPix level and number and then to 4 WorldPt corners using functions in HipsUtil and HealpixIndex   * We need to make new DrawObject will need to be created that can draw a filled polygon, fill with a transparent color for overlaying. Possibly we could modify FootprintObj or MarkerFootprintObj.    * If the points reduce to a single point (based on zoom level) it should draw a single point.   * The drawing layer should be able to change the color in our standard way.     ",20
"DM-14438","05/14/2018 17:31:38","Load MOCs from other sources","     We would like multiple MOCs to be able to be displayed at once.   * Load more MOCs with the upload panel   * -Load MOCs with the search panel the can search the CDS server.- _Moved to DM-15570_         For immediate implementation, we will put a test button at the top.",8
"DM-14440","05/14/2018 17:55:45","dbserv v1: ParseException when spatial constraint is followed by other constraints","In dbserv v1 (albuquery), if I add another constraint to the spatial constraint, the query fails. For example,    {noformat}  > curl -L http://lsst-qserv-dax01:8080/sync/ -H ""Accept: application/json"" -d 'query=SELECT * FROM W13_sdss_v2.sdss_stripe82_01.RunDeepSource WHERE qserv_areaspec_circle(9.5,-1.1,0.002777777777777778) AND deblend_nchild < 2'    {""message"":""(conn=2417) Query processing error: QI=?: Failed to instantiate query: ParseException:Parse error(ANTLR):unexpected token: (:"",""type"":""SQLException"",""code"":null,""cause"":null}  {noformat}    Without AND part it works fine. The query also works in dbserv v0.    [SLAC discussion|https://lsstc.slack.com/archives/C2B709EQK/p1526333780000583]",2
"DM-14441","05/14/2018 18:45:22","{{detect_isPrimary}} is not consistently set","Running to the end of the ""Getting Started with the LSST piplines"" tutorials    https://pipelines.lsst.io/getting-started/multiband-analysis.html    I think I have encountered a bug in how {{detect_isPrimary}} gets set.  The documentation says that it is the union of {{deblend_nChild==0}} with {{detect_isPatchInner}} and {{detect_isTractInner}}, however:    {code}  >>> by_hand = (refTable['deblend_nChild']==0) & refTable['detect_isPatchInner'] & refTable['detect_isTractInner']    >>> np.where(by_hand != refTable['detect_isPrimary'])  (array([4708, 4709, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4720,         4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4730, 4731, 4732,         4733, 4734, 4735, 4736, 4737, 4738, 4740, 4741, 4743, 4744, 4745,         4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756,         4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4766, 4768, 4769,         4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780,         4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791,         4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802,         4803, 4804, 4805, 4806, 4807, 4918]),)  {\code}    where {{refTable}} is loaded as specified in the tutorial outlined above.    As always: let me know if you need more information to recreate this behavior.  It is not impossible that this is just user error.",2
"DM-14442","05/14/2018 19:46:57","F18 Qserv Release Engineering","This epic holds F18 effort budget for ongoing chores such as updating upstream packages in eups, Jenkins and Travis configuration maintenance, and compiler and platform compatibility fixes.",3
"DM-14455","05/15/2018 02:02:59","F18 Butler Gen2 Critical Support","This epic holds the F18 effort budget for critical maintenance to Butler Gen 2.",20
"DM-14458","05/15/2018 05:47:45","Add Redis scoreboard for incrementing session_id, job_num, and ack_ids","Since the principle components were built for level one image ingest and distribution, they have used ad hoc means for generating unique numbers and keeping track of the last number between restarts. With the development of the ATS Turnkey system, a redis conf file was produced that sets the default number of redis instances at 18 instead of 16, so there is room for an increment scoreboard. This is a much cleaner way of addressing this and with db dumps before shutting down and by not flushing the increment scoreboard class at creation, this need is finally, properly handled.",2
"DM-14459","05/15/2018 08:13:14","Add check to (Posix)Datastore that prevents silent overwrite","A {{Datastore.put}} should raise when the file already exists (perhaps adding an option to force it).",1
"DM-14471","05/16/2018 04:48:36","HiPS display test","DM-14149 made it possible to display HiPS images at level 20 level by using JavaScript 56 bit integer. It created new methods to do bit-shift.  This ticket to test the result from science point of view. ",2
"DM-14472","05/16/2018 11:20:40","Integrate user expression parser into command line tool","User expression parser is almost complete, it can now be integrated in supertask CLI. ",2
"DM-14474","05/16/2018 15:28:22","Add environment variables to Jupyterlab deployments for Firefly","As part of the ease-of-use improvements in DM-14391, we would like to ask that two environment variables be provided in Jupyterlab deployments.    FIREFLY_URL = path to the default Firefly server    e.g. [https://lsst-lspdev.ncsa.illinois.edu/firefly] for the [https://lsst-lspdev.ncsa.illinois.edu/nb] environment. A trailing slash will work.     FIREFLY_HTML = default landing page for Firefly, which is appended to the URL.    We'd like this set to 'slate.html' as we intend to build around this view.",0.5
"DM-14490","05/17/2018 02:24:36","Fix k8S virtual network for Qserv at CC-IN2P3","Test will be run on NCSA openstack using 25 nodes and more...",8
"DM-14491","05/17/2018 09:24:10","FireflyClient display_url does not make weblink in Jupyterlabdemo","The {{display_url}} method added to {{FireflyClient}} in DM-14391 should make a clickable weblink. This works in my classic notebook tests, but in the Jupyterlabdemo environment the URL is only being printed out. Some restructuring of the try/except block in this function will make it work.",1
"DM-14494","05/17/2018 11:12:43","selectImages.py depends on defunct lsst.geom","{{selectImages.py}} depends on {{lsst.geom.convexHull}}, which was removed in DM-13790. It should be rewritten to use {{lsst.sphgeom.ConvexPolygon.convexHull}} instead.    This issue is blocker priority because {{selectImages}} is included in all imports of {{lsst.ap.pipe}}.",1
"DM-14496","05/17/2018 13:36:50","test_association broken","{{ap_verify}}'s {{test_association}} does not run with the latest version of {{ap_association}}, complaining about {{ap.association.DIAObject}} not existing. Please update the {{ap_verify}} tests so that they pass.    I have tried to update the tests to use source catalogs instead of {{DIAObjectCollection}} myself, but wasn't able to make a valid DIA Object catalog.",1
"DM-14497","05/17/2018 17:40:44","ap_pipe doesn't know filters for AssociationTask","{{AssociationDBSqliteConfig}} now requires an observatory-specific filter list as part of its config. This issue adds the DECam value to {{ApPipeConfig.setDefaults}} as a quick fix; presumably the settings can be moved to an obs-specific config override file in DM-12315.    [Apologies for the edit wars; I had thought this could be fixed at the level of {{AssociationDBSqliteConfig}} itself]",1
"DM-14505","05/18/2018 10:07:38","Fixed coverage and cache bugs","[~tatianag] found several issues while working on DM-8215.     Fix the following:   *  FITS Cacheing is not working for URL type WebPlotRequest   * In certain cases coverage  drawing is not showing overlays for multiple catalog  tables     Extra work done:   * removed FileHolder.java, a interface that was only implement by on class   * removed useHiPSForCoverage option, now the is always on.   * fixed a problem an exception when loading wise images from metaConvert   * remove CoverageChooser since it is not longer used.    _to test cache bug:_   * load a image using the URL option, such as [http://web.ipac.caltech.edu/staff/roby/demo/wise-m51-band2.fits]   * then load the same image again. It should come from the cache.    _to test coverage bug:_   * use wise to load two different results   * They should both be overlaid.    ----",3
"DM-14508","05/18/2018 12:58:33","Implement RFC-474 to allow use of scikit-learn","Make a stub package for scikit-learn",2
"DM-14509","05/18/2018 13:35:05","Option to turn sparse matrices into dense ones to explore eigenvalues","I need to explore the properties of the matrix and gradient in jointcal's {{ConstrainedPhotometyModel}}, but the sparse matrix code in Eigen doesn't include many options for doing so. To better facilitate this, [~astier] suggested outputting the matrix elements and creating a dense matrix from it. This ticket is just for the output and checking that it's sensible: detailed exploration will be in a different ticket.    A quick search suggests that one can pass the sparse matrix to one of Eigen's constructors to directly get the dense matrix. It might be easier to explore the matrix properties with numpy.",2
"DM-14510","05/18/2018 13:49:47","Implement line search","[~astier] suggests that we should implement a [line search|https://en.wikipedia.org/wiki/Line_search] in jointcal, to deal with the non-linearities in the photometric model (and because we may eventually have those in astrometry, as the astrometry model gets more complex). He suggested using [Brent's method|https://en.wikipedia.org/wiki/Brent%27s_method], which is [available in GSL|https://www.gnu.org/software/gsl/doc/html/roots.html#c.gsl_root_fsolver_brent]. It might be easier to just take the code directly from GSL, instead of writing an interface layer, given the way the nature of the jointcal models.    At the top of the outlier rejection loop in {{FitterBase.minimize()}}, this would look something like:    {code}  Eigen::VectorXd delta = chol.solve(grad);  scale = lineSearch(delta);  offsetParams(delta*scale));  {code}  ",8
"DM-14512","05/18/2018 14:32:44","Remember filename of uploaded DS9 Region file","I notice that (successful) region file uploads produce an entry in the layer dialog of the form ""REGION_PLOT_TYPE-(serial number)"" (e.g., ""REGION_PLOT_TYPE-7""). It would be helpful to the user if the filename of the uploaded file were retained and displayed in the layer dialog.    Since regions can be created by API, not just by file upload, the file name convention can't apply in all cases, so the ""serial number"" approach may still be appropriate for API uploads. However, even in that case, I think a more user-friendly string than ""REGION_PLOT_TYPE"" would be an improvement. Does the API allow regions to be given names that could then be shown in the layer dialog?         TODO in this ticket:   * use the file name as label in the layer dialog. In cases that the file name is too long, treat it the same way as uploading an image, use only the characters fit in proper length space, and display the full name as tooltip when mouse over it.   * make sure it behaves the same in API, providing a label field for caller to set the label for display.  If there is no label supplied, use string ""ds9 region overlay-\{N}"". N will start at 1, and increase by sequence as it does now.     Also implemented:    * File upload pane is also updated to show the long file name of the uploaded file in shorter style as that shown in the layer dialog.",3
"DM-14514","05/18/2018 14:55:49","Provide UI for uploading ""instrument footprints"" to Firefly (relocatable regions)","Inside Firefly, there is a UI option to place ""footprints"" (e.g., of astronomical instruments) over displayed images.  Footprints are created from region files in the Firefly repo (https://github.com/Caltech-IPAC/firefly/tree/dev/src/firefly_data/footprint) and appear to be treated by Firefly as standard DS9 region files where the ra,dec coordinate system is treated as having a relocatable origin and orientation (i.e., as field angles rather than true ra and dec).    It would be very nice to make this capability available (1) in the UI, for region data uploaded from a file by a user, and (2) in the API, for region data sent from the JavaScript and/or Python APIs.    For (1), one possibility would be that the region file upload dialog might get a check-box added to it saying ""make origin movable"" or ""treat as footprint"".  Alternatively, an ""upload footprint"" element might get added to the existing footprint menu.    Please see also tickets I've filed in the IRSA system, including IRSA-1612 and IRSA-1613, regarding the treatment of footprints and their coordinates in the layer dialog.",5
"DM-14515","05/18/2018 15:00:08","Improvements to Firefly footprint menu","Two suggestions (only!) for the footprint menu in Firefly:    # Make it hierarchical, so that, e.g., all the HST instrument footprints appear in a submenu.  (I would like to have at least three footprints for LSST, for instance.)  # Make its content part of the application configuration, e.g., controlled via the {{suit}} package for LSST or the {{ife}} package for IRSA.  The Firefly repo could still contain a standard set of footprint files, but more could be added in the application, and even some of the standard ones suppressed, perhaps?",1
"DM-14516","05/18/2018 15:56:58","bug fix: make region file parsing case insensitive","When upload a ds9 region file to be overlaid on an image, the keyword is required to be lower-case characters. Lets make it case insensitive .    test:   * do image search on firefly on target (0,0, j2000) and upload footprint file from 'Load DS9 Region File' popup.   * go to firefly/demo/ffapi-footprint-test.html, and add footprint layer from the footprint tool page.",1
"DM-14517","05/18/2018 16:25:54","Fix bug in schema alias mapping in ap_association.","This work was originally on DM-14507 however it was found that the failure to use alias mapping was due to a bug and the originally alias mapping work. As such this ticket will fix the bug and create a new unittest testing the case of a miss matched schema in DIASource storage.",1
"DM-14520","05/18/2018 17:40:45","Re-enable Dataset->DataUnit foreign keys","Dataset's foreign keys to DataUnit have been commented-out in the schema file.    Uncomment them and get things working (actually, we'll need to update their representation in the schema in order to make them compound keys).    I've already started this in trying to get the schema on DM-12620 synced with master, and I think I'm pretty close, but it makes sense to split this off from that ticket.",1
"DM-14521","05/18/2018 17:54:44","Update qa_explorer for new coord types","There are mentions of {{IcrsCoord}} in {{qa_explorer}}.  These need to be updated to the new coord objects.",1
"DM-14533","05/21/2018 07:42:21","Remove geom from packages expected by ci_hsc","Geom was removed from the dependency tree in a previous ticket.  ci_hsc still expects in its test that setup packages are recorded, causing a failure.",1
"DM-14534","05/21/2018 10:36:51","Fix measurementInvestigationLib.makeRerunCatalog parent keys","lsst.meas.base.measurementInvestigationLib has convenience functions for setting up catalogs to rerun measurement tasks. However, if you call makeRerunCatalog with an id list including children but not their parents, these children will be skipped by runPlugins (e.g. from    lsst.meas.base.SingleFrameMeasurementTask). makeRerunCatalog should offer options to either reset child objects' parent keys to zero if their parents are not in the list (the default) or add the parents to the id list.",2
"DM-14536","05/21/2018 11:59:39","Utilities to include in Firefly docker container","Please add the following utilities to Firefly docker container:    - procps (http://procps.sourceforge.net/)  - wget  - emacs    These utilities will be helpful when debugging a dockerized Firefly deployment using kubectl.",1
"DM-14542","05/22/2018 02:42:09","Separate cluster provisioning and k8s setup","Separate the cluster provisioning (terraform) from k8s setup",3
"DM-14543","05/22/2018 07:12:15","Fix DatasetType registration","* Automatically register components, and  * Allow for duplicate insertion only if identical.",0.5
"DM-14545","05/22/2018 07:16:19","Add test for composite calexp to Butler","Add test for composite calexp to Butler",0.5
"DM-14548","05/22/2018 10:08:31","Many refraction functions are documented to return float but return Quantity","The new refraction code in afw has many functions that claim to return float, but actually return astropy Quantity.    This has two issues:  - The code in refraction is hard to understand because atmosTerm1 and 2 have units of {{Pa / mbar}} but they are supposed to be dimensionless. It's hard to see how one gets radians from that. For clarity I think some values should be converted to floats instead of being left as Quantities.  - The documented return value is wrong for many of the functions.    I stumbled across this while implementing DM-14429 and had some trouble figuring out what was going on.",2
"DM-14554","05/23/2018 11:02:17","Produce GraphViz plots from existing or example QuantumGraph","`stac` can make GraphViz files but `stack` is currently nit working - `run` command depends on a Butler and I need to update it to work with new repos. It would be useful to be able to generate GrpahViz plots from existing QGraph without executing stuff, will try to see how it can be done with the existing options.",2
"DM-14557","05/23/2018 11:35:42","Add package docs to datasets","The {{ap_verify}} dataset framework is currently documented in the Sphinx documentation for {{ap_verify}}. This documentation assumes that individual datasets will at least have documentation of their GitHub repository, so that the pages can be linked from lists of known datasets or from examples. However, no such documentation has been written yet.    This issue adds a documentation placeholder to {{ap_verify_dataset_template}}, and standardized package documentation to {{ap_verify_hits2015}}, {{ap_verify_testdata}}, and any other datasets existing at the time of work. It will *not* move the documentation of the dataset framework from its current location in {{ap_verify}}.",3
"DM-14558","05/23/2018 12:48:19","Load all DIASources at once in AssociationTask.update_dia_objects","Currently, the update_dia_objects method within AssociationTask performs separate loads for each DIAObject it updates. This ticket will change this to one access to the database for all DIASources that are associated with the updated DIAObjects.",2
"DM-14560","05/23/2018 14:10:30","Assist with KPM30 tests.","Debug Qserv shared scan scheduling and related issues for KPM30.",13
"DM-14572","05/24/2018 13:52:30","Modernize documenteer package deployment","Modernize Documenteer's PyPI deployment by:   * switch to setuptools_scm (common standard)   * use conditionals for PyPI deployment so only one matrix job deploys to PyPI.",0.5
"DM-14580","05/25/2018 11:14:02","Create tests for BestSeeingWcsSelectImagesTask.","Due to travel it was preferable to review DM-11953 without a unit test.  This ticket is to create the missing tests, possibly with advice from [~rowen].",2
"DM-14592","05/29/2018 15:55:58","Add default .user_setups file to Jupyter environment","It would be very helpful if the jupyter environment had a {{.user_setups}} file by default, with just a comment saying what the purpose of the file is.",0.5
"DM-14593","05/29/2018 17:46:23","Clarify use of repo subdirectory in ap_verify_hits2015 dataset","The README of ap_verify_hits2015 currently says the following about the {{repo}} subdirectory: ""Butler repo into which raw data can be ingested. This should be copied to an appropriate location before ingestion. Currently contains the appropriate DECam {{_mapper}} file.""    While this text does suggest a user should copying {{repo}} to a new location and then somehow make ingestion happen, it would benefit from clarification. Specifically:    1) The README should make it clear that the {{repo}} directory should not be altered in-place, as edits to a local copy of ap_verify_hits2015 would prevent users from git-pulling any changes,    and    2) Documentation about ingestion for this specific dataset should be added/linked appropriately (presumably via ap_verify's ingest_dataset.py script).",2
"DM-14594","05/30/2018 01:57:54","Refactor Terraform ressources creations","Refactor TF setup for ease of use and edge cases",1
"DM-14597","05/30/2018 08:51:21","Multiband driver uses wrong method signature in runDetection","Multiband driver can run detection on a coadd, but this code path is rarely exercised. The method signature to detect coadd sources in pipe_tasks has changed, but was not updated in pipe_drivers. This ticket should fix the call to runDetection in detectCoaddSources",3
"DM-14612","05/30/2018 14:53:50","Fix race condition in new jointcal matrix dump test","[~rowen] discovered a race condition in {{test_jointcal_cfht_minimal.test_jointcalTask_2_visits_photometry()}}, because the deletion of the files being tested (added in DM-14509) occurs in {{tearDown}}. Easiest fix is to delete the files after they've been checked for.",0.5
"DM-14620","05/31/2018 11:54:41","Move ap_association DIA schemas their own module","Incorporating l1dbproto into ap_association requires definitions and mappings between afw schemas and db schemas. This ticket will expand the methods make_minimal_dia_*_schema to more closely approximate that of l1dproto and the DPDD. This will also create mappings between the ip_diffim schemas input into association and the DIA schemas.",2
"DM-14625","05/31/2018 17:16:18","Fix ndarray compiler warnings","Fix ndarray compiler warnings in the few remaining packages that have not been updated.    This consists of removing     {code}      if (_import_array() < 0) {          PyErr_SetString(PyExc_ImportError, ""numpy.core.multiarray failed to import"");          return nullptr;      }  {code}    and import of numpy headers from pybind11 wrappers",1
"DM-14627","05/31/2018 18:13:33","Firefly performance test and analysis through NB ","This is to capture the work effort in testing the performance of FIrelfy image display in NB deployed at Kubernetes commons in NCSA.          The work and suggestions are here:    The performance study was prompted by demos of the qa_explorer notebook at the DMLT, with the ginga and the Firefly backends. Using the lsst-lspdev notebook environment and Firefly server, I checked the performance of Firefly panning at zoom level=1 on a 4k by 4k image. Panning to a new position typically took 5-10 seconds but occasionally as long as 20. There is a strong caching effect when panning to a position previously visited, as then the pan typically took between 0.2 and 1 second.    With assistance from Simon, I was able to install the qa_explorer notebook and operate its interactive feature of selecting a star or galaxy from a scatter plot, and zooming and panning to its location on the closest coadd image. The ginga backend typically took a few to 5 seconds to display the desired location. The Firefly backend typically took 15-20 seconds for the display, zoom and pan.    The Firefly display backend always shows the entire image fit to a frame, before zooming and panning to the desired location. Performance could be improved by batching together the initial display, zoom and pan. Alternatively, if DAX imageserv were available it would be more efficient to make a cutout at the desired location and then display it — a solution we used in our forced photometry demo on the PDAC.    The show_fits method of FireflyClient will take an initial zoom level as an argument. For the qa_explorer application, it would be better to set the initial zoom level to 1, to skip the time-consuming display of the entire image. Currently the afw.display abstraction doesn’t support this — it was developed for ds9 where zooming is easy. The mtv method of afw.display could be modified to take optional arguments like zoom level and a position on which to center the display.     ",2
"DM-14628","05/31/2018 18:13:38","meas_astrom pytest setup is missing E266","The setup.cfg file is correct for flake8, but not for the flake8 extension of pytest. This apparently causes building meas_astrom to fail on my Mac, but not on lsst-dev.    More generally, it would be good if there were a way to insist the flake8 exclusions were identical to the pytest flake8 exclusions to prevent this kind of problem. That's outside the scope of this quick fix ticket, however.",0.5
"DM-14630","05/31/2018 23:04:11","Use overlay2 on openstack","Some tuning is required to use overlay2 on Centos7. It will be done on openstack testbench.",2
"DM-14635","06/01/2018 12:21:59","Add support for pagmo2 optimizers in pyprofit","The pyprofit ([https://github.com/lsst-dm/pyprofit)] code originally supported only the optimizers from scipy.optimize, using L-BFGS-B by default. I added support for using pagmo2 ([https://github.com/esa/pagmo2/|https://github.com/esa/pagmo2/)]), an optimization library with a wider array of optimizers and interfaces to other libraries, as well as both python and C++ interfaces. I tested a few examples and L-BFGS-B with numerical gradients seems to work better than derivative-free optimizers for N<20 parameters, but it remains to be seen how general that result is.    Pycharm run configurations for HSC tests are available here; this will be kept up to date (see also DM-14647):    [https://github.com/lsst-dm/pyprofit/blob/master/.idea/runConfigurations/pyprofit_fit_hsc.xml]    At the time of writing, the command to run the HSC example is    python3 $PROJECT_DIR$/examples/hsc.py    Galaxy cutout arguments:    -radec 134.67675665 0.19143266 -size 19.9asec    Arguments to test pagmo vs scipy optimize (enabling automatic gradients for pagmo):    -optlib pygmo -grad 1    -optlib scipy    The -algo <algorithm> flag can be set, but valid arguments depend on the optlib:    [https://esa.github.io/pagmo2/docs/python/algorithms/py_algorithms.html#pygmo.cmaes]    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html]",2
"DM-14636","06/01/2018 12:42:48","Test ProFit and GalSim galaxy modelling speed and accuracy","I wrote and ran some systematic tests in pyprofit to compare the accuracy and speed of galaxy profile integration and convolution using pyprofit/libprofit and GalSim. When evaluating the profile and convolving with an analytic PSF, GalSim is considerably faster for n<4 and more accurate in most cases, especially for small axis ratios. However, ProFit can be faster for large Sersic indices (n>4) and supports an essentially infinite range, whereas GalSim is limited to 0.3<n<6.2. Furthermore, the tests I ran used 3x oversampling in libprofit, which has further room for optimization - as does libprofit's profile integration scheme. Thus, it's worth keeping support for libprofit in the future.    pyprofit code: [https://github.com/lsst-dm/pyprofit]    pyprofit arguments to generate a table of benchmarks (in pycharm run configuration format, which is human-readable xml): https://github.com/lsst-dm/pyprofit/blob/master/.idea/runConfigurations/pyprofit_bench_integ_long.xml    benchmarking notebook: [https://github.com/lsst-dm/modelling_research/blob/master/jupyternotebooks/pyprofit_benchmarks_plot.ipynb]    The plots should render better in a browser now, but I attached some copies anyway.",3
"DM-14637","06/01/2018 12:59:18","Test using Tractor for source modelling","Tractor ([http://thetractor.org|http://thetractor/], [https://github.com/dstndstn/tractor/]) is a python/C++ package for Bayesian source modelling, much like ProFit ([https://github.com/icrar/ProFit)|https://github.com/icrar/ProFit).]. However, Tractor has a number of extra useful features, including support for multi-band fitting, integration with astrometry.net and built-in descriptions of multi-component models. I installed Tractor and adapted the existing SDSS test to fit HSC images via the quarry tool ([https://hsc-release.mtk.nao.ac.jp/das_quarry/|https://hsc-release.mtk.nao.ac.jp/das_quarry/)]); the script is attached. Unfortunately, I found Tractor to be rather sparsely documented and not entirely intuitive to use, with custom optimizers that would be difficult to maintain. Furthermore, it has a lot of python2-isms and some C++ code wrapped with Swig which would require significant effort to make compliant with LSST standards, and so I don't think it's worth pursuing further. It may be worth contacting the Tractor authors (mainly Dustin Lang), though.",2
"DM-14648","06/01/2018 13:28:02","Add GalSim support to pyprofit","pyprofit previously exclusively used libprofit to generate models; I added support for integrating and convolving with empirical PSFs using galsim. Analytic PSFs are a WIP.     Changes are mainly in make_model_galsim: [https://github.com/lsst-dm/pyprofit/blob/master/python/profit.py]    Pycharm run configurations for HSC tests are available here; this will be kept up to date (see DM-14647):    [https://github.com/lsst-dm/pyprofit/blob/master/.idea/runConfigurations/pyprofit_fit_hsc.xml]    At the time of writing, the command to run the HSC example is    python3 $PROJECT_DIR$/examples/hsc.py    Galaxy cutout arguments:    -radec 134.67675665 0.19143266 -size 19.9asec    Enable galsim:    -galsim 1",2
"DM-14668","06/04/2018 08:33:48","Templates for EUPS table files should not include versions","We no longer recommend the use of versions in the {{setupRequired}} or {{setupOptional}} statements in hand-written table files.  Those table files are automatically expanded to include exact versions when products are installed, which provides much more rigorous dependency version handling.  The version inequalities we used to include manually never provided much rigor, and provide none now that stack packages are versioned together.",1
"DM-14673","06/04/2018 14:13:53","Label for marker disappears when it should not","# Display an image   # add a marker   # make the marker bigger   # move it around   # Please notice when any edge of the indication box is out of image, the label disappears.    Fix suggestion:    Check the preference of where the indication box should be,  when that corner (plus some margin) is in the image, the label should be displayed.          Copied from GitHub: (6/13/2018):    This development fixes the following issues,   * when the marker/footprint is moved towards the edge of the display area, the label for the marker/footprint disappears.  fixing: showing the label when the marker/footprint is moved out of plot area and showing the label in alternate corner location in case the label is being moved out of plot area.   * when the marker/footprint is relocated farther from the center or the target of the HiPS, the footprint is shown out of shape as the image is zoomed.  fixing: instead of translating each region component on image domain as regular image, each component is re-rendered in world coordinate per original region description.    test:  start image search  add a marker or footprint overlay  move the marker/footprint to anywhere close to the plot border and view the text display    start HiPS search (no FOV entered)  add a footprint overlay  click (relocate) at a point close to the curved border of the HiPS  zoom the image to see more detail of the footprint by zooming and rotating the HIPS",3
"DM-14674","06/04/2018 15:26:24","Extend class Chunker in package sphgeom to validate chunk numbers","Extend class Chunker of package sphgeom with a method testing chunk validity for a given partitioning configuration. This method is needed by Qserv replication system.",1
"DM-14685","06/05/2018 12:11:03","Replace afw.geom with lsst.geom in ap_association","DM-14429 moved afw.geom into lsst.geom. Since ap_association is not yet part of lsst_distrib this change did not propogate through the package. This ticket will change app instances of afwGeom to geom.",2
"DM-14690","06/05/2018 13:01:59","Add ability to construct centered boxes","As discussed on [GitHub|https://github.com/lsst/afw/pull/357#discussion_r192899255], it would be useful if {{Box2I}} and {{Box2D}} had methods for creating boxes centered on a particular (fractional) point.    Given that the {{Box*}} classes cannot afford more constructors, and we still don't have a good way to discover functions, this capability is best implemented as static factory methods:  {noformat}  Box2I Box2I::makeCenteredBox(Point2D const& center, Box2I::Extent const& size);  Box2D Box2D::makeCenteredBox(Point2D const& center, Box2D::Extent const& size);  {noformat}    I do not plan to add a {{Box2I::makeCenteredBox(Point2I, Extent)}} method: I'm worried that it might lead to confusion with {{(Point2D, Extent)}}, and as [~jbosch] pointed out it's not clear what the best behavior is for even-sized boxes.    The desired functionality is already present in the implementation for {{Exposure::getCutout}}, so this work is just a matter of factoring the code, writing new unit tests, and being very careful about the distinctions between {{Box2I}} and {{Box2D}}.",2
"DM-14694","06/05/2018 16:26:42","Pixel coordinates off by half in Firefly relative to LSST conventions","When using {{lsst.afw.display.dot}} with the Firefly backend, positions appear at a half-pixel offset from where they should; LSST convention is to label the *center* of the lower-left pixel (0, 0).    I also see the same half-pixel offset in the mouse pixel coordinates displayed in the upper right corner of the Firefly GUI, so I'm guessing this is just because Firefly (presumably) using a coordinate convention in which integers are pixel boundaries rather than pixel centers.  If it's possible to address that, it'd be very nice to also include the ""xy0"" offset our image objects carry as well (note that {{dot}} already takes care of this offset correctly, presumably by removing it in Python before passing the position to Firefly).    I'd really appreciate a quick fix so I don't have to work around this in the tutorial notebooks for LSST@Europe3 next week.   If the only short-term options is a workaround for {{dot}} in the display_firefly Python code, I can do that much myself; please let me know if I should.",2
"DM-14695","06/05/2018 16:32:31","Mask overlays missing with more than one call to mtv on the same display object","When using the Firefly backend for {{lsst.afw.display}}, the first {{MaskedImage}} or {{Exposure}} displayed via {{Display.mtv}} has a wonderful mask overlay (much better than DS9's!).    Unfortunately, if I call {{mtv}} a second time on the same {{Display}} instance, the mask overlays do not appear, even though the image itself is updated.    If I close the image window in Firefly itself before calling {{Display.mtv}}, things work as expected.    If a quick fix is possible, it'd be great to get it in this week so I can avoid demoing the workaround at LSST@Europe3 next week (but it's not a terrible workaround if quick fix is not possible).",2
"DM-14699","06/06/2018 09:16:51","Silence NumPy FutureWarnings in meas_deblender","NumPy is changing the default behavior of {{numpy.linalg.lstsq}}'s {{rcond}} parameter, and now warns on any call that uses the default.  meas_deblender does that extensively, resulting in lots of warnings.",0.5
"DM-14712","06/06/2018 19:43:08","PPDB Performance test on NCSA Oracle instances","Run ap_proto against Oracle instance at NCSA and extract some performance numbers from these tests.",20
"DM-14715","06/07/2018 10:46:41","Fix deployment issues for recent tech notes","Please sort out travis and deployment for recent tech notes: DMTN-080 (which was created manually), 81, 82, and 83, 84. The latter 4 were created by sqrbot but deployment credentials were not created (I did turn on travis manually).",0.5
"DM-14725","06/07/2018 13:45:12","Eliminate explicit use of ndarray::EigenView in C++ code","ndarray::EigenView relies on undocumented internals of Eigen. Stop using it, in order to allow upgrading Eigen.    The full conversion consists of two parts:  1) Stop using {{ndarray::EigenView}} explicitly in C++ code. That is what this ticket is about.  2) Stop using {{ndarray::EigenView}} indirectly via {{ndarray::Array::asEigen}} by having that function return an {{Eigen::Map}}. That is DM-14728",1
"DM-14727","06/07/2018 14:20:42","Add header service requirements into model and make RFC","Take the requirements spreadsheet from [~felipe], add it into the MagicDraw model in LDM-638, make the document, and submit to CCB for review.",2
"DM-14732","06/08/2018 06:21:01","Regions appear on subsequent afw Displays with Firefly backend","When symbols are overlaid on an {{afw.display.Display}} with the Firefly backend, subsequent {{mtv}} commands will have the regions layer from the first display also overlaid. This regions layer is not removed by using {{erase}}.    ",2
"DM-14734","06/08/2018 10:23:22","Allow zoom to be set before mtv in afw Displays for Firefly backend","A common pattern for using {{afw.display}} is to display an image with a zoom level. With the Firefly backend, currently it is necessary to {{mtv}} (display) the image and then issue a zoom command. This displays the entire image, which can take some time for large images, and then applies the zoom. This improvement is for the backend to keep track of the last commanded zoom level, and to include it in the call to {{FIreflyClient.show_fits}}.",2
"DM-14741","06/08/2018 14:50:19","Set Cores for Firefly docker container","Until we go to Java 10 we need a way to control the number of  cores the firefly docker container uses.         The docker container will name allow for a environment variable JVM_CORES",1
"DM-14742","06/08/2018 15:38:47","Let ap_verify run ap_pipe with dataset-specific configs","{{lsst.ap.verify.DatasetIngestTask}} currently accepts configs from a dataset, allowing for dataset-specific file masks and similar features. It would be useful for {{ap_verify}}'s calls to {{ApPipeTask}} to have the same capability, so that dataset-specific options (for example, the distinction between ""deep"" and ""goodSeeing"" templates) can be set for the instance(s) of {{ApPipeTask}} created by {{ap_verify}}.    Unlike {{DatasetIngestTask}}, {{ApPipeTask}} does not need special support for obs-specific configs as it already provides them automatically as a {{CmdLineTask}}.",2
"DM-14747","06/11/2018 02:04:33","Change Timeouts for webserv to 2 hours","Make sure the blocking timeout for webserv is close to 2 hours.",2
"DM-14749","06/11/2018 06:57:21","Update Qserv deploy documentation","Update READMEs and cleanup examples",1
"DM-14758","06/11/2018 13:00:33","Text defined in region 'text' is not displayed","the following region text sample has no text display:     text 2101 2131 ""my label""",1
"DM-14759","06/11/2018 13:08:31","Support XY0 for image readout and make it the default in slate.html","Add support for XY0 using LTV1/2 or CRVAL\{1/2}A.    XY0 will be default for slate.html entry point. Therefore it will be the default for the API.",2
"DM-14763","06/11/2018 22:38:37","Improve region ID handling in display_firefly","When a display is redefined, the region layer Id is set to None. When a display is erase, the region layer Id is also set to None. When a display is redefined and then a region is added, the previous regions show up as well.    Since for {{afw.display}} we always use ""lsstRegions"" plus the frame number, there is no need to set the regionLayerId to None.    Alternatively, if a display is not re-defined in a Python session then the problem is avoided.",2
"DM-14772","06/12/2018 13:50:31","Allow ""."" and ""_"" in LTD edition names for LSST Science Pipelines EUPS tags","Allow ""."" and ""_"" in LTD edition names since those are used for Git and EUPS tags by the LSST Stack (LSST Science Pipelines). This is needed to properly version and deploy pipelines.lsst.io from the LSST Science PIpelines' code base.",0.5
"DM-14774","06/12/2018 17:18:50","Upgrade Plotly library","We are using plotly-1.28.2.min.js As of today, the latest release is plotly-1.38.3.min.js.    A lot of bugs in scattergl have been fixed, we might want to use scattergl for larger scatters.    One of the bugs still present in scattergl is disappearing error bars on relayout. For this reason, we might not want to use scattergl as a default plot when the number of points is reasonably low.",8
"DM-14778","06/13/2018 11:56:51","Fix asinh ","We still do not have asinh quite right, the following should be done:   * Determine what is wrong, maybe why it is wrapping?   * Understand why have a beta parameter exposed to the user and not a Q parameter. You might want to talk to Lijun about this since she did the original work but no longer works for LSST.   * From the UI- determine if we we need a slider entry instead of a number text box entry. Note- when asinh was originally done we had not yet brought in a slider component. It makes more sense now.   * Work with [~shupe] to validate the implementation plan   * FitsRead.java has most of the stretch code as static methods.  Move these out of FitsRead.java into a Stretch.java   * [~shupe] thinks we have the same problem with Power law Gamma.  If that is a simple fix after the above work then fix it, otherwise we will make a second ticket.    6/25/2018    This ticket will deal with changing asinh algorithm to be consistent with asinh stretch implemented in [https://github.com/lsst/afw/blob/master/python/lsst/afw/display/rgb.py] and [https://github.com/astropy/astropy/blob/master/astropy/visualization/lupton_rgb.py]    The parametrization using Q is explained in the footnote on page 3 of [https://arxiv.org/pdf/astro-ph/0312483.pdf]    The algorithm will accept Q parameter from 0.1 to 10, which should be controlled by a slider.   The mapping from flux to color value is {{255 * 0.1 * asinh(Q*(x-xMin)/(xMax-xMin)) / asinh(0.1*Q)}}    Below xMin, the color will be 0; above xMax, the equation has to be applied and then clipped to 244 (because we use 255 for blank pixel).    Per [~shupe]  {quote}Zscale + linear is often used with CCD images to show faint features. I am thinking of Lupton’s asinh as keeping zscale + linear at low values, and bending the stretch function over at intermediate and large intensities. It uses only a fraction of the color range for the max value from zscale; but it will show brighter features above zscale.   Lupton’s formulation assumes that xMax is far below the bright features in the image. He wants to see the features above xMax as computed by zscale. This is different from how I have traditionally thought about these stretches.  {quote}     ________________    The first and last items in the original description above were fixed by DM-14780.      Stretch code will stay in FitsRead.java to avoid conflicts with Lijun's IRSA-1498 ticket, which takes care of code refactoring.    Per [~shupe], it would also be helpful to display boundaries, calculated by z-stretch algorithm rather than the original data boundaries. I will see if I can add the calculated values to z-stretch dialog.",8
"DM-14779","06/13/2018 12:27:19","make last Git commit ID available in Firefly build","As we have more projects using Firefly at different stages of its development, it is necessary for project to know which version of Firefly it is using.  Git commit ID uniquely identifies the point of the build.    * provide an API call to get the commit ID string   * display the commit ID as part of the build information   ** dev currently: ""v1.0.0_Development-0 Built On:Sat Jun 09 18:05:11 PDT 2018""   ** IRSA ops: v3.0.0_Final-2629 Built On:Tue May 15 16:07:17 PDT 2018    We may want more discussion on how/where to display this in operations.     Implemented as the following:    Show a detail view of the version info when the shorten version is clicked.   * click on the version info on the lower-right corner to see the details    Exposes the same information via the JS API as well as from Java server-side environment.   * In JS, open a demo page, open console. In console, enter {{> window.firefly.util.getVersion()}}",2
"DM-14780","06/13/2018 14:47:06","Data values in lower and upper range are not mapped correctly for power law and asinh stretch","The attached screenshot shows the problem caused by the  bug:    data values <= Lower range should be black, data values >= Upper range should be white.",2
"DM-14781","06/13/2018 17:22:51","Upgrade Eigen to 3.2.10","As part of DM-14305 upgrading Eigen to 3.3 we need to transition to a version new enough that pybind11 supports it. The earliest version of Eigen that our pybind11 supports is 3.2.7. This intermediate step is required in order to release a version of ndarray that no longer uses EigenView for pybind11, and thus allows us to switch to the standard pybind11 wrappers.    I will go to 3.2.10, the latest 3.2 release, in order to get as many bug fixes and other improvements as we can.",1
"DM-14785","06/14/2018 12:01:33","Make sure .user_setups lands in the right place","Currently, the system looks for a the {{.user_setups}} file in {{$HOME}} and adds a template file there if it doesn't exist.  It should, instead, be looking in {{$HOME/notebooks}} as that is where the file is actually sourced.",0.5
"DM-14794","06/15/2018 10:10:21","Notebooks and Scientific Input for SQuaRE Science Platform services","This epic covers scientific input into SQuaRE Science Platform services including the  development of example notebooks appropriate to Commissioning and other early users,and science-level design input on aspects of SQuaRE developed services.",40
"DM-14804","06/15/2018 13:40:46","SelectionSet.load_single_package fails silently","When reading in the attached file via _lsst.verify.SelectionSet.load_single_package_, it returns an empty set. This means there is something wrong with the file, but the load fails silently. It should yell at me as to the issue with what I'm passing it.",0.5
"DM-14806","06/15/2018 16:29:25","convert stack demo to be a regular eups product","The ""demo"" currently exists as a special snowflake feature of jenkins jobs and is not packaged as an an eups product. A script is currently invoked by the CI machinery after an {{lsstsw/lsst_build}} or {{eups distrib install}} has completed which downloads the demo repo from github as a tarball.    An unfortunate consequence of this implementation is that changes on the master branch of the demo result in previous git tags no longer working with the demo when built by {{ci-scripts/lsstsw}} and requires knowledge of the correct git ref to use after a direct {{eups distrib install}}. This also presents an irritation when tagging an official release as there is no source of truth as to where the tag should be located, requiring human intervention. For at least the third time, the demo on master has changed during the release process and now fails with the current release candidate.    A much less error prone solution would be to convert the demo into a regular eups product, which is a dependency of either {{lsst_distrib}} and/or {{lsst_ci}}. This would result in demo metadata being incorporated into eups distrib tags and solving the science-pipeline/demo version mismatch problem both for end users with a local installation and under CI.    I believe the basic tasks to accomplish this would be:   - convert {{lsst/lsst_dm_stack_demo}} into an eups product – essentially add a {{ups}} dir and a table while which depends on {{lsst_apps}}   - add {{lsst_dm_stack_demo}} as a dependency of {{lsst_ci}} and/or {{lsst_distrib}}   - add a test script under {{lsst_ci/tests/}} to trigger a demo run   - remove {{lsst-sqre/ci-scripts/runManifestDemo.sh}} and update {{lsst-sqre/ci-scripts/lsstswBuild.sh}} to not run the demo   - update various jenkins jobs in {{lsst-sqre/jenkins-dm-jobs}} to {{setup lsst_dm_stack_demo}} rather then invoking {{runManifestDemosh.sh}}",5
"DM-14812","06/18/2018 12:20:16","Make alert printer print every Nth alert","Current alert_stream alert deserializer/printer will print all received alerts to stdout, which will overwhelm the logs.  Change this to every Nth alert as a command line argument.",0.5
"DM-14814","06/18/2018 13:24:19","Change invalid pixel handling by Exposure::getCutout","Currently, {{Exposure::getCutout}} returns ""blank"" pixels (in the sense of e.g. {{Exposure(dimensions)}}) for parts of the cutout that extend off the edge of the original image. The more natural behavior to veteran stack users is to fill the off-image pixels with the value of [{{afw::math::edgePixel}}|http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/namespacelsst_1_1afw_1_1math.html#a44ebf02a4fe421404fe9cbd6e9a6f699].    I propose that, if the {{NO_DATA}} flag has been deleted (e.g., with {{Mask::removeMaskPlane}}), {{getCutout}} should return edge pixels with a blank mask but with value and variance conforming to {{edgePixel}}'s behavior. However, I'm open to throwing an exception instead.    This ticket shall modify {{getCutout}} as described and add unit tests for pixel values, which we neglected to do before.",2
"DM-14819","06/18/2018 15:06:21","Refactor LoadReferenceObjectsTask for SuperTask compatibility","Some of the work currently done by LoadReferenceTask - the actual lookup of which shards overlap an area of interest - will be done by preflight (outside Task code) in the Gen3 era.  That will ultimately let us greatly simplify it, but in the meantime we will need to make sure it provides APIs appropriate for both CmdLineTask usage and SuperTask usage.    Reference catalogs also have a lot in common with catalogs of simulated objects we might want to insert into real images - they'll have different columns, but we'd want to shard them and look them up the same way.  We should look for opportunities in this refactor to support that use case as well.     ",8
"DM-14822","06/18/2018 15:52:08","Gen3 get/put with DatasetRef only","We've accidentally made it a bit painful to use the DatasetRef and Butler classes together, because Butler's {{get}} and {{put}} interfaces require one to unpack the contents of a DatasetRef.    Make sure {{Butler.get(datasetRef)}} and {{Butler.put(obj, datasetRef)}} work.    Note that the former is not just a renaming of {{Butler.getDirect}}; it should not permit loading datasets outside the {{Butler}}'s collection, and it should not require the {{DatasetRef}} to have already been associated with a dataset_id (though it should take advantage of one, if present).     ",2
"DM-14824","06/18/2018 16:12:05","Add syntactic sugar for ConfigFields of *DatasetConfigs","As we discovered in the SuperTask conversion kickoff meeting, writing (e.g.)  {code:java}  myInput = ConfigField(      dtype=InputDatasetConfig,      doc=""Input image dataset type definition."",      default=InputDatasetConfig(          name=""name"",          units=(""Camera"", ""Visit"", ""Sensor""),          storageClass=""Exposure""      )  ){code}  is much too verbose.    Add custom {{Field}} objects so the above can be written as  {code:java}  myInput = InputDatasetField(      doc=""Input image dataset type definition."",      name=""name"",      units=(""Camera"", ""Visit"", ""Sensor""),      storageClass=""Exposure"")  ){code}",2
"DM-14827","06/18/2018 17:44:47","Update WISE table references to use logical db name from metaserv","Test WISE metadata and update the references to WISE tables to use logical db.",1
"DM-14834","06/19/2018 10:11:52","Use pybind11's native Eigen wrapping instead of ndarray EigenView","Update our pybind11 wrappers as needed to use pybind11's native Eigen support. This is necessary in order to upgrade to Eigen 3.3.    Changes include:  - Build ndarray without EigenView support and with native pybind11 Eigen support. The latter is not necessary but avoids the need to change all our wrappers that wrap code that uses Eigen to explicitly import pybind11/eigen.h  - Update code and tests and needed. For example {{geom}} has one failing test because pybind11's Eigen wrappers are more lenient than ndarray, so it is now possible to construct an lsst.geom.Extent2I from an lsst.geom.Extent2D.",2
"DM-14837","06/19/2018 11:02:59","Firefly improvements to intro-with-globular notebook","Some improvements are available for the Firefly parts of [~jbosch]'s intro-with-globular notebook used in the Lyon 2018 hands-on session and later demos.   * Define the Display so the proper URL is available before the IFrame cell, so that `lsp-demo.lsst.codes` need not be hardcoded into the notebook   * Dial down mask transparency via afw.display   * Show how the {{firefly_client.plot}} convenience module can upload the catalog and overlay on the images.",1
"DM-14840","06/19/2018 14:03:05","Make mask transparency and color ""sticky"" in display_firefly","{{afw.display}} provides methods for setting mask plane colors and transparencies. These should be ""sticky"" in the display_firefly backend, meaning that once they are set for a Display object, they should be applied when an image is sent again to that display.    The display_firefly backend also needs to ignore masks whose color is set to ""ignore"" or ""IGNORE"".     Related to this, {{afw.display}} provides {{setDefaultMaskTransparency}} and {{setDefaultMaskPlaneColor}} which are used when Display instances are created. Fix a small bug in {{setDefaultMaskTransparency}} and verify that both of these work with the display_firefly backend.",3
"DM-14841","06/19/2018 14:28:51","NERSC password file has moved so fd leak checker fails tests","As reported on Slack, {{pipe_tasks}} is failing to build at NERSC with:  {code}  File open: /var/lib/sss/mcpath/passwd   {code}    {{utils}} has a filter for {{/var/lib/sss/mc/passwd}} (from DM-7186) so it seems that file moved in the past few weeks. Update the filter to be more general.",0.5
"DM-14842","06/19/2018 15:00:14","Fix deprecation warnings from PropertyList/Set.get","Fix deprecation warnings from PropertySet.get and PropertyList.get by switching to getScalar or getArray",3
"DM-14844","06/19/2018 16:09:19","Two FITS tests in afw assume they run relative to AFW_DIR","Tests in test_propertyListPersistence.py and test_footprint.py assume that test FITS files are in {{tests/data}} rather than looking for them relative to the location of the test file. This breaks them if you attempt to run the afw tests from outside AFW_DIR. The fix is to use {{__file__}}. There may also be an executable C++ test that fails in the same way.",0.5
"DM-14846","06/19/2018 17:21:38","display_ginga won't build","The {{display_ginga}} package has configuration issues that prevent it being built.    Specifically, there is a missing import and a missing package setup.",1
"DM-14847","06/19/2018 18:09:25","Missing keys in association DB","The changes to the DB schema in DM-14620 have invalidated -flux handling in {{ap_pipe}} and- metrics computations in {{ap_verify}}. This prevents {{ap_verify}} or its tests from running. Fix things so that all unit and integration tests pass.",1
"DM-14848","06/19/2018 18:15:21","ingest_dataset.py must be run from final working directory","{{ingest_dataset.py}} internally uses relative paths for calibration files, causing failed defect lookups if later tasks are run from a different directory than {{ingest_dataset.py}} was. Use absolute paths throughout and test that the resulting repositories can be passed as-is to {{ap_pipe}}.",1
"DM-14849","06/19/2018 18:43:18","Metadata-based metrics not measured","Integration tests of {{ap_verify}} produce all expected metrics extracted from the Butler repository or the association database, but none from {{ApPipeTask}} metadata -- not even timing measurements. Find out what is going on and add integration tests and, if possible, regression tests to {{ap_verify}}.",2
"DM-14861","06/20/2018 16:16:23","Disable CC requirement for obs_base","Remove the need for a C++ compiler as it's python-only.",1
"DM-14863","06/21/2018 11:47:48","Place initial data backbone requirements in model and generate document","[~mgower] has sent me an initial set of requirements for the Data Backbone Services. I will put them in the model and upload an initial draft to DocuShare.",1
"DM-14867","06/21/2018 17:57:53","Firefly API: user can't replace a failed image coverage","When using Firefly API (i.e. Gator case), if the image search fails, then the user can't replace the image anymore because toolbar is not accessible anymore.    To test the problem: Go to Gator, do a 100"" search on WISE around position '298.0 29.87' and change coverage image with SDSS u band then try to change back again.    Please fix, thanks.",1
"DM-14870","06/22/2018 10:50:02","eups.lsst.codes sync from s3 does not update objects of identical size","[~fritzm] reported yesterday that the {{qserv-dev}} eups distrib tag was not updating after being published.  The s3 object was confirmed to be correct but was not syncing to the k8s service.  It was assumed at the time that this was a random case of s3 eventual consistency taking an excessively long time and the k8s pod was always getting an old version of the object.  However, > 12 hours seems excessive for this.    Upon further investigation this morning, it appears that {{aws s3 sync}} from {{awscli}}, which is used to to perform the sync, *does not* checksum the local file to determine if it is in-sync with s3.  All it does it look at the file size by default, and can optionally compare timestamps (which isn't enabled) -- there is no option to force checksums (ie., {{rsync -c}}).  This is rather unfortunate as s3 does have an {{ETag}} (md5) for all objects. Eg.,    {code:java}  $ aws s3api head-object  --bucket eups.lsst.codes --key stack/src/tags/qserv-dev.list   {      ""AcceptRanges"": ""bytes"",      ""LastModified"": ""Fri, 22 Jun 2018 01:14:45 GMT"",      ""ContentLength"": 2495,      ""ETag"": ""\""04d0d2da6b4b1107bb03453177813201\"""",      ""VersionId"": ""null"",      ""ContentType"": ""binary/octet-stream"",      ""Metadata"": {}  }   $ aws s3 cp s3://eups.lsst.codes/stack/src/tags/qserv-dev.list .  download: s3://eups.lsst.codes/stack/src/tags/qserv-dev.list to ./qserv-dev.list   $ md5sum qserv-dev.list   04d0d2da6b4b1107bb03453177813201  qserv-dev.list  {code}    Demonstration that the s3 object and the stale eups.lsst.codes file are the same size:    {code:java}  [root@pkgroot-rc-jh4lf /]# grep BUILD= /var/www/html/stack/src/tags/qserv-dev.list  #BUILD=b3668  [root@pkgroot-rc-jh4lf /]# ls -la /var/www/html/stack/src/tags/qserv-dev.list  -rw-r--r-- 1 root root 2495 Jun 21 12:42 /var/www/html/stack/src/tags/qserv-dev.list    $ grep BUILD= qserv-dev.list   #BUILD=b3670  $ ls -la qserv-dev.list   -rw-rw-r--. 1 jhoblitt jhoblitt 2495 Jun 21 18:14 qserv-dev.list  {code}    ",1
"DM-14874","06/22/2018 13:11:50","Add options to select against duplicate coadd sources","We should only ever be plotting the ""primary"" sources for coadd QA-ing.  The description for the *detect_isPrimary* flag is:  {code:java}  doc=""true if source has no children and is in the inner region of a coadd patch and is in the inner region of a coadd tract""{code}  We are currently selecting on *deblend_nChild* *= 0* (which is appropriate at the visit level).",5
"DM-14878","06/24/2018 20:43:02","generateAcronyms.py seems to not complain on missing acronym ","I noticed that the acronym DBA did not show up in my generated acronyms. There is a lot of output which is not needed so it may have complained.    When I added it to myacronyms.txt it showed up fine .. now I wonder if I am missing others ...    [~tjenness] I may try to look at this next week :) unless its just too verbose output",1
"DM-14879","06/25/2018 06:24:24","Use last Qserv version in Qserv deploy","Update Qserv version with the last container image in Qserv deploy tool",2
"DM-14885","06/25/2018 17:24:26","Use either nanoseconds or MJD in PPDB CcdVisit table","The prompt product database currently has a column called expMidptMJD which is in units of seconds. These timestamps are constructed via {{exposure.getId().getVisitId().getDate().nsec() * 1.e-9}}, which means zero seconds corresponds to the daf_base DateTime nanosecond zeropoint. However, none of this is clear without some digging.    Since this field has MJD in the name, it would make sense to have MJD as units here. Another option would be to return nanoseconds and change the column name accordingly, since that is the default time unit for our daf_base DateTime objects. Either way, MJD should be a float and nanoseconds should be an int in order to have consistent behavior with DateTime.",2
"DM-14886","06/25/2018 18:49:10","multiple filters on a heat map failed ","reproduce   # do a search on WISE Multiepoch photometry table with the example polygon:  20.7 21.5, 20.5 20.5, 21.5 20.5, 21.5 21.5   # use the rectangle filter over the image, a tiny rectange returned about 2000 entries   # do any filter on image, table column, or on plot, it will fail. Error message look like this    Column not found: decimate_key(ra,dec,20.5099374,20.500176,100,100,0.00990003299934003,0.009995321952220013)    Analysis guess:    This only happens when the plot is heatmap. ",3
"DM-14894","06/26/2018 11:56:33","Add pipeBase.timeMethod to score and match methods in AssociationTask","Timing the match scoring and matching methods would be useful in the context routing out slow downs in the association step of ap_pipe.",1
"DM-14910","06/26/2018 16:13:34","Firefly zscale differs from other implementations","The upper output (z2) of Firefly's zscale implementation is often significantly greater than z2 from other implementations, including those in ds9, astropy and lsst.afw.display.    Comparison on coadd file calexp-HSC-I-9813-4,4.fits:  ||Implementation||Contrast (%)||Nsamples||Samples_per_line||z1||z2||  |Firefly|25|600|120|-0.0491993|0.409582|  |ds9|25|600|120|-0.0491993|0.145803|  |lsst.afw.display.displayLib|25|600|N/A|-0.0577342|0.155637|  |astropy.visualization|25|600|N/A|-0.0550090|0.170189|  |Firefly|25|1000|120|-0.0487988|0.951774|  |ds9|25|1000|120|-0.0487988|0.15138|  |lsst.afw.display.displayLib|25|1000|N/A|-0.0577120|0.165587|  |astropy.visualization|25|1000|N/A|-0.0535599|0.171046|  |Firefly|80|600|120|-0.0379127|0.132309|  |ds9|80|600|120|-0.0373926|0.0524557|  |lsst.afw.display.displayLib|80|600|N/A|-0.0392628|0.0535469|  |astropy.visualization|80|600|N/A|-0.0437599|0.0578543|     ",2
"DM-14914","06/27/2018 09:09:41","ccutils from ci-scripts appears to be unsetting error exit","I've seen build failures a few times over the last several month that should have died earlier then then did.  This was observed again last night with a tarball build and my theory is that {{ccutils}} from {{ci-scripts}} is unsetting {{errexit}} as an accidental side effect (the tarball build would have failed either way from what appears to be a broken eups install).    From https://ci.lsst.codes/blue/organizations/jenkins/release%2Ftarball/detail/tarball/2958/pipeline :    {code:java}  + . ./loadLSST.bash    ++ export PATH=/build/python/miniconda3-4.3.21/bin:/opt/rh/devtoolset-6/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin    ++ PATH=/build/python/miniconda3-4.3.21/bin:/opt/rh/devtoolset-6/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin    ++++ dirname ./loadLSST.bash    +++ cd .    +++ pwd    ++ LSST_HOME=/build    ++ EUPS_DIR=/build/eups/2.1.4    ++ source /build/eups/2.1.4/bin/setups.sh    +++ export EUPS_SHELL=sh    +++ EUPS_SHELL=sh    +++ export EUPS_DIR=/build/eups/2.1.4    +++ EUPS_DIR=/build/eups/2.1.4    ++++ echo /build/eups/2.1.4    ++++ sed -e 's/ /-+-/g'    +++ eupslocalpath=/build/eups/2.1.4    ++++ /build/python/miniconda3-4.3.21/bin/python -E -c '    from __future__ import print_function    import sys    pp = []    for d in sys.argv[1].split("":""):        if d and d not in pp:            pp += [d]    if not sys.argv[2] in pp:        pp = [sys.argv[2]] + pp    print("":"".join(pp))' '' /build/stack/miniconda3-4.3.21-10a4fa6    +++ export EUPS_PATH=/build/stack/miniconda3-4.3.21-10a4fa6    +++ EUPS_PATH=/build/stack/miniconda3-4.3.21-10a4fa6    +++ _eups_path=/build/stack/miniconda3-4.3.21-10a4fa6    ++++ /build/eups/2.1.4/bin/eups_setup DYLD_LIBRARY_PATH= eups -r /build/eups/2.1.4    setup: Unable to take shared lock on /build/stack/miniconda3-4.3.21-10a4fa6: an exclusive lock is held by [user=jenkins-slave, pid=590]    +++ eval false    ++++ false    +++ export 'SETUP_EUPS=eups LOCAL:/build/eups/2.1.4 -f (none) -Z (none)'    +++ SETUP_EUPS='eups LOCAL:/build/eups/2.1.4 -f (none) -Z (none)'    +++ export EUPS_PATH=/build/stack/miniconda3-4.3.21-10a4fa6    +++ EUPS_PATH=/build/stack/miniconda3-4.3.21-10a4fa6    +++ unset eupslocalpath _eups_path    +++ '[' X '!=' X -a -f /build/eups/2.1.4/etc/bash_completion.d/eups ']'    ++ export -f setup    ./loadLSST.bash: line 11: export: setup: not a function    ++ export -f unsetup    ./loadLSST.bash: line 12: export: unsetup: not a function    ++ export 'EUPS_PKGROOT=https://****/stack/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6|https://****/stack/src'    ++ EUPS_PKGROOT='https://****/stack/redhat/el6/devtoolset-6/miniconda3-4.3.21-10a4fa6|https://****/stack/src'    + for prod in lsst_distrib    + eups distrib install lsst_distrib -t d_2018_06_27 -vvv    /build/scripts/run.sh: line 36: eups: command not found    + export EUPS_PKGROOT=/distrib    + EUPS_PKGROOT=/distrib    + [[ -e /distrib ]]    + rm -f '/distrib/*.list'    + for prod in lsst_distrib    + eups distrib create --server-dir /distrib -d tarball lsst_distrib -t d_2018_06_27 -vvv    /build/scripts/run.sh: line 50: eups: command not found    + eups distrib declare --server-dir /distrib -t d_2018_06_27 -vvv    /build/scripts/run.sh: line 52: eups: command not found    script returned exit code 127  {code}  ",1
"DM-14915","06/27/2018 09:25:16","rewrite_shebang is not run in ctrl_orca  ","For some reasons {{ctrl_orca}} is missing the {{bin}} folder, at least in recent releases (e.g. docker images v15, w_2018_23, ...).  Usually a {{bin}} folder is created for packages with a {{bin.src}} folder during the build process. But the {{bin}} folder is not made in {{ctrl_orca}}; {{rewrite_shebang}} is not run in the build process.   This is also true in {{lsstsw}} build and the shared stack in {{/software}} on LSST machines.  Manually {{scons}}-ing it works. ",1
"DM-14919","06/27/2018 12:36:11","Pointer file error in ap_verify_testdata","I re-downloaded {{ap_verify_testdata}} and got the following error:  {noformat}  Pointer file error: Unable to parse pointer at: ""refcats/gaia_example.tar.gz""  Checking out files: 100% (21/21), done.  {noformat}  -The file is treated as an invalid symlink by unix.- The file is downloaded correctly, but Git will refuse to make (or even revert) any changes to it.    This appears to be a [bug in how the repository was created|https://github.com/git-lfs/git-lfs/issues/1828] in the first place. Fix {{ap_verify_testdata}} as described in the linked discussion, and re-download the other datasets to make sure they haven't been corrupted in the same way.",2
"DM-14921","06/27/2018 13:26:34","Remove Python 2 references from pipelines.lsst.io","With the {{v16_0}} release, the LSST Science Pipelines don't support Python 2.7 anymore. Thus we need to remove references to Python 2.7 from the installation documentation.",0.5
"DM-14928","06/28/2018 11:48:55","Fix error in DM-14765 implementation","Had forward reference to extracting instrument name.  Instrument name extraction wasn't using getCamera() but instead incorrectly treating object as its name.  Instrument name should be upper-case to match {{verify_metrics}} package definition.",1
"DM-14932","06/28/2018 13:29:40","Add utility functions for creating SkyWcss from boresight/rotator + cameraGeom","Given an accurate boresight and rotator angle from a camera, we should be able to construct our best initial estimate of the WCS for a sensor by combining that information with our own camera geometry, as this will probably produce better results that relying on any WCS provided directly by the camera.  In fact, for LSST, it's not clear that there even will be a sensor WCS provided directly by the camera, because LSST raw data has amps in different HDUs and no full-sensor HDU that could unproblematically hold a full-sensor WCS.    To do without using astshim code directly, we should provide utility functions in afw with something like the following signatures:  {code:java}  TransformPoint2ToSpherePoint makeIntermediateWorldCoordsToSky(      SpherePoint const & position,  // (ra, dec) that corresponds to FIELD_ANGLE origin      Angle rotation // rotation about FIELD_ANGLE origin  );    std::shared_ptr<SkyWcs> makeSkyWcs(      TransformPoint2ToPoint2 const & pixToIwc,      TransformPoint2ToSpherePoint const & iwcToSky  );{code}",2
"DM-14993","07/02/2018 09:29:32","Process DES SN field on lsst-dev in automated way; produce basic metrics","In previous epic, I was able to process DES SN field from raw/cals through ccd, coadd and difference processing. Now we want to automate biweekly difference image processing to study changes in the pipeline. This will sometime involve full end-to-end processing as above and sometimes be just difference imaging, depending on which parts of the pipeline need to be tested. I will also add some basic metrics/evaluation of this process.",40
"DM-14998","07/02/2018 12:16:05","Document schema naming conventions","Schema naming conventions changed from "".""-separated with no case consistency to ""_"" and camelCase with the introduction of meas_base.  I remember writing the new conventions down somewhere, but the first two places I looked:     - afw::table::Schema class docs     - afw::table overview page in Doxygen    ...document the old convention.         Fix those, ideally by locating the original text (maybe Confluence somewhere?) and transferring it to those locations.     ",1
"DM-14999","07/02/2018 13:23:43","conda bleed package list should not be code","The conda ""bleed"" package list is inline in {{lsstsw/bin/deploy}} as ""code"", which it is merely ""data"" which should be editable without modifying ""code"".",1
"DM-15007","07/03/2018 10:23:43","Add ability to exclude packages/modules from package-toctree and module-toctree","Add the ability to exclude named packages and modules from the piplines.lsst.io documentation through a ""skip"" field in the package-toctree and module-toctree directives.    The ""skip"" field takes a comma-delimited list of names.    This field is useful for temporarily removing packages that break the documentation build.",1
"DM-15008","07/03/2018 10:40:08","anetAstrometry.py uses self.distortionContext, which does not exist","James Mullaney reported the following bug on [confluence|https://community.lsst.org/t/bug-report-in-meas-extensions-astrometrynet/3013]:       We're using the astrometry.net extension to perform astrometry. In v15 and v16 of the stack, I've noticed that meas_extensions_astrometryNet/python/lsst/meas/extensions/astrometryNet/anetAstrometry.py refers to self.distortionContext (see [here|https://github.com/lsst/meas_extensions_astrometryNet/blob/519ff978fa8fd669da3308833b2808ba4e3ed595/python/lsst/meas/extensions/astrometryNet/anetAstrometry.py#L227]). However, it seems that the distortionContext function was removed on the 4th of November, 2017 by [this commit|https://github.com/lsst/meas_extensions_astrometryNet/blob/519ff978fa8fd669da3308833b2808ba4e3ed595/python/lsst/meas/extensions/astrometryNet/anetAstrometry.py#L227]. I may be mistaken, but this seems to be a bug introduced by not catching all references to distortionContext when making that commit.",2
"DM-15010","07/03/2018 11:03:25","Add a sims build triggered by the lsst_distrib weekly build","The sims team would like an automatically built weekly sims version that is built on the weekly build of {{lsst_distrib}}.  They would like the result to be a binary distribution that would allow end users to install a full stack of sims plus DM dependencies that are all consistent with the weekly build.  I.e.  {code}  $> eups distrib install lsst_sims -t w_2018_13  {code}    This is currently possible to do by hand, but it would be useful to them to have it automatically build.  They are very clear that a failing {{lsst_sims}} weekly build should not imply a failing {{lsst_distrib}} build.",1
"DM-15011","07/03/2018 12:48:29","implement separate Visit and Chip fitting for photometry","While doing additional testing of DM-14510, I noticed that the astrometry model fit could be thrown off by doing a line search during the initialization {{DistortionVisit}} fit step. This reminded me that I haven't implemented the separate ""Visit"" and ""Chip"" fitting option for Photometry: it may help things by pushing the toward the global minimum on the first step (as it does in astrometry). It should be very easy to implement.",2
"DM-15014","07/04/2018 03:29:59","Write error when creating k8s manifests","Qserv_deployt try to write to a root owned folder when creating Qserv k8s manifests for the first time",1
"DM-15023","07/05/2018 13:46:29","meas_modelfit is not compatible with Eigen 3.3.4","meas_modelfit has some issues with Eigen 3.3.4. I am making this a separate ticket from DM-14305 in hopes that these can be fixed in a way that is backwards compatible.    The first issue is that this does not compile:  {code}  ndarray::asEigenMatrix(*ix) = component._mu      + std::sqrt(_df/rng.chisq(_df)) * (component._sigmaLLT.matrixL() * _workspace);  {code}  but the solution to that is trivial: swap {{component._sigmaLLT.matrixL()}} and {{\_workspace}}, though [~jbosch] suggested a solution that reduces duplication with nearby code.    A more serious concern is that the following code in {{TruncatedGaussian.cc}} {{TruncatedGaussian::maximize}} raises an assertion error that a matrix is empty when {{running tests/test_truncatedGaussian.py}}:  {code}  Eigen::FullPivLU<Matrix> solver(G.topLeftCorner(n - k, n - k));  {code}  To fix that problem [~jbosch] suggests returning a vector of all zeros if n == k.",0.5
"DM-15036","07/06/2018 09:45:51","Restore MariaDB JDBC driver to latest 2.2.5 after mysql-proxy protocol fix","[~salnikov] has upgraded the MySQL protocol support in mysql-proxy, so CLIENT_DEPRECATE_EOF is now being handled properly through the proxy.      So no need to use the older JDBC driver anymore - it's time to 'undo' DM-14924.",1
"DM-15043","07/06/2018 16:45:56","Broken build in meas_algorithms","A last-minute change for DM-9937, combined with sloppy testing, caused non-compiling C++ to get committed to {{master}}. This ticket is to fix the broken code.",0
"DM-15044","07/08/2018 16:46:56","Seemingly Large demo change with bleeding edge pipelines build","In testing a bleeding edge conda install on my Mac, which includes numpy 1.14, astropy 3.0.3 and matplotlib 2.2.2, lsst_distrib builds fine but when I run lsst_dm_stack_demo I get nearly 100,000 failures, 8000 of which are failing at the 1e-06 level. I have no idea whether this is reasonable but it sounds very large. The detected-sources txt file is attached.    I have not tested ci_hsc or lsst_ci. I will see if I can run lsst_ci on this system.",3
"DM-15049","07/09/2018 12:47:33","Continue QuantumGraph implementation","This is a continuation of work started in DM-14334. Main goal of this ticket is to extend preflight solver with joins between skymap units and camera units so that we have more or less complete solution for building quantum graphs. ",8
"DM-15072","07/10/2018 09:29:36","Kombu error with LTD Keeper 1.11.0","There may be an incompatibility in an implicit dependency found through Celery that is preventing the LTD Keeper workers from booting up:    {code}  Traceback (most recent call last): File ""/usr/local/bin/celery"", line 11, in <module> sys.exit(main()) File ""/usr/local/lib/python3.5/site-packages/celery/__main__.py"", line 14, in main _main() File ""/usr/local/lib/python3.5/site-packages/celery/bin/celery.py"", line 326, in main cmd.execute_from_commandline(argv) File ""/usr/local/lib/python3.5/site-packages/celery/bin/celery.py"", line 488, in execute_from_commandline super(CeleryCommand, self).execute_from_commandline(argv))) File ""/usr/local/lib/python3.5/site-packages/celery/bin/base.py"", line 281, in execute_from_commandline return self.handle_argv(self.prog_name, argv[1:]) File ""/usr/local/lib/python3.5/site-packages/celery/bin/celery.py"", line 480, in handle_argv return self.execute(command, argv) File ""/usr/local/lib/python3.5/site-packages/celery/bin/celery.py"", line 412, in execute ).run_from_argv(self.prog_name, argv[1:], command=argv[0]) File ""/usr/local/lib/python3.5/site-packages/celery/bin/worker.py"", line 221, in run_from_argv return self(*args, **options) File ""/usr/local/lib/python3.5/site-packages/celery/bin/base.py"", line 244, in __call__ ret = self.run(*args, **kwargs) File ""/usr/local/lib/python3.5/site-packages/celery/bin/worker.py"", line 255, in run **kwargs) File ""/usr/local/lib/python3.5/site-packages/celery/worker/worker.py"", line 99, in __init__ self.setup_instance(**self.prepare_args(**kwargs)) File ""/usr/local/lib/python3.5/site-packages/celery/worker/worker.py"", line 122, in setup_instance self.should_use_eventloop() if use_eventloop is None File ""/usr/local/lib/python3.5/site-packages/celery/worker/worker.py"", line 241, in should_use_eventloop self._conninfo.transport.implements.async and File ""/usr/local/lib/python3.5/site-packages/kombu/transport/base.py"", line 125, in __getattr__ raise AttributeError(key) AttributeError: async  {code}    https://console.cloud.google.com/logs/viewer?project=plasma-geode-127520&authuser=1&_ga=2.223555801.-353975650.1530195679&pli=1&minLogLevel=0&expandAll=false&timestamp=2018-07-09T22%3A49%3A09.738000000Z&customFacets&limitCustomFacetWidth=true&dateRangeStart=2018-07-09T21%3A49%3A09.990Z&dateRangeEnd=2018-07-09T22%3A49%3A09.990Z&interval=PT1H&resource=container%2Fcluster_name%2Flsst-docs&scrollTimestamp=2018-07-09T22%3A36%3A18.000000000Z&advancedFilter=resource.type%3D%22container%22%0Aresource.labels.pod_id%3D%22keeper-worker-deployment-5457bf656-zrpmf%22%0Aresource.labels.zone%3D%22us-central1-b%22%0Aresource.labels.project_id%3D%22plasma-geode-127520%22%0Aresource.labels.cluster_name%3D%22lsst-docs%22%0Aresource.labels.container_name%3D%22keeper-worker%22%0Aresource.labels.namespace_id%3D%22ltd-prod%22%0Aresource.labels.instance_id%3D%2244590524507142367%22%0Atimestamp%3D%222018-07-09T22%3A36%3A18.000000000Z%22%0AinsertId%3D%22towqstg1iy3niu%22    Appears with LTD Keeper 1.11.0.    The tickets-DM-14122 Docker image deploys ok. But the 1.9.0 image does not.    1.9.0 was built retroactively much later than the associated ticket was merged (and tickets-DM-14122 was built).    This means I think there was a change in kombu that we're picking up as a floating implicit dependency of the pinned celery dep.",0.5
"DM-15082","07/10/2018 15:00:28","Switch to YamlStorage instead of BoostStorage in all obs packages","Now that YamlStorage has been implemented, we should use it.",2
"DM-15084","07/10/2018 16:59:46","Enable extra checks/warnings in qserv compilation","We do not have every possible check enabled during C++ compilation, adding {{-Wextra}} to compilation flags should help us to uncover possible problematic code.",0.5
"DM-15085","07/11/2018 07:26:15","Fix gen3-middleware ci_hsc SConscript","From [~salnikov] on Slack:  {quote}OK, I think I know what is happening. If I do `rm DATA/gen3.sqlite3 DATA/butler.yaml; makeButlerRepo.py DATA; bin/gen3.py` then it runs successfully (or it fails with different exception). If I do `rm DATA/gen3.sqlite3 DATA/butler.yaml; scons gen3repo` then it fails with the above exception (no such table: Camera). I watched DATA directory and I see that `gen3.sqlite3` is removed between execution of `makeButlerRepo.py` and `gen3.py`. Looking at SConscript I think this happens because scons believes that `gen3.sqlite3` is produced by `gen3.py` when in reality it is produced by `makeButlerRepo.py` and updated by `gen3.py`. Scons by default removes target file before rebuilding it, so it does that before executing `gen3.py`.  I'm not sure what is the best way to fix this but scons has `Precious` function to prevent removal of target before rebuilding (https://scons.org/doc/3.0.1/HTML/scons-user.html#chap-file-removal), maybe we should use that  {quote}",1
"DM-15090","07/11/2018 16:43:09","Stop using file in Python code","We are still using the old `file` class in some Python code, even though it is not available in Python 3. It should be replaced with `open`.    Also, pep8 catches this so I suggest making the packages pep8 compliant, enabling automatic checking and removing python 2 support (all of which should be trivial).",0.5
"DM-15098","07/13/2018 09:25:27","Add Registry.getRegion(DataId)","Add {{Registry.getRegion(DataId) -> sphgeom.Region}} that returns the intersection of all regions associated with the {{DataUnit}} in the {{DataId}}.",8
"DM-15104","07/13/2018 13:18:47","Move SourceDeblendTask out of MeasureCoaddSources","Currently the deblender is run as part of the {{MeasureCoaddSources}} task. In order to make it easier to swap out the current and future deblenders , {{SourceDeblendTask}} should be pulled out and placed in a new {{DeblendCoaddSourcesTask}}, along with an option to run {{MultibandDeblendTask}} instead. The current version of SCARLET also needs to be added to the stack as a 3rd party package.",8
"DM-15105","07/13/2018 13:21:17","Fix bare except in obs_subaru and other pep8 fixes","obs_subaru has many bare {{except}} usages. Fix these and make the codebase pass flake8.",0.5
"DM-15109","07/13/2018 14:04:28","Adapt pipe_analysis to RFC-498 implementation","Adapt the {{pipe_analysis}} scripts to be compatible with the changes coming out of RFC-498.  Also make accommodations for it to be backwards compatible with catalogs run on older, pre-RFC-498, processed catalogs.",3
"DM-15133","07/16/2018 16:13:15","Empty matches in coaddAnalysis.py at COSMOS field?","Using {{w_2018_28}} and {{pipe_analysis}} at commit {{ebd48cf}}, {{coaddAnalysis.py}} failed in the COSMOS tract=9813 filters HSC-G, HSC-R, HSC-Z, and HSC-Y:     {code:java}  Traceback (most recent call last):    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-2-g852da13+6/python/lsst/pipe/base/cmdLineTask.py"", line 392, in __call__      result = task.run(dataRef, **kwargs)    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/coaddAnalysis.py"", line 348, in run      hscRun=repoInfo.hscRun, wcs=repoInfo.wcs)    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/coaddAnalysis.py"", line 450, in readSrcMatches      if not hasattr(matches[0].first, ""schema""):  IndexError: list index out of range  {code}    The command I ran was   {{coaddAnalysis.py /datasets/hsc/repo/ --calib /datasets/hsc/repo/CALIB  --rerun RC/w_2018_28/DM-14988  --id tract=9813 filter=HSC-G -c doWriteParquetTables=True}}   (and 3 other filters)",2
"DM-15138","07/17/2018 10:43:22","Incorrect instructions in ap_verify readme","The {{ap_verify}} readme says to run {{python/lsst/ap/verify/ap_verify.py}}, even though this file has not been executable for almost a year. While the Sphinx documentation has been kept up-to-date, it appears the readme has bitrotted.    Correct the example command line, and proofread the readme for any other out-of-date or missing information.",1
"DM-15139","07/17/2018 11:07:31","Rename invert() and getInverse() to inverted()","Implement RFC-500 by renaming the {{invert}} method of geom {{AffineTransform}}, geom {{LinearTransform}} and afw {{GridTransform}} and the {{getInverse}} method of afw {{Transform}} and astshim {{Mapping}} to {{inverted}}.    Also rename the {{simplify}} method of astshim {{Mapping}} to {{simplifed}} for the same reason. Note that this is little used outside of astshim.",2
"DM-15152","07/17/2018 13:39:32","crosstalk correction was moved above assembleCcd, which broke it","In cc1e91fc90, running the crosstalk task was moved above assembleCcd, which broke the crosstalk correction in ip_isr.  Due to the fact that HSC uses a custom ISR this fact was not visible in ci_hsc, and in fact no tests were checking for this error.  The tests of the crosstalk task itself were unaffected by changes in {{isrTask}}.  The problem only showed up when we started processing lsstCamera data (ts8 and in particular phosim) through {{obs_lsstCam}}.    I propose that we revert this change.  It was made to support DECam, but I do not understand why it was necessary so I think it's better to fix {{obs_decam}} than {{ip_isr}}, but would like to understand why the DECam code was written this way before taking a final decision.  In discussion, Meredith pointed out that she didn't want to have to assemble CCDs to process inter-CCD crosstalk, but I don't think that this would be necessary (you'd use the {{Detector}} to iterate over the amplifier segments, applying suitable flips as described in the {{Detector's amplifier objects)}}",1
"DM-15162","07/19/2018 09:44:01","Improve documentation for DataIdContainer","Enhance the documentation for {{DataIdContainer}} to document the fields and expand the documentation for the methods. For example what does ""castDataIds"" actually do -- what does it mean to cast a data ID to the correct type?    Also either explain why the user cannot set the dataset type in the constructor or else add that capability.",0.5
"DM-15169","07/19/2018 11:26:58","Replace all use of ""mock"" with ""unittest.mock""","We have Python code in using the {{mock}} package, which is a 3rd party library. Change this to use {{unittest.mock}}, which is part of the standard library.",3
"DM-15177","07/20/2018 13:39:46","Data display bug in SUIT caused by wrong datatype in JSON ","During the test of the newly deployed lsst-pdac/portal/suit, I stumbled on something that is obviously wrong. A position search on ""NEOWISE-R Year 1 Single Exposure (L1b) Source Table"" at  (0.014937 -0.008658) with radius 10"" returned data with w1flux and w1sigflux empty while w1mpro and w1sigmpro both have data.          Upon further study, the w1flux and w1sigflux columns seem to have ""char"" as data type, while the meta data showed that they should be ""float"". We need further investigation into this.          The downloaded data is attached.          7/25/2018    Further investigation revealed that those two fields had datatype ""UNKNOWN"" when returned from DAX API dbserv V1. Ticket DM-15206 has been created for this.           8/28/2018    An unknown datatype's value could not be passed properly and displayed. Firefly does not need to be modified. ",1
"DM-15178","07/20/2018 13:50:29","Investigate position errors in scarlet blends","Lorena Mezini, an undergraduate working with Erin Sheldon, noticed that when scarlet fits positions, the mean position errors are ~{{0.15}} in both x and y (see attached plot). What's stranger is that one of the objects in her simulations seems to have a mean positional error of 0 in x and y while the other shows the offset.    Her dataset were generated using galsim, with very low noise and fixed bounding boxes (25 pixels) on single band images. The sources both have very small ellipticity (~.01) with random orientations, the same shear, and identical FWHM. The plot shows an example with 100k blends, each with 2 sources separated by 11 pixels.    This ticket is to investigate this behavior to understand the conditions that lead to two seemingly identical objects being deblended differently.",5
"DM-15179","07/20/2018 15:03:43","Table: Do not send ROW_IDX and ROW_NUM columns when not requested","Currently, columns ROW_IDX and ROW_NUM are sent to the client for every table request.    It's only used in a few cases.  Change it so that these columns are only sent if requested and not by default.",3
"DM-15182","07/23/2018 08:48:08","Add facility to change matplotlib colormap","On https://github.com/lsst/display_matplotlib/issues/1, the request is:    {quote}  Would it be possible to give the user control over the colormap used to display images? I see that this is currently set to cmap=pyplot.cm.gray.  {quote}",1
"DM-15183","07/23/2018 08:51:11","Update developer guide for pybind11 2.2","Update `dm_dev_guide` for changes in pybind11 2.2",0.5
"DM-15187","07/23/2018 12:29:08","Modernize sphgeom pickle support for pybind11 2.2","In DM-14828 I missed the fact that the pickle support ({{\_\_getstate\_\_}} and {{\_\_setstate\_\_}}) in sphgeom needed updating. Do that on this ticket branch.",0.5
"DM-15190","07/23/2018 18:08:18","Fix ip_diffim FutureWarning","After upgrading to numpy 1.14, `lsst.ip.diffim.dipoleFitTask` raises a new error:    {code:python}  python/lsst/ip/diffim/dipoleFitTask.py:303: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.  To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.  {code}    It was suggested by [~jbosch] that we simply set `rcond=-1`.",0.5
"DM-15194","07/24/2018 10:21:22","display_matplotlib: dot fails to plot ellipses","From https://github.com/lsst/display_matplotlib/issues/2:    {quote}  I've tried to plot source catalog objects as ellipses using the display.dot function and passing source.getShape as the first argument:    {code}  #Display the cutout and sources with afw display  image = cutout.image  #image = calexp.image    plt.figure()  afw_display = afwDisplay.Display()  afw_display.scale('asinh', 'zscale')  afw_display.mtv(image)  plt.gca().axis('off')    # We use display buffering to avoid re-drawing the image after each source is plotted  with afw_display.Buffering():      for s in sources:          afw_display.dot('+', s.getX(), s.getY(), ctype=afwDisplay.RED)          afw_display.dot('o', s.getX(), s.getY(), size=20, ctype='orange')               #ADW: This should work, but doesn't?          afw_display.dot(s.getShape(), s.getX(), s.getY(), size=35, ctype='orange')      # Our ""ultra-faint galaxy"" (e.g. smudge)      afw_display.dot('o', x_target, y_target, size=35, ctype='cyan')  {code}    However I get the following error:    {code}  /opt/lsst/software/stack/stack/miniconda3-4.5.4-10a4fa6/Linux64/afw/16.0-19-g2da375352/python/lsst/afw/display/interface.py in dot(self, symb, c, r, size, ctype, origin, *args, **kwargs)      526             symb = afwGeom.ellipses.Axes(symb)      527   --> 528         self._impl._dot(symb, c, r, size, ctype, **kwargs)      529       530     def line(self, points, origin=afwImage.PARENT, symbs=False, ctype=None, size=0.5):    /opt/lsst/software/stack/stack/miniconda3-4.5.4-10a4fa6/Linux64/display_matplotlib/16.0+11/python/lsst/display/matplotlib/matplotlib.py in _dot(self, symb, c, r, size, ctype, fontFamily, textAngle)      371       372             axis.add_artist(Ellipse((c + x0, r + y0), xradius=symb.getA(), yradius=symb.getB(),  --> 373                                           rot_deg=math.degrees(symb.getTheta()), color=ctype))      374         elif symb == 'o':      375             from matplotlib.patches import CirclePolygon as Circle    TypeError: __init__() missing 2 required positional arguments: 'width' and 'height'  {code}    I've confirmed that {{matplotlib.patches.Ellipse}} does expect the width and height arguments (at least in recent versions).  {quote}",1
"DM-15201","07/24/2018 17:11:48","Forward python logging to lsst.log in pipe_base","In RFC-29 we allowed Python {{logging}} to be used in Python code so long as the log messages were forwarded to {{lsst.log}} whenever C++ logging is a possibility. A forwarder does exist as {{lsst.log.LogHandler}} and instructions for its use are available at https://github.com/lsst/log/blob/master/doc/mainpage.dox#L59    To enable science pipelines to forward these Python log messages we need to setup this log handler in {{pipe_base}}.",1
"DM-15204","07/25/2018 13:28:25","Add chained datastore support to butler","An in memory datastore was added in DM-13363. Now we need to support chaining of datastores such that the in memory datastore can be used as a cache.    This will include modifications of the configuration system to specify these relationships and the creation of at least one special datastore class for chaining.    per-datasetType Cache expiry may be included.",8
"DM-15206","07/25/2018 16:01:06","dbserv_v1 mapping float to Uknown for DS_wise table","Per [~xiuqin], the query results from dbserv_v1 is returning 'Unknown' as datatype for column that's actually 'float'.    Example 1 query on PDAC:     url -o q3_result.json -L [http://lsst-qserv-dax01:8080/api/db/v1/tap/sync/] -H ""Accept: application/json"" -d 'query=SELECT * FROM DS_wise.neowiser_yr1_00.neowiser_yr1_p1bs_psd WHERE w1flux > 0 LIMIT 1'    Output  section with incorrect datatype:    \{ ""datatype"": ""UNKNOWN"", ""description"": """", ""name"": ""w1flux"", ""tableName"": ""result_11899"", ""ucd"": null, ""unit"": null }, \{ ""datatype"": ""UNKNOWN"", ""description"": """", ""name"": ""w1sigflux"", ""tableName"": ""result_11899"", ""ucd"": null, ""unit"": null },         Example 2 query from SUIT:     curl -o neowiseY1.json -d ‘query=SELECT+count(*)+FROM+DS_wise.neowiser_yr1_00.neowiser_yr1_p1bs_psd+WHERE+qserv_areaspec_circle(10.68479,41.26906,0.02777777777777777);’ [http://lsst-qserv-dax01:8080/api/db/v1/tap/sync|http://lsst-qserv-dax01.ncsa.illinois.edu:8080/api/db/v1/tap/sync]     ",2
"DM-15208","07/25/2018 18:22:07","validate_drp skipTEx is being ignored","During the port to the verify system,  support for the {{skipTEx}} option was broken in {{matchreduce.py}} (it was permanently set to False).",0.5
"DM-15210","07/26/2018 09:17:52","Update registry schema (and documentation thereof) following review","Make adjustments to the schema to incorporate feedback from RFC-484 and the associated review.    Should also sync the technote with the schema YAML on daf_butler master (only very minor and structural changes so far).     ",2
"DM-15212","07/26/2018 09:59:35","Improved transactions and ingest-with-transfer for datastores","This is a spin-off of DM-15189 containing (hopefully all of) the supporting work in daf_butler for that ticket, separated into this ticket to merge it sooner and avoid conflicts with other daf_butler development.",2
"DM-15213","07/26/2018 10:10:02","display.dot of ellipses fails with display_firefly when buffering","When using {{afw.display}} with the Firefly backend, writing ellipses with buffering does not draw ellipses:     {code:python}  with display.Buffering():      for record in src256[::10]:          display.dot(record.getShape(), record.getX(), record.getY())  {code}    It works without the buffering.",2
"DM-15215","07/26/2018 12:26:57","Provide parameters for initial pan position in pixels when displaying image","For DM-14736 we want to be able to apply a pan position in pixels at the time of displaying an image with {{afw.display}}. ",2
"DM-15217","07/26/2018 12:53:27","Rename sha1 to general hash in Butler schema and API","On the PR for DM-15098 [~tjenness] noticed we are explicitly specifying {{sha1}} where we should (probably) use a more general {{hash}} in both {{schema.yaml}} and the Butler API. Fix this.",0.5
"DM-15220","07/26/2018 15:20:10","Moving SuperTask to pipe_base","We have ~agreed on a better name for SuperTask class, {{PipelineTask}} looks like a better one for everyone. We should rename the class and move it to {{pipe_base}} in preparation to merging all gen3 stuff with master.",2
"DM-15222","07/26/2018 15:44:33","firefly_client updates for new asinh stretch","Update firefly_client to accept Q parameter for asinh stretch algorithm. When no value is specified, 'NaN' (case sensitive) should be passed in serialized range string to Firefly. (This would make Firefly to pick up a reasonable default based on stretch and data range.)    Update the documentation for set_stretch.  ",2
"DM-15225","07/26/2018 16:53:08","SUIT and Firefly container should not run as root by default","There are two Firefly servers running in lsst-lspdev k8s cluster environment. IT was discovered that the Firefly server were running as root after they were deployed by k8s. The container settings need to be changed so that the server is not started as root.           Also configure the proxy to not require the trailing slash in URL https://lsst-pdac.ncsa.illinois.edu/portal/suit/",3
"DM-15230","07/27/2018 13:29:10","Fix MultibandExposure initialization","When a new {{MultibandExposure}} is initialized using the {{fromExposures}} method, it doesn't pass the {{psf}} from each {{Exposure}} into the new objects.  To fix this, {{afw.image.multiband.fromTriples}} should be updated to take keyword arguments that can be used to pass additional parameters to the new multiband objects.",2
"DM-15232","07/27/2018 15:29:04","Add parameters for asinh and power law_gamma to display_firefly","With support for {{asinh_q_value}} and {{gamma_value}} added to {{firefly_client.FireflyClient.set_stretch}}, make these available in the {{display_firefly}} backend as {{Q}} and {{gamma}}.",1
"DM-15233","07/27/2018 18:26:37","ds9 region global coordsys statement support","Currently we don't support ""global coordsys"" statement in the region file. We need to support it so user can define one global coorsys and only declare a few if they are different from the global coordsys.      We will support the following coordinate system:    {{FK4, B1950            }}  {{FK5, J2000}}   {{GALACTIC}}   {{ECLIPTIC}} ",2
"DM-15237","07/30/2018 09:45:18","Ensure StorageClass singleton is append-only.","Make sure new storage class registrations cannot modify old ones.         To make the discussion on this ticket readable for posterity, the original ticket description was:  {quote}Having all consumers of StorageClasses (which includes any code that wants to create a DatasetType) responsible for constructing a StorageClassFactory (from a config file, since they don't know if the singleton already exists) is not tenable. StorageClasses are global state, and they need to be available at module-import time.    That also means we should move them out of the configuration for Butler, which makes sense, because Butlers don't get to pick their StorageClasses    This is probably also a good time to make sure we don't have any custom StorageClasses in tests, or if we do, make sure they are run in a way that prevents them from ever polluting the real set of StorageClasses in other tests.  {quote}",2
"DM-15241","07/30/2018 18:00:24","log error when final chi2 is large","For some reason, PDR1 tract 15832 HSC-Y got a chi2/ndof of 827 for astrometry, with validate_drp agreeing that the fit was poor, but no obvious ""this fit might have trouble"" message was logged. We should log an error message for all jointcal final fit results that have {{chi2/ndof >= X}}, where X might be 5 or 10 (as a first guess at ""possibly bad"" chi2). ",0.5
"DM-15243","07/31/2018 11:03:10","LSST the Docs Keeper features for Notebook-based report system (nbreport)","Add features to LTD Keeper that are needed by the notebook-based report system's API server (as developed in DM-15199):    * {{autoincrememnt}} feature for edition slugs so that LTD Keeper can create editions with monotonically increasing integer edition numbers that serve as notebook report instance IDs.  * {{manual}} tracking mode for editions so that the nbreport server can manage the updates of new editions with builds.",1
"DM-15244","07/31/2018 11:11:18","Change fluxSigma to fluxErr and similarly for apCorr and covariances","Change fluxSigma to fluxErr, apCorrSigma to apCorrErr and similarly for the diagonal elements of covariance matrices. This includes variable names, argument names and afw table field names. This is necessary in order to implement RFC-333.    Change {{afw::table::CovarianceMatrixKey}} to use ""Err"" as a suffix rather than ""Sigma"".    Update the FITS catalog persistence as follows:  - Bump the schema VERSION constant from 1 to 2 in SchemaImpl.h  - When reading version 1 catalogs make a {{fooErr}} as an alias to every {{fooSigma}} field  - When reading version 0 catalogs make {{Err}} aliases instead of {{Sigma}} aliases.  ",5
"DM-15245","07/31/2018 11:46:23","bug fix in regions API: draw the correct regions, discard the wrong ones","DM-15213 is fixing the bug in Python  calling function for LSST. We need also to fix it in the JS API.     I checked region file upload in current dev version. The non-parsable regions are ignore already in UI.  API should behave the same.      ",2
"DM-15246","07/31/2018 13:14:53","Add scarlet as a 3rd party package to the stack","Per RFC-504, scarlet will be added as a GitHub fork to the stack.",3
"DM-15248","07/31/2018 14:09:04","Support textangle in ds9 text regions in Firefly","When {{textangle}} is provided in a region description, Firefly does not draw the associated region. The {{afw.display.dot}} method allows specifying the textangle, but when this argument is provided, the region is not displayed.    Example of region text generated by {{afw.display.dot}} that does not display:  {noformat}text 1074 110 ""Text"" # color=yellow textangle=45.0  {noformat}    The format we need to support for angle is {{textangle=}} after the hash symbol.",3
"DM-15249","07/31/2018 14:32:31","Extend catalog upload to allow specifying a list of columns","The {{firefly_client.plot}} module includes a method for uploading and showing a table in Firefly. Add options for specifying the columns from the table that appear, as well as filters.",2
"DM-15252","07/31/2018 17:48:01","columns and types returned by metaserv should match db","We need an automated way to verify that the columns and data types returned by metaserv match the columns and types in DB, for all tables returned by metaserv.    Currently, it's not the case. Here is an example:    For _DS_wise.wise_00.allwise_p3am_cdd_, metaserv returns ""double"" for all types that should be ""int"" or ""long"" in db: _band_, _naxis_, _naxis1_, _naxis2_ - these are all ""int"" fields in DB, _cntr_ - is ""long"".    SUIT uses _band_ to construct WISE image URL, wrong type results in wrong URL.",8
"DM-15255","08/01/2018 09:49:37","Metric description does not update","On the monitor app (sandbox deployment) the metric description does not update when the verification package changes.",0.5
"DM-15268","08/01/2018 13:19:01","Merge gen3-middleware branches to master","Rebase and merge these branches after making sure they work.  Will require daf_butler to be added as a dependency of at least some of these packages.     ",1
"DM-15272","08/01/2018 15:21:49","UI design proposal for table option pop-up","Gregory and Vandana will discuss the table pop-up options need from science user point of view, make a design proposal for developers. ",2
"DM-15275","08/01/2018 17:09:44","Communicate with users about Emacs in Jellybean","I communicated with Alex Drlica-Walker about best practices for interacting with Emacs in the JupyterLab terminal emulator.  See [here|https://github.com/LSSTScienceCollaborations/DMStackClub/issues/56] for conversation.    Relatedly, I made my first attempt at using the new lspdev on-boarding procedure.  Procedure emailed to [~frossie].    I also set up my duo account.",1
"DM-15278","08/02/2018 10:51:41","Re-allow file checksumming before moving file and verifying after","Most of the needed code is present - this just turns it back on in preparation for C++ checksum calculation in a separate branch.",1
"DM-15286","08/02/2018 12:35:18","Change way DMCS informs the ocs_bridge the current state","Return state string rather than integer.",1
"DM-17276","08/03/2018 11:12:49","XML - Update MTMount XMLs to conform to Naming Conventions","Once the software is officially delivered to LSST, it will be necessary to update the design and code to conform to SAL constraints and conventions, as defined in [https://confluence.lsstcorp.org/display/SYSENG/SAL+constraints+and+recommendations]    This includes:   * Naming conventions   * Enumeration definition   * Implementation of all standard commands and events.",5
"DM-15310","08/03/2018 13:12:54","Refactor ForcedPhotImageTask (and children) per RFC-352","Implementing RFC-352  prepping for the supertask conversion in two stages. DM-2639 changed the TaskRunner to call a Task's runDataRef method.  During code review [~rowen] points out that {{ForcedPhotImageTask}} (and its children {{ForcedPhotCcdTask}} and {{ForcedPhotCoaddTask }} would be simple to refactor.",2
"DM-15311","08/03/2018 13:20:41","Refactor MeasureCrosstalkTask per RFC-352","Implementing RFC-352  prepping for the PipelineTask fka SuperTask conversion in two stages. DM-2639 changed the TaskRunner to call a Task's runDataRef method.  During code review [~rowen] points out that  {{MeasureCrosstalkTask}} would be a simple refactor. The {{run}} method is essentially the currently public-interfacing function {{extractCrosstalkRatios}}.  ",2
"DM-15320","08/03/2018 16:34:37","Merge Qserv code performance updates","This ticket is concerned with merging changes to the code made to improve performance. There was a large drop in performance between KPM20 and KPM30 in the cluster at in2p3. Performance was restored to previous level for the LSST dataset, but only in the case where join queries were not included in the batches of test queries. Single join queries run quickly, but when run with a group of queries the entire system slows down significantly. This is thought to be caused by MariaDB not having sufficient resources and is still being sorted out. MariaDB optimizations should improve the situation. The number of computers in the cluster at in2p3 may simple not have enough resources to meet the KPM30 requirements.    Code was modified to add instrumentation for diagnosis and other changes were made to improve performance. Most of the changes were in qdisp (where the threading model was changed significantly between KPM20 and KPM30), wsched (as testing showed some significant problems), and memman (used by wsched and its mlock calls took significant time).    Part of the performance issue was configuration values appropriate for the PDAC were applied to the in2p3 cluster. Configuration files are part of the container. Kubernetes has tools for writing the appropriate configuration files to individual systems and they should be used as soon as practical.",1
"DM-15332","08/06/2018 11:54:17","Create prototype of JSON for afwDetect footprints","Create a prototype JSON representation of all the footprints in an LSST SourceCatalog, for the regular footprints only (excluding heavy footprints), for use in Firefly.    Include bounding box information as well as the details of the individual pixels included in each footprint. Coordinate with development of DM-15326.",3
"DM-15333","08/06/2018 12:08:49","Apply pan and scale at time of image display in display_firefly","To improve performance of image display in Firefly, allow an afwDisplay instance with the Firefly backend to track {{display.scale}} and {{display.pan}} settings that precede {{display.mtv}}, and apply these at the time of image display. Also add a small correction to the pan method so that it uses the LSST pixel coordinate convention.    Implemented:    Modify the {{display_firefly}} backend to remember the results of scale (stretch) commands, and to apply scale and pan parameters when displaying an image with {{lsst.afw.display.mtv}}.   * Remember and apply the serialized RangeValues string that Firefly uses   * Correct pan command by 0.5 pixels to follow LSST convention   * Apply the remembered pan position when using {{mtv}}   * Make {{'slate.html'}} the default if it is not passed explicitly or if the environment variable {{FIREFLY_HTML}} is not defined. This is necessary for Firefly to show readout of LSST pixel convention.",2
"DM-15337","08/06/2018 15:15:13","mysql-proxy logger still logs after changes in log4cxx config","While running no-logging tests at IN2P3, it was noticed that altering {{log4cxx.czar/worker}} configuration to {{ERROR}} level still produces mysql-proxy log messages even after qserv is restarted.    It seems that proxy logger (name ""mysql-proxy"") is initialized sooner than we read log4cxx config so it is initialized with default {{INFO}} level, while all other loggers use {{ERROR}}.",1
"DM-15341","08/06/2018 16:41:16","Write unit test for `applyDcr`","Several methods of the new {{DcrModel}} class need unit tests written for them.",2
"DM-15350","08/07/2018 15:39:03","Fix segfault in sphgeom::HtmPixelization::pixel","After merging DM-15098 it was found that {{sphgeom::HtmPixelization::pixel}} causes a segfault when called on Linux.  Some investigation has shown that this bug was prexisting (and not unit tested) and caused by an upstream pybind11 bug (https://github.com/pybind/pybind11/issues/1132).  Until that is fixed, work around it by changing the holder type of all {{sphgeom::Pixelization}} subclasses to {{std::unique_ptr<T>}}.",1
"DM-15353","08/07/2018 16:51:45","Detail process for self serve LSP accounts","Describe the process for getting an LSP account at the LDF by the self registration mechanism.",1
"DM-15355","08/07/2018 18:18:38","ci_hsc failure: aperture correction fields for PsfFlux not present","ci_hsc is failing with the following error, almost certainly due to DM-15244:  {code}  File ""/j/ws/stack-os-matrix/centos-6.devtoolset-6.py3/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 205, in validateSources    self.checkApertureCorrections(catalog)  File ""/j/ws/stack-os-matrix/centos-6.devtoolset-6.py3/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 117, in checkApertureCorrections    ((""%s_flag_apCorr"" % alg) in catalog.schema))  File ""/j/ws/stack-os-matrix/centos-6.devtoolset-6.py3/lsstsw/build/ci_hsc/python/lsst/ci/hsc/validate.py"", line 90, in assertTrue    raise AssertionError(""Failed test: %s"" % description)  AssertionError: Failed test: Aperture correction fields for base_PsfFlux are present.  {code}",2
"DM-15365","08/08/2018 15:03:40","Move SqlRegistry API back down into Registry","For expediency we (mostly me) haven't been as diligent in making sure our {{Registry}} ABC stayed up to date with changes to {{SqlRegistry}}.  Update it.",1
"DM-15372","08/08/2018 17:27:14","Race conditions in daf_butler tests","Some of the butler/datastore tests use a fixed name for the datastore root. Now that we are regularly building daf_butler on Jenkins we are getting test failures due to race conditions since now test methods using the same root are running in parallel. Modify the tests to use a different root in {{setUp}}.",1
"DM-15374","08/08/2018 17:36:58","Per dataset allow/deny lists for datastores","It is necessary for a datastore to be able to specify a list of datasetTypes that are supported, or a list that are unsupported. For posixdatastore this can currently happen by a StorageClass not being listed as a formatter, but a more general approach is needed. In particular, it may be that an InMemoryDatastore should only cache a subset of datasettypes when used as part of a ChainedDatastore.    I think we need to support allow lists and deny lists, although it only makes sense to specify one of those options in a particular configuration (allow means only those listed should be allowed so deny becomes superfluous).",3
"DM-15377","08/08/2018 17:46:58","Handle parameters robustly for get()","So far we are not using parameters for datastore.get. This will have to change if we wish to support efficient subsetting of large datasets.  To be able to handle ChainedDatastores properly, parameters will need to be able to be handled by formatters, and generically by using code in StorageClass assembler classes. This requires that the formatter (which will be called first) has to be able to indicate which parameters it has already handled before the remaining parameters are passed to the assembler class.",3
"DM-15385","08/09/2018 12:18:05","Update boost to v1.68","Upgrade boost to v1.68 -- this will enable boost-python to build with python3.7 (we still need boost-python for galsim).",0.5
"DM-15389","08/09/2018 13:15:18","Update PyYAML package to v1.13","PyYAML 1.11 does not seem to build with Python 3.7. v1.13 does build fine.",0.5
"DM-15390","08/09/2018 14:55:31","FutureWarning in sconsUtils with python 3.7","Using sconsUtils with Python 3.7:  {code}  /sconsUtils/16.0-1-gd93e90d/python/lsst/sconsUtils/builders.py:202: FutureWarning: Possible nested set at position 11    files_expr += ""-name %s -prune"" % re.sub(r""(^|[^\\])([[*])"", r""\1\\\2"", file)  {code}",1
"DM-15394","08/09/2018 15:35:10","afw does not work with Boost v1.68","When testing Boost 1.68 in DM-15385, afw fails to build almost immediately because of failures in {{lsstGil.h}}.    {code}  include/lsst/afw/image/lsstGil.h:119:34: error: macro ""GIL_DEFINE_BASE_TYPEDEFS"" requires 3 arguments, but only 2 given  {code}    In v1.68 {{GIL_DEFINE_BASE_TYPEDEFS}} gains a new argument in the middle:    {code:c}  // B - bits size/signedness, CM - channel model, CS - colour space, LAYOUT - pixel layout  // Example: B = '8', CM = 'uint8_t', CS = 'bgr,  LAYOUT='bgr_layout_t'  ...  #define GIL_DEFINE_BASE_TYPEDEFS(B, CM, CS)  {code}    This is not documented as a breaking change so I wonder if we are using internal APIs here. The immediate fix would be to add the channel model definition to our header file, but it would be preferable to use a public API if one exists since that might also work for boost 1.66 (that we are currently using).  ",1
"DM-15399","08/09/2018 19:22:11","MOC display doesn't show  correctly in Glalatic coordinate system","The MOC display doesn't show correctly when 'Galactic' is selected from the image viewer.          Implemented:    this development fixed the MOC display bugs in Galactic coordinate system by converting the world point with the same coordinate system as that of the project center while finding some pixel alongside the great circle passing the given projection center and a world point.    test:  do hips image search and get moc display in galactic coordinate system.",1
"DM-15400","08/09/2018 20:04:27","Galsim does not work with boost 1.68","Bost 1.67 renamed the boost python library to {{libboost_python36}}. This leads to Galsim 1.66 to fail to build because it can't find that library. The SConstruct needs to be patched to add the new name as an option.",0.5
"DM-15402","08/10/2018 10:00:39","Enhance tests for setConfigRoot","[~jbosch] reports that he may be seeing cases where {{setConfigRoot}} is not updating anything, resulting in the temp directory from DM-15372 being ignored.  Investigate this problem and add more tests. Also investigate removal of an unnecessary line in config.py constructor that may be confusing things.",2
"DM-15414","08/11/2018 18:20:29","Deploy a proof-of-concept notebook-based test report resembling the characterization report","This ticket is to deploy a proof-of-concept report to show off the notebook-based reporting system. [~krughoff] created a notebook based on the characterization report that consumes data from the SQUASH API (https://gist.github.com/SimonKrughoff/0ada129d858dd92e9a46acea310612b2). I'll use that as the starting point. ",0.5
"DM-15417","08/12/2018 15:03:24","Remove deprecated getInverse and invert methods","DM-15139 standardized the method for returning an inverse transform to {{inverted}} but left the old {{getInverse}} and {{invert}} methods for backwards compatibility (even though all DM stack code was updated).    Please remove the deprecated methods after a suitable time period has elapsed, e.g. after the next major release.",1
"DM-15419","08/13/2018 10:43:31","utils cache tests fail in python 3.7 and boost 1.68","Testing the stack with Python 3.7 fails when building {{utils}}.    Python 3.7 requires boost 1.68 (DM-15385). To reproduce:    * Install lsstsw using {{bin/deploy -b}} to get the ""bleeding edge"" conda packages.  * Rebuild utils with boost 1.68: {{rebuild -r tickets/DM-15385 utils}}    This will hang during the utils {{test_cache.py}} tests. The hang occurs even with optimization disabled in {{utils}}. I have not tried to disable boost optimization.",0.5
"DM-15420","08/13/2018 12:33:30","Methods for checksumming finished up.","Finished up csum methods in Forwarder, on branch jbparsons_csum_additions.",2
"DM-15421","08/13/2018 14:56:45","Change to use constrained models by default","Now that there is a working constrained photometry model, I believe it is time to switch jointcal's default config to use the constrained models instead of the simple models. The rationale is that all of the ""important"" cameras we work with are mosaic cameras (the constrained model doesn't make sense for single chip cameras), and the constrained model is a better model of the system behavior than the simple model.",1
"DM-15424","08/13/2018 23:08:18","Revisit LimitedRegistry concept","In discussions with [~mgower] about temporary-scratch-repo use cases, we identified that it would be nice to have a Registry implementation that supports only generalized get and put (not expressions), and hence does not need DataUnit tables.  This is similar in functionality (but not implementation) to a previous LimitedRegistry concept that we envisioned not using SQL at all; the new concept would use SQLite, but would have only a subset of the full Registry schema.    Whether there are any non-DataUnit tables we could drop from LimitedRegistry is an open question; one possibility is that it could also only represent a single Collection, and hence have no need for DatasetCollection either.",3
"DM-15435","08/14/2018 16:56:44","Remove python 2 support from pex packages","Remove python 2 support from the pex packages, make pep8 compatible and enable automatic pep8 checking",0
"DM-15438","08/15/2018 08:43:31","display_firefly setMaskTransparency is backwards","setMaskTransparency(100) in display_firefly makes makes mask overlays fully opaque, while the the documention (and the behavior of the same call in display_matplotlib, at least) suggest it should make them fully transparent.",0.5
"DM-15440","08/15/2018 10:38:36","Update sconsUtils to not use python_future","Remove python 2 support from sconsUtils and update to not use python_future",0
"DM-15442","08/15/2018 11:33:35","Remove python 2 support to more packages and add pyList=[]","For packages with automatic flake8 testing the file tests/SConscript should contain   {code}  from lsst.sconsUtils import scripts  scripts.BasicSConscript.tests(pyList=[])  {code}  but some packages are missing {{pyList=[]}} so the tests are not being run",0
"DM-15443","08/15/2018 12:02:32","Update mpi4py","mpi4py 2 does not build with python3.7. Update to v3.0.0.",2
"DM-15448","08/15/2018 16:02:24","Add datasets document information to DMTN-091","Add the dataset definitions and identifications from the SST-based document by Bellm, Bosch, Ivezic, Slater, and Wood-Vasey to DMTN-091.",2
"DM-15450","08/15/2018 22:42:17","Update deprecated use of time.clock()","In running some tests involving pipe_base I got this warning:  {code}  /Volumes/ExternalSSD/Users/timj/work/lsstsw37/stack/DarwinX86/pipe_base/16.0-10-g0b41441+6/python/lsst/pipe/base/timer.py:91: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead  {code}    Not urgent yet but the Python developers have now given an explicit timeline for when it will become a problem.",0.5
"DM-15451","08/16/2018 05:09:04","nightly-release d.2018.08.16 failed due to disk fulll","All three canonical {{rebuild}} build attempts failed with {{OSError(28, 'No space left on device')}}.    [https://ci.lsst.codes/blue/organizations/jenkins/release%2Fnightly-release/detail/nightly-release/396/pipeline]    * [https://ci.lsst.codes/blue/organizations/jenkins/release%2Frun-rebuild/detail/run-rebuild/958/pipeline/]  * [https://ci.lsst.codes/blue/organizations/jenkins/release%2Frun-rebuild/detail/run-rebuild/959/pipeline/]  * [https://ci.lsst.codes/blue/organizations/jenkins/release%2Frun-rebuild/detail/run-rebuild/960/pipeline/]     ",0.5
"DM-15452","08/16/2018 12:37:53","Fix DCR multiband bugs introduced by new deblender","The recent changes to the deblender broke multiband source measurement on DCR coadds. This ticket is to identify and fix the breakages.",2
"DM-15455","08/16/2018 17:58:04","Error in Gen3 Butler when template format ends in a literal","Using w_2018-32.    From butler.yaml:  {code}    templates:      modified_raw: ""{datasetType}/\{camera:?}-{exposure:?}-{sensor:03?}-r12p01""  {code}    {code}  (lsst-scipipe-10a4fa6) [mgower@lsst-dev01 HSC-904010-4_r12p01_dummy_j0001]$ dummyTask-Gen3.py  Traceback (most recent call last):    File ""/home/mgower/mgeups/mgstack/w_2018-32-git/bin/dummyTask-Gen3.py"", line 35, in <module>      butler.put(exposure, ""modified_raw"", dataId2)    File ""/project/production/py3/lsstsw/w_2018_32/stack/miniconda3-4.5.4-10a4fa6/Linux64/daf_butler/master-g475f724dc8/python/lsst/daf/butler/core/utils.py"", line 330, in inner      return func(self, *args, **kwargs)    File ""/project/production/py3/lsstsw/w_2018_32/stack/miniconda3-4.5.4-10a4fa6/Linux64/daf_butler/master-g475f724dc8/python/lsst/daf/butler/butler.py"", line 254, in put      self.datastore.put(obj, ref)    File ""/project/production/py3/lsstsw/w_2018_32/stack/miniconda3-4.5.4-10a4fa6/Linux64/daf_butler/master-g475f724dc8/python/lsst/daf/butler/core/utils.py"", line 330, in inner      return func(self, *args, **kwargs)    File ""/project/production/py3/lsstsw/w_2018_32/stack/miniconda3-4.5.4-10a4fa6/Linux64/daf_butler/master-g475f724dc8/python/lsst/daf/butler/datastores/posixDatastore.py"", line 323, in put      location = self.locationFactory.fromPath(template.format(ref))    File ""/project/production/py3/lsstsw/w_2018_32/stack/miniconda3-4.5.4-10a4fa6/Linux64/daf_butler/master-g475f724dc8/python/lsst/daf/butler/core/fileTemplates.py"", line 154, in format      if ""?"" in format_spec:  TypeError: argument of type 'NoneType' is not iterable  {code}",0.5
"DM-15457","08/16/2018 23:31:04","Create a template for nbreport projects (LSST2018 hack)","Create a template in [https://github.com/lsst/templates] for nbreport projects (see [https://nbreport.lsst.io).|https://nbreport.lsst.io)./]    This is an LSST2018 hack project",1
"DM-15458","08/17/2018 09:42:51","Qserv does not build with boost 1.68, compiler warnings","Testing qserv with the new boost it fails:  {code}  :::::  [2018-08-17T15:38:55.627988Z] In file included from core/modules/replica/WorkerFindRequest.h:38:0,  :::::  [2018-08-17T15:38:55.628004Z]                  from core/modules/replica/WorkerFindRequest.cc:24:  :::::  [2018-08-17T15:38:55.628018Z] core/modules/replica/WorkerRequest.h:267:18: note: candidate: lsst::qserv::replica::WorkerRequest::ErrorContext lsst::qserv::replica::WorkerRequest::reportErrorIf(bool, lsst::qserv::replica::ExtendedCompletionStatus, const string&)  :::::  [2018-08-17T15:38:55.628034Z]      ErrorContext reportErrorIf(bool condition,  :::::  [2018-08-17T15:38:55.628049Z]                   ^~~~~~~~~~~~~  :::::  [2018-08-17T15:38:55.628063Z] core/modules/replica/WorkerRequest.h:267:18: note:   no known conversion for argument 1 from 'boost::system::error_code' to 'bool'  {code}    Also, fix compiler warnings for the following (and similar) statements:  {code}  In file included from core/modules/wconfig/WorkerConfig.cc:25:0:  core/modules/wconfig/WorkerConfig.h:200:18: warning: type qualifiers ignored on function return type [-Wignored-qualifiers]       unsigned int const getPriorityFast() const {                    ^~~~~  {code}  And for this warning as well:  {code}  core/modules/replica/WorkerRequest.cc: In member function 'void lsst::qserv::replica::WorkerRequest::setStatus(const lsst::qserv::util::Lock&, lsst::qserv::replica::WorkerRequest::CompletionStatus, lsst::qserv::replica::ExtendedCompletionStatus)':  core/modules/replica/WorkerRequest.cc:248:13: warning: this statement may fall through [-Wimplicit-fallthrough=]               if (0 == _performance.start_time) _performance.setUpdateStart();               ^~  core/modules/replica/WorkerRequest.cc:250:9: note: here           case STATUS_SUCCEEDED:           ^~~~  {code}    See https://ci.lsst.codes/blue/organizations/jenkins/stack-os-matrix/detail/stack-os-matrix/28425/pipeline/45",1
"DM-15459","08/17/2018 12:22:05","Make dependencies optional  in sets-of-DataUnits lookups","In the various places where we look up DataUnit information (joins, regions) for different sets of DataUnit [names], we are not consistent about whether dependencies should be included in those sets.  We should make sure all APIs support any amount of included dependencies (including optionals).    While I'd originally wanted to do this on DM-15034, and I still expect that to provide the best solution, some functionality is currently broken, so we can't afford to wait to do *something* about it.",2
"DM-15471","08/20/2018 10:34:11","Control composite disassembly at butler level","Following discussions at LSST2018 last week, we have decided that controlling whether a composite should be disassembled or not should be a butler configuration level item and not a storage class item. This allows disassembly to be controlled by datasetType or StorageClass name. There will be a new top level butler configuration item, name TBD.",3
"DM-15477","08/20/2018 12:29:11","Rename DAQForwarder and create Forwarder.h","Cleaning up the Forwarder directory before moving it.",1
"DM-15478","08/20/2018 13:06:29","Exceptions from importing lsst.afw.image cause SIGABRT","If astropy.units fails to import and a raises an exception (eg if warnings are converted to errors) when being imported via Schema.cc during import of {{lsst.afw.image}} an abort signal is received:    {code}  terminating with uncaught exception of type pybind11::error_already_set: SystemError: <built-in method __contains__ of dict object at 0x10049b900> returned a result with an error set  {code}    Stepping through schema.cc the line triggering this is:    {code}  * thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 2.1      frame #0: 0x0000000116e3d53d schema.so`void lsst::afw::table::(anonymous namespace)::declareSchemaType<unsigned char>(mod=0x00007ffeefbf33c0) at schema.cc:187 [opt]     184      std::string suffix = FieldBase<T>::getTypeString();     185      py::str pySuffix(suffix);     186    -> 187      py::object astropyUnit = py::module::import(""astropy.units"").attr(""Unit"");     188       189      // FieldBase     190      PyFieldBase<T> clsFieldBase(mod, (""FieldBase"" + suffix).c_str());      * frame #0: 0x0000000116e3d53d schema.so`void lsst::afw::table::(anonymous namespace)::declareSchemaType<unsigned char>(mod=0x00007ffeefbf33c0) at schema.cc:187 [opt]      frame #1: 0x0000000116e3d143 schema.so`lsst::afw::table::(anonymous namespace)::pybind11_init_schema(mod=0x00007ffeefbf33c0) at schema.cc:398 [opt]      frame #2: 0x0000000116e3cf5d schema.so`::PyInit_schema() at schema.cc:390 [opt]    {code}    Something in the pybind11 layer is failing to catch the python exception and forward it on.    Depending on your system this can be reproduced with:  {code:python}  import warnings  warnings.simplefilter(""error"")  import lsst.afw.image  {code}    Given the reference to schema.cc and {{__contains__}} I initially thought this was related to DM-15406 and there is some commentary there.",1
"DM-15486","08/21/2018 09:43:48","ImageServ not logging to file","During k8s tesing, [~cbanek] saw the nginx log showing ImageServ can't open its log file:           /var/log/imageserv.log    Further diagnosis indicates the /var/log directory has write permission for root only, whereas ImageServ is running as webserv.     The proper fix is to move the log file to location where webserv has write permission.",2
"DM-18266","08/21/2018 11:52:26","Begin TMA software FAT ","Travel to Spain for Mount FAT campaign",1
"DM-15490","08/21/2018 12:53:00","Modify behavior of 2D-histogram (heatmap) fallback in Firefly","The Firefly science advisory panel has discussed the desired behavior for Firefly across multiple applications in respect of the toolkit's ability to fall back to creating a 2D histogram (""heat map"") in cases where a user requests too many points for a scatterplot.    We recommend the following for implementation:    • If the user explicitly requests a 2D histogram (“heat map”), via API or UI action, they get that no matter what.  It never changes to a scatterplot.  • If the user requests a scatterplot, but there are more than {{N_hist}} (an application-configurable parameter) rows in the associated table, the plot is generated and displayed as a 2D histogram, but it is internally marked as “really” being a scatterplot.  • Then if, as a result of filtering or some other action that changes the size of the table, the number of points falls back to or below {{N_hist}}, the plot is regenerated and displayed as a scatterplot.  • This applies regardless of whether the filter is applied to some other variable(s) in the table or to the plotted variables themselves.  Thus, for example, if the user draws a selection rectangle on the 2D histogram - effectively “zooming in” on a region - which selects {{N_hist}} or fewer points, and clicks on the “filter” button on the plot, the selected region will be re-drawn as a scatterplot.  • There may be a second application-configurable size-limit parameter, less than or equal to {{N_hist}}, which I’ll call {{N_GL}}, which controls a transition between “traditional” Plotly scatterplots and Plotly WebGL scatterplots, with the latter used when the number of points exceeds {{N_GL}}.  The parameterization must allow an application to set {{N_GL = N_hist}} or use some equivalent means of requiring that the WebGL scatterplot never be used.  (Eventually, however, we hope that the WebGL scatterplots become a complete replacement for the traditional ones.)    This issue has already been through ""FireflyCCB-D""-style review and we are requesting implementation.    The schedule should be discussed at today's CCB meeting.  [~gpdf], at least, views this as a high-priority item, with the current behavior leaving an unfavorable impression of Firefly's ability to handle large datasets.",8
"DM-15491","08/21/2018 13:21:36","Parenthesis are ignored in the WHERE clause of qserv queries","In a query such as   {code:java}MariaDB [qservTest_case01_qserv]> select objectId, ra_PS from Object where objectId = 417853073271391 or objectId = 399294519599888;  +-----------------+------------------+  | objectId        | ra_PS            |  +-----------------+------------------+  | 399294519599888 | 359.337499495437 |  | 417853073271391 | 359.894210551011 |  +-----------------+------------------+  2 rows in set (0.11 sec)MariaDB [qservTest_case01_qserv]> select objectId, ra_PS from Object where ra_PS > 359.5 and (objectId = 417853073271391 or  objectId = 399294519599888);  +-----------------+------------------+  | objectId        | ra_PS            |  +-----------------+------------------+  | 399294519599888 | 359.337499495437 |  | 417853073271391 | 359.894210551011 |  +-----------------+------------------+  2 rows in set (0.11 sec) {code}  the second query should have returned only objectId 417853073271391, because the ra_PS value for objectId 399294519599888 is below the comparison value (of 359.5).    Based on initial research, it appears that the parenthesis are getting dropped from the query during processing in the Czar; the mysql-proxy-lua.log shows:  {code:java}(core/modules/qproc/QuerySession.cc:120) - Query Plugins applied:   QuerySession description:    original: select filterId,filterName from Filter where filterId > 2 and(filterName='y' or filterName='g')    has chunks: 0    chunks: []    needs merge: 0    1st parallel statement: SELECT filterId,filterName FROM qservTest_case01_qserv.Filter AS QST_1_ WHERE filterId>2 AND filterName='y' OR filterName='g' {code}  (note that in ""1st parallel statement"", the parens are omitted)          ",8
"DM-15492","08/21/2018 14:17:43","Create prototype of Portal's generic table-browsing interface, based on TAP","*(Editing not finished, please wait before responding, etc.)*     With {{dbserv}} now converging on providing a TAP service, it's time to implement the ""generic table browsing"" feature that we need in the Portal so that we don't have to implement custom screens for every table.    The ""generic browsing"" screen should take a TAP endpoint as a parameter (this may come from a configuration parameter of the Portal in LSST's standard usage, or it may come from a ""higher level"" search facility that interacts with the VO Registry).    Given this parameter, it should use {{TAP_SCHEMA}} and/or the {{.../tables}} endpoint of the TAP service to obtain a list of tables, and display this list to the user.",0
"DM-15494","08/21/2018 15:15:14","Pyarrow segfaults on shared stack on lsst-dev","When I try to run {{writeObjectTable.py}} on lsst-dev, it fails with a segfault & long backtrace, starting like:  {noformat}  [tmorton@lsst-dev01 DM-14289]$ bash writeTables_test.sh  Caught signal 11, backtrace follows:  /ssd/lsstsw/stack3_20171021/stack/miniconda3-4.3.21-10a4fa6/Linux64/utils/16.0-6-g3610b4f/lib/libutils.so(+0x15214) [0x7f078a865214]  /usr/lib64/libc.so.6(+0x362f0) [0x7f080a5892f0]  /ssd/lsstsw/stack3_20171021/python/miniconda3-4.3.21/bin/../lib/libstdc++.so.6(std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)+0xb) [0x7f08032b640b]  /ssd/lsstsw/stack3_20171021/python/miniconda3-4.3.21/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.1(+0x177ddc) [0x7f071bcb6ddc]  /ssd/lsstsw/stack3_20171021/python/miniconda3-4.3.21/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.1(+0x1696c3) [0x7f071bca86c3]  /ssd/lsstsw/stack3_20171021/python/miniconda3-4.3.21/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.1(+0x175e8b) [0x7f071bcb4e8b]  /ssd/lsstsw/stack3_20171021/python/miniconda3-4.3.21/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.1(parquet::ApplicationVersion::ApplicationVersion(std::string const&)+0x6d) [0x7f071bc4aa5d]  /ssd/lsstsw/stack3_20171021/python/miniconda3-4.3.21/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.1(+0x83590) [0x7f071bbc2590]  /lib64/ld-linux-x86-64.so.2(+0xfb03) [0x7f080b964b03]  /lib64/ld-linux-x86-64.so.2(+0x146de) [0x7f080b9696de]  /lib64/ld-linux-x86-64.so.2(+0xf914) [0x7f080b964914]  /lib64/ld-linux-x86-64.so.2(+0x13ccb) [0x7f080b968ccb]  /usr/lib64/libdl.so.2(+0xfbb) [0x7f080b02dfbb]  ...etc  {noformat}    The script I am running is the following:  {noformat}  RERUN1=/datasets/hsc/repo/rerun/RC/w_2018_28/DM-14988/  OUTPUT1=/project/tmorton/DM-14289/w28  RERUN2=/datasets/hsc/repo/rerun/RC/w_2018_26/DM-14689/  OUTPUT2=/project/tmorton/DM-14289/w26    TRACT=9615  PATCH=4,4  FILTERS=HSC-G^HSC-R^HSC-I    writeObjectTable.py $RERUN1 --output $OUTPUT1 --id tract=$TRACT patch=$PATCH filter=$FILTERS --no-versions -j 20  writeObjectTable.py $RERUN2 --output $OUTPUT2 --id tract=$TRACT patch=$PATCH filter=$FILTERS --no-versions -j 20  writeQATable.py $OUTPUT1 --output $OUTPUT1 --id tract=$TRACT  patch=$PATCH --no-versions -j 20 --clobber-config  writeQATable.py $OUTPUT2 --output $OUTPUT2 --id tract=$TRACT  patch=$PATCH --no-versions -j 20 --clobber-config  {noformat}    I can run the identical script in the jupyterlab environment container and everything is great.    I think this is related to the fact that I can run the qa_explorer tests on the JL env but not lsst-dev, where it hangs with a segfault as well (DM-14224).  It is making testing of DM-14289 difficult, since I can't do it on lsst-dev.",1
"DM-15498","08/21/2018 16:43:57","MOC display doesn't show up when the image is zoomed in with FOV around 1 degree","view HiPS image by unchecking 'IRSA featured' and selecting image 'HST/NICMOS' which is with 'hips_order = 16'.     Get the MOC display and zoom in the image until the MOC display hangs at some point where fov is about 1 degree or less.          Implemented:    This development fixes the MOC display bug by using 'division' instead of 'right shift' to calculate 'npix' of lower tile where the higher order tile is nested in. This change is due to the fact that Javascript bitwise operation treats the operands as a sequence of 32 bits, the result will become inaccurate if doing bit shift on 'npix' of some higher order tile.    test:  searching HiPS 'HST/NICMOS - all bands' by un-checking IRSA Featured and selecting the last image from the list.  check 'MOC' layer  zoom in the image  the MOC of all levels (up to order 16) will be shown as image is zoomed in with smaller fov.",1
"DM-15500","08/21/2018 19:37:44","Add FITS image, catalog readers that infer types from file","As they're implemented in C++ and return non-polymorphic types, all of our FITS readers coerce the on-disk object to a specific type.  Gen3 Butler code could be greatly simplified by providing Image, Mask, Exposure, and Catalog readers that return infer the template specialization from what's actually in the file.",2
"DM-15502","08/22/2018 05:54:19","rsync data and support test stand data ingestion better","Some headers seem to be missing, set defaults where it's safe, and do whatever else is necessary within reason.",1
"DM-15504","08/22/2018 12:12:11","Separate a local firefly work area from a shared work area","Firefly will get better performance when it can use local disk (SSD drives are even better) for cache.  Since in the multi server environment we only need to share a small amount of work area we need to change the system to have both a work area and a shared work area.  The shared work area will be for uploads and staging.  The work area is for everything else.",3
"DM-15509","08/22/2018 14:34:43","Fix bitrot due to premature merge of DM-13293 in obs_lsstCam","TL;DR - Merlin messed up and merged early (there were reasons at the time), so this ticket is to apply the changes necessary here to deal with the changes to {{cp_pipe}} from review comments.",1
"DM-15513","08/22/2018 17:47:28","jointcal test outputs collide","It appears that at least two jointcal tests write to {{.test/JointcalTestCFHTMinimal/run/verify}}.  This may be due to an incorrect {{caller}} in https://github.com/lsst/jointcal/blob/master/tests/test_jointcal_cfht_minimal.py#L155",0.5
"DM-17337","08/23/2018 13:27:56","Work with PTG release v0.1","Perform some initial evaluation of release:    'ts_pointing_common'  tagged it as v0.1.",2
"DM-15523","08/23/2018 15:42:40","geom has random failure on macOS in polynomials test","Some builds are failing on macOS in test_polynomials. It seems that the test is unstable in that it sometimes passes and sometimes fails.    Example failure:    {code}  Failed test output:  tests/test_polynomials    Running 9 test cases...  a=0.2272165537183282, b=0.2272165537183142, diff=1.398881011027697e-14 > 1.009044198056236e-14  tests/test_polynomials.cc:398: error: in ""simplified2d"": check compare(a1, b1, 100*DEFAULT_RTOL) has failed  a=-0.01667616780189118, b=-0.01667616780189041, diff=7.771561172376096e-16 > 7.405706182468739e-16  tests/test_polynomials.cc:398: error: in ""simplified2d"": check compare(a1, b1, 100*DEFAULT_RTOL) has failed  a=0.0221159430012603, b=0.02211594300126229, diff=1.998401444325282e-15 > 9.821451652519151e-16  tests/test_polynomials.cc:398: error: in ""simplified2d"": check compare(a1, b1, 100*DEFAULT_RTOL) has failed  a=0.0221159430012598, b=0.02211594300126181, diff=2.012279232133096e-15 > 4.910725826259466e-16  tests/test_polynomials.cc:402: error: in ""simplified2d"": check compare(a1, b1, 50*DEFAULT_RTOL) has failed  a=-0.02074950336545145, b=-0.02074950336544967, diff=1.77635683940025e-15 > 9.214630554344155e-16  tests/test_polynomials.cc:398: error: in ""simplified2d"": check compare(a1, b1, 100*DEFAULT_RTOL) has failed  a=-0.0207495033654517, b=-0.02074950336545071, diff=9.853229343548264e-16 > 4.60731527717222e-16  tests/test_polynomials.cc:402: error: in ""simplified2d"": check compare(a1, b1, 50*DEFAULT_RTOL) has failed    *** 6 failures are detected in the test module ""polynomials""  {code}    Master does seem to build fine some of the time. Running the test gives a different answer each time so there seems to be a repeatability issue.",0.5
"DM-15527","08/23/2018 17:49:11","Albuquery returns 400 when qserv is down.","Kenny noticed that when qserv was down, albuquery was returning a 400 as a response.  This is tricky because a 400 class response implies that something was wrong with the request, or the person making the request.  A 500 class error seems more appropriate, although you can still do a retry on a 500, which isn't exactly perfect either.  But at least a specific error code with a better message while a database is down is probably a good idea.",1
"DM-15529","08/24/2018 09:43:19","Allow datastores to disable default file templates","Following a discussion in DM-15189, we need to be able to turn off default file name templating in local datastore configurations. A default is provided in the base configuration but there is currently no way to turn this off without putting in a dummy string that will cause a confusing error message.",0.5
"DM-15533","08/24/2018 15:08:40","Allow StorageClasses to define an inheritance tree","It would be useful to be able to define a hierarchy of StorageClasses in Python so that checks isinstance() checks can work and we can define related storage classes across different configuration files (currently YAML reference syntax only works within the single file).",3
"DM-15534","08/24/2018 15:40:14","Undefined variable names in MultibandDriver","DM-15104 introduced some undefined variable names into MultibandDriver.py. Because there are no automated checks in in place for pipe_drivers (no unit tests or Travis) these were missed.    {{--reuse deblendCoaddSources}} will not work:  https://github.com/lsst/pipe_drivers/blob/master/python/lsst/pipe/drivers/multiBandDriver.py#L419",3
"DM-15536","08/24/2018 16:59:31","Add level of indirection in defining Visits from Exposures","In discussions with [~rhl], we decided that it would be best for multiple mutually-inconsistent associations of exposures (snaps) into visits should be permitted in the same Gen3 data repository.  We cannot assume that we will be able to do that association in advance and not want to revisit it in later reprocessing.    On the database side, I think we'd need to add a field to the visit table indicating which of several named systems the visit belongs to, while ensuring that visit IDs are unique across all systems.  We'd select exactly one system via configuration to be used by any butler client, though we'd need to think about when we'd have to be able to refer to other systems instead of relying on that selection (e.g. when interacting with datasets produced on some other system).  We'll also need to move the {{exposure.visit}} field into a separate join table.    The way information about visits will be transmitted from the LSST hardware is via a ""GROUP"" header card; to make use of that, we should:   * propagate that through ObservationInfo into the exposure table during raw ingest;   * remove visit definition (and all of the stuff dealing with visit regions) from raw ingest;   * add a new task to be run after ingest that creates visits (with regions, where appropriate) from groups of exposures with the same GROUP value.  That would involve generating visit IDs and names from GROUP via some algorithm.    For instruments that want visits to be 1-1 with exposures, we'll just define GROUP accordingly.  If we ever want to define a set of visits that isn't strictly derived from GROUP, we can write an alternate version of the task that populates the same tables via some other means.",8
"DM-15537","08/24/2018 17:00:47","Rename Sensor to Detector in Gen3 schema","Detector is the name already heavily used throughout afw.cameraGeom in particular, and we should adopt that terminology in Gen3 rather than trying to change the existing codebase.",2
"DM-15543","08/27/2018 06:31:23","Test for zero-length header skels and other related issues","Prevent the Forwarder from tanking if something is amiss when forwarding.    NOTE: This ticket does +not+ check the FITS cards against an inclusion list or such. It merely catches problems that upset FITSIO.",2
"DM-15544","08/27/2018 09:49:22","ExposureCatalog should support new photoCalib objects ","Currently, {{ExposureCatalog}} supports old {{Calib}} calibration objects.  However, for use with {{jointcal}} or {{fgcmcal}} these need to support new {{PhotoCalib}} calibration objects.  This will also allow multiple {{PhotoCalib}} objects to be bundled into one persistence file (per visit, for example, the number of required inodes would be reduced by a factor of 100 for HSC data for {{fgcmcal}} outputs).",2
"DM-15558","08/27/2018 14:22:19","re-enable osx tarball builds","EUPS binary/tarball builds on osx were disabled many weeks ago due to strange problems with eups https downloads from {{eups.lsst.codes}} hanging.  Since that time, the sqre osx environment has been ""white-listed"" with the NOAO traffic mangler – it needs to be determined if this has resolved the problem and if not, the root cause needs to be identified.",1
"DM-15561","08/27/2018 15:26:16","Rationalize Composites/Formatter/Template access to use DatasetType","Currently the getters for formatters and templates use a name string to determine which formatter/template to use. It would be better to match the CompositesMap interface and allow DatasetType to be given.",3
"DM-15563","08/28/2018 06:49:10","Refactor Mask global state and make it thread-friendly","This is a spin-off from DM-15500: I needed to move some code out of Mask.cc; in order to do that, I needed to refactor it a bit, and from there it snowballed into a near rewrite of one corner of Mask's implementation, including adding a mutex to guard access to global state.  Since that brings it pretty far from DM-15500's original goal, it makes sense to make that a separate review and merge.    The end result is intended to be entirely backwards compatible with the old Mask API and behavior, and that takes some other cleanup work off the table.  But I think the result is still a much safer and easier-to-understand implementation.",2
"DM-15574","08/29/2018 06:28:50","Amend DM ATS system l1d check services script","The check services script in the start_up dir currently dumps systemctl details to the screen. This means that there are _active_ AND _running_ states listed for each of the 7 services. This may be daunting or confusing for an end-user. While this capability is needed at times, it should require a `-v` for verbose switch on the command line of the script. The non-switch default running of the script should simply list each DM service and state whether it is running or not.         The script should also call redis-cli and determine what OCS command state the system is currently in, and print that to the screen by default as well - whether the -v switch is used or not.",2
"DM-15576","08/29/2018 07:16:45","Unique names for ATS archive image dirs each day","The target location for an assembled and formatted image file is currently specified within the L1 system config file, as a directory path. This ticket is for the addition of a way to automate the target directory. The L1 system config file will still include a path to the archive location, but the software will append the day-month-year string to the path.    This is an initial and probably temporary measure, but as Patrick and Robert take test images with the ATS camera, they need some way to easily distinguish which images were taken on which day. Eventually, the archive will work hand in hand with the Data Backbone; and this may require a change when it arrives.",2
"DM-15577","08/29/2018 10:56:06","Fix typo in PackedIndex.h header guard","PackedIndex.h has a typo in its header guard which produces a clang warning    LSST_AFW_MATH_POLYNOMIALS_PackedInded_h_INCLUDED should be LSST_AFW_MATH_POLYNOMIALS_PackedIndex_h_INCLUDED    That is the only compiler warning I see on clang",0
"DM-15587","08/29/2018 15:37:27","jenkins ""ALPN callback dropped"" error message in logs","This error message is reported over and over again in the jenkin's master logs, which is is an irritation:  {code:java}  ALPN callback dropped: SPDY and HTTP/2 are disabled. Is alpn-boot on the boot class path?  {code}  ",0.5
"DM-15606","08/30/2018 13:13:31","Add jointcal config defaults to at least obs_subaru","Add reference catalog defaults to at least obs_subaru to facilitate running jointcal on the HSC biweeklies.     [~Parejkoj]  I assigned it to you, but I can just as easily do it if one of you sends me the command to run jointcal on the RC2 dataset so I can test that the configs make it do what we expect.  ",3
"DM-15613","08/31/2018 09:23:18","Unsigned, uncompressed FITS images written with incorrect BZERO","Writing an uncompressed Image with an unsigned integer type to FITS with our code results in a BZERO value one less than what CFITSIO or astropy write (without our code intervening in the cast of CFITSIO), and this confuses CFITSIO, astropy, _and_ our own code about the type of the pixels when reading the image back in (because the convention is to use BZERO to distinguish between signed and unsigned types).  I'm not sure if the same problem appears in compressed images or not.    I *think* this can be fixed by modifying the Bzero traits class in fitsCompression.cc, and if that's right most of the work here will just be test code.",1
"DM-15634","09/05/2018 11:58:27","Fix DcrCoadd variance plane bug","The variance planes of DcrCoadds have unusual-looking double lobed features near most bright sources. This ticket is to investigate the variance planes to determine the cause of the feature, and fix it if it is a bug. Additionally, investigate the impact of the variance plane on source measurement.",8
"DM-15641","09/05/2018 14:52:44","Reviews and consulting for numpydoc conversion work","This ticket covers work associated with reviewing tickets and advising with regards to numpydoc conversions. Primarily for DM-15347.",1
"DM-15646","09/05/2018 17:24:05","Rewrite header translation infrastructure for butler Gen 3","As decribed [on confluence|https://confluence.lsstcorp.org/display/DM/Observational+Metadata+Data+Structures+and+Extraction] we need a set of classes and methods for taking a PropertyList, extracting the relevant information and creating an object summarizing that information in a standard form. Cameras can implement their own subclasses to override specific information, with the intent to reuse code wherever possible when a header has the same definition across multiple cameras.    We have not yet decided whether this should go into obs_base or be a standalone infrastructure package.",20
"DM-15652","09/06/2018 13:08:11","Add missing calexp_camera dataset template to obs_decam","The dataset template {{calexp_camera}} is required to use {{visualizeVisit.py}} in pipe_drivers. This ticket is to add an appropriate template for obs_decam.",0.5
"DM-15653","09/06/2018 14:41:09","Add native yaml serialization support to daf_base","In DM-14503 support for YAML serialization of PropertySet and PropertyList was added to daf_persistence.  To allow gen3 butler to serialize these using YAML it makes sense to move the YAML support directly into daf_base.  This would require a pyyaml dependency on daf_base but we already use yaml in many places and it's in the core python dependency list. YAML support could be optional in the sense that it is only enabled if yaml can be found.",1
"DM-15659","09/06/2018 18:57:38","Change AssociationL1DBProtoConfig's default `dia_object_index` to ""baseline""","Recent changes to l1dbproto's DiaObjectLast table have caused issues within ap_association. Since ap_pipe does not not currently utilize the full features of l1dbproto it seems safer to switch this default to ""baseline"" instead.",1
"DM-15676","09/07/2018 15:47:05","Make PropertySet/List more dict-like","Whilst working on DM-15653 it became clear that test code would be significantly simplified in places if more dunder methods were implements in PropertySet and PropertyList. This would also simplify the work on obs_metadata and make that less dependent on the LSST FITS header representation in DM-15646.    For this ticket dict-like dunder methods will be added as well as others such as eq.",2
"DM-15677","09/07/2018 16:18:25","Update DPDD to specify contents of DIAForcedSources and include them in alerts","Implementation ticket for RFC-517.",3
"DM-15678","09/07/2018 16:36:17","Outline DMTN for Alert Distribution Design Doc","This ticket is to create a skeleton design document for Alert Distribution as a DMTN.",2
"DM-15682","09/07/2018 16:50:21","Add str() for afw::Image and afw::Mask","It would be a useful convenience to not have to always append {{.getArray()}} when wanting to print an {{afw::image}} when debugging. It should be simple to add a {{std::cout}}/{{str()}} method that delegates to the existing {{.getArray()}} and adds the xy0/origin, and other relevant (short) metadata (not the maskPlaneDict for Masks though).",1
"DM-15688","09/10/2018 10:31:48","Qserv testQDisp unit test is broken in master","Building qserv from master on CentOS7 (against current `qserv-dev` tag) fails in testQDisp unit test:  {noformat}  [salnikov@centos-dev-vm qserv]$ ./build/qdisp/testQDisp -l all -x  Running 5 test cases...  Entering test module ""Qdisp_1""  core/modules/qdisp/testQDisp.cc(198): Entering test suite ""Suite""  core/modules/qdisp/testQDisp.cc(207): Entering test case ""Executive""  lsst.qserv.qdisp.JobStatus INFO: QI=0:0; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:0; Updating state to: COMPLETE (success) code=0   core/modules/qdisp/testQDisp.cc(229): info: check tEnv.jqTest->getStatus()->getInfo().state == qdisp::JobStatus::COMPLETE has passed  core/modules/qdisp/testQDisp.cc(230): info: check tEnv.ex->getEmpty() == true has passed  lsst.qserv.qdisp.JobStatus INFO: QI=0:0; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:0; Updating state to: COMPLETE (success) code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:1; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:1; Updating state to: COMPLETE (success) code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:2; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:2; Updating state to: COMPLETE (success) code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:3; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:3; Updating state to: COMPLETE (success) code=0   core/modules/qdisp/testQDisp.cc(243): info: check tEnv.ex->getEmpty() == true has passed  lsst.qserv.qdisp.JobStatus INFO: QI=0:0; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:1; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:2; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:3; Updating state to: REQUEST code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:4; Updating state to: REQUEST code=0   core/modules/qdisp/testQDisp.cc(260): info: check tEnv.ex->getEmpty() == false has passed  lsst.qserv.qdisp.JobStatus INFO: QI=0:0; Updating state to: COMPLETE (success) code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:1; Updating state to: COMPLETE (success) code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:2; Updating state to: COMPLETE (success) code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:3; Updating state to: COMPLETE (success) code=0   lsst.qserv.qdisp.JobStatus INFO: QI=0:4; Updating state to: COMPLETE (success) code=0   core/modules/qdisp/testQDisp.cc(265): info: check tEnv.ex->getEmpty() == true has passed  root ERROR: timeoutFunc done=1 total=1000000 timedOut=0  core/modules/qdisp/testQDisp.cc(170): info: check done == true has passed  core/modules/qdisp/testQDisp.cc(207): Leaving test case ""Executive""; testing time: 490ms  core/modules/qdisp/testQDisp.cc(272): Entering test case ""MessageStore""  core/modules/qdisp/testQDisp.cc(275): info: check ms.messageCount() == 0 has passed  lsst.qserv.qdisp.MessageStore ERROR: Add msg: 124 -12 test2  lsst.qserv.qdisp.MessageStore ERROR: Add msg: 86 -12 test3  core/modules/qdisp/testQDisp.cc(280): info: check ms.messageCount() == 3 has passed  core/modules/qdisp/testQDisp.cc(281): info: check ms.messageCount(-12) == 2 has passed  core/modules/qdisp/testQDisp.cc(283): info: check qm.chunkId == 124 && qm.code == -12 && str.compare(qm.description) == 0 has passed  core/modules/qdisp/testQDisp.cc(272): Leaving test case ""MessageStore""; testing time: 10ms  core/modules/qdisp/testQDisp.cc(287): Entering test case ""QueryRequest""  unknown location(0): fatal error: in ""Suite/QueryRequest"": std::system_error: Resource temporarily unavailable  core/modules/qdisp/testQDisp.cc(287): last checkpoint: ""QueryRequest"" test entry  core/modules/qdisp/testQDisp.cc(287): Leaving test case ""QueryRequest""; testing time: 10ms  core/modules/qdisp/testQDisp.cc(347): Entering test case ""ExecutiveCancel""  unknown location(0): fatal error: in ""Suite/ExecutiveCancel"": std::system_error: Resource temporarily unavailable  core/modules/qdisp/testQDisp.cc(347): last checkpoint: ""ExecutiveCancel"" test entry  core/modules/qdisp/testQDisp.cc(347): Leaving test case ""ExecutiveCancel""; testing time: 10ms  core/modules/qdisp/testQDisp.cc(388): Entering test case ""ServiceMock""  core/modules/qdisp/testQDisp.cc(390): info: check qdisp::XrdSsiServiceMock::isAOK() has passed  core/modules/qdisp/testQDisp.cc(388): Leaving test case ""ServiceMock""  core/modules/qdisp/testQDisp.cc(198): Leaving test suite ""Suite""; testing time: 520ms  Leaving test module ""Qdisp_1""; testing time: 520ms    *** 2 failures are detected in the test module ""Qdisp_1""  {noformat}    Full log with DEBUG messages is attached too.",1
"DM-15689","09/10/2018 11:13:28","Fix for PreFlight needed after schema migration","DM-15336 changed API of the DataUnits which breaks PreFlight. Apparently unit tests that we have for PreFlight in daf_butler do not catch this sort of change. Need to check unit test to see if it can cover something like this. And fix the issue of course.",1
"DM-15691","09/10/2018 14:05:32","The raw_visit table in obs_lsstCam registry is not optimized","As is in obs_lsstCam, the {{raw_visit}} table in registry.sqlite3 has the same per-file informarion as the {{raw}} table. But it should be just the visit information for optimization in obs_base.     I think this is to fix {{config.register.visit}} in obs_lsstCam config/ingest.py          ",1
"DM-15693","09/10/2018 15:24:25","RadioGroupInputField: options modified in reducer function are not always rendered correctly","Firefly allows to maintain the state of the field in reducer function.    When the number new options is smaller than the number of previous options, the extra options are still displayed.    The culprit is smartMerge in FieldGroupCntlr.fireFieldsReducer - it's not correct to merge oldFields and newFields here , the new fields that have changed should replace the old.",2
"DM-15694","09/10/2018 15:47:27","Problem running the integration tests with missing config","[~cbanek] reported that integration tests can't be run, and the problem encountered was missing imgserv_conf.json.    Turns out the tests still referred to the old ""settings.json"" that only exists in my dev/testing environment.",3
"DM-15695","09/10/2018 17:31:11","Measure jointcal acceptance statistics","Now that we have reliable output from jointcal+validate_drp, we need to compute the final summary statistics described on the [acceptance test|https://confluence.lsstcorp.org/display/DM/JointCal+Acceptance+Tests] page. This is probably most easily added as a final step of jointcal_compare's {{summarizePerformanceRst.py}}.",2
"DM-15705","09/10/2018 19:28:47","Create an RFC for a dict-like interface to PropertySet/List","Before finalizing DM-15646, we should get feedback from the DM developers on exactly what the interface should look like.",0.5
"DM-15707","09/10/2018 19:30:20","Get instrument, telescope, and DATE-OBS working in obs_metadata","Write a prototype obs_metadata implementation that can translate FITS standard header cards for instrument, telescope and observation date.",1
"DM-15713","09/11/2018 03:32:43","Run jointcal comparison with higher order polynomial","To see whether it improves the outlier rejection fraction, I'm going to increase the visit polynomial order by 2 (from 5 to 7) and see if that helps. It's just CPU time: don't have to make any real modifications to the execution scripts.",1
"DM-15719","09/11/2018 13:07:41","afw's test testReadFitsWithOptions needs afwData","The test {{testReadFitsWithOptions}} in {{test_maskedImageIO.py}} assumes that {{afwData is available, so it needs a}}  {code:java}  @unittest.skipIf(dataDir is None, ""afwdata not setup"") {code}  decorator",1
"DM-15721","09/11/2018 14:07:54","Config __delitem__ doesn't work","Config inherits from UserDict and hence gets a lot of dict-like methods.   They don't all seem to work; {{get}} didn't until DM-15424, and {{del}} doesn't seem to work either, at least when nested below the top level.    We should probably add tests for all dict-like operations.    As an alternative, I've been working around these issues by just working directly with the {{data}} attribute, which really is a nested dictionary (with some lists).  I'm finding that much more pleasant.  Perhaps we should expose that more directly, and remove more of Config's implementation, i.e. make it so the children of a Config are not themselves expected to be Configs.",1
"DM-15724","09/11/2018 18:13:55","Test plan for milestone DM-SUIT-6 (LSSTCam data display and visualization)","Write the test plan for milestone DM-SUIT-6,     LSSTCam data display and visualization    Dec. 12, 2019 update:    Test case LVV-T368 includes the test case for Firefly    * That results can be displayed in the Firefly display tool.    DMTR-112 covered testing the display with Firefly Python API firefly_client. ",2
"DM-15728","09/12/2018 12:13:10","Get next ACK_ID message","The OCS_Bridge has a very occasional need for a unique number from a sequence. Rather than including an OTS in-memory DB like the type that the python components use (Redis), or fabricate one, the OCS_Bridge could simply request a unique number from the DMCS via an internal message...as the DMCS has a scoreboard class specifically for number sequences. This would not be a resource drain or a possible introduction of latency, because these values would only be requested at startup, when the internal message network is at its most quiet.         This ticket, if pursued, has two parts; one is the addition of a simple handler for the new message in the DMCS, and the other part is the generation of the request in the OCS_Bridge and the handling of the response.",2
"DM-15735","09/12/2018 22:53:38","Notebook for Super Computing 2018","Create a notebook for Jeff to display every nth image comming into some dir.",2
"DM-15740","09/13/2018 10:19:03","Monthly report ","Write summary for Monthly report",1
"DM-15743","09/13/2018 11:18:28","Prepare Talk for Brazil ","DM overview and status , a bit of LSST status as well.",3
"DM-15751","09/14/2018 08:38:22","Configure ability to build coadds with either Jointcal or meas_mosaic","John, I added you as a watcher in case I was duplicating effort.    Plan is to make lightweight task that can ""applyJointcalResults"" to calexps (based on [https://github.com/lsst/jointcal/blob/master/python/lsst/jointcal/jointcalCoadd.py)]  and make choice of meas_mosaic or jointcal retargetable.  Need to add the ability to do apply the photometric calibration in addition to the WCS (like [https://github.com/lsst/meas_mosaic/blob/master/python/lsst/meas/mosaic/updateExposure.py#L47)]          Outstanding question: do we want to be able to mix and match? Photo from one and WCS from another?     ",2
"DM-15756","09/14/2018 11:32:53","biasCorr calculation code results disagrees when using main task code","{noformat}import lsst.cp.pipe.makeBrighterFatterKernel as kernelGen  biases, means, xcorrs  = kernelGen.calcBiasCorr([70000, 90000, 110000], (2000,1000), useTaskCode=False, nSigma=5, repeats=3){noformat}  and  {noformat}biases2, means2, xcorrs2 = kernelGen.calcBiasCorr([70000, 90000, 110000], (2000,1000), useTaskCode=True, nSigma=5, repeats=3){noformat}  differ, and they shouldn't. Investigate and fix.         Furthermore, as per what was originally DM-15401: something seems to not be quite right in the calculation of cross-correlations in {{MakeBrighterFatterKernelTask}}, as the sum of the cross-correlations are coming out much higher than they should be (I think, based on the code it was ported from at least).    Find out why, and once that's done, put the config value {{xcorrCheckRejectLevel}} back to the nominal value of 0.2 (or was it 0.1, check Will's code), if appropriate.    Given that these may be related, these are being combined.",5
"DM-15757","09/14/2018 13:36:19","obs_decam's apPipe config should default to CP calibs for now","For now, the majority of ap_pipe use cases with obs_decam use community pipeline (CP) bias and flat calibration products. It is well known that these require a separate set of processCcd configs, and when running processCcd from the command line, users typically provide the {{obs_decam/config/processCcdCpIsr.py}} file.    However, the {{obs_decam/config/apPipe.py}} file (which ap_pipe automatically uses! no command-line config specification required!) presently points to {{obs_decam/config/processCcd.py}}. This causes the infamous ""No locations for get: bias"" error because there are no biases, there are instead cpBiases.    This ticket is to switch the default in {{obs_decam/config/apPipe.py}} to point to {{obs_decam/config/processCcdCpIsr.py}} and add a comment so future ap_pipe users are hopefully less confused than I was.",0.5
"DM-15764","09/14/2018 16:38:12","fail to retrieve calibration data with obs_lsstCam","obs_lsstCam calibrations policy uses as {{date}} as {{obsTimeName}}, not {{dateObs}}.     {noformat}  RuntimeError: Unable to retrieve bias for {'visit': 185615, 'detector': 29, 'run': '185615', 'raftName': 'R10', 'detectorName': 'S02', 'snap': 0}: no such column: date  {noformat}",2
"DM-15771","09/17/2018 09:29:27","Remove unused remnants of pex_policy","Remove remnants of pex_policy, which was long ago shifted to used {{pex_config}}.    The main things being gotten rid of are:    Commands:    killcondor.py - used the old style pex_policy job description to identify and kill jobs.  writeNodeList.py - this was required in an early version of ctrl_orca to enumerate the nodes used (in pbs jobs, I believe).  ProvenanceRecorder.py - This was used to make database records for policy files used in a run.  This was in addition to the provenance information that used to be recorded via the provenance package, which is also obsolete, since it also used pex_policy.  I don't think a replacement was ever made for that package.    lsst/ctrl/orca:    PolicyUtils.py - used to get all file names listed in a policy    The ups/ctrl_orca.table needs to have pex_policy removed as a required setup.",1
"DM-15772","09/17/2018 09:53:48","Rename cpTask.py","Task is misnamed - should be called something like `runEotestTask.py`. Nobody uses this at the moment, so no RFC should be required.    Also, add Travis checking and reformat docstring for flake8.",1
"DM-15774","09/17/2018 11:00:38","Initial Sphinx-based Task documentation for packages","Implement initial Task documentation in packages (such as {{lsst.pipe.tasks}}) based on the in-development template ( DM-15422) and using the custom Sphinx roles and directives (DM-15472 ).",3
"DM-15776","09/17/2018 13:20:31","Reimplement FitsStorage support for direct PropertyList reads","The move of the implementation of FitsStorage on DM-15599 neglected to include support for reading PropertyLists directly from FITS.  This is somewhat understandable; we had no test coverage for that functionality and it was used in only one place (obs_lsstCam) that's not yet in CI.  Should be easy to add it back in.",1
"DM-15785","09/17/2018 18:13:12","Figure out if and where to document local Jupyter","Agree with [~jsick] where to document local jupyter even if it is NOT SUPPORTED. We need a place for hacks and things people will do ..",1
"DM-15786","09/17/2018 18:17:58","Take any comments and update September 2018 Brazil talk ","do any needed updates",2
"DM-15789","09/18/2018 10:32:24","Fix symbol visibility warnings in ndarray pybind11 converters","   {code:java}  4/ndarray/1.5.1.lsst1+2/include/ndarray/pybind11.h:83:8: warning: 'ndarray::Pybind11Helper<double, 2, 2>' declared with greater visibility than the type of its field 'ndarray::Pybind11Helper<double, 2, 2>::wrapper' [-Wattributes]  /home/jbosch/LSST/lsstsw/stack/Linux64/ndarray/1.5.1.lsst1+2/include/ndarray/pybind11.h: In instantiation of 'struct ndarray::Pybind11Helper<double, 1, 0>':  /usr/include/c++/7/tuple:185:12:   recursively required from 'struct std::_Tuple_impl<1, pybind11::detail::type_caster<ndarray::Array<double, 1, 0>, void>, pybind11::detail::type_caster<ndarray::Array<double, 2, 0>, void>, pybind11::detail::type_caster<lsst::geom::Point<int, 2>, void> >' {code}  I had thought these would go away with DM-15151, but I guess not, and it's probably better for us to fix them in the code anyway.",1
"DM-15794","09/18/2018 15:17:52","Change fluxSigma to fluxErr in ap_association","The change in variable names from Sigma to Err where not caught in ap_association as it not currently part of lsst_distrib. This ticket makes that change in package.",1
"DM-15795","09/18/2018 16:25:31","Create DiaForcedSourceTask","Create a task to force photometer difference images and PVIs at DiaObject locations. This ticket only creates the task. Further tickets will integrate it into ap_association and ap_pipe.",5
"DM-15796","09/19/2018 00:12:50","Update documentation to reflect flux->instFlux change","Now that the code has been updated from {{_flux}} to {{_instFlux}} in schemas, we should do a minimal pass over our documentation to bring it up to date. [~wmwood-vasey] had some suggestions of where we should look in that regard, on DM-10302",0.5
"DM-15799","09/19/2018 10:21:55","Remove STOP command from OCS_Bridge","SSIA",2
"DM-15800","09/19/2018 11:28:17","Rename flux to instFlux in ap_association","DM-10302 changed the names of `flux` columns to instFlux. As ap_association isn't currently in distrib the change was missed on the package. This ticket will modify the names of the columns ap_association expects from flux to instFlux",1
"DM-15801","09/19/2018 11:32:16","Detect loops in QuantumGraph","It looks like in current implementation of graph builder it is possible to make graphs containing loops. Simplest example that was observed already is when pipeline of a single task reads and writes into the same dataset type. We are already supposed to have loop detection when we build Pipeline, maybe logic there is not robust enough and needs a small fix.",1
"DM-15809","09/20/2018 14:09:53","Replace boost::regex in utils package","The utils package uses boost::regex for sexagesimal string parsing and for demangling. Neither of these examples need the power of boost::regex and std::regex can be used instead.",0.5
"DM-15829","09/21/2018 13:50:13","Fix shell handling inside sconsUtils commands to be Bourne compatible","The commands inside the sconsUtils test module should be written for bourne shells and not bash shells. ",1
"DM-15831","09/21/2018 14:47:11","Remove unused ra/dec angle handling methods from afw and utils","Following RFC-528, remove the unused sexagesimal angle routines from utils and afw. obs_ctio0m9 needs fixing since it is the only package that uses the utils routines. Replace that with astropy usage.",1
"DM-15836","09/22/2018 09:35:24","Add helper code for invoking C++ templates from Python dtype arguments","In C++, it's natural to require an explicit template argument for templated functions whose template parameters only affect their return type.  That doesn't work when wrapping normally with pybind11, which just sees a set of equivalent overloaded functions and blithely selects the first one defined.    The [Num-]Pythonic interface for functions like this would take a {{dtype}} argument to select the type returned, and this ticket will provide a helper class that makes that easy to implement.    This is a spin-off from DM-15500, where this helper code is now almost complete.    I'm calling this gen3-middleware because that's why I'm doing DM-15500, even though this ticket is a few steps removed from the core gen3 work.  TCAMs are welcome to reclassify.",1
"DM-15837","09/22/2018 21:20:09","mosaic.py error ""Field with name 'i_fluxErr' not found"" ","With {{w_2018_38}}, {{mosaic.py}} failed with many errors such as:  {noformat}  root WARN: Failed to read {'ccd': 19, 'visit': 1270, 'tract': 9615, 'filter': 'HSC-I'}: ""Field with name 'i_fluxErr' not found""  {noformat}    Then  {noformat}  Mosaic INFO: frameIds : []  Mosaic INFO: ccdIds : []  Mosaic INFO: Creating kd-tree for matched catalog ...  Mosaic INFO: len(matchList) = 0 []  Caught signal 11, backtrace follows:  <....>  Segmentation fault  {noformat}        It looks related to the changes of DM-10302? ",1
"DM-15838","09/23/2018 13:04:36","Modify Ack Timer for AuxDevice wait for xfer_params ack","There have been issues with getting a successful transfer of readout params to the forwarders. This seemed a sometimes intermittent problem that was caused by     not getting a proper ack from the forwarder about 10% of the time. After analysis, the xfer_params ack is returning SO quickly that calling the 'clear_forwarder_response_state'    method immediately after publishing the xfer_params message  left too litle time for the ack response code to begin listening, as the response is returned in another thread. The fix here was simple, and painfully obvious in hindsight - clear the state first and then publish the xfer_params message. The debug timer has also been replaced with a progressive ack timer.  This is an important fix, because a FAULT state is entered when time exceeds the limit for the xfer_params_ack to return.",3
"DM-15848","09/24/2018 13:45:42","Add Temporary LDAP group for access to LSPdev at LineA","I hear that we need to temporarily add a new LDAP group for access to LSPdev for demos at LineA (Brazil) later this week. The new LDAP group is 'lsst_int_lspdev_linea2018'.    Adam is out of the office this week and suggested we coordinate with Simon.    I think this needs to be in place ASAP, for onsite testing ahead of Wil's presentation tomorrow morning.",1
"DM-15849","09/24/2018 14:19:34","Cleanup Calib and zero point references in documentation","The pipeline user documentation talks about using the {{Calib}} object and ""coadd's zeropoint"", with some particular details that only apply to the old {{Calib}}. We should clean that up, and any other user docs, so they refer to the new {{PhotoCalib}}.",2
"DM-15850","09/24/2018 14:35:32","Standard StorageClasses should always be loaded","Currently the default Storage Classes are loaded whenever a Butler is created. Supertask needs to know about Storage Classes without an associated butler. To allow this to work change StorageClassFactory so that it always tries to load the definitions from the daf_butler/config directory.    PipelineTask may still need to provide a way for people to define non-standard StorageClasses but that is out of scope of this ticket.",1
"DM-15853","09/24/2018 16:04:19","obs_lsstCam default processCcd config needs updates","processCcd.py an imSim image with the master obs_lsstCam and the w_2018_38 stack failed with:  {noformat}  processCcd.charImage.measurePsf INFO: Measuring PSF  processCcd FATAL: Failed on dataId={'visit': 185615, 'detector': 29, 'run': '185615', 'raftName': 'R10', 'detectorName': 'S02', 'snap': 0}: KeyError: ""Field with name 'base_PsfFlux_flux' not found""  Traceback (most recent call last):    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/timer.py"", line 149, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_tasks/16.0-17-gd75be2c9/python/lsst/pipe/tasks/processCcd.py"", line 188, in runDataRef      doUnpersist=False,    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/timer.py"", line 149, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_tasks/16.0-17-gd75be2c9/python/lsst/pipe/tasks/characterizeImage.py"", line 349, in runDataRef      background=background,    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/timer.py"", line 149, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_tasks/16.0-17-gd75be2c9/python/lsst/pipe/tasks/characterizeImage.py"", line 407, in run      background=background,    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/timer.py"", line 149, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_tasks/16.0-17-gd75be2c9/python/lsst/pipe/tasks/characterizeImage.py"", line 501, in detectMeasureAndEstimatePsf      expId=exposureIdInfo.expId)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/timer.py"", line 149, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_tasks/16.0-17-gd75be2c9/python/lsst/pipe/tasks/measurePsf.py"", line 274, in run      stars = self.starSelector.run(sourceCat=sources, matches=matches, exposure=exposure)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/meas_algorithms/16.0-17-g6a7bfb3b+3/python/lsst/meas/algorithms/sourceSelector.py"", line 110, in run      matches=matches)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/meas_algorithms/16.0-17-g6a7bfb3b+3/python/lsst/meas/algorithms/objectSizeStarSelector.py"", line 394, in selectSources      flux = sourceCat.get(self.config.sourceFluxField)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/afw/16.0-42-gaa8aebfeb+1/python/lsst/afw/table/baseColumnView/baseColumnViewContinued.py"", line 59, in __getitem__      keyobj = self.schema.find(key).key  KeyError: ""Field with name 'base_PsfFlux_flux' not found""  {noformat}  ",0.5
"DM-15857","09/25/2018 11:55:00","jointcal of w_2018_38 fails to run","Running w_2018_38 jointcal with HSC-RC2 data failed, as follows:    {noformat}  INFO  2018-09-25T01:02:15.015 jointcal.Associations ()(src/Associations.cc:141)- Unmatched objects: 95  FATAL 2018-09-25T01:02:29.029 jointcal ()(jointcal.py:103)- Failed processing tract 9615, TypeError: _basicget(): incompatible function arguments. The following argument types are supported:      1. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyB) -> ndarray::Array<unsigned char, 1, 0>      2. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyU) -> ndarray::Array<unsigned short, 1, 0>      3. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyI) -> ndarray::Array<int, 1, 0>      4. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyL) -> ndarray::Array<long, 1, 0>      5. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyF) -> ndarray::Array<float, 1, 0>      6. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyD) -> ndarray::Array<double, 1, 0>      7. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyFlag) -> ndarray::Array<bool const, 1, 1>      8. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyArrayB) -> ndarray::Array<unsigned char, 2, 1>      9. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyArrayU) -> ndarray::Array<unsigned short, 2, 1>      10. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyArrayI) -> ndarray::Array<int, 2, 1>      11. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyArrayF) -> ndarray::Array<float, 2, 1>      12. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyArrayD) -> ndarray::Array<double, 2, 1>      13. (self: lsst.afw.table.baseColumnView.baseColumnView._BaseColumnViewBase, arg0: lsst.afw.table.schema.schema.KeyAngle) -> ndarray::Array<double, 1, 0>    Invoked with: <lsst.afw.table.simple.simple.SimpleColumnView object at 0x7f41807f77d8>, None  {noformat}    The command I ran to get the above error was:  {noformat}  jointcal.py /datasets/hsc/repo --calib /datasets/hsc/repo/CALIB --rerun RC/w_2018_38/DM-15690:private/your-user-name/w38jointcal --id ccd=0..8^10..103 visit=380^384^388^404^408^424^426^436^440 filter=HSC-Y tract=9615 -C=/home/hchiang2/weeklies/w38/w38-jointcal-config.py   {noformat}     The config override file has:   {noformat}  config.astrometryRefObjLoader.ref_dataset_name='ps1_pv3_3pi_20170110'  config.photometryRefObjLoader.ref_dataset_name='ps1_pv3_3pi_20170110'  config.astrometryRefObjLoader.filterMap={'B': 'g', 'r2': 'r', 'N1010': 'z', 'N816': 'i', 'I': 'i', 'N387': 'g', 'i2': 'i', 'R': 'r', 'N921': 'z', 'N515': 'g', 'V': 'r'}  config.photometryRefObjLoader.filterMap={'B': 'g', 'r2': 'r', 'N1010': 'z', 'N816': 'i', 'I': 'i', 'N387': 'g', 'i2': 'i', 'R': 'r', 'N921': 'z', 'N515': 'g', 'V': 'r'}  {noformat}    ",2
"DM-15858","09/25/2018 12:40:08","copyright.py breaks on Python 3.6+","Running the {{copyright.py}} file provided at https://developer.lsst.io/legal/copyright-overview.html#background gives the following exception:  {noformat}  Traceback (most recent call last):    File ""copyright.py"", line 190, in <module>      for line in log.split(""\n""):  TypeError: a bytes-like object is required, not 'str'  {noformat}    Adding an explicit encoding to the call that creates {{log}} fixes the problem.",1
"DM-15862","09/25/2018 15:24:04","Reduce ISR code duplication between ip_isr, obs_subaru, and obs_decam","In sorting out tests for `ip_isr`, I've noticed that there is duplicated code between that package and `obs_subaru`.  There is additional ISR related code in `obs_decam`.  The goal is to unify as much as possible into the default `ip_isr` processing, with the `obs_*` packages only modifying methods when needed.",8
"DM-15865","09/25/2018 15:47:40","PropertyList __copy__ is broken","The new {{__copy__}} method in PropertyList is broken (there were no tests). Fixing it is trivial. I will also add a test.",0.5
"DM-15869","09/25/2018 21:37:20","Fix for renaming *_flux to *_instFlux ","{{pipe_analysis}} scripts (visitAnalysis and compareVisitAnalysis at least) failed with {{w_2018_38}}. It looks like {{pipe_analysis}} may need some updates for the DM-10302 changes:    {noformat}  RuntimeError: No flux keys found  Traceback (most recent call last):    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/software/lsstsw/stack3_20171023/stack/miniconda3-4.3.21-10a4fa6/Linux64/pipe_base/16.0-12-g726f8f3+1/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/visitAnalysis.py"", line 236, in runDataRef      aliasDictList=aliasDictList, hscRun=repoInfo.hscRun)    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/visitAnalysis.py"", line 407, in readCatalogs      commonZpCat = calibrateSourceCatalog(commonZpCat, self.config.analysis.commonZp)    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/utils.py"", line 896, in calibrateSourceCatalog      fluxKeys, errKeys = getFluxKeys(catalog.schema)    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/utils.py"", line 537, in getFluxKeys      raise RuntimeError(""No flux keys found"")  {noformat}  ",3
"DM-15871","09/26/2018 10:44:58","Move daf_butler's doImport function to utils","daf_butler has a really nice, robust function for importing a symbol given a string.  I want that in afw for importing PupilFactory classes in cameraGeom, so I can replace a Python class object with a string when moving Camera to C++.",0.5
"DM-15887","09/26/2018 12:05:44","Make Pipeline work with non-standard storage classes","Pipeline builder needs to instantiate DatasetTypes which depend on StorageClass (and they both are configured via task config). We want to keep pipeline builder independent of butler but that means that initialization of StorageClassFactory becomes an issue. DM-15850 adds support for loading all standard StorageClasses (which come from a standard YAML config) but any non-standard configuration will become an issue in this approach.    I want to see how we can solve this problem by either pre-loading non-standard config for the factory or avoiding its use entirely. ",2
"DM-15897","09/27/2018 15:16:22","Pipelinetask init should take kwargs","Pipeline task init method should take kwargs that that in cases where a pipeline task is used as a subtask all arguments are appropriately forwarded to all constructors.",1
"DM-15899","09/28/2018 07:23:49","Check Done Epics","Check quality of Done epics as per LDM-294",2
"DM-15908","09/28/2018 19:14:53","Firefly target input field/validation is broken","In dev, when i start typing a target name in target panel, the validation fails and overwrite my key strokes.     Start typing 'ngc 7009' it doens't work, the cursor goes back and forth...          I think it could be this change but i'm not sure:    [https://github.com/Caltech-IPAC/firefly/commit/8074feeecc8584c0b9cbc6306c8e8ec56246a774]    Console error includes:       {noformat}  TypeError: Cannot read property 'name' of undefined      at eval (TargetPanelWorker.js?92e5:189)   {noformat}  Please fix. Thanks.",1
"DM-15916","10/01/2018 12:51:07","Make meas_mosaic backwards compatible with *_flux --> *_instFlux rename","The implementation of RFC-322 in DM-10302 saw the renaming of the flux fields in the catalogs from *_flux to *_instFlux along with an increment in the afw table VERSION to 3.  Aliases are added on read of catalogs that have VERSION < 3 for backwards compatibility.  However, the getFluxKeys() function in meas_mosaic cannot take advantage of these aliases as written, so meas_mosaic is currently broken for VERSION < 3 catalogs.  This needs to be fixed.",2
"DM-15923","10/01/2018 19:03:17","doxygen does not build on macOS Mojave","[~bvan] reported that doxygen does not build on macOS Mojave. The problem turns out to be that doxygen tries to be compatible with OS X 10.5 but the Mojave compilers no longer support anything older than 10.9. The fix is to force doxygen to target OS X 10.9.",0.5
"DM-15925","10/02/2018 08:25:00","Add Archive Controller to Tucson system service dependency schema","SSIA.",2
"DM-15928","10/02/2018 11:15:00","Propagate DIASource flags into PPDB","Currently flags are not being copied from the DIASource catalogs into the PPDB.  This ticket is to do so.",8
"DM-15974","10/02/2018 15:36:20","Provide intro text for meas_base","I wrote a brief orientation guide to meas_base. Include it in the meas_base Sphinx documentation.",1
"DM-15975","10/02/2018 16:18:19","Firefly: mouseover help for ""zoom to fit"" and ""zoom to fit width"" buttons is inadequately distinguished","The Firefly image toolbar has a ""fit image to frame"" button (with four arrows in a magnifier) and a ""fit image width"" button (with just a left-right double-ended arrow).  The mouseover help for these buttons is not helpful in distinguishing them:    fit-to-frame: ""Zoom the image to fit into the visible space""  fit-width: ""Zoom the image to Fill the visible space"" (capitalization: sic)    The first should say ""Zoom the image to fit entirely within its frame"" and the second ""Zoom the image to fill its frame horizontally"".",1
"DM-15982","10/03/2018 10:29:29","Firefly fails to load a table with a quoted column","Attached is the IPAC table, which is a result of conversion of CADC ""columns"" table from VOTable format to IPAC format. It has one quoted column - ""size"".    I am getting ""Invalid Statement:"" when trying to load this table into Firefly.     ",1
"DM-15983","10/03/2018 11:18:33","Create a Jenkins job for ap_verify","{{ap_verify}} will be most useful for integration testing if it can be run as a nightly job in Jenkins. Please create a job that exercises {{ap_verify}} on the {{ap_verify_ci_hits2015}} dataset. A solution that can be easily extended to additional datasets would be preferred.    Once the HiTS dataset is downloaded and set up, it can be run using {{run_ci_dataset.sh -d CI-HiTS2015}}.",2
"DM-16012","10/03/2018 13:14:38","Firefly fails to load a table with '\n' and '\r' characters in a cell","An attached table is binary VOTable from Gaia query  {noformat}  http://gea.esac.esa.int/tap-server/tap/sync?REQUEST=doQuery&LANG=ADQL&QUERY=SELECT+*+FROM+TAP_SCHEMA.tables+WHERE+schema_name+like+'public'{noformat}    It has '\n'  and '\r' characters in the description column of row indexes 3 and 4.    Starlink parses this table successfully, and it is successfully converted into _DataGroup_.  However _IpacTableWriter.save_ produces invalid table. It looks like the control characters are not escaped, and the description column is not formatted correctly.      ",2
"DM-16018","10/03/2018 16:34:28","assertAnglesAlmostEqual fails for NaN angles","In removing VisitInfo creation from obs_subaru and moving it to obs_base, all the tests passed in test_repository.py despite me not yet calculating an ERA. Turns out that {{geom.Angle(NaN)}} compared to a finite Angle is always fine. Fix the assert to trap NaN in either argument.",1
"DM-16020","10/04/2018 05:28:54","Decouple EndReadout ack from Fault state notification","During recent stress testing, it was found that if many exposures are taken very quickly one after the next, the Forwarder may get behind by 15 seconds or more. This will delay the acknowledgement that the readout is complete and the file has been processed and sent to archive. One solution is to extend the max time that the ATS CSC will wait for a response - which will allow even more delay to accumulate - then if the max time is exceeded, the system is put in Fault state as there is no response ACK.    Another solution is to NOT make the CSC dependent on the ACK arriving within a prescribed window of time, and simply address the result set information in the ACK when it arrives.[1] There is already an Forwarder health check every exposure, as well as a check that the transfer params were received by working Forwarders - both of which will send the system to FAULT state in no Forwarders are available to work...if a Forwarder dies during EndReadout, the AT CSC will know that an ACK was +never+ received. Then the health state of the Forwarder will be checked on the next exposure.    [1] If checksum verification of archived files is selected to be used, the 'result set' is a struct that includes the path to the file on the Archive and the checksum calculated for the file before it was sent. When the file is verified as existing and its checksum is correct, the ArchiveController issues a receipt for that image file and the receipt is sent to the DMCS bookkeeping database.",1
"DM-16021","10/04/2018 10:00:08","Create a Jupyter extension to start Firefly slate in a tab","Add a command on the sidebar, when the user clicks then show Firefly slate in a tab. The extension will server as the basis for doing more advance interaction with the python api.",20
"DM-16023","10/04/2018 12:54:04","Include alias maps in output schema when denormalizing matches","There are two functions in the stack that effectively generate a denormalized catalog of matches.  The matches between a reference and source catalog are ""joined' into a single ""match"" catalog, putting a {{ref_}} or {{src_}} prefix on the field names for the reference and source fields, respectively.  However, when the schema for the match catalog is created, it does not propagate any aliases that may be set, which will then break any code looking for a field by its alias name (often happens with aliases set for backwards compatibility).  This ticket is to update the following functions to include the aliases from the individual catalogs in the schema for the matched catalog:    - {{matchesToCatalog}} in {{afw}} ([here|https://github.com/lsst/afw/blob/master/python/lsst/afw/table/catalogMatches.py#L101])   - {{denormalizeMatches}} in {{meas_astrom}} [(here)|https://github.com/lsst/meas_astrom/blob/master/python/lsst/meas/astrom/denormalizeMatches.py]    I will leave it for a future tickets whether these two very similar functions should be consolidated into one.",2
"DM-16025","10/04/2018 16:56:10","DESC coord package doesn't build on macOS Mojave","The DESC coord package does not build in our anaconda system because anaconda by default tries to build macOS binaries targeting OS X 10.7.  Mojave refuses to support that:  {code}  clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/timj/work/lsstsw3/miniconda/envs/lsst-scipipe/include -arch x86_64 -I/Users/timj/work/lsstsw3/miniconda/envs/lsst-scipipe/include -arch x86_64 -Iinclude -I/Users/timj/work/lsstsw3/miniconda/envs/lsst-scipipe/include/python3.7m -c src/Angle.cpp -o build/temp.macosx-10.7-x86_64-3.7/src/Angle.o  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]  src/Angle.cpp:23:10: fatal error: 'cmath' file not found  #include <cmath>           ^~~~~~~  1 warning and 1 error generated.  {code}    The fix is to force MACOSX_DEPLOYMENT_TARGET to be at least 10.9.",0.5
"DM-16034","10/05/2018 16:57:46","TabPanel's state may get out-of-sync when tabs are removed","selectedIdx is a state kept by TabPanel.  It's possible that selectedIdx is greater than the number of tabs due to adding and removing tab dynamically.    Currently, this condition is handled in the render function of the component.  This visually appears correct, but the state is out of sync.",2
"DM-16037","10/05/2018 18:35:49","Refactor some searchProcessor code to not use the file as source of data","During the discussion of DM-15826, the team realized there are several searchProcessors in Firefly will get the table data from a file in IPAC table format persisted by Firefly before the Firefly internal DB was introduced. With the augmentation of DataGroup required by keeping more meta data from ingesting VOTable, this practice will cause problem.  We need to refactor the code of the old search processors not to write out the data result to IPAC table and access it later.    In particular, the class TableFromSource.java has to be refactored first.     Also UserCatalogQuery.java",0
"DM-16040","10/05/2018 21:51:15","Remove E251 rule from the stack_package template's setup.cfg","The E251 rule exception is not longer in the developer guide, but it's still in the templates. This ticket will fix that.    (This was started by [~jcarlin], I'm just finishing it up now)",0.5
"DM-16043","10/08/2018 10:13:16","Revert accidental dependency on numpy 1.14","Fixing some warnings on DM-15500 that looked like some that were safely fixed in meas_* accidentally brought in a requirement on numpy >= 1.14.  Revert that.     ",0.5
"DM-16068","10/09/2018 15:34:54","Some flux fields are not getting their units set","Since the flux -> instFlux name change of DM-10302, some backwards compatibility issues with older catalogs were revealed and *thought* to be fixed on DM-15857. The idea was to set aliases for all flux fields, identifying them by their units: any field with unit ""count"", ""dn"", or ""adu"" gets an alias set. This relies on the proper units having been set for all flux fields in the catalog. I have just found several cases (two very important ones!) where units are not being set:  {code:java}(Field['D'](name=""modelfit_CModel_flux"", doc=""flux from the final cmodel fit""), Key<D>(offset=984, nElements=1))  (Field['D'](name=""modelfit_CModel_fluxErr"", doc=""flux uncertainty from the final cmodel fit""), Key<D>(offset=992, nElements=1))  (Field['D'](name=""modelfit_CModel_initial_flux_inner"", doc=""flux within the fit region, with no extrapolation""), Key<D>(offset=928, nElements=1))  (Field['D'](name=""modelfit_CModel_dev_flux_inner"", doc=""flux within the fit region, with no extrapolation""), Key<D>(offset=976, nElements=1))  (Field['D'](name=""modelfit_CModel_exp_flux_inner"", doc=""flux within the fit region, with no extrapolation""), Key<D>(offset=1376, nElements=1))  (Field['D'](name=""modelfit_CModel_flux_inner"", doc=""flux within the fit region, with no extrapolation""), Key<D>(offset=1000, nElements=1))  {code}  As a result, we currently (with {{w_2018_40}}) can't even read in an older coadd catalog as a butler.get throws with:  {code:java}Field or subfield with name 'modelfit_CModel_instFlux' not found with type 'D'. {0}  lsst::pex::exceptions::NotFoundError: 'Field or subfield with name 'modelfit_CModel_instFlux' not found with type 'D'.'  {code}  This needs to be fixed for future processing, but still needs a workaround for old catalogs that do not have the units set properly.",1
"DM-16069","10/09/2018 15:52:19","Create tickets for Test Specs","create Jira tickts for the test specs assigned to relevant people with reasonable dates on them  .. tagged so we can find them easily .. ?",1
"DM-16070","10/09/2018 15:52:55","Implement RFC-534: Update naming of base_Blendedness fields","The implementation of this RFC includes:   # strip ""_instFlux"" from the base_Blendedness_raw_instFlux and base_Blendedness_abs_instFlux names   # move ""_instFlux"" to the end of the name in the fields with actual flux units, thus   # base_Blendedness_raw_instFlux_child --> base_Blendedness_raw_child_instFlux   # base_Blendedness_raw_instFlux_parent --> base_Blendedness_raw_parent_instFlux   # base_Blendedness_abs_instFlux_child --> base_Blendedness_abs_child_instFlux   # base_Blendedness_abs_instFlux_parent --> base_Blendedness_abs_parent_instFlux   # an update to the doc strings to make a clear distinction/description between ""raw"" and ""abs""",3
"DM-16077","10/09/2018 17:27:35","Fix multi-process setup for CmdLineFwk","I did not check multi-process option for some time and now it breaks:  {noformat}  Traceback (most recent call last):    File ""/project/salnikov/DM-15686/pipe_supertask/bin/stac"", line 25, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/project/salnikov/DM-15686/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 226, in parseAndRun      return self.runPipeline(qgraph, butler, args)    File ""/project/salnikov/DM-15686/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 372, in runPipeline      mapFunc(self._executePipelineTask, target_list)    File ""/project/salnikov/DM-15686/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 96, in __call__      return result.get(self.timeout)    File ""/software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/multiprocessing/pool.py"", line 644, in get      raise self._value    File ""/software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/multiprocessing/pool.py"", line 424, in _handle_tasks      put(task)    File ""/software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/multiprocessing/connection.py"", line 206, in send      self._send_bytes(_ForkingPickler.dumps(obj))    File ""/software/lsstsw/stack3_20171023/python/miniconda3-4.3.21/lib/python3.6/multiprocessing/reduction.py"", line 51, in dumps      cls(buf, protocol).dump(obj)  TypeError: can't pickle lsst.log.log.log.Log objects  {noformat}    Need to fix it ASAP   ",2
"DM-16082","10/10/2018 08:59:17","nopytest_test_coadds.py throws warnings, and should be fixed","nopytest_test_coadds.py throws the following warnings in many places:    {code:cpp}  lsst.afw.image WARN: Could not parse options; writing with defaults:    File ""src/PropertySet.cc"", line 189, in T lsst::daf::base::PropertySet::get(const string&) const [with T = std::shared_ptr<lsst::daf::base::PropertySet>; std::string = std::basic_string<char>]      image not found {0}  lsst::pex::exceptions::NotFoundError: 'image not found'  {code}    This seems to be coming from:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/posixStorage.py#L642  which calls and executes to:  https://github.com/lsst/afw/blob/a4679bc1e1d40d112e1484359a353fee477c1331/python/lsst/afw/image/image/fitsIoWithOptions.py#L97    Where it expects a property of the property set to be called `image`. However, the options that are passed through the call chain are:  `  ['visit', 'ccd']  `    This optional prameter is generated:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/posixStorage.py#L637  and that variable comes from:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/posixStorage.py#L249  which in turn comes from the butler here:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/butler.py#L1421  the location variable is created here:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/butler.py#L1297  That optional data ultimately is initialized here:  https://github.com/lsst/daf_persistence/blob/master/python/lsst/daf/persistence/butlerLocation.py#L207  So whatever is creating butler locations (Some repository or mapper) is not adding the correct fields, or afw should be looking for different things to write.    These are just notes for what I have found so far, to help anyone who gets to this ticket. I am tagging [~price] because I think he was the last one to work with the additionalData code, and [~jbosch] because he may be familiar with this work, or how it impacts gen3 middleware and will know if this is even worth pursuing. Also tagging [~yusra], as I am not sure if I put this in the right epic.    ",1
"DM-16131","10/11/2018 11:22:04","Add a unittest to address the issue of DM-16068","Clearly we have no current test that would catch this, please add one (which, on the current {{w_2018_40}} weekly would fail).  See DM-16068 for details.",2
"DM-16135","10/11/2018 11:32:33","Add LongString type to DataType","Add a LongString data type into table data model to indicate a very long string.    This is a way to optimize storage and limit capability.",1
"DM-17162","10/11/2018 16:28:13","Handle Basic standard behavior for AtHexapod CSC ","To have this task done, the CSC should:   * read configurations from configuration files   * publish settingsApplied for configurations used    * publish settingsAppliedMatchStart when applies   * publish settingVersions when first running the CSC   * publish detailedState    * Publish telemetry positionStatus with current position   * accept and execute commands: moveToPosition, applyPositionLimits, applyPositionOffset, stopAllAxes, pivot   * Publish detailedState when the state change   * Publish settings Applied for: settingsAppliedPositionLimits, settingsAppliedPivot, settingsAppliedTcp   * Publish positionUpdate when position command change try to move ",3
"DM-17161","10/11/2018 16:44:38","Develop GUI and Unittest or ATHexapod","Develop GUI and Unittest for ATHexapod.    This task should deliver:   * set of unittests to test the behavior of the CSC   ** values out of range   ** state transitions   ** try position limits   ** move command    ** offset command   ** stop command   ** settingsApplied events   ** appliedSettingsMatchStart   ** settingsVersions   * A very *basic* GUI to control the CSC",3
"DM-16159","10/11/2018 20:41:28","Revert DM-15686 to un-break obs_subaru master","The change to how we detect uniqueness violations in associate broke the Gen3 ingest tests in obs_subaru, because those rely (probably unwisely) on detecting the conflict *before* a transaction rollback is initiated.    The correct fix for this may be in obs_base (where most of Gen3 ingest is implemented), but if so, it's non-trivial, and master is broken right now.  The easy way to un-break it is to revert the merge of DM-15686 is daf_butler.     ",0.5
"DM-16160","10/11/2018 21:37:00","Fix constraint check in associate method","My first attempt to implement constraint check in associate() method (DM-15686) apparently broke everything (DM-16159). Looks like code in obs_subaru depends on uniqueness check to be done in addDataset() method. My commit moved that check to associate() which is apparently too late, tha fix for that it to leave the test in addDataset(). IT would be interesting to also understand what is going on in obs_subaru because in the future we'll probably switch to table-level constraints and that can break things again.",1
"DM-16168","10/12/2018 17:03:23","Update matchedVisitsMetricsTask ","While trying to run validate_drp on the jointcal DC1.2 output, I found that the {{_makeArgumentParser}} needs to be updated to use the new jointcal_wcs name.",0.5
"DM-18210","10/12/2018 17:35:32","Please have salgenerator return a non-zero exit code as soon as a step fails","It would be helpful if {{salgenerator}} would reliably halt with a non-zero exit code at the first failure. That would make it much easier to see problems, and for automated scripts that run multiple steps, it would avoid trying to continue to later steps (e.g. {{salgenerator <foo> sal python}} after a failed {{salgenerator <foo> sal cpp}}), or even an attempt to build any libraries after validation fails.",2
"DM-16170","10/14/2018 14:19:08","mosaic.py error ""Field with name 'i_instFlux' not found""","With {{w_2018_41}}, {{meas_mosaic}} fails with errors like    {noformat}  root WARN: Failed to read {'ccd': 24, 'visit': 1324, 'tract': 9615, 'filter': 'HSC-I'}: ""Field with name 'i_instFlux' not found""  {noformat}    and then Segmentation fault from zero matchList ({{Mosaic INFO: len(matchList) = 0 []}} )      One (longer than necessary) command to reproduce is     {noformat}  mosaic.py /datasets/hsc/repo --calib /datasets/hsc/repo/CALIB --rerun RC/w_2018_41/DM-16011:private/usename/   --numCoresForReadSource=12 --id ccd=0..8^10..103 visit=26024^26028^26032^26036^26044^26046^26048^26050^26058^26060^26062^26070^26072^26074^26080^26084^26094 tract=9615   {noformat}    The same command (with the same input data) works using the {{w_2018_39}} stack. ",1
"DM-16179","10/15/2018 11:38:20","Change log level to WARN for the footprint skipping","The current log level for noting that a footprint is getting skipped is at {{log.trace}}, so this information does not get printed to the logs.  This is an useful and important piece of processing information, so should be set to WARN at these two locations:  https://github.com/lsst/meas_deblender/blob/master/python/lsst/meas/deblender/deblend.py#L297  https://github.com/lsst/meas_deblender/blob/master/python/lsst/meas/deblender/deblend.py#L302  See DM-16151 for more details.",1
"DM-16180","10/15/2018 12:18:04","TSSW Jenkins Test servers not responsive","I am not familiar with DM's components, so that section should be changed to the appropriate item.    TSSW's AWS server with our test suite running Jenkins is completely non-responsive.  We have been not been receiving emails when code is checked in and the webpages current give a 502 bad gateway error.   Here is the URL I am using:    [https://ts-ci.lsst.codes|https://ts-ci.lsst.codes/]    It is my understanding that this functionality is necessary for our code development and is currently hindering us on progressing as such.",2
"DM-16183","10/15/2018 13:33:34","w_2018_41 coaddDriver is broken with detectCoaddSources API changes","Looks like the API change in DM-15663 broken coaddDriver and gave the following error:    {noformat}   Traceback:  Traceback (most recent call last):    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/ctrl_pool/16.0-3-gbc759ec+15/python/lsst/ctrl/pool/parallel.py"", line 509, in logOperation      yield    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/pipe_drivers/16.0-6-gf9cb114+19/python/lsst/pipe/drivers/coaddDriver.py"", line 323, in coadd      self.detectCoaddSources.write(coadd, detResults, patchRef)  TypeError: write() takes 3 positional arguments but 4 were given  {noformat}      One command to reproduce is:  {code:java}  coaddDriver.py  /datasets/hsc/repo --calib /datasets/hsc/repo/CALIB/ --rerun RC/w_2018_41/DM-16011:private/user/name --batch-type=slurm --mpiexec='-bind-to socket' --job  bug --time 600 --cores 24  --id tract=9697 patch=4,8 filter=HSC-I --selectId ccd=0..8^10..103 visit=35892^36140^36144  {code}  ",1
"DM-16184","10/15/2018 15:57:17","Monthly report","DO management seciton and summary.",1
"DM-16197","10/15/2018 20:40:06","Create test plan for LDM-503-11b (Pipelines Release Fall 2019)","Pipelines Release Fall 2019  Science Pipelines release in support of operations rehearsal LDM-503-11. Release contents described in LDM-564. Will be accompanied by a characterization report.",2
"DM-16206","10/16/2018 11:34:08","Make new EUPS release for v17.0","There has been a request to make a new release of EUPS so that it can be included as a dependency for the v17.0 pipelines release.  This ticket is to create the new EUPS release.  Further tickets are needed to change lsstsw/newinstall to use it and to enable to new shebangtron support.",1
"DM-16208","10/16/2018 13:57:42","Add magnitudeToInstFlux method that takes a Point to PhotoCalib","For fake source injection, we should be able to place a source at a specific pixel location, which means we need a way to convert a magnitude to an instrumental flux at that location. The existing {{magnitudeToInstFlux}} method only works with the mean calibration, but should be overloaded to also take a Point (the desired location of the fake source).",2
"DM-16224","10/16/2018 20:00:12","Reduce excessive database traffic from the Replication Controller","A development effort associated with this ticket is meant to address one of the side-effects introduced during the code review of [DM-14262]. The new version of the code (which is now formally correct) puts too much unnecessary load onto the MariaDB service when updating persistent states of the Replication-Qserv synchronization requests. This results in million rows of mostly unneeded data stored in the databases each time such synchronization is happening. Each such row contains names of a databases and chunk numbers which are sent from the Replication system to the Qserv workers. Another problem with the implementation is the rapid growth of the database space used by the Replication system (roughly 200 GB over 48 hours).    In this ticket the detailed reporting of the database names and chunks numbers will be replaced with a simple counter of replicas sent to Qserv workers by each such request.    The second improvement is to be made in the default configuration of the master Replication Controller. The frequency of checks made by the Cluster Health Monitoring and Replication threads is going to be significantly reduces.",0.5
"DM-16235","10/17/2018 14:57:25","Jointcal PhotoCalib returns negative calibrations","{code}  import lsst.daf.persistence as dafPersist  tract = 9813  visit = 23692  ccd = 62  root = '/datasets/hsc/repo/rerun/RC/w_2018_41/DM-16011-jointcal'  butler = dafPersist.Butler(root)  photoCalib = butler.get('jointcal_photoCalib', visit=visit, ccd=ccd, tract=tract)  {code}  {code}  >>> photoCalib.getInstFluxMag0()  -278609934862735.6  >>> butler.get('jointcal_photoCalib_filename', visit=visit, ccd=ccd, tract=tract.getId())  ['/datasets/hsc/repo/rerun/RC/w_2018_41/DM-16011-jointcal/jointcal-results/HSC-R/9813/jointcal_photoCalib-0023692-062.fits']  >>> str(photoCalib)  'TransformBoundedField on Box2I(Point2I(0, 0), Extent2I(2048, 4176)) with mean: -3.59376e-15 err: 2.98536e-17'  >>> photoCalib.instFluxToMaggies(100)  -3.589247456278528e-13  >>> photoCalib.instFluxToMaggies(100, lsst.geom.Point2D(0,0))  3.9565953264617065e-12  {code}",8
"DM-16246","10/18/2018 13:05:26","Ppdb attempts to create duplicate columns when an AFW and Ppdb yaml column have same name."," When creating a new table with Ppdb using an afw schema, Ppdb attempts to create a duplicate column if an afw column has the same name as the Ppdb column specified in the yaml file. The error output below shows this failure for the IxxPSF column but the error has been duplicated the other columns in the DiaSource table. The problem likely occurs in the following code block: [https://github.com/lsst/dax_ppdb/blob/5768c874b9fc52b6e9c3e9dab5f534ae8e636085/python/lsst/dax/ppdb/ppdbSchema.py#L542]    The DiaObject table seems to behave as expected and does not create the duplicate column. Using the afw-schema.yaml file prevents the duplication in the DiaSource table.         Unittest below was executed with the tickets/DM-15588 branch of ap_association.  {code}python tests/test_association_task.py TestAssociationTask.test_update_dia_objects  E  ======================================================================  ERROR: test_update_dia_objects (__main__.TestAssociationTask)  Test the update_dia_objects method.  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1193, in _execute_context      context)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 507, in do_execute      cursor.execute(statement, parameters)  sqlite3.OperationalError: duplicate column name: IxxPSF    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""tests/test_association_task.py"", line 374, in test_update_dia_objects      self._store_dia_objects_and_sources()    File ""tests/test_association_task.py"", line 322, in _store_dia_objects_and_sources      ppdb._schema.makeSchema()    File ""/home/morriscb/src/dax_ppdb/python/lsst/dax/ppdb/ppdbSchema.py"", line 364, in makeSchema      self._metadata.create_all()    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/sql/schema.py"", line 4004, in create_all      tables=tables)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1940, in _run_visitor      conn._run_visitor(visitorcallable, element, **kwargs)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1549, in _run_visitor      **kwargs).traverse_single(element)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/sql/visitors.py"", line 121, in traverse_single      return meth(obj, **kw)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 757, in visit_metadata      _is_metadata_operation=True)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/sql/visitors.py"", line 121, in traverse_single      return meth(obj, **kw)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 791, in visit_table      include_foreign_key_constraints=include_foreign_key_constraints    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 948, in execute      return meth(self, multiparams, params)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 68, in _execute_on_connection      return connection._execute_ddl(self, multiparams, params)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1009, in _execute_ddl      compiled    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1200, in _execute_context      context)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1413, in _handle_dbapi_exception      exc_info    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 203, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb, cause=cause)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 186, in reraise      raise value.with_traceback(tb)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1193, in _execute_context      context)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 507, in do_execute      cursor.execute(statement, parameters)  sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) duplicate column name: IxxPSF [SQL: '\nCREATE TABLE ""DiaSource"" (\n\t""diaSourceId"" BIGINT DEFAULT \'0\' NOT NULL, \n\t""ccdVisitId"" ...  {code}",1
"DM-16259","10/20/2018 16:32:31","cookie cutter and tex changes for PSTN","Added PST and PSTN to the texmf",1
"DM-16261","10/20/2018 16:46:13","obs_lsst code review","Look at obs_lsst with [~tjenness] - perhaps after [~krughoff] adds some tests ...    1 point enough ??",1
"DM-16274","10/22/2018 14:35:15","Tidy code on Demo noteook for Supercomputing","There seems to have been a merge conflict on the notebook in    [https://github.com/womullan/watch]         I got Jeff to check out an old one which is fine for now,    There is also some code in the notebook which should just be in a python file.",1
"DM-16275","10/22/2018 14:45:11","PipelineTask should always use overridable methods to get DatasetTypes","It looks like PipelineTask.runQuantum (at least) uses iteration through the Config instance to find all of the input and output DatasetTypes.  This at least partially ignores the get*DatasetType methods, which should be permitted to add and modify the set of DatasetTypes processed.    Fixing this may involving changing the signatures of the get*DatasetType methods a bit - there is at least some information (e.g. ""scalar"") that is currently only available from the configs, and that probably needs to be forwarded through the get*DatasetType methods so they can be considered the final source of truth by PipelineTask and all activators.     ",1
"DM-16276","10/22/2018 14:53:39","Move data to Qserv node disks from bucket storage","Move data from bucket storage to qserv-ready persistent disks on the k8's cluster for both WISE and KPM30",8
"DM-16277","10/22/2018 15:18:58","Generate metadata and chunklists for WISE and KPM30","Worker level data exists in the cloud, but deploying it in a new cluster requires regenerating CSS metadata, schema, secondary index and chunks lists.",8
"DM-16284","10/22/2018 23:21:20","Kavli workshop telecons and prep","At least one telecon and trying to sort out the invitee list",1
"DM-16286","10/23/2018 09:50:32","Update version checks in EUPS stub packages to match lsstsw minimums","In DM-14011 the lsstsw conda packages were updated.  This technically did not change the baseline for minimum supported package versions but after some debate (RFC-537) it is deemed desirable that Jenkins should always track our minimum version baseline.  If we say it's a minimum we should be testing that minimum.    This ticket will update the EUPS stub packages for astropy, numpy, matplotlib, and scipy to match those set in DM-14011.",0.5
"DM-16291","10/23/2018 14:27:19","str(Image) tests too strict about formatting","While the changes introduced in DM-15682 passed formal testing, I discovered that they do not pass if {{afw}} is built against NumPy 1.13 -- that version of numpy includes more whitespace in the array than the test expects:  {noformat}  AssertionError: '[[0. 0.]\n [0. 0.]]' not found in '[[ 0.  0.]\n [ 0.  0.]], bbox=(minimum=(0, 0), maximum=(1, 1))'  {noformat}  While the build failure can be worked around by upgrading NumPy to 1.14 or later (which will soon be required anyway), the consensus on [#dm-build-problems|https://lsstc.slack.com/archives/C4RKBLK33/p1540325009000100] was that any future formatting changes in NumPy would break the test again.    Please modify the tests so that they're less sensitive to NumPy's formatting decisions, preferably while keeping some diagnostic of whether the array is included in the string.",2
"DM-16294","10/24/2018 06:59:30","Fix coadd dataset templates in obs_lsst and possibly elsewhere","obs_lsst has some unnecessary overrides of obs_base datasets and some datasets that probably shouldn't exist at all.  For the latter we should try to track down where they were cargo-culted from as well.",1
"DM-16295","10/24/2018 08:22:00","Get cp_pipe etc working for ts8 for bootcamp","There will be several changes necessary, don't know what they are till we try and they pop up.    Put them all on this ticket.",2
"DM-18209","10/24/2018 16:12:42","Make topic get and put functions safe for nullptr / None","At present SAL commands that get and put data take the data as a pointer argument and segfault if {{nullptr}} is used as the argument (or {{None}} in Python).    If practical, I strongly suggest making these functions take a reference instead of a pointer. That is the preferred C++ technique for a writable argument when {{nullptr}} is not allowed because it completely eliminates the possibility without any testing needed in the code. It also clearly expresses the need for an argument, whereas a pointer suggests that nullptr is a useful alternative.    If that is not practical for some reason (e.g. if the pointer has to be passed on down to DDS code) then please add explicit tests to the C++ which raise an exception if the data argument is {{nullptr}}.",3
"DM-18223","10/24/2018 16:21:27","Functions that get or put topics should raise an exception if resources not setup","At present some or all SAL functions that get or put topics (telemetry, events and commands) segfault if the appropriate function has not been called to set up resources:{{salTelemetryPub}}, {{salTelemetrySub}} and the corresponding pairs of functions for logevents and commands.    Please make the commands raise an exception if the necessary resources have not been set up.    Please also have them raise an exception if the the resources are gone, e.g. if {{salShutdown}} has been called.    The SALPY unit tests I wrote have a test case for the first of these, but not the second.",1
"DM-16302","10/24/2018 17:00:29","ap_verify_hits2015 readme has invalid visit ID","The readme for the {{ap_verify_hits2015}} dataset has an example run of {{ap_verify.py}} that uses {{visit=54123}}. This visit does not exist. Modify the example to use a visit ID that is present in the dataset, and confirm that the example works.",1
"DM-16304","10/24/2018 17:58:17","Update overview paper ","Update Gaia performance in overview paper - include new references.",1
"DM-16305","10/24/2018 21:25:41","Implement bbox integrator for PhotometryTransform","I implemented a stop-gap solution to DM-16235 (using the original calibration for the mean calibration so that mean calibrations are never negative). This ticket is to finish the integration code that was started there, so that we can compare the jointcal means with the original processCcd calibration means.    [~jbosch] suggests using the Chebyshev1dBasis in {{lsst.geom.polynomials}} to help implement the integration code.",8
"DM-16309","10/25/2018 09:58:35","Please clarify documentation on line lengths in docstrings","Per [discussion on Slack of 2018-10-25|https://lsstc.slack.com/archives/C2B6DQBAL/p1540481139000100], there's a certain amount of ambiguity in our developer documentation about formatting docstrings. Please propose some text to make this clearer.",1
"DM-16312","10/25/2018 10:56:50","Rename Sensor to Detector in pipe_supertask","DM-15537 renamed couple of things in gen3 registry, those names should also be changed in pipe_supertask.",1
"DM-16314","10/25/2018 11:25:56","Look at obs_lsstCam","I have been asked to take a look at obs_lsstCam to do a mini review.",1
"DM-16319","10/25/2018 13:56:59","ap_verify source count metrics do not exist","{{ap.association}} and {{ap.verify.measurements}} record a number of source count metrics using the {{lsst.verify}} framework. However, these metrics are not actually defined in {{verify_metrics}}. Add proper definitions for these metrics so that they appear in SQuaSH.",1
"DM-16326","10/26/2018 10:57:35","Fix astropy's formatting of FITS times","In TSEIA-83 we noted that Astropy has been generating time strings in a non-standard format.  Fix Astropy.",0.5
"DM-18212","10/26/2018 15:48:11","The heartbeat event is missing from SALGenerics.xml in ts_xml","The heartbeat event is missing from SALGenerics.xml in ts_xml    I assume this is an oversight, since it is a standard event. But if not, it should not have been removed from the Test product. I will restore it there for now to work around this bug. But please feel free to remove it from Test again if this issue is fixed.",1
"DM-16335","10/28/2018 09:05:28","Refresh shared stack on (Princeton's) tiger2/perseus","Following DM-16286, the shared stack build on Princeton systems is broken. We'll need to bootstrap a new shared stack to revive it.",1
"DM-16338","10/28/2018 20:55:10","Add fluxErr to LoadReferenceObjectsTask makeMinimalSchema","Adding fluxErr to {{LoadReferenceObjectsTask.makeMinimalSchema()}} helps with some of the problem posed in RFC-535, while being uncontroversial. This will ensure that future reference catalogs have this field by default.",2
"DM-18242","10/29/2018 15:07:46","Wrap a SAL method ","Work on task https://jira.lsstcorp.org/browse/TSS-3196",1
"DM-16343","10/29/2018 15:13:51","pytest-flake8 creates many forked processes","[~jbosch] noticed that when he runs {{scons}} he gets many unexpected python processes created by pytest.  It looks like for every pytest core an additional N processes are created where N seems to be the number of effective cores.",0.5
"DM-16347","10/29/2018 17:13:01","DcrAssembleCoadd array size mismatch","Bug encountered while trying to test the new astro metadata package that fixes obs_decam observatory longitude:    {code:}  dcrAssembleCoadd FATAL: Failed on dataId=DataId(initialdata={'filter': 'g', 'patch': '0,0', 'tract': 0}, tag=set()): ValueError: operands could not be broadcast together with shapes (2003,2003) (2000,2000) (2003,2003)   Traceback (most recent call last):    File ""/Users/sullivan/LSST/code/eups_distrib/stack/miniconda3-4.5.4-fcd27eb/DarwinX86/pipe_base/16.0-13-gb122224+4/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/Users/sullivan/LSST/code/eups_distrib/stack/miniconda3-4.5.4-fcd27eb/DarwinX86/pipe_base/16.0-13-gb122224+4/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/Users/sullivan/LSST/code/eups_distrib/stack/miniconda3-4.5.4-fcd27eb/DarwinX86/pipe_base/16.0-13-gb122224+4/python/lsst/pipe/base/timer.py"", line 149, in wrapper      res = func(self, *args, **keyArgs)    File ""/Users/sullivan/LSST/code/eups_distrib/build/pipe_tasks/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 217, in runDataRef      results = AssembleCoaddTask.runDataRef(self, dataRef, selectDataList=selectDataList)    File ""/Users/sullivan/LSST/code/eups_distrib/stack/miniconda3-4.5.4-fcd27eb/DarwinX86/pipe_base/16.0-13-gb122224+4/python/lsst/pipe/base/timer.py"", line 149, in wrapper      res = func(self, *args, **keyArgs)    File ""/Users/sullivan/LSST/code/eups_distrib/build/pipe_tasks/python/lsst/pipe/tasks/assembleCoadd.py"", line 361, in runDataRef      inputData.weightList, supplementaryData=supplementaryData)    File ""/Users/sullivan/LSST/code/eups_distrib/build/pipe_tasks/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 378, in run      modelWeights)    File ""/Users/sullivan/LSST/code/eups_distrib/build/pipe_tasks/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 521, in dcrAssembleSubregion      maskedImage.image.array *= modelWeights  ValueError: operands could not be broadcast together with shapes (2003,2003) (2000,2000) (2003,2003)   {code}",1
"DM-16363","10/30/2018 11:47:56","ValueError in coaddAnalysis with HSC-RC2 tract=9615 filter=HSC-R ","coaddAnalysis with HSC-RC2 tract=9615 filter=HSC-R data gave the following error:  {noformat}  Traceback (most recent call last):    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/pipe_base/16.0-13-gb122224+8/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/pipe_base/16.0-13-gb122224+8/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/coaddAnalysis.py"", line 334, in runDataRef      postFix=""_unforced"")    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/coaddAnalysis.py"", line 577, in plotSizes      labeller=StarGalaxyLabeller(), flagsCat=flagsCat,    File ""/home/hchiang2/stack/pipe_analysis/python/lsst/pipe/analysis/analysis.py"", line 104, in __init__      self.good = np.isfinite(self.quantity) & np.isfinite(self.mag) if self.quantity is not None else None  ValueError: operands could not be broadcast together with shapes (23872,) (23873,)  {noformat}    Other tracts/filters in HSC-RC2 finish without errors. A command to reproduce is   {noformat}  coaddAnalysis.py /datasets/hsc/repo/ --calib /datasets/hsc/repo/CALIB  --rerun RC/w_2018_42/DM-16095:private/user/name/coaddAnalysis  --id tract=9615 filter=HSC-R   {noformat}    This used to finish fine in early September with {{w_2018_36}} (before the DM-15869 issue). ",3
"DM-16392","10/31/2018 16:09:25","Aperture correction field keys not guaranteed to point the same offsets within a given reprocessing","In perusing the {{pipe_analysis}} outputs of the RC2 reprocessing runs to check that our results are all looking as they should, I noticed that the aperture corrections were comparing differently between reprocessing runs (e.g. w42 vs. w41). While, in principle, this could happen as a result of code changes, this did not appear to be the case as the flux comparisons were all identical. Digging into it, I discovered that the key offsets point to a different place for different patches/visits for all aperture correction fields *within the same processing run*. I.e. the catalog schemas don't match each other, nor the one defined in the schema files in the repo.    E.g. in {{/datasets/hsc/repo/rerun/RC/w_2018_42/DM-16095/}}, the meas cat schema in {{schema/deepCoadd_meas.fits}} has:  {code:java}base_GaussianFlux_apCorr Key<D>(offset=1832, nElements=1)  base_PsfFlux_apCorr Key<D>(offset=1992, nElements=1)  {code}  But for two patches, we find:  {code:java}dataId = {""patch"": ""8,0"", ""tract"": 9615, ""filter"":""HSC-I""}  base_GaussianFlux_apCorr Key<D>(offset=2040, nElements=1)  base_PsfFlux_apCorr Key<D>(offset=1992, nElements=1)  {code}  {code:java}dataId = {""patch"": ""3,7"", ""tract"": 9615, ""filter"":""HSC-I""}  base_GaussianFlux_apCorr Key<D>(offset=1880, nElements=1)  base_PsfFlux_apCorr Key<D>(offset=1928, nElements=1)  {code}  This is most undesirable as it precludes concatenating all patches in one tract into a single catalog (which requires them all to have identical schemas).  The fix may be as simple as looping over a sorted list (as opposed to one where order is not guaranteed) when adding the apCorr keys to the schema.",0.5
"DM-16393","10/31/2018 16:25:21","Dask investigations","Investigate dask as an option for doing distributed analysis.  Ideally this would lead to a proof of concept visualization that is only possible with this kind of system.",20
"DM-16400","11/01/2018 10:49:37","Create a timing metric for ApPipeTask","Now that {{ap_pipe}} is implemented as a {{CmdLineTask}} with a {{timeMethod}} decorator, and with {{ap_verify}} actually calling {{ApPipeTask.runDataRef}} (as of DM-15901), it would be easy to time the entire task and store it as a SQuaSH metric. Register such a metric in {{verify_metrics}} and start tracking it in {{ap_verify}}.",1
"DM-16401","11/01/2018 11:46:25","Disable writing metadata for MergeDetections and MergeMeasurements Tasks","When splitting apart these two tasks, the writeMetadata method overload in the old base class was not propagated to each of the merge tasks. Add the no-op method overload to each of the merge tasks.",2
"DM-16404","11/01/2018 16:00:49","Implement multi-node support in ap_proto","for more realistic tests I'd like to add support for running ap_proto on many hosts, maybe using MPI or some other sort of IPC (it should be a low-level data exchange so there is no critical point here).",5
"DM-16405","11/01/2018 16:24:45","Python client: add support for hue-preserving rgb","Add support for hue preserving rgb. Might need a variant of set_stretch function.",5
"DM-16409","11/02/2018 04:50:27","Familiarization with the ip_diffim codebase","[ip_diffim|https://github.com/lsst/ip_diffim] is the home of image differencing algorithms within LSST. Read through the current {{master}} branch, understand what's there and think about the structure.    Understanding the way ip_diffim is structured will (at a minimum) involve developing some familiarity with the pipe_base and meas_base frameworks. The former defines the structure of “tasks”, which form the basis of LSST pipelines; the latter, the “measurement framework” which provides a system of plugins for measuring source properties given a collection of pixels. A good target for this ticket would be to understand:    - What {{Task}}​s exist in the ip_diffim package, and what they do;  - Major {{Task}} configuration options;  - What measurement plugins exist in ip_diffim, and what they do.    Detailed understanding of how the algorithms are implemented isn't important for now, as long as you have a good mental map of what's available and the large scale structure of how the package fits together.    The following documentation may be helpful:    - https://pipelines.lsst.io/v/daily/modules/lsst.pipe.base/task-framework-overview.html  - https://pipelines.lsst.io/v/daily/modules/lsst.meas.base/intro.html    ",8
"DM-16413","11/02/2018 10:09:12","Remove more paf files from obs_base","These somehow escaped the purge of DM-15767.  They *should* no longer be in use, and hence easy to remove.",0.5
"DM-16414","11/02/2018 10:36:50","Confirm fix of DM-16392 after next RC2 processing","The fix on DM-16392 is easiest to confirm on the {{pipe_analysis}} processing outputs of the RC2 bi-weekly reprocessing.  The issue was that the order aperture corrections were being added to the schema was not deterministic (loop was being done on a set).  As a result, when attempting to concatenate all patches of a given tract into a single catalog and using a template schema, the aperture corrections were often pointing to different key offsets, thus getting the wrong column appended into catalog.  This issue was discovered in perusal of the plots output by {{pipe_analysis}}'s *compareCoaddAnalysis.py* and *compareVisitAnalysis.py* when comparing outputs from one reprocessing to the previous one (between which no algorithmic changes had been made that could've caused this discrepancy, backed up by the identical flux measurements for the same points).  This ticket is to capture the effort that went into diagnosing the bug and ensuring the fix was successful after the {{w_2018_44}} -- assuming the fix lands on time -- is completed.",2
"DM-16415","11/02/2018 10:57:05","Establish MPC Database Replication Environment","Create accounts for the Minor Planet Center staff on the epyc machine at UW, and set up a basic PostgreSQL server. The MPC staff will use this to set up a replica of the internal MPC database, running on SSDs. This will later be used to test the existing database throughputs, and whether they can scale to LSST requirements.",5
"DM-16418","11/02/2018 11:06:43","Monthly LSST-MPC collaboration status review","Monthly review of LSST-MPC collaboration status, presentation of progress, review of issues.",1
"DM-16421","11/02/2018 11:12:01","MOPS Change Request Preparation","Continue developing the change request to incorporate the updated thinking on Solar System data products and algorithms.    The deliverables of this element include:   * Development timeline for the next 6 months, sufficient to support the next development cycle planning. ✅   * Draft long-term (4-yr) plan in form of a list of milestones and a Gantt chart. ✅   * Updates to DPDD ✅   * Updates to LSE-61   * Updates to LDM-151 ✅   * Updates to LSST Schema Document ✅   * Resolution of LSST-MPC MOU    [~eggl] and [~mjuric] will be collaborating on this story (splitting the story points 50:50).",20
"DM-16423","11/02/2018 11:54:58","Ensure that templates are not updated when no data available from SAL","Ensure the header templates are unmodified and therefore undefined values are preserved when no data is available from SAL for a given keyword.",2
"DM-16426","11/02/2018 12:57:38","Remove errant print statement in multiband.py","DRP accidentally let a print statement slip through to master. Remove it.    This is a first ticket Andres, and also includes familiarizing himself with our workflow.           ",1
"DM-16428","11/02/2018 13:04:40","Implement and publish SODA 1.0 in ImgServ through VO Service Interface","The latest SODA 1.0 Recommendation from IVOA focuses on image cutout and stitching operations, in addition to the standard VO service registration.         For reference, see [SODA 1.0|http://www.ivoa.net/documents/SODA/20170604/REC-SODA-1.0.html]",40
"DM-16429","11/02/2018 14:16:45","Ensure WCS (and other Exposure components) are retrieved properly when loaded individually","It looks like <exposure>_wcs datasets are coming from the header (which may be an approximation) instead of the true WCS in the FITS binary tables.  Fix this, and add support for more components if it's easy to do so.    Note that using the header was once thought to be necessary to do this efficiently, but in fact there's always been a trick to get anything out of the binary tables efficiently (due originally to [~price], I think): load a 1-pixel subimage of the exposure, and pull the component out of that.    However, since DM-15500 we've had an {{ExposureFitsReader}} class that provides direct and efficient access to all of those components without that trick, and that's what we should use here.",1
"DM-16436","11/05/2018 08:04:03","Fix broken inheritance in WriteObjectTableTask","Changes in multiband.py removed a base class that was only loosely tied to the derived classes and moved shared functionality out into free functions. At the time of merge, qa_explorer was not changed as it was not part of the main stack and so the dependency change went unnoticed.    This ticket will fix the inheritance breakage in WriteObjectTableTask and clean up some coding standards in the file it is placed in.",2
"DM-16441","11/05/2018 13:17:05","Speed up creation of footprints table for Firefly","DM-15823 implemented a function {{createFootprintsTable}} to convert an {{afwTable.SourceCatalog}} into an {{astropy.io.VOTableFile}}  for Firefly to draw LSST detection footprints. The function includes a slow loop over rows of the table to copy data from the input table to the output table, which can take tens of minutes for a large {{SourceCatalog}}.      A column-wise copy appears to greatly speed up this function.",2
"DM-16442","11/05/2018 15:20:44","Build and test HeaderService against new version of SAL and xml","New version of ts_sal and ts_xml have been committed to master by T&S. The new ts_xml needs to be rebuilt and the HeaderService and salpytools need to be tested against this new version.",8
"DM-16443","11/05/2018 17:31:05","RST Guide Example Broken","The RestructuredText style guide includes an [example for linking to images|https://developer.lsst.io/restructuredtext/style.html#plain-images], but this example works only on local builds because it links to the uningested image. The example should say  {noformat}     :target: _images/development/docs/lsst_logo.jpg  {noformat}  which will work on both local and web builds.",1
"DM-16447","11/06/2018 14:32:27","Ability to add comments to the templates.","At the moment the HeaderService uses the function  \{\{read_scamp_head()}} to read in the header templates. This function does not have the ability to ignore lines with comments. The simplest and fastest solution would be to clone and modified that function and make it part of the headerservice.",2
"DM-16451","11/06/2018 16:01:16","Fix color parameter names for overlaying footprints","In {{firefly_client.FireflyClient.overlay_footprints}}, the highlight color parameter is {{highlightColor}} and the selection color parameter is {{selectColor}}. These names need to be used in {{display_firefly}} in the {{overlayFootprints}} method.",0.5
"DM-18208","11/07/2018 07:39:01","Please provide a variant of getSample_x that always returns the last value seen","It would be very helpful for {{salobj}} and ameliorate the need for TSS-3195 and TSS-3218 to have a function like {{getSample_x}} that always returned the last value seen (instead of returning ""no new data"" if called multiple times). This could be a new flag argument to {{getSample_x}}.    I have discussed this with [~dmills] before but I don't think I ever filed a ticket, so here it is. I believe [~dmills] prefers to have such a function query the EFD if necessary. I am strongly opposed to this for the following reasons:  - It adds a lot of complexity which will be difficult to test  - It is likely to make the function unpredictably slower    Also I worry that it is likely to delay implementation of this useful feature.    But in any case, I would rather have a different function for EFD lookup, or at least a flag argument that will enable or disable it.",2
"DM-16461","11/08/2018 08:09:02","Make message broker creds its own config file.","SSIA",2
"DM-16462","11/08/2018 08:13:10","Separate Forwarder Allocation Assignment to its own config file.","SSIA",3
"DM-16463","11/08/2018 08:40:06","Introduce user configurable cfg file options","Under the config/ dir, will be a 'configurables' directory. A user-defined configuration file can be dropped in this directory, and any time the DM L1 system is moved into standby state, the files in this directory will be read and inserted into a redis database instance. When DM offers a list of possible configs for the operator to choose from, the primary keys  (config file name) that define each separate config file will be presented to the operator as a comma delimited list. When the operator chooses a particular name, the key/value pairs for that configuration is presented.    Note: A formal enumeration of keys in a user configurable file will be provided. It is expected that the list of configurable options will grow as the system approaches completion. Users will be able to select values for the list of some or all of the keys, and name this configuration with a unique name.         A default configuration will be present and used if a unique config is not selected.    If a config file only sets values for two out of say, 50 possible keys in the formal enumeration, the other 48 keys will be set to default values.    One of the many uses these files will be used for is to describe which CCDs in the FPA are to be read. This data will be able to be specified as *_inclusive_* or _*exclusive.*_         The configurables dir will have a README and a couple of example files.",8
"DM-17356","11/08/2018 11:07:32","Enhance ts_SALLabVIEW to make it externally scriptable","The ts_SALLabVIEW VI currently requires user interaction to provide the    location of the .idl and .so files. We need to be able to have this information   be provided externally (eg at command line) so that the entire process can be   run automatically with no user interaction required.   e.g.    updateSALLVcontrols ATMonochromator [optional location]    the default location would be $SAL_WORK_DIR/subsystem-name/labview/lib    Below are notes to myself to finish this task. ",3
"DM-16467","11/08/2018 13:12:29","isrTask conversion to pipelineTask","isrTask needs to be updated to work with a Gen3 butler as a valid pipelineTask.  DM-15862 will unify the ISR processing, so this should make all cameras functional with a Gen3 butler.",8
"DM-16468","11/08/2018 15:14:26","Speed up and stabilize dcrModel convergence","With larger numbers of DCR subfilters (e.g. 5) the dcrModel solution can sometimes oscillate between iterations of forward modeling. This results in premature termination of the modeling loop, because the convergence may improve by a large amount in one iteration, and by a very small amount in the next, triggering the convergence end condition. One solution is to set a small enough gain on the new model solutions so that modeling converges smoothly, but then more iterations are required to reach the same level of convergence. Instead, the gain should adapt to how well the model is improving compared to predictions.",3
"DM-16473","11/09/2018 08:52:02","Support BNL ts8 data and add raft data","Support BNL ts8 data and add raft data to the serial number dictionary",1
"DM-16476","11/09/2018 10:18:31","Revamp table watching sagas so there is one ""Master Table Watcher"" ","Revamp table watching sagas or watcher so there is one ""Master Table Watcher"" which will:   # Analyze the table for type. Right now we have three- catalogs, image meta data, footprints but there will be more.   # Start the appropriate watcher for just that table id.  This watcher will live as long as the table is loaded.   # The ""Master Table Watcher"" has configuration to determine which watcher are supported. i.e. it would not load a LSST footprint watcher in the IRSA case.   # A watcher would have two functions.  One to determine if it is a supported table and the other that would be the watcher.",8
"DM-16481","11/10/2018 15:50:27","cmdLineFwk crashes when filling output collection","stac crashes when it tries to add a DatasetRef to output collection:  {noformat}  stac --clobber-config --clobber-versions -b ../ci_hsc/DATA/ -o test1 -i raw/HSC -p lsst.ip.isr --debug -L isr=debug -d ""PhysicalFilter.physical_filter='HSC-I' AND Detector.detector=16 AND Exposure.exposure=903988"" run -t IsrTask --show=graph  TaskDef(lsst.ip.isr.isrTask.IsrTask, label=IsrTask)    Quantum 0:      inputs:        raw: [DataId({'exposure': 903988, 'instrument': 'HSC', 'detector': '16'})]      outputs:        ISRG3PostIsrCcd: [DataId({'detector': 16, 'instrument': 'HSC', 'visit': 903988})]  Traceback (most recent call last):    File ""/project/salnikov/gen3-middleware/pipe_supertask/bin/stac"", line 25, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 228, in parseAndRun      return self.runPipeline(qgraph, butler, args)    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 334, in runPipeline      self._updateOutputCollection(graph, butler)    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 412, in _updateOutputCollection      registry.associate(collection, id2ref.values())    File ""/project/salnikov/gen3-middleware/daf_butler/python/lsst/daf/butler/core/utils.py"", line 290, in inner      return func(self, *args, **kwargs)    File ""/project/salnikov/gen3-middleware/daf_butler/python/lsst/daf/butler/registries/sqlRegistry.py"", line 589, in associate      ref = refs[0]  TypeError: 'dict_values' object does not support indexing {noformat}  Apparently in {{_updateOutputCollection()}} I'm passing dict.values() to an argument that is supposed to be a list.",0.5
"DM-18229","11/12/2018 10:50:06","MTMount OSS telemetry topic causes Java too many parameters error","This is a ticket to track the Java ""too many parameters"" error in the MtMount OSS telemetry topic.  This issue will probably go away when the vendor updates the topics.  QA needs the ticket to properly shutdown the tests that are affected.",2
"DM-16494","11/13/2018 14:25:16","Ensure that Firefly can read VOTable 1.3 and in particular the BINARY2 data format","VOTable 1.3 is the current standard, and the BINARY2 data format is the one most likely to be used as the LSST default.  BINARY2 adds proper support of NULL values, in particular.    The action is to upgrade the Java support libraries used by Firefly to a version that provides full support for this.  I suggest at least STIL Version 3.2-1 (see http://www.star.bristol.ac.uk/~mbt/stil/sun252/history.html).    This is required for LSST and also has to work in production for IPAC since attempts to access external services via Firefly are increasingly likely to generate data in this format.    Two previous tickets (DM-7225 and DM-7405) have touched on this, with comments stating that the issue was resolved, but tests today showed that Firefly was not able to read a table in BINARY2 format.    This may be related to the ""bit"" bug fixed in STIL Version 3.0-13 (17 Aug 2015).",1
"DM-16502","11/14/2018 10:28:09","Restore SUIT functionality","After the recent DAX changes, SUIT is broken.    Update SUIT to use upper case ""QUERY"" parameter and new [PDAC service URLs|https://confluence.lsstcorp.org/display/DM/DAX+service+URLs].  Fix the display of the search page (bug exposed by TabPanel changes).",2
"DM-16504","11/14/2018 12:34:21","Build/Install and test new ts_sal and ts_xml ","Build/Install and test new ts_sal and ts_xml for the HeaderService in preparation for the Reverification of the Auxtel CSC funcionality.    https://confluence.lsstcorp.org/display/SYSENG/Reverification+of+AuxTel+CSC+Functionality",8
"DM-16521","11/15/2018 13:39:25","Add information for ctrl_platform_lsstvc and add queue option","Add Additional explanation of how jobs and nodes are labeled.",2
"DM-16522","11/15/2018 15:48:25","Dev imageviewer Git Commit: c79022a fails on the attached FITS image file","This problem was found on NED's ivv.ned.ipac.caltech.edu/byname, using NGC 4151.  Click on images the click on the first row 'View Image'.   The image gets fetched by the browser (Chrome and Firefox), but the display gets greyed out, and all subsequent attempts to 'View Images' appear to result in fetches, but the screen remains greyed out.    It also is easily reproduced using an IRSA viewer on a firefly server using v1.0.0 Built On: 2018-10-31, Git commit 79022a. Try [http://ff1.ned.ipac.caltech.edu:8080/firefly/] and upload the attached fits file.    Note: Uploading the attached image to the irsaviewer application hosted on [https://irsa.ipac.caltech.edu|https://irsa.ipac.caltech.edu/] (v3.1.0_Final-3099) works as advertised.         Implementation fixed 2 issues:   * ThumbnailViewer does a undefined check it should have been doing, this protects the {{render()}}   * A plate projection sets up the coordinate system correctly",2
"DM-16530","11/16/2018 11:51:09","Changes to salpytools to due to updates on ts_sal and ts_xml","Due to changes to ts_sal and ts_xml and underlaying names and function, the salpytools module  needs to be updated,",2
"DM-16538","11/17/2018 17:06:34","Add PSTN series to www.lsst.io and include refresh date on page","Add PSTN series to www.lsst.io and include refresh date on page.",0.5
"DM-16542","11/19/2018 11:25:20","Qserv data loader fails with wide table","Sabine sent me a bug report on slack, apparently integration test fails while loading data for one particular table. Here are few pieces of info for that:  {quote}  I currently try to upload a Run1.1 dataset to the Qserv nodes available at CC-IN2P3. One of the data catalogs deepCoadd_meas contains ~840 parameters per entry which lead to a problem with the Qserv http get request that should retrieve the schema of the table.  More precisely, the deepCoadd_meas table is created properly in Qserv, all the columns are well defined and the mysql command ""describe deepCoadd_meas"" works fine. However the https get request that should retrieve the table schema before the data ingestion fails (internal server error): I tracked the problem down and went to the conclusion that the error is due to the length of the response - everything works fine when I reduce the number of columns in the table.  {quote}    {quote}  I managed to reproduce the problem by using directly the following command :  curl --verbose  --anyauth -u ... --X GET http://127.0.0.1:5012/dbs/qservTest_case149_qserv/tables/deepCoadd_meas/schema  this command works fine when the number of columns in the table is reasonable, but as soon as I reach a given number of columns an ""internal server error"" is thrown.  During my tests I checked each time that the deepCoadd_meas tables were not corrupted by using the equivalent mysql/qserv command:   mysql -P 30040 -u qsmater -h ccqserv125 qservTest_case149_qserv -e ""describe deepCoadd_meas;"". This command always worked fine, I therefore concluded that the http request goes wrong when the size of the answer to the request is too big.  {quote}    {quote}  I created a tarball that contains the qserv-check-integration.py input data. It is available for download here : https://mydrive.lapp.in2p3.fr/s/oXUApFgSU2FAy8A  . I use the following command to upload the data python qserv-check-integration.py -l -i 130 --mode qserv -t <data_directory_name>  {quote}  ",1
"DM-16544","11/19/2018 12:47:55","Investigate mask propagation through ip_diffim","As first identified in DM-13081, the template mask planes seem to propagate into unreasonably large masks in the difference image.  This ticket is to investigate the source of this behavior and fix it, if appropriate.  It may be useful to consult with [~gkovacs].",8
"DM-16550","11/20/2018 12:05:58","Races in YAML tests in daf_persistence","The YAML storage tests in daf_persistence create a temporary directory but do not use it. This leads to test failures when the tests are run in parallel.  Also fix the pickle test so that it no longer uses the root directory.",0.5
"DM-16552","11/20/2018 12:40:39","Produce summary of sensor effects and strategies to mitigate","Develop a summary on Confluence of the various sensor effects which DM team should be aware of. Collate as much information as possible on:    - A description of the effect;  - What strategies the Camera Team have developed to mitigate it;  - What strategies DM have developed to mitigate it;  - Identify where those strategies diverge;  - What requirements there are for handling this effect.    Note that, in some cases, the strategies developed to date may be “none”!    Record this information on Confluence at: https://confluence.lsstcorp.org/display/DM/Sensor+Characterization+and+ISR    The aim is to use this page as a central “knowledge base” for future work, and to keep it updated as we learn about new effects or refine our approach to addressing them.",20
"DM-16557","11/21/2018 16:21:29","Move ap_verify_queries function into Ppdb","DM-15588 removed the placeholder database APIs from ap_association, pipe, and verify. In doing this, a convenience function was created to compute a summary metric on the PPDB for use in ap_verify. This ticket will move that function and it's unittest into the Ppdb class and modifies ap_verify's usage of the function.",2
"DM-16558","11/22/2018 16:04:11","removeMaskPlane function in multiband.py does not work","In perusing the stack for examples of mask usage and manipulation, I came across this funtction:    [https://github.com/lsst/afw/blob/master/python/lsst/afw/image/image/multiband.py#L481-L490]    It looks like there are two issues with it:    1) {{lsst.afw.image.MaskX}} has no attribute {{removeMaskPlaneDict}}, so any call to the function results in:  {code:python}  AttributeError: type object 'lsst.afw.image.image.image.MaskX' has no attribute 'removeMaskPlaneDict'  {code}  The fix appears to be {{removeMaskPlaneDict}} --> {{removeMaskPlane}}    2) the function ignores the variable *name*, i.e. does not pass it to the call to [_sic_!] {{removeMaskPlaneDict()}}.  Having corrected the function name to {{removeMaskPlane}}, the we get the following:  {code:python}  TypeError: removeMaskPlane(): incompatible function arguments. The following argument types are supported:      1. (arg0: str) -> None  {code}  The fix here is to pass *name* to the function.    Finally, I also noticed that the *MultibandMask* class is missing the {{removeAndClearMaskPlane()}} function.  I don't think this was intentional, so one should be added.",3
"DM-16561","11/25/2018 20:47:01","Brokenness when comparing configs with inheritance relationship","...or just similar field names, I suppose. Viz:    {code:python}  In [1]: import lsst.pex.config as pexConfig    In [2]: class SuperConfig(pexConfig.Config):     ...:     foo = pexConfig.Field(doc=""test"", dtype=str)     ...:     In [3]: class SubConfig(SuperConfig):     ...:     bar = pexConfig.Field(doc=""test"", dtype=str)     ...:     In [4]: super_c, sub_c = SuperConfig(), SubConfig()    In [5]: pexConfig.compareConfigs("""", super_c, sub_c)  Out[5]: True    In [6]: pexConfig.compareConfigs("""", sub_c, super_c)  ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)  <ipython-input-7-e502e8e334d3> in <module>()  ----> 1 pexConfig.compareConfigs("""", sub_c, super_c)    /software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/pex_config/16.0-7-g9645df7+1/python/lsst/pex/config/comparison.py in compareConfigs(name, c1, c2, shortcut, rtol, atol, output)      101     equal = True      102     for field in c1._fields.values():  --> 103         result = field._compare(c1, c2, shortcut=shortcut, rtol=rtol, atol=atol, output=output)      104         if not result and shortcut:      105             return False    /software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/pex_config/16.0-7-g9645df7+1/python/lsst/pex/config/config.py in _compare(self, instance1, instance2, shortcut, rtol, atol, output)      356         """"""      357         v1 = getattr(instance1, self.name)  --> 358         v2 = getattr(instance2, self.name)      359         name = getComparisonName(      360             _joinNamePath(instance1._name, self.name),    AttributeError: 'SuperConfig' object has no attribute 'bar'  {code}    *Both* of these comparisons are mistakes: the configs aren't equal, so shouldn't return as if they are (in the first case), and the comparison should certainly not raise.",2
"DM-16563","11/26/2018 09:14:50","Fix \input detection regex in lsst-projectmeta-kit","In https://github.com/lsst-dm/dmtn-101, there are commands like {{\inputData{CBP}}}. These are being detected by lsst-projectmeta-kit as {{\input}} commands.",1
"DM-17277","11/26/2018 11:52:10","Provide support for Kafka writers","Support the auto generation of Kafka capable data writers for SAL mediated data",2
"DM-16589","11/26/2018 15:35:43","Create REQ for SLAC","Create the fy19 requisition for SLAC budgets",1
"DM-16598","11/26/2018 18:54:46","Add PhotoCalib.calibrateImage() option to compute variance without calib err term","Currently {{PhotoCalib.calibrateImage()}} computes the variance plane as:  {code}  fluxVar = instFlux.variance * scaleVar +  instFlux.variance * scale**2 + instFlux.image**2 * scaleVar  {code}    where {{scale}} and {{scaleVar}} are my own names for the scale factor that the photocalib applies and the measured uncertainty on that photocalib.     This equation assumes that the  scale factor variance and the image variance are independent, and this isn't obviously true.    Whether this is the correct variance equation is irrelevant, however. It is a change to how the variance plane in the zeropoint-normalized warps is currently computed, and I suspect  it is the primary source of the photometry differences we're seeing in coadds produced with jointcal.   While we are testing the effects of using {{jointcal}} vs. {{meas_mosaic}}'s calibration on coadd (DM-16596), it is important that we are able to test these two changes (jointcal's calibrations and the variance plane equation change) separately.     Implementation: This could be done with an extra argument like {{photoCalib.calibrateImage(Image, addCalibVar=False}} or with a state switch like {{Calib}}'s  {{Calib.setThrowOnNegativeFlux(False)}}.    ",2
"DM-16599","11/27/2018 07:18:57","merge_footprint_XXX flags are not being set/propagated","There seems to be a problem with propagating {{merge_footprint_XXX}} flags.         E.g. in some DESC processing an object has  {code:java} {'merge_footprint_i': False,   'merge_footprint_r': False,   'merge_footprint_z': False,   'merge_footprint_y': False,   'merge_footprint_g': False,   'merge_footprint_u': False,   'merge_footprint_sky': False,   'merge_peak_i': False,   'merge_peak_r': False,   'merge_peak_z': False,   'merge_peak_y': False,   'merge_peak_g': False,   'merge_peak_u': True,   'merge_peak_sky': False}{code}  note that there is a {{merge_peak_sky}} but no {{merge_footprint_sky}}. When [~jbosch] looked into this, it seemed as if the problem was generic rather than just for sky objects.",1
"DM-16603","11/27/2018 12:30:31","Fix dcrAssembleCoadd config issues","The {{config}} of {{dcrAssembleCoaddTask}} was not properly checked before, and appears to fail validation. This ticket is to make the simple fixes needed for the {{config}} to pass validation.",1
"DM-16612","11/28/2018 08:52:42","Fix compiler warnings in Meas Modelfit","Fix compiler warnings in meas model fit related to marking methods as overloads",1
"DM-16632","11/28/2018 15:28:14","dcrAssembleCoadd log should refer to patches/quadrants, not coords","The dcrAssembleCoadd log prints things like ""Computing coadd over (minimum=(39900, 39900), maximum=(41899, 41899))"". These are apparently x,y coordinates of some sort within a patch, and it works on each patch in quadrants. It would be clearer/friendlier to print something like ""Computing coadd for patch=1,1 quadrant 1/4.""",0.5
"DM-16641","11/29/2018 12:14:25","dcrAssembleCoadd makes too many nImages","The option doNImage is very handy for coadd assembly, as it creates an ""nImage"" of each patch with pixel values that show how many images (i.e., warps) will be assembled at that location. The nImages are saved as fits files. However, things get a little complicated with dcrAssembleCoadd's nImages.    This ticket is to change the present situation so only one nImage is created one time. The current state of affairs with doNImage (approximately in order):   * dcrAssembleCoadd calls regular assembleCoadd, which creates an nImage for all patches in the deepCoadd directory before beginning assembly   * These nImages are overwritten with nImages that have pixel values 3x larger than before (in this example, 3 is the number of DCR subfilters being used)   * Another set of nImages is created in the dcrCoadd directory alongside the dcr coadds, which are identical to the original nImages    Somewhat related to this, when one uses the butler to load a DCR coadd and does {{dcrCoadd.getInfo().getCoaddInputs().visits}}, it lists each constituent visit 3 times (i.e., nSubfilter times). This is probably also a bug.",1
"DM-16642","11/29/2018 13:06:58","Generalize job metadata code","The SQuaSH-requested metadata are currently hardcoded into {{MetricsControllerTask}}. Generalize this system by delegating the work to a SQuaSH-specific subtask, as described in [DMTN-098|https://dmtn-098.lsst.io/].",2
"DM-16648","11/30/2018 09:45:19","New design for DataUnit/Dimension objects","Prototype a new design to replace the DataUnit, DataUnitJoin, and DataUnitRegistry classes, without integrating them into the rest of the system (yet).     ",3
"DM-16651","11/30/2018 10:30:29","sphinxcontrib-bibtex error for DMTN builds (floating version incompatibility)","[~krzys]:    {quote}  hi, I'm having trouble building a DMTN. The build (https://travis-ci.org/lsst-dm/dmtn-098/builds/461067187?utm_source=github_status) fails with  {quote}  {code}  ```Exception occurred:    File ""/home/[secure]/virtualenv/python3.5.6/lib/python3.5/site-packages/sphinxcontrib/bibtex/directives.py"", line 29, in <module>      logger = sphinx.util.logging.getLogger(__name__)  AttributeError: module 'sphinx.util.logging' has no attribute 'getLogger'```  {code}    I think the issue that documenteer has been allowing the sphinxcontrib-bibtex version to flow, but is proactively pinning Sphinx. A new version of sphinxcontrib-bibtex might be assuming Sphinx 1.8 APIs, which we're not using yet. The obvious solution is to pin sphinx-contrib-bibtex.",0.5
"DM-16654","11/30/2018 11:31:16","Merge external PR for afw for/from Jim Chiang","PR from Jim here:    [https://github.com/lsst/afw/pull/415/files]    Ticket, test, merge.",0.5
"DM-16656","11/30/2018 14:03:11","LC Viewer: error loading periodogram and peak tables","Time Series viewer fails to load periodogram and peak tables.    Line 62 of PeriodogramAPIRequest.java - LC_FILE (""original_table"") parameter is always null. It comes from the tableMeta source attribute (line 508 of LcPeriodogram.jsx), which has been removed  as a part of DM-16273.      ",2
"DM-16668","12/02/2018 18:44:10","Implement the UI for TAP service, Master Ticket","- UI should be 3-boxes layout. Left: selection of schema and table, bottom: columns with constraints, right: spatial/temporal/wavelength constraints if the columns with the corresponding UCDs are present.   - We need a widget for TableItemListField, (similar to but hopefully more generic than IRSA's CatalogTableListField, which would map table rows into the selectable display items. This widget will be used to display schema and table lists. (Both of them can have long descriptions with links.)   - We need to upgrade CatalogConstraintsPanel to take table model (rather than giving it processor id and parameters), so that we can pre-sort table model by column_index, if it is available, and use columns UCD information to display spatial/temporal/wavelength constraints. Also, as Loi pointed out, CatalogConstraintsPanel is buggy: tab should be moving the focus to the next input box, filters and selections should clear when different table is used. (There are some hard-to-reproduce edge cases, when they don't.)   - Should we allow user to view and modify ADQL query? As shown by the prototype implementation, keeping ADQL and ADQL-building UI on the same page is tricky, because they have to be in sync, and while UI-to-ADQL is not hard, ADQL-to-UI is non-trivial. Should we allow free-form ADQL with tree-like schema browser on the side as an alternative to the table driven UI above?    Each of the items above can be a separate ticket.    Other comments from prototype implementation [https://github.com/Caltech-IPAC/firefly/pull/713:]   - ADQL query is what get submitted, but it's not obvious that you need to click on Set ADQL from Constraints before Search.   - When new schema and table are selected, ADQL query remained the previous value. Very confusing when you've gone through the selection/inputs process then click Search to get the same previous search results.   - Having fixed boxes with spiny during loading may improve look and feel. As is, UI is resizing without feedback.   - At some point, I assume there would not be both Additional constraints and ADQL Query in this dialog.   - The native drop-down list box is not the best widget for displaying schema and table lists. We'll need one with multi-line formatting, similar to CatalogTableListField.",2
"DM-16690","12/03/2018 12:52:39","Change totFlux column names in imageDifferenceTask","The direct image, forced flux columns created in imageDifferenceTask are not currently usable with slots and therefore awkward to use with calibration objects. This ticket will change the names to ip_diffim_forced_PsfFlux_[value] and the associated slot_forcedDirectFlux. The ticket will also propagate the PsfFlux plugin flag columns to the DiaSource catalog produced by ip_diffim.",2
"DM-16693","12/03/2018 16:27:50","Long Decam DCR run failures with invalid values","When [~mrawls] ran {{DcrAssembleCoaddTask}} it failed on patches with large regions of no data. The Task should test whether subregions contain data at an earlier stage, and skip them or fail earlier. See the log below for an example:      {code}  ==> slurm/job146732-24.out <==      res = func(self, *args, **keyArgs)    File ""/project/mrawls/pipe_tasks/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 233, in runDataRef      results = AssembleCoaddTask.runDataRef(self, dataRef, selectDataList=selectDataList)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/pipe_base/16.0-16-ge6a35c8+5/python/lsst/pipe/base/timer.py"", line 149, in wrapper      res = func(self, *args, **keyArgs)    File ""/project/mrawls/pipe_tasks/python/lsst/pipe/tasks/assembleCoadd.py"", line 361, in runDataRef      inputData.weightList, supplementaryData=supplementaryData)    File ""/project/mrawls/pipe_tasks/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 404, in run      convergenceCheck = (convergenceList[-1] - convergenceMetric)/convergenceMetric  ZeroDivisionError: float division by zero  {code}",1
"DM-16696","12/03/2018 18:35:29","PhotoCalib produces negative flux errors for negative flux measurements.","The `PhotoCalib`, `instFluxToMaggies` method is missing an absolute value causing calibrated flux errors to be negative when fluxes are negative (as can happen in a difference image).",1
"DM-16697","12/03/2018 21:55:52","Create instrucions for AAS workshop","Dig out LSST Europe instructions and update for AAS - ask NCSA about account sign up page.",1
"DM-16699","12/04/2018 07:42:08","Initial atmospec development work","Write a hacky, zeroth order version of the pipeline so that we have some skeleton tasks and structures in place to iterate on.    This should probably be done on a user branch, but I wanted to have a ticket number for this work to save git hassles later on as this will touch more than just atmospec.    This will initially be pretty hacky in places and probably not adhere to all standards, but will be tidied up before review.         Checklist/to-do list/steps in Augustin's pipeline:       {noformat}  Prepare Image:      Load image from disk      Subtract overscan and trim & assemble    Initial ISR Step      Load the dark by filename      Scale the dark by exposure time and subtract          (you don not want to see the details of this!)    Source extraction (using SExtractor):      Find the objects and centroids    Load SExtractor catalog    Process catalog to find the primary object      return the x,y of the brightest, roundest object's centroid    Calculate the spectrum's bounding box, depending on dispersion direction and the order used      Dispersion direction is either just x or y, not arbitrary angles yet      Will need extending for arbitrary angles    Secondary ISR:      Load masks from disk if masking      removal of periodic pattern on chip      Create initial dispersion relation      Apply initial flatfielding      Find and interpolate over cosmics    Extraction (choice between three methods:)      Spectral extraction method 1:          Calculate row-wise mean and sigma of background and subtract          Row-wise, calculate:              aperture flux              PSF 1D-Gaussian flux              Moffat flux              Gauss-Moffat Flux              Voigt flux        Spectral extraction method 2:          Calculate row-wise sum for aperture flux without subtracting background)          Fit 1D Gaus for PSF flux (again, without bgSub, and without pedestal in fit function)          Same for Voigt fit (no bgSub here either)        Spectral extraction method 3 (two pass method):          1st pass: fixed aperture, no centroid, meaning that:              Per row:                  Fit 1D Gaus for PSF flux (again, without bgSub, and without pedestal in fit function)                    Between passes, fit the row-wise Gaussian's parameters with linear and quadratics functions describing the position and width of them as a function of row number i.e. position in the spectrum, respectively          2nd pass: applying a variable aperture, using the 1D centroid and 1D sigma provided, meaning that:              Per row:                  Regular aperture flux summing (still without bgSub I think)                      aperture is centered at the row's centroid, and 6 sigma around it                  fit Gaussian over a very narrow range (120-180 pixels) up spectrum    Post-extraction:    Reflatfield    Find lines in spectrum    Match to known lines    Refine the dispersion relation calculation{noformat}        ",40
"DM-16700","12/04/2018 07:56:46","Add additional filter throughput terms to fgcmcal to fix HSC r/r2 bimodality","DM-15895 investigated the source of the bimodality in the HSC S18a fgcm run when comparing HSC r-band to Gaia G band.  Most of the relative offset between different regions of the footprint was due to differing throughput of the HSC r and r2 filters.  This ticket will add additional fit terms to fgcm to allow for these sorts of throughput offsets (which also applies to the HSC i/i2 filters).",2
"DM-19913","12/04/2018 09:51:44","XML - Create test to verify Alias matches EFDB_Name","The topic Alias must match the last part of the EFDB_Name, otherwise the SAL can't properly build the libraries.  Add a test to robotframework_ts_xml to verify this.",1
"DM-16707","12/04/2018 12:18:12","TAP search prototype","Create a prototype page which would allow to browse TAP schema and submit queries to the selected TAP service.",8
"DM-16729","12/05/2018 15:40:21","Test quotas for DASK workers","JupyterLab allows creating dask clusters.  Putting users in their own namespace allows for putting limits on the size of those clusters.  Test whether this works from the LSP at LDF.",2
"DM-16735","12/06/2018 11:14:48","Add sqlalchemy engine timeout setting to Ppdb config","Mutli-processing runs of ap_pipe using the sqlite setting for Ppdb are running into the issue of timeouts when storing, assessing the database. This is a similar issue to what to was discovered in the previous sqlite, db backend. This ticket will add a timeout config to Ppdb that will allow this setting to be increased for both sqlite and other db options.    Additionally, [~salnikov] stated that PpdbProtoVisits was not meant to be used in production, only his testing. As such I will remove this call from being used in ap_association.",2
"DM-16737","12/06/2018 12:05:35","Test release process","Test the build and release process.  This includes making all the relevant branches and mocking a feature branch that needs to be included (i.e. more than one release candidate).",3
"DM-16757","12/06/2018 18:16:27","Test the lsst-lsp-int(PDAC) after PDAC hardware merge into the k8s commons","test to make sure the system still work as it used to after the hardware merged into the k8s commons. ",2
"DM-16758","12/06/2018 18:17:34","Test the lsst-lsp-int(PDAC) in February/March/April","Monthly test to catch issues early",3
"DM-17310","12/07/2018 10:07:40","Complete implementation of basic functionality of TunableLaser CSC","Technically the code is there, but has not been functionally tested. In order to be complete, it must be tested and working. This allows the TunableLaser to start emitting the light beam and stop emitting through SAL state transitions. The component API already implements this functionality. Had Russell Owen look over the current implementation and he had some suggestions. I will be making those changes as part of this task.    The CSC now demonstrates basic state transitions including substates for propagating the laser. It demonstrates both local host functionality and network functionality with the chesterfield script machine. It can start propagating when in the enabled state. It can also stop propagating and be in the enabled state or if disabled will also stop propagating. Revamped the hardware wrapper api based on better understanding of hardware itself, updated the firmware to gain access to several error registers. Revamped unit tests to a slight degree. There is also a very simple settings api. Added all temperature sensors to telemetry in preparation for some stability(plus the fact that we don't know which ones are important). Added changedWavelength event to indicate that the wavelength has changed(may be redundant if wavelength is published as telemetry).          [Github Pull Request|https://github.com/lsst-ts/ts_TunableLaser/pull/8]",5
"DM-17324","12/10/2018 09:12:06","Configure mirror lab efd access via vpn","Make the mirror lab EFD instance accessible via a VPN connection using the second ethernet port",1
"DM-17325","12/10/2018 09:14:25","Update EFD writers for multiple inserts per transaction","Modify the EFD writer generators to produce code that bundles multiple available updates into single transactions to increase efficiency when the message rate it high",2
"DM-17269","12/10/2018 09:18:04","Add 10g switch and configure private network","Continue adding hw test cluster machines to optionally be no a private network,   provide mechanism for outside access (collaborate with IT N+S)",2
"DM-17333","12/10/2018 10:31:26","Finish acmCmd","For the most part I have the changes I have made here complete TSS-3276, this task is to go over with Dave on my computer and make sure indeed that this is the behavior that is desired. I also have questions.     Primarily which files are involved with the Python wrapping. I will also include these relevant files with the visuals I am creating to help understand the salgenerator pipeline. ",1
"DM-16788","12/10/2018 16:55:24","Add REST services for monitoring and managing the Replication System","Deliverables:  * REST services for monitoring and managing the Replication System  * Web UI for displaying the status and managing the Replication system and Qserv",20
"DM-16797","12/11/2018 08:30:30","Add template string names and formatters to PipelineTask configs","There is a common pattern in man of our tasks where dataset types share a common sub string. This ticket will introduce a way to template these names and format them at configuration time.",2
"DM-16801","12/11/2018 11:16:41","Add method to turn sequential indexes to pairs in skymap","Skymap supports turning index pairs into a sequential number. A method is needed to turn these sequential numbers back into pairs.",2
"DM-16810","12/12/2018 07:47:17","Butler schema changes to run on Oracle","We had a few issues importing the Butler schema into Oracle that should be relatively easy correct.    Oracle does not have a boolean data type for table columns. The column actual in table Datasetconsumers is a boolean in SQLite. We suggest changing that to varchar2(1) and adding a ""actual in ('T','F')""check constraint in Oracle.    ""Size"" is a reserved word in Oracle. The size column in table posixdatastorerecords will need to be renamed.    ""Group"" is a reserved word in Oracle. The group column in table detector will need to be renamed.    There is a data type mismatch in constraint Dataset_fk6 between column detector (number) in table detector and column detector (varchar) in table dataset. Data types must match in columns defined in FK/PK constraints.",1
"DM-16813","12/12/2018 12:16:02","Crosstalk correction doesn't raise when no xtalk matrix found","[~lauren] reports that if doCrosstalk is set to True, rather than raising like all other parts of isr (right?!) when failing to find the necessary calibration products, processing continues, and sets all xtalk bits in the mask plane.         Fix should be to raise if parameters aren't found, in keeping with doBias, doFlat, etc.",2
"DM-16814","12/12/2018 15:59:40","Replace Packages section in C++ Documentation Guide","The C++ documentation guide recommends providing package documentation using Doxygen groups, which are being phased out. The section on package documentation should be rewritten (possibly as a link) pointing developers to the new Sphinx-based system.",1
"DM-17323","12/13/2018 11:42:02","EFD Writers optimization","*Improvements*:   * Insert more data in 1 query   * Increase loop time on the EFD writers to reduce CPU usage    *Tests*:    (EFD writers used to perform these tests are in: [https://github.com/aanania/sal_initialization_scripts] , they are a copy of the SAL efd writers + small updates)    From some tests with the EFD writers with the M1M3 simulator (50hz data) on the VMs. These tests were done in VMs with limited resources so performance may not be as good as in real servers.    For the case of the MariaDB (same for influxdb) wih the VMs I got the issue that the insertion rate is slower than the data received.    With the EFD writers that comes with SAL I get this plot for (sndTimestamp – rcvdTimestamp). When the time is at ~220 I start losing messages due to overflow. (files attached with some data)     Then I did some udpates to the EFD writers to insert more rows in just 1 query (see files attached, they are not production updates because I removed EFD topic case from the code generation and I’m not very familiar with tcl). With these updates it seems that the performance issue was fixed and I’m not getting an increase in the time difference between (sndTimestamp – rcvdTimestamp) messages. The plot with this update (sndTimestamp – rcvdTimestamp) looks like this for a few hours run.    From mysql page (there’s also a similar page for Influx that says the same):         “To optimize insert speed, combine many small operations into a single large operation. Ideally, you make a single connection, send the data for many new rows at once, and delay all index updates and consistency checking until the very end.”      Ref: [https://dev.mysql.com/doc/refman/8.0/en/insert-optimization.html]     ",1
"DM-16817","12/13/2018 14:07:33","ingestCalibs resets all validity ranges","ingestCalibs [modifies the validity ranges of calibs|https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/ingestCalibs.py#L97-L175], since there may be some overlap in the validity ranges after adding new calibs. However, this applies the new validity range to all calibs, even those that do not have an overlapping validity range. It also operates on all types of calibs, so that when ingesting flats, the validity ranges of biases can be modified.  The end result is that is not currently possible to maintain more than a single validity range within the entire calibs registry.",2
"DM-16818","12/13/2018 14:40:10","t&s jenkins / docker hub credentials","[~ttsai] would like to be able to build docker images under jenkins and push them to docker hub.  This requires a docker hub ""role"" account to be setup and credentials installed into jenkins.",0.5
"DM-16819","12/13/2018 15:03:00","Make minimal Gen3 shim for Gen2 DataRef, ButlerSubset, and Butler","Write minimal shims that implement *some* Gen2 APIs using Gen3 implementations.  That will start with just get and put on DataRef and Butler, for Tasks that (at most) just pass Data IDs around without looking at what's in them.    Future ticket could add some data ID translation, if we decide that's better than putting conditional logic in the Tasks themselves.    Also add a ""gen"" attribute to both Butler classes, so conditional code can actually be written.",2
"DM-16821","12/13/2018 15:43:33","Add better debug output to butler queries","Format sql queries with query parameters for better debugging",1
"DM-16825","12/13/2018 16:32:15","Please add the RTM-006  raft ","Got the following while butlerizing some Dev runs in the {{LCA-11021_RTM-006-Dev}} folder of the SLAC test stand data:   {{ingest.parse WARN: translate_detector failed to translate detector: 'RTM-006'}}    Is it valid to add to the raft serial dict?     One example file:    {{/project/rgruendl/SLACmirror/SLACgpfs/jh_archive-test/LCA-11021_RTM/LCA-11021_RTM-006-Dev/5884D/xy_stage_acq/v0/38955/S22/E2V-CCD250-217-Dev_10.50_10.60_021_5884D_20180426174356.fits}}",1
"DM-16828","12/13/2018 17:42:57","Add Job viewer to lsst.verify","While working on {{ap_verify}} I found it easier to have a standalone program for summarizing a Job's contents than to manually load and inspect the Job object as described in [SQR-019|https://sqr-019.lsst.io/]. I add this script to {{lsst.verify}}, with documentation, in the hope that it will be useful for other developers.",1
"DM-16830","12/14/2018 10:30:55","Add versioning to PhotoCalib","While converting PhotoCalib to be defined in nanojansky, I realized it would be good to add versioning to its persisted output. [~jbosch] says that SkyWcs is probably the best example of how to do this (sadly, we have neither a standard nor a common convention for how to version persisted afw objects).    For example, we are going to want to add a wavelength dependency in the future, which will definitely change the persistence.",2
"DM-16831","12/14/2018 10:39:28","Crash when running multi-task pipeline","We tried to run {{stac}} with two tasks in a pipeline and it crashed:  {noformat}  $ stac -b ../ci_hsc/DATA/butler.yaml -i raw/HSC -o coll12 -d """" run -t RawToCalexpTask -t CalexpToCoaddTask  Traceback (most recent call last):    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1170, in _execute_context      context)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 504, in do_executemany      cursor.executemany(statement, parameters)  sqlite3.IntegrityError: NOT NULL constraint failed: DatasetCollection.dataset_id    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""/project/salnikov/gen3-middleware/pipe_supertask/bin/stac"", line 25, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 228, in parseAndRun      return self.runPipeline(qgraph, butler, args)    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 334, in runPipeline      self._updateOutputCollection(graph, butler)    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 412, in _updateOutputCollection      registry.associate(collection, list(id2ref.values()))    File ""/project/salnikov/gen3-middleware/daf_butler/python/lsst/daf/butler/core/utils.py"", line 290, in inner      return func(self, *args, **kwargs)    File ""/project/salnikov/gen3-middleware/daf_butler/python/lsst/daf/butler/registries/sqlRegistry.py"", line 613, in associate      [{""dataset_id"": ref.id, ""collection"": collection} for ref in refs])    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 948, in execute      return meth(self, multiparams, params)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/sql/elements.py"", line 269, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1060, in _execute_clauseelement      compiled_sql, distilled_params    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1200, in _execute_context      context)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1413, in _handle_dbapi_exception      exc_info    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 203, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb, cause=cause)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 186, in reraise      raise value.with_traceback(tb)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1170, in _execute_context      context)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/sqlalchemy/1.2.2+2/lib/python/SQLAlchemy-1.2.2-py3.6-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 504, in do_executemany      cursor.executemany(statement, parameters)  sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: DatasetCollection.dataset_id [SQL: 'INSERT INTO ""DatasetCollection"" (dataset_id, collection) VALUES (?, ?)'] [parameters: ((6, 'coll12'), (12, 'coll12'), (2, 'coll12'), (1, 'coll12'), (16, 'coll12'), (10, 'coll12'), (11, 'coll12'), (15, 'coll12')  ... displaying 10 of 34 total bound parameter sets ...  (22, 'coll12'), (None, 'coll12'))] (Background on this error at: http://sqlalche.me/e/gkpj)  {noformat}     This looks like it's trying to insert NULL as dataset_id into collection table for the new collection, and it happens when adding inputs to a new output collection. Maybe it also tries to add intermediate non-existing dataset?",2
"DM-16832","12/14/2018 11:14:50","Prepare summary of design & scope options for mini broker / alert filtering service","Requested by Zeljko. To include an overview of capabilities as well as potential cost savings. Initial text provided by [~ebellm].",1
"DM-18205","12/14/2018 11:51:04","Please enforce string length limits","SALPY libraries do not enforce any limits for strings when sending topics:  - If a string for a fixed-length string item is longer than specified by the XML, the whole string is sent without warning. This can lead to many bad things, including a segfault when DDS tries to clean up.  - Surely there is an upper limit for a variable-length strings -- the maximum amount of data for a topic, of nothing else. What happens if this limit is exceeded?    One or both of these can lead to a *segfault* when DDS tries to clean up later.    I think both conditions should raise an exception when one tries to send the topic. It is important to catch errors early, and guarantee that data received by all listeners is correct.",1
"DM-16834","12/14/2018 12:25:54","Fix skymap Dimension setting for converted Gen2 SkyMap Datasets","Running deepCoadd_skyMap (and similar Datasets) through gen2convert does not result in the set the ""skymap"" dimension key, which breaks PipelineTasks that use those Datasets as inputs.  This is due to missing translation rules in the gen2convert package.     ",0.5
"DM-16835","12/14/2018 12:32:17","scipipe/ap_verify job timing out","[ap_verify build #74|https://ci.lsst.codes/blue/organizations/jenkins/scipipe%2Fap_verify/detail/ap_verify/74/] failed with what is believed to be a timeout error triggered by DM-16017 doubling the number of Job files from 6 to 12. Rather than rolling back the merge, the timeout on the {{ap_verify}} job should be increased (see discussion on DM-16519).    Please increase {{run_timelimit}} to accommodate at least 12 Job files. This is the maximum number of {{ap_verify}} jobs expected in the foreseeable future; DM-16536 will decrease the number of files to 7, and any future increases will not happen until we introduce metrics at new levels of granularity (e.g., visit- or tract-level).",1
"DM-16837","12/14/2018 14:40:17","Let PipelineTasks accept config parameters that are lists from the command line","PipelineTask command line activator should be able to accept config overrides which are lists from the command line.",1
"DM-16849","12/17/2018 10:31:01","Get Augustin's reduction pipeline running","\{\{git clone https://github.com/guyonnet/slitless_image_reduction/}}    clone croaks & install it    Install and configure SExtractor    Get the pipeline running.    Then port it to py3 so it can be run from the same env as the stack.     ",5
"DM-16855","12/17/2018 15:03:40","Convert afw.cameraGeom to numpydoc","Convert all Doxygen-formatted docstrings in {{afw.cameraGeom}} to Numpydoc style, and move corresponding topic documentation to {{doc/}}.",1
"DM-16856","12/17/2018 15:07:21","Convert afw.coord to numpydoc","Convert all Doxygen-formatted docstrings in {{afw.coord}} to Numpydoc style, and move corresponding topic documentation to {{doc/}}.",1
"DM-16858","12/17/2018 15:08:26","Convert afw.display to numpydoc","Convert all Doxygen-formatted docstrings in {{afw.display}} to Numpydoc style, and move corresponding topic documentation to {{doc/}}.",1
"DM-16862","12/17/2018 15:09:56","Convert afw.math to numpydoc","Convert all Doxygen-formatted docstrings in {{afw.math}} to Numpydoc style, and move corresponding topic documentation to {{doc/}}.",2
"DM-16864","12/17/2018 15:10:47","Investigate relative DcrModel option","An option that might improve the DcrModel is to make the model relative to a reference image. Each subfilter model plane would act like a ""rubber sheet"" that can enhance or suppress the flux of the reference image in a pixel, rather than storing the absolute flux for that subfilter. This may make it easier to apply regularization, and ideally will also prevent noise being amplified.    If this feature ends up creating more problems than it solves, or otherwise does not perform as well as the original code, it should be closed as Won't Fix.",8
"DM-16869","12/17/2018 21:50:16","Inconsistent hash for DimensionSet/DimensionNameSet","It looks like recent switch from units to dimensions created issuers in GraphBuilder, I'm getting error when trying to run simple pipeline of two tasks :  {noformat}  Traceback (most recent call last):    File ""/project/salnikov/gen3-middleware/pipe_supertask/bin/stac"", line 25, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/cmdLineFwk.py"", line 198, in parseAndRun      qgraph = graphBuilder.makeGraph(pipeline, coll, args.data_query)    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/graphBuilder.py"", line 198, in makeGraph      originInfo, userQuery)    File ""/project/salnikov/gen3-middleware/pipe_supertask/python/lsst/pipe/supertask/graphBuilder.py"", line 324, in _makeGraph      dataRef = row.datasetRefs[dsType]  KeyError: DatasetType(calexp, {Visit, Instrument, Detector}, ExposureF)  {noformat}    Looks like the dict indexing on DatasetType is broken, and this seems to be related to hash returning different values when dimension is either DimensionGraph or DimensionNameSet.",1
"DM-17331","12/18/2018 10:20:56","Pybind11 Leaning","This task captures the time spent learning pybind 11. I have a standalone program that I have been working on as well which I used as a sandbox. Doing this helped with the learning curve jumping onto the templates used for ts_sal. ",3
"DM-17332","12/18/2018 10:32:59","Standalone OpenSplice","Create a standalone program that can run on multiple machines. ",3
"DM-16872","12/18/2018 11:08:10","Fix numpy warnings in afw","Building {{afw}} with Numpy 1.15 gives a large number of {{FutureWarning}} and {{PendingDeprecationWarning}}, such as:  {noformat}  PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.      dtype=float))    FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.      array = self._array[allSlices]    FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.    To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.  {noformat}    Rewrite the code to use non-deprecated APIs, as long as it's possible to do so using the Numpy 1.14 API.",1
"DM-16873","12/18/2018 11:39:27","Convert MeasureMergedCoaddSources to Pipeline Task","Turn MeasureMergedCoaddSources into a PipelineTak",8
"DM-16901","12/18/2018 16:24:45","Delete DeblendAndMeasureTask. ","DeblendAndMeasureTask has not been updated since 2014. It has bitrotted.  ",1
"DM-16904","12/19/2018 08:08:38","Pass butler object to adaptArgsAndRun in PipelineTask","The adaptArgsAndRun api for PipelineTask needs to include a butler object.",1
"DM-16905","12/19/2018 09:29:16","Install lsst-texmf into the docker image in full","Install the lsst-texmf repository in full into its Docker distribution. This will fix issues where \{\{generateAcronyms.py}} attempts to find a glossary file in etc/, but it is missing.    Also check that Python is installed in the Docker image (and drop Java since GaiaAcr.jar is replaced by generateAcronyms.py).    See also: https://github.com/lsst/lsst-texmf/issues/147",0.5
"DM-16907","12/19/2018 09:41:47","Test ATDome Algorithm","Test ATDome Algorithm",5
"DM-16908","12/19/2018 09:43:37","Finishing ATDomeTrajectory Algorithm","Finishing ATDomeTrajectory Algorithm         Duration: 10 Days",5
"DM-16914","12/19/2018 10:21:18","AuxTel Pneumatics & Pointing Configuration Integration in Tucson","Integration to be partitioned as follows:   # ATPointing + ATAOS integration    # ATAOS + ATPneumatics + ATHexapod integration   # ATPointing + ATAOS + ATPneumatics + ATHexapod integration    Required CSCs:   * ATPointing   * ATAOS   * ATHexapod   * ATPneumatics    Resources Required:   * Integration & Test Environment   * SAL   * EFD   * AuxTel Scripts",5
"DM-16917","12/19/2018 10:41:49","Test ATHexapod Algorithm","Test ATHexapod Algorithm",5
"DM-16922","12/19/2018 11:23:02","Region description parsing error on region size unit in arcsec and arcmin","The region description parser at the client side doesn't interpret the region description with size unit in arcsec or arcmin correctly.    ex. the region description is like  image;box(100, 150, 1.0', 1.2', 0) # color=red     ",1
"DM-17330","12/19/2018 14:32:40","Review Pointing Component v0.2 Release","This task covers the time spent to review the test results for the v0.2 Release of the Pointing Component.",3
"DM-16931","12/19/2018 15:33:53","Activator should register dataset types before trying to run pipeline","Currently you can't change the config options that specify DatasetTypes to something new without explicitly registering them yourself first.  Make this something the command-line activator can (optionally) do for you.     ",1
"DM-17364","12/19/2018 15:34:42","Please provide an event in ATDome that tells us where the dome is headed","The ATDomeTrajectory CSC needs to know where in azimuth the dome is headed, in order to work properly. It would be really nice to get this as an event instead of having to follow a telemetry stream, because the one value we care about only changes occasionally.    I am requesting a new event that outputs the commanded state. For this to be useful the dome would have to output the event when it starts up, presumably listing the current state of the dome as the commanded state.",2
"DM-16938","12/20/2018 12:55:34","Test ATPneumatics Algorithm","Test ATPneumatics Algorithm",5
"DM-16943","12/20/2018 13:35:40","Finishing ATPointing Algorithm","Finishing ATPointing Algorithm         Duration: 10 Days",5
"DM-16945","12/20/2018 13:39:43","Finishing ATAOS Algorithm","Finishing ATAOS Algorithm         Duration: 10 Days",5
"DM-16946","12/20/2018 13:40:13","Test ATAOS Algorithm","Test ATAOS Algorithm",5
"DM-16954","12/20/2018 16:51:15","Test ATPneumatics Algorithm w/ Hardware","Test ATPneumatics Algorithm",5
"DM-16957","12/20/2018 17:30:44","Multi-process option is broken in pipe_supertask","Looks like another victim of Unit to Dimension switch - {{-j}} option stopped working in stac:  {noformat}  $ stac -b ../ci_hsc/DATA/butler.yaml -i shared/ci_hsc -o coll-C -j2 run -t RawToCalexpTask  Process ForkPoolWorker-1:  Process ForkPoolWorker-2:  Traceback (most recent call last):    File ""/software/lsstsw/stack_20181012/python/miniconda3-4.5.4/envs/lsst-scipipe/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap      self.run()  Traceback (most recent call last):    File ""/software/lsstsw/stack_20181012/python/miniconda3-4.5.4/envs/lsst-scipipe/lib/python3.6/multiprocessing/process.py"", line 93, in run      self._target(*self._args, **self._kwargs)    File ""/software/lsstsw/stack_20181012/python/miniconda3-4.5.4/envs/lsst-scipipe/lib/python3.6/multiprocessing/pool.py"", line 108, in worker      task = get()    File ""/software/lsstsw/stack_20181012/python/miniconda3-4.5.4/envs/lsst-scipipe/lib/python3.6/multiprocessing/queues.py"", line 337, in get      return _ForkingPickler.loads(res)    File ""/project/salnikov/gen3-middleware/daf_butler/python/lsst/daf/butler/core/dimensions/dataId.py"", line 134, in __new__      raise ValueError(f""Cannot infer dimensions without universe."")    File ""/software/lsstsw/stack_20181012/python/miniconda3-4.5.4/envs/lsst-scipipe/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap      self.run()    File ""/software/lsstsw/stack_20181012/python/miniconda3-4.5.4/envs/lsst-scipipe/lib/python3.6/multiprocessing/process.py"", line 93, in run      self._target(*self._args, **self._kwargs)  ValueError: Cannot infer dimensions without universe.    File ""/software/lsstsw/stack_20181012/python/miniconda3-4.5.4/envs/lsst-scipipe/lib/python3.6/multiprocessing/pool.py"", line 108, in worker      task = get()    File ""/software/lsstsw/stack_20181012/python/miniconda3-4.5.4/envs/lsst-scipipe/lib/python3.6/multiprocessing/queues.py"", line 337, in get      return _ForkingPickler.loads(res)    File ""/project/salnikov/gen3-middleware/daf_butler/python/lsst/daf/butler/core/dimensions/dataId.py"", line 134, in __new__      raise ValueError(f""Cannot infer dimensions without universe."")  ValueError: Cannot infer dimensions without universe.  {noformat}  I think DataId is not trivially pickled and may need special serialization override.  ",1
"DM-16998","12/21/2018 10:03:52","Update k8s/qserv/qserv_deploy and run Replication Service at CC-IN2P3","- k8s 1.10.x upgrade was not working on Centos7  - old qserv version was not working on k8S 1.13 so qserv/qserv_version was updated to lates   - support for local storage was improved (using local-storage abstraction instead of hostpath 'raw' directive)  - replication service support was added (i.e. repl-svc is now able to use local storage trough k8s)  ",8
"DM-17003","12/21/2018 10:32:35","ci_hsc tests (may) not have correct linker environment on macOS","This is a probable regression introduced in DM-16819, when I tried to re-enable the test_import.py script in SCons (it looked like it was supposed to be run, but wasn't because the SConstruct wasn't talking to the SConscript because ci_hsc doesn't really use sconsUtils).    The causality was masked by a transient known download bug in Astropy.",2
"DM-17004","12/21/2018 10:46:49","JointcalRunner.__call__ not receiving ""butler"" in kwargs","[~boutigny] reported [on slack|https://lsstc.slack.com/archives/C2K1NGR8E/p1545327822003100] that he was getting failures in jointcal when running multiple tracts, because ""butler"" was not in the kwargs list. There haven't been any relevant changes to {{pipeBase.ButlerInitializedTaskRunner}}, so I'm not sure what could have happened.    I'll try to come up with a test case for this, but to start with I should try to reproduce it on lsst-dev.",3
"DM-17029","12/27/2018 13:57:28","Update LoadReferenceObjectsTask to output fluxes in nanojansky","PhotoCalib is now defined in terms of nanojansky, but our reference catalogs are saved in Jy, and remain in Jy when the user loads them. We need to get our refcat loaders to produce nJy to keep our calibrations self-consistent.    It may be enough to just multiply fluxes and flux errors by 1e9 after the data is read from disk, but [~krughoff] may have other suggestions.    We will have to add a few work-arounds for this to {{PhotoCal}} and other tasks that use the {{Calib}} object, as it will remained defined in Jy. If only we could do DM-10153 simultaneously (but I think that would be maddening).",8
"DM-17041","01/02/2019 12:57:27","Make PipelineTask command-line label default to Task class name","stac/pipetask currently require the user to create a label explicitly in order to provide config overrides.  We should make the default label to the Task class name (i.e. the same name that was just passed to {{-t}}).  While it's possible to to have multiple instances of the same Task, that's currently quite rare and we should make the common case simpler and more intuitive.    After a bit of discussion, [~nlust], [~yusra], and I agreed it'd be better to use the Task class name rather than {{Task._defaultName}} because the former is what's also typically present on the same command line.",0.5
"DM-17042","01/02/2019 13:10:10","PipelineTask single-config override does not parse booleans correctly.","Options like {{-c label.option=True}} don't seem to work.  If needed, I think [~yusra] has a more complete how-to-reproduce.",1
"DM-17043","01/02/2019 13:26:43","Add selection on S/N in objectSizeStarSelector","Currently, the *objectSizeStarSelector* Task allows for source selection based on flux limits. A selection on signal-to-noise (S/N), i.e. flux/fluxErr, is often preferred, so should be added as an option for this source selector.    Additionally, there is currently no unittest for *objectSizeStarSelector*, so a minimal one should be added (following the *test_astrometrySourceSelector.py* and *test_matcherSourceSelector.py* examples).    For this ticket, the option will simply be added in the same manner as the flux selection is done, but all current defaults will be left as is. Looking towards the future, I anticipate submitting an RFC to make S/N selection the default (instead of a flux-based selection, but, in principle, both could be activated simultaneously) as well as adapting all the *SourceSelectorTask* to make use of the configuration-based selection classes in *BaseSourceSelector*-derived Tasks (e.g. *objectSizeStarSelectorTask*, *AstrometrySourceSelectorTask*, *MatcherSourceSelectorTask*, *FlaggedSourceSelectorConfig*) [https://github.com/lsst/meas_algorithms/blob/master/python/lsst/meas/algorithms/sourceSelector.py#L153]).",8
"DM-17044","01/02/2019 13:42:58","Make searching packages for PipelineTasks optional in command-line activator","The process of importing all modules in lsst.pipe.tasks to search for PipelineTask is slow enough that it's not always a convenience; while it's very nice to have for the list subcommand, it'd be nice to disable it if the user provides fully-qualified names for the Tasks to be run.    Another option might be to cache the results of the search to a temporary file so it's only slow the first time the activator command-line tool is invoked in (e.g.) a terminal session, but it seems to me that it'd be difficult to know when to invalidate that cache.",0.5
"DM-17048","01/03/2019 13:50:38","Fix handling of ref_cat in gen2convert","We've been accidentally ignoring ref_cat because we didn't have its StorageClass (SimpleCatalog) in Gen3 yet; adding that yields a sequence of (minor) additional problems.",0.5
"DM-17049","01/04/2019 10:24:21","Add verify timestamp as a new field in InfluxDB measurements ","I was trying to add to the SQuaSH dashboards a single stat showing the time of the last validate_drp measurement - it turns out that stats visualization does not display timestamps, only numbers, and it is not possible to do math on timestamps in InfluxQL to get say ""number of days without measurements""     https://github.com/influxdata/docs.influxdata.com/blob/master/content/influxdb/v0.13/troubleshooting/frequently_encountered_issues.md#doing-math-on-timestamps     So I think it is a good idea to add the timestamp of the measurement as a new field, as we did for the EFD topics, there that information was useful to report on things like latency for instance. ",0.5
"DM-17338","01/04/2019 16:58:55","Create SmartDome command analysis summary","Create a summary analysis of the SmartDome commands, indicating which interrupt which others, etc.",1
"DM-17055","01/04/2019 17:36:12","Change Sigmas to Err in dax_ppdb schemas","Some of the columns names in dax_ppdb still has several columns that were not converted by myself from Sigma to Err. This ticket will convert the column names to the expected Err name.",1
"DM-17060","01/07/2019 14:13:21","Fix non-merged ticket","Some work did not get properly merged in pipe_supertasks on ticket-16797 before much of that content was refactored into different packages. This ticket will add that missing code to the right places.",1
"DM-17160","01/08/2019 07:33:04","Add error check to ATHexapod ","Add error check to ATHexapod code. This also includes command rejection.",2
"DM-17172","01/08/2019 07:53:01","Update configuration to MySQL Cluster to be able to add more topics ","The default configuration doesn't give enough memory to MySQL cluster to allocate all the tables for the EFD. Need to be reviewed why and how to solve it.",2
"DM-17171","01/08/2019 07:55:08","MySQL TIER tests using separate table spaces","Try to use table spaces to allocate tables in different disks for TIER purposes. This seems to be a better solution than having multiple instances (one for each TIER).    This test will be performed in a virtualized environment.",3
"DM-17170","01/08/2019 07:58:47","Meet with people from DatControl","Meet with Orlando Castro from DatControl to test and review communication protocol between our software and their HVAC system. It seems that is can be done using a SQL data base to send commands to their system.    Also need to check if this also give us the telemetry of the system, so we can publish it through SAL later.    This task is to keep things moving forward while German is on vacation.",1
"DM-17246","01/08/2019 11:49:41","Write ATMCS simulator","Write a simulator for the ATMCS CSC in Python. I'm not positive it will have the necessary performance, but it's quick to try.",5
"DM-17066","01/08/2019 11:55:54","update albuquery to work with latest QServ parser on UDFs","Per incoming request from [~gpdf], we are to deploy legacy DAX services at lsst-lsp-int in PDAC. Upon deployment, it was found that queries with UDFs including the following were failing:                     qserv_areaspec_circle()                     qserv_areaspec_poly()    As it turned off, the issue was caused by the addition of back ticks in albuquery to work around old Qserv parser not being able to handle top-level groupings with UDF.  The latest parser improvements by [~npease] seem to have removed that need to do that.           This ticket is to remove the workaround, retest, and deploy albuquery to lsst-lsp-int.     ",5
"DM-17067","01/08/2019 12:02:03","Move parsing of dataset-name-substitution option to ctrl_mpexec","DM-17060 implements parsing of the value of the option {{--dataset-name-substitution}} inside PipelineBuilder class. I think it would be better and more generic for PipelineBuilder to accept a dict and parsing should happen in a cmdLineParser.",0.5
"DM-17073","01/09/2019 09:02:07","ISR is too chatty","Lower some of the new logger INFO messages to DEBUG",1
"DM-17088","01/09/2019 15:00:24","Fix collections import deprecation warning in python 3.7","In python3.7 these warnings turn up:    {quote}  DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working  {quote}    pex_config and daf_persistence are known to have the problem.",0.5
"DM-17089","01/09/2019 15:10:54","remove antlr2 and related code from qserv","Remove any use of Antlr 2 and related code (refactor or remove as appropriate) from qserv.",5
"DM-17095","01/10/2019 09:58:26","Tests are very slow in obs_lsst","The butler tests in obs_lsst take a very long time (12 minutes on my laptop just for imsim). It seems that this is almost entirely down to a butler being setup for every single test. Investigate whether the butler can be set up once per test class instead (there might be an issue with the global variable when tests run in parallel).",0.5
"DM-17110","01/10/2019 13:57:54","Save tables in VO format","- Save the table in VO format.      Firefly will have more meta data about table supplied by VOTable, and keeping them in the saved file would be desirable.     - fix the following table save' related issues as commented in DM-16048 https://jira.lsstcorp.org/browse/DM-16048     Comment 2:  . In the ""Save table"" dialog, the second line ""Save as""  is changed to ""File name"".    Comment 3.   The default filename for a cone search has a dash, underscore, parenthesis, colon, space, and double quotes.     Fixing methods:        remove all of the special characters except for dashes and underscores.        Replace left parentheses with dashes.        Remove the trailing right parentheses, or replace the right parentheses with dashss.        Replace double-quotes with 'asec'.        Remove spaces.        Replace decimal points with 'p' for 'point'.          ex: for cone search,        WISE-allwise_p3as_psd (Cone:100'').csv => WISE-allwise_p3as_psd-Cone_100asec.csv       for elliptical, box, polygon, multiple-object and all sky search, form the filenames        respectively to be like:       WISE-allwise_p3as_psd-Ellipse60_0_0p26asec.csv        WISE-allwise_p3as_psd-Box_100asec.csv      WISE-allwise_p3as_psd-Polygon.csv      WISE-allwise_p3as_psd-MultiObject.csv      IRAS-irasgal-AllSky.csv    Comment 4,   For elliptical searches, the filename comes out with the string 'Eliptical'.     Fixing method:     Change it to be 'Ellipse', and the filename contains information on position angle and axial ratio as well as the semi-major axis.     Comment 5     When you upload a table for Multi-Object search,  there is no string to indicate what type of search was done.    Fixing method:   'MultiObject' is appended to the filename to indicate the type of search.    Comment 7,   For the ""Load Catalog Dialog"", the chosen filename is severely truncated in the display.   I see ""Custom catalog in IPAC table format"" in that dialog, even though I seem to be able to load csv and tsv files.    Fixing methods:   More space is shown to contain the chosen filename.   Fix the hint sentence that custom catalog in IPAC, CSV, TSV, VOTABLE and FITS table format are able to be loaded.    Other fixings:  Fixes the help link for 'table save' dialog and 'Load Catalog' panel.         ",8
"DM-17112","01/10/2019 14:26:16","Update corner rafts to have correct detector type","Since the base detector type is {{SCIENCE}} the Corner rafts are inheriting the same type.  I need to add some detector types that implement the other enumerations.",3
"DM-17279","01/10/2019 14:36:52","Debug network non-connection on mirror lab EFD machine and VPN","Debug network configuration issues involved in establishing remote access  (via reverse VPN to gateway pc) to EFD in mirror lab for test campaign",3
"DM-17326","01/10/2019 14:38:40","SAL mentoring","Continue SAL mentoring, advise in building single task command/event test programs for faster Jenkins testing",1
"DM-17144","01/11/2019 02:06:33","Update qserv and qserv_deploy image","- Update qserv and qserv_deploy container images an validate using CI  - Put all openstack script in `obsolete/` directories, final goal is not to have kubernetes provisioning script inside qserv_deploy, but to delegate it to underlying infrastructure  - update GKE procedure for creating k8s cluster",1
"DM-17146","01/11/2019 09:19:19","Fix storageClass for DetectCoaddSources PipelineTask","At some point daf_butler became more strict and there was no testing to catch that all instances of Exposure should be ExposureF. Fix this in DetectCoaddSourcesConfig",0.5
"DM-17148","01/11/2019 09:35:07","pipeline task activator squashes import errors","If the activator is being used to run or list, it will not find, or hide results for modules for which there was an import error, and report to the user that that module does not exists. This should at least produce a warning that there was a problem importing a module by python so the user can track down why there was a problem importing.",1
"DM-17149","01/11/2019 09:44:10","MergeDetectionTask PipelineTask mode should export peak schema","The pipeline task mode of MergeDetection task is only writing out it's detection schema, but should also be writing out its peak schema, add this behavior.",0.5
"DM-17327","01/11/2019 11:01:11","Support SAL on RPI platform for use as EAS generic sensor processor","Many of the EAS sensors have a requirement for SAL support on an embedded system  (Raspberry Pi). Create a reference implementation and consult with the developers to deinfe the appropriate generic EAS XML for the various sensor types",3
"DM-17173","01/11/2019 13:34:41","QServ parser returning errors for UDF queries that used to work","Per [~tatianag] report, the following queries are failing (they worked as of Nov 14, 2018):(    SELECT * FROM sdss_stripe82_01.Science_Ccd_Exposure WHERE scisql_s2PtInBox(corner1Ra,corner1Decl,0,-1,5,1) = 1 AND scisql_s2PtInBox(corner2Ra,corner2Decl,0,-1,5,1) = 1 AND scisql_s2PtInBox(corner3Ra,corner3Decl,0,-1,5,1) = 1 AND scisql_s2PtInBox(corner4Ra,corner4Decl,0,-1,5,1) = 1;   ERROR 4110 (Proxy): Query processing error: QI=?: Failed to instantiate query: ParseException:Error parsing query, near ""-1""       MariaDB [(none)]> SELECT * FROM sdss_stripe82_01.Science_Ccd_Exposure WHERE (scisql_s2PtInBox(corner1Ra, corner1Decl, 0, -1, 5, 1)=1) AND (scisql_s2PtInBox(corner2Ra, corner2Decl,0, -1, 5, 1) =1) AND (scisql_s2PtInBox(corner3Ra, corner3Decl, 0, -1, 5, 1)=1) AND (scisql_s2PtInBox(corner4Ra, corner4Decl, 0, -1, 5, 1)=1);   ERROR 4110 (Proxy): Query processing error: QI=?: Failed to instantiate query: ParseException:Failed to instantiate query: ""SELECT * FROM W13_sdss_v2.sdss_stripe82_01.Science_Ccd_Exposure WHERE(scisql_s2PtInBox(corner1Ra,corner1Decl,0,-1,5,1)=1) AND(scisql_s2PtInBox(corner2Ra,corner2Decl,0,-1,5,1)=1) AND(scisql_s2PtInBox(corner3Ra,corner3Decl,0,-1,5,1)=1) AND(scisql_s2PtInBox(corner4Ra,corner4Decl,0,-1,5,1)=1)""         MariaDB [(none)]> SELECT * FROM sdss_stripe82_01.Science_Ccd_Exposure WHERE scisql_s2PtInBox(corner1Ra,corner1Decl,0,-1,5,1) = 1 AND scisql_s2PtInBox(corner2Ra,corner2Decl,0,-1,5,1) = 1 AND scisql_s2PtInBox(corner3Ra,corner3Decl,0,-1,5,1) = 1 AND scisql_s2PtInBox(corner4Ra,corner4Decl,0,-1,5,1) = 1;   ERROR 4110 (Proxy): Query processing error: QI=?: Failed to instantiate query: ParseException:Error parsing query, near ""-1""          ",1
"DM-17183","01/11/2019 15:28:36","Testing of ATWhiteLightSource SW","Testing of ATWhiteLightSource SW",20
"DM-17185","01/11/2019 15:32:09","Testing of ATThermoelectricCooler SW","Testing of ATThermoelectricCooler SW Development",5
"DM-17186","01/11/2019 16:47:24","show_salpy_attributes.py fails for non-indexed components","The ts_salobj script show_salpy_attributes.py fails for SAL components that are not indexed because it uses index 1 instead of 0. The fix is trivial.",1
"DM-17188","01/11/2019 19:22:05","Knowledge Transfer of Active Optics","Teach David (PhD student in Stanford University) the background knowledge of active optics and the use of AOS codes (contains ts_tcs_wep, ts_tcs_wep_phosim, and ts_tcs_ofc). Discuss the AOS closed-loop simulation with PhoSim.",5
"DM-17189","01/11/2019 19:41:16","Document the WEP with New Update","Document the wavefront estimation pipeline (WEP) with new update to adapt the new scientific pipeline feature (tag: sims_w_2018_47). The obs_lsst replaces the obs_lsstSim. The phosim_utils is used to repackage the PhoSim amplifier outputs. ",2
"DM-17197","01/14/2019 10:38:14","Support PipelineTask execution without writing init output datasets","Running {{pipeTask}} with the same output collection and different quanta tries to write init outputs (schemas) multiple times, failing.  Provide (ideally) ways to run any combination of   - Preflight   - Initialization I/O   - Execution    or if that's too hard, at least make it possible to run Execution without Initialization I/O.",2
"DM-17198","01/14/2019 10:54:25","Finishing DIMM SW Development","Finishing DIMM SW Development:   * CSC Development   * Integration with Turbina software   * Integration with vendor provided control software",5
"DM-17199","01/14/2019 10:55:40","Testing of DIMM SW","Testing of DIMM SW",5
"DM-17200","01/14/2019 10:58:37","DIMM + EFD HW Integration","DIMM + EFD HW Integration",5
"DM-17202","01/14/2019 11:19:45","Move SAL unit tests into one","Move all the SAL unit tests into a single file so that when running under an automated test environment it is much faster to finish testing.     It is faster because unlike how it currently is, you need to start SAL services for every single topic. By having all the tests in a single file the SAL services only needs to start once.",3
"DM-17204","01/14/2019 12:15:41","NDB Cluster constraint","MySQL Cluster has constraints that are not present in mariadb particularly that there is a limit in the ammount of columns in a table (512 in MySQL Cluster and larger in mariadb). Currently there are CSCs publishing more than 512 attributes in their topics which prevent us from creating the tables in the EFD for those cases (MTM1M3 and DomeLouvers Telemetry, those I know and I don't know if there are more cases).    Reference:     [https://dev.mysql.com/doc/refman/8.0/en/mysql-cluster-limitations-database-objects.html]     ",2
"DM-17207","01/14/2019 13:56:54","ts_salobj refinements","A few refinements that would make ts_salobj a bit easier to use:  * All topics should have an instance of the data as attribute {{data}}. It makes sending commands and outputting telemetry and events much simpler and avoids the need for the CSC to create its own attributes containing received data.  * Add a {{set}} method to all topics that allows setting fields of the contained data.  * Use this contained data by default for methods that take {{data}} as an argument, e.g. {{put}} and {{start}}.  * Add {{set_put}} to {{ControllerTelemetry}} which calls {{set}} and then {{put}}, allowing you to update the data and output it with a single call. I anticipate that this will become the standard way to output telemetry.  * Add a {{set_put}} method to {{ControllerEvent}} that only puts the data if it has changed or has never been output. Again, I anticipate that this will become the usual way to output events.  * Hoist some names in {{test_utils}} up into the {{lsst.ts.salobj}} namespace, in particular the {{assertRaisesAckError}} and {{set_random_lsst_dds_domain}}  * Make {{heartbeat_interval}} an attribute of {{BaseCsc}} so it can easily be changed, e.g. for unit tests.",1
"DM-17210","01/14/2019 14:08:54","Confluence Page Organization","Go through the teams Confluence page. Reorganize so that finding documentation becomes easier. Have had multiple conversations with team members and Jonathan Sick before making the actual changes as well as a tech talk where team members can give input. ",3
"DM-18252","01/14/2019 15:31:48","SAL mentoring","Continue SAL mentoring activities with Andrew, Tei-wei, Garry , Petr etc",1
"DM-18261","01/14/2019 15:32:48","Support SAL on RPI platform for use as EAS generic sensor processor","Many of the EAS sensors have a requirement for SAL support on an embedded system  (Raspberry Pi). Create a reference implementation and consult with the developers to deinfe the appropriate generic EAS XML for the various sensor types",3
"DM-17322","01/14/2019 15:35:07","EFD Writers optimization","Change writers to implement transactions which batch sets of inserts for increased effciency",2
"DM-17211","01/14/2019 15:38:50","TAP Search: Update the TAP panel layout/infrastructure to allow for concurrent work","Do the following:   * Update layout to all for a blank spacial/time search panel   * Update layout to allow to a tab area with advance tap search panel - move existing dialog box.  Possibly  add a button to populate and switch tabs.   * Add area to select TAP search service.   * Any other clean up to allow for concurrent work on the TAP search panel",8
"DM-17212","01/14/2019 15:46:43","TAP Search: Create a component to select tables and schema that is Firefly/IRSA viewer like","Do the following:   * Write the top left side of the panel of the TAP panel   * Change the select tables on be one similar to IRSA viewer/firefly catalog   * Write this component   * Add button to be a placeholder for a advanced table select popup   * Anything else to complete this panel",8
"DM-17213","01/14/2019 15:52:02","TAP Search: Spatial and temporal specialized search boxes","To the following:   * Write top top right box on the TAP search panel - Spatial and time selection   * Work with [~tatianag] and [~gpdf] to determine what should be in it.   * Add analysis of tap schema to control what is displayed",8
"DM-17214","01/14/2019 15:55:00","TAP search: Advance ADQL query panel for ","Do the following:   * Write advance query panel part of the TAP search   * work with [~tatianag] and [~gpdf] to determine what should be in it.",8
"DM-17219","01/14/2019 17:04:10","milestone upates","1. Milestone for Science Platfiorm       Depends on DAX services for ADQL and Imgacutout        decide if it test uses the new CAOM model            2. DAQ milestone - nominally April as thats when Huffer the new DAQ (full) would be delivered. ",1
"DM-17227","01/15/2019 06:09:05","Update ATMCS interface","Update ATMCS interface according to discussion with Patrick, Rolando and Russell    Changes:  - Add M3 motor encoder data to existing telemetry topics.  - Remove ATMountState enumeration values that overlap summary state.  - Remove M3State enumeration values that overlap summary state.  - Reorder fields in the {{mountEncoders}} telemetry topic to the standard order used everywhere else.  - Rename command topic fields that are ignored to {{ignored}}.  - Rename brake fields from {{engage}} to {{engaged}}.  - Remove {{m3RotatorDetentLimitSwitch}} to {{m3RotatorDetentSwitches}} and remove {{Detent}} from the field names.  - Remove redundant {{State}} suffix from {{ATMountState_}} and {{M3State_}} events, e.g. change {{M3State_DisabledState}} to {{M3State_Disabled}}.  - Add documentation for all topics (and fix existing incorrect documentation).  ",2
"DM-17247","01/15/2019 12:22:53","Fix unresolved conflict error on last merge to master","Fix commit error on last commit: 4fc891f69ed99fa223f464564b31d495e6902a8ba",1
"DM-17248","01/15/2019 13:00:38","ap_verify should create .json files in the workspace by default","Currently, {{ap_verify}} creates its metrics files in the caller's working directory. A more compartmentalized way to handle the output is to put it in the workspace created with the {{\-\-output}} command-line argument. This will not interfere with the Butler (since the workspace is not a repository) or CI upload (which searches for files in the entire directory tree, see DM-16728).    The main complication is how to handle the {{\-\-metrics-file}} argument. The best option is probably to include an \{output} placeholder (making the default value ""\{output}ap_verify.\{dataId}.verify.json""). This will avoid surprising behavior from making the directory implicit, and will be backwards-compatible with any scripts that already use {{\-\-metrics-file}}.",2
"DM-17249","01/15/2019 13:11:01","Check target from controller for ATHexapod","Make the CSC to check the target directly from the PI Controller. Currently it directly keeps record of the target.",1
"DM-17281","01/16/2019 11:19:35","Update difference imaging templates in datasets","Both {{ap_verify_hits2015}} and {{ap_verify_ci_hits2015}} use very old coadds from HiTS 2014 data as templates for difference imaging. These should be updated with the latest and greatest (most likely CompareWarp coadds constructed from a subset of best seeing images).",2
"DM-17284","01/16/2019 11:55:00","Add ctrl_mpexec to lsst_distrib","Last item for RFC-554 is to add new re-factored package to lsst_distrib.",0.5
"DM-17288","01/16/2019 15:21:28","Test multicast on GKE","Evaluate whether multicast can be used in a GKE cluster for multiple firefly servers.",8
"DM-17289","01/16/2019 15:31:15","Monthly report","write the Management summary",1
"DM-17294","01/16/2019 16:08:55","Move and/or document miscellaneous files in obs_lsst","data/input/hack_python.txt needs at least an explanation of what it does    bin.src/makeFpSummary.py probably belongs somewhere else, not in obs_lsst.",1
"DM-17297","01/16/2019 16:55:15","Remove CatalogStarSelector","Implement the adopted change of RFC-560, removing the unused CatalogStarSlectorTask",2
"DM-17298","01/16/2019 18:42:53","LSP Notebook scale test on Google Cloud","Perform a large scale (cores greater than available at NCSA) test of notebook aspect deployed on the Google Cloud. Scheduled for Thursday, 01/17/19 12pm to EOB project time.",8
"DM-17299","01/16/2019 19:14:57","Clean up obs_lsst scripts","Test scripts in {{bin/}} need to either go elsewhere or have names that are more clear. They'll get loaded into the default search path and nobody will know why {{processCcd}} does something totally different than {{processCcd.py}}.    In the Python library directory, the files {{segmentationToRafts.py}} and {{phosimToRafts.py}}, {{generateCamera.py}} are more like scripts, belong elsewhere (e.g. in {{bin/}}). They have  {\_\_main\_\_}} blocks and they do not look like they are meant to get imported by anything else so there is no reason to have them in the module.    These issues should be addressed before inclusion in lsst_distrib.",3
"DM-17303","01/17/2019 11:06:15","Prepare ATPointing and ATAOS for integration test","This task will consist in making sure ATPointing and ATAOS run with the same version of SAL, write an OCS-Script to drive the test and run the test locally.     Final integration will happen when both systems can be deployed on the Test environment using our deployment strategy.     Artifacts:   * Docker container with ATPointing component   * Docker container with ATAOS component   * OCS-Script to drive the test    Required CSC:   * ATPointing   * ATAOS   * ScriptQueue          ",1
"DM-17315","01/17/2019 14:09:26","Create Makefile tcl","Create the tcl script that will produce a makefile for the cpp generated file. ",2
"DM-17317","01/17/2019 14:39:26","Clean ts_statemachine and salpytools logic for LinearStage CSC","The LinearStage has logic that is tied to the now deprecated ts_statemachine and salpytools. The hardware api shall be cleaned up of this logic. The statemachine api will be removed because salobj inherently handles state functionality. The project structure will also change to a more traditional pythonic structure. The zaber library will be upgraded to the latest one released.         [Github Pull Request|https://github.com/lsst-ts/ts_LinearStage/pull/6]",1
"DM-17318","01/17/2019 14:45:18","Update sqlalchemy to v1.2.16","Update the EUPS version of sqlalchemy following RFC-564.",0.5
"DM-17319","01/17/2019 15:33:24","Add basic simulation_mode to TunableLaser hardware api","As part of the new simulation mode for CSC, design and implement a simulation mode for the hardware. Mock serial port and return sensible values for registers.    Read only registers will set static value and writable registers will set whatever acceptable values are sent.    [Github pull request|https://github.com/lsst-ts/ts_TunableLaser/pull/9]",2
"DM-17320","01/17/2019 15:37:51","Add settings & configuration implementation to TunableLaser api","Add settings & configuration reader to TunableLaser api that is in accordance with the yaml specification. It will use pyyaml to parse and update settings. Currently blocked due to in-progress design of salobj configurable CSC.",2
"DM-17328","01/17/2019 16:21:51","Add MovingState substate to LinearStage CSC","Add moving state substate to LinearStage xml and CSC. Involves adding Enum subclass and finishing detailed_state getter and setter.    Tested with localhost vm and embedded pc using chesterfield as remote. Linear Stages are now on separate USB ports and are capable of moving at the same time. They will not accept a moving command while moving. Script code has been modified accordingly. XML was adjusted for index-ability.    [Github Pull Request|https://github.com/lsst-ts/ts_LinearStage/pull/7]",2
"DM-17341","01/18/2019 09:24:00","Verify SAL java with latest release","# Test SAL java interface with aux tel camera CSC in simulation mode on the test hardware cluster.   # Have [~tjohnson] test with his latest auxtel camera code and the physical camera system.",2
"DM-17342","01/18/2019 09:24:36","Add simulation mode & settings to CBP CSC","Add simulation mode to CBP CSC. Commands should return sensible values.    Add settings and configuration to CBP CSC.",3
"DM-17344","01/18/2019 09:26:20","C++ Events","Currently only the commands in c++ are being generated into a single file. This task is to complete tcl scripts to produce the telemetry tests into a single file as well.",3
"DM-17345","01/18/2019 09:27:30","Stress test m1m3 EFD and Kafka writers","Add kafka high data rate writers to the end machine in the mirror lab.  Run on a separate machine as well. Verify binary log backups",3
"DM-17346","01/18/2019 09:32:04","C++ Telemetry Test file","Currently only the commands in c++ are being tested from a single file. This task is to complete tcl scripts to produce the telemetry tests into a single file as well.",3
"DM-17347","01/18/2019 09:33:43","Configurable speeds ATHexapod","* Implement setMaximumSpeeds in the ATHexapod, this will be used from the configuration files.   * Update target position from the controller directly instead of following at a higher level ",1
"DM-17348","01/18/2019 09:35:35","Java Test scripts | part 1","salgenerator should now be producing the C++ telemetry, commands, and event tests into a single file so that during automated testing the tests can be completed much faster than having to fire up a sal manager every time. Which currently is all we have.  This task is to do the same for Java. The reason there is part 1 to the task is I am not quite sure how it works. Once I identify what I need to know I can better estimate the length that this task will take me.",2
"DM-17366","01/18/2019 10:38:33","Get Pointing Component to build in the TSSW CI","Get the Pointing Component tests created and running.  Get the Docker image from Andrés that has the PtgComp installed.  -Figure out how to run the vendor supplied Robot-Framework tests.-  ",2
"DM-17368","01/18/2019 10:58:11","Support M1M3 Mirror Lab testing - part 2","This task covers the re-training on M1M3 control in support of Mirror Lab testing.",3
"DM-17369","01/18/2019 11:01:18","Enable sphinx documentation in obs_lsst","obs_lsst needs to have the documentation build enabled. It still mostly uses doxygen strings (presumably as a hold over from the early days).",2
"DM-17370","01/18/2019 11:07:48","Update the AOS modules to Use the Tag: w_2019_02","Update the ts_tcs_ofcPython, ts_tcs_wep, and ts_tcs_wep_phosim to use the scientific pipeline with the tag of w_2019_02. This is to adapt the update of obs_lsst.",2
"DM-17371","01/18/2019 11:08:09","Research Docker and Jenkins usages and integration","This task covers the effort to learn how to use Docker and how it integrates with Jenkins.  The TSSW Jenkins environment was updated to be more DM like and therefore, requires using Jenkins to run jobs via Docker containers.  I need to time to learn and implement the testing in this new paradigm.",2
"DM-17372","01/18/2019 11:29:46","Write analysis report on SmartDome software","An action item out of the ATDome meeting relating to [https://confluence.lsstcorp.org/display/LTS/ATDome+Control+Issues] was to look at the vendor-supplied SmartDome software, identify what desired features are missing or already present somehow or how they could be met, and summarize options for making modifications.    The deliverable will be a report that addresses these topics.",2
"DM-17373","01/18/2019 11:33:42","Go over pointing component contract status with Tiago","The task is to meet with Tiago (and Andy Clements) and provide Tiago with the information he needs to take over management of the pointing component contract.",1
"DM-17374","01/18/2019 11:45:52","Review initial ATHexapod code","This task is to review the following ATHexapod tasks.    https://jira.lsstcorp.org/browse/DM-17161    https://jira.lsstcorp.org/browse/DM-17162    Since this review is expected to be rather involved, it deserves a task of its own.",2
"DM-17375","01/18/2019 12:03:06","Change license of astro_metadata_translator","astro_metadata_translator has by default been using GPLv3. In order to encourage contributions from the astropy community in this ticket we will change the license to BSD 3-clause.",2
"DM-17382","01/18/2019 16:02:59","Make CharacterizeImageTask a pipelineTask","CharacterizeImageTask needs to be converted to a gen3 pipelineTask.",2
"DM-17383","01/18/2019 16:11:51","Onboarding instructions for Nebula are confusing","https://developer.lsst.io/team/onboarding.html#data-facility-resources states (or, at least, heavily implies) that everybody being onboarded to DM automatically gets issued with an account which provides access to Nebula.    Apparently, that's not actually the case: Nebula accounts are only available following the procedures at https://developer.lsst.io/services/nebula/index.html and are not issued automatically.    Please update the Dev Guide to make this clear.",1
"DM-17384","01/18/2019 17:35:17","Write the Self-Evaluation","Write the self-evaluation in Ultipro for the 2018 performance evaluation.",1
"DM-17390","01/22/2019 10:39:01","Convert CalibrateTask into a PipelineTask ","Convert CalibrateTask into a Pipeline task",8
"DM-17394","01/22/2019 13:17:15","Re-enable init-only option ","It looks like in the DM-17038 refactoring an important option was ""factored out"". Need to check for init-only in the new implementation of runPipeline method.",0.5
"DM-17395","01/22/2019 13:28:22","Write all outputs from CharacterizeTask in ci_hsc","By default ProcessCcdTask does not write out all the outputs generated by CharacterizeImageTask. This ticket will set configs such that all outputs are written out when ci_hsc is run. This will aid in comparison checks and migration to gen3 middleware. This will not increase the run-time of ci_hsc, and will only minimally impact storage.",1
"DM-17397","01/22/2019 14:27:06","Improve error reporting for set and set_put","{{BaseTopic.set}} and {{ControllerEvent.set_put}} both rely on {{getattr}} and {{setattr}} to do their work, neither of which gives a clear error when something goes wrong. Improve error reporting by catching the error and adding additional information (e.g. using exception chaining).",1
"DM-17398","01/22/2019 14:48:58","Support execution of incomplete graphs","Michelle discovered an interesting issue while trying to run {{pipetask}} on a single Qunatum (extracted from a full graph). Graph {{traverse()}} method tries to determine prerequisites for a quantum execution by looking at quanta inputs and determining which other quanta produced those inputs. It works OK on a full graph but when single quantum is removed from a graph its prerequisites are not in the same graph anymore and {{traverse()}} currently does not like this (it crashes).      ",0.5
"DM-17399","01/22/2019 15:01:59","Issue(s) with test_measure.py in meas_algorithms","In the {{testFootprintsMeasure}} function in *tests/test_measure.py* in {{meas_algorithms}}, the test claims to ""Check that we can measure the objects in a detectionSet"". All of the ""asserts"" of the test occur within a loop over the sources in {{measCat}} ([here|https://github.com/lsst/meas_algorithms/blob/master/tests/test_measure.py#L147-L167]). HOWEVER, when I run this test, I get 0 detections, so that loop is never entered and no assertions get made (which I noticed when I was not seeing any dots on the sources in ds9 when updating the display code). It seems that something is seriously amiss if we are expecting (it looks like at least 3) detections but getting 0.  (I also think I see this as the line   {code}  measurement INFO: Measuring 0 sources (0 parents, 0 children)  {code}  in Jenkins artifact files, e.g. [here|https://ci.lsst.codes/job/stack-os-matrix/29286/artifact/centos-6.devtoolset-6.py3/lsstsw/build/meas_algorithms/tests/.tests/pytest-meas_algorithms.xml], so it's not just a local setup issue).    Also, in the {{testDetection}} function, nothing is actually ""asserted"" in the function, so it seems that the ""test"" is that it does not crash when running through the operations? Would a check on a (minimum?) number of detections be appropriate here?",1
"DM-17404","01/22/2019 15:21:29","Finishing LinearStage CSC Development","Finishing LinearStage CSC Development",5
"DM-17406","01/22/2019 15:24:39","Integration of TunableLaser CSC","Integration of TunableLaser CSC",20
"DM-17407","01/22/2019 15:25:58","Finishing of CBP CSC Development","Finishing of CBP CSC Development",5
"DM-17412","01/22/2019 16:39:35","Make MergeMeasurementsTask a valid pipelineTask","MergeMeasurementsTask needs to be converted to a gen3 pipelineTask.",2
"DM-17416","01/23/2019 06:11:27","Fix origin parameter name in Gen2->Gen3 Butler shim","The shim calls this parameter ""origin"", but the actual Gen2 butler calls it ""imageOrigin"".",0.5
"DM-17429","01/23/2019 12:23:28","PSFs on coadds are narrower than in model","It has been observed that the PSF as measured on the coadd is narrower than the PSF model says it should be. This has found to be true on the warps as well, so we believe it's an artifact of warping. [~rearmstr] is experimenting with higher-order Lanczos kernels.",1
"DM-17433","01/23/2019 13:44:30","Remove unused code from coadd_utils","Remove the {{Coadd}} class, any supporting code, and the {{makeBitMask}} function.",1
"DM-17437","01/23/2019 17:09:52","Add UC Davis camera support to obs_lsst","In [obs_lsst #43|https://github.com/lsst/obs_lsst/pull/43] [~cslage] has added support for the UC Davis camera to obs_lsst.    We need to shepherd this through adoption into the DM codebase.   ",2
"DM-17441","01/24/2019 11:17:02","Modernize ATDome simulator and make it the real CSC","Improve the ATDome CSC as follows:  - Make it the real CSC, not just a simulator  - Update for XML changes in DM-17610  - Add a command for homing  - Reject attempts to move in azimuth or home while already homing  - Make the engineering commands that move individual doors safer (the usual commands that open or close both doors are always safe; they have internal sequencing) and add unit tests for them:  -- Reject attempts to move the main door unless the dropout door is fully open or fully closed  -- Reject attempts to move the dropout door unless the main door is fully open  - Rename the bin script to remove the ""_simulator"" suffix and add a unit test for it  - Take a first pass at supporting configuration. The real code will have to wait until we have a standard for this.",2
"DM-17442","01/24/2019 12:57:57","review laser tracker protocol interface docs","Review laser tracker protocol interface docs, update text and diagrams to include new terminology (Laser Tracker CSC instead of Laser Tracker Controller, remove references to labview, etc). Our vendor, New River Kinematics, is proposing an API for communication between our CSC and their part of the code. Review this API and prepare feedback for meeting next week",2
"DM-17446","01/24/2019 13:53:34","overscan improperly sets bounding boxes when leading/trailing columns are skipped","Bounding box handling of `overscanNumLeadingColumnsToSkip` and `overscanNumTrailingColumnsToSkip` is incorrect.",1
"DM-17447","01/24/2019 13:59:49","Update ci_hsc preSfm command","DM-17395 changed ci_hsc to write outputs for CharacterizeImageTask, but missed updating the SConsript for generating the pre-sfm run. This ticket fixes that.",1
"DM-17449","01/24/2019 14:55:09","Photocal not setting up DirectMatchTask correctly","PhotoCalTask passes a config object as the first argument to the DirectMatchTask constructor, however DirectMatchTask defines butler as its first argument, not config. This causes the config object to be assigned to the butler argument. If the referenceObjectLoader is passed to DirectMatchTask's constructor then this has no impact. However if the referenceObjectLoader is None this will cause the task to fail.    See:  https://github.com/lsst/pipe_tasks/blob/master/python/lsst/pipe/tasks/photoCal.py#L256  and:  https://github.com/lsst/meas_astrom/blob/master/python/lsst/meas/astrom/directMatch.py#L42",1
"DM-17451","01/24/2019 15:13:47","Invalid memory access for getX/getY when slots aren't defined","Looks like we have an out-of-range memory access error hiding in the catalog centroid getters, if the catalog does not have centroid slots defined:    {code}  import lsst.afw.table  schema = lsst.afw.table.SourceTable.makeMinimalSchema()  lsst.afw.table.Point2DKey.addFields(schema, ""centroid"", ""centroid"", ""pixels"")  catalog = lsst.afw.table.SourceCatalog(schema)  record = catalog.addNew()  record.set('centroid_x', 1)  record.set('centroid_y', 2)  record = catalog.addNew()  record.set('centroid_x', 4)  record.set('centroid_y', 5)  # no slots defined results in nonsense!  print(catalog.getX())  print(catalog.getY())  {code}    prints various nonsense values, like:    {code}  [1.265e-321 2.846e-321]  [1.265e-321 2.846e-321]  {code}    We should probably check the other getters for similar bugs.",1
"DM-17452","01/24/2019 15:27:30","slots are not propagated into MultiMatch output schema","I discovered DM-17451 because the output schema produced by MultiMatch does not include the {{slots}} that were defined on the input catalog, even if you ensure that they are defined on the schema that is passed in to MultiMatch.    I suspect a trivial fix for this is to add {{outSchema.setAliasMap(self.mapper.getInputSchema.getAliasMap())}} to multiMatch.py around line 37.",1
"DM-17454","01/24/2019 16:38:57","Missed visit ID change in bin/runIsr","It seems that {{runPipeline}} isn't really running an ISR processing because it can't find the data:  {noformat}  root WARN: No data found for dataId=OrderedDict([('visit', 270095325)])  root INFO: Running: /software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/ip_isr/16.0-17-g5cf0468+4/bin/runIsr.py testout-10/ts8e2v --rerun hchiang2/tmp --id visit=270095325 -c isr.overscanNumLeadingColumnsToSkip=0 isr.doDark=False isr.doFlat=False isr.doCrosstalk=False  root WARN: Not running the task because there is no data to process; you may preview data using ""--show data""  {noformat}    I believe this is because the visit numbering has changed in DM-17228 and {{visit=270095325}} should now be {{visit=201807241028453}}   ",0.5
"DM-17467","01/25/2019 10:33:54","Add obs_lsst to pipelines.lsst.io sphinx build","Once RFC-569 is adopted and obs_lsst is added to lsst_distrib, the sphinx build in pipelines.lsst.io has to be modified to include obs_lsst.",1
"DM-17481","01/25/2019 15:58:45","Write the Self-Evaluation","Write the self-evaluation in Ultipro for the 2018 performance evaluation.",1
"DM-17482","01/25/2019 15:59:53","Write the Self-Evaluation","Write the self-evaluation in Ultipro for the 2018 performance evaluation.",1
"DM-17483","01/25/2019 16:01:26","Write the Self-Evaluation","Write the self-evaluation in Ultipro for the 2018 performance evaluation.",1
"DM-17484","01/25/2019 16:02:00","Write the Self-Evaluation","Write the self-evaluation in Ultipro for the 2018 performance evaluation.",1
"DM-17485","01/25/2019 16:02:57","Write the Self-Evaluation","Write the self-evaluation in Ultipro for the 2018 performance evaluation.",1
"DM-17486","01/25/2019 16:03:25","Write the Self-Evaluation","Write the self-evaluation in Ultipro for the 2018 performance evaluation.",1
"DM-17487","01/25/2019 16:03:51","Write the Self-Evaluation","Write the self-evaluation in Ultipro for the 2018 performance evaluation.",1
"DM-17489","01/25/2019 17:27:15","Initial prototype of Gen3 interfaces for human-curated master calibrations","DM-16467 added some Python and bash scripts that add brighter-fatter kernel and defect datasets to a Gen3 repo.  These use direct SQL calls instead of the Registry API, and have been broken by changes on DM-16227.  This ticket will move that work into the {{daf.butler.Instrument}} class and its obs_* package specializations, allowing it to be done automatically by the {{gen2convert}} tool (in addition to fixing the breakage, of course).    Coming up with a new, consistent-across-obs-packages way to manage human-curated source for these datasets is definitely out of scope right now, but this might provide an example of how the Butler side of that might look to those who do design that.",2
"DM-17491","01/26/2019 18:40:12","Implement Butler deletion APIs","Implement Registry.dissociate as well as a way to remove all Datasets from one or more Collections.",2
"DM-17493","01/27/2019 10:26:48","Include Filter in Gen3 HSC raw formatter","The Gen3 formatter class for HSC raw data doesn't call {{setFilter}} on the returned {{Exposure}}, which is a problem for downstream processing.  To fix that, we need to:   * add a {{makeFilter}} to {{FitsRawExposureFormatterBase}} in daf_butler, similar to the existing {{makeWcs}} and {{makeVisitInfo}} (including using it in the base class {{readInfoComponent}}.   * implement {{makeFilter}} in obs_subaru appropriately.    We'll probably have to call the {{afw.image.Filter}} constructor with {{force=True}} to try to avoid mucking with the global variables (and hope no one else does); a better solution will have to wait for RFC-541.",1
"DM-17505","01/28/2019 11:51:48","Fix bug in associate implementation","[~salnikov] notes that this bit:    [https://github.com/lsst/daf_butler/blob/master/python/lsst/daf/butler/registries/sqlRegistry.py#L548-L550]    is looping over {{refs}} many times, instead of just the outer loop.     ",0.5
"DM-17508","01/28/2019 13:22:29","Risk review and Management board","Go through risks assign to CAMS and also do review ahead of board ..",2
"DM-17509","01/28/2019 13:23:49","Check Done Epics","Spot check some epics marked done and make sure I can find deliverables.",1
"DM-17510","01/28/2019 13:27:28","Create stories for CAP-9","Work out details of CAP-9",1
"DM-17511","01/28/2019 13:33:37","Update ATMCS simulator based on updated XML and new knowledge","The ATMCS simulator has some issues that need addressing:    * Updated XML, including M3 telemetry  * Motors should be enabled by standby->disabled and vice versa (the current logic is more complicated). That said, we are discussing whether there is some way to only enable a rotator if it is needed, in order to reduce heat load.  * Update limits based on current knowledge  * Output azimuth raw encoder counts in range [0, 360]*resolution and offset encoders by appropriate amounts when there are multiple encoders per axis  * Do not output m3InPosition and probably not m3PortSelected ([~rcantarutti]?) until the setInstrumentPort command is received.  * Enable tests that were accidentally left disabled",2
"DM-17514","01/28/2019 14:11:43","Review of Besalco contract documents","Review of Besalco contract documents...",3
"DM-17515","01/28/2019 14:26:15","Unpack & Install M2 controller in Summit computer room","Unpack and Install M2 controller in computer room; then achieve network connectivity.         This Epic will correspond to the following P6 Activity:   * T&SC-14040401-1010 Unpack & Install M2 controller in computer room",2
"DM-17521","01/28/2019 16:18:18","Add warning when ap_pipe skips association","In the review to DM-14019, [~cmorrison] pointed out that the results of association depend on the order in which exposures are fed into it, and that this can cause problems when rerunning a set of exposures where some are skippable and others are not. Modify {{ap_pipe}} to print a warning (using {{warnings.warn}} to avoid spam) either when skipping association is requested or when it happens.",1
"DM-17522","01/28/2019 16:26:04","Write getting started notes on ip_diffim","Take what you learned on DM-16409 and turn it into a set of notes on the ip_diffim codebase — what are the key options? How is the code structured?    This shouldn't attempt to be comprehensive, user focused documentation: use it to draw out the key themes and options that you think we should schedule time to investigate over the next several months, and record what you wish you'd known when you were getting started a few months ago.",8
"DM-17523","01/28/2019 16:31:57","Add RTM-006 back to raft_serial_data ","[This commit|https://github.com/lsst/obs_lsst/commit/e5d76efe711834f2ebddeea01aeaf623f2611014#diff-db4cf806a9d1fc03bc687b05e5f549f5] and [this commit|https://github.com/lsst/obs_lsst/commit/6470ea684160c57387a3e893c779cd07661f8df4] moved the {{raft_serial_data}} dict from {{python/lsst/obs/lsst/ts8.py}} to {{python/lsst/obs/lsst/translators/ts8.py}}, but somehow omitted {{RTM-006}}.  This led to failures in ingesting  RTM-006 data.      {noformat}  Error calculating property 'detector_num' using translator <class 'lsst.obs.lsst.translators.ts8.LsstTS8Translator'>: 'RTM-006'  ...  Error calculating property 'detector_name' using translator <class 'lsst.obs.lsst.translators.ts8.LsstTS8Translator'>: 'RTM-006'  ...  Error calculating property 'detector_exposure_id' using translator <class 'lsst.obs.lsst.translators.ts8.LsstTS8Translator'>: 'RTM-006'  ...  ingest WARN: Failed to ingest file /project/rgruendl/SLACmirror/SLACgpfs/jh_archive-test/LCA-11021_RTM/LCA-11021_RTM-006-Dev/5867D/qe_raft_acq/v0/38892/S22/E2V-CCD250-217-Dev_lambda_flat_0500_5867D_20180417041357.fits: 'detector'  {noformat}  ",0.5
"DM-17525","01/28/2019 16:33:34","WLS end-to-end testing","Need to develop tests that fully exercise the WLS CSC through SAL, where we send it a message or command through SAL, and then look for an event on SAL as a result. ",2
"DM-17529","01/28/2019 23:44:10","Fix czar message table insertion bug","Qserv czar is missing a length check at message table insertion; a too long error message can cause message insertion failure.  This may result in an empty message table, leading the czar to conclude that the query succeeded and subsequently attempt to access a non-existent result table.",1
"DM-17533","01/29/2019 11:57:10","Make topic destructors more robust","Make all topic destructors more robust in ts_salobj. Some assume attributes are present and they should not do that.    Also improve error output of test_main by using shtuil.which to look for the executable and suggesting scons or setup if not found.",1
"DM-17535","01/29/2019 13:33:54","32 bits no longer suffice for imsim expId","The {{expId}} calculation has changed and now it can be much larger, hence sometimes, with large visit IDs, expBits=32 is no longer enough:    {noformat}  processCcd FATAL: Failed on dataId={'visit': 439196, 'raftName': 'R11', 'detectorName': 'S11', 'run': '439196', 'detector': 40, 'snap': 0}: RuntimeError: expId=4391960040 uses 33 bits > expBits=32  {noformat}      So, I tried increasing {{ccdExposureId_bits}} to 36 bits, then it failed at processCcd characterizeImage detectMeasureAndEstimatePsf  {noformat}    File ""/home/hchiang2/DC2/meas_algorithms/python/lsst/meas/algorithms/reserveSourcesTask.py"", line 132, in select      rng = np.random.RandomState(self.config.seed + expId)    File ""mtrand.pyx"", line 644, in mtrand.RandomState.__init__    File ""mtrand.pyx"", line 680, in mtrand.RandomState.seed  ValueError: Seed must be between 0 and 2**32 - 1  {noformat}    It's okay to truncate the ID sent to {{rng}} because it just wants something deterministic and not-usually-the-same for different units of data.",1
"DM-17543","01/29/2019 15:58:54","Rename lsst.verify.compatibility to gen2compatibility","The {{compatibility}} package was originally proposed in [DMTN-098|https://dmtn-098.lsst.io/] as a subpackage of a {{lsst.verify.measurements}} package that would contain extra infrastructure. However, following RFC-550, this package was instead added to {{lsst.verify}}. Since {{lsst.verify}} is much broader in scope than the proposed {{lsst.verify.measurements}} would have been, the name {{compatibility}} has caused much confusion in the new context (what needs to be compatible with what?).    We can alleviate some of the confusion by renaming {{compatibility}} to {{gen2compatibility}}. This must be done before the {{MetricTask}} framework is widely adopted.",1
"DM-17544","01/29/2019 16:32:51","new refcats in validation_data repos are invalid off lsst-dev","The symlink from {{validation_data_*/data/ref_cats}} that is supposed to go to {{../ref_cats}} instead goes to {{/project/wmwv/validation_data_*/ref_cats}}. This is true for each of the cfht, decam, and hsc repos.    Please make these symlinks relative, so that they are valid for non-lsst-dev checkouts.",0.5
"DM-17545","01/29/2019 18:37:14","Fix MeasureCoaddSources regression ","Converting to pipeline tasks introduced a bug in runDataRef in MeasureCoaddSourcesTask, fix the bug.",1
"DM-17546","01/29/2019 19:35:32","Update script queue for ts_salob 3.8","Update ts_scriptqueue to take advantage of features in ts_salobj 3.8:  * Use data contained in the topics  * Stop using test_utils    Also modernize the documentation layout to match the current template (T&S needs no top layer)",1
"DM-17547","01/30/2019 11:05:28","Integrate obs_lsst camera instructions into sphinx docs","There is useful documentation in README files in obs_lsst that needs to be integrated into the sphinx documentation.",1
"DM-17550","01/30/2019 11:51:44","Update DMTN-093 to describe alert schema versioning","As developed in DM-17549.",1
"DM-17560","01/30/2019 18:43:38","Add force_output argument to ControllerEvent.set_put","Add {{force_output=False}} to {{ControllerEvent.set_put}} so we can assure output of the new event value.    Also skip keys that have a value of None in all {{set_put}} methods, making them easier to use for possibly omitted values.    Also remove some remaining instances of unnecessary use of {{DataType()}}",1
"DM-17561","01/31/2019 09:27:57","Update salgenerator tests to work with new docker infrastructure","This task covers   * Time to research and learn about enabling SSH to Docker containers.   * Update the salgenerator tests to work with the Docker image   ** This may involve updating the tests to remove all SSH functionality.",1
"DM-17562","01/31/2019 11:50:15","Broken links in afw doxygen","When I build {{afw}}, I get the following warnings:  {noformat}  include/lsst/afw/geom/SkyWcs.h:524: warning: unable to resolve reference to `afwCameraGeomPIXELS' for \ref command  include/lsst/afw/geom/SkyWcs.h:525: warning: unable to resolve reference to `afwCameraGeomFIELD_ANGLE' for \ref command  doc/mainpage.dox:14: warning: unable to resolve reference to `afwCameraGeom' for \ref command  {noformat}  I assume these are a side effect of removing some {{.dox}} files in DM-16855. Update these links so that they're valid; for example, the first two could probably be simple API links to the corresponding constants.",1
"DM-17591","01/31/2019 15:35:35","OFC Interface between the Algorithm and Control System","Construct the OFC interface between the algorithm and control system.",1
"DM-17593","01/31/2019 15:37:59","Investigate non-subtask “subtasks” in ip_diffim","Per DM-12558, ip_diffim creates and uses Task objects without going through the regular subtask mechanism. Why? If this is necessary, we should document it and properly account for it on DM-12558. If it's not necessary, we should fix it by using {{makeSubtask}} so we can keep track of the execution time, etc, of these tasks in the regular way.",8
"DM-17596","01/31/2019 17:39:09","Docker Struggle","Fight with dockerfiles in attempt to get a working environment with sal, salobj, AND pymodbus, while nominally learning a bit about how docker is supposed to work",1
"DM-17600","02/01/2019 02:44:42","Improve qserv_deploy in order to fix DNS issue at CC-IN2P3","UPDATE: DNS error was fixed by disabling the cache in CoreDNS configuration.    DNS is crashing using weave    - calico and flannel automated installation does not fix this issue, and, even worst, crash the nodes  - removing DNS cache seems to fix this issue  - an example procedure to cleanup DC2 data has been added",8
"DM-17603","02/01/2019 09:46:51","Devise and implement the build and packaging process","This task covers time to discuss with the TSSW team the particular needs of application building and packaging for deployment.  Most of the pieces already exist, but the process needs to be documented and formalized.",3
"DM-17604","02/01/2019 09:49:03","Prevent fault state invalid transition into fault state","Inside of telemetry method, if a fault value is read in one of the registers, the laser will go into a fault state however, if the fault is not cleared before the next telemetry it will continue to go into a fault state which is not a valid transition.     The solution should check that the CSC is already in a fault state and not go into fault state.",1
"DM-17605","02/01/2019 10:04:23","Implement S matrix normalization in scarlet","To deblend crowded fields it is useful to model stars as a single point convolved with the PSF. Through experimentation [~pmelchior] and I discovered that normalizing the A matrix (colors) causes too much variation in the S matrix (morphology) updates. Instead it appears that normalizing the S matrix (and fixing a single pixel) for point sources and allowing the A matrix to encode both SED and intensity information works better.    This ticket is to give users the option to use either A matrix or S matrix normalization for each source.",1
"DM-17606","02/01/2019 10:11:53","Publish ascii error codes to SAL & serial timeout to fault state","Publish ascii errors as part of error code event in salobj. Handle serial timeout as fault state. Add simple retry to timeout to allow CSC to potentially continuing functioning.",1
"DM-17608","02/01/2019 11:06:24","Fix performance regression from SQLite transaction changes","DM-17495 fixed the deadlocks we saw in SQLite Registries at the expense of adding much more aggressive locking.  That has caused a pretty dramatic slowdown in operations that make a large number of Registry calls without first starting an explicit transaction, at least on GPFS (because this results in many small transactions, and hence many lock/unlock calls in sequence).    A simple workaround for this (which may also be the best we can do long-term for SQLite) is to wrap such calls in an explicit transaction; that needs to be done now for {{gen2convert}} (the only tool we have right now that does not use explicit transactions) to fix a serious regression in ci_hsc performance.",0.5
"DM-17609","02/01/2019 11:41:06","Improve ATPneumatics XML","Improve the ATPneumatics XML as per disussion with [~rcantarutti]    * Ditch the Position events because they are fully duplicated by the limit switch events.  * Add Invalid mirror cover state to handle the case that both an open and a closed switch are active for the same cover.  * Ditch the overlap of the “state” enums with summary state and shorten the names to remove redundancy and stutter, so that the MirrorCoverState enums become:  {code}  <Enumeration>MirrorCovers_Closed,MirrorCovers_Opened,MirrorCovers_InMotionState, MirrorCovers_Invalid</Enumeration>  {code}  and the mirror vents state enum becomes:  {code}  <Enumeration>CellVents_Opened,CellVents_Closed,CellVents_InMotion</Enumeration>  {code}  * Delete the {{resetEStopTriggered}} event, rename the {{eStopTriggered}} event to {{eStopTrigger}} and set the {{triggered}} field appropriately.  * Add two new events for reporting m1 and m2 commanded air pressure (each with a single field), unless folks have a very strong desire to add fields to the existing m1AirPressure and m2AirPressure telemetry instead.  * Flesh out the Description and Units fields (including the units for pressure, which are Pa).",1
"DM-17610","02/01/2019 11:44:34","Improve ATDome XML","Improve ATDome XML as per discussions with [~pingraham] and Paul.    Command changes:   * Add a {{homeAzimuth}} command.   * For the {{moveShutterDropoutDoor}} and {{moveShutterMainDoor}} commands replace the field with a boolean named {{open}}; the TCP/IP interface doesn't support partially opening or closing a door.   * Rename the {{stopAllAxis}} command to {{stopMotion}}.   * Remove the {{stopShutter}} and {{stopAzimuth}} commands; the TCP/IP interface only supports stopping all motion.    Event changes:   * Modify {{settingsAppliedDomeController}} as follows:   ** Rename {{xActivated}} to {{xEnabled}} for all fields because this indicates whether detecting the condition is enabled, not whether the condition is detected.   ** Remove the {{learnManual}} field; the TCP/IP interface does not provide the information.   ** Add a {{homeAzimuth}} field to report the configured position of the home switch; this may be useful in conjunction with the {{homeAzimuth}} command and is output by the TCP/IP interface.  * Add a {{homing}} field to {{azimuthState}}. It could be made part of the {{state}} field but then we would need to treat the enums as bits for a bit mask and the XML doesn't seem to have good support for that.   * Modify {{settingsAppliedDomeTcp}} as follows:   ** Change portRange field to port.   ** Change ip to host and allow to be longer (to enable domain lookup).  ** Remove {{writeTimeout}}; the CSC doesn't use it.   * Remove the following events, as we will not be outputting them:   ** {{internalCommand}}   ** {{loopTimeOutOfRange}}   ** {{rejectedCommand}}   ** {{internalStatus}}   ** {{detailedState}} (and associated enum) because there is nothing to report beyond summaryState.   * Rename the allAxisInPosition event to allAxesInPosition.    Telemetry changes:   * Remove the following telemetry topics, as they are not output:   ** {{loopTime}}   ** {{timestamp}} telemetry topic; it is not output.  * Modify the position topic by renaming the dropout door fields from dropoutOpening... to dropoutDoorOpening... for consistency with the main door.    Enum changes:   * Remove the {{ShutterDoorState_NotInMotionState}} enum value; it cannot be output.   * Remove the {{AzimuthState_InMotionState}} enum value; it cannot be output.   * Remove the {{AzimuthState_StoppingMotionState}} enum value; it cannot be reliably output.   * Remove the redundant trailing {{State}} from {{AzimuthState_}} and {{ShutterDoorState_}} enums. For instance {{AzimuthState_NotInMotionState}} becomes {{AzimuthState_NotInMotion}} and {{ShutterDoorState_ClosedState}} becomes {{ShutterDoorState_Closed}}.  ",2
"DM-17611","02/01/2019 11:55:57","Performance optimizations to data ID code","Profiling on DM-17496 revealed that the slow speed of preflight was not due to the big {{selectDimensions}} query, but rather follow-up data ID manipulations.    Improvements were also done and reviewed DM-17496, but I'm moving them here because they weren't actually relevant for that ticket, and the work that is relevant isn't done yet.",1
"DM-17626","02/01/2019 12:55:44","Enable travis flake8 tests in log","Enable travis flake8 checks in {{log}} so that branch protection can be enabled properly.",0.5
"DM-17638","02/01/2019 13:39:47","Do the Unit Test of OFC Interface and Related Documentation","Do the unit test of OFC interface between the algorithm and control system (MTAOS). This is to make sure the behaviors of OFC interface fulfills the need of control system. This task will do the related documentation contains the class diagram.",2
"DM-17639","02/01/2019 13:48:49","WEP Interface between the Algorithm and Control System","Discuss and define the WEP interface between the algorithm and control system with Chris.",1
"DM-17640","02/01/2019 13:52:53","Do the Unit Test of WEP Interface and Related Documentation","Do the unit test of WEP interface between the algorithm and control system (MTAOS). This is to make sure the behaviors of WEP interface fulfills the need of control system. This task will do the related documentation contains the class diagram.",2
"DM-17641","02/01/2019 13:56:31","Review the SHWFS FAT Document and LSE-150","Review the SHWFS FAT (factory acceptance test) document and presentation by the vendor and feedback the problems to Jacques. This task will also review the document of LSE-150.",1
"DM-17642","02/01/2019 13:58:49","Attend the DAQ Workshop at NCSA remotely","Attend the DAQ workshop at NCSA remotely at 2/12 - 2/13 from 8 am in Tucson time. The confluence page is at: [https://confluence.lsstcorp.org/display/SYSENG/DAQ+2.5+API+Workshop+and+COB+installation+at+NCSA+Test+Stand]     ",2
"DM-17650","02/01/2019 16:21:13","jenkins sierra-[34] build agents down","The osx nodes jenkins uses for the build agents sierra-[34] have been down for a couple of days now.",0.5
"DM-17651","02/01/2019 16:46:22","Forward lsst.log to Python logging","When writing unit tests it is sometimes useful to check that a log message was created. Python provides {{TestCase.assertLogs}} for this purpose but this does not work with {{lsst.log}}. It would be useful to allow unit tests to temporarily forward {{lsst.log}} messages to Python {{logging}} so that they could be tested.    This would have to ensure that DM-15201 does not cause a logging loop.    Ideally C++ log4cxx messages could be forwarded but it seems much easier to focus on Python {{lsst.log}} messages.",2
"DM-17653","02/01/2019 16:59:59","Implement CIRCLE, RANGE, POLYGON in Imgserv SODA","This ticket is to implement the shapes for image cutout, mandated by SODA 1.0,  as already marked ToDo in ImgServ SODA code base. The approach is to use the geometry functions in the priority 1) lsst/afw 2) astropy.",20
"DM-17654","02/01/2019 18:45:08","PPDB Scaling Test in Google Cloud","Perform PPDB scaling test in Google Cloud with other database technologies like postgres and using node configuration not available at NCSA.",8
"DM-17659","02/02/2019 09:01:04","Fix F632 flake8 warnings","The new flake8 version warns about F632 (using {{is}} rather than {{==}}) and this triggered a few flake8 failures over the weekend when the weekly tags were added.    The full list I can find:    {code}  qserv/admin/bin/qserv-parallel-cmd.py:191:12: F632 use ==/!= to compare str, bytes, and int literals  meas_modelfit/python/lsst/meas/modelfit/display/cModelDisplay.py:34:8: F632 use ==/!= to compare str, bytes, and int literals  meas_modelfit/python/lsst/meas/modelfit/display/cModelDisplay.py:36:10: F632 use ==/!= to compare str, bytes, and int literals  daf_persistence/python/lsst/daf/persistence/butler.py:917:12: F632 use ==/!= to compare str, bytes, and int literals  display_firefly/python/lsst/display/firefly/firefly.py:406:12: F632 use ==/!= to compare str, bytes, and int literals  synpipe/bin.src/compFakeGalaxy.py:184:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/bin.src/tractFindVisits.py:25:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/bin/compFakeGalaxy.py:184:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/bin/tractFindVisits.py:25:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/positionGalSimFakes.py:74:12: F632 use ==/!= to compare str, bytes, and int literals  ./synpipe/python/lsst/synpipe/positionGalSimFakes.py:133:20: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/makeFakeGalaxy.py:32:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/makeFakeGalaxy.py:41:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/makeFakeGalaxy.py:52:8: F632 use ==/!= to compare str, bytes, and int literals  synpipe/python/lsst/synpipe/makeFakeGalaxy.py:71:8: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:121:16: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:123:40: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:128:36: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:198:16: F632 use ==/!= to compare str, bytes, and int literals  obs_lsstSim/python/lsst/obs/lsstSim/processCalibLsstSim.py:200:18: F632 use ==/!= to compare str, bytes, and int literals  {code}  ",0.5
"DM-17660","02/04/2019 09:36:46","Get the Ptg Comp vendor-supplied tests to run in LSST CI","This is a continuation of DM-17366.  That task ended up only comprising getting the Ptg Component to build.  This task covers time needed to get the vendor-supplied automated tests running in the TSSW CI environment.",3
"DM-17662","02/04/2019 09:55:46","PyBind 11 ack bug","I have a Data struct that looks like this    *.h*   class AckCmd   \{     public:       SALData_ackcmdC ackData;       AckCmd(int ack, int error, std::string result);   };    *.cpp*    AckCmd::AckCmd(int ack, int error, std::string result)   \{     ackData.ack = ack;     ackData.error = error;     ackData.result = result;   }        I then am binding it as follows        py::class_<AckCmd>(m,""AckCmd"", py::module_local(), py::dynamic_attr())               .def(py::init<int, int, const std::string &>())              ;       I have the `py::module_local()` tag because I was getting an `ImportError: generic_type: type ""*"" is already registered!` error.    I have the `py::dynamic_attr()` tag to set the `ack`, `error` and `result` attributes of the ackData object.        However, when I enter the following within a python shell       >>> import SALPY_Library   >>> myAck = SALPY_Library.AckCmd( 1, 2, ""myack"")   >>> print(myAck.ack)   Traceback (most recent call last):     File ""<stdin>"", line 1, in <module>   AttributeError: 'SALPY_Scheduler.AckCmd' object has no attribute 'ack'   The attributes are not there. When using double tabbing to view the attributes of the object it doesn't show any of the attributes from the c++ struct.    >>> myAck.__   myAck.__class__(          myAck.__doc_             _myAck.__getattribute__(   myAck.__init_subclass__(  myAck.__ne__(             myAck.__repr__(           myAck.__subclasshook__(      myAck.__delattr__(        myAck.__eq__(             myAck.__gt__(             myAck.__le__(             myAck.__new__(            myAck.__setattr__(           myAck.__dict_            _myAck.__format__(         myAck.__hash__(           myAck.__lt__(             myAck.__reduce__(         myAck.__sizeof__(            myAck.__dir__(            myAck.__ge__(             myAck.__init__(           myAck.__module_          _myAck.__reduce_ex__(      myAck.__str__(            ",2
"DM-17663","02/04/2019 09:59:23","Make Registry table names lowercase","Oracle works much better with lowercase field names, and case-sensitivity is in general inconsistent in SQL databases.    We should do this after unifying Dimension names and link names on DM-17023, since that will eliminate the only current use of differently-conventioned names to refer to (essentially) the same concepts, and make a good chunk of the Registry table names lowercase already.",1
"DM-17664","02/04/2019 10:07:10","Weather station (environment) CSC.","Write down environment CSC that will capture data from the weather station and publish it to SAL. ",2
"DM-17666","02/04/2019 10:17:41","Initial version of ATSpectrograph CSC using salobj","Write the ATSpectrograph CSC using salobj. The main goal of this task is to create a CSC that is capable of running in simulation model. Stretch goal is to have it integrated with the hardware. But, since I'm not entirely familiar with the hardware, it may need a follow-up ticket to complete that. ",2
"DM-17667","02/04/2019 10:34:39","Finishing Weather Station SW Development","Finishing Weather Station SW Development",5
"DM-17668","02/04/2019 10:35:58","Testing of Weather Station SW","Testing of Weather Station SW",5
"DM-17670","02/04/2019 11:09:28","Update scarlet notebooks and docs","The scarlet docs currently fail building. There was probably a breaking API change at some point that needs to be fixed, and they also need to be updated with an example of using S matrix normalization to model point sources.",2
"DM-17672","02/04/2019 11:58:10","Make ATAOS listen for pointing component instead of mount.","Instead of listening for the mount position from the ATMount, make ATAOS listen for the pointing component. ",1
"DM-17675","02/04/2019 13:52:40","Resolve flake8 errors in afw","The release of pycodestyle 2.5.0 has caused flake8 to be unhappy with the current {{master}} of afw. Please fix it.    Unhappy flake8/Travis: https://travis-ci.org/lsst/afw/builds/488690857  pycodestyle release notes: https://pypi.org/project/pycodestyle/2.5.0/  Relevant Slack discussion: https://lsstc.slack.com/archives/C2JP8GGVC/p1549313396138300",1
"DM-17676","02/04/2019 14:25:14","Review TMA FAT reports","Review the final versions of the TMA FAT verification test reports",2
"DM-17677","02/04/2019 14:26:53","Deploy SAL runtime RPMs","Build and deploy SAL runtime as a set of RPM's (per CSC)  ",3
"DM-17678","02/04/2019 14:27:48","SAL mentoring","Continue SAL mentoring",1
"DM-17688","02/04/2019 16:50:16","T&S Organization of Documentation on Confluence","This task is to create the subsystem ""Confluence Template"" that the T&S team will use. This template is planned to be used by the team to contain an overview of a single CSC. For example, some of the items on the template will specify who the product owner is, what the release strategy is and where the links to the Jenkins automated test builds are if they exist.     This task will also go over the time spent working with James and getting his CSC into one of these templates so that we can go through a test run with the template as well an example for a future Tech talk when I go over the teams documentation. ",1
"DM-17690","02/04/2019 18:46:46","Add error code argument to BaseCsc.fault","Add an error code argument to {{BaseCsc.fault}} and emit the errorCode event when it is called.",0
"DM-17693","02/05/2019 08:48:32","Create a Docker image for TSSW testing - part 2","Continuation of https://jira.lsstcorp.org/browse/DM-17693    ------------------------------------------------------------------    This task covers   * time to research and learn about Docker.   * create a Docker image configured for testing.   ** miniconda or scl/ius for Python3 support   ** Robot-Framework   ** XML parser   ** dependencies",2
"DM-17720","02/05/2019 10:11:45","Improve user expression handling in pre-flight","User expression is now passed to pre-flight as a plain string (even though that expression is parsed and checked for syntax in advance). This makes it difficult to do smart things with it like deducing table names from link names or converting it to true sqlalchemy construct.    I want to try to pass parsed tree to preflight and build sqlalchemy selection from that instead.",2
"DM-17722","02/05/2019 11:02:49","Upgrade react to 16.8","Upgrade react and related components from 16.2 to 16.8    * don't upgrade redux   * upgrade any other react components.   * only do webpack and babel         Implemented:    * upgraded React.js to version 16.8  * converted AdvancedADQL to use react hooks which is a big feature of this release    The converted file make use of the new useState and useRef hooks, as well as demonstrate how updated props can be compared directly in the render function.  It serves as a test for the upgrade as well as demonstration of how new features are used.    Here is a complete changelog for react:  https://github.com/facebook/react/blob/master/CHANGELOG.md#1680-february-6-2019    This blog highlight some of the changes in v16, up to v16.7.  https://hackernoon.com/react-16-0-16-3-new-features-for-every-day-use-f397da374acf    Summary of the blog:    * Returning multiple elements from components with fragments (16.2)   <React.Fragment>   …   </React.Fragment>   or   <>   …   </>  * Returning strings and numbers from components  * Cancelling setState() to avoid rerendering  * Avoiding prop drilling with the official context API (16.3)  * Updating state based on props with getDerivedStateFromProps() (16.3)  * Re-rendering function components on props change with React.memo() (16.6)  * Easier access to context in class components with contextType (16.6)",8
"DM-17726","02/05/2019 11:41:56","Update flake8 and pycodestyle to support max-doc-length","With the release of new versions of pycodestyle and flake8 that support max-doc-length, we can update our eups packages to allow packages to use max-doc-length in their setup.cfg files.",3
"DM-17739","02/05/2019 15:46:59","Recent update to TaskLoader breaks pickle","[~mgower] reported issues with pickling QuantumGraph which produces diagnostics like this:  {noformat}  Failed to build graph: Can't pickle <class 'lsst.ip.isr.isrTask.IsrTaskConfig'>: it's not the same object as lsst.ip.isr.isrTask.IsrTaskConfig  Traceback (most recent call last):    File ""/home/mgower/gen3work/full_hsc/git/ctrl_mpexec/bin/pipetask"", line 26, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/home/mgower/gen3work/full_hsc/git/ctrl_mpexec/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 127, in parseAndRun      qgraph = self.makeGraph(pipeline, taskFactory, args)    File ""/home/mgower/gen3work/full_hsc/git/ctrl_mpexec/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 359, in makeGraph      pickle.dump(qgraph, pickleFile)  _pickle.PicklingError: Can't pickle <class 'lsst.ip.isr.isrTask.IsrTaskConfig'>: it's not the same object as lsst.ip.isr.isrTask.IsrTaskConfig {noformat}  This seems to be related to last change in module import in TaskLoader.",0.5
"DM-17741","02/05/2019 17:42:27","pytest extra chatty on failed tests due to fonts and matplotlib","The new changes that forward python log messages to lsst.log have resulted in pytest dumping a giant list of DEBUG messages related to {{matplotlib}} and/or {{font_manager}} after failed tests. This can require scrolling up through several pages of useless messages before you get to the actual pytest output.    [~tjenness] suggests either changing the default logging level we use when running pytest, or to configure our logger to ignore debug matplotlib messages.    Portion of an example ""Captured log call"" from a failed fgcm test:    {code}  ------------------------------ Captured log call -------------------------------  font_manager.py           1343 DEBUG    findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=14.0 to DejaVu Sans ('/home/parejkoj/lsst/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000  font_manager.py           1343 DEBUG    findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=16.0 to DejaVu Sans ('/home/parejkoj/lsst/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000  font_manager.py           1343 DEBUG    findfont: Matching :family=STIXGeneral:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXGeneral ('/home/parejkoj/lsst/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf') with score of 0.050000  font_manager.py           1343 DEBUG    findfont: Matching :family=STIXGeneral:style=italic:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXGeneral ('/home/parejkoj/lsst/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf') with score of 0.050000  ...  and more  ...  {code}",1
"DM-17747","02/06/2019 08:37:22","Russell Pybind 11 hack session","I removed a bug that I introduced to develop. It prevented scons from being able to run. I plan on meeting for a day with [~rowen] to be able to hack out this feature. ",1
"DM-17752","02/06/2019 10:45:31","No image display in Firefly JLab extension after kernel restart","For a notebook using image display with {{jupyter_firefly_extensions}}, image display does not work after the Python notebook kernel is restarted and the notebook is re-executed. When the Jupyterlab browser tab is reloaded, the notebook executes fine again.    To reproduce, execute {{slate-widget-demo.ipynb}} that is one of the examples for the extension. Run all the cells once; then restart the Python kernel for that notebook; then re-execute. Image display does not work the second time around.    I have checked that {{channel}} and {{render_tree_id}} do not change.    For users, the workaround is to restart the Python kernel and to reload the browser tab, when wishing to re-execute a Firefly notebook.",3
"DM-17753","02/06/2019 10:53:33","Update Firefly notebook for jupyter_firefly_extensions","The LSP now includes the Firefly extension for Jupyterlab. This extension has been integrated with the Firefly backend to {{afwDisplay}} and simplifies the startup of the Firefly viewer. This ticket is for updates to the Firefly notebook curated by SQuaRE that take into account the new capabilities provided by the extension. Some cleanup of Markdown will also be proposed.",2
"DM-17758","02/06/2019 11:47:33","Update supported browser list and Webpack","Update supported browser list. We usually do this every 12 to 18 months.    Typically we try to support the current browser version plus 2 back or any browser version in the last year.    Update support browser list:    * Safari 9 => 10  (current: 12)   * Chrome 62 => 67  (current: 71)   * Firefly 56 => 60 (current: 65)   * Edge  14 => 16 (current: 18)    Upgrading the supported list provides the following:   * Allows for more confidence in the testing of our products.   * Allows for more stable version of library since they are limited in testing of old browsers   * Give opportunity to use newer HTML/CSS features such as Grid layout   * Requires less polyfills and code transforms. This allows more native code and smaller load sizes.",3
"DM-17769","02/06/2019 14:50:50","Restore Explanation fields to the original urls in ATDome XML","In DM-17610 I messed up by putting text in the Explanation fields for commands. It has to be a URL. Undo that change (while leaving the help in as XML comments for now so they can be used for Description fields if and when DM-17768 is implemented)",0
"DM-17782","02/07/2019 10:45:54","Initial M2 Software setup @ Summit","Initial M2 Software setup @ Summit",2
"DM-17794","02/07/2019 13:13:50","Modify exposure_id calculations in obs_lsst","Following a discussion on community we have decided to adjust the calculation of detector_exposure_id and exposure_id to remove additional 0-padding between exposure ID and detector (3 digits for lsstCam rather than 4) and to have a maximum sequence number of 99,999 rather than 999,999. It would be good if we could get this done before v17.0.",1
"DM-17795","02/07/2019 13:30:56","Work more on LSO-011","Need to work some more on the release scenarios and talk to some people about things like alert possibilities ..",2
"DM-17810","02/07/2019 14:59:52","Review TMA FAT documents","This task is for reviewing the TMA FAT documents.  Andrew Serio is leading this effort and tasked me to review a couple of documents, so this task tracks the time necessary to do so.",1
"DM-17811","02/07/2019 15:24:04","White Light Docker packaging","learn about how to package the CSC with docker, according to the grand LSE-150 plan. This has been ongoing for a while but will be a success when I have a docker container that runs the component and has all the required dependencies.",3
"DM-17812","02/07/2019 16:20:27","Worker database management in the Replication system","*Objective*:   Extend the Replication system to support operations on the Qserv worker databases via worker's MariaDB service. The operations shall be broadcasted either to all known workers or a subset of those. Build the programmatic API and the command-line tool for that.    Further information on a scope of the project can be found at:  * [https://confluence.lsstcorp.org/display/DM/Worker+database+management]    ",8
"DM-17813","02/07/2019 16:25:35","Review laser tracker protocol interface docs - part 2","Review laser tracker protocol interface docs, update text and diagrams to include new terminology (Laser Tracker CSC instead of Laser Tracker Controller, remove references to labview, etc). Our vendor, New River Kinematics, is proposing an API for communication between our CSC and their part of the code. Review this API and prepare feedback for meeting next week",2
"DM-17816","02/07/2019 16:44:25","Add unit test framework to OpenOrb","Add unit test framework to [OpenOrb|https://github.com/oorb/oorb] (our adopted orbit integration engine).    This includes:    * A make target so running 'make test' will execute all unit and integration tests   * A few example unit tests (for versioning functionality)   * Running the python/test.py integration test    Unit test framework will use pytest. The idea is that all code (including command line utilities) will be tested via pytest drivers (to maintain uniformity).    The work will be on [https://github.com/mjuric/oorb/tree/build-system-rework,] and will be PR-ed upstream upon completion.",3
"DM-17817","02/07/2019 16:53:46","Set up initial CI for OpenOrb","Set up continuous integration for OpenOrb (our adopted orbit integration engine).    This includes:    * Building on Mac and Linux with Anaconda compilers with Python 2.7, 3.6, 3.7   * Building on Linux (Ubuntu 16.04) with Ubuntu-supplied compilers and Python 2.7 and 3.5   * Builds are to be triggered by pushes/PRs to GitHub repository    We will be using the Azure Pipelines service, as we did with pytrax (DM-17027).      The work will be on https://github.com/mjuric/oorb/tree/ci-and-codecov, and will be PR-ed upstream upon completion. It depends on DM-17814 being merged.",3
"DM-17818","02/07/2019 16:57:52","Set up initial test coverage monitoring for OpenOrb","Set up test coverage monitoring for OpenOrb (our adopted orbit integration engine).    This includes:    * Having a make target to run the test coverage tools as 'make coverage'   * Have this target automatically invoked by one of the CI runs   * Have the results uploaded to a cloud test coverage visualization service.    We will be using [codecov.io|http://codecov.io] as the service for visualization of coverage reports.    The work will be on [https://github.com/mjuric/oorb/tree/ci-and-codecov], and will be PR-ed upstream upon completion. It depends on DM-17814 being merged.",3
"DM-17820","02/08/2019 07:53:36","Update ATPneumatics simulator for changes to the XML","Update ts_ATPneumaticsSimulator for DM-17609",1
"DM-17824","02/08/2019 11:52:27","Update LPM-251","Update LPM-251 to include Open Data Framework and rework intro and title.",2
"DM-17827","02/08/2019 14:20:09","Update doc/ directories of packages for latest standards","These packages have doc/ directories that don't fit the current standards (mostly because they include package documention when they really only should have module documentation). This ticket is to fix these packages:   * afw   * base   * coadd_utils   * daf_butler   * display_ds9   * ip_diffim   * ip_isr   * jointcal   * log   * meas_algorithms   * meas_deblender   * meas_extensions_photometryKron   * meas_extensions_shapeHSM   * meas_extensions_simpleShape   * meas_modelfit   * obs_base   * obs_cfht   * obs_lsstSim   * obs_test   * pex_exceptions   * pipe_base   * pipe_drivers   * shapelet   * utils   * validate_drp   * verify    Also, there's a typo in the title of the {{lsst.meas.extensions.simpleShape}} module docs.",1
"DM-17831","02/09/2019 08:40:21","Convert sconsUtils to sphinx documentation","Currently sconsUtils requires that doxygen strings be used to create the documentation. This is done because sconsUtils requires that SCons be imported and that is only possible if SCons is importing sconsUtils. Since pydoc and sphinx import python code to extract documentation this makes it hard to use numpydoc.    For example:  {code}  $ pydoc lsst.sconsUtils  problem in lsst.sconsUtils - ImportError: lsst.sconsUtils cannot be imported outside an scons script.  {code}    It seems though that this is easy to work around. The following patch allows {{pydoc}} to work and opens the possibility of migrating sconsUtils away from doxygen.    {code:diff}  diff --git a/python/lsst/sconsUtils/__init__.py b/python/lsst/sconsUtils/__init__.py  index ee9f02a..6e3c456 100644  --- a/python/lsst/sconsUtils/__init__.py  +++ b/python/lsst/sconsUtils/__init__.py  @@ -1,4 +1,13 @@   # Explain what happens when you try to import outside scons  +# When building documentation you want to force a SCons import  +import os  +import sys  +  +if (""pydoc"" in sys.modules or ""sphinx"" in sys.modules) and ""SCONS_DIR"" in os.environ:  +    scons_path = os.path.join(os.environ[""SCONS_DIR""], ""lib"", ""scons"")  +    if scons_path not in sys.path:  +        sys.path.append(scons_path)  +   try:       import SCons.Script   except ImportError:  {code}  ",2
"DM-17835","02/11/2019 08:24:19","Improve qserv_deploy configuration","- Make qserv_deploy configuration clearer:    * Mount cfg/tmp path correctly:  {code}  fjammes@[qserv-deploy]:~ # ls -rtl /etc/qserv-deploy/  total 24  drwxrwxr-x 3 fjammes fjammes 4096 Jan 25 15:17 gcloud  drwxr-xr-x 4 fjammes fjammes 4096 Feb  1 11:41 tmp  -rw-rw-r-- 1 fjammes fjammes 1019 Feb 12 11:35 env.sh  -rw-rw-r-- 1 fjammes fjammes  605 Feb 12 11:35 env-infrastructure.sh  -rw-rw-r-- 1 fjammes fjammes  679 Feb 12 11:38 env-gke.sh  drwxrwxr-x 4 fjammes fjammes 4096 Feb 12 11:50 dot-kube  {code}  - Allow to easily manage several configuration.  - Record which qserv_deploy version was used to install a given cluster  - Add pseudo /etc/password for NFS/LDAP setup?  - Test it to create a GKE cluster  - Test it on CC-IN2P3 cluster  - Update CI",8
"DM-17837","02/11/2019 09:47:05","Fix incorrect detector serial in TS8 RTM-004","[~plazas] noticed that ingestion of TS8 RTM-004 was failing. It seems I mistyped 285 for 385 in the detector serial of S20. Fix the typo and the associated bug that cascaded in the error handling from this.",0.5
"DM-17840","02/11/2019 10:16:04","Create a Sphinx documentation site for astro_metadata_translator","This ticket is to deploy [https://astro-metadata-translator.lsst.io|https://astro-metadata-translator.lsst.io/] to LSST the Docs. Since it's a standalone Python package, the deployment will be done from Travis CI (following the pattern of SQuaRE's Python packages).    As well, we'll add {{astro_metadata_translator}} to the intersphinx configuration of pipelines.lsst.io so that links will be automatically resolved.",1
"DM-17845","02/11/2019 19:30:32","Fix warnings in image coaddition","Running {{AssembleCoaddTask}} with default settings produces multiple warnings: {{assembleCoadd WARN: Unable to remove mask plane NOT_DEBLENDED: Invalid mask plane name: NOT_DEBLENDED}}    These warnings are caused by the default value for the config option {{removeMaskPlanes}} including {{NOT_DEBLENDED}}, and would be fixed by removing it from the default.",0.5
"DM-17847","02/12/2019 08:12:53","Update defaults in scarlet and create changelog","[~pmelchior] and I have found that some of the default parameters in scarlet need to be modified to more useful values.     1. Normalizing the morphology (S matrix) so that the peak value is set to unity is the most effective way to break the SED/morphology degeneracy, as opposed to the current method of normalizing the SED (A matrix). This provides a method to separate sources with similar colors but very different intensities and also converges more quickly than the current default normalization.    2. Turning off Nesterov acceleration. There appears to be an issue with combining our proximal gradient method with Nesterov acceleration, making the optimization algorithm asymptotic. It also appears to be even more unstable with the new normalization scheme, however using S matrix normalization _without_ acceleration still runs faster than A matrix normalization _with_ acceleration.    This ticket will also create a change log in scarlet to keep track of these and other modifications to keep users up to date with the changes. It will also make it easier to keep stack users up to date when updated versions of scarlet are imported into the stack.",1
"DM-17851","02/12/2019 10:40:35","Finishing ATSpectrograph SW & Algorithm","Finishing ATSpectrograph SW & Algorithm",5
"DM-17855","02/12/2019 11:36:17","Draft LSE document describing the adopted proposal","The result of this RFC will be a proposal to the SE subsystem.  It will be put forward to the CCB and if approved, become a change controlled LSE document.  At that point, we can also add a requirement to the OSS that refers to that LSE document, if desired.",3
"DM-17856","02/12/2019 12:17:54","Support M1M3 Mirror Lab testing - part 3","This task covers the time spent to support M1M3 Mirror Lab testing.",1
"DM-17857","02/12/2019 12:20:16","HTTP 403 status from JupyterLab reported poorly","When accessing a ""shareable"" URL under {{https://lsst-lsp-stable.ncsa.illinois.edu/nb/user/$USER/lab/tree/notebooks/}} to a notebook for which the accessing user does not have permission, JupyterLab reports:  {quote}Cannot find template: ""403.html""   In ""/usr/local/share/jupyter/lab/static""  {quote}  This is undesirable, compared to the result when accessing a ""download"" URL under {{https://lsst-lsp-stable.ncsa.illinois.edu/nb/user/$USER/files/notebooks/}}, which reports:  {quote}403: Forbidden   The error was:   user ktlim was not allowed.  {quote}",0.5
"DM-17858","02/12/2019 12:22:24","Develop and Support the Active Optics Closed-Loop Simulation in Phase 2","Develop and support the active optics closed-loop simulation. This task will integrate the modules of ts_tcs_wep_phosim, ts_tcs_wep, and ts_tcs_ofcPython. This task will be a prototype of active optics closed-loop simulation. The prototype here will support the simulation of main telescope active optics system (MTAOS) in the final. This task is in the phase 2 of the development.",1
"DM-17859","02/12/2019 12:42:42","jupyter_firefly_extensions server extension is failing in LSP","The server extension for the Jupyterlab Firefly extension has not been working in the science platform since the {{d_2019_01_30}} daily. This manifests as a failure to connect to the Firefly server. The environment variables set by the server side of the extension, like {{fireflyLabExtension}}, {{fireflyURLLab}}, and {{fireflyChannelLab}} are undefined.    The quickest way to test the connection in Jupyterlab is to open the Command Palette, find the Firefly section and then Open Firefly. The tab that opens should show a large ""Firefly Ready"" with the Firefly toolbars. When the server extension does not work, the tab is all white with a small ""Firefly Loading"" message at the top.    The build environment changed to use Python and pip, from SCL (Software Collections) to EPEL (Extra Packages for Enterprise Linux).    The initial fix we will make in this ticket is to remove a {{pip install}} from the packages.json file, and to change the version to 0.2.2. ",2
"DM-17861","02/12/2019 14:17:21","Update developer guide instructions for managing change-controlled documents","The current instructions for building change controlled documents are a bit convoluted in that they require the use of cherry-pick and leave unmerged branches hanging around.  This works against the idea of using protected master branches and results in confusing usage of pull requests.    At the recent CCB meeting it was decided to change approach and use a new process involving {{git revert}} and merging of pull requests to master once the document has been released.",2
"DM-17866","02/12/2019 15:11:43","Support BOT data gen 2 ingest once data exists","Following conversations around Feb. 8th 2019 in #dm-bootcamps (for some reason) it is noted that `obs_lsst` will need to be extended/enhanced to support BOT* data when it becomes available.    How this looks will depend on exactly when that happens, and whether version-able cameras are a reality by then, as this data will necessitate that.    *Bench for Optical Test - the IR2 test station where real data from the real camera will be taken as it is populated with rafts",3
"DM-17871","02/12/2019 18:41:28","Add DcrAssembleCoaddTask to docs","{{DcrAssembleCoaddTask}} is missing from [https://pipelines.lsst.io/v/daily/modules/lsst.pipe.tasks/index.html |https://pipelines.lsst.io/v/daily/modules/lsst.pipe.tasks/index.html,]and should be added.",0.5
"DM-17874","02/13/2019 09:03:40","Remove hierarchical logging context","Implementing RFC-570. Not sure what epic this belongs to.",2
"DM-17876","02/13/2019 10:15:44","Update ATDomeSimulator for changes to ATDome","The recent changes to the ATDome interface DM-17610 require an update to ATDome simulator in the ts_ATDomeTrajectory package.    While I'm in there I'll update the code to take advantage of cached data in ts_salobj 3.8 topics.",1
"DM-17881","02/13/2019 16:35:17","Check weekly for regressions","Make sure the weekly under consideration for the next release candidate does not have significant regressions from the previous major release.",2
"DM-17903","02/14/2019 08:57:24","imsim ci_lsst tests fail with bad key ccd","When running the ci_lsst imsim tests they fail in the calibration construction phase with a bad key:  {code}  root INFO: Running: .../stack/DarwinX86/pipe_drivers/16.0-10-g0ee56ad/bin/constructBias.py ./tmp/imsim --rerun timj/tmp --id visit=3010000..3010009:3 -j 4  LsstCamMapper WARN: Unable to find valid calib root directory  CameraMapper INFO: Loading exposure registry from /Users/timj/work/lsstsw3/build/obs_lsst_new/tmp/imsim/registry.sqlite3  LsstCamMapper WARN: Unable to find valid calib root directory  bias FATAL: Failed: 'ccd'  Traceback (most recent call last):    File "".../stack/DarwinX86/pipe_drivers/16.0-10-g0ee56ad/python/lsst/pipe/drivers/constructCalibs.py"", line 365, in __call__      result = task.runDataRef(**args)    File "".../stack/DarwinX86/pipe_drivers/16.0-10-g0ee56ad/python/lsst/pipe/drivers/constructCalibs.py"", line 441, in runDataRef      expRefList, level=""sensor"", ccdKeys=self.config.ccdKeys)    File "".../stack/DarwinX86/pipe_drivers/16.0-10-g0ee56ad/python/lsst/pipe/drivers/constructCalibs.py"", line 230, in getCcdIdListFromExposures      name = dictToTuple(ccdId, ccdKeys)    File "".../stack/DarwinX86/pipe_drivers/16.0-10-g0ee56ad/python/lsst/pipe/drivers/constructCalibs.py"", line 196, in dictToTuple      return tuple(dict_[k] for k in keys)    File "".../stack/DarwinX86/pipe_drivers/16.0-10-g0ee56ad/python/lsst/pipe/drivers/constructCalibs.py"", line 196, in <genexpr>      return tuple(dict_[k] for k in keys)    File "".../miniconda/envs/lsst-scipipe/lib/python3.7/collections/__init__.py"", line 1025, in __getitem__      raise KeyError(key)  KeyError: 'ccd'  Failed to process imsim   {code}    It is likely a problem with the configurations for imsim in that the default keys are being used rather than the ccdKeys override.",1
"DM-17909","02/14/2019 10:45:44","Write ATPtg/ATMCS integration test script","Write an integration test for ATPtg to ATMCS as a SAL script and add it to ts_standardscripts.    There may be some overhead because this is the first script added to that package.",3
"DM-17912","02/14/2019 12:07:51","gen3 ip_isr can attempt to remove a non-existant dataset, causing KeyError","If a config option is disabled, but a gen3 inputTypeDict entry doesn't exist, attempting to cull that entry to match the config can fail with a KeyError.",1
"DM-17913","02/14/2019 13:46:18","Add a function to ts_salobj that enables a CSC from any state","Add a function to {{ts_salobj}} that, given a {{Remote}} for a CSC will send standard state transition commands to put that CSC into the {{ENABLED}} state, regard of its current state. This function is essential for SAL scripts, but is sufficiently general purpose that it belongs in {{ts_salobj}}. Unfortunately it presently relies on reliably getting the summary state, which may not be possible until DM-18035 is fixed. It may be possible to hack around this but I'd like to see if we can get that ticket fixed before getting too ugly.    Also add support for {{BaseCsc.fault}} for outputting the {{errorCode}} event.    Also add a bin script to purge SALPY topics.    Simplify log handling so that log messages are sent as soon as they are requested. Eliminating the queue makes it much easier to assure that the last log messages are seen as a script or CSC is shut down.",1
"DM-17914","02/14/2019 14:42:13","afw PhotoCalibTestCase failure using bleed environment","Using a bleed version of the conda environment, afw test case PhotoCalibTestCase fails with the following error:         > self.assertEqual(photoCalib.getCalibrationErr(), calibrationErr)   E AssertionError: 161742302.08062094 != 161742302.08062097     ",1
"DM-17929","02/15/2019 09:17:51","Update Illumination system coordination script and move to standard scripts","Take Tiago's script that coordinates the monochromator, electrometer and fiber spectrograph to perform the steps needed when taking a flat field.    This script will serve as an example to future script composers.",2
"DM-17930","02/15/2019 09:25:06","Support M1M3 Mirror Lab testing - part 4","This task covers the time spent to support M1M3 Mirror Lab testing.",1
"DM-17932","02/15/2019 09:55:59","Validate object type when reading pipeline or graph from pickle","cmdLineFwk can read pre-built pipeline and quantum graph from pickle file, but it is not presently checking that object of correct type is read which can lead to exceptions in other places. Need to add simple check of the type of the object that was read.",1
"DM-17942","02/15/2019 13:57:25","TunableLaser fix Jenkinsfile","In order to get Jenkins to pass the unit tests, the Jenkinsfile needs to be changed because of some environment factors.     Started writing a dockerfile that contains all dependencies needed to run TunableLaser CSC as docker container. Have gotten through most of the setup with eups. Once the file is complete, upload to dockerhub and pull image in Jenkinsfile. ",2
"DM-17951","02/17/2019 09:01:56","Integrate the AOS Closed-Loop Simulation with Jenkins Server","Integrate the AOS closed-loop simulation with Jenkins server. I have the problem to let the Jenkins to get the repos existed in the docker image. This task will solve this problem and make sure the simulator can do the automatic unit tests on the Jenkins server.",2
"DM-17952","02/17/2019 09:04:18","Develop and Support the Active Optics Closed-Loop Simulation in Phase 3","Develop and support the active optics closed-loop simulation. This task will integrate the modules of ts_tcs_wep_phosim, ts_tcs_wep, and ts_tcs_ofcPython. This task will be a prototype of active optics closed-loop simulation. The prototype here will support the simulation of main telescope active optics system (MTAOS) in the final. This task is in the phase 3 of the development.",5
"DM-17956","02/18/2019 00:43:53","Add option to output chi2 files at each outer fit iteration","To get more information about the details of jointcal's fitting processes, we can output the ""chi2 contributions"" file at each stage of the {{_iterate_fit()}} ""outer"" python loop. This will provide some more details, without being too overwhelming. This would be a boolean config option (named {{writeChi2ContributionFilesOuter}}?) that defaults to False. The output files would follow the same naming convention as the initial/final ones, with the fit iteration appended.    This would also be a good time to clean up how {{saveChi2Contributions()}} generates the names of the files: we should pass some kind of format string to it and use a combination of {{find}} and {{replace}} in C++ to generate the two file names for the measured and reference stars. {{_iterate_fit()}} should also output a count for each iteration, so that we can more easily associate these new files with the outer fit loop log messages (which will have that count in their filename).    We need to decide whether to write the files before or after the fit loop: the current ""initial_chi2"" file is written immediately after the model is constructed, before any of the ""initialization steps"" are taken, while the ""final"" step happens after all fitting is complete.    Should this option also result in the ""initialization steps"" also being logged, or should the existing {{writeChi2ContributionFiles}} be modified to do that? The 3-4 initialization steps are by far the largest chi2 change, and have outlier rejection turned off, so they are both qualitatively and quantitatively different from the fit loop.",2
"DM-17964","02/18/2019 17:05:25","Draft external code policy","Draft a policy for including external code into data release production processing    This will be added to DMTN-106 to start a discussion at least.    Then perhaps something will go in LDM-294 or another controlled document.",2
"DM-17967","02/19/2019 09:10:37","Add target event to ATMCS","Add a new event to ATMCS that provides the commanded target, so users don't have to ""tap"" commands to see what the MCS is trying to do.    Update ATMCS simulator accordingly.    This will make the integration test between ATPtg and ATMCS much easier, so I am increasing its priority.",1
"DM-17969","02/19/2019 09:11:43","Add tests to Jenkins","Get the newly created single file cpp scripts running on jenkins. This should improve speed at which the tests are ran. This because behind the scenes when your program does any interaction with DDS there is a lot of overhead. By having all the tests in a single program this overhead is started only once.    Hopefully we reduce the jenkins build time by using these test programs rather than running each individual program. ",2
"DM-17970","02/19/2019 09:12:53","Install Labview","Install Labvier on my machine. and try to get the ts_sal Labview program running. I will be needing to run this for development. ",1
"DM-17973","02/19/2019 11:47:55","Please document all Firefly-related URLs that are currently used in LSST","Please document, and note in this ticket where it's documented, all URLs currently in use in any Firefly application that is in use or planned to be in use in LSST (default data portal, slate, time series viewer, etc.).    Before our closeout I want to be sure that we have a coherently laid out scheme for these.",1
"DM-17974","02/19/2019 11:52:28","Adjust SAL hashcode for types support","Change the implementation of SAL hash codes to include control over topic names as well as DDS types. This should avoid namespace collisions when CSC XML is updated",2
"DM-17975","02/19/2019 11:55:32","Backup m1m3 test campaign EFD data","Backup all the EFD data collected during the M1M3 test campaigns at the mirror lab.    Retrieve the EDF machine and integrate back into the simulation cluster.    Copy data to NCSA if a destination location is provided",2
"DM-17976","02/19/2019 11:57:38","SAL mentoring","Continue SAL++ mentoring",1
"DM-17977","02/19/2019 12:49:16","Improve handling of failed configuration by the script queue","The script queue has some bugs and quirks in handling failed configuration. The most important is that if configuring a script fails then the script can end up as the current script. Do the following:  - Add a unit test  - Change the {{ScriptInfo.configured}} property to be True only if configuration succeeds and add property {{ScriptInfo.configure_failed}}  - Make sure {{ScriptInfo.process_state == ConfigureFailed}} if configuration fails. At present it is terminated before this has a chance to occur.    Also make the {{stopScripts}} command more reliable about removing scripts; if a script has managed to stay on the queue despite being done then remove it.",1
"DM-17978","02/19/2019 13:42:17","Visit id not being cast as int","If the header card being used to generate {{visit_id}} is a string, it can end up as a {{str}} in the {{ObservationInfo}}. [^test_hdr.txt] ",0.5
"DM-17984","02/19/2019 15:38:49","Rewrite request_script script.","request_script.py is a utility script to interact with the queue. It was written some time ago with an old version of sal and when I did not have a clear idea where to put it. I will re-write the script using the newest version of SAL and will put in in ts_scriptqueue.    =============================================    The ui module contains two submodules; a model interface and a command line front end. The model hosts a global ui for the script queue and the front end provides the interface to interact with it (both input and output).    For the front end I'm using a the python cmd standard library, with the possibility to extend it to use cmd2. This gives user some nice capabilities like auto complete and helper. functions. One of the main issues with that library is that it is not asyncio friendly, so I had to do some workaround and rely on SAL to store the information until the user request it. The idea of having the process running all the time and rely on cmd is to recycle the remote to talk to the queue. The previous version was really slow since you had to subscribe to the remote every time you run the script.    I think this script will probably be a useful utility at early stages of interacting with the queue. I did not included tests for the ui modules. I think this could be a good addition for a next release, but I could also think about some simple tests to perform.    Also, updates timeouts for the script queue.     ",2
"DM-17985","02/19/2019 15:41:10","Rewrite monochromator CSC using SalObj","The current monochromator CSC, currently written in LabView, has an issue that we are unable to solve. I will re-write it using salobj. ",2
"DM-18007","02/21/2019 10:37:12","Install software and services on PPDB gcloud machines","We have two beefy ""machines"" in Google cloud for our tests (ppdb-test-server and ppdb-test-client), need to install all software that is necessary for the tests:   * for client I just need lsst stack with dax_ppdb in it, plus the database backend driver for potgres (I think I used psycopg2 at in2p3)   * for server we need more or less recent version (10?) of Postgres installed from RPMs, properly configured",2
"DM-18010","02/21/2019 11:20:45","Finishing ATMonochromator SW Development","Port ATMonochromator from LabView to Python.",5
"DM-18012","02/21/2019 12:05:49","Add showSchema command to the script queue. ","Each script available to the queue have different sets of configuration parameters. To know them, we currently have to look into the code… It would be very helpful to be able to “ask the queue” what are the parameters of a specific script are. Ideally, when the queue receives such a command it would not put the script to run on the queue, but rather, read the configuration parameters from it and publish it through an event.     I think that it would suffice to get the docstring from the {{configure}} method and output that to SAL.       {code:python}  In [1]: from lsst.ts.scriptqueue.test_utils import TestScript  Could not import lsstcppimport; please ensure the base package has been built (not just setup).      In [2]: print(TestScript.configure.__doc__)  Configure the script.            Parameters          ----------          wait_time : `float`              Time to wait, in seconds          fail_run : `bool`              If True then raise an exception in `run` after the ""start""              checkpoint but before waiting.          fail_cleanup : `bool`              If True then raise an exception in `cleanup`.            Raises          ------          `salobj.ExpectedError`              If ``wait_time < 0``. This can be used to make config fail.  {code}    Although I'm not sure how that would work for non-python scripts.  ",2
"DM-18019","02/21/2019 13:19:56","Create note on security of images ","Steve Kahn asked for a ""paragraph"" on image security by next week.",2
"DM-18024","02/21/2019 15:53:12","State transitions for white light source CSC","WLS CSC needs to implement state transitions:    -upon receiving relevant SAL commands    -upon receiving an error signal from the KiloArc hardware    -in the event of communication loss between host computer and ADAM hardware",3
"DM-18026","02/21/2019 15:58:57","Investigate pymodbus/ADAM channel issue","The white light source needs to continuously read four analog channels on the ADAM 6024 device, which the KiloArc device uses to signal errors and some additional information about the hardware state. My test hardware can only simulate a signal on one channel at a time, and I discovered that my assumptions about how I would read the other channels are apparently wrong. It's possible that this component could work with only one channel (the error signal) but I'd like to figure out what's amiss.",2
"DM-18028","02/21/2019 17:13:44","Allow BaseScript.main(descr=None)","At present {{BaseScript.main}} requires the descr argument, but most scripts will not accept descr as a constructor argument. Modify {{BaseScript.main}} to use None as the default for {{descr}} and only pass the argument if the value is not None.",0
"DM-18031","02/22/2019 12:35:42","Add CADC TAP service to default list on Firefly TAP interface prototype","03/12/2019 CADC TAP service was added to the list of Firefly TAP services.    [http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/tap/] is often unresponsive, when called from IPAC. CADC #75724 was created for this issue with the following response from Patrick Dowler at CADC:       {quote}  We have been able to reproduce this and it seems to be a strange network issue... could be some network security equipment near our end. We did also discover that https seems to resolve it and we have recently converted all our web services to use https only (in the capabilities docs - not yet propagated in IVOA registries)    I don't think we will be able to solve this for http but https appears to work so I will close the ticket. Thanks again for bringing it to our attention - I found evidence in logs that it also effected other services as well.{quote}       Firefly is now using     [https://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/tap] end point.         02/22/2019    Please add the CADC TAP service at [http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/tap/] as soon as reasonably possible to the standard list of TAP services in the Firefly TAP search screen. This is a very useful service for testing, both because of content and because it's based on the same code that [~cbanek] is using for the LSST TAP service.    This service has both ObsTAP (ObsCore format, table {{ivoa.ObsCore}}) and CAOM2 image metadata collections, which are a valuable resource for our development.    At this moment, something goes wrong when trying to manually access this service. The message  {quote}Failed to get schemas for [http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/tap/:] Error: Error from Server for command tableSearch: code: 504, text:  {quote}  appears. However, I can verify manually and/or with TOPCAT or [http://saada.unistra.fr/taphandle?url=http%3A//www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/tap/] that the {{TAP_SCHEMA.schemas}} table is available.",2
"DM-18032","02/22/2019 13:18:11","Update descriptions of DM L2 milestones","Review LDM-503, and ensure that each L2 milestone has a coherent description of what it involves.    For most of them, this will likely involve checking in with the relevant milestone ”owners” — see the list of action items from the [Nov 2018 DMLT|https://confluence.lsstcorp.org/display/DM/DM+Leadership+Team+Face-to-Face+Meeting%2C+2018-11-06+to+08].",2
"DM-18033","02/22/2019 13:56:23","ts_scriptqueue unit tests failing on some machines","One or the other of the npytest_x unit tests is not reliable on some machines. In particular nopytest_script_queue.py sometimes (?) fails for [~tribeiro] and test_stop_scripts in nopytest_queue_model.py usually times out on my work iMac (but runs reliably on my identical home iMac).    Unfortunately for the test failures I see, they go away if I only run the failing test so each test is clearly not running completely independently. Two things I would like to try:  - Measure the time it takes to start each script and the time it takes to configure it and report the max time, standard deviation and mean time for each of these when running all tests.  - For nopytest_queue_model.py use a unique SAL index for each script, in case that reduces interference between tests (though I don't see why it should).  - Put a pause at the start of the failing test to see if giving SAL a chance to clean things up helps.",1
"DM-18036","02/22/2019 21:23:59","Convert stack demo refcat to HTM indexed","{{lsst_dm_stack_demo}} uses an astrometry.net reference catalog. In order to remove a.net support from the stack, we need to convert this refcat from a.net format to HTM Indexed format. [~krughoff] claimed he had some code to do such conversions in the past.    Converting the existing refcat is probably better than creating a new refcat because any changes to the refcat contents would be reflected in changes in the demo output, and we don't want to have to go and vet those changes again.",3
"DM-18039","02/25/2019 03:51:32","Troubleshoot data loading","- create GKE cluster for [~selles]  - load data inside GKE cluster  - study logs for qserv and dataloader",8
"DM-18065","02/25/2019 17:35:37","Bad logic in saturation interpolation config options","[~mfisherlevine] was just puzzled as to why it didn't look like the interpolation of saturated pixels had been turned off despite having set {{doSaturationInterpolation=False}}.  To stop the interpolation from ocurring, he had to set {{doSaturation=False}}.  The docs say:  {code:python}  doSaturation = pexConfig.Field(          dtype=bool,          doc=""Mask saturated pixels?"",          default=True,      )    doSaturationInterpolation = pexConfig.Field(          dtype=bool,          doc=""Perform interpolation over pixels masked as saturated?"",          default=True,      )  {code}  indicating some false logic in the task.  It looks like the fix should happen with a check for {{doSaturationInterpolation}} here: https://github.com/lsst/ip_isr/blob/master/python/lsst/ip/isr/isrTask.py#L1216    If these configs are meant to operate entirely independently, (a perhaps unlikely, but _possible_ use case: the run() method was passed an image that had the SAT bit set elsewhere), this should be indicated clearly in the docs to avoid potential confusion along the lines ""why are/aren't saturated pixels getting interpolated if I set {{doSaturation=False}}/{{doSaturationInterpolation=True}}).",1
"DM-18067","02/25/2019 22:23:28","Add fluxMag0 PhotoCalib factory function","While working on DM-10156, I found that it would in fact be useful to have a factory function that took the {{Calib}}-style {{fluxMag0}}/{{fluxMag0Err}} and returned a {{PhotoCalib}}. I implemented it on that ticket, but it stands alone, so I'm going to cherry-pick it here to save the poor reviewer of DM-10156 a little bit of effort.",2
"DM-18101","02/26/2019 10:33:57","Develop documents for HVAC Linux upgrade software contract","To develop statement of work that leads to generate purchase order for getting Linux upgrade software to be utilize in main telescope HVAC system",5
"DM-18102","02/26/2019 10:34:12","Firefly table overlay of coord_ra, coord_dec has stopped working","Uploading a table to Firefly with {{coord_ra}} and {{coord_dec}}, units of radians, worked as of a few months ago. In testing a Firefly notebook for DM-17753, in the past two weeks, the overlay of sources does not show up and is not present in the layers.    To reproduce: go to the [PR for DM-17753|https://github.com/lsst-sqre/notebook-demo/pull/17] and run that notebook in the LSST Science Platform Notebook Aspect.         Fix: regex error in pattern match.",1
"DM-18108","02/26/2019 17:15:37","SAL v3.9 testing","This task covers the time to review the proposed combined SAL-generated test interfaces.",2
"DM-18115","02/28/2019 08:00:34","AckError str and repr should include the added fields","{{AckError}} str and repr should include the extra fields ""error"" and ""result"". That way tracebacks and such will display the information.",0
"DM-18116","02/28/2019 10:08:24","Enum filtering fails if the value contains semicolon","UCDs often have semicolon separator between words. If we are filtering UCDs by selecting the values with semicolon from the list of values (""categories""), it fails.    Test case:   1. [http://localhost:8080/firefly/firefly-dev.html?__action=layout.showDropDown&visible=true&view=TestTAPSearch]   2. Use GAIA tap service.   3. Search in ADQL tab: SELECT column_name, ucd FROM tap_schema.columns WHERE ucd like '%meta.main%' .    3. In the result table, click column filter, click arrow next to ucd filer box, check 2 last entries (""pos.eq.ra;meta.main"" and ""pos.eq.dec;meta.main"") from the list, and click filter link.   4. Table load error is displayed: _Invalid statement: ""ucd"" IN ('pos.eq.dec_         _FIREFLY-59 has been created._         _June 21, 2019_     _ticket has been expanded to more filtering capabilities, (AND OR) conditions are available now._  |As reported, filtering fail when the value contains semicolon. Because we were using semicolon to separate conditions, this messes up the underlying query statement.  Instead of just fixing the problem, I went ahead and allow the conditions delimiter to be either {{AND}} or {{OR}}. Although this added a useful requested feature, it also may require updates to our online help. Please coordinate with the appropriate people after merging.  Sample conditions:  {{> 0 and < 50 or is NULL}}  {{< 0 or > 100}}  {{in ('m31', 'm41') or is NULL}}|         ",8
"DM-18117","02/28/2019 11:28:19","Investigate ADAM functionality for loss of network connectivity scenario","Right now, if the ADAM device is sending a voltage signal to the KiloArc, it will continue to send that signal until it receives a command to stop. The failure mode we are concerned about is, what happens if the CSC loses its connection to ADAM or crashes. Is there a way to detect that from the ADAM device, and stop sending the power signal as a result? This task is to investigate this. If there is not, then the higher level Illumination CSC will need the ability to cut power to the ADAM device, to ensure that the KiloArc can be gracefully powered down in the event of a CSC failure.",1
"DM-18120","02/28/2019 12:29:11","Remove --silent command-line argument from ap_verify.py","{{ap_verify.py}} originally had built-in support for SQuaSH upload, but it was removed in DM-16536. The frequently used {{--silent}} command-line argument, which disables SQuaSH upload, was deprecated.    Remove the deprecated {{--silent}} argument after Science Pipeline release 18.0 but before release 19.0.",1
"DM-18123","02/28/2019 12:53:21","TAP Search: Relayout TAP UI","Relayout the TAP search panel UI according to the screen shot attach.   * Target area and table area division should be about 40/60 and resizable    * Add help the indicates one-way to ADQL query panel       |Change layout according to attached mock-up.   * TAP search panel is now resizable   * Bottom section is expand/shrink when resizing   * Column constraint toolbar is separated and placed on top for better symmetry   * Message added to Advanced ADQL panel to warn user of lost query when switching back to Basic   * Added loading mask to Select Constraints when waiting for data to be loaded|    TODOs: * All labels should be reviewed.   *    ** Submit change request as needed.    Known Issues: * Left panel of Constraints (spatial/temporal section)   *    ** Has awkward alignment when expanded   ** Horizontal scrolling will not appear when shrink   ** Minor, collapsible icon and label not aligned   * Service Provider is default to {{IRSA}}, but is not shown in input field.|            ",8
"DM-18124","02/28/2019 12:53:37","Time server installation on the summit network","Review documentation procedures and perform installation of time server onto Summit network.",2
"DM-18127","03/01/2019 09:09:39","Update OFC to Use the DM Repository Template","Update the ts_tcs_ofcPython to use the DM repository template. This will use the eups to do the package management and scons to build the repository. This is a preparation to put the AOS modules into the scientific pipeline (this means the user can use 'eups -k -r .' to declare the module). This task will also need to modify the code to fulfill the PEP-8 coding style.",2
"DM-18128","03/01/2019 09:12:08","Use the yaml Format in Configuration File of OFC","Use the yaml format instead of txt format to do the configuration file management. The yaml is the standard in DM team for the configuration files. Since I never use the yaml before, this task will take some time for me to learn the use of yaml with Python.",3
"DM-18129","03/01/2019 09:15:19","Realize the OFC Interface Classes to MTAOS in Phase 1","Realize the OFC interface classes (in ts_ofc) to let the MTAOS to use. Chris and I discussed and defined the interface classes in the last sprint. I plan to realize them for Chris to be able to use with the real calculations instead of interface only. This is the phase 1 of task.",1
"DM-18132","03/01/2019 10:36:21","Change how astropy models are dealt with in SpectralExtractionTask","Sort out how/where the functions are defined, so that the can be evaluated neatly with the fitted params.",2
"DM-18134","03/01/2019 10:36:26","Get atmospec pipeline working with ctio0m9 data","We already have obs_ctio0m9, but some of the data we have from that telescope is not compatible with the obs_package as it stands, as they kept changing which amps they read out, and in what configuration.    With the help of Augustin, pick a few files with the same layout, and for which there exist a few biases (and other calibs if/as required) and get them readable.    This might mean making a temporary fork of obs_ctio0m9 to get things working.    Build the calibration products, and get processStarTask running on it.    Necessary because we want to test this pipeline with absorption spectra before going on sky, and the lab will only ever provide emission spectra, not to mention it would be nice to have a more realistic PSF.",2
"DM-18139","03/01/2019 10:36:34","Add functions for making and applying gain flats","Given a list of gains per-amplifier, apply them as if flatfielding.    Write a function to make this into a fake flatfield, so that it can be applied as part of ISR if required.",8
"DM-18148","03/01/2019 11:48:47","Create the initial model of the Batch Production Services requirements in MagicDraw","Attached to [DM-16084|https://jira.lsstcorp.org/browse/DM-16084] is an Excel spreadsheet with Batch Production Services requirements.   Please ingest them into the LSST model.  The spreadsheet contains a second sheet with opening paragraphs and the list of authors if needed.    Also, please note that that in one of the requirements (row 4) we reference LSE-81 as the parent requirement.",2
"DM-18154","03/01/2019 12:59:50","Remove azimuthDirection from the target event","We have agreed to remove the {{azimuthDirection}} field from {{trackTarget}} command of ATMCS and make the azimuth and rotator angles absolute in TPC-163. This ticket is to update the XML and the ATMCS Simulator.    The ATMCS will also have to be updated.    One question: which is indirectly related:  * Should the azimuthCalculatedAngle field of the mountEncoders event be in the range 0, 360 or -270 to 270? We should make sure that the XML has the correct information.    MTPtg and MTMount should probably get the same treatment, but on a different ticket. Whether we actually do it depends on what the MTMount vendor says (they may have already coded it this way!).",2
"DM-18158","03/01/2019 14:15:36","Clean up RemoteCommand waiting","The code that waits for command acks in {{RemoteCommand}} is hard to follow. Clean it up.    Also enable_csc fails if the summary state cannot be obtained due to DM-18035. Make it more robust.",0
"DM-18159","03/01/2019 14:19:32","Update SAL tests for v3.9","SAL v3.9 contains two new features   * RPM packagaing   * Combined interface modules    This task covers the effort required to update the tests for these new features.    There is also a desire to improve/increase the level of Java testing at the SAL layer.  This task also covers some time to work on this.",3
"DM-18160","03/01/2019 14:20:20","Add note about the Calib->PhotoCalib replacement to v17 Release Notes","Sometime in the next month, I will be replacing {{Calib}} with {{PhotoCalib}} (DM-10153). We need a note in the Release Notes for v17 to prepare people (the transition is not fully backwards compatible).",1
"DM-18161","03/01/2019 14:24:40","Plan M1M3 Support","Work on planning for future M1M3 support work",1
"DM-18167","03/01/2019 16:01:04","display_firefly needs to handle viewer_ids properly","In the LSST Science Platform Notebook Aspect, when using the Firefly for Jupyterlab extension, the {{lsst.display.firefly}} backend needs to properly set the viewer id when displaying an image. Then that will fix the problem of restarting a notebook kernel and having display of images going to the newly opened Firefly tab.    [~roby] found the problem area in the {{lsst.display.firefly}} code and it has been tested.",2
"DM-18168","03/01/2019 17:25:47","Remove workarounds for DM-18035 from ts_salobj","I have had to add some hacks to ts_salobj to work around DM-18035: late joiners do not reliably get topic data. Once that is fixed use this ticket to clean up the code.",0
"DM-18169","03/04/2019 06:23:01","Plan M1M3 Thermal Work","Work on and Finish thermal initial plan.",2
"DM-18172","03/04/2019 08:18:17","Write script to take standard detector characterization dataset(s)","This task captures the work required to write and test a script to take standard detector characterization dataset(s) for ATCamera. ",2
"DM-18173","03/04/2019 08:28:10","Auxtel Calibration System integration and test in the lab","Now that the software components for the calibration system are up and running again I'll be able to run some integration tests in the lab. I'll take this opportunity to properly debug an document any SAL/ScriptQueue related issue I encounter.     Required Components:    - ATMonochromator  - FiberSpectrograph  - Electrometer  - ATSpectrograph  - ATCamera      Additional components:    - White Light Source  - Chiller     * - Additional components are those that are not required for the tests but will be added if available.     The artifacts of this task will be ""at least"" a script to run the Calibration system and probably one that also involves the ATCamera and ATSpectrograph, as well as confluence page with description of the issues encountered and jira tickets.    ",2
"DM-18175","03/04/2019 09:11:59","Implement the build and packaging process for one AuxTel CSC","Continuing the effort to implement the build and packaging process for T&S CSCs.",3
"DM-18181","03/04/2019 09:41:23","Provide tool to validate datastore template configurations","DM-17025 added the ability to validate a DatasetRef against a selected file template. Now we need the ability to be able to test that every single registered DatasetType is consistent with the corresponding file template (for datastores that support file templates).    We also need to be able to form a DatasetRef for every DatasetType and every instrument to check that instrument specializations also conform.    It should probably also check that every one of those DatasetRefs has a corresponding formatter assigned.    Chained datastores will have to forward the validation to each contained datastore noting that not all datastores have to support all datasetTypes in a chained datastore.    I am assuming this should be a command that is run on demand rather than something that occurs every time the butler is instantiated.",8
"DM-18184","03/04/2019 11:32:54","upgrade babel to address some security concerns","Both Babel and ESlint are behind and should be upgraded.   * For ESLint there are also the rules to react functional components that can be added.   * We are currently using Babel 6.26, upgrade to the most recent 7.4.   * We are currently using ESlint 4.5, upgrade to 5.1    _Babel Helpful docs:_   * [https://babeljs.io/]   * [https://babeljs.io/docs/en/v7-migration\|https://babeljs.io/docs/en/v7-migration]    _ESLint Helpful docs:_   * [https://eslint.org/]   * to add react functional component rules   ** [https://reactjs.org/docs/hooks-rules.html#eslint-plugin]   ** {{npm package: eslint-plugin-react-hooks}}    _Driving reasons:_   * We like to keep Babel up-to-date for support purposes.   * Github posted by security vulnerability alert at [https://github.com/Caltech-IPAC/firefly/network/alerts].   * Babel 7 also adds better JSX support for react fragments.    A similar ticket in IPAC Firefly team has been scheduled for July sprint.  [https://jira.ipac.caltech.edu/browse/FIREFLY-74]      [|https://jira.ipac.caltech.edu/browse/FIREFLY-74]",2
"DM-18196","03/04/2019 17:09:14","Fix afw schema missmatch between ap_association and dax_ppdb","Schemas are slightly misaligned between ap_association and dax_ppdb. This causes a crash in running ap_pipe on round-tripped diaObjects. This ticket will add the nessacary columns (validityStart, validityEnd, lastNonForcedSource) to the default diaObject schema in ap_association",1
"DM-18201","03/04/2019 17:24:18","Write job advertisement for the DM Science Validation Scientist","The role of the DM Science Validation Scientist, as described in [LDM-294|http://ls.st/LDM-294] and [LDM-503|http://ls.st/LDM-503] will relocate to  LSST HQ in Tucson as a 100% role from mid 2019",1
"DM-18203","03/04/2019 21:08:54","numpy unicode warnings in readTextCatalogTask.py","Running meas_algorithms tests locally, I see a number of the following warnings:    {code}  tests/test_readTextCatalog.py::ReadTextCatalogTaskTestCase::testGivenNames    /home/parejkoj/lsst/repos/meas_algorithms/python/lsst/meas/algorithms/readTextCatalogTask.py:117: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.      names=names)  {code}    It looks like this code is used by the refcat ingester.",1
"DM-18282","03/05/2019 08:48:59","ReConfigure hardware cluster EFD","Update SAL runtime and writers on hardware test cluster pLAN EFD",2
"DM-18283","03/05/2019 08:59:40","Write a generic camera SAL XML and CSC","Write a new set of XML for a generic camera CSC and implement it with ts_salobj.    Required items:   * Set Exposure Time   * Set ROI   ** Top, Left, Width, Height   * Set Full Frame (short cut for the set ROI command)   * Start Live View   * Stop Live View   * Event indicating live view ip / tcp port for image data   * Event indicating current exposure time   * Event indicating current ROI   * Event for camera specific properties (key / value pair basically)   * Event for camera make / model   * Telemetry for temperature",3
"DM-18290","03/05/2019 11:52:54","Implement Detailed State on White Light CSC","Implement detailed state on the WLS CSC. This will simply be an enumeration of the hardware states that we can receive from the KiloArc. Since we already react to an error signal from the KiloArc, this won't change CSC behavior, it is strictly about reporting more information.",2
"DM-18291","03/05/2019 12:14:31","Lab Testing for White Light Source CSC","Hook the ADAM box and our CSC up to the actual KiloArc device in the lab, and try sending commands to it. ",3
"DM-18292","03/05/2019 12:17:40","Attend SpatialAnalyzer training (Including Travel) ","Attend Spatial Analyzer training event in Los Angeles, March 12-14, plus travel days",5
"DM-18297","03/05/2019 15:13:08","Make it easy to run a single script from the command line, for testing","I would like a simple way to configure and run a single script from the command line and see its SAL outputs. This is intended to support development by making it easy to run the script without needing the script queue. I realized the need when writing the ATPtg/ATMCS integration script.    It may be that [~tribeiro]'s run_queue UI is enough, but I was hoping for something even simpler -- something that didn't use the script queue at all.    I initially thought of just expanding {{BaseScript.main}} but that is a non-starter because scripts are intentionally not on the $PATH. Instead I think it has to be a standalone script that accepts the name of the script to run. The interface and code can be based on [~tribeiro]'s run_queue UI.    To make this safe without colliding with a script queue I suggest that it fail if the script queue is alive.",1
"DM-18305","03/05/2019 23:22:13","Specify exactly the number of cores and GB to be used for Firefly/Portal pods on the LSP instances","This sub-ticket of DM-18304 just asks the Portal team to specify precisely the number of cores and GB to be allocated to each Firefly pod on the NCSA LSP instances {{lsst-lsp-int}} and {{lsst-lsp-stable}}.    The idea is to get the numbers on the record on this ticket.  Subsequently as part of Portal close-out work this information will be included in the deployment manual.    The actual Kubernetes configuration modifications will be done on DM-18304.",0.5
"DM-18311","03/06/2019 11:28:52","Add an option to keep the files generated by salgenerator in make_salpy_library","Sometimes, when trying to debug SAL communication,  it is useful to have the test scripts generated by salgenerator. I think it would be helpful if make_salpy_lib has an option to keep those files. ",1
"DM-18312","03/06/2019 11:34:08","Prepare and deliver talk to Kitt Peak Docents","Talk Tuesday March 5th 18:00",2
"DM-18313","03/06/2019 11:36:39","February 2019 Monthly report","write summary",1
"DM-18318","03/06/2019 13:44:52","Create initial subset of timeseries features for DIAObject","While we wait for a complete list of DIAObject timeseries features (https://jira.lsstcorp.org/browse/DM-11962) it will be useful to have various simple statistics to assist with exploring AP processing outputs.  The implementation is not expected to be final.    In a brown bag discussion, we suggested the following (all computed on calibrated fluxes):   * max   * min   * mean   * std   * skew   * median   * MAD   * percentiles ([5,25,50,75,95])   * Stetson J   * Linear trend   * max delta flux/delta t   * average flux error",8
"DM-18320","03/06/2019 14:30:29","Update to make FITS files ingestable","The FITS files generated by the ATArchiver using the headers created by the HeaderService had to be adjusted to proper ingestion.",2
"DM-18339","03/07/2019 17:13:22","Fix W505 errors ts_ATDome","Check for and fix W505 errors (doc strings and comments too long) in ts_ATDome, ts_ATPneumaticSimulator and ts_standardscripts",1
"DM-18340","03/07/2019 17:26:33","Update ATDomeTrajectory to use the target event from ATMCS","The ATDomeTrajectory CSC presently tries to use the pointing component to get its target, but a much better choice is the new {{target}} event from ATMCS. Update it accordingly.",1
"DM-18341","03/07/2019 17:44:45","implement the multi-cast alternate recommended by DM-17601","Using multi-cast to do service discovery does not work in GKE environment, and also it requires low-level hardware configuration. DM-17601 was created to explore the alternate way to accomplish service discovery. It recommended using redis. This is the implementation ticket.          <text taken from PR>    *Objective:*  The objective is to implement an alternative to multicast cache peer discovery mechanism. Multicast is not supported in the cloud, making Firefly unable to scale horizontally.    The implemented alternative is to use a publish-subscribe messaging system for peer discovery. The flow is similar to multicast in which it relies on a periodic heartbeat message from each active peer to determine ‘liveliness’. Subscribing to the same channel determine the membership of the group.    *Implementation Details:*  A custom CachePeerProviderFactory is created to determine which discovery mechanism to use based on configuration.  Messaging is implemented using Redis server and Jedis client. The preferred deployment of Redis is via Docker. Jedis dependencies were added to Firefly.     ",20
"DM-18342","03/08/2019 08:39:06","Add ci_id, ci_url and squash_id as InfluxDB fields instead of tags","{{ci_id}}, {{ci_url}} and {{squash_id}} are sequential numbers. If mapped to InfluxDB tags that will   [increase InfluxDB series cardinality|https://docs.influxdata.com/influxdb/v1.7/concepts/schema_and_data_layout/#discouraged-schema-design] which is not recommended.    We should add {{ci_id}}, {{ci_url}} and {{squash_id}} as InfluxDB fields instead.",0.5
"DM-18352","03/08/2019 11:31:38","update allocateNodes.py with ability to start HTCondor with partitionable slots","May not be required if HTCondor pool is available for use instead of Slurm.    Need partitionable slots for efficient use because certain pipeline tasks need more memory/cpus than others.",2
"DM-18354","03/08/2019 13:29:41","afw test fails due to assertEqual on float","Using a bleed version of the conda environment, afw test case PhotoCalibTestCase fails with the following error:       {code:java}  # create with non-zero fluxMag0 and err   photoCalib = lsst.afw.image.makePhotoCalibFromCalibZeroPoint(fluxMag0, fluxMag0Err)   self.assertEqual(photoCalib.getInstFluxAtZeroMagnitude(), fluxMag0)   > self.assertEqual(photoCalib.getCalibrationErr(), calibrationErr)   E AssertionError: 1617423020.8062096 != 1617423020.8062098{code}",1
"DM-18355","03/08/2019 13:43:20","Sorting images in the nublado spawner page","The images in the drop down menu of the LSP spawner are currently sorted by date. This is ok, but it makes it a bit hard to find a specific image, especially if it is ancient (i.e., the preceding major release). It would be nice to instead sort by image name. I think this would mostly have the effect of being a sort by category and then a sort by date within category (at least if the naming conventions hold). I could see arguments for major release, weekly, daily (from least populated to most) or daily, weekly, major release (from most recent to least).    This will require changes to [scanrepo.py|https://github.com/lsst-sqre/jupyterhubutils/blob/master/jupyterhubutils/scanrepo/scanrepo.py]",1
"DM-18357","03/08/2019 14:44:51","Improvements on request_script","After using the new version of {{request_script}} for a couple of weeks I have gathered some new features that will be really helpful (I am also glad to hear that people liked the script and think we will be using it for a while). This task is to add some of the requests I've got to improve the script.    Add user login/logout. To be able to coordinate multiple people interacting with the same instance of {{request_script}}, add a log in/out functionality. This is not to provide any kind of authentication but rather to inform users if there is anyone using at the time. Basically when someone enters the shell it will have something like:  {code:bash}Please, log in with a user handle (use your slack username to make it easy to contact you).  (login): tribeiro  (cmd[tribeiro]): logout  Please, log in with a user handle (use your slack username to make it easy to contact you).  (login):  {code}    Check usability of [asynccmd|https://pypi.org/project/asynccmd/]. This is an asyncio capable version of cmd. It would make the script much more responsive if it can be incorporated.    Add unit test.     limit number of script in the past queue.    Improve parameter parsing for the run command.      ",2
"DM-18358","03/08/2019 14:48:28","ATSpectrograph updates","The current version of ATSpectrograph was developed while the linear stage was not working. The control algorithm needs to be properly implemented. We are also missing the feature where it is possible to select filter and grating by name instead of only id.  ",1
"DM-18363","03/09/2019 00:07:36","Fix Qserv centos-llvm builds","The Qserv centos7-llvm builds have been failing for a long time now, in the ANTLR4 package, on an include of standard library header {{<algorithm>}}.  Track this down and fix.",1
"DM-18370","03/11/2019 08:46:51","OpenSplice Representative meeting","This task will be to meet with the OpenSplice folks that will be meeting here from Tuesday to Thursday. I will attend all sessions. ",2
"DM-18371","03/11/2019 08:56:10","Confluence Doc stubs","This task is to create stubs for all the CSC's. I will create the template pages that the relevent CSC developers need to populate. This will help get the ball rolling as to how our teams documentation needs to look like. ",2
"DM-18374","03/11/2019 09:44:22","Hexapod/Rotator documentation review","Review the Hexapod/Rotator documentation received from MOOG.",1
"DM-18377","03/11/2019 10:25:20","Opensplice representative meeting","Consult with Opensplice reps",3
"DM-18382","03/11/2019 10:45:33","Write an ATDomeTrajectory/ATDome/ATMCS integration script","Write a script to test the integration of ATDomeTrajectory, ATDome and ATMCS. Put it in ts_standardscripts.",3
"DM-18388","03/11/2019 13:03:51","Learn script queue","Learn script queue. Review documentation:     https://confluence.lsstcorp.org/display/LTS/Running+the+script+queue+with+containers",2
"DM-18389","03/11/2019 13:04:16","Understand what's going on with the EFD","Understand what's going on with the EFD",2
"DM-18391","03/11/2019 13:52:05","scons should only build Test and Script libraries if running tests","The current scons system builds the Test and Script libraries for almost all commands. It should only do so when running tests, and not cleaning.",0
"DM-18394","03/11/2019 14:44:49","Crash when running ap_pipe on calexp templates","Trying to call {{ap_pipe}} on {{ap_verify_hits2015}} with a calexp template crashes with an error. This appears to be caused by the change to {{ImageDifferenceTask.refObjLoader}} in DM-10242, and can be fixed by explicitly configuring {{config.differencer.refObjLoader}} in {{ApPipeTaskConfig}}. In hindsight, this means that we were previously testing calexp templates using a different reference catalog for {{ProcessCcdTask}} and {{ImageDifferenceTask}}!    Although this issue should never come up in {{ap_verify}} work (the loader subtask is created only if {{config.differencer.doSelectSources = True}}), we use modified versions of the dataset configs frequently enough that we should probably configure {{config.differencer.refObjLoader}} in {{ap_verify_hits2015}} and {{ap_verify_ci_hits2015}}. This will prevent unexpected crashes when other config parameters get overridden on the command line.",1
"DM-18397","03/11/2019 15:26:39","Release new version of LSE-61 with LCR-1425/LCR-1463/LCR-1465 incorporated","Work with [~mrodriguez] to update the LSE-61 MagicDraw model for the final release of approved LCRs LCR-1425/LCR-1463/LCR-1465 and create a new version of LSE-61 PDF for upload to Docushare.     This includes checking the output from the latex docgen, adding flowdown from OSS, and adding priorities to the newly added requirements.",2
"DM-18401","03/11/2019 19:07:22","Minor code refactoring and cleanup in the Qserv Replication system","A few minor projects to improve the quality of the code:  * eliminating _std::_ in the CC files  * fixing Doxygen documentation  * fixed syntactical and grammatical errors in comments  * replace absolute names of methods in logging statements and exception messages with the compiler-provided string ___func___  * fix indentations  * eliminate or add empty lines as required by the LSST C++ Style Guide  * fix class declarations to comply with the Style Guide",3
"DM-18406","03/12/2019 10:23:10","Add templatekit.yaml files to lsst/templates","The purpose of this ticket is to implement {{templatekit.yaml}} files in the templates repository. These files customize and refine the experience of generating files and projects from Slack with tempatekit and templatebot. {{templatekit.yaml}} files are being concurrently designed in DM-18405.",5
"DM-18409","03/12/2019 11:33:49","Loading mask z-index is breaking natural stacking order of the components","In TAP Search panel, loading masks for Spatial/Temporal area and Column Constraints area prevent user from selecting another schema or table, while loading mask is displayed.    If loading-mask css class did not set z-index, the components would be displayed correctly, and select drop-down would appear on top of the loading mask.    Whenever possible, we should avoid using z-index, so that our components are following the natural stacking order, described in the link below:    https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Positioning/Understanding_z_index/Stacking_without_z-index",2
"DM-18418","03/12/2019 14:29:45","Read the EFD Related Documents ","Review the EFD related documents and attend the related meetings.",1
"DM-18421","03/12/2019 15:28:49","Add function set_summary_state","Add a new function {{set_summary_state}} to {{ts_salobj}} that allows us to put a CSC into any valid summary state.    This will primarily be used to enable or disable CSCs but it may also be handy for changing configuration (by moving it to STANDBY, then the next desired state).",1
"DM-18424","03/12/2019 15:44:29","Remove heartbeat event from subsystem XML","Once the blocking ticket is completed, the heartbeat event in all subsystem XMLs need to be removed.",1
"DM-18430","03/13/2019 08:09:12","Attend the DDS Workshop","Attend the DDS Workshop. The agenda is here:    [https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LTS&title=Opensplice+Vendor+Visit]     ",2
"DM-18433","03/13/2019 09:02:16","C++ code that uses yaml files needs to point at the new location for those files.","The C++ code which loads config files (YAML and otherwise) needs to be modified to point at the locations to which the YAML has moved.      ",3
"DM-18438","03/13/2019 12:35:22","Specify string lengths in PosixDatastoreRecords table","We do not currently specify string lengths for DatabaseDict tables. Update the constructor used from PosixDatastore to include lengths for the fields.  This is done currently in code because the tables are dynamic and each datastore gets to name the table to use.",2
"DM-18439","03/13/2019 13:09:38","Attend DDS workshop","Remotely attend the DDS workshop.",2
"DM-18440","03/13/2019 13:42:26","Document MySQL Cluster limitations + review MariaDB High Availability MaxScale","Check MySQL Cluster limitations we are currently having and document them in confluence. As this task was shorter than expected I will investigate how MariaDB High Availability MaxScale works.",2
"DM-18451","03/13/2019 16:53:43","TAP: set schema_index in TAP_SCHEMA.schemas table","In TAP_SCHEMA.schemas table, 'schema_index' is used to order schemas by importance.     Currently, gaiadr2 with non-existing table comes first, wise_00 comes last.    Please, set schema_index so that ""sdss_stripe82_01"", ""wise_00"", and ""tap_schema"" come first, followed (if they need to be kept) by non-existing or dev. datasets. ",1
"DM-18452","03/14/2019 04:11:07","Backup Qserv data for GKE/Wise cluster","Write a procedure to backup/restore all Qserv data for Qserv on GKE.  Data will be backup inside a Google storage bucket.  ",8
"DM-18453","03/14/2019 06:48:01","Deployment environtment for MySQL Cluster + CSC","Work with [~avillalo] to deploy the MySQL Cluster + CSC to replicate some of the issues we've been experimenting:   * High memory usage",2
"DM-18454","03/14/2019 06:49:19","Test ATDome vendor interface ","Test ATDome vendor interface, prior to going to the summit I will add a minimum set of test I will try at the summit.    All of these are done manually using the telnet interface, if unit testing is required can be discussed later.    Tests to perform:    * open/close shutters   ** main door *{color:#00875a}(DONE){color}*   ** dropout shutter *{color:#00875a}(DONE){color}*   ** synchronized *{color:#00875a}(DONE){color}*   * rotate to various positions{color:#00875a} *(Move just fine)*{color}   * phase wrap {color:#00875a}*(DONE)*{color}   ** (meaning go to 330 degrees, then go to 30 degrees... and it should only move 60 degrees)   * go to home position *{color:#00875a}(DONE){color}*   * open shutter then stop in the middle of the command {color:#00875a} *(DONE but not recorded as it was as expected)*{color}   * close shutter then stop in the middle of the command  *{color:#00875a}(DONE){color}*   * values out of range {color:#00875a}*(DONE)*{color}   * move commands while it is moving     *    ** dome shutter  *{color:#00875a}(DONE){color}*   ** azimuth{color:#00875a} *(DONE, it does the latest command)*{color}   * home command while it is moving {color:#00875a}*(DONE)*{color}   * emergency stop while it is moving {color:#00875a}*(DONE for shutter, same behavior as for the next test)*{color}   * emergency stop then move *{color:#00875a}(DONE for shutter){color}*   * Invalid commands *{color:#00875a}(DONE){color}*    All tests should be recorded in a text file exactly how I get responses from telnet",2
"DM-18456","03/14/2019 06:52:59","Test ATHexapod CSC at the summit","Test ATHexapod CSC at the summit. Tests performed were:   * Execute the CSC and see if reponds to commands   * Check possible bugs and document them   * Create next tasks to have a working version for ATHexapod CSC",2
"DM-18483","03/14/2019 09:48:07","Attend DDS workshop - Rob","Attend the DDS workshop.",2
"DM-18484","03/14/2019 10:37:18","Export the SAL generated HTML to the project webserver","There is a need to automatically update the SAL HTML definition pages on the LSST project page, [https://project.lsst.org/ts/sal_objects/].  It would be good to have the Jenkins build publish these page, however there is an issue getting the data off the AWS hosted server.  This task covers the time it will take to investigate, propose and implement a solution.",1
"DM-18488","03/14/2019 12:54:40","Update pyyaml to v5.x","pyyaml 5.1 is out. Test it with the stack and use it if all tests pass. Includes some fixes that are needed for python3.7.  ",1
"DM-18490","03/14/2019 14:20:04","Move TimingMetricTask to verify","Due to a misinterpretation of RFC-550, DM-16536 left {{TimingMetricTask}} in {{ap_verify}}.    Provisionally, this involves moving {{lsst.ap.verify.measurements.profiling.TimingMetricTask}} to {{lsst.verify.commonMetrics.TimingMetricTask}}, and updating documentation and tests accordingly.",2
"DM-18494","03/14/2019 17:22:35","Mimic the meas_base plugin system for use in ap_association, DiaObject summary metrics","Currently all the summary statistics created in DM-18318 are haphazardly listed in a single function and run as a block. This ticket will make the summary statistics run configurable, expandable, etc. by mimicking the measurement plugin system implemented for the measurement tasks within meas_base.",8
"DM-18500","03/15/2019 11:10:35","Java Test scripts | part 2","This is the second half effort that will capture the work for the Java. More details to be put after completing the first half. I imagine the first half will be getting my machine able to build the tasks and learning how the binding works.",2
"DM-18501","03/15/2019 11:13:10","ts_xml CSC overview table generator.","This task is to get the CSC overview table automatically generated by the xml. It is also to add the different tags withing the SALGenerator.xml to have the values that we want it to have to be displayed on the table.",2
"DM-18508","03/15/2019 17:30:43","Kavli workshop telecon and doc update","Chair telecon March 15 – prepare - updates to doc",2
"DM-18509","03/15/2019 17:33:00","CADC meeting","Discussions with CADC on a range of topics including TAP etc . and travel time",8
"DM-18511","03/17/2019 15:07:49","Translate the OFC Code of LabVIEW to Python","Translate the LabVIEW code (ts_tcs_ofc) related to the coordination transformation from ZEMAX to subsystems to Python code (ts_ofc, I renamed the ts_tcs_ofcPython to ts_ofc to fulfill the eups package name requirement). This is to translate the OFC calculated DOF in ZEMAX coordinate to the hexapod position and actuator forces in the subsystem's coordinate system.",3
"DM-18512","03/17/2019 15:09:50","Realize the OFC Interface Classes to MTAOS in Phase 2","Realize the OFC interface classes (in ts_ofc) to let the MTAOS to use. Chris and I discussed and defined the interface classes in the last sprint. I plan to realize them for Chris to be able to use with the real calculations instead of interface only. This is the phase 2 of task.    This task will focus on the algorithm verification (coordinate transformation) and functional units test of control interface classes.",2
"DM-18513","03/17/2019 15:14:02","Install the MySQL Cluster with Puppet on local machine","Install the MySQL Cluster with Puppet on local machine in Tucson.    This is to be familiar with the baseline software setup. This task will also try to compare the MySQL Cluster and InfluxDB (NoSQL) and discuss with Andres.    The puppet script by Andres V. is here:    [https://github.com/avillalobos/lsst_devops]",2
"DM-18514","03/17/2019 15:43:11","Prepare for AMCL ","Prepare talk with Zeljko for AMCL on April 1.     ",2
"DM-18520","03/18/2019 09:43:40","ATPointing + ATAOS integration test","Finalize ATAOS and ATPointing integration test",1
"DM-18521","03/18/2019 09:52:27","Integrate DIMM with vendor software. ","We got updated information about the interface to the DIMM software. This task is to start integrating the software. ",1
"DM-18522","03/18/2019 09:54:59","Initial test of DIMM CSC with vendor software. ","Testing of the software integration between DIMM CSC and vendor provided software. ",1
"DM-18527","03/18/2019 11:47:33","Write ""slew telescope"" script","Write a initial version of the ""slew telescope script"". ",1
"DM-18529","03/18/2019 12:04:51","Add unique parts of aggregated dataids to the job metadata","Currently only the filter is passed through to the job metadata.  In some cases, there is additional metadata that can be passed on.  For example, some metrics are produced on a per tract basis.  A general way to do this is to create a dictionary that has just the entries that are common over the full list of data ids passed on to {{runOneFilter}} [here|https://github.com/lsst/validate_drp/blob/e350b0cd818a6640ea27be1ff037cf698d316a1b/python/lsst/validate/drp/validate.py#L293].",3
"DM-18542","03/19/2019 13:23:30","Provide JSR reposnses ","Victor has sent the JSR response template - fill in DM part",1
"DM-18547","03/20/2019 09:08:19","M1M3 Thermal FPGA Development Part 1","Start development of the thermal system FPGA.",8
"DM-18548","03/20/2019 09:09:22","Upgrade pLAN EFD","Rebuild the pLAN EFD with the tables corresponding to tag 3.8.41  and prepare RPM's for switch to 3.9.0. Install OpenSplice debug tools",3
"DM-18549","03/20/2019 09:13:49","Investigate OpenSpliceDDS native python api","Prototype a SAL API based in the OpenSpliceDDS native python interface,  and add distribution to ts_opensplice for V6.9 community including the python  asserts pre-built",3
"DM-18552","03/20/2019 10:55:27","Support for new brighter-fatter kernels reverted","DM-13293 created support for new-style brighter-fatter kernels, specifically in b14fc9a134e7937ea703f4b93a9a79429136d7b7.    It was easy back then to support both the new style kernel objects, and the old, HSC-style numpy arrays, as Subaru still had its own isrTask.    However, during the great ISR merge, this was no longer the case, and only the HSC-style kernels are now supported.    The change was reverted here:    https://github.com/lsst/ip_isr/commit/141792033a1ca1d5d97d72520d06508c51e9a28c#diff-3a9973baa2fe4526d09a95e405f90269L599    We need to be able to do both.",1
"DM-18553","03/20/2019 11:01:16","Demonstrate WCS inconsistency with WFS chips in phosim","It appears that phosim assumes the eimages it produces are always oriented in focal plane coordinates when it pastes on the rough WCS.  This is true of all science detectors, but appears not to be true for the wavefront sensors, which are rotated 90 deg in each subsequent corner raft.    This leads to a rough WCS that is wrong for the WFS chips.  See the following figure.  RA and Dec labeled as inferred from the attached WCS.  Red circles are the pixel positions from the phosim {{centroid.txt}} file.  The blue circles are the RA/Dec positions from the input file to phosim.  All circles are annotated with: object identifier, RA/x, Dec/y.    As you can see the relative positions of object 0 and object 1 are flipped in RA/Dec relative to x/y.  My interpretation is that this means the translation from RA/Dec to pupil coordinates in phosim and the ray tracing step are being done correctly.  I think the issue just comes from an incorrect assumption for {{CRPIX}} and possibly the {{CD}} matrix when computing the WCS.     !labeled.png!    P.S. It's possible the bulk offset is due purely to distortions in the optics that the simple WCS does not account for. ",3
"DM-18556","03/20/2019 12:48:28","Adapt visit and coadd qa analysis scripts to run on DESC DC2 outputs","This will concentrate on adapting the visit and coadd analysis scripts to run on DC2 data. The colorAnalysis script will be addressed on a separate ticket.",8
"DM-18557","03/20/2019 13:51:56","Column UCDs","Bug:    sdss_stripe82_01.RunDeepSource has variance for 'ra' has wrong ucd: ""stat.variance;pos.eq"" instead of ""stat.variance;pos.eq.ra""         Possible improvement:    sdss_stripe82_01.DeepCoadd and sdss_stripe82_01.Science_Ccd_Exposure - there are 5 columns with ucd=""pos.eq.ra"" and 5 columns with ucd=""pos.eq.dec"". Center ra, decl cols can be marked with extra ucd word ""meta.main"" if we'd like people to search on it. Then ra would have ucd=""pos.eq.ra;meta.main"" decl would have ucd=""pos.eq.dec;meta.main"".         In general, using meta.main is helpful, when there are several columns with the same ucd",2
"DM-18573","03/20/2019 19:06:40","Fix bug for running on DC2 data introduced by DM-13202","A minor bug was introduced in DM-13202 in that it assumed that ""ccd"" is a valid dataId key.  This is not true for DC2 data.",1
"DM-18574","03/20/2019 20:43:39","Electrometer CSC outputs incorrect URL for large file collection","The electrometer outputs a large file which is collected for the LFA, however, the URL it's providing is incorrect. The directory and naming structure also appears odd (but is probably all the same bug).    the Electrometer file string being reported is:  `https://127.0.0.1/1-1553029665.151835`  whereas the proper string should be:  `http://10.0.100.133:8000/1-1553029665.151835.fits`    so http instead of https, wrong IP, missing port, missing "".fits"" - files are being put in:  `/home/saluser/ts_electrometer2/electrometerFitsFiles/1` ",1
"DM-18576","03/21/2019 10:01:27","Issue warnings if translator methods are shadowed","Shadowing can happen if you add en explicit translator method and forget to remove it from the constant or trivial mappings list. Check for this scenario and warn.  Also check that const/trivial methods are not being inherited from a parent and overriding the versions defined in the current class.",1
"DM-18581","03/21/2019 13:26:43","Write meeting abstract and description for AMCL","Provide abstract for the AMCL meeting ",1
"DM-18582","03/21/2019 14:02:54","A few ts_scriptqueue tests fail with recent ts_salobj","[~tribeiro]'s lsst/queue:ospl6.9-sal3.9 Docker container dated 2019-03-21 causes a few ts_scriptqueue tests to fail: timing out when waiting for a process to terminate. I have also seen this in ts_salobj and fixed that as part of DM-18344.    It appears that SAL components run as subprocesses take longer to exit than they used to, so a longer timeout is required.",0
"DM-18588","03/21/2019 16:06:28","Create stub pip package to test installation on nublado","We have a test case, LVV-T768, that tries to verify that code is installable in the nublado system.  I would like a stub pip package that we control that is essentially a no op (maybe it prints some amusing message).  The name will be {{sqre-pip_install_test}}.",2
"DM-18589","03/21/2019 16:27:52","Improve queue behavior when script fails to load.","The queue is not properly reporting when a script fails to load. For instance, following an import or syntax error… I think we need to monitor the result of the script execution and take appropriate measurements… Specially right after loading it…    Ideally it would be nice if the add command is rejected if the script is not able to load at all, but I understand it is hard to identify load errors from different type of errors. So I guess, the queue must be able to check the execution returned value and would be good to get some traceback information.   ",2
"DM-18592","03/22/2019 08:58:09","Add standard tests to ts_salobj for use by other packages","Several CSC test are repeated in most packages that use ts_salobj, including:  * Check standard summary state transition commands  * Check that the bin.src script that runs the CSC works    Write these tests in ts_salobj in a way that they can be used by other packages, e.g. by providing a base class for CSC unit tests.    Also add documentation with suggestions for how to test CSCs, SAL components and Scripts.    Also convert at least one other package to make sure the new code is usable.",1
"DM-18596","03/22/2019 10:57:40","Run Laser Stabilization test for TunableLaser","As part of Nick's visit, he discovered that the laser suffers an unknown power degregation by 10 percent that appears to correspond with one of the temperature registers starting to run hot.     The vendor has asked us to run the test with a data dump from the laser before and after the test.",2
"DM-18598","03/22/2019 11:39:11","Testing of Electrometer (AuxTel) SW","Testing of Electrometer (AuxTel) SW",5
"DM-18601","03/22/2019 12:45:18","Investigate newer SQLAlchemy versions","With the merge of DM-18367 we are getting many DeprecationWarnings from sqlalchemy when running the test in daf_butler (about 85 of them).  This seems to be down to some new code paths being used from within sqlalchemy.    We are currently running 1.2.16.  On this ticket I will see if 1.2.18 gets rid of the warnings and possibly try the new 1.3 series.    The current warning comes during schema creation:  {code}    File ""/Users/timj/work/lsstsw3/build/daf_butler/python/lsst/daf/butler/core/registry.py"", line 177, in fromConfig      return cls(registryConfig, schemaConfig, dimensionConfig, create=create)    File ""/Users/timj/work/lsstsw3/build/daf_butler/python/lsst/daf/butler/registries/sqliteRegistry.py"", line 94, in __init__      super().__init__(registryConfig, schemaConfig, dimensionConfig, create)    File ""/Users/timj/work/lsstsw3/build/daf_butler/python/lsst/daf/butler/registries/sqlRegistry.py"", line 82, in __init__      self._createTables(self._schema, self._connection)    File ""/Users/timj/work/lsstsw3/build/daf_butler/python/lsst/daf/butler/registries/sqlRegistry.py"", line 154, in _createTables      schema.metadata.create_all(connection)    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/schema.py"", line 4201, in create_all      ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/engine/base.py"", line 1593, in _run_visitor      visitorcallable(self.dialect, self, **kwargs).traverse_single(element)    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/visitors.py"", line 130, in traverse_single      return meth(obj, **kw)    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/ddl.py"", line 757, in visit_metadata      [t for t in tables if self._can_create_table(t)]    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/ddl.py"", line 1174, in sort_tables_and_constraints      deterministic_order=True,    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/util/topological.py"", line 50, in sort      for set_ in sort_as_subsets(tuples, allitems, deterministic_order):    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/util/topological.py"", line 36, in sort_as_subsets      _gen_edges(edges),    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/exc.py"", line 139, in __init__      message += "" (%s)"" % "", "".join(repr(s) for s in cycles)    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/exc.py"", line 139, in <genexpr>      message += "" (%s)"" % "", "".join(repr(s) for s in cycles)    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/schema.py"", line 692, in __repr__      + [""%s=%s"" % (k, repr(getattr(self, k))) for k in [""schema""]]    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/schema.py"", line 691, in <listcomp>      + [repr(x) for x in self.columns]    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/schema.py"", line 1400, in __repr__      + [""%s=%s"" % (k, repr(getattr(self, k))) for k in kwarg]    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/schema.py"", line 1400, in <listcomp>      + [""%s=%s"" % (k, repr(getattr(self, k))) for k in kwarg]    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/sql/schema.py"", line 112, in __repr__      return util.generic_repr(self, omit_kwarg=[""info""])    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/util/langhelpers.py"", line 556, in generic_repr      val = getattr(obj, arg, missing)    File ""<string>"", line 2, in quote    File ""/Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/util/deprecations.py"", line 118, in warned      warnings.warn(message, wtype, stacklevel=3)    File ""/Users/timj/work/lsstsw3/miniconda/envs/lsst-scipipe/lib/python3.7/warnings.py"", line 110, in _showwarnmsg      msg.file, msg.line)    File ""xxx.py"", line 12, in warn_with_traceback      traceback.print_stack(file=log)  /Users/timj/work/lsstsw3/stack/DarwinX86/sqlalchemy/1.2.16/lib/python/SQLAlchemy-1.2.16-py3.7-macosx-10.7-x86_64.egg/sqlalchemy/util/langhelpers.py:556: SADeprecationWarning: Use ``<obj>.name.quote``    val = getattr(obj, arg, missing)  {code}",2
"DM-18605","03/22/2019 15:57:28","TSS Jenkins Executor offline","The TSS Jenkins executor has hit the 50 gig alert limit again and has gone offline.",2
"DM-18610","03/25/2019 08:12:15","Add fields, limited mutability, and trim/assembly-state tracking to cameraGeom","Add additional functionality to cameraGeom to reflect use cases identified in obs_lsst, while bringing obs_lsst usage in line with the intent of the cameraGeom design.  This includes:   * Add additional amplifier/detector fields for flips in pre-raw to raw transmission.   * Add accessor to get (computed) raw readout corner.   * Rename methods that always reflect assembled state to make this explicit in the names; deprecate the old names.   * Add new fields that track the trim/assembly state.   * Add new accessors for bounding boxes and readout corner that utilize trim/assembly state.   * Add setters for all fields, along with runtime freezing: only frozen Detectors can be added to an Exposure, but an unfrozen Detector can be obtained by copying a frozen Detector.    At least some of these changes are API changes and will need an RFC.  I'd like to prototype them out on this ticket branch first.",40
"DM-18611","03/25/2019 09:20:54","Update LSE-229","This task covers the time needed to update LSE-229.  It is a draft document covering the middleware communication design.",2
"DM-18618","03/25/2019 14:04:10","jenkins 2019-03-25 security advisory","[https://jenkins.io/security/advisory/2019-03-25/]",1
"DM-18621","03/25/2019 14:44:21","`Too many CR pixels (max 500000)` when running `constructDark.py` on auxTel dark","When running `constructDark.py` on an auxtel dark (2019030800135, see below), the following error appears:          lsst::pex::exceptions::LengthError: 'Too many CR pixels (max 500000)'         Visual inspection of     ""AT_O_20190308_000135-ats-wfs_ccd.fits"" (see directory below) shows that there are not as many CR's.    -Data (see IMGTYPE in headers for types, which follows the entry in [https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=SYSENG&title=LogBook+for+AuxTel+camera+diagnostic+data+taking] under MAR08, 2019):     /project/plazas/data/auxtel_DMCS_images_2019-03-08/cal    -ID:     WARN: Unable to process DataId(initialdata=\{'imageType': 'DARK', 'dayObs': '2019-03-08', 'visit': 2019030800135, 'detector': 0, 'detectorName': 'S00'}    -Command run:    * constructDark.py /project/plazas/calibrations_auxtel_mar20_REPO --calib /project/plazas/calibrations_auxtel_mar20_CALIB/ --output /project/plazas/calibrations_auxtel_mar20_OUTPUT/ --id imageType='DARK' --batch-type none --clobber-config    stack: ""w_2019_11"" with branch ""tickets/DM-18051"" of ""obs_lsst""",5
"DM-18624","03/25/2019 17:35:35","speed up Table pickling","With Schema pickleable (DM-17950), I can now use ProcessPools to parallelize reading data. But pickle/unpickle of SourceCatalogs seems to be preventing that from actually providing a speed boost. I've found at least one obvious place to gain a big chunk of speed. I'll spend a little more time looking for others.",2
"DM-18638","03/26/2019 19:02:24","Check for at least 2 visits in AMx metric calculation.","Running `matchedVisitMetrics`for DM-17830 was yielding AM1 of 0 for HSC WIDE tracts 9615 and 9697 which have visits numbers in the single digits. AM1 is the median of the RMS of the distances between 2 stars in N visits. Closer inspection revealed that more than half of the RMSs were exactly zero, which is why the median was exactly zero.  [~wmwood-vasey] quickly found the problem which was that if a pair appeared in exactly one visit, the stdev([one distance]) was exactly 0  and not NaN.    Require at least 2 distances before computing their stdev.    [~wmwood-vasey] will investigate the potential bias is in the median RMS as a function of N.    This change will increase our reported AMx reported in squash.",1
"DM-18642","03/27/2019 07:15:10","Fix ATHexpod CSC bugs","Fix ATHexapod bugs, this includes:   * Update motion detection to use in-built queries inside the PI controller properly  {color:#00875a}*(DONE by waiting until is ready for next command after doing any motion)*{color}   * Define pivot setup at the start properly. The pivot cannot be configure if the hexapod is in an angle, so trying to configure when starting if the hexapod was left with an angle different to 0, will fail the start command. Need to be discussed with Patrick. *{color:#00875a}(DONE by removing Pivot from settings){color}*   * Prior to executing command, check if the controller is ready to receive command, if not wait until ready (with a certain timeout)  *{color:#00875a}(DONE by waiting until is ready for next command after doing any motion){color}*   * Fix initialization, current initialization fails due that it takes some time to reference the hexapod at the start. If we don't initialize the Hexapod and there's no reference due to reference lost (lost of power), we won't be able to command it.Need to be discussed with Patrick. {color:#00875a} *(DONE by waiting until is ready for next command after doing any motion)*{color}",2
"DM-18655","03/27/2019 12:46:46","Improve Electrometer documentation in the XML","    Improve Electrometer documentation in the XML",1
"DM-18656","03/27/2019 12:46:54","Version 5.1 of PyYAML breaks DMCS if safe_load() is used","An upgrade to PyYAML 5.1 now gives a warning on using the yaml.load() method.   Switching to yaml.safe_load() causes the DMCS to fail with the following:      {code:python}  [srp@lsst-l1-cl-dmcs iip]$ python DMCS.py  Extracting values from Config dictionary L1SystemCfg.yaml  Logs will be written to /tmp/DMCS.log  CONSUMER THREADS: amqp://DMCS:DMCS@141.142.238.10:5672/%2ftest_at_srp  DMCS seems to be working  In On OCS Msg, msg is: {'ACK_ID': 'START_INT_ACK_76', 'IMAGES_IN_SEQUENCE': '3', 'IMAGE_ID': 'AT_C_20181111_000602', 'IMAGE_INDEX': '2', 'IMAGE_SEQUENCE_NAME': 'MAIN', 'MSG_TYPE': 'DMCS_AT_START_INTEGRATION', 'REPLY_QUEUE': 'dmcs_ack_consume'}  Processing message in OCS message callback  Message and properties from DMCS callback message body is: %s (""{'ACK_ID': 'START_INT_ACK_76', 'IMAGES_IN_SEQUENCE': '3', 'IMAGE_ID': 'AT_C_20181111_000602', 'IMAGE_INDEX': '2', 'IMAGE_SEQUENCE_NAME': 'MAIN', 'MSG_TYPE': 'DMCS_AT_START_INTEGRATION', 'REPLY_QUEUE': 'dmcs_ack_consume'}"", <BasicProperties>)  enabled is set to: False  In On OCS Msg, msg is: {'ACK_ID': 'READOUT_ACK_77', 'IMAGES_IN_SEQUENCE': '3', 'IMAGE_ID': 'AT_C_20181111_000602', 'IMAGE_INDEX': '2', 'IMAGE_SEQUENCE_NAME': 'MAIN', 'MSG_TYPE': 'DMCS_AT_END_READOUT', 'RESPONSE_QUEUE': 'dmcs_ack_consume'}  Processing message in OCS message callback  Message and properties from DMCS callback message body is: %s (""{'ACK_ID': 'READOUT_ACK_77', 'IMAGES_IN_SEQUENCE': '3', 'IMAGE_ID': 'AT_C_20181111_000602', 'IMAGE_INDEX': '2', 'IMAGE_SEQUENCE_NAME': 'MAIN', 'MSG_TYPE': 'DMCS_AT_END_READOUT', 'RESPONSE_QUEUE': 'dmcs_ack_consume'}"", <BasicProperties>)  In INCR scbd, new ack_id is AT_END_READOUT_ACK_001542  Exception in thread Thread-dmcs_ack_consume:  Traceback (most recent call last):    File ""/usr/lib64/python3.6/threading.py"", line 916, in _bootstrap_inner      self.run()    File ""/home/srp/dev/ctrl_iip/python/lsst/ctrl/iip/Consumer.py"", line 365, in run      self._connection.ioloop.start()    File ""/usr/local/lib/python3.6/site-packages/pika/adapters/select_connection.py"", line 461, in start      self._poller.start()    File ""/usr/local/lib/python3.6/site-packages/pika/adapters/select_connection.py"", line 721, in start      self.poll()    File ""/usr/local/lib/python3.6/site-packages/pika/adapters/select_connection.py"", line 1114, in poll      self._dispatch_fd_events(fd_event_map)    File ""/usr/local/lib/python3.6/site-packages/pika/adapters/select_connection.py"", line 831, in _dispatch_fd_events      handler(fileno, events)    File ""/usr/local/lib/python3.6/site-packages/pika/adapters/base_connection.py"", line 410, in _handle_events      self._handle_read()    File ""/usr/local/lib/python3.6/site-packages/pika/adapters/base_connection.py"", line 464, in _handle_read      self._on_data_available(data)    File ""/usr/local/lib/python3.6/site-packages/pika/connection.py"", line 2041, in _on_data_available      self._process_frame(frame_value)    File ""/usr/local/lib/python3.6/site-packages/pika/connection.py"", line 2175, in _process_frame      self._deliver_frame_to_channel(frame_value)    File ""/usr/local/lib/python3.6/site-packages/pika/connection.py"", line 1568, in _deliver_frame_to_channel      self._channels[value.channel_number]._handle_content_frame(value)    File ""/usr/local/lib/python3.6/site-packages/pika/channel.py"", line 996, in _handle_content_frame      self._on_deliver(*response)    File ""/usr/local/lib/python3.6/site-packages/pika/channel.py"", line 1125, in _on_deliver      header_frame.properties, body)    File ""/home/srp/dev/ctrl_iip/python/lsst/ctrl/iip/YamlHandler.py"", line 39, in yaml_callback      pydict = self.decode_message(body)    File ""/home/srp/dev/ctrl_iip/python/lsst/ctrl/iip/YamlHandler.py"", line 50, in decode_message      tmpdict = yaml.safe_load(body)    File ""/usr/local/lib64/python3.6/site-packages/yaml/__init__.py"", line 162, in safe_load      return load(stream, SafeLoader)    File ""/usr/local/lib64/python3.6/site-packages/yaml/__init__.py"", line 114, in load      return loader.get_single_data()    File ""/usr/local/lib64/python3.6/site-packages/yaml/constructor.py"", line 43, in get_single_data      return self.construct_document(node)    File ""/usr/local/lib64/python3.6/site-packages/yaml/constructor.py"", line 52, in construct_document      for dummy in generator:    File ""/usr/local/lib64/python3.6/site-packages/yaml/constructor.py"", line 404, in construct_yaml_map      value = self.construct_mapping(node)    File ""/usr/local/lib64/python3.6/site-packages/yaml/constructor.py"", line 210, in construct_mapping      return super().construct_mapping(node, deep=deep)    File ""/usr/local/lib64/python3.6/site-packages/yaml/constructor.py"", line 135, in construct_mapping      value = self.construct_object(value_node, deep=deep)    File ""/usr/local/lib64/python3.6/site-packages/yaml/constructor.py"", line 92, in construct_object      data = constructor(self, node)    File ""/usr/local/lib64/python3.6/site-packages/yaml/constructor.py"", line 420, in construct_undefined      node.start_mark)  yaml.constructor.ConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:datetime.time'    in ""<byte string>"", line 2, column 14:      EXPIRY_TIME: !!python/object/apply:datetime.time  {code}  ",1
"DM-18667","03/27/2019 14:42:53","Add support to display_firefly for obtaining and passing along an authorization token for Firefly","Based on the API support for authentication tokens to be added to {{firefly_client}} under DM-18666, this ticket asks that {{display_firefly}} be modified to support both:  * manually supplying an authorization token for Firefly when the display object is created, and  * automatically picking up an appropriate token from a TBD mechanism in the Python process environment established by the Notebook Aspect (e.g., this may be from an environment variable with an agreed-upon name, or from a dot-file).    The second subtask requires, of course, that the ""TBD mechanism"" has been defined.  This will require work on the Notebook Aspect / Nublado as well.    In each case, the token should then be passed down to the underlying {{firefly_client}} object.    Care must be taken to be aware the different servers that might be used and to respect overrides that may be in place.  The ""second subtask"", it-just-works, method is initially only required to work for whatever the active LSP-instance default Firefly server is (including any system-configured overrides to that).  If the _user_ wishes to use a different server, s/he may be required to supply an appropriate token manually.",3
"DM-18671","03/27/2019 15:43:07","Add ts_sal unit tests for DM-18491 and DM-18637","Add ts_sal unit tests to test DM-18491 current time issues and DM-18637 get newest topic after get oldest does not always return the most recent value.",1
"DM-18674","03/28/2019 05:59:01","Re-package Electrometer","This task is to re-package electrometer to follow new standards and make it work with dm-stack (and Tiagos Docker image).",3
"DM-18675","03/28/2019 06:56:27","Improve Electrometer documentation inside python","     Improve Electrometer documentation inside python",1
"DM-18676","03/28/2019 07:02:22","Finish first release for electrometer","    Clean up Electrometer python code and add limits when applies (for example integration time should be limited). Also create a docker image for deployment.    Update:    * Configuration to use the API from salobj   * update intensity in events to ""CurrentValue"" or ""Measurement"" according to Patricks comments in [https://github.com/lsst-ts/ts_xml/pull/83#discussion-diff-279486587R15]",3
"DM-18677","03/28/2019 07:07:06","Test ATHexapod CSC fixed at the summit","Test ATHexapod CSC fixes from at the summit. Fixed should be done in: https://jira.lsstcorp.org/browse/DM-18642",2
"DM-18678","03/28/2019 08:40:56","Update to boost 1.69","Clang version 10.0.1 (clang-1001.0.46.3) on macOS Mojave can not build boost 1.68.  v1.69 looks to have some fixes for the problem so we need to upgrade (Jenkins currently uses an older clang so may not have the problem since we know 1.68 builds fine with version 10.0.0 (clang-1000.10.44.4)).",2
"DM-18683","03/28/2019 10:41:44","Review Craig's PR and merge","[~cslage] has submitted a PR here:    https://github.com/lsst/cp_pipe/pull/10/    I will make any changes necessary to bring this to comply with DM standards (effectively doing a ""review"" but in the more efficient way where I just make the changes - quicker for both parties here). I will then check with Craig that I haven't ruined his work, and merge.    [~swinbank] has blessed this model.    ",40
"DM-18685","03/28/2019 10:53:12","Change all yaml.load() instances to yaml.safe_load()","There are a number of yaml.load() invocations that need to be changed to yaml.safe_load().  This may require a fix similar to what was implemented in YamlHandler.py, for DM-18656.",0.5
"DM-18686","03/28/2019 11:03:27","represent time properly throughout ctrl_iip python code","While debugging something, I saw a date being constructed in the DMCS add_seconds method did this:    {code:python}      def add_seconds(self, intime, secs):          basetime = datetime.datetime(100, 1, 1, intime.hour, intime.minute, intime.second)          newtime = basetime + datetime.timedelta(seconds=secs)          return newtime.time()  {code}    (Day is set to Jan 1, year 100).    Which it shouldn't do.  A survey throughout this code must be done to fix things like this and represent time in a consistent manner.",1
"DM-18689","03/28/2019 11:51:33","Fix scisql build with Python 3.7","Environment update to Python 3.7 seems to have busted the scisql build.  The problem seems to be an incompato in the ""waf"" build too used by scisql.  The waf project has already implemented a fix; this needs to be tested with scisql and upstreamed there if it addresses the build issue.",0.5
"DM-18690","03/28/2019 11:52:36","Implement maximum record limit in TAP query screens in Firefly","04/03/2019    [https://github.com/Caltech-IPAC/firefly/pull/783]    ""Row Limit"" field is displayed at the bottom of the TAP Search, along with ""Search"" and ""Cancel"" buttons. It is propagated to the server and is translated into MAXREC parameter of the search request. We allow to disable it by clearing.    tap.maxrec.hardlimit property (default 10000000) is a config property. If MAXREC is set, it can not exceed this value.  defaultMaxrec is an application property (default 50000 for Firefly). It is the default value for MAXREC parameter.    Other changes:   * Filters are now shown by default in the TAP result tables   * Removed 'tables.showInfoButton' app property and made it a table option instead.   * Fixed Service URL select box style: the text was truncated a bit on the bottom and selected item was shown in white, which was hard to see on light green.    03/28/2019    In the Firefly TAP search UI, we need a way to specify an upper limit on the size of the query result.    This limit should be independent of any ""TOP (nnn)"" clause that may be included in the user's ADQL query text. If both are present, the desired effect is that the smaller of the two limits will determine the size of the query result.    After extensive discussion in the IPAC LSST group, we propose to put the UI element for this limit in the ""grey bar"" at the bottom of the TAP search screens in which the ""Search"" and ""Cancel"" buttons already appear. The UI element must appear in both ""single table"" mode and in the type-my-own-ADQL mode, and _should be in exactly the same location in the grey bar_ so that it does not ""move"" when changing modes. (This emphasizes that it is a global property of TAP searches via Firefly.)    The UI element should allow the user to type in a non-negative integer (yes, I think 0 should be allowed - see the IVOA standards' discussion of {{MAXREC=0}} for the reason), as well as to select suggested numbers from a menu. The menu must always contain at least one entry: a ""hard limit"" that cannot be exceeded even by typing. The value here should be the lesser of a ""Firefly hard limit"" (which should be a Firefly application-level configuration parameter) designed to preserve correct, if perhaps sluggish, performance, and any TAP-service-specific hard record limit (which might, for instance, have been returned from a {{.../capabilities}} endpoint query or from the Registry).    The native Firefly hard limit on a good-sized server with the current implementation might be somewhere in the 2M-5M range.    A _default_ value should also be provided in the menu, and pre-selected in a new search session. This should also be controlled by a configuration parameter so that it can be application-specific, and this should be coded so that it can be overridden by a number obtained from the Registry. (A {{defaultMaxRecords}} parameter does exist.) A number like 5000 may be appropriate here.    *This ticket does not ask for the work of actually querying the Registry or the {{.../capabilities}} endpoint.* The work on this ticket is only intended to _facilitate_ adding those features at a later date.    The above describes the *UI*.    The *implementation* should ultimately be based on the use of the {{MAXREC}} DALI parameter to the underlying TAP service. However, we first need to determine whether LSST's TAP service supports this (and check IRSA and NED as well). It may be that we need a fallback to editing the ADQL query to add, or, if necessary, modify a user-provided {{TOP (nnn)}} clause.",8
"DM-18703","03/28/2019 20:34:58","constructFlat.py --config isr.doCrosstalkBeforeAssemble=False raises a LengthError exception","Here's a full command line that produces the error:  {code:java}  constructFlat.py output --rerun jchiang/xtalk_after_config \  --id raftName=R22 detectorName=S11 --config isr.doBias=False \  isr.doDark=False isr.doCrosstalkBeforeAssemble=False --longlog \  --batch-type=None{code}  and the last several lines of the traceback (full log attached):  {code:java}    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/ip_isr/17.0.1-7-g69836a1+1/python/lsst/ip/isr/isrTask.py"", line 1195, in run      self.crosstalk.run(ccdExposure, crosstalkSources=crosstalkSources)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/ip_isr/17.0.1-7-g69836a1+1/python/lsst/ip/isr/crosstalk.py"", line 90, in run      crosstalkStr=self.config.crosstalkMaskPlane)    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/ip_isr/17.0.1-7-g69836a1+1/python/lsst/ip/isr/crosstalk.py"", line 212, in subtractCrosstalk      jImage = extractAmp(mi, jAmp, iAmp.getReadoutCorner())    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/ip_isr/17.0.1-7-g69836a1+1/python/lsst/ip/isr/crosstalk.py"", line 130, in extractAmp      output = image[amp.getBBox() if isTrimmed else amp.getRawDataBBox()]    File ""/software/lsstsw/stack_20181012/stack/miniconda3-4.5.4-fcd27eb/Linux64/afw/17.0.1-4-g41c8d5dc0+1/python/lsst/afw/image/slicing.py"", line 262, in __getitem__      return self.subset(box, origin=origin)  lsst.pex.exceptions.wrappers.LengthError:     File ""src/image/Image.cc"", line 83, in static lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const Extent2I&, const Extent2I&, const _view_t&) [with PixelT = float; lsst::afw::image::ImageBase<PixelT>::_view_t = boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float, boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t> > >*> > >; lsst::geom::Extent2I = lsst::geom::Extent<int, 2>]      Box2I(Point2I(3840,0),lsst::geom::Extent2I(509,2000)) doesn't fit in image 4072x4000 {0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(3840,0),lsst::geom::Extent2I(509,2000)) doesn't fit in image 4072x4000'{code}",1
"DM-18708","03/29/2019 09:02:59","Investigate ingest problems with BOT data","There have been reports of some issues with ingest of BOT flat field data. [~tjohnson] has given me a file to use for the investigation.",1
"DM-18723","03/29/2019 12:22:37","Support M2 mirror coating chamber activity","Support M2 mirror coating chamber activity in Chile",5
"DM-18726","03/29/2019 13:03:18","Fix Qserv container builds","Qserv container builds seem to be now picking up binary tarballs, which is causing a shebang lossage with {{pytest}} when {{eups distrib}} tries to build the {{db}} package.    Proposed fix is to first install {{pytest}}, then run {{shebangatron}}, then install {{qserv}}.",0.5
"DM-18729","03/29/2019 13:16:30","Use the Yaml Configuration File Format Phase 1","Use the yaml configuration file format phase 1. This task will use the yaml to manage the configuration files. This task is the phase 1.",1
"DM-18730","03/29/2019 13:20:19","Research the InfluxDB and Kafka","Do the research of InfluxDB and Kafka and learn how to use them. This is based on the discussion of EFD meeting:    [https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LTS&title=2019-03-29+EFD+Meeting+notes]     ",3
"DM-18731","03/29/2019 13:40:14","Configure MariaDB + efdwriters and Influx + efdwriters in La Serena","Configure MariaDB + efdwriters and Influx + efdwriters in La Serena and run MTM1M3 simulator for testing ",1
"DM-18737","03/29/2019 15:05:18","Support for M2 Hexapod System Ready for Integration","Support for M2 Hexapod System Ready for Integration         Corresponding P6 Activity:   * T&SC-140403-1300   ** M2 Hexapod System Ready for Integration",5
"DM-18738","03/29/2019 15:07:06","Support for Camera Hexapod installation and test with CCW","Support for Camera Hexapod installation and test on TMA     Corresponding P6 Activity:   * T&SC-140403-1700   ** M2 Hexapod installation and test on TMA         CCW: Camera Cable Wrap.",5
"DM-18743","03/29/2019 17:29:30","Improve tracking of the asynchronous operations (jobs/requests) in the Replication System","The following pattern is often seen in the current implementation of the Replication system:  {code}  atomic<bool> finished(false);  auto const request = controller()->sql(      worker,      query,      user,      password,      maxRows,      [&finished] (SqlRequest::Ptr const& request) {          finished = true;      }  );  util::BlockPost blockPost(10, 20);  // for random delays (in milliseconds) between iterations  while (not finished) {      blockPost.wait();  }  {code}  And while it has its own benefits (such as implementing a periodic heartbeat reports on the operations, or an ability to cancel long operations after a certain timeout which might be shorter than the one the asynchronous activity would normally last for), the pattern suffers from certain problems:  * the same code appear in multiple (a few dozens as of today) locations  * it requires making non-obvious choices on the duration of intervals in the BlockPost class's constructor.    Hence a goal of this effort is to implement a more efficient method for synchronizing threads with the asynchronous operations of the *Request* and *Job* classes.    Further details on this subject, including in-depth analysis of the proposals, and the final choice can be found at the following document:  * https://confluence.lsstcorp.org/pages/viewpage.action?pageId=102471961",2
"DM-18744","03/31/2019 08:49:00","Create presentation for DESC BTF","Write my portion of presentation on the LSST deblender for the April 1st DESC Blending Task Force meeting. This will describe the work to use a new normalization scheme, choosing the proper PSF model, issues with PSF convolution in HSC and (likely) LSST, and the new architecture.",2
"DM-18748","04/01/2019 08:36:12","Missing ExpTime in auxTel teststand images cause ingestImages.py to abort","3 auxtel teststand images from 03/29/2019 failed to ingest with the following output:    {code:java}  $ ingestImages.py /project/production/tmpdataloc/auxTel/gen2repo --calib /project/production/tmpdataloc/auxTel/gen2repo/CALIB --ignore-ingested --doraise /project/production/tmpdataloc/auxTel/storage/2019-03-29/AT_O_20190329_000022-ats-wfs_ccd.fits  root INFO: Loading config overrride file '/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/obs_lsst/17.0.1-8-g5a63230+6/config/ingest.py'  root INFO: Loading config overrride file '/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/obs_lsst/17.0.1-8-g5a63230+6/config/auxTel/ingest.py'  CameraMapper INFO: Loading exposure registry from /project/production/tmpdataloc/auxTel/gen2repo/registry.sqlite3  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'datetime_end' using translator <class 'lsst.obs.lsst.translators.auxTel.LsstAuxTelTranslator'>: ""Could not find ['EXPTIME'] in header""  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'exposure_time' using translator <class 'lsst.obs.lsst.translators.auxTel.LsstAuxTelTranslator'>: ""Could not find ['EXPTIME'] in header""  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'dark_time' using translator <class 'lsst.obs.lsst.translators.auxTel.LsstAuxTelTranslator'>: ""Could not find ['DARKTIME', 'EXPTIME'] in header""  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'boresight_airmass' using translator <class 'lsst.obs.lsst.translators.auxTel.LsstAuxTelTranslator'>: ""Could not find ['AMSTART'] in header""  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'boresight_rotation_angle' using translator <class 'lsst.obs.lsst.translators.auxTel.LsstAuxTelTranslator'>: ""Could not find ['ROTANGLE'] in header""  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'object' using translator <class 'lsst.obs.lsst.translators.auxTel.LsstAuxTelTranslator'>: ""Could not find ['OBJECT'] in header""  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'observation_type' using translator <class 'lsst.obs.lsst.translators.auxTel.LsstAuxTelTranslator'>: ""Could not find ['EXPTIME'] in header""  ingest.parse WARN: translate_expTime failed to translate expTime: 'NoneType' object has no attribute 'value'  ingest.parse WARN: translate_imageType failed to translate imageType: 'NoneType' object has no attribute 'upper'  ingest INFO: /project/production/tmpdataloc/auxTel/storage/2019-03-29/AT_O_20190329_000022-ats-wfs_ccd.fits --<link>--> /project/production/tmpdataloc/auxTel/gen2repo/raw/2019-03-29/2019032900022-det000.fits  Traceback (most recent call last):    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/17.0.1-5-gf0ac6446+8/bin/ingestImages.py"", line 3, in <module>      IngestTask.parseAndRun()    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/17.0.1-5-gf0ac6446+8/python/lsst/pipe/tasks/ingest.py"", line 416, in parseAndRun      task.run(args)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/17.0.1-5-gf0ac6446+8/python/lsst/pipe/tasks/ingest.py"", line 553, in run      self.register.addRow(registry, info, dryrun=args.dryrun, create=args.create)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/17.0.1-5-gf0ac6446+8/python/lsst/pipe/tasks/ingest.py"", line 357, in addRow      values = [self.typemap[tt](info[col]) for col, tt in self.config.columns.items()]    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/17.0.1-5-gf0ac6446+8/python/lsst/pipe/tasks/ingest.py"", line 357, in <listcomp>      values = [self.typemap[tt](info[col]) for col, tt in self.config.columns.items()]  KeyError: 'expTime'  {code}        {code:java}  translate_header.py -p lsst.obs.lsst.translators /project/production/tmpdataloc/auxTel/storage/2019-03-29/AT_O_20190329_000022-ats-wfs_ccd.fits  Analyzing /project/production/tmpdataloc/auxTel/storage/2019-03-29/AT_O_20190329_000022-ats-wfs_ccd.fits...  KeyError(""Error calculating property 'datetime_end' using translator <class 'lsst.obs.lsst.translators.auxTel.LsstAuxTelTranslator'> and file /project/production/tmpdataloc/auxTel/storage/2019-03-29/AT_O_20190329_000022-ats-wfs_ccd.fits"")  Files with failed translations:   /project/production/tmpdataloc/auxTel/storage/2019-03-29/AT_O_20190329_000022-ats-wfs_ccd.fits  {code}  ",3
"DM-18749","04/01/2019 09:10:11","Create defects file for the AuxTel sensor","Deliverable is the defects for the AuxTel sensor in the right format for it to be applied by ISR.    This work follows from some work being done by [~plazas] on TS8 data, so shouldn't be too hard to run that code, but the output will need putting into the right format for ISR application. See {{obs_subaru}} for details on how this is currently done (I think an ASCII file is parsed at scons-time and turned into a FITS file which is then retrieve by the butler, but _not_ via the calib registry, but I could be wrong).",8
"DM-18750","04/01/2019 09:16:56","Make ScriptQueue automatically find standard and external script directories","Enhance ScriptQueue to find the standard and external script directories if those constructor arguments are None, make None the default, and make the command line arguments optional.    The default values will be given by environment variables {{TS_STANDARDSSCRIPTS_DIR}} and {{TS_EXTERNALSCRIPTS_DIR}}. If one uses eups then these will be automatically defined. For deployment one must either explicitly define these or provide the information on the command line.",1
"DM-18753","04/01/2019 10:34:50","Deploy docker containers for key auxtel simulators","Using Tiago's docker container method, deploy key Auxiliary Telescope simulator containers down in lab. ",2
"DM-18754","04/01/2019 10:38:59","macOS Jenkins nodes need to change default matplotlib backend","The nightly failed because macOS mojave does not interact well with the JSON loading test of validate_drp.  Some of the plotting tests result in a segv with the TkAgg matplotlib backend (some strange interaction with tkinter and macOS).  The fix is to add {{~/matplotlib/matplotlibrc}} to the worker nodes and have a line in that file saying {{backend : Agg}}.",1
"DM-18781","04/01/2019 13:58:14","Mojave on jenkins are failing to find pip","The mojave builds are failing with the following error:  {quote}  + pip install virtualenv  /Users/square/j/ws/release/tarball/osx/10.9/clang-1000.10.44.4/miniconda3-4.5.12-1172c30_tmp/durable-67d1fe51/script.sh: line 3: pip: command not found  script returned exit code 127  {quote}    An example failing build is [here|https://ci.lsst.codes/blue/organizations/jenkins/release%2Ftarball/detail/tarball/4040/pipeline/].  Which is calling code [here|https://github.com/lsst-sqre/jenkins-dm-jobs/blob/fd5ab2e1c2b73eb8ec08550bf7e9bcfa125154a3/pipelines/release/tarball.groovy#L610]",1
"DM-18838","04/01/2019 19:28:20","Fix butlerRoot in OracleRegistry","The butlerRoot change missed a file in the Oracle registry, this fixes that.",1
"DM-18839","04/01/2019 19:38:26","Remove explicit registry close in the butler","In working with oracle, exceptions were being thrown related to connections not being closed properly, leading to the addition of the __del__ method in the butler to make sure the connections were properly cleaned up in registry held by the butler.    This however had the side effect of creating an exception when a butler was used in a single threaded way. The temporary solution to that was to add an explicit close method to the butler to make sure everything was finalized before garbage collection (as __del__ is not predicable in the timing or ordering when it is evoked.    This ticket will explore why this is all necessary, and address what is causing the unexpected behavior. ",8
"DM-18840","04/02/2019 09:23:11","Test Kafka with M1M3 simulator data","Test the performance of writing the EFD data to the Kafka brokers  provided by the DM test setup, use the M1M3 simulator to generate the maximum  data rate",1
"DM-18841","04/02/2019 09:25:34","Install InfluxDB and writers to EFD test machine","Install  and test the EFD writers for Influxdb built against the latest OpenSplice",3
"DM-18842","04/02/2019 09:29:59","Install SAL 3.9 built EFD writers to test pLAN machine","Build the EFD writers against SAL 3.9 and deploy to a test machine on the pLAN  cluster to evaluate compatibility issues",2
"DM-18846","04/02/2019 10:15:39","MTDome SW Planning Meetings at Vendor Site (EIE-Italy)","MTDome SW Planning Meetings at Vendor Site (EIE-Italy)",5
"DM-18847","04/02/2019 10:16:19","MTDome SW Planning Meetings at Vendor Site (EIE-Italy)","MTDome SW Planning Meetings at Vendor Site (EIE-Italy)",5
"DM-18849","04/02/2019 10:33:57","Make crosstalk coefficient calculation code in obs_subaru generic","Take the code in https://github.com/lsst/obs_subaru/blob/master/python/lsst/obs/subaru/crosstalk.py and:    * Make it camera-agnostic  * Make it comply with coding standards  * Put it in {{cp_pipe}} (the old code can be removed later, so this needn't be considered a _move_, it's just a new cp_pipe task for now)    Then, use this task and some darks from the AuxTel chip to calc xtalk coefficients for DM-18050.    If it looks like the code if specifically for working with darks then the task should be named as such. If it looks like it will be sufficiently generic to work with saturated stars/CBP exposures then it should have a more generic name.    Output should be a written with a {{butler.put()}} with an appropriate dataset type defined in {{obs_base datasets.yaml}}.",8
"DM-18851","04/02/2019 11:23:10","Spec and create requisition for phase 2 EFD hardware","Specify the configuration and price the phase 2 EFD servers, then   create a requisition for their purchase. Separately spec the add-ons  (nVME, SSD, HD)",1
"DM-18855","04/02/2019 11:55:06","Pex exceptions TypeError should not inherit from RuntimeError","In working on DM-9873 I was getting confusing test failures because sometimes a pex_exceptions C++ TypeError was turning up in Python as a RuntimeError.  It transpires that DM-9435 changed the C++ side such that TypeError inherits from LogicError but did not change the Python interface wrapper.  This ticket will change the Python side to inherit from LogicError rather than RuntimeError.",1
"DM-18860","04/02/2019 12:43:30","Update ATDome to be configured in the standard way","Make ATDome configurable in the standard fashion:  * Update ATDome to inherit from {{salobj.ConfigurableCsc}}  * Add a schema  * Add an ATDome directory to {{ts_config_attcs}}.",2
"DM-18861","04/02/2019 12:49:14","Enhance ""how to write a SAL script"" documentation","Improve the ""how to write a SAL script"" documentation to add clarity and fix glitches.",1
"DM-18863","04/02/2019 12:57:36","Investigate and fix warnings in daf_butler","Changes to daf_butler introduced some behavior that cause sqlalchemy deprecation warnings to be thrown. This ticket will find out why they are being thrown and attempt to fix our code or prevent the warnings from being thrown.",2
"DM-18864","04/02/2019 15:48:24","Update afw to support undefined values in FITS headers","DM-9873 added support for {{None}} to be added to PropertyList. Update afw FITS reading to support undefined values.",1
"DM-18869","04/03/2019 09:44:14","LabVIEW Training","Time spent going to Labview training. ",5
"DM-18874","04/03/2019 13:04:50","Multiple threads use single Pika publisher","ATArchiver (and possibly other code) uses multiple threads to service consumer callbacks, but they all share a single publisher.  Pika specifically warns against doing this.       {quote}  Is Pika thread safe?    Pika does not have any notion of threading in the code. If you want to use Pika with threading, make sure you have a Pika connection per thread, created in that thread. It is not safe to share one Pika connection across threads, with one exception: you may call the connection method add_callback_threadsafe from another thread to schedule a callback within an active pika connection.  {quote}  ",0
"DM-18875","04/03/2019 13:27:23","test upgrades of out-of-date python support packages for ctrl_iip","Pika is very out of date, with the release we're using, 0.11.2, out in November 30, 2017.  There have been nine releases since then, and the most current version is 1.0.0, released on March 27th, 2019.         Usually this wouldn't be a big deal, but there are constructor and method signature changes between these releases, so we have to go through and get the code changed.   This is going to have to be tested on a separate system that we're using now.      Deploying this when it goes live is going to have to be a staged deployment, since we have to make sure all the machines we are deploying to will have the current version of ctrl_iip that supports it.",2
"DM-18881","04/03/2019 16:28:50","Provide Bob Blum with an LSP account","See triggering ticket for discussion.",1
"DM-18885","04/04/2019 00:36:15","Log number of MeasuredStars in Associations","Looking at the recent log from the failed 300 visit jointcal run by [~ikedahr], I noticed that I don't appear to ever log the total number of MeasuredStars in jointcal. This is an important number to help estimate memory pressure.    It's probably simplest to count and log it at the end of {{Associations::selectFittedStars()}}.",0.5
"DM-18887","04/04/2019 11:51:59","Add privileges to the T&S S3 role","The T&S group would like some added privileges for their S3 bucket credentials.  Specifically, they would like to execute {{aws s3 website s3://sal-topic-registry --index-document index.html}}.  This is so they can serve things directly from the buckets.    They would also like delete privileges.",0.5
"DM-18895","04/05/2019 06:13:49","Use std::ptrdiff_t as index type in jointcal Eigen objects","[~price] noticed that the use of {{unsigned}} (== uint32, usually) as the index type for Eigen matrices in jointcal _might_ be a problem:   * Eigen docs appear to want a signed integer;   * our number of sources in some of our processing is approaching 2^31.    There's no hard evidence that this is what's causing the segfault we're seeing when processing the (larger, still-proprietary) HSC UltraDeep, and since {{unsigned}} should be able to handle 2^32, I'm personally skeptical that this is the problem.  But it should be easy to fix, and a good thing to do regardless, especially for the first reason.    {{std::ptrdiff_t}} is probably the right choice to use instead - that will be a signed integer as large as the pointer type (i.e. int64, usually), and hence the largest unsigned type that's actually usable as an index.     ",1
"DM-18905","04/05/2019 16:10:11","Use safe YAML loading in dax_ppdb","{{dax_ppdb}} produces some warning messages about use of the unsafe and now-deprecated {{yaml.load}}. Please update the code as described on [Community|https://community.lsst.org/t/update-of-pyyaml/3631].",1
"DM-18906","04/05/2019 16:12:00","Use safe YAML loading in verify","{{verify}} produces some warning messages about use of the unsafe and now-deprecated {{yaml.load}}. Please update the code as described on [Community|https://community.lsst.org/t/update-of-pyyaml/3631].",1
"DM-18911","04/06/2019 18:45:07","Evaluate dds for ts_salobj using asyncio","Do a preliminary evaluation of the feasibility of using the OpenSplice dds package for ts_salobj. See how it can be made to work with asyncio.    Start with [~dmills]'s feature/Prototyping_for_ddes_python_api branch of ts_sal.",2
"DM-18914","04/08/2019 08:30:46","Jenkins docs build failure","As of 2019-04-08, [Jenkins is failing|https://ci.lsst.codes/job/sqre/job/infra/job/documenteer/436/display/redirect]. Error is:  {code}      File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx/environment/__init__.py"", line 785, in get_doctree      with open(doctree_filename, 'rb') as f:  FileNotFoundError: [Errno 2] No such file or directory: '/j/ws/sqre/infra/documenteer/doc_template/_build/doctree/py-api/lsst.meas.base.PhotoCalib.doctree'    Exception occurred:    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx/environment/__init__.py"", line 785, in get_doctree      with open(doctree_filename, 'rb') as f:  FileNotFoundError: [Errno 2] No such file or directory: '/j/ws/sqre/infra/documenteer/doc_template/_build/doctree/py-api/lsst.meas.base.PhotoCalib.doctree'  {code}",1
"DM-18917","04/08/2019 11:47:28","CPP single file program output update","Currently the output generated from the c++ single file programs are not descriptive enough. This makes it difficult to be parsed by the current automated test pipeline in place. This task is to modify the output to be more descriptive.     Example of the desired output is in the following link  https://docs.google.com/document/d/1M3ZDKKYpGXBDOgwese2kYT_cisGXg8y6JT040SHNxPU/edit",1
"DM-19015","04/08/2019 13:00:34","HSC warp making is broken with doApplyUberCal=True","makeCoaddTempExp.py with doApplyUberCal=True to use meas_mosaic outputs fail to build warps despite the input data exist. A small example to reproduce on lsst-dev is:       {code:java}makeCoaddTempExp.py /datasets/hsc/repo/ --rerun RC/w_2019_14/DM-18300:private/user/name --id tract=9615 filter=HSC-G patch=4,8 --selectId ccd=0..8^10..103 visit=26050^26036^26060^26032   {code}  This doesn't really ""fail"" (and no logs in ERROR or FATAL level) but instead gives many confusing/misleading warnings and produce no outputs:       {noformat}WARN 2019-04-08T01:32:59.059 makeCoaddTempExp (DataId(initialdata={'tract': 9615, 'filter': 'HSC-G', 'patch': '4,8'}, tag=set()))(makeCoaddTempExp.py:341)- Calexp DataId(initialdata={'ccd': 103, 'visit': 26032, 'pointing': 1179, 'filter': 'HSC-G', 'field': 'SSP_WIDE', 'dateObs': '2015-03-25', 'taiObs': '2015-03-25', 'expTime': 150.0, 'tract': 9615}  , tag=set()) not found; skipping it: 'NoneType' object has no attribute'getInstFluxAtZeroMagnitude'  {noformat}       The log is misleading; this is DM-16537 about the generic exceptions. I troubleshoot a bit and trace to this line   [https://github.com/lsst/meas_mosaic/blob/6e395ac11a625c877374f99e2bed771b427835b6/python/lsst/meas/mosaic/updateExposure.py#L103]    where a non-empty header is obtained but no photoCalib is returned (that {{photCalib}} is {{None}} there). Feels like a bug in afw or meas_mosaic files; I paused there to file this ticket. Besides the bug, it'd be nice if the {{None}} calib is caught earlier.",1
"DM-19075","04/08/2019 15:11:14","jointcal is ignoring `writeChi2FilesOuterLoop`","When implementing DM-17956, I introduced a bug: jointcal should only set {{writeChi2Name}} in the outer minimization loop if {{writeChi2FilesOuterLoop}} is True. Otherwise, the ""outer loop"" chi2 CSV files are always written (which is bad, because they're big). Fortunately, it's an easy fix.",1
"DM-19127","04/08/2019 15:36:48","Update EDF/HeaderService LFO monitoring script","We need to auxiliary diagnostic scripts to monitor the payload from the LFO messages from HeaderService and EDF. These need t be updated.",2
"DM-19152","04/08/2019 17:47:52","Add a short sleep after every SAL function call","Add a short sleep after every SAL funciton call, as recommended by [~dmills] and implemented in ts_sal unit tests DM-19139.",1
"DM-19153","04/08/2019 18:06:14","Fix further spec definitions for validate_drp in verify_metrics","This also involves updating how {{validate_drp}} discovers specs in the report making process.",5
"DM-19154","04/08/2019 18:23:40","Add RemoteCommand.set_start","Add new coroutine RemoteCommand.set_start and appropriate unit tests.    Update ts_salobj and ts_scriptqueue to use the new capability.    This will be very easy, but I plan to wait until DM-18344 is merged in order to reduce conflicts.",1
"DM-19156","04/08/2019 20:38:23","Improvements to the deployments scripts to support the new Ingest system","This ticket has been split into multiple tickets to make the code review easier. The corresponding branch has been eliminated. And the PR request has been closed w/o reviews with notes indicating reasons for that. Please, disregard this ticket!",0
"DM-19162","04/09/2019 09:33:46","Update HS to run with new CCS code version","Need to update and test the HS once CCS is updated on the NCSA L1 TestStand to remove the 1e6 factor in the timeStamp that went from micro seconds to seconds",2
"DM-19166","04/09/2019 09:44:15","The SAL objects in unit tests should only publish or subscribe","The unit tests in ts_sal have some SAL objects that registered both as publishers and subscribers to the same topic. Until DM-18915 it seemed to work, but it was never a good idea. Update the tests accordingly.",1
"DM-19189","04/09/2019 12:57:20","Update flux limit for computing statistics to be based on S/N ","The qa scripts in {{pipe_analysis}} were written and tuned based entirely on HSC-SSP data.  Now that we are moving to generalizing the scripts to work on any dataset produced by the lsst stack, certain hard-coded values need to be adapted to be appropriate for the dataset of interest.  As [~erykoff] points out in DM-18635, one such setting that requires adjustment is the minimum flux limit used for computing the basic statistics (mean and stddev) for the various metrics.  While it is currently a config parameter (whose default was set for HSC and specified by RHL), it would be even better to set this limit based on a S/N cut (e.g. S/N>100 would also match what {{validate_drp}} is using).  Once the new setting has been tested and shown to be the better option, make it the default, but allow for an override option in the configs to force a fixed mag cut (i.e the current behavior).",8
"DM-19195","04/09/2019 14:27:43","Attempting to get or put a topic segfaults if not subscribed","Attempting to get or put an event or telemetry topic causes a segfault in SAL if one has not first subscribed to the event. This is demonstrated by the following unit tests in {{test_sal.py}}: {{test_evt_no_registration}} and {{test_tel_no_registration}}. Oddly enough this does not appear to cause a segfault for commands -- see {{test_cmd_no_registration}}.    In all cases I think this should raise an exception instead of segfaulting.    As part of fixing this issue please enable the unit tests listed above.    I consider this fairly low priority, despite being a segfault, because it is fairly easy to work around and ts_salobj seems to do a good job of preventing users from running into this.",2
"DM-19203","04/09/2019 14:42:53","Update generic event payload for settingsVersions event","Add fields to settingsVersions event and allow payloads to be maximum  variable string size",1
"DM-19204","04/09/2019 15:07:29","Include explanatory text in DASK notebooks.","Add explanatory text to the exploratory dask notebooks so users can follow along.",1
"DM-19207","04/09/2019 17:35:57","Remove deprecated Calib interfaces after next release","After v18 is released, we can remove the deprecated {{getCalib}}/{{setCalib}} accessors on {{Exposure}} (both in table, and image), {{ExposureInfo}}, and the deprecated {{Calib}}-style interface in {{PhotoCalib}}. ",1
"DM-19208","04/09/2019 17:44:32","Provide description for LDM-503-10a","Level 2 milestone LDM-503-10a has appeared in PMCS, but has the (very minimal) description ""LSP"".    Based on discussions between [~gpdf] and [~fritzm] the following text (from the linked Confluence page) is proposed for LDM-503-10a:  {quote}This test demonstrates the successful integration of a single-sign-on federated authentication system, and a basic authorization system, with the three Aspects of the LSST Science Platform (Portal, Notebook, and API), with the API Aspect containing at least a TAP service.  It will be demonstrated on a Kubernetes cluster provided by NCSA.  It is not required for authorization to be applied at the database level; it is sufficient for this milestone to apply at the TAP level.  Data served will remain that from the original PDAC work, i.e., SDSS Stripe 82 and/or WISE.  {quote}  In addition, we propose the creation of a further milestone with the following short description:  {quote}This test demonstrates the ingest and service of an ""LSST-like"" dataset in an instance of the LSST Science Platform at NCSA.  The dataset is expected to be based on the HSC public data release(s) and should include both images and catalogs.  The ingest should be performed through a version of the Science Data Model Standardization mechanism for generating concrete datasets compatible with the DPDD.  Catalogs should be available through TAP and queryable from both the Portal and Notebook Aspects.  Image metadata should be available through an ObsTAP service (and preferably also through an SIAv2 service, but this is not mandatory for this milestone), and images should be available from the URLs provided by the metadata service(s).  Image cutouts should be available through a SODA service.  Images should be queryable from both the Portal and Notebook Aspects, and there should be a well-defined relationship documented between API-based and Butler-based access to images.  {quote}  Note that these milestones reflect functional capabilities of the LSP itself, and represent steps on the way to the capabilities needed by LSST science users.  Other milestones capture issues associated with the usability of the LSP for commissioning activities.    Dates for these milestones will be determined in consultation with the LSP project manager, [~frossie].       ----  -Please provide a more descriptive title (-[~womullan] -suggests “LSP with Authtentication and TAP”) and a brief description of the contents of this milestone.-    -Refer to [https://github.com/lsst-dm/milestones/pull/10/files] for examples of the sort of descriptive text we're looking for. Something following the general pattern of:-  {quote}-""This test demonstrates the successful execution of a prototype Alert Generation science payload (LDM-148, LDM-151), processing data from precursor surveys at a relatively small scale using compute resources at the LSST Data Facility at NCSA.""-  {quote}  -or-  {quote}-This milestone records successful transfer of an image equivalent to one raft from the DAQ at the summit to reliable storage in the LSST Data Facility at NCSA, from where it will be made available for scientific evaluation through the LSST Science Platform.-  {quote}",1
"DM-19219","04/10/2019 13:07:44","M2 software deployment support","Give support to run the m2controller inside the cRio",2
"DM-19220","04/10/2019 13:30:51","Update ts_ATDomeTrajectory for changes in ts_ATDome v0.4.0","Read commanded azimuth from the new {{azimuthCommandedState}} ATDome event rather than {{position}} ATDome telemetry.  ",1
"DM-19221","04/10/2019 14:54:31","pyvo doesn't work with TAP async","Async queries don't work well with pyvo, because of uws:result XML elements in the response it looks like.  Works with TOPCAT.  Works without those elements around (this is how pyvo tests it)",2
"DM-19226","04/11/2019 07:16:47","Implement a framework for sampling monitoring parameters of Qserv workers ","Extend the Replication system to sample internal monitoring parameters (status, on-going queries, used chunks, etc.) from Qserv workers via XRootD/SSI. Collect and aggregate (if needed) results in memory of the Master Replication Controller. Extend the REST API of the Master Replication Controller to report results to a client application (Python tool or a Web application) for further display and analysis.    Specifically:  * extend the Protobuf protocol to allow requesting (from workers) and passing back (to the Master Replication Controller) the monitoring data  * implement a worker ""command"" (an XRootD/SSI service) sampling monitoring data and packaging it into a JSON object o be sent back to  the Master Replication Controller via XROOTD/SSI and Protobuf  * implement a *QservMgtRequst* and *Job* classes for making monitoring requests from the Replication system's controllers  * add REST handlers to class *replica::HttpProcessor*  * figure out (discuss with the colleagues) which monitoring parameters need to be sampled and define a schema of the JSON object sent back to the Controller  * implement a basic Web-based display for the monitoring samples ",8
"DM-19229","04/11/2019 15:18:54","Update dometrajectory_mcs script for changes to ATDome","Update the auxtel/integration_tests/dometrajectory_mcs script for changes to ATDome v0.4.0. Also fix the fact that although the script fails right now, the unit test still passes.    Finally, add the missing set_summary_state script (we have the code and the unit test, but no script) and see about adding a test of the script itself to the unit test.",1
"DM-19236","04/11/2019 21:15:02","Remove errant cout when reading old Calibs","Looks like one of my debug statements from writing the PhotoCalib code to read old calibs got left in. This ticket is to remove it.    {code}          std::cout << ""!!!!!!!!!!PhotoCalib: "" << calibration << "" "" << calibrationErr << std::endl;  {code}",0.5
"DM-19239","04/11/2019 21:59:36","M2 Support part 2","Give software support to M2 at the summit",2
"DM-19240","04/11/2019 22:13:19","exposed credentials for services","There are a large number of user/credential pairs in the YAML configuration files.  This should NOT be in GitHub, and should be in a secure directory in the user's space.  Additionally, there are about 28 user/credential pairs;  there should be one pair that all daemons use, since the extra pairs serve no useful purpose.",2
"DM-19242","04/11/2019 23:29:33","Recent changes to afw compression broke macos builds","https://github.com/lsst/afw/pull/447 seems to have broken macos builds.  Also might be unstable on centos?    https://ci.lsst.codes/blue/organizations/jenkins/stack-os-matrix/detail/stack-os-matrix/29661/tests",1
"DM-19243","04/12/2019 00:29:24","Investigate Kafka for Influx EFD implementation","Investigate Kafka for Influx EFD implementation.",3
"DM-19245","04/12/2019 16:06:53","Use the Yaml Configuration File Format Phase 2","This task will use the yaml to manage the configuration files. This task is the phase 2. I need to merge the lsst an comcam configuration files.",3
"DM-19246","04/12/2019 16:09:50","Document the Change of WEP Code","This task will document the change of code at this moment and test the sphinx as the documentation tool.",2
"DM-19247","04/12/2019 16:29:00","Evaluate the InfluxDB with SAL","This task will evaluate the integration between influxDB and influx_writer of SAL. This task will also benchmark the performance of InfluxDB to use the SSD, NVME, or hard disk.    Compare the MariaDB quary language and InfluxDB quary language.    Evaluation steps:   # Check with the AT CSC owners with the telemetry rate according to the page: [https://confluence.lsstcorp.org/display/SYSENG/Auxiliary+Telescope+Control+Computers+in+Tucson]   # Clone the ts_sal repository to the /home/ttsai. Install the SAL and build the AT CSC test scripts I need. Make sure the sal version is 3.9 and opensplice version is 6.9.   # Change the test telemetry script to run infinity times (iseq in C++) with the correct telemetry frequency (delay_1s in C, the first argument is sec and the second is nano-sec).   # Run the test for SSD, NVME, and Disk.   # The path variable of ""LSST_EFD_HOST"" gives the IP_address of influxDB.   # I can do ""./some_writer >& log.1 &"" to run the writer in the background. And I can use ""tail -f log.1 to see the log file"".   # Once I changed the cpp file, I can go to the ""stand_alone"" directory and do ""make -f Makefile.sacpp_blahblak_pub"" to build the code in the same directory.   # I can also do the above modifications by python.",3
"DM-19248","04/12/2019 17:18:19","Reorganization of jupyter notebooks Phase I","The notebooks on pontus need a reorganization. Things should be located by username.",1
"DM-19249","04/12/2019 17:25:05","Experiment with thorlabs flipper communication potential","thorlabs flipper has a purpose as a potential part of a csc. Explore the potential communication protocols. Looking into pyusb as a possible library. Working on solving pyusb permission error. Probably need to write a udev rule to give the device the proper permissions.",1
"DM-19250","04/12/2019 17:26:48","Help Andrew with TSSW Documentation","Help Andrew with TSSW documentation.    Met with Jonathan Sick and Andrew to discuss documentation tooling.    Wrote a barebones Java documentation example using Orchid.",1
"DM-19251","04/12/2019 17:50:59","Update ATMCS Simulator to not enable unused axes","Once DM-17775 is implemented update the ATMCS simulator to match and make sure the integration tests that use ATMCS still work.    The changes are as follows:  - The only rotator drive that is enabled is the one that M3 is pointing to (neither if M3 is pointing to PORT3).  - allAxesInPosition applies to the axes that are actually in use, as one might expect.  ",2
"DM-19258","04/12/2019 19:31:40","Update EXPTIME, SHTTIME, OBS-BEG keywords","Need to update EXPTIME to capture the request exposure time from the takeImages command.    The computed exposure time from shutter motion profile will be now directed to SHTTIME. Finally, DATE-BEG will be added to comply with LSE-400.",3
"DM-19265","04/13/2019 00:29:52","Jacobian lost in meas_mosaic photometric solution","It looks like we lost the Jacobian from the meas_mosaic photometric solution with this deletion from DM-10156:   [https://github.com/lsst/meas_mosaic/commit/6e395ac11a625c877374f99e2bed771b427835b6#diff-ad69537790bfe1f2b36095cbbc6f80a9L709]   I noticed this in comparing meas_mosaic-calibrated magnitudes between the w_2019_10 and w_2019_14 RC2 reprocessings, which look like:   !compareVisit-v1228-diff_base_GaussianFlux-sky-gals.png|width=450!     ",1
"DM-19268","04/15/2019 15:26:36","Update documentation for CSC overview","Updated the time I spend this sprint on documentation to focus on getting the csc overview completed. The original titles of this task was to get the templates created for the various code repo's.",2
"DM-19272","04/15/2019 16:43:21","Make script for creating skymaps in gen3 bulter","The script makeSkyMap.py currently uses command line tasks to create and ingest a skymap into a gen2 butler. This ticket will create an equivalent script for the gen3 butler.    This ticket will have a simple command line parser that will not attempt to capture all options or allow setting configs from the command line. That work will be captured by a future ticket which will encompass command line parsing for scripts in a generic way to work across all gen3 middleware scripts.",5
"DM-19274","04/15/2019 17:33:59","Reorganize script in standard and external script repository","After Robert visit we discussed ways to reorganize the way we write and store SAL Script so the code is mode re-usable from other SAL Script and usable from notebooks. I will implement this reorganization to the currently existing scripts and will write some example notebooks to show how that is achieved. ",2
"DM-19276","04/15/2019 18:12:44","Implement build system for ImgServ off pre-built LSST stack image","The new system to build the Docker image needs to achieve the following:   # Use latest official stable build from Square with CentOS and LSST stack, and build a multi-stage Docker file in ImgSev, as a standalone app.   # Pull in and build/install the dependencies or requirements, e.g. uwsgi.   # Pull in ImgServ, off the master OR feature branch, setup to run with scons.     ",5
"DM-19278","04/15/2019 21:35:39","Fix Scoreboard initialization","During initial refactoring, it wasn't apparent how the Scoreboard and its subclasses were being utilized.  In looking at it now, it appears it has always been doing a reload of the configuration file the main class was loading.  This was discovered  when doing DM-19240 and loading of Credentials had to be put in there until the config reload could be dealt with.  I'm creating this ticket to eradicate the reloading of the config/credential files.",0.5
"DM-19281","04/15/2019 22:57:58","Early April ASC work","early april work on the ASC, including meetings and reviewing the requirements doc and CSC/T2SA API spec. This ticket duplicates TSS-3459, which was created under the wrong project.",2
"DM-19286","04/15/2019 23:44:03","TMA Task list & meeting","Confluence page link:   * [https://confluence.lsstcorp.org/display/LTS/17-4-19+Meeting+notes]",2
"DM-19287","04/15/2019 23:49:30","study chiller communication protocol","read chiller docs, learn about how it likes its packets assembled. This is a duplicate of TSS-3457, which was accidentally created under the wrong project",1
"DM-19292","04/16/2019 00:44:33","Compare detection rates to Gaussian noise for HiTS processing","This ticket is to effectively repeat the relevant portions of the DMTN-006/DMTN-021 analysis for our current HiTS processing to bound our false positive rates.  This ticket is simply to perform the work in a notebook, but it is expected that future work will incorporate relevant portions into the metrics system.",8
"DM-19293","04/16/2019 00:59:09","Clean up photoCalib examples in docstrings","Meredith pointed out some improvements to the PhotoCalib docstrings, including incorrect values in the {{fluxField}} examples. I'll try to clean them up to make the docs more useful.",0.5
"DM-19311","04/16/2019 22:04:13","Please update to pybind11 2.2.4","After updating to the latest weekly T&S is seeing a lot of warnings when building ts_sal as described here: https://github.com/pybind/pybind11/issues/1444 -- our pybind11 is using an API that has been deprecated in Python 3.7.    pybind11 2.2.4 fixes this issue. Could we please update to that?",0.5
"DM-19318","04/17/2019 00:21:20","Notebook command-line tools: zip is installed but not unzip","In recent builds available in the Notebook Aspect, the {{zip}} utility is available but not the {{unzip}} utility.  This is arguably a bug, and in particular causes {{zip -T}} to fail.    Please either provide both or neither (but preferably both).",0.5
"DM-19328","04/17/2019 18:25:57","Investigate cause of small differences in Uber calibrations from w_2019_[06/10/14]","While validating the fix in DM-19265, it was noted that there are differences in the uber calibration results from both {{jointcal}} and {{meas_mosaic}} between recent weekly runs of the RC2 dataset.  As the plots on DM-19265 show, there have been some subtle changes from {{w_2019_06}}\-to\-{{w_2019_10}}\-to\-{{w_2019_14}}.  The differences are small (stdev of difference is at the sub mmag level), but it is still important that we understand where they originated.  Since both algorithms are affected, the route cause must be something that effects both.  A first guess would be something about how object selection is being done.",3
"DM-19331","04/17/2019 18:57:21","Fix detector name for HSC","The HSC detector name was being read from the T_CCDSN header but this value does not match common usage.  The names are defined in the obs_subaru camera geom files of the form RAFT_NUMBER where raft is 0 and 1 and numbers are zero-padded two digits.    Currently raft is not a concept used in obs_subaru.",2
"DM-19354","04/18/2019 00:54:23","Test ATDome vendor interface v2","The AT Dome vendor provided a new version based on comments provided as part of DM-18454. This ticket is to test this new version both at the TCP/IP vendor interface level and the CSC level.",1
"DM-19355","04/18/2019 00:58:18","Assist with TVS Roadmap ","As the TVS Science Collaboration prepare their roadmap to science, I have been participating as their DM liaison to ensure the roadmap contains correct (and DPDD-ified) language about the LSST data products.",1
"DM-19357","04/18/2019 15:30:11","Update the salEvent Call in salpytools for SAL v3.9","In version 3.9 of SAL, it looks like the the function {{mgr.salEvent}}  to publish an event is no longer supported and needs to migrated to {{mgr.salEventPub}} now.",2
"DM-19358","04/18/2019 16:17:19","Presentation to DESC PSF Task Force","Prepare and give a ~50 minute talk to the DSC PSF Task Force, on DM's perspective on PSF modeling.",2
"DM-19362","04/18/2019 16:39:40","Acquire new data using new configuration(s) of calsys_takedata.py Script","Take new dataset for monochromator characterization using new configuration values for calsys_takedata.py to see if an issue with an earlier data set is fixed.    Fixing calsys_takedata.py as it somehow became broken. Code was added to finish writing a csv file to user supplied directory that downloaded electrometer and fiber spectrograph fits files. The code was copy and pasted from the narrowband version of the script and was not completely changed over.         Fixes were implemented as part of the following PR - [https://github.com/lsst-ts/ts_externalscripts/pull/4]",1
"DM-19363","04/18/2019 17:09:37","Make OpenSplice RPM relocatable","To better support the SLAC build and test system for CCS, the OpenSplice RPM needs to be made relocatable.",1
"DM-19366","04/18/2019 19:07:57","afw unit tests do not run on some platforms","DM-10384 did away with {{lsst.geom}}'s subpackages, replacing them with a monolithic {{lsst.geom}} package. This change broke the backward-compatibility aliases provided by {{lsst.afw.geom}} for DM-14429. Some runs of {{pytest}} or {{scons}} try to import these aliases, and crash.    In the discussion of this issue on [#dm|https://lsstc.slack.com/archives/C2JPL2DGD/p1555091757000700], it was informally decided to simply remove the broken aliases instead of updating them -- no code will be broken by this change, since the existing aliases are unusable anyway.",1
"DM-19370","04/18/2019 20:27:24","Use ImageServ image in webserv Kubernetes deployment","Now that ImgServ build has been implemented as standalone image using prebuilt weekly builds from CentOS/lsst_stack, it's time to modify the k8s deployment to use that image, essentially splitting existing webserv service into the new dax-imageserv, and the rest.               ",2
"DM-19372","04/18/2019 21:19:56","Produce warning message when QuantumGraph is empty","When pipetask makes an empty QuantumGraph it would be useful to notify user about that, people get confused if everything looks OK and there are no messages, but then nothing is executed. I think it deserves a WARNING message if graph is empry, maybe we can also print some INFO like number of quanta in a graph.  Understanding why graph is empty is a more complicated (existential) problem, it will be addressed on a separate ticket.",0.5
"DM-19382","04/19/2019 00:47:09","Refactor and reorder ISR steps to support writing pre-interpolated pixels","At DRP Team meeting April 3 2019, we discussed the steps in ISR that could be reordered in order to support storing pre-interpolated pixels (that have had ISR applied to them so that they can be meaningfully swapped in downstream processing).     The interpolation step before brighter-fatter is necessary. Therefore, we decided to reorder ISRTask as follows:    * First ISR Steps…  * Interpolate  * Do Brighter-Fatter Correction  * Put the pre-interpolated pixels back into the now BF-corrected raw.   * …Continue ISR…  * Saturated/NaN pixels and defects are *masked only*  (no interpolation)  * Second to last step: write out pre-interpolated pixels  * Last step:  Interpolate the *union* pixels masked for interpolation (sat/bad/nan)    [~rhl] pointed out that it is more correct to interpolate the union of all regions that need to be interpolated rather than  in separate stages for each of SAT, BAD, NaN. In addition to the extra steps of reinserting pre-interp pixels, this tickets also includes refactoring to separate the masking step and interpolation steps in {{maskAndInterpNan}}, {{maskAndInterpDefects}}, and maybe {{saturationIntepolation}}.  ",8
"DM-19383","04/19/2019 00:57:47","Fix BOT translator when DARKTIME is present","[~echarles] reports that ingest of BOT is failing because there is a typo in the dark time calculation. This code has not been tested before since DARKTIME has never previously been written to files. The fix is trivial.",0.5
"DM-19389","04/19/2019 20:43:16","Convert lsst.geom to numpydoc","Enable Sphinx output of {{lsst.geom}}, and move corresponding topic documentation to {{doc/}}. Note that {{lsst.geom}} does not need any Numpydoc conversion as such because, except for {{testUtils.py}}, all its components are defined in C++.",1
"DM-19393","04/22/2019 15:32:43","Fix HSC y stray-light lookup In Gen2","DM-15862 broke y-band stray-light calibration lookup in Gen2 (which has always been a hack) by hard-coding a path, then deferring a fix for that to DM-16805.    That more complete fix will still have to wait for that ticket, but I'll try to get a fix for Gen2 only done here.     ",1
"DM-19398","04/22/2019 17:16:42","Tests directory should be brought up to DM compliance","The tests in the test directory need to be looked at in order to bring them up to DM standard testing.  Some of the tests require external services (Redis, RabbitMQ), and will only on systems that can access those services.  Those tests shouldn't be run  so that when the services aren't available.",1
"DM-19399","04/22/2019 17:19:31","Build files need to generically point to dependent libraries","The build files for this package have hard-coded locations to some of the packages they link against.  This needs to be fixed to use environment variables which point at the root of those packages, so that the build files will work in different environments and/or when the locations of those packages changes.",0
"DM-19409","04/22/2019 20:28:15","Add getCenter to Box2I","Many region specifications (DS9, STC) define boxes in terms of center and width. Box2D has a {{getCenter()}} method but it would be useful if {{Box2I}} also had one to allow for more seamless usage.",1
"DM-19412","04/22/2019 20:42:39","Bad CCD rotations in visualizeVisit.py","Something seems to be going wrong in the application of NQUARTER in visualizeVisit.py (maybe it's being applied twice, or maybe not at all).  [~lauren] suspects this was broken in DM-19371, and that the fix involves the new {{obeyNQuarter}} clause: [https://github.com/lsst/afw/commit/f154d3474b433e269d65c697c02cd1738e918942]",1
"DM-19428","04/22/2019 21:43:29","Add eups python as a dependency of pybind11","[~jbosch] points out that our pybind11 eups package doesn't depend on numpy and astropy when in fact it technically should (and eigen as well I think).",0.5
"DM-19440","04/23/2019 00:20:50","Docker Container problems","There was a review I did for Russell called slew scripts that would help me get up to speed on how docker containers work and how the team is using the scripting language. During the review I was having problems with getting my containers to get installed, I went through the process once on my centos machine, and again on my Mac. James also found a solution that solved the space problem that I was having on the cent os. This tasks is to cover the time spent on this effort. ",2
"DM-19453","04/23/2019 22:06:25","Whitelisting Supported Generic Interfaces in SAL","In the _SALSubsystems.xml_, the _Generics_ attribute should be converted from a boolean to a whitelist. This list is the generic commands and events that the CSC WILL support and implement. Only those items list here should be transitioned into the CSC API.",1
"DM-19458","04/24/2019 05:47:37","FieldValidationError usage incorrect in many tasks","While looking up how to use {{FieldValidationError}}, I discovered that several of our existing instantiations of it are incorrect and would thus raise an exception themselves! It takes {{(field, config, msg)}}, and I found 4 uses that just pass {{msg}}.    It is probably worth cleaning up the docstrings of this custom exception at the same time.",1
"DM-19464","04/24/2019 19:15:44","Add DAX imgserv and metaserv builds to jenkins-dm-jobs","Add standalone dax_imgserv, and dax_metaserv docker builds to Jenkins CI, under the old webserv, as moniker.         As Josh suggested, this is to be done via a PR on lsst-sqre/jenkins-dm-jobs.",5
"DM-19474","04/24/2019 23:43:29","Remove the char0 field from the array topics of Test","When I wrote the Test XML I thought ""char"" as the C type, but ""char"" is actually a synonym for ""string"". Arrays of strings are not permitted so remove the ""char0"" field from each arrays topic: command ""setArrays"", event ""arrays"" and telemetry ""arrays"". Update ts_salobj accordingly.",0
"DM-19475","04/25/2019 00:27:27","setup connection to chiller","The chiller is set up in the lab with an N-Port ethernet-to-rs232 converter. Figure out how to get that talking to the whitelight CSC.",2
"DM-19479","04/25/2019 16:28:24","Build metaserv image in its own Docker container","The proposal here is to apply the same framework established in imgserv to build a standalone container image for metaserv, to be taken out of webserv container, which can then be deployed to lss-lsp-int and lsst-lsp-stable.    After this work, the webserv container, which built lsst_stack from source, can be retired from DAX.",5
"DM-19485","04/25/2019 21:26:41","Fix length of instrument name in gen 3 butler schema","Currently the schema reserves 8 characters for the instrument name. This is insufficient for many instruments. Change to 16.",0.5
"DM-19488","04/25/2019 22:24:56","Evaluate the InfluxDB to be the EFD Candidate","This will evaluate the performance of InfluxDB with the SAL influxwriter on SSD, NVME, and spinning disk.",1
"DM-19491","04/25/2019 23:38:45","Enumerate options for creating new HTM Indexed refcats","After some slack discussion, [~krughoff], [~ctslater], and [~erykoff] have all provided me with some suggestions about how to make refcats.    There are the DR2 .csv files on lsst-dev here: {{/project/shared/data/gaia_dr2/gaia_source/csv}}    and a Parquet version here: {{/project/shared/data/gaia_dr2/gaia_source.parquet}}    {{meas_algorithms}} has {{IngestIndexReferenceTask}}, which I've been told is very slow. [~erykoff] suggested using {{fgcmOutputProduct._outputStandardStars}} from {{fgcmcal}}, but it does not handle proper motion or parallax or many of the other fields. I should run some simple tests to see just how slow the ""standard"" code is and make some guesses as to what might be necessary to speed it up. And similarly, compare it with [~erykoff]'s code and decide whether we should just use something like that. As part of this, I should look at reading Parquet vs. CSV.",2
"DM-19495","04/26/2019 03:27:58","Liaise with Stars, Milky Way, and Local Volume Science Collaboration","All activities as the DM liaison to the Stars, Milky Way, and Local Volume",1
"DM-19506","04/26/2019 17:03:06","Adjust defect FITS files to be compatible with DS9","The FITS region files we are writing use LSST pixel 0,0 as origin.  This means that when the regions are loaded into DS9 the regions are shifted by a single pixel.  Adjust the output of the region to correct for this (and also adjust when reading back in).",1
"DM-19507","04/26/2019 17:22:01","Update SAL C++ telemetry messaging tests to use the single-file interfaces","This task is to track the effort to update the SAL messaging tests.  I want to maintain the individual message testing, so updating the scripts that generate the tests will be somewhat tricky.",5
"DM-19508","04/26/2019 17:24:31","Preparation for TMA workshop","My time has been requested to help prepare the team for the TMA workshop visit in June.  This task covers the time needed to attend meetings, read documentation and other efforts, as needed.",1
"DM-19509","04/26/2019 17:27:28","TSSW Jenkins service configuration","This task covers time needed to setup and configure the TSSW Jenkins environment.   * Install and run Docker on the host   * Get the ts_xml job configured and running",1
"DM-19510","04/26/2019 17:28:14","Update SAL C++ event messaging tests to use the single-file interfaces","This task is to track the effort to update the SAL messaging tests.  I want to maintain the individual message testing, so updating the scripts that generate the tests will be somewhat tricky.",3
"DM-19511","04/26/2019 17:33:03","Write conda recipe for ts_tunablelaser","Write a conda recipe that works for the ts_tunablelaser.",1
"DM-19512","04/26/2019 17:36:12","Grab and build TMA PXI simulator & Install EUI on Simulation Cluster","Pull down the code from Docker hub and using their instructions build onto the Simulation Cluster.",3
"DM-19513","04/26/2019 17:37:09","Experiment with C# .net framework for thorlabs flipper","Downloaded control software from thorlabs which contains the libraries necessary for the .net framework. Followed their example for the MFF101 flipper to flip back and forth, setup .net framework using mono(a cross platform implementation of the .net framework). Flipper example would not compile because native c dll were only compatible with a windows c library.",1
"DM-19514","04/26/2019 17:38:22","TMA Meetings / Working with Shawn / Agenda Work","This task will cover the effort going over the TMA Agenda which will include meetings with Shawn to go over and assist in any activities. Meeting with Sandrine to cover high level view of the TMA project that she has agreed to go over with me. And reading documentation with Tiago to gather knowledge and better assist in identifying what requirements we are missing or need from Tekniker. ",3
"DM-19515","04/26/2019 17:40:40","Go Through Sal Issue and clean up","Go through the SAL issues we currently have and identify which ones are high priority, low priority, and which we can delete.",2
"DM-19518","04/26/2019 18:10:09","Setup Sundararaman development environment ","I would be using this task to set up my development environment, Research the libraries I would need for fiberSpectrograph c++ code to be imported to python. Mostly R&D and setting up hardware. ",2
"DM-19528","04/27/2019 02:05:48","Update ATMCS Simulator to use absolute angles","Update the ATMCS simulator to use absolute angles, now that ATPtg has been updated to do so.    If practical, also implement DM-19251 at the same time.",1
"DM-19535","04/29/2019 18:02:22","Move MakeBrighterFatterKernelTaskRunner to cp_pipe/pairedVisitTaskRunner","Several tasks in calibration product production (and some others actually) will work on visit pairs rather than single visits.    There is already an implementation of a custom TaskRunner in {{cp_pipe/makeBrighterFatterKernel.py:MakeBrighterFatterKernelTaskRunner}}. This should be made into a utility in {{cp_pipe}} so that its other tasks can use it, rather than either reinventing the wheel, or importing something misnamed and in the wrong place.    1) Move that task runner out of {{makeBrighterFatterKernel.py}}   2) Get {{makeBrighterFatterKernelTask}} to work using the new runner.",1
"DM-19539","04/29/2019 19:42:37","Create TOR for Image working group","Action from QAWG - to set up a group to look into needs for image display.",1
"DM-19541","04/29/2019 21:27:58","Test latest version of pointing component with SAL 3.9","Vendor released latest version of pointing component. Initial tests showed some issues with the component that need to be addressed and may have shown an issue with SAL 3.9/SalObj 3.11.0. This task will be used to continue testing the pointing component  and SAL.     Following Dave's suggestion I will spin up a EFD for the components to monitor the SAL traffic. I will also use this task to document a procedure on running integration and test on stage machines.     Documentation will go here:     https://confluence.lsstcorp.org/display/LTS/Integration+Testing+and+Deployment+Development+Procedure    ",3
"DM-19543","04/29/2019 21:47:00","Evaluate LSP review committee's recommendations and map to LIT tickets","Review the recommendations in the LSP review report, flatten the structure so that 1 item can be mapped to 1 LIT ticket.      ",2
"DM-19544","04/29/2019 21:48:05","Oversee creation of LSP Review LIT tickets with DM admins","The DM admins can help with entering LIT tickets",1
"DM-19547","04/29/2019 22:52:12","scoreboard date reported as 1900-01-01","values for 'EXPIRY_TIME' is being reported as having date 1900-01-01;  the time is correct, just the date is off.",0.5
"DM-19567","04/30/2019 22:15:38","Creat script to simulate forwarder for OR#1","take a file every N second s and put it in another directory",1
"DM-19569","05/01/2019 01:07:56","Update ts_ScriptQueue to use the dds version of ts_salobj","Update ts_ScriptQueue to use the version of ts_salobj based on dds. This is primarily intended to exercise the new version of ts_salobj.",3
"DM-19577","05/01/2019 19:11:37","Prepare for TMA software workshop","Attend planning meetings, review documents and software in preparation  for  the planned June workshop on TMA software at Tekniker",3
"DM-19578","05/01/2019 19:14:17","Assist developers with ADlink cases for DDS","Submit cases to the ADlink issue management system and assist with   investigation on the LSST side (of dds native python interface usage)",3
"DM-19579","05/01/2019 19:44:20","The same command IDs are being used by different commands","It appears that in ts_sal 3.9 every command topic starts with the same private_seqNum (some large integer). As a result the same seqNum is being used by all the different commands.    Two solutions that come to mind (and I'm sure there are others):    1) Use another existing ackcmd field or add a field that identifies which command the ack is for. For instance an integer that specifies the index of the command in the known command topics.    Then one can use a different content-filtered ackcmd reader for each command, for instance in SALPY using the ackCommand_x and getResponse_x functions will do exactly what a user might expect.    Does SAL provide similar functions for other languages?      2) Divide the sequence numbers up so that each command has its own range of sequence numbers. Each command writer starts out with the minimum value and rolls over when it hits the maximum.    This is a simpler change, but it means that code must (continue to) read one ackcmd topic to read acks for all commands. For instance one would use getResponse_start to read acks for all commands, including enable, disable, exitControl... which is surprising, to say the least.    salobj has not been doing this and so has a subtle bug I need to fix. I wonder how many other languages have the same bug?    (You could fix the bug using content filtered readers based on seqNum range, but that sounds riskier and hackier than using a separate field to identify the command).  ",2
"DM-19583","05/01/2019 20:01:45","Investigate butler gen 3 configuration system","The butler yaml configuration system might need some tweaking. Investigate [~mgower]'s problem and possibly adjust how overrides work.",3
"DM-19595","05/01/2019 22:29:52","Make new ap_pipe rerun with latest weekly","We should do new ap_pipe reruns on the HiTS2015 dataset approximately monthly with the latest weekly stack. In this case, we will not regenerate the coadd templates and will just use the CompareWarp coadds which are a few months old.",2
"DM-19599","05/02/2019 15:27:20","Run services as one user, out of one directory.","Currently, services run as different users (ARCHIE, DM, etc) depending on the type of service.  Code has to be installed in multiple home directories, separate configuration information and log files are written at each of these users.   The log file issue has been resolved, and files are written to a directory listed in the configuration file.    The software install has to be in one common directory where the code exists and is invoked by all services on that system all running as one user.    I suggest /opt/lsst/ctrl_iip for the $CTRL_IIP_DIR and DM as the user.",2
"DM-19601","05/02/2019 18:40:33","Update ATHexapod reference logic and add init command","Add the logic discussed in ticket https://jira.lsstcorp.org/browse/DM-18642 :       So we should certainly add a SAL command, regardless. (To do a position reference)  We should avoid movement in the disabled state, so on the start command, check the reference. If it's good, then continue as normal without re-referencing. If the reference is bad, then on enable, execute a reference and tell the user a reference is happening and therefore it might take more time.    Also add a reference event that checks and announce if the controller is properly referenced.",1
"DM-19602","05/02/2019 18:41:45","Test ticket DM-19601","Test:  DM-19601",1
"DM-19610","05/03/2019 15:48:50","Update to SAL 3.9","We're in the process of updating to SAL 3.9.  This story will contain the subtasks we need to do in order to complete that work.s",2
"DM-19614","05/03/2019 16:42:06","Write transmission curves in writeCuratedCalibrations","Transmission curves should be written to the butler when writing human curated calibration products",3
"DM-19615","05/03/2019 17:08:43","Change raw storage class in isr task","The gen3 butler storage class in IsrTask does not match what it is ingested as, fix this.",1
"DM-19616","05/03/2019 17:16:17","Make IngestIndexReferenceObjectsTask multiprocessing capable","In order to be able to generate a Gaia (or PS1-DR2) reference catalog in a reasonable amount of time, we need a way to process files in parallel. The existing IngestIndexReferenceObjectsTask is not capable of running in parallel. One approach that should be fairly simple is to use multiprocessing plus file locking: lock each output catalog before appending.",8
"DM-19618","05/03/2019 19:02:55","Allow gen3.py to be called with an external butler configuration","Currently the butler configuration used for gen 3 testing in ci_hsc is hard coded into the lsst.ci.hsc.gen3 module.  This makes it hard to use for oracle testing where we want to override the configuration.  Update gen3.py to be a class allowing an override butler and add command line to the gen3.py script to use that functionality.",2
"DM-19622","05/03/2019 21:24:03","Make PosixDatastore's internal table lowercase","DM-17633 failed to lowercase PosixDatastoreRecords, as it's defined in a totally different way from the rest of the schema.  Fix that.",0.5
"DM-19623","05/03/2019 21:24:34","Change sqlalchemy syntax in addDimensionEntryList","sqlalchemy has a few different ways to insert multiple objects at a time, change to one that works across more databases reliably.",2
"DM-19627","05/03/2019 22:48:11","Add text file serialization to meas_algorithms Defects class","It was noted on RFC-595 that the creation of the Defects text files to be stored in the data repositories is distinct from Defects.  It would be more useful for people creating defects lists if the Defects class could serialize to the text format.    [~rhl] also requests that metadata be stored in the text files (at least the format version number and the detector name and instrument).  To satisfy that requirement I will see if I can write out in the Astropy ECSV format.",2
"DM-19628","05/03/2019 23:28:02","Worker CSV loader service","Implement a loading service within existing Replication System's workers. The service will initially support data in the TSV format. The services will create MySQL partitions in the chunk tables as per the super-transaction identifiers. Implement a command-line application which would accepts pre-partitioned TSV files and perform all required interactions with the Master Replication Controller (via the HTTP REST API). There will be a suite of simple integration and performance testing applications (scripts) to drive the parallel loading.",20
"DM-19633","05/03/2019 23:55:51","The ups table lost crucial info and make_idl_files is broken","The recent merge to develop of ts_sal has broken the use of ups and also my new make_idl_files script. I had not realized that [~dmills] ripped out the new ""sal pydds"" salgenerator command, which make_idl_files used to generate the all-in-one IDL file (ts_sal/test/idl-interfaces/validated/sal/sal_revCoded_<component_name>.idl) needed by dds. Fortunately we can use ""sal cpp"" instead, though it's a lot slower and less efficient.    For the future it would be really nice to have a salgenerator command that would make the IDL file without building anything extra (such as C++) except, preferably, a file of enums as Python enum.IntEnum instances.",0
"DM-19636","05/04/2019 01:24:14","Fix Qserv CI container builds","Recent change of Jenkins build workers to Alpine broke Qserv container build scripts",1
"DM-19637","05/04/2019 02:50:22","Moar fix Qserv CI container builds","more Alpine incompatos in container build script...",0.5
"DM-19641","05/06/2019 15:49:17","Use jointcal instead of meas_mosaic in obs_subaru HSC coaddition","Use jointcal instead of meas_mosaic!     We've done sufficient QA, integration work and improvements to believe this is now working at least as well as meas_mosaic. This doesn't require an RFC because it just affects obs_subaru  (RC2 reprocessing and future HSC data releases).",1
"DM-19643","05/06/2019 17:12:51","Investigate using select to read data with dds in ts_salobj","For the dds version of salobj I am presently using one dds.WaitSet per topic, which takes one thread per topic. However, it may be possible to use dds select to have a single thread that reads all topics. That should be more efficient and scale better for the Watcher. Look into this and implement it if practical.",1
"DM-19644","05/06/2019 17:15:19","Use a single remote to interact with all scripts","The ScriptQueue should use a single Remote to communicate with all scripts, rather than creating a separate remote for each script. This is trivial with the dds version of salobj and would also be easy with SALPY if it ever exposes the hidden fields in topis.",2
"DM-19646","05/06/2019 18:00:42","Check software installed by vendor in laptop and Desktop","Laptop and main computer is with Doug. I will check the startup and software installed on the PC and laptop before we ship it to Chile. Review documentation that were delivered along with the devices.",2
"DM-19650","05/06/2019 19:30:20","Ensure scarlet failures are propagated to the stack","scarlet has internal diagnostic information and flags that are set but not passed to the LSST stack. {{ScarletDeblendTask}} needs to be updated to update the catalogs with the relevant flags.",2
"DM-19656","05/07/2019 15:01:12","Write utility to test transitioning states for a generic CSC ","As part of the testing of and development of CSC, and in particular for the L1 NCSA Test Stand we need a utility that can test and verify the reception and acks of command for transitioning states. This is particularly useful when testing new version of SAL.",3
"DM-19659","05/07/2019 18:44:15","Update ts_phosim to use eups and yaml","Update ts_phosim (from ts_tcs_wep_phosim) to use eups and documenteer and add dependency to ts_wep. This is to unify the package manager and reduce the code redundancy. This task will also update the configuration files to use the yaml format.",3
"DM-19663","05/07/2019 20:21:47","Update States Enumerations to use ""SAL__STATE_STATENAME""","The enumeration (integers) of the CSC states need to be updated in salpytools. Instead of using the detailed states, it should the SAL__STATE_CSCNAME from now one.",2
"DM-19671","05/08/2019 00:02:27","setConfigRoot sometimes needs to not update the root","[~dinob] is running into problems with {{Butler.makeRepo}} whereby the registry specified from his external config (which happens to be an in-memory sqlite) is being overwritten by a default value assuming a SQLite file on disk with butlerRoot.    I think it is reasonable to assume that if makeRepo is being called with a populated config and that that config has {{registry.root}} defined, that the caller of makeRepo does not want makeRepo to override the values. Should makeRepo attempt to create the tables in the provided root? What about {{datastore.root}}?    It's a fairly small change to allow setConfigRoot to not override root if root is already present in config (we have to call it anyhow because it copies items over as well as updating the root). I think a flag to setConfigRoot (""overrideRoot""?) defaulting to False would work which would be True when called from makeRepo and would be forwarded to overrideParameters or else not even pass toUpdate to overrideParameters.",1
"DM-19677","05/08/2019 17:05:57","Disable writing postISRCCDs in ProcessCcdTask","Per [discussion on Slack|https://lsstc.slack.com/archives/C2JPMCF5X/p1557330175343800?thread_ts=1555439400.008300&cid=C2JPMCF5X], {{ProcessCcdTask}} should set {{doWrite=False}} on it's {{IsrTask}} subtask.    And please also make sure there aren't unnecessary overrides in obs packages.",1
"DM-19679","05/08/2019 18:59:41","Please parameterize the ID provider in the LSP instance config","(Emergent from a configuration problem that emerged on 2019-05-08; see Slack #dm-lsp and #dm-lsp-coord for more information.)",0.5
"DM-19681","05/08/2019 20:52:26","Clarify who has authority to override coding standards","https://developer.lsst.io/coding/intro.html#stringency-levels currently refers to the TCT and the “lead developer”, neither of which are meaningful.",1
"DM-19682","05/08/2019 21:26:29","Fix DAYOBS calculation and allow for gen2 header correction for ingest","[~rhl] reported that the gen2 butler dayObs quantity is not being calculated correctly. It is always assumed to be calculated from the date and not from the DAYOBS header. This needs to be fixed to only use the date if DAYOBS is missing.    Additionally, since this is a gen2 ingest model parameter and not part of the gen3 data model, the value does not get corrected if there is a FITS header correction file. Update ingest so that header fixups occur on ingest even outside of metadata translation.",1
"DM-19685","05/08/2019 22:54:42","Identify slowdown in SAL command acks","Try to pinpoint the slowdown in SAL command acks that [~tribeiro], [~spietrowicz] and I are seeing. In particular:  - See if it is present OpenSplice 6.10  - If it still present there then try to come up with a simple case that shows the problem.  - If the problem is in dds file an ADLink support ticket. If not, try to figure out how SAL could be improved.    Note: I see the problem (or what I think is the problem) in dds salobj.",3
"DM-19686","05/08/2019 23:12:20","Interactive matplotlib plots work if notebooks are rendered from the ""classic"" view","The matplotlib plots rendered in notebooks served within JupyterLab do not have the standard interactive controls.  I believe this is a conscious decision, but users find the interactive controls useful enough that, in some cases, users are specifically using the classic (""tree"") view of the notebooks in order to have this feature.    [~frossie] suggested that we look into whether the JL position is flexible on this.",2
"DM-19694","05/09/2019 17:20:42","Make Defects presize internal tables.","Currently, the internal table that holds the defects in {{Defects}} is not presized, so is not contiguous in memory.  This is to make sure the table is contiguous from construction time.",2
"DM-19695","05/09/2019 17:25:16","ci_hsc fails tests on NFS","Since moving Jenkins to NCSA one of the ci_hsc tests fails because the clean up code attempts to clear out a directory but some files are still open and owned by the process. On local filesystems this works fine but on NFS deleted but open files become temporary files and {{rmtree}} gets upset when they hang around.  tempfile.TemporaryDirectory does not know about those open files and does not run rmtree with the flag to ignore errors.    The fix is either to move the clean up code to tearDown or else see if a {{del}} works.",1
"DM-19697","05/09/2019 18:05:58","Proposed additions to the ackcmd topic: Origin and command ID","I propose that the {{ackcmd}} topic be enhanced to add a field for the ""origin"" datum of the command that triggered the ack and another field for ""host"" if ""origin"" is not unique. This will make it simple for the commanders to only pay attention to ackcmd messages meant for them.    The present situation is risky because there is no way to tell which commander an ackcmd topic is intended for. A recent change to the way private_seqNum was incremented makes this far more likely because the starting number is now the same for all commanders (for a given command).    Also I suggest adding field to commands that identifies the command (perhaps the integer position of the command in the list of commands, or a hash generated from the command name -- anything the commander wants as long as it is a 1:1 mapping between command name and identifier). The component being commanded will copy this field to the ackcmd topic.     This is less crucial than the first suggest change, but it seems simpler than relying on ranges of sequence numbers. If nothing else we can more easily debug problems since the person doing the debugging won't have to work out the range of sequence numbers that applies to a particular command.    I suggest that this ticket replace DM-19579 as a more thorough solution.",3
"DM-19702","05/09/2019 22:40:35","Make ATAOS configurable and add handling of lookup tables","In preparation for the tests in Chile, I'll finalize the ATAOS CSC by making it a configurable CSC and adding support for the lookup tables. ",2
"DM-19704","05/09/2019 23:13:45","Update logging in ATHS/salpytools to also direct to a file","Update logging on the salpytools section of ATHS to be directed to a file in addition that to the screen.",1
"DM-19709","05/10/2019 12:50:15","Write a first version of a defect finding task","Lab testing of ComCam motivates a first pass at developing some long-overdue calibration product production code, as this will allow the ComCam commissioning team to efficiently assess performance as a function of temperature (among other things) as they acquire lab data over the (past and) coming weeks.    The ticket will cover making a first pass at writing a defect finding task to support this work.    Defect Task:   * Find bright & dark pixels from darks and flats   * Persist as ASCII text files so they can be treated as defects in the normal (old & bad) way   * Summary stats for each amp (and for each sensor)    Task should be able to run on either single master calibs (both darks and flats), or a list or raw input files, processing each as it should, and applying pixel-wise cuts to reduce the false-positive rate, _i.e._ making use of being fed multiple raw inputs.",20
"DM-19710","05/10/2019 14:00:55","Add astro_metadata_translator corrections for older HSC data","Older HSC files can have incorrect DATA-TYP headers, causing some science frames to be ingested as calibrations and probably vice versa.  The OBJECT headers seem to provide values that could be used instead if we can completely enumerate the values used for calibrations; given the finite time duration of the problem.  [~furusawa.hisanori] also reports that the STARS database at NAOJ should be updated with correct values for these, so we could also query that to obtain more complete corrections for all potentially-incorrect files.    Priority for this ticket should be to get in a fix that enables the correct Gen3 Butler ingestion of all affected science frames in the RC2 dataset, which I can provide a complete list of later.  We can create a new ticket for a more thorough fix if the solution here is more localized.    [~tjenness], please steal this if you think you'll have time to work on it today.  If not I'll be bugging you about the best way to approach putting the corrections in astro_metadata_translator.",1
"DM-19717","05/10/2019 17:52:03","Add setup_module to ip_isr unit tests.","Memory testing was not enabled correctly for all ip_isr unit tests.",1
"DM-19720","05/10/2019 20:16:13","Change multiband.py to support meas_extensions_scarlet","Modify multiband.py deblender command line task to use meas_extensions_scarlet instead of the scarlet functionality in meas_deblender",5
"DM-19726","05/11/2019 00:25:14","update efd-kafka prometheus deployment","The {{prometheus-operator}} chart, in addition to providing CRDs for configuring prometheus scraping, bundles some extra goodies, such as grafana dashboard, that are not present in the {{prometheus}} chart.",3
"DM-19731","05/11/2019 15:53:36","Investigate inaccuracy of initial HSC WCSs","Gen3 ingest needs to be able to generate inclusive on-sky regions for all visits, using raw WCS and potentially camera geometry.  As those initial WCSs are inaccurate, we expect to need to pad somehow to ensure the region always includes the true region, even if it's much larger.    As a result, we need to look at the difference between some initial HSC WCSs and their post-astrometry counterparts.    This investigation could also be informative for future attempts to fit initial astrometry with as few free parameters as possible.",2
"DM-19732","05/12/2019 08:31:05","remnant ip_isr debug statement left in","An errant debug fits write was left in.",1
"DM-19733","05/13/2019 13:48:54","Make scarlet models more general","In order to implement symmetry in the new deblender we need to move to a model that allows components to have more control over the parameters they require (to include fractional pixel shifts dy and dx) and how their updates are calculated (to calculate updates to dy and dx). This requires an update to scarlet.",8
"DM-19739","05/13/2019 18:24:53","Add atmcs data to atptg_ataos integration test","Update the atptg_ataos integration test to include getting target status from the atmcs instead of the atptg component for the ataos.",1
"DM-19740","05/13/2019 19:31:41","Fix dependency for EFD runtime RPM","The EFD runtime RPM requires _tclsh_ but the current version doesn't resolve that correctly. That dependency resolution needs to be corrected for the RPM to install without using special flags.",1
"DM-19742","05/13/2019 22:39:28","Test new release for the cRio software on ATDome ","Test new release for the cRio software for ATDome. This should stop shutter commands when using the emergency stop.",1
"DM-19743","05/13/2019 22:41:25","Knowledge transfer 1 - ATDome M2 ATHexapod","Create documentation for:   * Document how M2 is connected to the computer   * Document how to configure AuxTel Dome cRios    * Document how to configure PI Hexapod",3
"DM-19750","05/14/2019 02:33:21","Test the updated AOCLC with PhoSim","Test the AOCLC with PhoSim and the WEP and OFC interface classes. This is to test the updated code in the [DM-19749|https://jira.lsstcorp.org/browse/DM-19749].",2
"DM-19751","05/14/2019 02:35:38","Study the Kubernetes","Take a free course ([Scalable Microservices with Kubernetes)|https://classroom.udacity.com/courses/ud615] on Udacity to learn the Kubernetes with the microservice.",1
"DM-19752","05/14/2019 10:35:56","monowl string parse in gen2 butler ingest of ts8 data","uncovered a slight issue with ingest, had put in MONOWL into the comcam (ts8) headers, but they were as strings, and ingest barfed saying it couldn't do a logical comparison or some such... I got around it by deleting the card and explicitly adding it as an int.    original error returned:    {code}  ingest.parse WARN: translate_wavelength failed to translate wavelength: '<' not supported between instances of 'str' and 'int'  {code}",0.5
"DM-19756","05/14/2019 18:55:15","Create updated RPI SAL SDK image","Rebuild the SAL  V3.9 and OpenSplice V6.9 for the Raspberry Pi environment and create an SDK image for use by the EAS contractor",3
"DM-19757","05/14/2019 18:57:25","Review EAS XML and add topics as necessary","Review the current EAS XML and add items as necessary to cover the entire EAS sensor suite in type and number. Create ts_sal runtime thereof.",2
"DM-19758","05/14/2019 19:02:29","Prepare for TMA software workshop","Prepare for the upcoming workshop, review code and build instructions, review documentation and suggest improvement, update agenda and add details",3
"DM-19761","05/14/2019 21:21:36","Test run gaia dr2 refcat with jointcal and/or processCcd","-I have created a ""symlink repository"" for HSC at {{/scratch/parejkoj/hsc}} (should be group-writeable by {{lsst_users}}), which we can use to do a trial run of processing data with the Gaia DR2 catalog in jointcal and/or processCcd. This is easier than moving data or symlinking into the read-only {{/datasets/ref_cats}}.-    We now have a {{/datasets/refCats/htm/v1/gaia_DR2}} symlink to my {{/scratch}} gaia refcat, which we can use to do a trial processing run with this new catalog in jointcal.    [~hchiang2]: can you please help me start an appropriate HSC run (PDR1, RC2 or something else small-ish I don't really care) using this? I'm not sure what we need to do to test it other than just demonstrating that it runs to completion: it has more sources than Gaia DR1, but not more than PS1, and it's still only usable for astrometry.",2
"DM-19762","05/14/2019 21:35:43","Write tracking integration test for atptg, atmcs, ataos, athexapod & atpneumatics","The test should set a starting altitude, azimuth and tracking_duration. It will then check that as tracking occurs that no CSCs go into a fault state. ",3
"DM-19764","05/14/2019 22:08:38","MTAOS - WEP Algorithm Integration","Integrate the WEP into MTAOS.",3
"DM-19766","05/15/2019 01:02:20","DECam instcals fail to process with invalid DateTime","When a user ingests a DECam instcal and runs {{exposure.getVisitInfo().getDate()}}, it returns an empty DateTime. ProcessCcd naturally assumes a non-empty DateTime and fails with a ""DateTime not valid"" error.    We think this problem originates in {{astro_metadata_translator}}, which likely was made to work with DECam raws and not DECam instcals. Instcals are apparently missing some of the expected header keywords.    This ticket is to fix things so ingested instcals have proper DateTimes and to add a test (presumably in {{astro_metadata_translator}}) that checks an instcal header.    See the full discussion on Slack [here|https://lsstc.slack.com/archives/C2JPMCF5X/p1557874729491100].",2
"DM-19768","05/15/2019 05:53:34","Fix jointcal handling of coordinate errors","Since we didn't have a refcat with coordinate errors, I couldn't write a test of jointcal's use of them. And thus I got it wrong: disabling {{config.astrometryReferenceErr}} resulted in an exception because the error field is {{coord_raErr}} not {{coord_ra_err}}!    Unrelatedly, but I might as well fix it as part of this work: it looks like jointcal is trying to apply colorterms when loading the astrometry reference catalog. That is both not necessary, and unhelpful when the astrometry refcat doesn't have useful photometry (e.g. Gaia). Might as well fix that on this ticket, too.",1
"DM-19770","05/15/2019 18:03:28","Backup Hexapod and Rotator management PC and support requests from SLAC","Tony Johnson has started working on Rotator and has sent us a bunch of questions. This task will be used to support SLAC and also to backup harddrive of Management PC without which Tony would not be able to operate the rotator. ",1
"DM-19771","05/15/2019 18:31:45","Debug FITSIO LabVIEW Library error","Debug CFITSIO library error and get the .so file working with the current version of fits library. This is a prerequisite to update FiberSpectrograph code to use SAL version 3.9.",2
"DM-19772","05/15/2019 19:28:26","SQRE network routing reconfig post NOAO core changes","NOAO/Tucson CIS is planning to re-plumb the core network on Saturday, 2019-05-18.  DM Jenkins OSX builds will need to be disabled/reenabled before/after this work and a console cable will need to be connected to the SQRE layer3 switch for manual reconfiguration.",1
"DM-19773","05/15/2019 19:28:57","Debug device not detected issue","Compiled C++ program does not detect the USB Spectrograph (FiberSpectrograph) connected to the Linux machine. Debugging to figure out if its a hardware or software issue",1
"DM-19780","05/16/2019 00:17:23","Revise LCR-1554 (corrections to LSR and OSS regarding flowdown of SRD ellipticity correlation requirements)","Produce a revised version of the requirements proposal in LCR-1554, to take into account concerns expressed at the voting stage by Austin and Bill G.",1
"DM-19791","05/16/2019 21:17:05","Update tai_from_utc in ts_salobj to handle leap seconds","In ts_salobj DM-19385 I coded {{Domain.tai_utc}} to return a hard coded number, with the intention that the code be changed to use a better solution (e.g. emulate ts_sal) at some point. This ticket is for that ""better solution"".    Also we need a function that returns the current TAI, not just tai_from_utc. Most of the time we only want the current time. The main use case for tai_from_utc is converting the received time sample metadata provided by DDS to the private_rcvStamp field in read DDS samples. See DM-21097 for that function.    I propose to use astropy. But we have to update astropy whenever a leap second was due and it will probably give slightly incorrect answers near a leap second. To quote a warning from the documentation for {{astro.py.time.TimeUnix}}:  {code}  NOTE: this quantity is not exactly unix time and differs from the strict      POSIX definition by up to 1 second on days with a leap second.  POSIX      unix time actually jumps backward by 1 second at midnight on leap second      days while this class value is monotonically increasing at 86400 seconds      per UTC day.  {code}  and {{astropy.time.Time.now()}} uses {{datetime.utcnow}} and the {{datetime}} library does not handle leap seconds.",2
"DM-19801","05/17/2019 01:39:56","Gracefully handle failure to retreive alerts from disk","Per https://github.com/lsst-dm/alert_stream/issues/23, when {{sendAlertStream.py}} tries to read a file which contains no data, it throws a {{ValueError}} with a pretty obscure traceback. Please handle this more gracefully.",1
"DM-19802","05/17/2019 09:29:10","Fix jointcal ra/dec bounding box calculations","[~sogo.mineo] noticed that jointcal does not correctly calculate the field center and radius to load the reference catalog. If the center is near 0/360, the radius may end up being ~360º, causing out of memory errors due to all of the refcat being loaded. The calculations at {{jointcal.py:524}} need to be redone to properly handle that wrap-around. SpherePoint should help with this.",2
"DM-19807","05/17/2019 18:04:57","Avoid savepoints in Gen3 ingest and bulk conversions","The savepoints used to implement nested transactions are taking the bulk of the time in raw and calibration in Oracle.  See if we can implement lighter-weight, Python-side nested transactions for at least some operations.",0
"DM-19810","05/17/2019 20:00:23","Backup HexRot Management PC for copy to SLAC","SLAC needs a copy of the Hexapod Rotator management pc software",1
"DM-19811","05/17/2019 23:02:11","Setup the Linux Desktop","Setup the linux desktop returned by IT. The system OS was re-installed. This task will install the needed application and personal settings for the software development environment.",1
"DM-19812","05/18/2019 00:21:36","Remove rule on return value policy from pybind11 style guide","The consensus on RFC-597 was that the [return value policy rule|https://developer.lsst.io/pybind11/style.html#the-reference-internal-policy-shall-be-used-for-functions-or-properties-giving-write-access-to-internal-data-members] in the pybind11 style guide is unneccessary, so remove it. Developers will be expected to directly apply the pybind11 documentation concerning return value policies.",1
"DM-19818","05/20/2019 17:51:44","Fix Gen3 Butler pickling broken on DM-19638","DM-19638 added a new butler constructor argument and broke pickling.",1
"DM-19822","05/20/2019 20:17:48","Updates to ""adding a new package to the build"" developer docs","I recently created the {{ap_pipe_testdata}} package, and found some places where the developer guide could be clarified.    1. Specify in what cases an RFC is *not* required to add a new package. My pair coder and reviewer agreed with my interpretation that a new testdata package is not ""intended for distribution to end users"" and I therefore did not need an RFC.    2. Configure things on GitHub for the new package:   * Disable squash and rebase merging (this part is pretty clear as-is)   * The part that's not clear is the steps provided to configure branch protection for master. The screenshots and directions are outdated; the services on the ""Integrations and services"" screen appear to be deprecated and I am not sure where one would put a ""simple Travis script"" like the flake8 example. I had to click ""Add rule"" from the ""Branches"" screen to get to a screen similar to ""Branch protection for master"" as shown in the dev guide. However, without something checked under ""Require branches to be up to date before merging,"" it does not let me add a new ""rule."" So I skipped this. In my situation with a new testdata repo, it seems rather superfluous anyway since there is no code. More guidance is needed here.   * Set the team membership properly. Specifically, for a package like mine, set it to both ""Data Management"" and ""Overlords."" This is the step I misunderstood and inadvertently skipped. Please explain what team membership is, how to set it properly (why is there a team called Overlords?!), and mention that Jenkins will pass even if you don't do this, but the daily/weekly build will fail.    3. Explicitly say that you must add {{setupRequired(new_package_name)}} or {{setupOptional(new_package_name)}} to the appropriate {{other_package/ups/other_package.table}} files, i.e., any stack package which imports the new package. And if there are C++ dependencies involved, you also need to update the appropriate {{other_package/ups/other_package.cfg}} files. (Please note that giving the full example path to the files which must be updated is helpful throughout, even when there is a link included.)    4. Add the new package to {{lsst/repos/etc/repos.yaml}} following the conventions there. *Merge this change to master before you attempt to run Jenkins;* it is a special case exemption from the usual code review process.    5. If it is a Git LFS-backed repo (which mine was), add optional dependencies to {{lsst/lsstsw/etc/manifest.remap}} following the conventions there. Include this in the code review (do not merge it in advance like you must do for the changes in {{lsst/repos}}).",1
"DM-19827","05/20/2019 22:51:57","Set up JSR 2019 review presentation outline","set up the review slide deck .. help to structure the sessions ahead of DMLT F2F",3
"DM-19828","05/21/2019 00:02:47","Investigate oddness in error propagation with meas_mosaic's photoCalib","In working on DM-19189, I have been looking at the S/N distributions of the various fluxes  (raw instrumental and calibrated) from single frame processing (where here {{S/N = flux/fluxErr}} from the catalog).  The S/N distributions look as expected for the raw instrumental fluxes, the calibrated values based on application of the photoCalib of {{jointcal}} outputs, and the {{meas_mosaic}} solution using the ""old style"" calibration (i.e. using {{meas_mosaic}}'s own {{applyMosaicResultsExposure()}} function – which uses the persisted {{fcr}} dataset to apply the calibration).  However, the S/N distribution when applying the {{photoCalib}} from {{meas_mosaic}} is not at all as expected (plots to be attached). Look further into this pathology and what might be causing it.",3
"DM-19832","05/21/2019 17:19:55","set up windows host for t2sa","need to setup a windows server with a static IP on the LSST network, with a SpatialAnalyzer license. It will host the vendor's T2SA application, which is an intermediary between the alignment CSC and the laser tracker hardware.",2
"DM-19839","05/22/2019 01:06:12","Fix bug in recent DcrCoadd PSF calculation","The PSF calculated for DcrCoadds in DM-19517 fails in some cases for Decam data. The source detection task that was used incorrectly scales the source detection threshold, and can result in no sources being detected. The detection Task should be switched to the base version, and it should possibly only be a warning if psf measurement fails, rather than an error.",2
"DM-19845","05/22/2019 18:25:09","Bring up imgserv and metaserv on lsst-lsp-int-dax","Need to configure these DAX services to run in the new Kubernetes environments on LSP.",1
"DM-19847","05/22/2019 18:53:24","Coating Chamber Software Support","Initial Reviewing / Learning of Coating Chamber Vendor Software    This will also include:   * learning/support during M2 coating   * support during M1M3 coating   * possible training at VA for WICOM",20
"DM-19848","05/22/2019 18:59:58","TMA Work Phase 1","TMA Training & Review of Labview-based vendor software in Tucson & Spain.    Also any on-going work & support regarding the TMA.",20
"DM-19853","05/22/2019 22:12:16","SODA examples at LSP service endpoints need to return the right host names","This is reported by KT.  Currently it's returning the pod name (via gethostbyname()) in the links  returned for /examples.     The correct hostname should be the k8s namespace:  lsst-lsp-int-dax, lsst-lsp-stable-dax.",2
"DM-19857","05/23/2019 00:30:41","Update ap_verify to use new DECam defect ingestion","Now that we have a new way to ingest DECam defects, we need to update ap_verify and the two ap_verify datasets (ap_verify_hits2015 and ap_verify_ci_hits2015) to use this new mechanism.",2
"DM-19858","05/23/2019 02:09:27","Ensure that Portal / Firefly deployments are insensitive to trailing slashes on the application root","Please ensure that all deployments of Firefly applications visible to LSST are insensitive to the presence of a trailing slash at the application root.    E.g., ""lsst-lsp-stable.ncsa.illinois.edu/portal/suit"" vs. ""lsst-lsp-stable.ncsa.illinois.edu/portal/suit/""    This should apply to lsst-demo, to Adam's /firefly deployments, and to the actual Portal application(s).",1
"DM-19859","05/23/2019 17:47:03","Add XML tests for Unit and Description fields","Add tests on ts_xml for <Unit> and <Description> fields, in accordance with [https://confluence.lsstcorp.org/display/LTS/SAL+XML+Unit+Definition+-+Discussion].  Also, add tests for XML Version; see VERSION file in the repo.",2
"DM-19871","05/23/2019 21:22:06","Fix validity range end in Gen3 calibration bootstrapping","The bootstrap scripts added on DM-19638 upgrade Gen2's dates to Gen3's datetimes without adding one day to the end dates.  Because those intervals are inclusive instead of half-open, that means we end up with a day-long gap between adjacent intervals.",1
"DM-19873","05/24/2019 00:02:48","Implement PropertySet.getitem and return get()","Following the adoption of RFC-596:  * Add getitem support (returning scalars in all cases)  * Undeprecate {{get()}}, modifying it to return scalars and to support a default value.  * Investigate adding {{update()}} method.",2
"DM-19874","05/24/2019 00:14:50","Add support for decam illumcor calibration products","Partway through DM-18417, it became clear that illumination correction appears to be needed to eliminate false positive sources from difference imaging on nights with high sky brightness. While illumination correction is a standard step in the DECam community pipeline (CP), and ""illumcor"" CP data products exist for the HiTS2015 dataset, we have never used them with the LSST Science Pipelines.    This ticket is to add the ability to ingest, and subsequently use during ISR, the illumination correction calibration products provided by the DECam community pipeline. They are multi-extension FITS files much like the ""cpBIAS"" and ""cpFLAT"" masterCals we have been using, and should be able to be ingested and used in a similar way.    There will be Mappers.",8
"DM-19875","05/24/2019 15:52:00","Clean up Script and ScriptQueue XML","The Script and ScriptQueue XML both use topic-specific enumerations when shared would probably be better (and certainly more common).    This is especially important for Location but also useful for ScriptState and ScriptProcessState. I am less convinced about the three metadata enumerations, but using the same technique for all is simplest.",0
"DM-19876","05/24/2019 16:20:01","Remove old SAL C++ code, and update requirements.txt ","Remove old SAL command C++ code.  The code is from a previous version of SAL, and wouldn't work anyway.  Any commands we may need will be created in another ticket. Also update requirements.txt",1
"DM-19877","05/24/2019 16:23:34","Replace PropertySet.get with getScalar or getArray","DM-19873 is changing the API of PropertySet.get() such that it will now return a default value rather than raising KeyError. Before that can happen, all extant calls to {{get()}} must be replaced to avoid any surprises.    The approach I will take is to change get() so that it always throws an exception and then wait for tests to fail. This may lead to some remaining usage of {{get()}} in code that is not used.",1
"DM-19880","05/24/2019 17:15:54","Update scripts in ts_standardscripts for schema+dds scriptqueue","Update the scripts in ts_standardscripts to work with dds ts_salobj and schema+dds ts_scriptqueue",3
"DM-19881","05/24/2019 17:24:59","Make dds salobj compatible with SAL","dds salobj is not compatible with SALPY. See the last comment for details of what was wrong.",2
"DM-19884","05/24/2019 21:14:43","Add configs to __all__ in isrTask.py","The {{RunIsrConfig}} class was omitted when {{__all__}} was added to {{isrTask.py}}.  This breaks running `RunIsrTask` more than once with the same rerun but different data ids.  Adding the configs fixes this.",1
"DM-19885","05/24/2019 21:49:02","Export the ts_xml job into the TSSW Jenkins environment","This task covers the work to finish the export of the ts_xml job into the TSSW Jenkins instance.   * Configure and run the ts_xml job.   * Create a Jenkins user on the LSST GitHub organization to use for Job Credentials.",3
"DM-19886","05/24/2019 21:56:46","Preserve blank line from templates into output header files","The Header Service reads in the primary and segment hdu from templates that define the content of the headers. The fitsio (Erin Sheldon's) module doesn't not preserve or store this at the moment. We need to make a request to make sure that these can be read, stored and passed on to the output files created by the HeaderServiuce.",5
"DM-19887","05/24/2019 21:58:59","Switch skymap to geom rather than afwGeom","SkyMap uses lots of lsst.afw.geom when it mostly only needs to use lsst.geom.  Replace the afwGeom where appropriate.",0.5
"DM-19890","05/27/2019 21:45:55","Auxiliary Telescope in-dome software test","Ticket for the work being done in Chile with Aux Tel. ",2
"DM-19891","05/28/2019 02:33:18","Fix __eq__ for defects class","Zipping the bboxes only works if objects are same length, so check that first.",1
"DM-19894","05/28/2019 16:16:02","Play the Kubernetes API","This task is to play the Kubernetes API by gcloud sdk to be familiar with it.",2
"DM-19896","05/28/2019 16:19:16","Document the use of AOS closed loop simulation","This task is to document the use of AOS code by the docker-compose with the PhoSim. This task will also prepare the related docker image for Chris to write the organizer script with MTAOS.    This task also reviews the TSSW Organization and Management Document (DM-19898). ",2
"DM-19897","05/28/2019 16:34:59","M1M3 Support and Thermal Documentation","Write up some informal documentation on the M1M3 support system and M1M3 thermal system.",8
"DM-19902","05/28/2019 20:26:58","String representation of Observatory coordinates flips lat/lon","The string representation of the Observatory class prints ""W"" following the latitude and ""N"" following the longitude. These should be switched.",0.5
"DM-19905","05/28/2019 22:17:44","Attend the ComCam MIE Pre-ship Review","Attend the ComCam MIE Pre-ship Review:    [https://confluence.lsstcorp.org/display/LTS/MIE+Pre-ship+Review]",1
"DM-19908","05/29/2019 14:18:56","M2 Support part 3","Give support on the software for the M2",2
"DM-19916","05/29/2019 20:26:52","Investigate URI inconsistencies in daf_butler LocationFactory","In a [GitHub pull request|https://github.com/lsst/daf_butler/pull/160] [~dinob] makes the case that URIs in the Location classes are not working properly.  His fix is not quite right so take the PR and see if it's possible to fix things such that URIs with relative paths are properly handled.",2
"DM-19919","05/29/2019 20:35:25","Update test_enumerations_present for changes to Script enums","In DM-19875 I updated Script and ScriptQueue enums to be shared. This requires a trivial change to ts_sal's test_sal test_enumerations_present method because it changes the Script enum names.    I will implement the ticket, but I leave it to [~dmills] and [~rbovill] when to merge it, since it should be coordinated with the next release of ts_xml.",0
"DM-19952","05/30/2019 19:04:33","Partially update HeaderService for dds salobj","Convert some of hslib https://github.com/lsst-dm/HeaderService/blob/master/python/HeaderService/hslib.py for salobj as an example of how to do it.",1
"DM-19961","05/31/2019 02:32:58","Add region padding to HSC config for Gen3 raw ingest ","The investigations on DM-19731 did not (as promised there) yield config changes on DM-19638.  Add the config that fell through the cracks.",0.5
"DM-19962","05/31/2019 15:25:50","First week at the summit","Shadow Tiago, learn about integration, work on auxiliary telescope slewing, develop unit tests for ATTCS",5
"DM-19964","05/31/2019 16:01:26","Read the ts_MTAOS Code and Document","Read the ts_MTAOS code and document to be familiar with it. This task will also to learn the Python asyncio library to have an initial understanding of how to use it. This task also plans to read the code of ts_salobj.",2
"DM-19968","05/31/2019 17:14:16","Fix sphinx errors in ts_scriptqueue","ts_scriptqueue v2.0.0 has some sphinx errors caused by trying to export ScriptState. The fix is trivial.",0
"DM-19969","05/31/2019 19:35:47","Coordinate the v3.10.0 Release of SAL and XML","I was tasked with coordinating the Release of SAL and XML v3.10.0.  This task is to track that work.",1
"DM-19972","05/31/2019 20:49:45","ds9 region file upload dialog misplaced the information","The ""load region file"" dialog has the information text ""No file chosen"" outside the dialog.   Please see the attached screen.  My local Firefly build version:   Version v2018.2  Type Development  Build Number 0  Built On Fri Apr 26 16:32:00 PDT 2019  Git commit 3c4c504b8          FIREFLY-96",2
"DM-19982","06/01/2019 01:17:32","Integrate ts_MTAOS with PhoSim","Integrate ts_MTAOS with PhoSim for the AOS close loop simulation. This task will update the xml file for the intra- and extra-focal visits.",3
"DM-19989","06/03/2019 05:13:37","Document how to generate a refcat","The steps required to turn a collection of files from some archive into an LSST-style reference catalog need to be written down so that the next catalog is easier to generate. A DM Tech Note seems like the appropriate place for such a description.",8
"DM-19991","06/03/2019 11:10:18","Apply changes from deblending sprint to master","To complete the DRP deblender sprint from May the changes must be pushed to master. This ticket includes pushing the required changes to scarlet and updating {{meas_extensions_scarlet}}, {{pipe_tasks}}, and {{pipe_drivers}} with the appropriate changes so that scarlet v0.5 can be run on LSST.",3
"DM-19994","06/03/2019 15:13:40","Flow down LCR-1554 changes to DMSR","DMS-REQ-0362 derives from the OSS ellipticity correlation requirement OSS-REQ-0390.  As the latter requirement was long incorrect, presumably DMS-REQ-0362 is as well and the corrections from LCR-1554 will need to be flowed down to it, and, given the nature of the LCR, additional DMSR requirements may be needed to parallel new ones added at LSR and OSS level.",2
"DM-20008","06/04/2019 02:18:48","AuxTel translator in obs_lsst needs TSTART adjusted","The current TSTART date in {{obs_lsst}} _LsstAuxTelTranslator_ is stopping ingestion of files now that we are past that date. Since we still do not have ""mountain"" headers, that date needs to be adjusted further into the future to accommodate the situation.",1
"DM-20010","06/04/2019 14:16:05","Summit week 2","Continue summit work, including ATTCS unit tests, slewing and dome integration tests, hitting e-stop button when we accidentally slew the telescope into a crane, etc.    Returning to Tucson fri/sat",5
"DM-20019","06/04/2019 20:37:48","Fix pickling of String Fields","While finishing DM-19616, I noticed a problem with the multiprocessing Pool and a schema that contained a {{String}} {{Field}}. I added a test to afw that reproduces this error, and I will fix it on this ticket.    {code}  E           lsst.pex.exceptions.wrappers.LengthError:   E             File ""src/table/FieldBase.cc"", line 72, in lsst::afw::table::FieldBase<std::__cxx11::basic_string<char> >::FieldBase(int)  E               Size must be provided when constructing a string field. {0}  E           lsst::pex::exceptions::LengthError: 'Size must be provided when constructing a string field.'  {code}    My guess is that I need to tweak the pybind11 {{py::pickle}} code in {{schema.cc}}, possibly special-casing the String FieldBase type.",2
"DM-20020","06/04/2019 20:45:19","Update script queue to use get_scripts_dir functions for script paths","Update ts_scriptqueue to use the {{get_scripts_dir}} functions in ts_standardscripts (that function exists) and ts_externalscripts (that function will be added as part of this ticket).",1
"DM-20045","06/05/2019 23:40:32","Fix azimuth comparison in DomeTrajectoryMCS","lsst.ts.standardscripts.auxtel.integration_tests.DomeTrajectoryMCS has a bug: it compares azimuth without taking wrap into account.",1
"DM-20046","06/05/2019 23:49:00","Cleanup docstrings in detection.py","As part of our weekly brown bag discussion, we did a deep dive into SourceDetectionTask and noticed a number of confusing or misleading docstrings. Here are the notes I took during the brown bag. I'll try to translate these into cleanups shortly, so I don't forget:    {quote}  DetectionConfig: threshold, thresholdValue  DetectionTask:      detectFootprints: doSmooth & sigma      convolveImage: under what circumstances would you not want `doSmooth==True`?  Better document relationship between reEstimateBackground and thresholdParity? - `applyThreshold` has a fun pair of `if`s, which might then be removed in `clearUnwantedResults`    remove `makeSourceCatalog = run  {quote}",1
"DM-20048","06/06/2019 19:32:27","Unauthenticated Redis database - lsst-sui-tomcat01.ncsa.illinois.edu","This is a recreation of a jira ticket submitted by NCSA security in our internal ticket system:    ""Qualys scan has found an instance of Redis that seems to allow unauthenticated connections on lsst-sui-tomcat01.ncsa.illinois.edu    We would like to make sure only authorized users and hosts are able to connect to this redis instance. There is concern that maybe the Jupiter hubs users/machines can connect to this database.""",2
"DM-20052","06/06/2019 22:54:31","Rename DDSSubcriber to DDSSubscriber","There was a misspelling on salpytools that named a function  DDSSubcriber, it has been changed to to DDSSubscriber. The HeaderService need to be updated to reflect this simple change.",1
"DM-20067","06/10/2019 16:13:56","Update at_tracking to use salobj 4.0","Update the newly written at_tracking integration test to use salobj 4.0.",2
"DM-20072","06/10/2019 18:20:16","Create support infrastructure for author handling in LSST commissioning papers","For the commissioning papers (and overview paper) we need a scheme for managing author lists in a standardized way.  The overview paper has some infrastructure for this but the scripts need to be modified to separate the author list for a specific paper from the author full name and affiliation.    In theory the name and affiliation information should come from contacts DB. In the interim I will use an intermediary YAML file.",3
"DM-20073","06/10/2019 22:32:25","Add ability to construct ObservationInfo from kwargs (or allow properties to be changed)","Currently an ObservationInfo can only be constructed from the metadata of an observation. There are some cases where it would be useful to create an ObservationInfo from either kwargs or else update values in an ObservationInfo after creation (currently they are immutable).    DM-19978 would be simplified if an ObservationInfo could be constructed from known values.",2
"DM-20078","06/11/2019 01:12:33","Solar System Collaboration Sprint #2","Participate in the SSSC Sprint #2.    Prepare the materials for the LSST status update presentation, update the SSSC github with most recent source code and binaries.",5
"DM-20089","06/11/2019 03:41:04","Update remaining code in ts_standardscripts for scriptqueue 2.1","In DM-19880 I missed some code that needs updating for ts_scriptqueue 2.1. Fix this!",1
"DM-20106","06/11/2019 16:12:11","TMA Software Workshop","TMA Software Workshop in Spain with vendor.",5
"DM-20108","06/11/2019 17:05:28","Header service should not defer python imports","The header service failed yesterday whilst operating because a deferred import to Astropy could not be completed. The header service should do all required imports at launch time so that we can detect problems early.",1
"DM-20109","06/11/2019 17:47:32","Improve ip_isr log messages to be more explicit","The log messages in ip_isr occasionally contradict what actually happens due to config/data conflicts.  This is most obvious in the fringe application.  This ticket will review the log messages and add clarifications about no-ops and surprises.",2
"DM-20110","06/11/2019 17:54:26","Containerize LinearStage CSC","Containerize the LinearStage CSC into a docker file. ",2
"DM-20113","06/11/2019 20:35:00","Prepare LDM-635 for RFC","At DMLT meeting in early June I was told to prepare LDM-635 for RFC. This mainly requires I fix the sort order of the requirements.",1
"DM-20115","06/11/2019 21:31:00","Fix circular dependency between ts_scriptqueue and ts_standard/eternalscripts"," ts_externalscripts and ts_standardscripts depend on ts_scriptqueue because that is where {{BaseScript}} is defined.    ts_scriptqueue depends on ts_externalscripts and ts_standardscripts in order to find the scripts in those packages. (This is an optional dependency in the ups table).    This is a real nuisance so I propose to break the dependency by moving {{BaseScript}} into ts_salobj. It's a bit of clutter there, but otherwise we need a new package, which seems overkill for a single class and its unit tests.    For simplicity I propose to merge DM-18012 first, then implement this ticket. After both are merged to develop I will tag new versions of ts_salobj and ts_scriptqueue.",2
"DM-20121","06/12/2019 15:46:34","Make GenericCamera configurable","For this task I'll convert the GenericCamera CSC to use ConfigurableCSC. I'll also add some tests.",3
"DM-20123","06/12/2019 17:56:28","Prepare new EFD hardware for upgrades","Manage the preparation of the new EFD hardware (24 dell servers) so that they can be upgraded before shipping to Chile. Test proposed upgrades on a sample machine and then issue a req for same",3
"DM-20124","06/12/2019 17:58:00","Review updated TMA deployment documents","Review the latest set of deployment documents for the TMA",2
"DM-20125","06/12/2019 18:00:32","Review spec and order PXI substitute hardware","To properly test the TMA software in Tucson a PXI like system will be installed using a standard desktop PC. Review the spec for such a machine and issue a req for it",2
"DM-20128","06/12/2019 18:46:45","Create unit tests for scarlet","The old unit tests need to be modified and new tests need to be created for scarlet v0.5. A separate ticket will be created for unit tests in the {{meas_extensions_scarlet}}.",13
"DM-20129","06/12/2019 18:48:44","Create unit tests for meas_extensions_scarlet","Currently there are no unit tests for {{mea_extensions_scarlet}}. Tests should be implemented to ensure that changes in the stack do not break this package.",5
"DM-20132","06/12/2019 21:38:19","Rebuild containers with SAL 3.10 and redeploy containers in the lab.","I'll have to make some updates do the docker and compose files to accomplish this task. I'll create this task to capture the work. ",1
"DM-20134","06/13/2019 00:01:35","HeaderService should compute RA/DEC from AZ/EL","     For ATHeaderService The keywords {{RASTART/END}} and {{DECSTART/END}} must be calculated from the above AZ/EL keywords by the HS",5
"DM-20143","06/13/2019 20:10:29","Improve handling of blank keyword comments in FITS headers","The HeaderService is now writing FITS headers to raw data that include header cards using 8 blank characters as the special comment keyword.  This is completely within the standard but the reader code in afw.fits is confused and stores them as keyword {{""""}} and value {{None}}.  This confuses things when output files are written.    Fix the reader. It may be simpler to move these blank keyword comments into {{COMMENT}} fields in PropertyList.  Since the order is lost once comments are read there is nothing to be gained from retaining the distinction between the two types of comments.",1
"DM-20145","06/13/2019 20:15:13","Write SAL XML for the Watcher","Write SAL XML files for the Watcher in ts_xml",2
"DM-20150","06/13/2019 20:39:07","Comments shifted one character to right in current header","When looking at a header created yesterday from AuxTel the first two blank keyword comments  are shifted one character to the right:    {code}  $ fitsheader tests/data/ticket20143.fits  | grep '\-\-'           ---- Date, night and basic image information ----                                 ---- Telescope info, location, observer ----                                     ---- Pointing info, etc. ----                                                     ---- Image-identifying used to build OBS-ID ----                                  ---- Information from Camera                                                      ---- Geometry from Camera ----                                                    ---- Filter/grating information ----                                              ---- Exposure-related information ----                                            ---- Header information ----                                                      ---- Checksums ----                       {code}  ",1
"DM-20151","06/13/2019 21:13:28","Update CalSysTake*Data scripts for dds salobj and schema-based configuration","Please update the scripts CalSysTakeNarrowbandData and CalSysTakeData in ts_externalscripts for dds salobj (ts_salobj v4.1) and schema-based configuration (ts_scriptqueue v2.1).    Notes:  * ts_standardscripts already has an updated version of CalSysTakeData. I strongly recommend that you start from that, instead of what's in ts_externalscripts, and try to figure out the differences (if any). If there are no differences then please consider deleting the version in ts_externalscripts!  * I suggest you update the scripts for ts_scriptqueue v2.1, which means the schema is returned using a property named {{schema}}. There are updates coming that will make trivial further changes (truly trivial and I'm happy to do the work), but I think we'd really like to have a version of ts_externalscripts that works with ts_scriptqueue v2.1 before we make those changes.  * For the scripts you keep please add {{from calsys_take... import *}} to {{auxtel/\_\_init.py\_\_}}  * Please fix flake8 errors.    Resources:  * https://community.lsst.org/t/changes-in-salobj-4-the-dds-version/3701/2  * https://community.lsst.org/t/changes-to-sal-script-schemas-and-dds/3709/2  * DM-20089 shows the same update for some scripts in ts_standardscripts, including CalSysTakeData  * I am happy to help as wanted.",2
"DM-20152","06/13/2019 21:24:24","Update the LaserCoordination script for dds salobj and schema-based configuration","Please update the LaserCoordination script in ts_externalscripts to be compatible with dds salobj (ts_salobj v4.1) and schema-based scripting (ts_scriptqueue v2.1).    I suggest you update the script for ts_scriptqueue v2.1, which means the schema is returned using a property named {{schema}}. There is an update in review that will make trivial further changes (truly trivial and I'm happy to do the work), but I think we'd really like to have a version of ts_externalscripts that works with ts_scriptqueue v2.1 before I make the additional changes for ts_scriptqueue v2.2.    Please also fix flake8 issues. scons found the following:  {code}  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:22:80: W505 doc line too long (101 > 79 characters)  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:23:80: W505 doc line too long (105 > 79 characters)  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:24:80: W505 doc line too long (104 > 79 characters)  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:82:13: F841 local variable 'set_electrometer_mode_ack_coro' is assigned to but never used  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:83:13: F841 local variable 'set_electrometer_integration_time_ack_coro' is assigned to but never used  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:111:13: F841 local variable 'setup_ack' is assigned to but never used  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:119:25: F841 local variable 'move_ls1_ack' is assigned to but never used  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:125:29: F841 local variable 'move_ls2_ack' is assigned to but never used  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:134:29: F841 local variable 'electrometer_scan_ack' is assigned to but never used  /home/saluser/tsrepos/ts_externalscripts/python/lsst/ts/externalscripts/coordination/laser_coordination.py:146:17: F841 local variable 'stop_propagate_ack' is assigned to but never used  {code}    Here are some resources:  * https://community.lsst.org/t/changes-in-salobj-4-the-dds-version/3701/2  * https://community.lsst.org/t/changes-to-sal-script-schemas-and-dds/3709/2  * DM-20089 shows the same update for some scripts in ts_standardscripts  * I am happy to answer questions about the conversion process  * [~ecoughlin] wrote LaserCoordination and can answer questions about the script",2
"DM-20153","06/13/2019 21:27:17","Remove unused content from ts_externalscripts","Delete unused content from ts_externalscripts:  * unused utility functions  * CalSysTakeData (use the version in ts_standardscripts instead)  * LaserCharacterization (superseded by a Jupyter notebook)",0
"DM-20154","06/13/2019 22:04:22","Implement new initial WCS design","The close of DM-19951 describes a new design for how we create an initial WCS from the raw exposure VisitInfo metadata. This ticket is to implement that design. In summary:    * Write tests against sensor bounding boxes on known data (e.g. {{testdata_*}}).  * Write a new function to create a SkyWcs from VisitInfo and Detector ({{createInitialSkyWcs}}?).  * Call that function from {{exposureFromImage}}.  * Reorder the reading of raw VisitInfo and WCS metadata to deal potential metadata stripping problems.  * Check that all existing cameras use this code path, and not some custom Exposure metadata handling.  * Turn off {{addDistortionModel}} in ISR for all cameras, so that the above WCS does not get overwritten.",8
"DM-20155","06/13/2019 22:51:15","Deploy docker AT_CSCs with SAL 3.10(Lab)","Update containers and write documentation on deploying docker containers. [https://confluence.lsstcorp.org/pages/viewpage.action?pageId=110233227]         Specific CSCs included are Electrometer, LinearStage, Monochromator, ATSpectrograph (built by Tiago deployed by Eric, but remains untested).",1
"DM-20159","06/14/2019 08:57:07","Learn about k8s operators and provide a qserv-operator POC","It seems a Qserv operator for k8s would be useful. I'll study how to build that first.",8
"DM-20160","06/14/2019 14:49:52","Fix stackclub notebooks to use meas_extensions_scarlet","We need to fix the stackclub notebooks so that users in the science collaborations can make use of scarlet v0.5 and the new {{meas_extensions_scarlet}} package. This ticket includes updating the stack club tutorials and using a selection of the last scarlet DESC-BTF slides for a quick presentation to stack club.",1
"DM-20169","06/14/2019 21:31:54","Enable header fixups for decam and cfht","On Slack there has been a request for fix_header to work for DECam data. Currently there is no search path available. Update to allow CFHT and DECam to have search paths defined. It may also be necessary to patch the obs packages to call fix_header.",2
"DM-20173","06/17/2019 15:10:17","Integration and testing of AT software components","Continuation of epic DM-16910.    This epic contains stories for testing and integration of software components at the Tucson Location - dealing mainly with the pLan testing and/or any other set up for integration testing in Tucson   # Allocate Linux Machines to private network   # Connect other AuxTel devices (embedded PCs, analog I/O, etc.) to private network   # EFD setup   # Puppet Deployment of EFD",20
"DM-20174","06/17/2019 15:25:48","AT Software Component Integration at Calibration Hill","This epic holds the stories for all auxiliary telescope integration work that is happening.  This is phase two of epics:   * DM-16913   * DM-16914   * DM-16915   * DM-16943   * DM-16944         Integration to be partitioned as follows:   # ATPointing + ATMCS integration    # ATPointing + ATDome + ATDomeTrajectory integration   # ATPointing + ATMCS + ATDome + ATDomeTrajectory integration        # ATPointing + ATAOS integration    # ATAOS + ATPneumatics + ATHexapod integration   # ATPointing + ATAOS + ATPneumatics + ATHexapod integration    Required CSCs:   * ATPointing   * ATMCS   * ATDome   * ATDomeTrajectory   * ATAOS   * ATHexapod   * ATPneumatics   * ScriptQueue   * Scripts    Resources Required:   * Integration & Test Environment   * SAL   * EFD   * AuxTel Scripts    Confluence links:   * [Auxiliary Telescope Mount and Mount Control System|https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LTS&title=Auxiliary+Telescope+Mount+and+Mount+Control+System]",20
"DM-20180","06/17/2019 17:05:31","DIMM & WeatherStation Development","This epic contains the stories for the development for the DIMM and the Weather Station.  This needs to be done in conjunction with on-sky of the Auxiliary Telescope.   This is phase 2 of    * DM-17200   * DM-17668     ",5
"DM-20183","06/17/2019 18:31:01","EFD writers indicating working but no information received","After the issue on Friday with the EFD writers was fixed, the writers indicated that they were functioning. However, no information was received and inserted into the DB.",2
"DM-20184","06/17/2019 19:02:18","Alignment System (ASC)","This epic will contain the stories for development and integration of the Alignment System.   This is phase two for:   * DM-17468   * DM-17469   * DM-18756     ",20
"DM-20190","06/17/2019 21:33:55","T & S Documentation Phase 2","This is the epic that will handle all the documentation stories.   This is phase 2 from:   * DM-18378     ",5
"DM-20192","06/17/2019 22:12:35","Refactor the Jenkins build job for Imgserv","This revamp is motivated by streamlining the docker build process for SODA imgserv, including support for running unit testing framework in Jenkins-CI.    These are the 2 main tasks:  1) Remove obsolete dax / webserv job (eups build) 2) Enhance the docker build pipeline for dax_imgserv by breaking it into 3 separate stages: build, test, publish.          ",5
"DM-20194","06/17/2019 22:34:58","EFD Work Phase 2","This is hold the stories for all work done on the EFD.   This is continuation of    * DM-18555     ",20
"DM-20195","06/17/2019 22:38:20","Middleware Testing Process & Infrastructure Phase 2","This epic holds all the stories associated with testing the SAL & EFD.",40
"DM-20196","06/17/2019 22:44:40","T&S Deployment Phase 2","This epic contains stories for all deployment work.   This is phase 2 for:   * DM-17888",20
"DM-20197","06/17/2019 22:55:13","EAS Work Phase 2","This epic is used to hold all stories associated with the Environmental Awareness System (EAS).   This is to continue work from:   * DM-17215   * DM-18732",20
"DM-20198","06/17/2019 23:10:19","Dome Work Phase 2","This epic is to contain all work for the Dome.   This is a continuation from:   * DM-18309",20
"DM-20200","06/17/2019 23:28:58","Update the ts_wep to Use the Eimage","Update the ts_wep to Use the eimage. The [DM-19837|https://jira.lsstcorp.org/browse/DM-19837] had updated the phosim_utils and obs_lsst to ingest the eimage by butler.",3
"DM-20201","06/18/2019 00:30:45","Fix DECam rotator type and angle in metadata translator","DECam has its rotator angle set to NaN in {{astro_metadata_translator}}, and its rotator type to UNKNOWN. Since it is on an equatorial telescope, we can just pre-compute the correct value, set {{rotType=SKY}} and set those in the translator and they will be correct for everything (or at least correct enough to provide an initial guess).    The diagram on this page should help figure out the correct on-sky rotation angle.    http://www.ctio.noao.edu/noao/node/2250    One open question is whether we need to set a {{rotatorHandedness}} (when +Y is north, is +X is east or west?) value in {{VisitInfo}}, or whether we can easily get that from the camera geometry. At present, {{VisitInfo}} does not preserve this information, because [~rowen] assumed it would be managed by the camera geometry.",2
"DM-20202","06/18/2019 00:32:14","Fix CFHT MegaPrime rotator type and angle in metadata translator ","CFHT MegaPrime has its rotator angle set to NaN in {{astro_metadata_translator}}, and its rotator type to UNKNOWN. Since it is on an equatorial telescope, we can just pre-compute the correct value, set {{rotType=SKY}} and set those in the translator and they will be correct for everything (or at least correct enough to provide an initial guess).    There are diagrams on this page that may help figure out the correct on-sky rotation angle, though it's not as helpful as the DECam diagrams:    https://www.cfht.hawaii.edu/Instruments/Imaging/Megacam/specsinformation.html    One open question is whether we need to set a {{rotatorHandedness}} (when +Y is north, is +X is east or west?) value in {{VisitInfo}}, or whether we can easily get that from the camera geometry. At present, {{VisitInfo}} does not preserve this information, because [~rowen] assumed it would be managed by the camera geometry.",2
"DM-20203","06/18/2019 00:55:21","Configure a Jenkins job to create daily RPMs and push to Nexus yum repo","Create a Jenkins job to build the SAL library RPMs, push them to the AWS S3 buckets and then push them into the Nexus3 yum repo.",3
"DM-20205","06/18/2019 14:43:55","Refactor PipelineTask interface","This ticket will be done in two parts  * Refactor the interface used to write PipelineTasks, with knowledge learned prepairing for the gen3 demo, in preparation for a design review  * Take input from a design review and create the final interface",20
"DM-20222","06/18/2019 18:14:05","recommendedSettingsVersion information incorrect","The recommendedSettingsVersion field of the settingVersions is mis-computed by ConfigurableClass: it needs to cwd to the config directory before running the git describe command.",0
"DM-20232","06/18/2019 21:30:43","Pin version of fastavro in use in alert_stream","Per https://github.com/lsst-dm/alert_stream/issues/24, the latest release of fastavro is incompatible with the serialized alert data in lsst-dm/alert_stream.    Long term, we need to fix the data. Short term, pin the version of fastavro to one that works in the Dockerfile.",1
"DM-20240","06/19/2019 15:27:56","Update ts_sal tests for no char0 Test arrays field","Update ts_sal unit tests for DM-19474: remove char0 field from all Test array topics: setArrays command, arrays event and arrays telemetry.",0
"DM-20244","06/19/2019 20:52:23","Gen3 ISR task broken wanting dataset illum ","With w_2019_24 and its ci_hsc repo, running the Gen3 ISR task failed     For example, to reproduce:  {noformat}  pipetask -d ""visit.visit=903334"" -d ""detector.detector=22"" -j 1 -b $CI_HSC_DIR/DATA/butler.yaml   -p lsst.ip.isr -p lsst.pipe.tasks     -i raw,calib,ref/ps1_pv3_3pi_20170110  -o test1 run --register-dataset-types      -t isrTask.IsrTask:isr -C isr:$OBS_SUBARU_DIR/config/hsc/isr.py  {noformat}      The error is   {noformat}  lsst.pipe.base.graphBuilder.PrerequisiteMissingError: Deferred search failed for prerequisite dataset illum using data ID {exposure: 903334, visit: 903334, physical_filter: HSC-R, abstract_filter: r, instrument: HSC, detector: 22}.  {noformat}    ",1
"DM-20250","06/20/2019 17:19:14","Update DM glossary to refer to the EFD as the “Engineering and Facility Database”","Per [discussion on Slack|https://lsstc.slack.com/archives/C2K6YMTK2/p1561046699106000], the full name of  the service often referred to as “the EFD” is “the Engineering and Facility Database”, following LTS-210. Please update the DM glossary appropriately.    (Bonus points for fixing any other documents which define the acronym without referring to the standard glossary).",1
"DM-20251","06/20/2019 17:37:37","Improve documentation for how to register metrics with ap_verify","{{ap_verify}}'s metrics are configured using several {{MetricsControllerConfigs}}, but currently this fact is only documented in the command-line reference. Add clearer documentation (perhaps even a full tutorial), delegating to the {{verify}} documentation for the details of the actual configuration.",2
"DM-20256","06/20/2019 19:02:27","Provide licensing & copyright information for alert distribution code","Currently, there is no licensing or copyright information in either of the alert_stream or sample-avro-alert repositories.    Since this code (or, possibly, some ZTF-derived variation of the same) is now popping up in [various|https://github.com/lsst-uk/lasair/blob/e6baca8a68a43de0f66a9a5848697dfb11545023/src/alert_stream_ztf/python/lsst/alert/stream/alertProducer.py] [places|https://github.com/astrolabsoftware/fink-broker/blob/1378d4066d4540e05632e1c9b5a872d3ba4b2013/fink_broker/alertProducer.py] (apparently in some cases with copyright claims added and released under an Apache license!), we should make sure that it is properly attributed and licensed so that this is all above board.",1
"DM-20257","06/20/2019 19:37:45","To develop MQTT software interface to communicate SAL with HVAC telemetry","Install MQTT broker, to accept communications from SAL applications and HVAC telemetry",5
"DM-20261","06/21/2019 17:36:00","Test SAL 3.10 FiberSpectrograph RPM","Test and ensure all the required files are in 3.10 rpm folder in [https://project.lsst.org/ts/lsstrepo/] 3.10 tag.",2
"DM-20264","06/21/2019 18:37:42","Test Jenkins required","A test Jenkins is required in order to test that the changes implemented in DM-18112 have no impacts on the existing builds.",1
"DM-20266","06/21/2019 20:31:37","Remove DetailedState enumeration from ATHeaderService XML","The top-level detailed enumeration is unnecessary as CSCs should not repeat summary states in detailed states and should be removed from the XML.    This captures the work from:  https://jira.lsstcorp.org/browse/CAP-225",1
"DM-20267","06/21/2019 20:33:40","Add required README.txt to /datasets/refcats/htm","The developer guide [""Responsibilities"" section|https://developer.lsst.io/services/datasets.html#responsibilities-on-ingest-or-maintenance] notes that datasets need a readme, and has a specific note about where the refcat readme goes. We have no such readme for any of the {{/datasets/refcats}} subdirectories. I'll writeup a description of the Gaia DR2 data and attempt to add some description of the other existing {{htm/v1}} refcats.",1
"DM-20269","06/21/2019 21:40:21","Support the AOS Closed-loop Simulation With Eimage","The repository of ts_wep has been updated to support the eimage. This task will test the update in ts_ofc and ts_phosim repositories. The PhoSim will be used to verify the closed-loop simulation with Eimage.",3
"DM-20270","06/21/2019 21:44:14","Review the M2 Document of Phase 1","Review the documents of M2 to have an initial understanding of status. This is the phase 1.",1
"DM-20271","06/21/2019 22:07:12","ATHeaderService needs to publish offline detailed state","As specified by LSE-209, there are two detailed states in offline that should be published. The ATCamera XML contains the appropriate information and can be used as an example.     The CAP ticket is here:  https://jira.lsstcorp.org/browse/CAP-224",1
"DM-20275","06/22/2019 00:37:17","ts_salobj 4 cannot hear SAL 3.10 command acks","ts_salobj 4 cannot seem to communicate with ts_sal v3.10.0. The symptoms are that events and commands are transmitted, but command acknowledgement is not received. Also I have seen segfaults if I create the SALPY object and the salobj Domain and Remote in the same process, when things finish shutting down. But this does not seem to happen if they are in separate processes.    Add a unit test for this that uses SALPY_Test. Add ts_sal as an optional dependency of ts_salobj since the SALPY_Test module will be there.    I tried to use the ADLink Tester tool but it cannot find anything to connect to. so I get no information from it, alas.",3
"DM-20280","06/24/2019 17:13:37","jenkins agent-ldfc-0 did not come back online after NCSA 06/23 maintaince","It appears that the pod has been stuck in terminating status for the last 12 hours.    {code:java}  [jhoblitt@lsst-bastion01 ~]$ kubectl -n jenkins-prod get pods   NAME           READY   STATUS        RESTARTS   AGE  agent-ldfc-0   1/3     Terminating   1          22h  agent-ldfc-1   3/3     Running       0          22h  agent-ldfc-2   3/3     Running       0          22h  agent-ldfc-3   3/3     Running       0          22h  agent-ldfc-4   3/3     Running       0          22h  agent-ldfc-5   3/3     Running       0          22h  [jhoblitt@lsst-bastion01 ~]$ kubectl -n jenkins-prod describe pod agent-ldfc-0  Name:                      agent-ldfc-0  Namespace:                 jenkins-prod  Priority:                  0  PriorityClassName:         <none>  Node:                      lsst-kub016/141.142.181.185  Start Time:                Sun, 23 Jun 2019 12:46:07 -0500  Labels:                    app.k8s.io/component=agent                             app.k8s.io/instance=prod                             app.k8s.io/managed-by=terraform                             app.k8s.io/name=agent-ldfc                             app.k8s.io/part-of=jenkins                             app.k8s.io/version=1.0.0                             controller-revision-hash=agent-ldfc-748c96c6f6                             statefulset.kubernetes.io/pod-name=agent-ldfc-0  Annotations:               kubernetes.io/psp: privileged  Status:                    Terminating (lasts 12h)  Termination Grace Period:  30s  IP:                        10.47.128.37  Controlled By:             StatefulSet/agent-ldfc  Containers:    dind:      Container ID:  docker://f2559135a5a3b3223985c4c368bf31919b07d1c5d129b8fe389ba7e88145817a      Image:         lsstsqre/dind:18.09.5      Image ID:      docker-pullable://lsstsqre/dind@sha256:3ed62672a84eb9c1a3c214b4d8159d77263aaec159eecab13ab18d39d3ea4652      Port:          <none>      Host Port:     <none>      Command:        /usr/local/bin/dockerd      Args:        --host=tcp://localhost:2375        --mtu=1376      State:          Running        Started:      Sun, 23 Jun 2019 12:53:59 -0500      Ready:          False      Restart Count:  0      Limits:        cpu:     32        memory:  96Gi      Requests:        cpu:      6        memory:   12Gi      Liveness:   exec [wget --spider -q http://localhost:2375/_ping] delay=5s timeout=1s period=5s #success=1 #failure=2      Readiness:  exec [wget --spider -q http://localhost:2375/_ping] delay=5s timeout=1s period=5s #success=1 #failure=2      Environment:        DOCKER_HOST:  tcp://localhost:2375      Mounts:        /j from ws (rw)        /var/lib/docker from docker-graph-storage (rw)        /var/run/secrets/kubernetes.io/serviceaccount from default-token-nkqvn (ro)    docker-gc:      Container ID:  docker://6f7bafaeb25a28ccb842a0532d6d2173e972407dcbc6ad7a1ef679778ff98cb8      Image:         lsstsqre/docker-gc:latest      Image ID:      docker-pullable://lsstsqre/docker-gc@sha256:83e4a8b52eef27ebf1b469e294ffe6905fc4e5de33b30d591811c8ec802c292c      Port:          <none>      Host Port:     <none>      Command:        sh        -c        while true; do /usr/local/bin/docker-gc; sleep $GRACE_PERIOD_SECONDS; done      State:          Running        Started:      Sun, 23 Jun 2019 12:54:14 -0500      Ready:          True      Restart Count:  0      Limits:        cpu:     500m        memory:  512Mi      Requests:        cpu:     200m        memory:  100Mi      Environment:        DOCKER_HOST:              tcp://localhost:2375        GRACE_PERIOD_SECONDS:     3600        MINIMUM_IMAGES_TO_SAVE:   5        REMOVE_VOLUMES:           1        FORCE_CONTAINER_REMOVAL:  1        FORCE_IMAGE_REMOVAL:      1      Mounts:        /var/run/secrets/kubernetes.io/serviceaccount from default-token-nkqvn (ro)    swarm:      Container ID:   docker://2190d76765e077023b8d1dc284a68295aec7417a6082a401ad4d3fdbb9a96d81      Image:          lsstsqre/jenkins-swarm-client:3.15-ldfc      Image ID:       docker-pullable://lsstsqre/jenkins-swarm-client@sha256:8dfa62ed9c4323982e552c353b040701b8051c781df26f9bd77302ff58ea2f76      Port:           <none>      Host Port:      <none>      State:          Running        Started:      Sun, 23 Jun 2019 12:55:07 -0500      Ready:          False      Restart Count:  1      Limits:        cpu:     2        memory:  3Gi      Requests:        cpu:      1        memory:   2Gi      Liveness:   exec [wget --spider -q http://localhost:8080/metrics] delay=5s timeout=1s period=5s #success=1 #failure=2      Readiness:  exec [wget --spider -q http://localhost:8080/metrics] delay=5s timeout=1s period=5s #success=1 #failure=2      Environment:        DOCKER_HOST:                       tcp://localhost:2375        JSWARM_MASTER_URL:                 https://ci.lsst.codes        JSWARM_MODE:                       normal        JSWARM_LABELS:                     docker ldfc        JSWARM_EXECUTORS:                  1        JSWARM_AGENT_NAME:                 agent-ldfc-0 (v1:metadata.name)        JSWARM_DISABLE_CLIENTS_UNIQUE_ID:  true        JSWARM_DELETE_EXISTING_CLIENTS:    true        JSWARM_USERNAME:                   <set to the key 'JSWARM_USERNAME' in secret 'agent-ldfc'>  Optional: false        JSWARM_PASSWORD:                   <set to the key 'JSWARM_PASSWORD' in secret 'agent-ldfc'>  Optional: false      Mounts:        /j from ws (rw)        /var/run/secrets/kubernetes.io/serviceaccount from default-token-nkqvn (ro)  Conditions:    Type              Status    Initialized       True     Ready             False     ContainersReady   False     PodScheduled      True   Volumes:    ws:      Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)      ClaimName:  ws-agent-ldfc-0      ReadOnly:   false    docker-graph-storage:      Type:       EmptyDir (a temporary directory that shares a pod's lifetime)      Medium:           SizeLimit:  <unset>    default-token-nkqvn:      Type:        Secret (a volume populated by a Secret)      SecretName:  default-token-nkqvn      Optional:    false  QoS Class:       Burstable  Node-Selectors:  <none>  Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s                   node.kubernetes.io/unreachable:NoExecute for 300s  Events:          <none>    {code}  ",0.5
"DM-20287","06/24/2019 19:47:47","Fix logging in IsrTask","I found log messages from ISR saying:  {code}  processCcd.isr INFO: Applying brighter fatter correction.  processCcd.isr INFO: Finished brighter fatter correction in {bfResults[1]} iterations.  {code}    This incorrectly uses the new python string formatting (missing a leading {{f}} on the string). However, we generally want log messages to defer the formatting, so the old {{%f}} style should be used instead.    I suggest auditing other recent changes to the logging for similar bugs.",2
"DM-20302","06/25/2019 16:02:05","Fix PSF normalization bugs in scarlet","While updating the API in scarlet some of the tests related to PSF normalization broke. Since I am the one who wrote the tests it will be easiest for me to diagnose the cause of the differences and the code and or the tests to give the correct results.",1
"DM-20314","06/26/2019 00:16:48","Add units and update subsystem metadata to Script, ScriptQueue and Test","Add units and update metadata in SALSubsystems.xml for the Script, ScriptQueue and Test SAL components.",0
"DM-20317","06/26/2019 01:13:21","DCR model performance improvements","While processing Decam data for DM-15340 I identified a few places in building the DCR model that could be made more efficient. These improvements speed up the calculation by 5 - 40%, depending on the properties of the input exposures.",2
"DM-20325","06/26/2019 19:50:14","Add option to get password securely from dispatch_verify.py","Currently, the SQuaSH password for {{dispatch_verify.py}} is supplied via environment variable or command line option.    On shared systems, neither mechanism is very secure.  This ticket will add an option to prompt for the password when the script is run interactively.",1
"DM-20328","06/26/2019 20:34:44","Put website in place for algorithms workshop ","Work with the communications team to put in place the workshop website and set up short link to ls.st/<>    Website request form: https://project.lsst.org/website-request",1
"DM-20329","06/26/2019 20:48:04","Update the ts_MTAOS xml for Description and Unit","Update the ts_MTAOS xml for description and unit. The related tests in ts_xml will be done by Rob. This task will also take time to be familiar with the unit module in astropy.",2
"DM-20333","06/26/2019 21:23:09","Review the SHWFS and Guider Codes by Vendor","Review the SHWFS and guider code by vendor (the developer is Pauline in France). The goal is to evaluate if LSST could take this code and change them or if we needed Pauline to help us (which would require a change request).    The new Factory Acceptance Test Review dates are July 18th and 19th and Sandrine would like to have an answer by then.",1
"DM-20334","06/26/2019 21:43:16","Release approved version of LSE-319 following LCR-1728","LCR-1728 on LSE-319 has been approved by the CCB and the PM.  It now needs to be released in order to reach the CCB ""Fully Implemented"" state.  Most of this work, for a LaTeX/Github/LTD-based document, falls to the proposer, with final steps done by [~mckercher].      This tickets asks for the appropriate branch and tag management in the document release process to be done, with the products provided to Rob for final release.",1
"DM-20335","06/26/2019 21:48:14","make_idl_libs.py fails if ts_sal and ts_idl are on separate volumes","The ts_sal script make_idl_files.py uses {{os.rename}} to move the generated IDL file to ts_idl. Unfortunately this call fails if the source and destination path are on different file systems, as can easily happen when using Docker.    The simplest fix is to copy the IDL files instead of moving them.",0
"DM-20342","06/27/2019 16:47:25","Fix typo in eImageIsr","DM-19382 changed the name of a keyword in {{ip_isr.interpolateFromMask}} from {{growFootprints}} to {{growSaturatedFootprints}}, but did not update {{obs_lsstSim.processEimage}} which calls it, breaking it.",1
"DM-20373","06/27/2019 23:12:17","Move FitsRawFormatterBase from daf_butler into obs_base","I ran into ci_hsc failures due to gen2/gen3 getter comparisons, meaning I need to update the gen3 raw readers to look like the changes I made to the gen2 ones. In order to allow both butlers to use the same underlying code, we need to move {{FitsRawFormatterBase}} from daf_butler into obs_base. [~jbosch] suggested this change.",2
"DM-20378","06/28/2019 02:00:46","System for deprecating Config fields","While working on DM-20154, I wanted to mark a Config field as deprecated, but we have no system for doing that (other than just adding a docstring). [~ktl] suggested an approach on slack, which was fairly simple to implement. I'll also add a note to the dev guide on how to use it as part of this ticket.",2
"DM-20384","06/28/2019 19:06:36","Clarify usage of make_ppdb.py script","A third-party guide to using {{ap_pipe.py}} suggests using command-line arguments to {{make_ppdb.py}} but passing the database config to {{ap_pipe.py}} through a file. This is not recommended usage (if the two scripts are configured the same way it's harder to make a hard-to-debug mistake), but the current Sphinx docs don't actually say that the two scripts accept the same config.    Clarify that {{make_ppdb.py}} is configured exactly the same way as {{ap_pipe.py}}, including taking config files written for {{ApPipeConfig}}. This might be worth putting on a separate topic page under ""Using lsst.ap.pipe"" -- not a tutorial or walkthrough, but just a page explaining what {{make_ppdb}} is and why we need it.",2
"DM-20428","06/28/2019 23:57:24","SAL updates for salobj feature requests - part 1","Update SAL to include a set of features  to improve interaction with salobj based components  This will be a multi task effort over the next 2 sprints",3
"DM-20431","06/29/2019 16:01:19","Re-Implement symmetry in scarlet","[~nlust] created a branch during the deblender sprint that included a significantly improved algorithm for symmetrizing scarlet models, along with an improved centering algorithm motivated by [~jbosch]. This ticket is for implementing [~nlust]'s code in scarlet master and the LSST stack.",2
"DM-20439","07/01/2019 21:19:44","Make ts_salobj unit tests more reliable","There are three issues:  * test_csc_utils.py sometimes fails with a timeout when setting the final state to OFFLINE. I am virtually certain the problem is that the CSC exits before the final state is reported.  * test_salpy.py succeeds for me, but reports a segfault after reporting success. The segfault is caused by creating a SALPY object and a dds domain in the same process. I don't know why, but it's easy to avoid that.  * test_salpy.py fails for Tiago sometimes. I have not been able to reproduce this.",2
"DM-20440","07/01/2019 21:25:29","Update sample footprints notebook in display_firefly","The HSC-Footprints notebook in the {{display_firefly}} repository predates the Firefly extension, and also points to a data location for reprocessed HSC data that is no longer valid. Update the notebook.",0.5
"DM-20441","07/01/2019 22:12:21","catch-all exceptions in gen3 butler should use `raise from`","While debugging some changes I made related to the new astrometry code, I tripped over the {{except Exception as e:}} at {{posixDatastore.py::309}}, which swallows the initial exception. This, and any other such broad exception catching in daf_butler should use {{raise from}} exception chaining, so that the original exception information does not get lost.",0.5
"DM-20457","07/02/2019 23:06:58","Fix SAL XML for Script, ScriptQueue and Test","Make the XML for Script, ScriptQueue and Test meet our new requirements of having a non-empty Description and Units entry for each topic and fields.    Also fix an existing problem where some topic-level Explanation fields were present that should have been Description fields (from before Description was supported for topics).",0
"DM-20469","07/03/2019 18:08:24","Fix log times to use UTC with Z","Fix log times to use UTC with Z",2
"DM-20471","07/03/2019 18:40:12","Fix SAL XML for Watcher","The new Watcher XML (DM-20145) has a few errors:  - Use of Explanation instead of Description for topics  - No description for the heartbeat event",0
"DM-20481","07/08/2019 16:25:33","Review the M2 Document of Phase 2","Review the documents of M2 to have an initial understanding of status. This is the phase 2.",2
"DM-20482","07/08/2019 16:28:51","Communicate the M2 Server and cRIO Phase 1","It looks like there is no communication between the m2 server and m2 cRIO now. Need to construct the communication to let the cRIO to run. There is a risk here that it looks like the m2 laptop can not talk to the cRIO also. Need some time to dig into this. This is the phase 1.",3
"DM-20488","07/09/2019 01:06:14","Update doc layout for cbp using the modern standard","Update the documentation layout for the cbp package using the modern standard which eliminates the unnecessary ""cbp"" subdirectory and moves ""lsst.cbp"" up to the top level.    Upload the new documentation.",0
"DM-20491","07/09/2019 17:56:59","Update units and desciption for CSCs","Update units and description for CSCs FiberSpectrograph, HR, Monochromator.",2
"DM-20492","07/09/2019 19:57:01","Changes to HeaderService for ts_xml version 3.10.2","The newest xml (version 3.10.2) introduced several changes to the message content sent by ATCamera. The HeaderService needs to be modified for this reason.",3
"DM-20497","07/09/2019 22:34:33","Update ts_xml for the HeaderService for version 4.0.0","The xml for the HeaderService on ts_xml needs to be update for the upcoming release of ts_xml version 4.0.0",2
"DM-20499","07/10/2019 00:48:37","Add basic stringification to SkyWcs","To aid in debugging and identifying different SkyWcs objects, we should add at least some very basic str and repr support (using the existing {{toString}} method: it's already called by {{operator<<}}). I'll add some basic support now, and if we want more in the future we can expand it.",1
"DM-20524","07/11/2019 18:41:27","Add deepDiff_diaObject dataset types and correct deepDiff_diaSrc templates","1.  Add {{deepDiff_diaObject}} datasetType     -- This is still a bit experimental.  It's being used and tested through DESC DC2 and {{dia_pipe}}, so it seems appropriate to just include these datasetTypes here in {{obs_lsst}} and not propagate to {{obs_base}}.  2. Correct deepDiff_diaSrc datasetType templates to use {{%08d}} to format {{visit}}.    Original work done by [~rearmstr].   [~wmwood-vasey] 's work is to do some light reformatting and testing.",1
"DM-20529","07/11/2019 22:03:26","Container deployment in Chile with SAL 3.9. ","This task is to capture the work needed to (re)deploy the CSCs, Jupyter notebooks and EFD containers in Chile for the upcoming on-sky test with the Auxiliary Telescope. ",1
"DM-20536","07/12/2019 05:07:21","Can't count on `obj_idxs` being in bounds","While trying to run ap_pipe with {{preConvolution=True}}, some dataIds failed on the ap_association stage. eg:  {code}FATAL 2019-07-11T13:02:53.312-0500 apPipe ({'visit': 1238, 'ccd': 98, 'field': 'SSP_UDEEP_COSMOS', 'dateObs': '2014-03-28', 'pointing': 817, 'filter': 'HSC-I', 'taiObs': '2014-03-28', 'expTime': 270.0})(cmdLineTask.py:397)- Failed on dataId={'visit': 1238, 'ccd': 98, 'field': 'SSP_UDEEP_COSMOS', 'dateObs': '2014-03-28', 'pointing': 817, 'filter': 'HSC-I', 'taiObs': '2014-03-28', 'expTime': 270.0}: IndexError: index 38 is out of bounds for axis 0 with size 38  FATAL 2019-07-11T13:04:15.720-0500 apPipe ({'visit': 1246, 'ccd': 103, 'field': 'SSP_UDEEP_COSMOS', 'dateObs': '2014-03-28', 'pointing': 817, 'filter': 'HSC-I', 'taiObs': '2014-03-28', 'expTime': 270.0})(cmdLineTask.py:397)- Failed on dataId={'visit': 1246, 'ccd': 103, 'field': 'SSP_UDEEP_COSMOS', 'dateObs': '2014-03-28', 'pointing': 817, 'filter': 'HSC-I', 'taiObs': '2014-03-28', 'expTime': 270.0}: RuntimeError: No patches found!  FATAL 2019-07-11T13:11:07.675-0500 apPipe ({'visit': 1238, 'ccd': 99, 'field': 'SSP_UDEEP_COSMOS', 'dateObs': '2014-03-28', 'pointing': 817, 'filter': 'HSC-I', 'taiObs': '2014-03-28', 'expTime': 270.0})(cmdLineTask.py:397)- Failed on dataId={'visit': 1238, 'ccd': 99, 'field': 'SSP_UDEEP_COSMOS', 'dateObs': '2014-03-28', 'pointing': 817, 'filter': 'HSC-I', 'taiObs': '2014-03-28', 'expTime': 270.0}: IndexError: index 34 is out of bounds for axis 0 with size 34  FATAL 2019-07-11T13:18:27.189-0500 apPipe ({'visit': 1238, 'ccd': 100, 'field': 'SSP_UDEEP_COSMOS', 'dateObs': '2014-03-28', 'pointing': 817, 'filter': 'HSC-I', 'taiObs': '2014-03-28', 'expTime': 270.0})(cmdLineTask.py:397)- Failed on dataId={'visit': 1238, 'ccd': 100, 'field': 'SSP_UDEEP_COSMOS', 'dateObs': '2014-03-28', 'pointing': 817, 'filter': 'HSC-I', 'taiObs': '2014-03-28', 'expTime': 270.0}: IndexError: index 224 is out of bounds for axis 0 with size 224  {code}  which is from {{[https://github.com/lsst/ap_association/blob/master/python/lsst/ap/association/association.py#L612]}}    This looks like some failure mode where the distance is returned as {{inf}} for some {{diaSources}}, (despite your not having set an upper bound) which produces the expected behavior of returning a flag value = n points (in this case 4):  {code}(Pdb) len(dia_objects)  4  (Pdb) dists  array([           inf, 3.39470366e-04, 2.87712120e-04, 3.24986976e-04,         2.09797388e-04, 5.00443603e-04, 4.77567395e-04, 4.99864336e-04,         1.33310026e-04, 3.58528102e-04, 1.12004350e-04, 3.54529474e-04,                    inf, 1.81771009e-04, 4.18082426e-04, 1.33462009e-04,         1.48876950e-04, 2.58281375e-04, 6.19140327e-04, 4.38035230e-04,         9.45884785e-05, 2.12677353e-04, 4.38683345e-04, 6.46062325e-04,         1.88193056e-04, 4.13965388e-04, 6.06120497e-04, 4.06044351e-04,         5.83160070e-04, 3.95516906e-04, 2.74059920e-04, 2.21553182e-04,         2.26415781e-04, 5.64072359e-04, 5.79131387e-04, 6.63503591e-04,         8.34488898e-04, 4.25167947e-04, 8.32268198e-04, 4.34112545e-04,         2.04662618e-04, 3.73198069e-04, 6.91330856e-04, 7.06692744e-04,         8.75070394e-05, 9.04586832e-04, 1.64555013e-04, 4.46136353e-04,         2.84480881e-04, 1.05083764e-03, 1.17729476e-03, 7.32235889e-04,         4.84333806e-04, 6.72715508e-04, 3.60535417e-04, 5.07654976e-04,         1.03698729e-03, 1.22701689e-03, 1.31727626e-03, 1.10645719e-03,         9.48651990e-04, 5.26152766e-04, 5.42898829e-04, 5.53111663e-04,         8.86445016e-04, 1.33463361e-03, 1.05134358e-03, 7.10162430e-04,         1.31023614e-03,            inf, 1.20190626e-03, 8.42393869e-04,         8.90818870e-04, 1.61531283e-03, 1.30757514e-03, 9.61624113e-04,         1.10305108e-03, 1.41183753e-03, 1.67478608e-03, 1.12513779e-03,         1.07850540e-03, 1.18741103e-03, 1.15155631e-03, 1.93253143e-03,         1.33435522e-03, 1.88592052e-03, 1.34288681e-03,            inf,         1.59098384e-03, 1.44037035e-03, 1.45541046e-03, 1.97276848e-03,         1.83746994e-03, 1.98287152e-03, 2.20175350e-03, 1.89541959e-03,         1.81040203e-03, 1.98075464e-03, 2.31995026e-03, 2.28856630e-03,         2.30179302e-03, 2.29817933e-03, 2.37023074e-03, 2.13633563e-03,         2.09017129e-03,            inf, 2.52452285e-03, 2.63064197e-03,         2.50920251e-03, 2.16545745e-03, 2.61624181e-03, 2.39901930e-03,         2.25197331e-03, 2.63819654e-03, 2.48899074e-03, 5.01476338e-04])  (Pdb) obj_idxs  array([4, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 4, 2, 2, 0, 0, 2, 0, 0, 2, 2,         0, 0, 2, 0, 0, 0, 0, 3, 2, 3, 3, 0, 0, 0, 0, 3, 0, 3, 3, 3, 0, 0,         3, 0, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 0,         3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4,         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3,         3, 3, 3, 3, 3, 0])  {code}  I'm not sure WHY it thinks that inf is greater than inf, but regardless, I'd expect this to just return no matches.    Can reproduce from {{/datasets/hsc/repo/rerun/private/yusra/RC2/ap_pipe/w_2019_26_preConv}} with {{  ap_pipe.py /datasets/hsc/repo --calib /datasets/hsc/repo/CALIB/ --template /datasets/hsc/repo/rerun/private/yusra/RC2/ap_pipe_templates_noSkyCorr --rerun RC/w_2019_26/DM-19560-sfm:private/yusra/RC2/ap_pipe/w_2019_26_preConv --configfile ./configfile.py --no-versions  --longlog --id visit=19712 ccd=49 --reuse-outputs-from=all --clobber-config}}",2
"DM-20540","07/12/2019 19:12:35","Remove long deprecated methods from utils package","The {{utils}} package has had some methods that have been deprecated for years. Remove them.",0.5
"DM-20541","07/12/2019 19:23:35","Determine if any existing obs packages have a flip from PIXELS->FOCAL","To better understand RFC-605, we need to determine if any of our existing obs packages have a flip in their pixel->focal transform. We think they are all defined as looking ""down"" on the focal plane, but we need to check.    1. Load the cameraGeom for each obs package.  2. Compute determinant of pixel->focal transform.  3. If determinant is negative, there is a flip.  ",1
"DM-20545","07/12/2019 22:43:10","Dask client loses connection with the cluster","We have been seeing situations where the client in a notebook will lose communication with the dask cluster it is constructed with.  The cell appears to just be hung.  Sometimes it does come back, but not for a long time.  The attached notebook will show this behavior on a weekly 20 build.  Just run the cell with {{len(df)}} a few times to see this behavior.",8
"DM-20546","07/13/2019 00:04:27","Cleanup some afw deprecations","afw has some deprecated code that is not formally marked with the new {{@deprecated}} scheme. Some of these have been deprecated for many years. For deprecations over a year old, remove them. For others try to use the modern scheme.    This ticket is not fixing the {{afw.geom}} deprecations described in DM-17566.",2
"DM-20553","07/15/2019 16:59:53","Pending messages can cause a crash in the OCSBridge","If a pending RabbitMQ is received before all of the SAL proxy devices have topics registered, a message will attempt to be processed and cause a crash.",2
"DM-20558","07/15/2019 20:00:18","Investigate rescaling the coadd variances for difference imaging templates","[~yusra] identified in the June '19 False Positive Sprint that the variances were wrong by 10-15% (  https://nbviewer.jupyter.org/github/yalsayyad/dm_notebooks/blob/master/examples/DiaSourceCensusCcdVisitNight-HSC-RC2.ipynb).  This is likely due to pixel covariances introduced by coaddition and warping.  This ticket is to investigate rescaling the variances appropriately with {{ScaleVariance}}.",8
"DM-20565","07/15/2019 22:08:53","Remove afwGeom aliases for geom","The {{afwGeom}} aliases for Point, Extent, and others that are now defined in {{geom}} must be removed prior to the release of v20.0.",0.5
"DM-20570","07/15/2019 23:34:54","Pipeline failure treated as ap_verify success","{{scipipe/ap_verify}} build [#286|https://ci.lsst.codes/blue/organizations/jenkins/scipipe%2Fap_verify/detail/ap_verify/286/artifacts] was unable to run the full pipeline. However, this was counted as a successful build in Jenkins. It appears that, while  {{ap_verify}} correctly generated metrics from the failed run, it did not return any error code to the shell. Fix this behavior so that failed processing runs can be detected by CI.",2
"DM-20573","07/16/2019 00:47:36","Remove obs_lsstSim package from lsst_obs metapackage","The obs_lsstSim package must be removed from the lsst_obs metapackage and moved to legacy prior to the release v20.0.0",1
"DM-20575","07/16/2019 00:52:55","Remove meas_algorithms deprecated functions from defects.py","The meas_algorithms functions must be removed from defects.py prior to the release v19.0.0",0.5
"DM-20582","07/16/2019 22:05:31","unable to rebuild jenkins builds","Per [~nlust]  {noformat}  https://ci.lsst.codes/blue/organizations/jenkins/stack-os-matrix/detail/stack-os-matrix/30116/pipeline    that is the failed job  if I click rebuild in there it does not work, those it seems to indicate it does  same if I go here https://ci.lsst.codes/job/stack-os-matrix/30116/rebuild/parameterized  {noformat}  ",0.5
"DM-20584","07/16/2019 22:26:01","Container deployment in Chile with SAL 3.10","Task to capture the work needed to deploy the containers in Chile with SAL 3.10. ",1
"DM-20585","07/17/2019 00:40:02","Add meas_extensions_scarlet to lsst_distrib","Implementation of RFC-606.",1
"DM-20603","07/17/2019 16:02:20","Design for ComCam Archiver","Work for design of ComCam Archiver.",8
"DM-20615","07/17/2019 16:07:55","create python command to send logevents","This command will be able to send startIntegration, endReadout and largeFileObjectAvailable logevents.",2
"DM-20661","07/17/2019 18:32:03","Check the LabVIEW Dependency in M2 Cell Code","Check the LabVIEW dependency for M2 Cell code, which should be the windows version.",1
"DM-20667","07/17/2019 20:20:39","Create conda version of develop-env","Create a conda environment and document detailing how to use said environment.",2
"DM-20692","07/18/2019 19:42:30","Convert PipelineTasks to new api","DM-20205 introduces a new api for pipeline tasks. This ticket will update the existing pipeline tasks to use this new api.",20
"DM-20702","07/18/2019 22:57:48","Create memory usage metric","Create a {{MetricTask}} that captures the [{{MaxResidentSetSize}} from timeMethod|https://github.com/lsst/pipe_base/blob/master/python/lsst/pipe/base/timer.py#L100]. The task can be written by cargo-culting {{lsst.verify.tasks.TimingMetricTask}}. Add metrics for {{ApPipeTask}} and its direct subtasks to the default {{ap_verify}} config. Note that astropy supports [data size units|https://docs.astropy.org/en/stable/units/standard_units.html#prefixes].    One outstanding question is how to define aggregation (which is still the responsibility of the {{MetricTask}} for now). For running time across multiple CCDs a sum is the natural choice; it's less natural for memory usage because the aggregated data IDs weren't necessarily run together.",2
"DM-20705","07/19/2019 13:02:24","Track down Gen3 processing failures in MergeMeasurementsTask","On Slack ([https://lsstc.slack.com/archives/C2JPT1KB7/p1563485540182700), |https://lsstc.slack.com/archives/C2JPT1KB7/p1563485540182700):]    [~hchiang2] writes:  {quote}{color:#1d1c1d}The other SciPi error I saw was{color}  {code:java}ValueError: Error in inputs to MergeCoaddMeasurements: source IDs do not match {code}  {color:#1d1c1d}But if I try to reproduce it directly, I'm seeing{color}  {code:java}File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.0.0-7-g4840a288+1/python/lsst/pipe/tasks/mergeMeasurements.py"", line 298, in run      mergedCatalog.reserve(len(orderedCatalogs[0]))   IndexError: list index out of range {code}{quote}  This affects 19/79 quanta.  An example of one that failed is at  {code:java}/project/hchiang2/bps/submit/G3M19c/000001/drp/025/input/mmt/quantum083381.pickle {code}  That's (tract=9615 patch=77).",1
"DM-20711","07/19/2019 14:30:40","Improve code documentation/style","Improve code documentation/style",3
"DM-20712","07/19/2019 14:31:12","Operator-type user test run ci_hsc","Operator-type user test run ci_hsc reporting problems to developer",3
"DM-20726","07/19/2019 20:24:43","Communicate the M2 Server and cRIO Phase 2","In the phase 1 [DM-20482|https://jira.lsstcorp.org/browse/DM-20482], I found the port: 55555 is hard-coded by M2 cell code to listen to M2 server. I issued the JIRA ticket ([IHS-2398|https://jira.lsstcorp.org/browse/IHS-2398]) for the help. This task is to check the communication between M2 cell and cRIO after the IT helped to open the port and fix the problems if the communication can not work even the port is unblocked.",2
"DM-20727","07/19/2019 20:27:30","Study the Algorithm of DOF with Camera Rotation Phase 1","This task is to study how to rotate the DOF with the camera rotation angle. This task will use the PhoSim to do the evaluation and verification. This task will focus on the OPD-level first. This means only the OFC will be used. The use of WEP will be in the following tasks.",5
"DM-20733","07/23/2019 00:14:38","Office move","move to room 202, set everything back up",1
"DM-20734","07/23/2019 00:38:58","simulate correction loop in alignment system","Build up alignment system so we can run a simulated correction loop.         In this version of the simulation we aren't actually moving the mirror, we're just reading the simulated variance of the instrument. So for now I measure each component a semi-random number of times to simulate iterations in the loop. ",3
"DM-20742","07/23/2019 18:47:28","Add ""raw_header_wcs"" datasetType to butler","To fully implement RFC-616, we need a way to get the original FITS WCS from a raw file. As proposed in that RFC, this will be a ""raw_header_wcs"" datasetType in the butler (gen2 and 3).",2
"DM-20751","07/23/2019 21:13:17","Fix nightly validate_drp build","The merge of the new defects work broke validate_drp because there are no defects in the repository, but that step is not skipped.    The fix is to add defects to the {{validation_data_drp}} package.",2
"DM-20754","07/23/2019 23:51:48","Coordinate the v4.0.0 Release of XML","This story covers the time necessary to cut the v4.0.0 release of ts_xml, and make appropriate updates to the various XML and SAL tests and CI jobs.",2
"DM-20755","07/23/2019 23:56:20","Remove several CSCs and update the tests accordingly","This story covers the time necessary to remove the   * ATCalCS   * ATEEC   * ATThermoelectricCooler   * MTCalCS   * Sequencer    CSCs from ts_xml and update the tests appropriately.",2
"DM-20760","07/24/2019 00:50:34","Drop DiaSources if RA/DEC is NaN and throw warning.","The Ppdb has a non-Null constraint on RA/DEC. However, depending on the algorithm used to compute a source centroid (and translate into RA/DEC using the WCS), NaN values can result.    This ticket will tests for NaN values in the DiaSource table, drop those objects, and throw a warning to the log with the offending DiaSource id.",2
"DM-20761","07/24/2019 01:49:23","Add CWFS package to base develop-env image","Add the following package as a docker container based on develop-env image. [https://github.com/bxin/cwfs]",0
"DM-20764","07/24/2019 17:31:20","Test CSC has wrong units","The Test_Commands.xml has the wrong units (seconds) in one of its units items.   Will set it to the correct units item (second).",0
"DM-20765","07/24/2019 18:49:38","Update the CSC MTM2 xml for Description and Unit","Update the MTM2 xml for description and unit. The related tests in ts_xml will be done by Rob. There is the document provided by Harris for the description. The unit will be based on my best guess if needed.",1
"DM-20769","07/24/2019 20:22:44","Update Units and Description for CSCs","Update units and descriptions for the following CSCs. Per Tiago's request.        * ATAOS   * ATMonochromator   * ATSpectrograph   * ATTCS   * DIMM   * Environment   * Electrometer",1
"DM-20770","07/24/2019 20:51:41","Unhelpful error message from PessimisticPatternMatcherB","The call {{PessimisticPatternMatcherB(ref_array[:, :3], self.log)}} in matchPessimisticB.py around line 387 generates the error message  {quote}ValueError: negative dimensions are not allowed  {quote}  when {{ref_array}} is empty. Next time someone's in the code, could we improve the error message?  It happens when there are no possible matches from the reference catalogue.",1
"DM-20771","07/24/2019 20:55:42","IsrTaskConfig.numEdgeSuspect field duplicated","It appears at [line 255|https://github.com/lsst/ip_isr/blob/4e3c610941f9a3dc4c7bc1469fe72bcc4d0858d2/python/lsst/ip/isr/isrTask.py#L255] and at [line 437|https://github.com/lsst/ip_isr/blob/4e3c610941f9a3dc4c7bc1469fe72bcc4d0858d2/python/lsst/ip/isr/isrTask.py#L437] of {{isrTask.py}}.",1
"DM-20782","07/25/2019 18:29:52","Update meas_extensions_ngmix to meet coding standard","This package was not following python coding standards. Update and add travis check.",1
"DM-20803","07/25/2019 19:22:00","Add a simulator column to the ts_xml / SALSubsystem.xml","Please add a new item to the Subsystem tag in the ts_xml/sal_interfaces/SAL_Subsystems.xml that will signify whether a CSC has and/or contains a simulator.   The idea is this information will then appear on the ts_xml lsst.io page for reference by the project.    As always, let me know if you have any questions.",1
"DM-20805","07/25/2019 20:13:24","Split EFD writer image into one stream per container instead of three","Split EFD writer image into one stream per container instead of three",0
"DM-20807","07/26/2019 18:08:42","Test HeaderService and other CSC at NCSA for ts_xml version 4.0.0","The newest xml (version 3.10.2) introduced several changes to the message content sent by ATCamera. The HeaderService needs to be modified for this reason.",3
"DM-20808","07/26/2019 18:42:50","Expand Kapacitor rules to alert on failed validate_drp for HSC and CFHT separately","We recently found that the deadman alert on the {{validate_drp}} processing in the nightly was just looking at any data landing in the {{validate_drp}} measurements.  This meant that the if just one failed, there would be no notification.",1
"DM-20812","07/26/2019 22:00:52","Rename TablePersistable storage classes in gen 3 to drop TablePersistable","[~jbosch] requests that we change all the storage class names starting with {{TablePeristable}} to drop that prefix.  The storage class should not be describing how the dataset type should be stored and by definition they are persistable.",1
"DM-20814","07/28/2019 12:00:27","Fix broken tests in meas_extensions_scarlet","{{meas_extensions_scarlet}} tests are missing the {{setup_module}} function. For some reason this did not cause the tests to fail when built locally or with Jenkins, but did prevent the weekly build.",1
"DM-20830","07/29/2019 23:21:46","Add support for LSST IT technotes","LSST IT would like a technote series called {{ittn}}, with the repos hosted at https://github.com/LSST-IT",1
"DM-20843","07/31/2019 16:52:39","Review the Documents of Hexapod and Rotator Phase 1","Review the documents of hexapod and rotator in phase 1.",1
"DM-20848","07/31/2019 19:26:31","The showSchema ScriptQueue command and its unit test are both broken","The showSchema ScriptQueue command unit test is broken and INRIA reports that the command does not output the desired event.    The unit test is broken because the doit method is never called in an event loop.  Fix the unit test and any bugs that show up in the code being tested.",1
"DM-20850","07/31/2019 20:47:08","Add a task index page to pipelines.lsst.io","Add a index page for every task in {{lsst_distrib}} to https://pipelines.lsst.io.    This is a simpler version of the process-context-based documentation sections designed in https://dmtn-030.lsst.io/#processing-section, but it's something that is doable now with little effort.    This index page could either be linked from the homepage, or from a [Task framework page|https://dmtn-030.lsst.io/#frameworks-section].",1
"DM-20856","07/31/2019 22:12:39","Enhance ATDomeTrajectory SimpleAlgorithm to use cos(el)","ATDomeTrajectory's SimpleAlgorithm presently bases its decision on whether to move the dome on the difference between telescope and dome azimuth. Scale that difference by cos(el) to reflecting the fact that azimuth difference can be larger at higher elevation.",1
"DM-20858","07/31/2019 23:33:26","Train backup on the XML/SAL release process","As I will be traveling for much of September and October, [~cwinslow] has agreed to learn the SAL/XML release process in order to stand-in during my absence.",3
"DM-20860","08/01/2019 00:13:52","WCS match fails for image with east - right","Some image with east-right are failing to align and rotate north correct.    Please see attachments for illustration of the problem.     IPAC Firefly ticket [https://jira.ipac.caltech.edu/browse/FIREFLY-179]",8
"DM-20863","08/01/2019 16:56:05","Reduce font size of code samples in pipelines.lsst.io docs","The code sample font size in the theme for pipelines.lsst.io (https://github.com/lsst-sqre/lsst-sphinx-bootstrap-theme) ought to be just a bit smaller so that an 80-character wide code sample can fit.",1
"DM-20899","08/02/2019 19:44:05","Review the Documents of Hexapod and Rotator in Phase 2","This task is to continually review the document of hexapod and rotator. Record the questions and get the knowledge transfer from the previous owner.    The ticket for ssh help is [IHS-2490|https://jira.lsstcorp.org/browse/IHS-2490].",3
"DM-20900","08/02/2019 19:45:41","Attend the Project Community Workshop","This is the event at 8/12 - 8/16. The website is at:    [https://project.lsst.org/meetings/lsst2019/]",5
"DM-20902","08/02/2019 20:20:51","FractionUpdatedDiaObjectsMetricTask should expect 0 DIAObjects","Currently, {{FractionUpdatedDiaObjectsMetricTask}} raises {{MetricComputationError}} if there are 0 pre-existing DIAObjects. However, this is normal in the context of verification, which always starts with an empty PPDB. Therefore, the behavior of this task when there are no DIAObjects to update should be changed to return {{None}} (i.e., the metric is not applicable) rather than raising an exception (i.e., the task's calculations failed).",1
"DM-20907","08/05/2019 04:03:47","Synchronization bug in the Qserv worker monitoring service","The current implementation of the qserv worker monitoring service has a synchronization bug which (reproduced under right operational conditions) crashes Qserv at the following location:  {code}  Core was generated by `/qserv/stack/stack/miniconda3-4.5.12-1172c30/Linux64/xrootd/master-g0f29de9b39/'.  Program terminated with signal 11, Segmentation fault.  #0  0x00007f5c1a4501a3 in lsst::qserv::wsched::SchedulerBase::statusToJson (this=0x0) at core/modules/wsched/SchedulerBase.cc:122  122    core/modules/wsched/SchedulerBase.cc: No such file or directory.  Missing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.el7.x86_64 elfutils-libelf-0.172-2.el7.x86_64 elfutils-libs-0.172-2.el7.x86_64 glibc-2.17-260.el7_6.6.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.15.1-37.el7_6.x86_64 libattr-2.4.46-13.el7.x86_64 libcap-2.22-9.el7.x86_64 libcom_err-1.42.9-13.el7.x86_64 libgcc-4.8.5-36.el7_6.2.x86_64 libselinux-2.5-14.1.el7.x86_64 libstdc++-4.8.5-36.el7_6.2.x86_64 libuuid-2.23.2-59.el7_6.1.x86_64 nss-softokn-freebl-3.36.0-5.el7_5.x86_64 openssl-libs-1.0.2k-16.el7_6.1.x86_64 pcre-8.32-17.el7.x86_64 systemd-libs-219-62.el7_6.7.x86_64 xz-libs-5.2.2-1.el7.x86_64 zlib-1.2.7-18.el7.x86_64  (gdb) where  #0  0x00007f5c1a4501a3 in lsst::qserv::wsched::SchedulerBase::statusToJson (this=0x0) at core/modules/wsched/SchedulerBase.cc:122  #1  0x00007f5c1a436382 in lsst::qserv::wsched::BlendScheduler::statusToJson (this=0x16ad850) at core/modules/wsched/BlendScheduler.cc:431  #2  0x00007f5c1a40ff95 in lsst::qserv::wpublish::QueriesAndChunks::statusToJson (this=0x1d246e0) at core/modules/wpublish/QueriesAndChunks.cc:337  #3  0x00007f5c1a3b512e in lsst::qserv::wcontrol::Foreman::statusToJson (this=0x1c6f040) at core/modules/wcontrol/Foreman.cc:120  #4  0x00007f5c1a405327 in lsst::qserv::wpublish::GetStatusCommand::run (this=0x7f5ba0561b40) at core/modules/wpublish/GetStatusCommand.cc:62  ...  {code}  It turns out the monitoring code is iterating over a mutable collection of schedulers which may be sorted by other threads. The code where this happens is:  {code}  nlohmann::json BlendScheduler::statusToJson() {      nlohmann::json status;      status[""name""] = getName();      status[""priority""] = getPriority();      status[""num_tasks_in_queue""] = getSize();      status[""num_tasks_in_flight""] = getInFlight();      nlohmann::json schedulers = nlohmann::json::array();      for (auto&& sched: _schedulers) {          schedulers.push_back(sched->statusToJson());  {code}  Reinforce the algorithm to acquire a lock before iterating over the collection.",1
"DM-20910","08/05/2019 16:14:37","Design salobj to Kafka feeder","Design the salobj to Kafka feeder and negotiate with [~afausti] and [~jsick] about details of the schema. I hope they will be willing to change the Avro schema to match the DDS topics.    Another point to discuss is to consider using the IDL or Python topic data classes, instead of the XML files, to create the Avro schema. However, unless doing such a change requires me to write some code, this is a side issue.    Figure out how to write unit tests for this code.    The product will be a preliminary implementation, not necessarily tested.",2
"DM-20913","08/05/2019 17:41:56","Check the M2 MATLAB Tool","Check the M2 MATLAB tool.",1
"DM-20915","08/05/2019 21:04:11","Add stringification to Formatter","DM-20842 added a location attribute to Formatter. It would be helpful if {{str(Formatter)}} returned the formatter name and location.  Also require the fileDescriptor argument.",1
"DM-20917","08/06/2019 16:46:30","In SODA shape CIRCLE, radius should be in degrees by default","Per Section 3.3.1 POS of the SODA 1.0 spec, all longitude and latitude values (plus the radius of the CIRCLE) are expressed in degrees in ICRS.    In addition to SPACE as delimiter to separate the values, COMMA was desired during testing. ",2
"DM-20918","08/06/2019 17:05:44","Implement BAND  in SODA to specify the wavelength intervals","Per Section 3.3.4 BAND of SODA 1.0 spec, the BAND parameter defines the wavelength interval(s) to be extracted from the data using a floating point number.    In LSST, the concept has been described using filters: u, g, r, i, z, y    Mapping these filters to corresponding BAND parameter values and recognizing them inside the code will be the gist of this ticket.",5
"DM-20921","08/06/2019 20:09:37","Clarify that ap_verify needs to be set up before running ap_verify.py","The usage instructions in the dataset repositories assume the user already has a Stack set up. Make it explicit that the user needs to set up either {{ap_verify}} or {{lsst_distrib}} before running {{ap_verify.py}}. Reword both the readme and the Sphinx docs, if appropriate.",1
"DM-20922","08/06/2019 20:12:55","ap_verify can't handle --id with empty argument","[~Parejkoj] discovered that {{ap_verify.py}} will misunderstand the {{\-\-id}} argument if it is followed by anything other than a data ID (e.g., {{ap_verify.py \-\-id \-\-output lotsOfRepos}}). Modify the argument parser so that the ID is optional, if this can be done without making {{\-\-id}} mandatory.",2
"DM-20924","08/06/2019 21:15:51","Release astro_metadata_translator to PyPI","Do a formal release of astro_metadata_translator to PyPI.  This will include determining a formal version number and tagging the repository.",2
"DM-20929","08/07/2019 00:46:41","Fix bug in DcrAssembleCoadd when running with slurm","DcrAssembleCoaddTask fails when running on Decam data on lsst-dev using slurm, though the same data runs fine using AssembleCoaddTask. I've copied the error below, which appears to be caused by the coaddInputs not being set up properly before trying to use them to compute a coaddPsf.       {code:}dcrAssembleCoadd FATAL: Failed on dataId=DataId(initialdata={'filter': 'g', 'tract': 0, 'patch': '12,12'}, tag=set()): LengthError:     File ""include/lsst/afw/table/Catalog.h"", line 186, in lsst::afw::table::CatalogT<RecordT> lsst::afw::table::CatalogT<RecordT>::subset(const ndarray::Array<const bool, 1>&) const [with Recor  dT = lsst::afw::table::ExposureRecord]      Mask array with 4 elements applied to catalog with 8 elements {0}  lsst::pex::exceptions::LengthError: 'Mask array with 4 elements applied to catalog with 8 elements'/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.0.0-8-g24ce6f0f+8/python/lsst/pipe/tasks/scaleVariance.py:147: RuntimeWarning: invalid value encountered   in sqrt    snr = maskedImage.image.array/np.sqrt(variance.array)  Traceback (most recent call last):    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.0.0-2-g0ee56d7+10/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.0.0-2-g0ee56d7+10/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.0.0-2-g0ee56d7+10/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.0.0-8-g24ce6f0f+8/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 294, in runDataRef      warpRefList=warpRefList)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.0.0-2-g0ee56d7+10/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.0.0-8-g24ce6f0f+8/python/lsst/pipe/tasks/assembleCoadd.py"", line 525, in runDataRef      inputData.weightList, supplementaryData=supplementaryData)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.0.0-8-g24ce6f0f+8/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 471, in run      dcrModels = self.prepareDcrInputs(templateCoadd, warpRefList, weightList)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.0.0-8-g24ce6f0f+8/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 395, in prepareDcrInputs      psf = self.selectCoaddPsf(templateCoadd, warpRefList)    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.0.0-8-g24ce6f0f+8/python/lsst/pipe/tasks/dcrAssembleCoadd.py"", line 1136, in selectCoaddPsf      psf = measAlg.CoaddPsf(ccds[goodVisits], templateCoadd.getWcs(),    File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/afw/18.0.0-10-g6da83990c/python/lsst/afw/table/_base.py"", line 117, in __getitem__      return self.subset(key)  lsst.pex.exceptions.wrappers.LengthError:     File ""include/lsst/afw/table/Catalog.h"", line 186, in lsst::afw::table::CatalogT<RecordT> lsst::afw::table::CatalogT<RecordT>::subset(const ndarray::Array<const bool, 1>&) const [with Recor  dT = lsst::afw::table::ExposureRecord]      Mask array with 4 elements applied to catalog with 8 elements {0}  lsst::pex::exceptions::LengthError: 'Mask array with 4 elements applied to catalog with 8 elements' {code}   ",2
"DM-20931","08/07/2019 02:26:13","Implement pinning of Firefly version in suit repo","Per today's discussion, this ticket asks that the {{suit}} package be modified to include a script that encapsulates the full build process for that package, including its dependency on an explicitly specified version of [Caltech-IPAC/firefly|https://github.com/Caltech-IPAC/firefly].    (The equivalent of this already exists in the IPAC Jenkins scripts repo for Firefly/suit builds, so the task here is just to ensure that that information is in the {{suit}} repo itself, to facilitate LSST-driven builds of the package.)    The Firefly version may be specified in the script itself or in a supporting data file.  [~gpdf] has a mild preference for the latter, decoupling the build _logic_ from the version numbering.    This is a high-priority task as we wish to converge with LSST on an LSST-maintainable build process by the end of FY19.",2
"DM-20933","08/07/2019 17:31:00","Epic to track folks time at the PCE","This epic is used to track time taken in the PCW (All-hands) meeting",40
"DM-20935","08/07/2019 18:41:24","PCW2019 - Rob","Story covering time at the LSST PCW for 2019.",5
"DM-20937","08/08/2019 00:39:03","Add deprecation warnings for to-be-removed distorted WCS functions","The changes in DM-20154 should result in some code being deprecate-able. {{makeDistortedTanWcs}} and {{computePixelToDistortedPixel}} are the first ones that comes to mind, but there may be others (e.g. in ip_isr). We should use the deprecation decorators so we can remove that code later.",1
"DM-20941","08/08/2019 18:22:43","Remove python_future dependency from lsst.verify","Remove use of the {{future}} package in {{lsst.verify}}.    Also fix third party packages that were accidentally removed in DM-17205.",1
"DM-20942","08/08/2019 18:26:32","Test and release SAL V4","Test the new features of SAL V4 and the deployment mechanisms",5
"DM-20943","08/08/2019 18:28:42","LSST 2019 meeting","Attend LSST 2019 project meetings",5
"DM-20946","08/08/2019 21:56:34","PCW 2019","Attend PCW 2019",5
"DM-20952","08/09/2019 16:55:17","Update latex technote template to use latexmk","The current Makefile for the [technote_latex template|https://github.com/lsst/templates/tree/master/project_templates/technote_latex] runs xelatex and BibTeX manually. However, this fails on the first-run experience because there aren't any citations, and BibTeX fails. Using latexmk is a solution because it's smart enough to know whether BibTeX needs to be run in the first place or now.",0.5
"DM-20963","08/12/2019 18:18:42","inspect_job.py broken","It is no longer possible to run {{inspect_job.py}} from the command line:  {noformat}  $ inspect_job.py foo/ap_verify.date2015-02-18_filterg_visit411371_ccdnum56_objectBlind15A_42.verify.json   Traceback (most recent call last):    File ""verify/bin/inspect_job.py"", line 28, in <module>      main()  TypeError: main() missing 1 required positional argument: 'filenames'  $ inspect_job.py -h  Traceback (most recent call last):    File ""verify/bin/inspect_job.py"", line 28, in <module>      main()  TypeError: main() missing 1 required positional argument: 'filenames'  {noformat}    Figure out what happened and fix.",1
"DM-20978","08/14/2019 18:00:42","Stub out base lsstimport and move lsstcppimport import to afw","After some investigation by [~ctslater] we have learned that the {{lsst/\_\_init__.py}} files do not need to import {{lsstimport}} from the {{base}} package. The library loader flags are no longer relevant for our pybind11 packages.    Furthermore we have learned that the {{lsstcppimport}} _is_ required but is only required by some packages that are using afw C++ interfaces.    We therefore propose the following:    1. Replace lsstimport.py with an empty stub file that does nothing.  2. Force afw to import lsstcppimport.    In a later ticket we will remove the {{lsst/\_\_init__.py}} files from all the LSST python packages. Once that is done {{lsstimport.py}} can be removed from {{base}} package.",1
"DM-20984","08/15/2019 04:48:36","Moving Work and logistics","Substantial time was taken out of my week meeting renters to rent my home. Visiting the vet to prepare required paperwork for the move. Visiting dealerships to sell my car. Emailing Erin and the rest of HR to get my ticket planned. Calling the airlines to ensure my dogs can board. Meeting with the movers to move all my household goods.     This task capture that time.",2
"DM-20985","08/15/2019 04:51:57","Create the pub sub program","A continuation of [] creating the scripts to generate the .robot files that test the single file Java Telemetry programs.",2
"DM-20997","08/15/2019 23:41:10","Attend the LabVIEW Core 3 Training","Attend the LabVIEW core 3 training in CA.",3
"DM-20998","08/15/2019 23:47:04","Review the Walk-through Record of Hexapod and Rotator Software","Review the walk-through record of hexapod and rotator software in the following link:    [https://confluence.lsstcorp.org/display/LTS/Software+Code+Walkthrough#/]         PS. Put the story point of [DM-21014|https://jira.lsstcorp.org/browse/DM-21014] here.",1
"DM-20999","08/15/2019 23:50:04","Check the SAL Telemetry in M2 Controller","Check the SAL Telemetry in M2 controller. In the previous time, I had rebuilt and re-deployed the M2 cell code in cRIO and there is the connection between M2 controller and cRIO now. However, it looks like there is no SAL telemetry. This task will dig into code to try to fix it.",3
"DM-21003","08/16/2019 18:21:43","pex_exceptions wrapper exceptions should only inherit from one kind of python exception","pex_exceptions exception wrapper, likely as a residual effect of future usage from python, inherits from multiple python exceptions. This seems to break some tests, as in the case of a test case from utils. The inheritance should be simplified.",0.5
"DM-21004","08/16/2019 18:55:03","Add writeInitialModel option to jointcal config","Ian Dell'Antonio (appears not to be on Jira) described a jointcal bug on slack where all of the on-sky residuals after initialization were NaN, but the tangent plane values were reasonable. The existing debug information jointcal can provide was not enough to readily narrow down the source of this; there may be something pathologically wrong with the initial model. To help debug such problems, we need a way to write out the initial model to disk. A {{writeInitialModel}} option in the jointcal config that just dumps text files for each model component would be a good start: I think all of the AstrometryTransforms have useful string representations.",2
"DM-21007","08/16/2019 21:43:49","Investigate setting up an information for community brokers public facing webpage ","Many members of community broker teams are not LSST project members and so do not receive information communicated via usual project channels (slack, community).  An LSST 'information for brokers' webpage similar to the [LSST Information for Scientists|https://www.lsst.org/content/lsst-information-scientists] page could be a good way to communicate with the wider community",1
"DM-21009","08/19/2019 18:54:59","Refactor S3 and Posix datastores to remove code duplication","The new S3Datastore duplicates a lot of logic from PosixDatastore. This is fine in the short term but we need to try to refactor a bit to prevent bugs appearing when a fix is made in one datastore but not the other.",5
"DM-21013","08/19/2019 22:02:32","Add description for daf_butler expression parser","We need to add documentation for the expression syntax understood by exprParser.",2
"DM-21014","08/19/2019 22:19:13","Fix the M2 Configuration Editor by MATLAB","Fix the M2 configuration editor by MATLAB, which has the broken GUI with missing button, text, and configuration files.         PS. Use the story point of [DM-20998|https://jira.lsstcorp.org/browse/DM-20998].",0
"DM-21016","08/20/2019 01:07:32","Handle DECam instrument signature data in gen3","DM-20763 dealt with ingesting DECam data, but punted on managing the instrument signature data. This will require at-minimum implementing {{writeCuratedCalibrations}}.    I've linked a variety of DECam calibration-related tickets as ""related', since we can maybe close some of those as won't fix once this is done (since they might be gen2-specific).",8
"DM-21031","08/20/2019 23:45:15","Coordinate the v4.1.1 Release of XML","This task covers the time to release v4.1.1 of the XMLs, as well as work with [~cwinslow] to train him to cover this work when I am unavailable.",2
"DM-21032","08/20/2019 23:46:30","Coordinate the v4.0.0 Release of SAL","This task covers the time to release v4.0.0 of SAL, update the tests accordingly, as well as work with [~cwinslow] to train him on the SAL release process.",3
"DM-21035","08/21/2019 04:36:42","Upgrade DIMM CSC to salobj 4 and make it a configurable CSC","DIMM CSC needs to be upgraded to SalObj 4 and also become a configurable CSC.",1
"DM-21039","08/21/2019 18:34:32","Fix calib reference in plot_photoCalib","[~lauren] pointed out on slack that I'd missed a `calib` reference in jointcal's plot_photoCalib script:    https://github.com/lsst/jointcal/blob/master/bin.src/plot_photoCalib.py#L143    I need to fix that, and I should check if any of the other bin/ scripts in jointcal have a similar issue.",0.5
"DM-21043","08/21/2019 20:19:02","Qserv log diet: use named context for query ID","Bunch of logging messages from qserv uses explicit ""QI=12345"" (or ""QI=12345:123"") to identify query ID. It would be better to replace it with a logging context if possible.",5
"DM-21053","08/22/2019 22:04:09","Officially deprecate meas_mosaic","The implementation of RFC-617's DM-20548 effectively breaks {{meas_mosaic}} and the RFC calls for its deprecation. Please perform the necessary steps to remove it from {{lsst_distrib}} and move it from the lsst GitHub organization to that of lsst-dm (as a legacy repo?)",1
"DM-21055","08/22/2019 22:35:43","Do not fill output collection if QuantumGraph is empty.","John Parejko was having issues with re-running pipeline on output collection:  https://lsstc.slack.com/archives/C2JPT1KB7/p1566508300359600  {noformat}  If I change the output collection with -o something, the first attempt fails with QuantumGraph is empty (even with @hsinfang’s updated -i arg above), while the second attempt fails with Output dataset icSrc_schema already exists.  {noformat}    I do not expect to be written if QuantumGraph is empty , need to check what is happening.",2
"DM-21072","08/26/2019 18:09:51","Add LOVE events","LOVE is now a SAL component. Add suitable events.",0
"DM-21078","08/26/2019 21:12:51","Fix pipelines.lsst.io build issue from d_2019_08_23","Since {{d_2019_08_23}} the pipelines.lsst.io builds have been breaking. ([#599|https://ci.lsst.codes/job/sqre/job/infra/job/documenteer/599/display/redirect])    The issue seems to be in {{obs_base}}:    {code}  Traceback (most recent call last):    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/documenteer/sphinxrunner.py"", line 85, in run_sphinx      app.build(force_all, filenames)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/application.py"", line 325, in build      self.builder.build_all()    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/builders/__init__.py"", line 299, in build_all      self.build(None, summary='all source files', method='all')    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/builders/__init__.py"", line 359, in build      for docname in self.env.check_dependents(self.app, updated_docnames):    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/__init__.py"", line 623, in check_dependents      for docnames in app.emit('env-get-updated', self):    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/application.py"", line 444, in emit      return self.events.emit(event, self, *args)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/events.py"", line 79, in emit      results.append(callback(*args))    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 135, in get_updated_docs      return self.assign_section_numbers(env) + self.assign_figure_numbers(env)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 278, in assign_figure_numbers      _walk_doc(env.config.master_doc, tuple())    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 275, in _walk_doc      _walk_doctree(docname, doctree, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 253, in _walk_doctree      _walk_doctree(docname, subnode, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 253, in _walk_doctree      _walk_doctree(docname, subnode, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 269, in _walk_doctree      _walk_doctree(docname, subnode, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 261, in _walk_doctree      _walk_doc(subdocname, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 275, in _walk_doc      _walk_doctree(docname, doctree, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 253, in _walk_doctree      _walk_doctree(docname, subnode, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 253, in _walk_doctree      _walk_doctree(docname, subnode, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 253, in _walk_doctree      _walk_doctree(docname, subnode, secnum)    [Previous line repeated 1 more time]    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 269, in _walk_doctree      _walk_doctree(docname, subnode, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 261, in _walk_doctree      _walk_doc(subdocname, secnum)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/collectors/toctree.py"", line 274, in _walk_doc      doctree = env.get_doctree(docname)    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/__init__.py"", line 785, in get_doctree      with open(doctree_filename, 'rb') as f:  FileNotFoundError: [Errno 2] No such file or directory: '/Users/jsick/lsst/pipelines_lsst_io_stack/pipelines_lsst_io/_build/doctree/py-api/lsst.obs.base.makeSkyWcs.doctree'    Exception occurred:    File ""/Users/jsick/lsst/pipelines_lsst_io_stack/stacks/d_2019_08_26/python/miniconda3-4.5.12/envs/lsst-scipipe-1172c30/lib/python3.7/site-packages/sphinx/environment/__init__.py"", line 785, in get_doctree      with open(doctree_filename, 'rb') as f:  FileNotFoundError: [Errno 2] No such file or directory: '/Users/jsick/lsst/pipelines_lsst_io_stack/pipelines_lsst_io/_build/doctree/py-api/lsst.obs.base.makeSkyWcs.doctree'  The full traceback has been saved in /var/folders/n6/2vtqtjr90fj1dftc8j4290w80000gn/T/sphinx-err-pqc580lx.log, if you want to report the issue to the developers.  Please also report this if it was a user error, so that a better error message can be provided next time.  A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!    {code}",0.5
"DM-21097","08/27/2019 22:40:45","Improve the current_tai salobj function to read TAI directly if available","In DM-20849 I added a {{current_tai}} function to ts_salobj. However, it works by calling {{time.time()}} and converting it to TAI using {{tai_from_utc}}. This is likely to be slightly off very near a leap second transition. (Fortunately we are not expecting one anytime soon, so there is time to deal with this).    The proposed long-term solution is to call Linux function {{clock_gettime(CLOCK_TAI, ...)}}. There are a few subtleties:  * By default this will return UTC, not TAI. Only on systems with a an NTP or PTP daemon configured to set the current TAI-UTC will this actually return TAI. We need to decide how to handle this. At present very few of our current CSCs have ptp or ntp configured such that CLOCK_TAI returns the correct value. This will, of course, change -- at least for deployed software. I am less sure if we can make it happen for our development environments.  * Even if TAI-UTC is not zero it may still be incorrect. That is harder to handle and I do not propose to try to detect it in salobj. It can easily be checked in the Watcher or by an EFD monitor.  * Some operating systems do not have CLOCK_TAI, including macOS. However, we think all our deployment or development systems support CLOCK_TAI.    Options include:  * Silently return UTC (as SAL does). I hate silent failures, but it makes for simple code and is a appropriate choice if our systems are almost always correctly configured (even our development environments).  * Raise an exception. This seems far too drastic.  * Return TAI (correct the time using AstroPy). This is rather complex, but I believe it necessary if we adopt this code now, because few CSCs have ptp or ntp correctly configured.    I think salobj should also provide a way to test CLOCK_TAI, e.g. return TAI-UTC. CSCs could do this as they start up and log a message if ptp is mis-configured.    I am inclined to do two things:  1) Silently return UTC  2) As a CSC starts up test the value to see if it matches UTC, and if it does, log a warning about clock error.    Another option is to use ts_sal directly, but this has several issues:  * At present ts_sal only makes the functions available as part of a SALPY library, and in particular as part of the SAL object. It is not safe to construct a SAL object in code that uses OpenSplice dds (it leads to segfaults). In any case we will want a smaller more focused library.  * We still have to perform the checking mentioned above. ts_sal's getCurrentTime function silently returns UTC not TAI unless something such as an ntp or ptp daemon is properly configured.",1
"DM-21103","08/28/2019 04:00:53","lsst.verify seems to be broken","If you unpack the attached tarball and run test_load_metrics.py, you will find that it works on w.2019.29 but not on w.2019.33.  test_load_metrics.py just tries to load a MetricsSet from a local directory.  The commissioning team was planning to use this sort of interface in building its test notebooks.",0.5
"DM-21104","08/28/2019 07:34:57","Upgrade GSL to v2.6 to see if this fixes intermittent failures to build gsl on macOS","A few times a month, Jenkins fails to build {{lsst_distrib}} because one or the other macOS platform cannot build the {{gsl}} package.  It is always due to an include file not being found, but the files differ.    [https://ci.lsst.codes/job/scipipe/job/lsst_distrib/732/artifact/osx-10.14.clang-1000.10.44.4.py3/lsstsw/build/gsl/_build.log] is an example.",0.5
"DM-21106","08/28/2019 18:44:17","Please add a salgenerator command that just generates the rev-coded IDL","Please add a salgenerator command that just makes rev-coded IDL files (e.g. sal_revCoded_ATCamera.idl) -- the files used by dds salobj.    The main purpose is to speed up generation of these IDL files, since there would be no need to create C++ or other source code.",2
"DM-21110","08/28/2019 20:56:05","SAL 4 is adding content that looks extra and unwanted to IDL files","The SAL 4 branch is adding some fields to the revcode IDL that I suspect are not wanted. I don't know if older SAL also does this.    After all the command and events, but before the ""ackcmd"" topic I see these entries, which stand out because they are indented (just as shown here). One command with no name and one logevent with no name:  {code} struct command {     string<8> private_revCode;     double private_sndStamp;     double private_rcvStamp;     long  private_origin;     long   private_host;     long  private_seqNum;     long TestID;     string<32> device;     string<32> property;     string<32> action;     string<32> itemValue;     string<128> modifiers;   };   #pragma keylist command TestID   struct logevent {     string<8> private_revCode;     double private_sndStamp;     double private_rcvStamp;     long  private_origin;     long   private_host;     long  private_seqNum;     long TestID;     string<128> message;   };   #pragma keylist logevent TestID  {code}  I suspect these should not be present and am worried they may confuse code that tries to parse the IDL, such as the sal/kafka producer (it will parse IDL files to find descriptions and units).",1
"DM-21117","08/28/2019 21:52:40","Document sqrbot-jr create project and create file usage in developer guide","The developer guide should reference the usage of {{@sqrbot-jr create project}} and {{@sqrbot-jr create file}}:    - To create technotes  - To create new repositories  - To create license boilerplate and preambles  - To create documentation topics",0.5
"DM-21129","08/29/2019 20:15:08","Improve ""unsupported operand types"" error for afwImage arithmetic","If a user tries to add two images, or multiply an image by a constant, or do any kind of not-in-place image arithmetic, they get an unfriendly {{TypeError: unspotted operand type(s)}}.    This ticket is to clarify the error message and suggest an alternative, for example, ""afwImages must be modified in place, e.g., if you want image1 + image2, try image1 += image2.""",1
"DM-21130","08/29/2019 20:42:07","Please add asynctest to Docker images","Please add the https://github.com/Martiusweb/asynctest package to Docker containers used for Python T&S software. This adds some useful support to unittest, including the ability to write asynchronous test cases directly, e.g.:  {code}  async def test_foo(self):      # code  {code}  instead of  {code}  def test_foo(self):      async def doit(self):          # code        asyncio.get_event_loop().run_until_complete(doit())  {code}  (which silently does nothing if one forgets the event loop call)",1
"DM-21132","08/29/2019 21:05:01","SalObj 4 Bootcamp","SalObj 4 Bootcamp",1
"DM-21138","08/29/2019 22:56:10","Check the M2 MATLAB Tools with Functions by Harris","Check the M2 MATLAB tools with functions provided by Harris. The attached files ([^FunctionsForLSST.zip]) are the missing functions provided by Harris. This is a follow-up task of [DM-21014|https://jira.lsstcorp.org/browse/DM-21014].",1
"DM-21147","09/01/2019 16:54:15","Update capitalization guidelines in dev guide as per RFC-623","Update the developer guide as proposed in RFC-623.",2
"DM-21153","09/03/2019 19:04:37","Fix bugs in DcrAssembleCoaddTask from PipelineTask merge","The merge of DM-20692 changed the call signature of {{AssembleCoaddTask.processResults()}}, which is used in {{DcrAssembleCoaddTask}}.  The breaking change was not caught by existing unit tests.",2
"DM-21154","09/03/2019 19:05:08","Bad credentials for authentication to rabbitmq server cause ungraceful failure","Ran into an issue where bad login credentials to attach to the RabbitMQ server caused  repeated failures (many repeated logging messages).   When this happens, it should be caught gracefully.",1
"DM-21156","09/03/2019 19:10:42","Transition to FAULT state should also disable heartbeat","Transitioning to the FAULT state should also disable the heartbeat, which it currently doesn't do, but should.",2
"DM-21158","09/03/2019 21:03:54","Upgrade the M2 Cell Control System in cRIO to LabVIEW 2018 SP1","This task is to update the M2 cell control system in cRIO to use the LabVIEW 2018 sp1 32 bit version. It is using the LabVIEW 2015 sp1 32 bit at this moment. The followings are the steps:   # Install the LabVIEW 2018 sp1 32 bit version and related toolboxs in the windows laptop.   # Fix the M2 cell projects for the possible interface changes between the LabVIEW 2015 sp1 and 2018.   # Deploy the M2 cell code to cRIO.   # Test the connection between M2 controller and cell.",2
"DM-21164","09/03/2019 23:07:47","Extend example notebook from Angelo to complex use cases","Extend the notebook produced in DM-21067 to show more complicated investigations.  Examples would be:    * Correlating series from different measurements in the EFD: current vs. temperature or similar  * Show how to investigate annotations in a notebook.  * Log entries with telemetry streams.",3
"DM-21167","09/04/2019 00:46:56","Migrate ap_association fully to new DiaCalculation plugin system for time-series features.","Remove pervious code in ap_association that produces static and time-series features and replace them with the new DiaCalculation measurement pluggin system created in DM-18494",8
"DM-21169","09/04/2019 16:21:29","Add Corner Rafts with correct positions and rotations to obs_lsst","The nominal positions and orientations of the CCDs in the corner rafts can be found on page 4 of ls.st/lca-13381.   [~roodman] can provide details on the as-built rotations and offsets.",5
"DM-21171","09/04/2019 18:11:56","Create task to add local PhotoCalib and WCS value to source table rows","As part of the ScienceDataModel we need to have calibration products available for use in the Functors in the SDM code. One solution to making these products available is to compute their local values and store them per row in the afw tables output by various SciencePipelines tasks. This ticket will implement a task to perform this operation.",8
"DM-21175","09/04/2019 21:11:47","Try using a single DDS sample cache shared by the script queue and scripts","The ADLink consultant we hired suggested that we use a single DDS cache shared between the script queue and scripts in order to speed up starting up scripts (apparently it will greatly reduce the time required to wait for historical data).    [~dmills] suggests adding the following to the ospl.xml file pointed to by OSPL_URI  (ie the global configuration):    {code}  <DurabilityService name=""durability"">        <Network>           <InitialDiscoveryPeriod>0.1</InitialDiscoveryPeriod>           <Alignment>              <TimeAlignment>false</TimeAlignment>              <RequestCombinePeriod>                 <Initial>1.0</Initial>                 <Operational>0.1</Operational>              </RequestCombinePeriod>           </Alignment>           <WaitForAttachment maxWaitCount=""1"">              <ServiceName>ddsi2</ServiceName>           </WaitForAttachment>        </Network>        <Persistent>           <StoreDirectory>/data/persist/</StoreDirectory>           <StoreMode>XML</StoreMode>         </Persistent>        <Tracing>           <Verbosity>NONE</Verbosity>  <!--          <OutputFile>durability.log</OutputFile> -->        </Tracing>        <NameSpaces>           <NameSpace name=""defaultNamespace"">              <Partition>*</Partition>           </NameSpace>           <Policy alignee=""Initial"" aligner=""true"" durability=""Durable"" nameSpace=""defaultNamespace""/>        </NameSpaces>     </DurabilityService>  {code}    I think the relevant bit is:  {code}      <Persistent>          <StoreDirectory>/data/persist/</StoreDirectory>          <StoreMode>XML</StoreMode>      </Persistent>  {code}",1
"DM-21180","09/04/2019 23:14:04","Change the default logging level of the XRootD/SSI","The current logging level for {{XRootD/SSI}} is set to {{DEBUG}}. This results in the large amount of (mostly - useless) messages stored in the Qserv {{mysql-proxy}} log file. A goal of this development is to change the level to {{WARN}}.",0.5
"DM-21181","09/05/2019 01:18:28","Add getLocalCalibration function to PhotoCalib","DM-21171 needs access to the local calibration from PhotoCalib. No currently exposed function within PhotoCalib takes a point and returns the local calibration and error. This ticket will create such a function and expose it within python.",2
"DM-21186","09/05/2019 18:29:53","Rename auxTel to LATISS","The name of the camera on AuxTel is LATISS.  Unfortunately we call the camera AuxTel in obs_lsst. It would be much clearer to use the proper name.    I think so long as AuxTelMapper class is still called the same name (so existing gen2 repos don't need to change) that everything else can be changed to LATISS and Gen3 (see DM-16297) can use the correct name.",2
"DM-21187","09/05/2019 19:37:30","Camera name is wrong in YAMLCamera files","The generated camera YAML files always have ""lsstCam"" as the name of the camera. This is not true for AuxTel or test stands.  This needs to be fixed.",3
"DM-21189","09/05/2019 19:59:52","Fix issue where exception can be thrown on sending basic_cancel on closed channel","There's an issue in the Consumer object where it's possible to send a basic_cancel on a closed connection.  The current code checks for the existence of the channel, rather than checking whether the channel is open or not.",2
"DM-21191","09/05/2019 20:29:37","Coordinate the v4.0.0 Release of SAL","This task covers the time to release v4.0.0 of SAL, update the tests accordingly, as well as work with [~cwinslow] to train him on the SAL release process.",5
"DM-21192","09/05/2019 20:31:01","Coordinate the v4.2.0 Release of XML","This task covers the time to release v4.2 (or whatever it gets tagged with) of the XMLs, as well as work with [~cwinslow] to train him to cover this work when I am unavailable.",3
"DM-21194","09/05/2019 21:23:06","Convert AVS mock into spectrograph simulator","Once I have the the complete FiberSpectrograph code written in DM-21188, I should lift the {{unittest.Mock}} system into a separate importable class ({{FiberSpectrographSimulator}}?), so that it can be used by the CSC and any CSC unittests. The simulator should have a way to disable the patches as well, to restore the real spectrograph communication.",2
"DM-21196","09/05/2019 23:49:44","Raise exception if remote is called but not fully ready","Currently, if the start_task is not run (and awaited) then it is possible to use a remote and it will timeout (or not respond) until it comes alive. This ticket is to implement the functionality that will raise an exception if it is not completed, and provide an error message to use the start_task to avoid the exception",2
"DM-21200","09/06/2019 16:51:56","Update CSC transition script to capture timeout in Remote","The current script {{csc_send_to_state}} fails to transition {atHeaderService} at the Tucson test stand. Tiago Riviero suggest to add an {await} to the {Remote.start_task}",2
"DM-21206","09/06/2019 18:25:43","xml build","build xml and sal with rob. There will be no errors and everything will go smoothly.",1
"DM-21207","09/06/2019 19:57:35","Remove deprecated Policy usage from pex_config, ip_diffim, and meas_algorithms","In DM-21065 we deprecated the use of Policy as an API argument in ip_diffim and meas_algorithms and announced they would be removed after release 19.0.    This ticket is to remove those deprecated APIs (retaining the PropertySet implementations) and to remove pex_config.makePolicy.",1
"DM-21208","09/06/2019 21:01:37","Pixels without data in images retrieved via SODA service set to zero","In [this|https://github.com/lsst-sqre/notebook-demo/blob/master/LSST%20SODA%20Tutorial.ipynb] notebook there is a demonstration of using the SODA service.  In one case, a circular cutout is retrieved.  The resulting image is the shape of the circumscribing square with the unpopulated pixels set to zero.  This has the problem that it makes it impossible to tell programmatically which pixels are in the cutout and which are not.    There are two solutions I came up with when talking to [~cbanek]:  1) Set the pixels with no data to {{NaN}} instead of 0  2) Return a masked image instead of an image    I would prefer option 2, and I believe the standard supports it, but would be OK with option 1 if there are technical reasons to prefer it.",5
"DM-21209","09/06/2019 21:28:17","Implement HVAC telemetry into XML files","Compile telemetry generated by HVAC control software. Organize the telemetry into XML files and generate updated version of MQTT publication software.",8
"DM-21212","09/06/2019 23:19:29","Update existing cp_pipe tasks to pipelineTasks","The new tasks for cp_pipe will need to be ready to work with the gen3 butler/as pipelineTasks in the near future, which likely requires some sort of plan.  This ticket is to remind everyone of that fact.",40
"DM-21213","09/06/2019 23:29:02","Implement Gauss's method for IOD","Baseline IOD and OD package will likely be pyoorb / oorb but we need to validate some of its basic functionality so having a python implementation of IOD is useful. To that end, we need an implementation of Gauss's method to get an estimate of an initial orbit from a linkage.     Enhancing Gauss's method with different methods to find the velocity at the time of the second observation is necessary especially when dealing with different observing arcs. Supporting velocity methods include Gibbs and Herrick+Gibbs.           ",8
"DM-21227","09/09/2019 22:23:40","Migrate the Test Jobs on Jenkins from DM to TSSW","Migrate the test jobs on Jenkins from DM to TSSW. The following repositories will be moved: ts_wep, ts_ofc, ts_phosim, and ts_MTAOS.",1
"DM-21232","09/10/2019 16:34:45","Support for Hexapod/Rotator System Ready for Integration in Phase 2","Continued changes and support for M2 Hexapod System to be ready for Integration    Corresponding P6 Activity:    T&SC-140403-1300 - M2 Hexapod System Ready for Integration",13
"DM-21237","09/10/2019 18:59:31","BBox error in some DCR image differencing templates","Some images have been failing during image differencing using the DCR-matched template, that succeeded with a standard coadd template. These images appear to fail for patches covering the extreme edges of the area with data, but more investigation is needed. Example error message below:     {code:python}    apPipe INFO: Running ImageDifference...  apPipe.differencer INFO: Processing DataId(initialdata=\{'ccdnum': 44, 'filter': 'g', 'visit': 410929, 'date': '2015-02-17', 'object': 'Blind15A_40'}, tag=set())  apPipe.differencer.getTemplate INFO: Using skyMap tract 0  apPipe.differencer.getTemplate INFO: Assembling 6 coadd patches  apPipe.differencer.getTemplate INFO: exposure dimensions=(2048, 4096); coadd dimensions=(5424, 2740)  apPipe.differencer.getTemplate INFO: Constructing DCR-matched template for patch \{'datasetType': 'dcrCoadd_sub', 'bbox': Box2I(minimum=Point2I(60087, 34140), dimensions=Extent2I(13, 1960)), 'tract': 0, 'patch': '14,8', 'numSubfilters': 3}  apPipe FATAL: Failed on dataId=\{'ccdnum': 44, 'filter': 'g', 'visit': 410929, 'date': '2015-02-17', 'hdu': 48, 'object': 'Blind15A_40'}: LengthError:    File ""src/image/Image.cc"", line 89, in static lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const Extent2I&, const Extent2I&, const _view_t&) [with PixelT = float; lsst::afw::image::ImageBase<PixelT>::_view_t = boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float, boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t> > >*> > >; lsst::geom::Extent2I = lsst::geom::Extent<int, 2>]   Box2I(Point2I(-60087,-34140),lsst::geom::Extent2I(0,0)) doesn't fit in image 13x1960 \{0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-60087,-34140),lsst::geom::Extent2I(0,0)) doesn't fit in image 13x1960'    /software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pex_config/18.1.0-4-gc96e915/python/lsst/pex/config/config.py:1276: FutureWarning: Config field ccdProcessor.isr.doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.   FutureWarning)  /software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ap_association/18.1.0-8-g5734da6/python/lsst/ap/association/mapApData.py:185: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.   table_list = list(yaml.load_all(yaml_stream))  /software/lsstsw/stack_20190330/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/astropy/units/function/logarithmic.py:43: RuntimeWarning: invalid value encountered in log10   return dex.to(self._function_unit, np.log10(x))  Traceback (most recent call last):   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__   result = self.runTask(task, dataRef, kwargs)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask   return task.runDataRef(dataRef, **kwargs)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/timer.py"", line 150, in wrapper   res = func(self, *args, **keyArgs)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ap_pipe/18.1.0-4-gcd8f52c+10/python/lsst/ap/pipe/ap_pipe.py"", line 202, in runDataRef   diffImResults = self.runDiffIm(calexpRef, templateIds)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/timer.py"", line 150, in wrapper   res = func(self, *args, **keyArgs)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ap_pipe/18.1.0-4-gcd8f52c+10/python/lsst/ap/pipe/ap_pipe.py"", line 276, in runDiffIm   return self.differencer.runDataRef(sensorRef, templateIdList=templateIds)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_base/18.1.0-4-g6c9d669+5/python/lsst/pipe/base/timer.py"", line 150, in wrapper   res = func(self, *args, **keyArgs)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/pipe_tasks/18.1.0-15-gc153667b/python/lsst/pipe/tasks/imageDifference.py"", line 353, in runDataRef   template = self.getTemplate.run(exposure, sensorRef, templateIdList=templateIdList)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/getTemplate.py"", line 153, in run   visitInfo=exposure.getInfo().getVisitInfo())   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/dcrModel.py"", line 386, in buildMatchedExposure   bbox=bbox, wcs=wcs, mask=mask)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/dcrModel.py"", line 349, in buildMatchedTemplate   refModel = self.getReferenceImage(bbox)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/dcrModel.py"", line 274, in getReferenceImage   return np.mean([model[bbox].array for model in self], axis=0)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/ip_diffim/18.1.0-9-gae7190a+1/python/lsst/ip/diffim/dcrModel.py"", line 274, in <listcomp>   return np.mean([model[bbox].array for model in self], axis=0)   File ""/software/lsstsw/stack_20190330/stack/miniconda3-4.5.12-1172c30/Linux64/afw/18.1.0-15-gbf79fb8fa/python/lsst/afw/image/slicing.py"", line 262, in __getitem__   return self.subset(box, origin=origin)  lsst.pex.exceptions.wrappers.LengthError:    File ""src/image/Image.cc"", line 89, in static lsst::afw::image::ImageBase<PixelT>::_view_t lsst::afw::image::ImageBase<PixelT>::_makeSubView(const Extent2I&, const Extent2I&, const _view_t&) [with PixelT = float; lsst::afw::image::ImageBase<PixelT>::_view_t = boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::pixel<float, boost::gil::layout<boost::mpl::vector1<boost::gil::gray_color_t> > >*> > >; lsst::geom::Extent2I = lsst::geom::Extent<int, 2>]   Box2I(Point2I(-60087,-34140),lsst::geom::Extent2I(0,0)) doesn't fit in image 13x1960 \{0}  lsst::pex::exceptions::LengthError: 'Box2I(Point2I(-60087,-34140),lsst::geom::Extent2I(0,0)) doesn't fit in image 13x1960'  {code}",2
"DM-21240","09/10/2019 20:59:39","Fix usage of auxTel in obs_lsst config files","DM-21186 didn't quite finish the job and some config files still refer to auxTel.",0.5
"DM-21246","09/11/2019 16:12:48","Allow deferred passing of run/collection to Butler","While working on ingest and especially gen2convert, I've found it frustrating to need to pass a collection/run to Butler at construction, rather than later, when calling get/put/ingest.  We often need an object that represents the combination of a Registry and a Datastore _without_ specifying a collection/run.    I think we should add optional ""run"" kwargs to all Butler write operations and ""collection"" kwargs to all read operations, and make the corresponding construction arguments an optional way to provide defaults for these.    As a side note, I think I'd like to eventually move to having Butler support lookup with ordered sequence of collections rather than just a single one.  We could anticipate that by making the new kwargs for read operations a ""collections"" sequence rather than than a ""collection"" scalar, even if we don't implement support for more than one yet, but I'm also willing to defer that for later.",2
"DM-21248","09/11/2019 17:13:01","cameraMapper _standardizeExposure should not try to create a WCS for each amp","After DM-20154, updates were made to {{\_standardizeExposure}} which now creates a WCS from visit metadata.  The problem is that {{\_standardizeExposure}} is called _per amp_ when reading raws, which leads to a cascade of warnings with {{obs_lsst}} because there are a lot of amps, and the {{visitInfo}} hasn't been created yet with the appropriate metadata to create a WCS.  However, even if the warnings aren't there, it is unnecessary and inefficient to create a WCS for each amp, when this should be done once at the ccd-level after assembly (and after the appropriate metadata has been created).    I believe this should be a quick fix that simply checks if it is called with the amp level before trying to set the wcs.  At the same time maybe the per-amp setting of the filter is also unnecessary?",0.5
"DM-21261","09/11/2019 22:45:46","Implement auto-unack and auto-ack in the Watcher","Implement the following features in the Watcher:  * unack (snooze): an acknowledged alarm will un-ack itself after a specified period if the condition persists.  * auto-ack: a stale alarm will auto-ack itself after a specified period (as long as the alarm remains stale).    There will be a default duration for each of these feature, which can be overridden by the alarm rule. A suitable duration (probably 0) will mean ""never"".    The default duration will be a configuration parameter for the CSC. Rule-specific durations may be part of the rule configuration, but it depends on the rule author.",2
"DM-21262","09/11/2019 22:49:31","Get agreement on how alarms should be escalated for the Watcher","Folks generally seem to agree that some really important alarms should be escalated to off-site personnel if not acknowledge in time. This ticket captures the work of coming to an agreement on *how* this escalation will be done, e.g. via a phone call (which will probably require a paid subscription to some service), text message, slack message or some combination of techniques.",1
"DM-21269","09/12/2019 02:52:45","Update the Middleware Code by Moog to Use the Latest SAL Version","Update the middleware code by Moog to use the latest SAL version. At this moment, the ts_sal 4.0 is unavailable. Therefore, the latest version of ts_sal and ts_xml will be used. The code update might be needed for the possible DDS/ xml interface changes. This task will also try to find a way to test the code even though the hexapod/ rotator cRIO controller and hardware are unavailable.",5
"DM-21276","09/12/2019 18:30:42","Ensure all prescan/overscan accessors exist","Camera generation from yaml and FITS disagree on naming of serial/parallel vs horizontal/vertical, and disagree on the existence of all types of -scans.  Ensure accessors exist so that any nomenclature works correctly.",2
"DM-21281","09/12/2019 20:09:34","Test activating an already-activated spectrograph, and add unittests as necessary","[~rowen] pointed out that we should probably be testing the {{AvsIdentity.DeviceStatus}} field returned by {{AVS_GetList}} for values other than {{1}}. The vendor docs don't say enough about under what conditions the {{0,2,3}} states can be set, so I'll just have to play with the device some to see if I can get any of them to trigger. If I can, I'll add some tests of those states, so that the controller can provide useful error messages if it encounters them.",1
"DM-21286","09/12/2019 22:14:57","Enable Astropy download cache","In Jenkins HOME is not available to tests so the Astropy cache directory is not shared between processes.  This leads to IERS table files being downloaded every single time a tests are run. Sometimes the download fails and that usually causes test failures that waste developer time. Forwarding HOME to the test environment would fix this but we are concerned that this may introduce side effects into the test environment. Astropy provides another mechanism via the XDG_CACHE_HOME environment variable.    If this variable is not set we should set it to {{~/.lsst/}} and create an {{astropy}} directory for the cache.  Do not forward XDG_CONFIG_HOME since that might affect tests using astropy.",1
"DM-21291","09/13/2019 02:09:16","Update ts_scriptqueue unit tests to use asynctest","Update the unit tests in ts_scriptqueue to use asynctest.",1
"DM-21295","09/13/2019 17:39:29","Setup and support M1M3 EFD for summit test campaign","Clone the old EFD machine using a copy of the disk. Install and setup in  EFD machine at the summit. Support the test activities.",5
"DM-21299","09/13/2019 18:40:14","Update SAL tests to verify significant files exist","It might be nice to have tests that verify significant data files and executables exist and are in the correct location before running the salgenerator builds.",1
"DM-21301","09/13/2019 20:28:37","pipetask --register-dataset-types doesn't work with Postgres registry on AWS","pipetask {{--register-dataset-types}} doesn't work with the RDS registry. It should work even if the dataset type already exists.     For example,   {noformat}  $ pipetask -d ""visit.visit=903334 and detector.detector=22"" -j 1 -b ./butler.yaml -p lsst.meas.base -p lsst.ip.isr -p lsst.pipe.tasks -i raw,calib,ref/ps1_pv3_3pi_20170110,shared/ci_hsc -o hfctest2 run --register-dataset-types -t isrTask.IsrTask:isr -C isr:$OBS_SUBARU_DIR/config/hsc/isr.py  py.warnings WARN: /home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/pex_config/18.1.0-4-gc96e915/python/lsst/pex/config/config.py:1276: FutureWarning: Config field doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.    FutureWarning)    botocore.credentials INFO: Found credentials in shared credentials file: ~/.aws/credentials  ctrl.mpexec.cmdLineFwk INFO: QuantumGraph contains 1 quanta for 1 tasks  Traceback (most recent call last):    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1230, in _execute_context      cursor, statement, parameters, context    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 536, in do_execute      cursor.execute(statement, parameters)  psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""dataset_type_pkey""  DETAIL:  Key (dataset_type_name)=(postISRCCD) already exists.      The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""/home/centos/dino/rds-w36/o/daf_butler/python/lsst/daf/butler/registries/sqlRegistry.py"", line 356, in registerDatasetType      storage_class=datasetType.storageClass.name    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 974, in execute      return meth(self, multiparams, params)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/sql/elements.py"", line 273, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1093, in _execute_clauseelement      distilled_params,    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1234, in _execute_context      e, statement, parameters, cursor, context    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1452, in _handle_dbapi_exception      util.raise_from_cause(sqlalchemy_exception, exc_info)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 296, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb, cause=cause)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 276, in reraise      raise value.with_traceback(tb)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1230, in _execute_context      cursor, statement, parameters, context    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 536, in do_execute      cursor.execute(statement, parameters)  sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""dataset_type_pkey""  DETAIL:  Key (dataset_type_name)=(postISRCCD) already exists.   [SQL: 'INSERT INTO dataset_type (dataset_type_name, storage_class) VALUES (%(dataset_type_name)s, %(storage_class)s)'] [parameters: {'dataset_type_name': 'postISRCCD', 'storage_class': 'ExposureF'}] (Background on this error at: http://sqlalche.me/e/gkpj)    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1230, in _execute_context      cursor, statement, parameters, context    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 536, in do_execute      cursor.execute(statement, parameters)  psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block      The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-4-gf8b9952+4/bin/pipetask"", line 26, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-4-gf8b9952+4/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 142, in parseAndRun      return self.runPipeline(qgraph, taskFactory, args)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-4-gf8b9952+4/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 411, in runPipeline      updateOutputCollection=True)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-4-gf8b9952+4/python/lsst/ctrl/mpexec/preExecInit.py"", line 75, in initialize      self.initializeDatasetTypes(graph, registerDatasetTypes)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-4-gf8b9952+4/python/lsst/ctrl/mpexec/preExecInit.py"", line 120, in initializeDatasetTypes      self.butler.registry.registerDatasetType(datasetType)    File ""/home/centos/dino/rds-w36/o/daf_butler/python/lsst/daf/butler/core/utils.py"", line 226, in inner      return func(self, *args, **kwargs)    File ""/home/centos/dino/rds-w36/o/daf_butler/python/lsst/daf/butler/registries/sqlRegistry.py"", line 363, in registerDatasetType      existingDatasetType = self.getDatasetType(datasetType.name)    File ""/home/centos/dino/rds-w36/o/daf_butler/python/lsst/daf/butler/registries/sqlRegistry.py"", line 408, in getDatasetType      datasetTypeTable.c.dataset_type_name == name)).fetchone()    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 974, in execute      return meth(self, multiparams, params)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/sql/elements.py"", line 273, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1093, in _execute_clauseelement      distilled_params,    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1234, in _execute_context      e, statement, parameters, cursor, context    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1452, in _handle_dbapi_exception      util.raise_from_cause(sqlalchemy_exception, exc_info)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 296, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb, cause=cause)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 276, in reraise      raise value.with_traceback(tb)    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1230, in _execute_context      cursor, statement, parameters, context    File ""/home/centos/lsst_stack/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 536, in do_execute      cursor.execute(statement, parameters)  sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block   [SQL: 'SELECT dataset_type.storage_class \nFROM dataset_type \nWHERE dataset_type.dataset_type_name = %(dataset_type_name_1)s'] [parameters: {'dataset_type_name_1': 'postISRCCD'}] (Background on this error at: http://sqlalche.me/e/2j85)  {noformat}  ",2
"DM-21304","09/13/2019 22:42:52","Add async method for handling new summary state to BaseCsc","Add an asynchronous method to BaseCsc that is called when the summary state changes, e.g. handle_summary_state. This will be the new preferred way to handle changes to summary state (leaving report_summary_state to just report the new state) and the base class will do nothing, so CSCs will not have to call super().handle_summary_state(...)    This may be a bit tricky because it needs to be called whenever report_summary_state is called, and that is called from two synchronous functions: fault and the summary_state setter. The following might work:    * Have fault start handle_summary_state but not wait for it to finish  * Deprecate the summary_state setter and stop using it in salobj. Just set self._summary_state directly when necessary rather than add a new synchronous public method. CSCs should not set this value.  * Have the summary_state setter start handle_summary_state but not wait for it to finish. I don't see much choice.  * When setting summary_state internally as part of a normal state transition call handle_summary_state before report_summary_state, so if something goes wrong one can go directly to fault state without claiming to have gone into the desired state (since the CSC never actually makes it to the new state).  * If report_summary_state or handle_summary_state raises an exception go into fault state. Ideally the CSC would already have faulted, so this is merely a fallback. This requires error codes to not be globally unique.",1
"DM-21308","09/16/2019 16:08:01","Update pipe_tasks to allow for changing external calibrations","Update {{pipe_tasks}} and related configs to allow coadditions with different external {{PhotoCalib}} and {{SkyWcs}} calibrations, supporting {{jointcal}}, {{fgcmcal}}, and {{fgcmcal_tract}} for photometric calibrations, and {{jointcal}} for astrometric calibrations.  Terminology is now matched to DM-21950.    Originally: Following on DM-20161, with fgcmcal tract-based calibrations, run RC2 coadds.",3
"DM-21324","09/16/2019 23:17:02","Fix missing logging for HeaderService sub-modules","The migration to {{ts_salobj}}  introduced a bug to the HeaderService were logging is missing from the sub-modules.",2
"DM-21327","09/16/2019 23:44:16","Replace ExposureInfo implementation with homogeneous map","{{lsst.afw.image.ExposureInfo}} is currently implemented in terms of a {{lsst.afw.typehandling.GenericMap}}. However, this class is brittle and difficult to use (see, e.g., DM-21216 and DM-21268), nor do we use its heterogeneity in {{ExposureInfo}}: we are only interested in storing {{lsst.afw.typehandling.Storable}} and {{shared_ptr<Storable>}}, and the only application for the former is incompatible with {{Storable}} due to custom persistence. Thus, {{GenericMap}} makes {{ExposureInfo}} difficult to use and maintain while not providing any needed functionality.    Reimplement {{ExposureInfo}} in terms of a homogeneous map from strings to {{shared_ptr<Storable>}}. This map should have its own class in order to encapsulate strongly-typed handling of {{Storable}} (the one form of heterogeneous type support in {{GenericMap}} that we do use). Unlike {{GenericMap}}, this class need not have a separate interface or a Python API, since it will exist only as an implementation detail of {{ExposureInfo}}.",8
"DM-21328","09/17/2019 00:35:54","Modernize the way tasks are run in salobj","Update salobj for Python 3.7 as follows:  * Use {{asyncio.run}} instead of {{asyncio.get_event_loop().run_until_complete}}  * Use {{asyncio.create_task}} instead of {{asyncio.ensure_future}} where practical. Stick to ensure_future in library code for backwards compatibility, especially with {{request_script.py}} in ts_scriptqueue which runs the event loop in a very unusual way that is not compatible with {{create_task}}.    To make this easier I added class method `amain` to `BaseCsc` and `BaseScript` and deprecated `main`.    Note that code should make sure an event loop exists when constructing a `Domain`, `Controller` or `Remote`, but we can't enforce that as long as we have {{request_script.py}}  ",2
"DM-21331","09/17/2019 01:45:54","Modernize the way tasks are run in scriptqueue","Modernize the following:  * Use {{asyncio.run}} instead of {{asyncio.get_event_loop().run_until_complete}}  * Use {{asyncio.create_task}} instead of {{asyncio.ensure_future}} where it is safe to do so.  * Make sure there is an event loop before any domain, remote or CSC  * Use {{ScriptQueue.amain}} to run the ScriptQueue instead of making a separate parser  * Run scripts using {{amain}} instead of {{main}}  * Update unit tests to use {{asynctest}}",1
"DM-21336","09/17/2019 18:50:38","Telescope Script Development Part 2","This is  a continuation of DM-20173 & DM-20176    which was a continuation of DM-16910    This Epic is to allocate resources to script development, irregardless of components the scripts are working on.  Non-specific integration work should go here.",20
"DM-21338","09/17/2019 19:20:43","Update ts_xml.lsst.io with new CSC developers","As folks have left the T&S team and CSCs have been redistributed, the listing at ts_xml.lsst.io is in need of a refresh.",1
"DM-21349","09/18/2019 14:57:26","Fix information exchange between ATArchiver and ArchiveController","The ArchiveController has information about which directories below the top level storage area should be used to put images.  The ATArchiverCSC needs to message the ArchiveController in order to get this information, otherwise everything ends up in the top level area.",1
"DM-21351","09/18/2019 16:32:08","Move pipe_base tests to obs_test to normalize dependencies","pipe_base depends on obs_test in order to run its unit tests.  This adds an indirect dependency from pipe_base on obs_base.    We'd like to both start adding Tasks for Gen3 data repository management (e.g. ingest) to obs_base (well, we've already done so, but don't yet import them at package scope) and keep meas_* from depending on obs_test.  This would accomplish both.    Work on this is already at least mostly done on DM-20864; this ticket will cherry-pick those changes in order to land them a bit faster than the rest of that ticket.",0.5
"DM-21353","09/18/2019 17:42:05","Test Science Pipelines software build with CentOS8","CentOS 8 will be released imminently. We need to check that lsst_distrib can be built with it as a precursor to dropping CentOS6 from Jenkins and replacing it with CentOS8.  ",1
"DM-21355","09/18/2019 21:20:55","A NaN value in a PropertySet can cause an abort when reading WCS","When reading a WCS with {{readFitsWcs}} in {{afw}}, then a {{SIGABRT}} is thrown if the associated {{PropertyList}} has a value for a {{NaN}} because {{astshim}} complains that fits header cards cannot contain {{NaN}} (even though I believe this is just a convenience and no fits headers are persisted in this way).    The fix according to [~tjenness] is to check for a {{NaN}} on line 351 of {{frameSetUtils.cc}}: https://github.com/lsst/afw/blob/7c0af834f66868c8a223af827bd0fe110619bc2e/src/geom/detail/frameSetUtils.cc#L351 and replace it with a legal sentinel value.",1
"DM-21357","09/18/2019 21:59:23","Add items() method to PropertySet/PropertyList","Prior to DM-16297 when a header was merged in astro_metadata_translator the resultant header was always a dict and never a PropertyList.  In DM-16297 that is changing because PropertyX can now support for dict-like methods and the conversion to a dict is not required, and also the result of the merge must be able to be sent to afw and it is important that the type does not change.    This breaks pipe_drivers which expects the result from merge_headers to support the {{items()}} method and PropertyX does not.  On this ticket I will add {{items()}} using the existing {{\__iter__}} support.",1
"DM-21358","09/19/2019 00:05:55","Only allow changing simulation mode in STANDBY state","Currently, one is allowed to change the simulation mode of a CSC in either STANDBY or DISABLED states (see {{base_csc.py:272}}), while the change from STANDBY to DISABLED is when the device connection should occur. This makes it more difficult to implement library mocks as I have done for the fiber spectrograph: the mock cannot patch over an already loaded library.    If we only allow changing the simulation mode in STANDBY, then these mocks are trivial to implement, and I believe it is safer overall as well: you don't want to start simulating over the top of an already existing connection.",1
"DM-21359","09/19/2019 01:35:44","Update XRootD from upstream","Update to latest upstream XRootD.  Should address issue with binary characters in log messages.",0.5
"DM-21360","09/19/2019 02:50:22","Repair Qserv container builds","Qserv container builds are busted since rh-git29 scl package has disappeared from distribution servers.    Roll up to rh-git218.  Roll up to devtoolset-8 while we are at it.",0.5
"DM-21363","09/19/2019 16:45:17","Fix semantics of PropertySet.update","In DM-19873 I added {{PropertySet.update}} but I misunderstood the internal semantics of {{PropertySet.combine}}.    It turns out that {{combine()}} effectively adds values into the PropertySet so if you have matching keys you get an array item in the combination. This is not what {{update}} is semantically required to do which is effectively to overwrite existing values ({{set}} semantics).  I need to remove the special case call of {{combine}} inside the {{update}} method.",1
"DM-21364","09/19/2019 17:50:22","Unit tests for chillermodel","Chiller model needs unit tests. Write some. ",2
"DM-21366","09/19/2019 18:14:38","Release new versions of ts_salobj, ts_ATDome, ts_scriptqueue once SAL 4 is released","Given the delay in releasing SAL 4 I am going to merge a series of tickets that rely on it to develop (in hopes that no last minute changes to SAL 4 require significant changes to Python).    This ticket is to cut releases of all these packages, after applying any last-minute changes. If the software can be released with no changes then simply merge the code from develop to master. Otherwise use this ticket branch for cleanup work.    Packages include:  * ts_salobj  * ts_scriptqueue  * ts_ATDome",1
"DM-21368","09/19/2019 22:22:50","Check that exposure time is in valid range","While discussing failure modes, we realized that the only user-configurable parameter in the `expose()` command is the exposure time, which has a vendor-specified range 0.002ms-600s). We should check that the requested exposure time is in this range, before we ever attempt to configure the measurement, and raise an exception early if it is not.    We can make this a custom exception so that the CSC doesn't go into FAULT in this instance (vs. what happens when there is an internal error to the spectrograph during an exposure).",1
"DM-21373","09/20/2019 00:34:41","Add an internal timeout counter to the spectrograph polling","After discussion with [~rowen], we decided that we should have some internal polling timer to allow the device controller to cancel the PollScan after some amount of time. We'll have to decide how long is ""too long"". If this timer is exceeded, the controller should call {{stop_exposure}} and raise either {{asyncio.CancelledError}}, or some other error to let the CSC know that the exposure was automatically (as opposed to user) aborted.    5 seconds is my first guess for this timer: the readout should be a fraction of a second, so this should only be relevant if the spectrograph internals get into some ""stuck"" state, which we should know about after a second or two.    This is also a usecase for the {{TIMEDOUT}} ExposureState.",1
"DM-21374","09/20/2019 14:57:05","Test and deploy HeaderService with ts_xml 4.2.0","A new version of {{ts_xml 4.2.0}} has been released. This new version and the accompanying new meta-data  need to be tested.",2
"DM-21378","09/20/2019 18:25:40","Support technote_aastex in templatebot","DM-21317 adds the {{technote_aastex}} template to https://github.com/lsst/templates.    This ticket is to add support for this new technote type to the lsst-templatebot-aide project and to commission the template for production.",1
"DM-21384","09/20/2019 22:14:42","Write a tech note on OCS triggered data processing","Write a technote [DMTN-133|https://dmtn-133.lsst.io/v] outlining how we think that OCS triggered data processing will happen.",5
"DM-21392","09/21/2019 01:10:29","Note for google pitch ","Rob pike suggested a short write up for google to see if we can make a deal ..",3
"DM-21398","09/23/2019 18:51:27","Change background handling defaults in imageDifference.py","This ticket is to implement the background handling changes approved in RFC-630",1
"DM-21399","09/23/2019 19:49:35","Add an enum to Test in ts_xml","Add an enum to the Test SAL component in ts_xml.    See DM-20432 to update ts_sal's unit tests to use it instead of the enum(s) in Script.",0
"DM-21400","09/23/2019 20:20:05","Update ts_ATDome for simplified simulation mode DM-21358","Take advantage of the simpler simulation mode support in salobj DM-21358 to simplify the implementation of simulation mode in ts_ATDome.    Instead of code that changes the existing connection when simulation mode is changed, just make the connection when in DISABLED or ENABLED state.",0
"DM-21401","09/23/2019 21:19:37","Write a Watcher config file for the summit","Write a config file for the Watcher appropriate to the current SAL components running on the summit. Tiago reports they are as follows:  {code}   - ATMCS (no heartbeat)   - ATPneumatics (no heartbeat)   - ATPtg (no heartbeat)   - Environment   - ATDome   - ATHexapod   - ATDomeTrajectory   - DIMM (upcoming)  {code}",0
"DM-21407","09/23/2019 23:25:27","Remove unneeded dependency on astrometry_net in meas_astrom","According to its table file meas_astrom still depends on astrometry_net even though that dependency was removed in DM-2186.  ",0.5
"DM-21410","09/24/2019 15:07:18","Turn ci_hsc into a metapackage for ci_hsc_gen2 and ci_hsc_gen3","We need to do this before DM-17023 lands or the nightly ci_hsc builds (which are still running the old ci_hsc package) will be broken.    Only tricky bit is the question of how gracefully LFS handles being removed from the master branch of a repo.",1
"DM-21411","09/24/2019 16:36:46","Update AOS Package to Use the Scientific Pipeline w_2019_38","Update AOS package to use the scientific pipeline w_2019_38. This is a preparation to update ts_MTAOS to use the salObj 4.",0
"DM-21419","09/24/2019 20:47:03","The definition of which timestamp the EFD uses should be done by the SAL Kafka producer ","In the future, we'll record EFD data in multiple stores like InfluxDB, Oracle, Parquet etc    Kafka replicates data using connectors, right now the decision of which timestamp is used as the InfluxDB time is done in the connector configuration, by a KSQL query:    {code}  INSERT INTO mytopic SELECT * FROM mytopic WITHTIMESTAMP private_sndStamp  {code}    a similar configuration would be done in the Oracle connector to define which timestamp field should be indexed.    To avoid making this decision in different connectors and potentially in different deployments of the EFD, the definition of which timestamp to use in the EFD should be done in the SAL Kafka Producer instead.     There are different ways to accomplish this.    The easiest one is to create a new field in the Avro schema called, for example, {{private_efdStamp}} which is a copy of whatever timestamp field we should use. Initially, that can be hardcoded to use {{private_sndStamp}}.    The other way is making use of ""aliases"" in Avro:    https://avro.apache.org/docs/1.8.1/spec.html#Aliases    that create an alternate name {{private_efdStamp}} for the timestamp field we want to use.     NOTE: we assume aliases work the way we expect but that needs to be tested. In the above KSQL query {{SELECT * FROM mytopic}} ensures that all fields in {{mytopic}} will be inserted as fields in InfluxDB with their original names and  {{WITHTIMESTAMP private_efdStamp}} will use the alternate name to use the right timestamp as the InfluxDB timestamp.    Either way, the different connectors always use {{private_efdStamp}}.    Another benefit of doing this in the SAL Kafka producer code or in the Avro schema is that changes to the code or to the Avro schema are versioned, while changes in the connector configuration are not.     This ticket is not blocking anything in the moment. But having the {{private_efdStamp}} would make the EFD configuration more realiable.",0
"DM-21420","09/24/2019 20:49:45","ap_verify datasets have out-of date refcat configs","As described on [Community|https://community.lsst.org/t/3854], the reference catalog configs recently chagned to require that users specify both Gen 2- and Gen 3-style configs. This change breaks the configs shipped with {{ap_verify}} datasets, which only have Gen 2 style configs:  {noformat}  Traceback (most recent call last):    File ""lsstsw3/stack/Linux64/ap_verify/18.1.0-5-g1dd6499+4/bin/ap_verify.py"", line 29, in <module>      result = runApVerify()    File ""lsstsw3/stack/Linux64/ap_verify/18.1.0-5-g1dd6499+4/python/lsst/ap/verify/ap_verify.py"", line 161, in runApVerify      apPipeResults = runApPipe(workspace, args)    File ""lsstsw3/stack/Linux64/ap_verify/18.1.0-5-g1dd6499+4/python/lsst/ap/verify/pipeline_driver.py"", line 75, in runApPipe      makePpdb(configArgs)    File ""lsstsw3/stack/Linux64/ap_pipe/18.1.0-4-gcd8f52c+5/python/lsst/ap/pipe/make_ppdb.py"", line 108, in makePpdb      parsedCmd = parser.parse_args(args=args)    File ""lsstsw3/stack/Linux64/ap_pipe/18.1.0-4-gcd8f52c+5/python/lsst/ap/pipe/make_ppdb.py"", line 84, in parse_args      namespace.config.validate()    File ""lsstsw3/stack/Linux64/ap_pipe/18.1.0-4-gcd8f52c+5/python/lsst/ap/pipe/ap_pipe.py"", line 100, in validate      pexConfig.Config.validate(self)    File ""lsstsw3/stack/Linux64/pex_config/18.1.0-5-gc286bb7+1/python/lsst/pex/config/config.py"", line 1234, in validate      field.validate(self)    File ""lsstsw3/stack/Linux64/pex_config/18.1.0-5-gc286bb7+1/python/lsst/pex/config/configurableField.py"", line 340, in validate      value.validate()    File ""lsstsw3/stack/Linux64/pex_config/18.1.0-5-gc286bb7+1/python/lsst/pex/config/config.py"", line 1234, in validate      field.validate(self)    File ""lsstsw3/stack/Linux64/pex_config/18.1.0-5-gc286bb7+1/python/lsst/pex/config/configurableField.py"", line 340, in validate      value.validate()    File ""lsstsw3/stack/Linux64/pipe_tasks/18.1.0-19-g9e516c23/python/lsst/pipe/tasks/calibrate.py"", line 294, in validate      f""Gen2 ({astromRefCatGen2}) and Gen3 ({self.connections.astromRefCat}) astrometry reference ""  ValueError: Gen2 (gaia) and Gen3 (ps1_pv3_3pi_20170110) astrometry reference catalogs are different.  These options must be kept in sync until Gen2 is retired.  {noformat}    Update the configs for {{ap_verify_ci_hits2015}} and {{ap_verify_hits2015}} so that {{ap_verify}} runs again.",1
"DM-21421","09/24/2019 20:56:36","Create a system to define pipeline level execution and configuration","This ticket will design and create the machinery necessary to define ""top"" level workflow pipelines and their associated configurations. This is envisioned to be a set of tasks commonly run together in a manor similar to how pipe_drivers currently groups tasks. However, this is not intended to be tied to concrete tasks in the way pipe_drivers is, but a framework for users to specify processing pipeline definitions so that they can be reusable and revision controlled.    Jim and Andy I am adding you as watches so that if you have any thoughts on this you can comment.",20
"DM-21422","09/24/2019 23:36:57","Fix ip_diffim mosaic debug plots broken by Mosaic.makeMosaic signature change and compiler warning","The {{frame=}} arg has been removed from {{afwDisplay.Mosaic.makeMosaic}}.    {quote}Robert Lupton 17:30    Use    disp = afwDisplay.Display(frame)  mos.makeMosaic(display=disp, title=""Kernel Basis Images"")  {quote}",2
"DM-21423","09/25/2019 01:28:18","Setup dimm CSCs on tower DIMM server","Setup two DIMM CSCs on tower DIMM server.",1
"DM-21424","09/25/2019 02:09:08","Remove the Hard-Coded Data in Middleware","Remove the hard-coded data in middleware by Moog. Consider to add the configuration file. Use the path variable is also a solution. Need to check the yaml support in c language.",2
"DM-21426","09/25/2019 14:52:19","Remove expectation that wcslib is in package provenance in ci_hsc_gen2","ci_hsc_gen2 includes a check that the package provenance recorded by the CmdLineTask machinery includes a few packages, and that list includes wcslib.  Since our own WCS classes now use AST instead, whether wcslib appears depends on whether other optional stack packages that still use wcslib are set up, and that makes this test fragile.",0.5
"DM-21429","09/25/2019 15:08:57","Stop S3-backed butler tests from attempting import/export","DM-17023's addition of import/export functionality included a test that is intended to be run only on PosixDatastore-backed butlers, because that's the only datastore that implements export at present.  However, the S30-backed butler test class inherits from the PosixDatastore-backed test class, so when the (optional) moto/boto3 dependencies are present, the import/export test is attempted there, too, and fails.    The right fix appears to be changing the inheritance relationship so that S3 and PosixDatastore butler tests both inherit from a common base class, mirroring the inheritance relationship of the datastore classes themselves.",0.5
"DM-21431","09/25/2019 18:14:59","Tag release candidates for code waiting for SAL 4","Tag release candidates on develop for all my packages that are waiting for SAL 4 for formal releases.",0
"DM-21437","09/25/2019 22:42:13","Match up spectrograph index to serial number","Once Patrick sends the serial numbers of the spectrographs and their ""names"" (e.g. MTRed: ""1234567""), we can figure out which SAL index corresponds to which serial number, and have the CSC use the index to connect to the correct device.    [~pingraham]: might as well attach the serial number information/photos to this ticket.",1
"DM-21450","09/26/2019 16:29:52","Add a clock monitoring rule to the Watcher","Add a rule to the Watcher that reports if the clock is off for a given CSC. I suggest using the heartbeat event for this, in anticipation that such an event will eventually be available from all CSCs.    It will probably be safest to compare private_sndStamp to private_rcvStamp since the latter is set when DDS reads the sample for the Watcher (eliminating one source of delays in processing the data).    Potential issues:  - If DDS samples are ever delayed significantly then we may get false alarms. Though we probably want to know about that.  - If the Watcher's clock is off then *all* CSCs will appear to have incorrect clocks.",2
"DM-21451","09/26/2019 17:48:50","Remove DatabaseDict and vectorize Datastore/Butler ingest APIs","From #dm-middleware on slack:  {quote}I'd like to get Datastores using the new schema stuff I added recently (TableSpec/FieldSpec) for anything they want to keep in the registry database, and I want to stop using a dict interface, as it's hard to map __setitem__ to insert and update, especially for bulk operations.  I'm thinking of:   * Removing DatabaseDict and DatabaseDictRecordBase entirely.   * Add abstract methods to StoredDatastoreItemInfo to translate to/from dictionaries that correspond to records to be saved.   * Moving the logic for inserting and retrieving StoredDatastoreItemInfo from the database directly into GenericDatastoreBase.  That would probably be _insert_info and _fetch_info methods that replace _info_to_record and _record_to_info.   * Overriding _insert_info and _fetch_info in InMemoryDatastore to save into a dict instead of letting GenericDatastoreBase's implementations write to the database.{quote}",3
"DM-21453","09/26/2019 19:21:49","Remove extra sdssCentroid plugin from default DipoleTask plugin list","There seems to be an duplicate sdssCentroid plugin run in ip_diffim. This ticket will remove it.",1
"DM-21454","09/26/2019 20:16:51","Foreign key error when running makeButlerRepo.py against Oracle","Running with daily stack d_2019_09_26 + master ci_hsc_gen3 + Oracle.  Start with clean local dir and clean Oracle db.   scons dies in makeButlerRepo.py   {code}  sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-02264: name already used by an existing constraint [SQL: 'ALTER TABLE run ADD CONSTRAINT run_dataset_fkey FOREIGN KEY(pipeline_id) REFERENCES dataset (dataset_id) ON DELETE SET NULL'] (Background on this error at: http://sqlalche.me/e/4xp6)  {code}  See end of description for full traceback.  Ran it with sqlalchemy echo=True and then looked for run_dataset_fkey in output:  {code}  $ grep run_dataset_fkey run.out  2019-09-26 13:58:39,356 INFO sqlalchemy.engine.base.Engine ALTER TABLE run ADD CONSTRAINT run_dataset_fkey FOREIGN KEY(environment_id) REFERENCES dataset (dataset_id) ON DELETE SET NULL  2019-09-26 13:58:39,591 INFO sqlalchemy.engine.base.Engine ALTER TABLE run ADD CONSTRAINT run_dataset_fkey FOREIGN KEY(pipeline_id) REFERENCES dataset (dataset_id) ON DELETE SET NULL  sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-02264: name already used by an existing constraint [SQL: 'ALTER TABLE run ADD CONSTRAINT run_dataset_fkey FOREIGN KEY(pipeline_id) REFERENCES dataset (dataset_id) ON DELETE SET NULL'] (Background on this error at: http://sqlalche.me/e/4xp6)  {code}  Full traceback  {code}  Traceback (most recent call last):    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-26-gfeec7ef/bin/makeButlerRepo.py"", line 56, in <module>      outfile=args.outfile)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-26-gfeec7ef/python/lsst/daf/butler/butler.py"", line 226, in makeRepo      registryClass.fromConfig(config, create=createRegistry, butlerRoot=root)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-26-gfeec7ef/python/lsst/daf/butler/core/registry.py"", line 165, in fromConfig      butlerRoot=butlerRoot)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-26-gfeec7ef/python/lsst/daf/butler/registries/oracleRegistry.py"", line 79, in __init__      butlerRoot=butlerRoot)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-26-gfeec7ef/python/lsst/daf/butler/registries/sqlRegistry.py"", line 112, in __init__      self._createTables(self._schema, self._connection)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-26-gfeec7ef/python/lsst/daf/butler/registries/sqlRegistry.py"", line 193, in _createTables      schema.metadata.create_all(connection)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/sql/schema.py"", line 4201, in create_all      ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1593, in _run_visitor      visitorcallable(self.dialect, self, **kwargs).traverse_single(element)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/sql/visitors.py"", line 130, in traverse_single      return meth(obj, **kw)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 788, in visit_metadata      self.traverse_single(fkc)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/sql/visitors.py"", line 130, in traverse_single      return meth(obj, **kw)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 857, in visit_foreign_key_constraint      self.connection.execute(AddConstraint(constraint))    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 974, in execute      return meth(self, multiparams, params)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 72, in _execute_on_connection      return connection._execute_ddl(self, multiparams, params)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1036, in _execute_ddl      compiled,    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1234, in _execute_context      e, statement, parameters, cursor, context    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1452, in _handle_dbapi_exception      util.raise_from_cause(sqlalchemy_exception, exc_info)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 296, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb, cause=cause)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 276, in reraise      raise value.with_traceback(tb)    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1230, in _execute_context      cursor, statement, parameters, context    File ""/usr/local/lsst_stack/d_2019_09_26/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.2.16+2/lib/python/SQLAlchemy-1.2.16-py3.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 536, in do_execute      cursor.execute(statement, parameters)  sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-02264: name already used by an existing constraint [SQL: 'ALTER TABLE run ADD CONSTRAINT run_dataset_fkey FOREIGN KEY(pipeline_id) REFERENCES dataset (dataset_id) ON DELETE SET NULL'] (Background on this error at: http://sqlalche.me/e/4xp6)  {code}    ",3
"DM-21458","09/26/2019 21:36:48","Fix a failing unit test in ts_ATDomeTrajectory","Using the SAL 4/salobj 5 release candidates ts_ATDomeTrajectory has a failing unit test:  test_simple_follow in test_dome_trajectory.py",0
"DM-21463","09/27/2019 16:30:48","Test the Middleware Code by Moog","I this task, I will test the TCP/IP message from middleware code in the googletest framework. The unit test of function will be implemented as well. I also need to make the Makefile can be switched between production and development modes.",5
"DM-21464","09/27/2019 16:35:07","Update the MTAOS to use SALOBJ 5.0.RC.2 in Phase 1","In this task, I will begin to update the ts_MTAOS to use the salobj 5.0. It is working with salobj 3.x right now. At this moment, I have only the salobj 5.0.RC.2 (RC: release candidate). The configuration file should be put in ts_config_mttcs.",3
"DM-21465","09/27/2019 19:17:35","Add moto to T&S docker for unittesting s3 connections","Please add the {{moto}} python module to the telescope and site docker container, so that we can use it to unittest the salobj code that will write to the newly-planned LFA. It is pip-installable.    https://github.com/spulec/moto",1
"DM-21466","09/27/2019 21:06:06","Update developer guide to reflect change to devtoolset-8","Now that we are using devtoolset-8 everywhere we need to update the developer guide and pipelines.lsst.io.",1
"DM-21472","09/30/2019 20:22:12","Please include conda-build in Docker containers","Please include conda-build in Docker containers (at least those intended for development work). It is installed using:  {code}  conda install conda-build  {code}    Also building conda recipes requires a conda version of {{pytest-flake8}}, which is apparently not what is in the Docker image, alas. That can be installed using the following, but it's probably only safe to have that one or the pip version, not both:  {code}  conda install -c conda-forge pytest-flake8  {code}  [~ecoughlin] and others are trying to make our T&S packages conda-installable and we need these packages for that.    @",1
"DM-21480","10/01/2019 06:10:53","Connect lsst-ts github group to Jira for branch/PR links","It would be very helpful for the telescope and site developers if we could have the same branch & PR auto-linking to Jira that happens for the DM github group. Could you please configure Jira to link to the {{lsst-ts}} group on github?",1
"DM-21486","10/01/2019 22:04:18","October reprocessing of HiTS data","This ticket is to perform the regular monthly reprocessing of the HiTS dataset.",2
"DM-21488","10/01/2019 22:42:53","Using asAstropy() on a BaseCatalog will raise an exception unless lsst.daf.base has been imported","If you try to use {{asAstropy()}} (for conversion or pretty printing) on a {{BaseCatalog}} then an exception will be raised unless {{lsst.daf.base}} has been specifically imported by the user.  Thanks to [~jbosch] for noticing the key package.    {code:python}  import lsst.afw.table as afwTable    schema = afwTable.Schema()  schema.addField('test', float)    cat = afwTable.BaseCatalog(schema)  rec = cat.addNew()  rec['test'] = 0.0    print(cat)  {code}    raises ""TypeError: Unable to convert function return value to a Python type!""  ",0.5
"DM-21490","10/02/2019 00:13:08","correct async issue with message handler","async/await directives are required in part of the handler code.",2
"DM-21491","10/02/2019 00:14:48","Update ATDome and ATDomeTrajectory to use  salobj angle_diff and BaseCsc.disabled_or_enabled","Update ts_ATDome as follows:  * Replace locally defined {{angle_diff}} and {{assertAnglesAlmostEqual}} with the versions from salobj.  * Use {{BaseCsc.disabled_or_enabled}} instead of the custom {{want_connection}} property which provides exactly the same information.",0
"DM-21499","10/02/2019 19:42:35","Add configurable  timeout values for ACK messages","Messages between components have a hard-coded time associated between when the message is sent and when and ACK is received;  this should be configurable.",2
"DM-21503","10/03/2019 16:27:53","Add forwarder heartbeat timeout to detect dropped connection to ATArchiver","The Forwarder needs to detect that an anticipated heartbeat was missed in it's heartbeat thread, and re-advertise it's availability to redis if a heartbeat is missed.",0
"DM-21505","10/03/2019 17:50:32","Write conda development procedure","Write a conda development guide and procedure.",2
"DM-21515","10/03/2019 21:47:16","Update texlive and texmf containers for new tooling","The {{db2authors.py}} tool uses python f-strings and {{pyyaml}}.  This requires the texlive container to be upgraded to python > 3.6.  Additionally, we need to install the {{pyyaml}} package in the texmf container.",3
"DM-21524","10/04/2019 21:04:21","Extend the core Replication Framework to support operations on behalf of the Ingest system","The main objective of the development is to make extensions to the existing Replication System's framework to support the new Ingest system. Specifically, the following changes have been made in this context:   * extend the *Protobuf*-based protocol to support operations with the Qserv worker databases (inspecting, creating databases and tables, granting/revoking access privileges, operations with MySQL partitions, etc.)   * changes in the database schema and the *DatabaseServices* class to support super-transactions and chunk replicas in the ""being ingested"" state   * implement a group of SQL requests and the worker-side support for them as per the above mentioned protocol extensions   * refactor and extend class *Controller* to support new types of requests   * extend the *ControllerApp* to allow testing new requests and the updated API of the *Controller*",40
"DM-21526","10/04/2019 22:19:27","Prepare the Docker Image of Middleware Code","This task will prepare the docker image of middleware code.",0
"DM-21531","10/07/2019 17:27:46","Update scarlet to latest version","To prepare for the scarlet mini-sprint we need to pull the latest version from scarlet/master to the LSST fork and test with Jenkins.",1
"DM-21536","10/07/2019 19:37:22","EAS Work Phase 3","This epic is used to hold all stories associated with the Environmental Awareness System (EAS).   This is to continue work from:   * DM-17215   * DM-18732   * DM-20197",20
"DM-21537","10/07/2019 19:50:14","TMA Work Phase 2","TMA Training & Review of Labview-based vendor software in Tucson & Spain.    Also any on-going work & support regarding the TMA.    This is a continuation of DM-19848",20
"DM-21686","10/07/2019 21:06:07","Generate events and commands XML files for HVAC system - 1","Continue generation of XML files needed to control the HVAC system. The new files consist of the events and commands. For the events, an analysis will be performed on the actions to be taken associated with alarms in the system.",8
"DM-21691","10/08/2019 00:14:40","Update references to calib_psfCandidate in pipelines.lsst.io","pipelines.lsst.io still references {{calib_psfCandidate}}. Should be updated to {{calib_psf_candidate}} ",1
"DM-21694","10/08/2019 16:45:07","Write an MT rotator CSC in Python","Write an MT rotator CSC in Python, including a basic simulation mode.    Note that the simulator is the hard part. I plan to start with a very elementary model (no slewing) in this ticket, then refine it in another ticket.",8
"DM-21701","10/09/2019 02:49:42","Rebase and update with deblender sprint code","{{pipe_tasks}} changes made during the deblender sprint were never merged to master, so it still does not support {{meas_extensions_scarlet}}. This ticket is to rebase the branch and merge changes, confirming that it properly runs the latest version of {{meas_extensions_scarlet}}.",1
"DM-21702","10/09/2019 03:51:49","Investigate the cause of the high number of false sources in convolveTemplate=False image differencing","{{ip_diffim}} has the bool configuration option {{convolveTemplate}} implemented. With {{convolveTemplate=False}}, it switches the roles of the science and template images and performs the AL convolution of _the science image_ to match _the template_. For cases when the science image is sharper than the template {{convolveTemplate=False}} and {{doDecorrelation=True}} setup should perform as well as the ""baseline"" case when the template is the sharper one and it is convolved.         The resulting difference image is bad quality, indicated by an order of magnitude (~ 10 times) more sources detected (half positive, half negative approximately).    This ticket covers the work by [~gkovacs] to investigate the cause of this bug.",0
"DM-21711","10/09/2019 16:54:17","Remove cameraGeom interfaces deprecated in DM-18610","This includes (at least):   * {{Amplifier.getHasRawInfo()}}         This ticket should not be merged until after the 19.0 release.",1
"DM-21716","10/09/2019 19:14:09","Modernize setup.cfg in salobj","The setup.cfg file in ts_salobj has gotten a bit stale. Update it.",0
"DM-21718","10/09/2019 21:23:07","Consider eliminating the ups cfg file for a pure python package","A pure python package ""foo"" does not need {{ups/foo.cfg}} if SConstruct specifies {{noCfgFile=True}}, e.g.:  {code}  scripts.BasicSConstruct(""ts_foo"", disableCc=True, noCfgFile=True)  {code}    Consider whether to take advantage of this and eliminate the unnecessary file. I admit it'll be harder to add C++ code later, but it's already a bit of a chore and not a common change.    I consider this a ""nice to have"" since the current system works just fine.",0.5
"DM-21719","10/09/2019 21:25:27","DDS packages need some extra env vars to run tests","Packages that use OpenSplice DDS need some environment variables that scons does not normally see. Please consider adding the following to {{tests/SConscript}} for T&S packages:    {code}  for name in (""OSPL_URI"", ""OPENSPLICE_LOC"", ""ADLINK_LICENSE""):      val = os.environ.get(name)      if val is not None:          env.AppendENVPath(name, val)  {code}    I consider this moderate priority since I keep getting bitten by it (pytest works and everything seems fine, but scons fails).",1
"DM-21724","10/09/2019 22:28:55","Unpickling error reading qgraph with DimensionUniverse","If I create a QuantumGraph and then try to read it, there's an error.  Originally got the same error using just BPS code, but this can be reproduced just using pipetask commands.  Using w_2019_40 + master ci_hsc_gen3.    Create QuantumGraph  {code}  $ pipetask -d ""visit = 903334"" -b ${CI_HSC_GEN3_DIR}/DATA/butler.yaml -p lsst.meas.base -p lsst.ip.isr -p lsst.pipe.tasks -i calib/hsc,raw/hsc,masks/hsc,ref_cats,skymaps -o isr1 qgraph -t isrTask.IsrTask:isr -C isr:${CI_HSC_GEN3_DIR}/configs/isr.py -q isr_qgraph.pickle  py.warnings WARN: /usr/local/lsst_stack/w_2019_40/stack/miniconda3-4.5.12-1172c30/Linux64/pex_config/18.1.0-5-gc286bb7+3/python/lsst/pex/config/config.py:1279: FutureWarning: Config field doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.    FutureWarning)    ctrl.mpexec.cmdLineFwk INFO: QuantumGraph contains 112 quanta for 1 tasks  {code}    Try to run using the QuantumGraph:  {code}  pipetask -b ${CI_HSC_GEN3_DIR}/DATA/butler.yaml -p lsst.meas.base -p lsst.ip.isr -p lsst.pipe.tasks -i calib/hsc,raw/hsc,masks/hsc,ref_cats,skymaps -o isr1 run -g isr_qgraph.pickle   Failed to build graph: DimensionUniverse with version '0' not found.  Note that DimensionUniverse objects are not truly serialized; when using pickle to transfer them between processes, an equivalent instance with the same version must already exist in the receiving process.  Traceback (most recent call last):    File ""/usr/local/lsst_stack/w_2019_40/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-28-g561929e/python/lsst/daf/butler/core/dimensions/universe.py"", line 235, in _unpickle      return cls._instances[version]  KeyError: 0    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""/usr/local/lsst_stack/w_2019_40/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-7-g391bbaf+3/bin/pipetask"", line 26, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/usr/local/lsst_stack/w_2019_40/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-7-g391bbaf+3/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 130, in parseAndRun      qgraph = self.makeGraph(pipeline, taskFactory, args)    File ""/usr/local/lsst_stack/w_2019_40/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-7-g391bbaf+3/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 335, in makeGraph      qgraph = pickle.load(pickleFile)    File ""/usr/local/lsst_stack/w_2019_40/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-28-g561929e/python/lsst/daf/butler/core/dimensions/universe.py"", line 243, in _unpickle      ) from err  _pickle.UnpicklingError: DimensionUniverse with version '0' not found.  Note that DimensionUniverse objects are not truly serialized; when using pickle to transfer them between processes, an equivalent instance with the same version must already exist in the receiving process.  {code}",1
"DM-21732","10/11/2019 15:41:13","Test the Updated Middleware with Bare Rotator","Test the software with the bare rotator in SLAC. The followings are my plan there:    1. To be familiar with the rotator behavior by some simple SAL commands by the original SAL version 3.5.1.    2. Install the SAL 3.10.0 and update middleware code in the mangement PC. In this step, I will need to stop the running middleware (in SAL 3.5.1) and backup the data in ""/nfsdemo"" directory.    3. Test the rotator hardware behavior by SAL 3.10.0 and do the record. If there is any document of rotator behavior recorded already in SLAC, I will be happy to read. I also want to talk to the users of rotator to see there is any bug found or not. This should be recorded and fixed in the following update.    4. Discuss with the developer of CCS to see how the CCS works with the rotator. Since I might be one of the client of CCS (I am responsible of the active optics), I do want to know how the CCS works.",3
"DM-21734","10/11/2019 19:06:26","Add new events to SALGenerics.xml","A _settingsApplied_ and _softwareVersions_ event need to be added to the _SALGenerics.xml_. See the section [Results from Group Discussion|https://confluence.lsstcorp.org/display/LSSTCOM/Commissioning+Activities+2019-05-14+Meeting+notes] for specification of the events and attributes.",1
"DM-21736","10/11/2019 20:55:59","Add setup information enabling Vim file type recognition to the dev guide","The DM dev guide contains useful setup information on how to configure Vim to be consistent with LSST coding standards, but by default, Vim will not use that setup without an appropriate entry in the ~/.vimrc file. A few extra lines in the dev guide are required to explain this. ",1
"DM-21737","10/11/2019 21:43:36","Issue Call for Proposals for Community Brokers","Write a call for broker proposals and clear it with the SAC.",8
"DM-21738","10/11/2019 22:18:27","Put Dual License on pex_config so it can be distibuted under bsd clause 3. ","Change the license associated with pex_config:    * Change LICENSE to state dual license and then refer to BSD and GPL license files.  * Remove GPL short license from all files and refer to LICENSE file.  ",1
"DM-21741","10/12/2019 00:30:08","Improvements on attcs class","Implement some improvements to ATTCS class with lessons learned on recent Auxiliary Telescope on-sky activities. ",1
"DM-21742","10/12/2019 00:31:37","Write Auxiliary Telescope operations documentation.","Write initial version of Auxiliary Telescope operations manual. ",2
"DM-21744","10/12/2019 01:25:16","ts_watcher v0.2 requires salobj v5 but claims to support salobj 4","ts_watcher v0.2 claims to be compatible with salobj v4.5 but the new Clock rule uses a feature only found in salobj v5.",0
"DM-21745","10/12/2019 18:02:27","Please allow enumerations with arbitrary values in our XML files","Please allow us to specify values for enumeration constants in XML files. This has two advantages:  * It allows us to specify mask bits using enumeration constants. This can greatly simplify documentation and use of bit masks in SAL messages.  * Enumerations can start at 0. Use cases for this include the MT rotator which outputs several enumeration values that start at 0 (and is vendor code so we can't fix that) and the ScriptProcessState for which Tiago has found a use for 0=unknown.    I suggest that if any value is specified then all values must be specified. That should simplify the code that interprets the XML, make it easier for a reader to be confident of the values, and prevents accidental omission from messing up mask enums.    One possible implementation is the obvious:  <Enumeration>EnumName_AValueName=5, EnumName_AnotherValueName=7...</Enumeration>",1
"DM-21747","10/14/2019 13:08:32","Coordinate the v4.4.0 Release of XML","Reivew pull-requests, update tests and push out the RPMs for the XML v4.4.0.",2
"DM-21748","10/14/2019 16:49:41","oracle ci_hsc_gen3  sqlalchemy.exc.ObjectNotExectuableError","Doing the weekly ci_hsc_gen3 run with Oracle throws an error during registerInstrument.py.  Using stack w_2019_41, ci_hsc_gen3 master  (sqlalchemy 1.3.8 from stack).  I do not see the following error if use sqlite3.  Verified get same error message using stack built on lsst-dev + oracle.  Verified I do not get the error message if using w_2019_40.    {code}  python /work/mgower/gen3work/weeklyCItest/git/ci_hsc_gen3/bin/registerInstrument.py /work/mgower/gen3work/weeklyCItest/git/ci_hsc_gen3/DATA  Traceback (most recent call last):    File ""/work/mgower/gen3work/weeklyCItest/git/ci_hsc_gen3/bin/registerInstrument.py"", line 33, in <module>      instrument.writeCuratedCalibrations(butler)    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/obs_subaru/18.1.0-19-g582278a1/python/lsst/obs/subaru/gen3/hsc/instrument.py"", line 150, in writeCuratedCalibrations      butler.put(camera, datasetType, unboundedDataId)    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-29-g0429a71/python/lsst/daf/butler/core/utils.py"", line 243, in inner      return func(self, *args, **kwargs)    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-29-g0429a71/python/lsst/daf/butler/butler.py"", line 400, in put      recursive=not isVirtualComposite)    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-29-g0429a71/python/lsst/daf/butler/core/utils.py"", line 243, in inner      return func(self, *args, **kwargs)    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-29-g0429a71/python/lsst/daf/butler/registries/sqlRegistry.py"", line 492, in addDataset      self.associate(run.collection, [datasetRef, ])    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-29-g0429a71/python/lsst/daf/butler/registries/sqlRegistry.py"", line 599, in associate      self._insert(datasetCollectionTable, values, onConflict=""ignore"")    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/daf_butler/18.1.0-29-g0429a71/python/lsst/daf/butler/registries/sqlRegistry.py"", line 1001, in _insert      self._connection.execute(query, values)    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 988, in execute      return meth(self, multiparams, params)    File ""/usr/local/lsst_stack/w_2019_41/stack/miniconda3-4.5.12-1172c30/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/sql/elements.py"", line 289, in _execute_on_connection      raise exc.ObjectNotExecutableError(self)  sqlalchemy.exc.ObjectNotExecutableError: Not an executable object: <lsst.daf.butler.registries.oracleRegistry._Merge object at 0x7f64a4a026d0>  scons: *** [DATA/calib] Error 1  scons: building terminated because of errors.  {code}",1
"DM-21751","10/14/2019 19:03:52","Update SAL SDK for rPI to lastest SAL and DDS","Build the latest OpenSpliceDDS Community Edition to run on the new Raspberry Pi 4 platform. Generate a SAL/DDS SDK image for use by the EAS contractor.",3
"DM-21758","10/14/2019 19:36:03"," Electrometer CSC Hardware integration Phase 1","Re-implement hardware communication for Electrometer CSC phase 1: Electric Boogaloo         Revamp serial communication.",3
"DM-21762","10/14/2019 21:27:12","Analyze options and select a fast algorithms for mock catalog generation","The problem this issue concerns is as follows:    > Given orbits (state vectors) for M objects, and times, centers, and fields of view for N visits, find all visits where objects M would appear.    This is a fundamental algorithm for:   * Mock catalog generation   * Population debiasing applications   * Precovery applications (finding which older visits could plausibly contain newly discovered objects)    Existing algorithms take ~weeks on O(100) cores to compute O(100k) orbits. Need to speed this up to O(1hr) on O(20) cores to comfortably fit within the computational budget for precovery.    This ticket is to track the exploration of various options.",20
"DM-21766","10/14/2019 21:37:14","Add per-dataset-type tables to Registry","Move dimension foreign key fields to separate, per-dataset-type tables.  Duplicate primary key fields and optionally (via configuration) duplicate other fields.    This should move the schema to something like the ""Partition datasets (only)"" design on [https://confluence.lsstcorp.org/display/DM/Dataset+and+Collection+Table+Reorganization|https://confluence.lsstcorp.org/display/DM/Dataset+and+Collection+Table+Reorganization].",0
"DM-21768","10/14/2019 21:41:02","Vectorize dataset insert API","Update the public and internal APIs for dataset insertion in Registry, but only use bulk insert operations where trivial (i.e. not when autoincrement IDs and hence DM-21203 are in play).",2
"DM-21772","10/14/2019 21:44:36","Merge upstream changes to LSST oorb","Merge (major) upstream changes to OpenOrb, resolve any conflicts, and update the eups build scripts to work with the new OpenOrb build system.",2
"DM-21779","10/14/2019 22:40:21","Test new enumerations with values","Test the new SAL support for enumerations with values by adding such an enumeration to Test and updating ts_sal tests to test for the existence and value of the enumerations in SALPY.",1
"DM-21780","10/14/2019 23:25:10","Make a few simple scripts that exercise the MT Rotator and Hexapod","Make a few simple command-line scripts that exercise the MT Rotator by talking to the Rotator CSC. This is intended to help [~ttsai] shake out the MT Rotator at SLAC, especially my new Python CSC.",1
"DM-21781","10/15/2019 00:20:23","Update MT Rotator and Hexapod to use XML enumerations with specified values","Update the XML for the MT Rotator and Hexapod to take advantage of the new capability in SAL 4 to specify values for enumerations. The desired changes are as follows:  * Update ts_xml to use 0 as the starting value for the State, OfflineSubstate and EnabledSubstate enumerations.  * Rename State to ControllerState to match the field name.  * Update ts_xml to include enumerations for the mask values for the applicationStatus field of controllerState event.  * Update ts_xml documentation for the controllerEvent fields accordingly.  * Update ts_idl Rotator and Hexapod enums to match the changed and new enums in ts_xml.  * Update ts_rotator and ts_hexapod to stop incrementing the values of the controllerState fields (except application) and to use the enums in ts_idl directly instead of making copies that are 0-based.",1
"DM-21795","10/15/2019 20:03:07","Rework Registry provenance objects to match prototype","(original description is no longer accurate; see comments)    -Registry's provenance tables - execution, run, and quantum - can't currently be fully populated without having database updates.  For example:-   * -you can't insert a dataset until after its run has been inserted;-   * -you can't insert a run until you've inserted the execution it inherits its ID from;-   * -you can't insert an execution until after it's completed, because it has an end timestamp field.-    -Work through low-level use cases for these tables and ensure we can actually have all of their values when it's time to insert them, splitting up tables as necessary.-    [~mgower]-,- [~cs2018]-: this ticket exists because I'm assuming updates are a problem.  Please let me know if they aren't or if you have any expections/requirements/wisdom on when various provenance records should be inserted relative to the datasets they refer to.-",2
"DM-21797","10/15/2019 22:12:01","Add standard WCS to FiberSpectrograph","DM-21263 packages the wavelength in output FITS files as a separate data array semantically distinct from the primary data. This ticket changes the output FITS files to store the wavelength using the FITS -TAB standard format as documented in FITS paper III.",3
"DM-21807","10/16/2019 14:57:20","TMA Software Preparation","1) Identify and gather software related wants from out team. Aside from the obvious SAL update, branstorm with Dave and other relevant members what is expected for me to be able to do with the software.    2) After identifying a few items attempt to understand the code BEFORE the visit to spain. This will produce nots and questions    3) Prepare and make these notes public or if useful put them down on confluence or an lsst.io page to begin documentation on this software. Perhaps talk to Andrew Serio since he has a way we should be organizing our documentation.     4) Put all of this preparation in a Confluence page and create some collaborative information.",2
"DM-21808","10/16/2019 20:57:54","Add missing commits from May deblending sprint","Somehow a number of changes made on May 16 in the {{deblender-sprint}} branch on {{meas_extensions_scarlet}} did not get merged into DM-19991 with the rest of the deblender sprint tickets. This ticket is to re-implement those changes, which includes    * adding a blank source for point sources that fail to initialize (this is only temporary, since a more permanent solution will be implemented in DM-19790  * fix bug that prevents PSFs that are different sizes in each band from being combined into a multiband PSF (to be implemented in {{afw}} with DM-19789)",2
"DM-21813","10/17/2019 15:08:01","The ""filter"" should only be set for raw ccds and not for raw amps in obs_lsst","With DM-21048 it is possible to read in raw data even when the filter is not listed in the obs package, although a warning is given.  This can be annoying because currently a warning is given for each amplifier, although it is unnecessary to set/check the filter when reading in amps, it only needs to be done (and is done!) when assembling the ccd.    It appears as though the place to put the change is in https://github.com/lsst/obs_lsst/blob/aeaebe9ad92137ac7a57e2bd580746e2cbffe16f/python/lsst/obs/lsst/lsstCamMapper.py#L393 where the call to {{self._standardizeExposure}} should have {{filter=False}}.",0.5
"DM-21826","10/18/2019 18:43:13","TAP availability endpoint returns a 503","Fritz, Gregory, and I have all seen that /availability throws a 503.  Here's the stack trace, looks like its trying to look up some java class and not finding it:         2019-10-18 17:39:58.231 TAP ObsCore [http-nio-8080-exec-8] ERROR AvailabilityServlet  - BUG    java.lang.NullPointerException    at java.lang.Class.forName0(Native Method)    at java.lang.Class.forName(Class.java:264)    at ca.nrc.cadc.vosi.AvailabilityServlet.doGet(AvailabilityServlet.java:126)    at javax.servlet.http.HttpServlet.service(HttpServlet.java:635)    at javax.servlet.http.HttpServlet.service(HttpServlet.java:742)    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198)    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:496)    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140)    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81)    at org.apache.catalina.valves.AbstractAccessLogValve.invoke(AbstractAccessLogValve.java:650)    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87)    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:803)    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:790)    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1468)    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)    at java.lang.Thread.run(Thread.java:748)",3
"DM-21828","10/18/2019 20:12:36","Create a notebook to determine if tokens are expired","So, tokens expire in the notebook all the time, and aren't refreshed.  This causes no end of confusion, and it's hard to tell when someone's token is busted.  Create a notebook to help debugging of token issues.",1
"DM-21830","10/18/2019 22:19:28","Run the ATHeaderService from a container in the NCSA Test Stand","The ATHeaderService has been ""dockerized"" and can be run a service from inside a container. However, the container version has not been tested outside docker at the NCSA test stand and need to be deployed and test communication with other CSCs on the same network.",1
"DM-21836","10/21/2019 15:00:46","Add OBSTYPE/purpose to Gen3 Registry exposure table","We need to make sure the type of exposure (e.g. ""science"", ""bias"", ""flat"") is extracted from metadata and put into the registry's exposure table in ingest.  I think this is what's called OBSTYPE in FITS, and I'm not sure if ObservationInfo already includes it and we drop it on the floor in ingest, or if we need to add it to astro_metadata_translator (either way, I'm sure that's on [~tjenness]'s radar).    In any case, this is becoming particular important as we prototype Gen3 CPP pipelines, which we of course want to run only on raws with specific types.",1
"DM-21843","10/21/2019 20:10:24","pipetask runner fails at constructing qgraph dot files","Attempting to generate a qgraph dot file to examine the quantum graph fails with traceback:    {code}  Failed to build graph: '<' not supported between instances of 'Dimension' and 'Dimension'  Traceback (most recent call last):   File ""/software/lsstsw/stack_20191001/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-9-g27d0315/bin/pipetask"", line 26, in <module>   sys.exit(CmdLineFwk().parseAndRun())   File ""/software/lsstsw/stack_20191001/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-9-g27d0315/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 130, in parseAndRun   qgraph = self.makeGraph(pipeline, taskFactory, args)   File ""/software/lsstsw/stack_20191001/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-9-g27d0315/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 408, in makeGraph   graph2dot(qgraph, args.qgraph_dot)   File ""/software/lsstsw/stack_20191001/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-9-g27d0315/python/lsst/ctrl/mpexec/dotTools.py"", line 182, in graph2dot   nodeName = _makeDSNode(dsRef, allDatasetRefs, file)   File ""/software/lsstsw/stack_20191001/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-9-g27d0315/python/lsst/ctrl/mpexec/dotTools.py"", line 122, in _makeDSNode   dsRefId = _datasetRefId(dsRef)   File ""/software/lsstsw/stack_20191001/stack/miniconda3-4.5.12-1172c30/Linux64/ctrl_mpexec/18.1.0-9-g27d0315/python/lsst/ctrl/mpexec/dotTools.py"", line 112, in _datasetRefId   for key in sorted(dsRef.dataId.keys()):  TypeError: '<' not supported between instances of 'Dimension' and 'Dimension'  {code}  The quantum graph is being generated, as issuing a ""run"" command to pipetask completes correctly.",0.5
"DM-21846","10/22/2019 00:52:10","Firefly TAP UI on LSST Portal is not defaulting to the correct TAP service","When the Portal (""suit"" application) is started up on the {{lsst-lsp-int}} instance of the LSST Science Platform, the initial TAP screen comes up pointed at the {{lsst-lsp-stable.ncsa.illinois.edu}} TAP service.  Both the {{-stable}} and {{-int}} services are configured into the TAP service menu, which used to work semi-accidentally, but now that authorization is fully enabled, only the matching TAP service will be accessible (-stable Portal must talk to -stable TAP, -int Portal must talk to -int TAP).    This ""instance matching"" has been in the design all along.    The TAP service list is currently statically configured in https://github.com/lsst/suit/blob/master/src/suit/js/SUIT.js    Can we change this code to get the LSST TAP service URL by extracting the base URL from the URL of the currently displayed page?  (Technically, by extracting everything before ""/portal"".)    We need this change not only to get `-int` working properly by default, but also to get this working on cloud-based LSP deployments.  Whatever the Portal URL is, let's say ""https://lsst.codes/lsp/portal"", the TAP service URL needs to be ""https://lsst.codes/lsp/api/tap"".",2
"DM-21853","10/22/2019 17:11:03","error occurring when ROTANGLE isn't set","There's currently a test which checks the date to see if the camera is up on the mountain, which masked a possible bug that [~tjenness] uncovered:    {quote}I think there's actually a logic error in the translator. The intent is that this should only get upset if it's a science observation and on the mountain and missing ROTANGLE. Instead it's getting upset with an ""unknown"" observation which is probably a dark or a flat which should never have a rotation angle set anyhow. The is_on_mountain check has been masking the other logic bug.{quote}    It might be a good idea to re-think the ""on the mountain"" logic (if it's actually important to differentiate being on the mountain... I'm not sure it is....), and set this as a configuration parameter, since we won't always be running this code ""on the mountain"".",1
"DM-21855","10/22/2019 17:22:29","Move daf.butler.instrument to obs_base","This move has long been planned.  Time to do it.",1
"DM-21859","10/22/2019 18:00:43","Multiple PrerequisiteInput quanta are clobbered by a single value.","Attempting to make a quantum graph that has multiple PrerequisiteInput entries of the same type results in one of the entries being processed N(=number of unique entries) times.",2
"DM-21860","10/22/2019 18:01:29","Add basic Gen3 butler support to obs_cfht","Add an Instrument class and PipelineTask config overrides sufficient to ingest CFHT data into a Gen3 butler repository.",2
"DM-21862","10/22/2019 18:14:21","Extend Gen3 butler support for obs_decam","obs_decam now has basic Gen3 butler support (sufficient for ingest), but we'll need to expand that to get to the point where we can run single-frame processing (IsrTask, CharacterizeImageTask, and CalibrateTask) via Gen3 on it.  That will include, at least:   * implementing {{writeCuratedCalibrations}} in the instrument class;   * specializing (and modifying, as necessary) the instrument-specific hooks for the gen2to3 tool, at least to the point where we can write scripts like the ones in ci_hsc_gen3/bin.src to make a fully-usable Gen3 repo.    I'm assigning this to [~Parejkoj] and the AP team for now, in the hopes it might be possible for him to finish the work he started with the initial DECam conversion.    A useful concrete end-point for this task would be a CI package that creates a Gen3 repo containing the HiTS (2015) raws, master calibrations, reference catalogs and runs the aforementioned tasks as a Pipeline.",8
"DM-21873","10/22/2019 20:04:19","Install the Docker in PhoSim Server","Install the docker in PhoSim server and change the image pull position setting. Record the steps to let Bo have the idea to follow on his server.",0
"DM-21874","10/22/2019 21:03:51","Create PipelineTask version of backward compatible ImageDifferenceTask","Create one or more PipelineTasks that allow us to run any image-differencing functionality we need to continue to support in Gen3.  My understanding is that this means we need all calexp-template differencing functionality, but not necessarily pairwise calexp-calexp differencing (I'm hoping [~swinbank] and/or [~ebellm] can confirm).    I'm creating this now and assigning it to the AP team for middleware planning purposes, with a wild guess at SPs.  We'll of course have some DRP team members helping out with the work (probably [~yusra] and/or [~nlust]).  People doing the work are welcome to turn this into an umbrella ticket in the future with no SPs or branches of its own, but please link blocker tickets to this one so the middleware team can easily track progress towards deprecating Gen2.",0
"DM-21875","10/22/2019 21:27:20","Add StorageClass and Formatter support necessary to persist lsst.verify.Measurement in Gen3 repos","Make it possible to persist and unpersist {{lsst.verify.Measurement}} objects via the Gen3 Butler.",2
"DM-21877","10/22/2019 22:36:10","Create ""marker"" Butler dataset for PPDB","Discussing the Gen 3 migration of {{lsst.ap.pipe.ApPipeTask}} and {{lsst.verify.tasks.PpdbMetricTask}} on [#dm-science-pipelines|https://lsstc.slack.com/archives/C2JPMCF5X/p1571778136107700], [~jbosch] and I concluded that the cleanest trigger for running a {{PpdbMetricTask}} would be to provide a dataset that indicates that the database contains all information for a particular data ID. I believe this can also simplify the problem of providing Butler-like input to {{PpdbMetricTask}}.    Prototype this sytem in a Gen 2 pipeline by doing the following:   * Create a new dataset type ({{ppdbConfig}}?), with visit+ccd granularity (matching the units of work of {{ApPipeTask}}).   * Make {{ApPipeTask}} dump its PPDB config at the end of DB-related processing for a particular data ID. Note that while the DB config is fixed, we still should have a dataset for each data ID so that Gen 3 code could know which units of processing are ""DB complete"".   * Create a new Task for creating a PPDB from a {{ppdbConfig}}, then configure {{PpdbMetricTask}} to use it in {{ap_verify}} runs.    These steps should test all aspects of the system except for tracking dependencies between pipeline and metric tasks, which is a Gen 3-only feature.",3
"DM-21882","10/23/2019 00:08:05","Hot Fix of Version Check in runIsr.py of ts_wep Repository","The version check is added in runIsr.py in the new LSST stack version. Since the binary file of ""tests/testData/bsc.db3"" will change when running the comcamCloseLoop.py in lsst-ts/ts_phosim, the continuous run of runIsr.py will fail in the same rerun/run1 directory.",0
"DM-21883","10/23/2019 00:09:12","Rotator and Hexapod settingsApplied events renamed","In DM-18424 the Rotator and Hexpod settingsApplied events were renamed to settingsAppliedLimits. This will break the vendor's CSC code and my own new CSC code. My code will be trivial to fix, but I worry about the vendor's code.    In addition the alias was not updated, so it does not match.    I suspect the driver was so we can use a standard settingsApplied event. But I don't think we can afford to break the vendor's code yet. We need to be sure that my new CSC code works first.",0
"DM-21885","10/23/2019 15:08:34","Convert concrete MetricTasks to PipelineTasks","The {{lsst.verify.MetricTask}} system was designed to be as forward-compatible with PipelineTask as possible, so it should be straightforward to convert it, its subclasses, and any related tasks that are still needed in Gen3.    This is an umbrella ticket for all of that work; I don't know exactly what that would entail, but would like to have any relevant tickets linked here somehow.  I've taken a wild guess at SPs, but would love to have that updated by someone who actually has a decent basis for estimation.",0
"DM-21886","10/23/2019 15:18:43","Create PipelineTask driver for ap_pipe tasks that interact with the APDB","Create a {{PipelineTask}} whose {{runQuantum}} method includes all of the logic in {{ApPipeTask.runAssociation}}.    This should probably be done after DM-21877, so it can use the same approach to signal completion of APDB writes to downstream PipelineTasks.     It's not clear what to do about ownership of {{ApPipeTask.ppdb}}. It must belong to the new task in Gen 3, but doing so will break config overrides in Gen 2 (and the DB location is _always_ overridden).",8
"DM-21889","10/23/2019 15:54:47","CmdLineActivator reuses command-line argument options in different sub-commands","The one I'm particularly aware of is {{-t}}, which is used in one context for TIMEOUT and another for Tasks.  We should do a sweep for others.    This may be best handled as part of an overall re-think of the command-line interface that maximizes consistency with other tools (DM-15257), but we should make sure to get it done one way or another.",2
"DM-21890","10/23/2019 15:57:33","Make command-line option command-line parsing order-independent.","It'd be very nice if at least most {{pipetask}} options could be passed before or after the subcommand, instead of requiring users to remember which options go where.    It's entirely possible this is a fundamental limitation of Python's argparse, but if that's the case I think we should strongly consider some way to mitigate the problem, perhaps even by duplicating all current top-level options for each subcommand so options always go after the subcommand.",2
"DM-21900","10/23/2019 18:22:37","More functionality for filename template generation","We'll have use cases (at least from HSC, but probably for LSST as well) for being able to include dimension metadata fields that aren't themselves the primary keys of other dimensions (e.g. {{detector.name_within_raft}}).  We may also (after DM-21773) want to be able to use per-dataset metadata fields as well.    This of course has implications for our ability to validate that templates with generate unique filenames, but I _think_ we already have cases where we trust the user because it's not practical for the system to be able to verify it.  If not, I think we'll want that here.",1
"DM-21909","10/23/2019 21:04:49","Orgainse DM session at PB2Science 3 in Boston","contact speaks and put agenda together (even if I am not attending !)",2
"DM-21910","10/23/2019 21:13:36","Move lsst.verify.gen2tasks.MetricTask to lsst.verify.tasks","The migration plan alluded to in [DMTN-098|https://dmtn-098.lsst.io/], and researched in DM-16503, involved creating separate Gen 2 and Gen 3 {{MetricTasks}}. This plan was motivated by the assumption that metrics would be in wide use by the time of the migration, making writing a class adapter for each and every metric impractical. However, {{MetricTask}} has seen limited uptake, making the careful approach unnecessary.    The new plan is to follow the process the Middleware group has been using, and retrofit Gen 3 functionality onto {{MetricTask}} directly. The only loose end is the current use of {{gen2tasks.MetricTask}}.    Move {{gen2tasks.MetricTask}} to {{tasks.MetricTask}}, keeping the old name as a deprecated alias (much as we did when we created the new {{geom}} package from {{afw.geom}}). Update known MetricTasks to use the new name immediately.",1
"DM-21912","10/23/2019 21:25:10","Implement PpdbMetricTask.runQuantum","{{lsst.verify.tasks.PpdbMetricTask}} must take a single database as input, but may compute a more fine-grained metric. Therefore, its {{run}} method takes the output data ID as a keyword argument. Create a custom {{runQuantum}} method to support this argument; it should be identical to {{PipelineTask.runQuantum}} except for the extra keyword.",1
"DM-21917","10/23/2019 22:57:43","Convert DCR templates to PipelineTasks","The DCR template generation code inherits from {{AssembleCoaddTask}} which has already been converted to a PipelineTask, so it should hopefully not be too much extra work to update {{DcrAssembleCoaddTask}}. I expect the majority of the work will involve adding a {{runQuantum}} method and a {{DcrAssembleCoaddConnections}} class.",8
"DM-21930","10/24/2019 16:16:13","MT AOS Development & Testing Phase 3","This epic will have the stories for the development and testing of the main telescope active optics system (MTAOS).    This is phase 3 of [DM-20178|https://jira.lsstcorp.org/browse/DM-20178].   ",20
"DM-21931","10/24/2019 16:20:27","Develop and Test the Hexapod and Rotator Software Phase 1","This epic will maintain, develop, and test the software of Hexapod and Rotator contains the middleware, GUI, and wrapper code in cRIO.    This will include testing of the CCW and Camera Hex/Rot on the Camera Cart at the Summit.  This testing is currently slated for the first two weeks of December.",20
"DM-21932","10/24/2019 17:24:24","Please support topics with no public fields","If practical, it would be helpful to allow topics that have no public fields (no <item>s in the XML). This would avoid the need for garbage fields that we ignore, making the intent clearer.         Obvious candidates include: the heartbeat event and most state transition commands (enterControl, enable, disable, standby, exitControl).",2
"DM-21936","10/24/2019 17:40:42","Make new package containing actuator simulators written in Python","Many CSC packages contain a simulation mode that relies on a simulated actuator, including ts_ATDome, ts_ATMCSSimulator, ts_rotator and ts_hexapod.    Most of them rely on a simulated point to point actuator but ts_ATDome also has a simulated slewing/tracking actuator that could be useful for ts_rotator.    Copy both the point to point actuator and tracking actuator to a new package named \{\{ts_simactuators}}. Then on a separate ticket update the packages that use simulated actuators to use the new package.",2
"DM-21937","10/24/2019 17:54:47","Create new error-handling protocol for MetricTask","Currently, running a {{MetricTask}} may either return a {{Measurement}}, return {{None}}, or raise an exception (preferably {{MetricComputationError}}). More details can be found [in the documentation|https://pipelines.lsst.io/v/v18_1_0/modules/lsst.verify/tasks/lsst.verify.gen2tasks.MetricTask.html#error-handling].    [~jbosch], in the DM-21885 discussion, said:  bq. I think we'll want to take a look at the expected-failure modes for {{MetricTasks}} you refer to in #3 as use cases for {{PipelineTasks}} in general, and define some rules that would allow them to work with generic activators.  We've done a tiny bit of work in that area so far, but have long known that we need more sophistication in classifying and handling failures.    Depending on how much these rules change the existing behavior, we may need to change the implementation of every concrete {{MetricTask}}, so story points are hard to estimate.",0
"DM-21941","10/24/2019 19:11:33","Update SAL for CentOS 8","Update the SAL SDK for CentOS 8, build and test CSC's against it for  C++, Python (pybind11), Java, LabVIEW",5
"DM-21942","10/24/2019 19:12:40","Update SAL runtime RPMS for CentOS 8","Update the RPM generation for the SAL runtimes to be compatible with CentOS 8",5
"DM-21946","10/24/2019 19:18:18","Implement code inspector to generate CSC RPM dependencies","Write code inspector scripts to parse CSC code in C++, Java, Python, LabVIEW  to determine interdependencies, and generate RPM dependency list for use  with CSC RPM builders",5
"DM-21947","10/24/2019 21:20:36","Prepare for DMLT Oct 2019","slides and stuff",1
"DM-21948","10/24/2019 21:21:32","The watcher needs ts_salobj in its ups table","The ts_watcher package depends on ts_salobj but does not list it in its ups table file.",0
"DM-21949","10/24/2019 22:04:33","Only output tracking and trackLost events when newly true","ts_rotator should only output the tracking and trackLost events when they are newly true. Note that these events will not be truly useful until the XML is cleaned up in DM-21699, but this is better than nothing.",0
"DM-21950","10/24/2019 22:16:36","Update validate_drp to work with fgcmcal calibrations as an option","In order to test the performance of fgcmcal calibrations, validate_drp needs to be updated to read fgcmcal and fgcmcal_tract photoCalib files if configured to do so.    Additionally, post metrics for running fgcmcal on RC2.",2
"DM-21951","10/24/2019 22:25:19","Support November 1 rotator tests at SLAC","Support testing my rotator CSC code on the rotator at SLAC.    This testing was originally scheduled for Monday, October 28 but was moved to November 1.  ",1
"DM-21953","10/25/2019 00:17:00","Create Community post about new Gaia refcat","To close out DM-19473, we need a Community post announcing the new refcat so people can use it. I will write that post now and declare the epic finished.",1
"DM-21957","10/25/2019 02:37:46","Update AT packages to use ts_simactuators","Update packages that have simulated actuators to use ts_simactuators. These include:    * ts_ATDome  * ts_ATMCSSImulator",1
"DM-21958","10/25/2019 04:26:36","Upgrade the M2 Control System to LabVIEW 2018","This task will upgrade the M2 control system from LabVIEW 2016 to 2018. This task will try to fix some minor bugs as well.    This is a follow-up task of [DM-21158|https://jira.lsstcorp.org/browse/DM-21158].",3
"DM-21959","10/25/2019 04:35:38","Test the Middleware of Hexapod and Rotator with SAL 4.0","This task will test the middleware code with SAL 4.0. The previous test in SLAC is SAL 3.10.0. T&S team plans to use the SAL 4.0 in the integration test in Dec.",0
"DM-21960","10/25/2019 04:37:42","Ship the Management and Diagnostic PC to Chile","This task will ship the management PC and diagnostic PC (ThinkPad) of hexapod and rotator to Chile.    The story point of this task contains the work in DM-21959.",1
"DM-21969","10/25/2019 17:07:05","Replace simulated actuator in ts_rotator with a tracking actuator","ts_rotator's mock controller uses a point to point actuator to model the rotator. That is not a very good fit (though tolerable given the frequency of tracking updates).    ts_simactuators includes TrackingActuator which is a much better model. Switch to that.",2
"DM-21970","10/25/2019 17:48:40","Mini integration test for ts_xml 4.4.0 and ts_sal 3.10.0 at the NCSA Test Stand","A new version of {{ts_xml 4.4.0}} has been released. This new version rpms/repo and the accompanying new meta-data need to be tested at the NCSA Test Stand. The test should include {{ATCamera}}, {{ATArchiver}} and {{ATHeaderService}}.",2
"DM-21972","10/25/2019 18:35:50","Create simple code to simulate script loading","Create a simple set of code that simulates loading scripts and can be used to quickly try different solutions to speed up script loading.    This continues the work started in DM-21175 to speed up script loading.  ",2
"DM-21973","10/25/2019 20:29:02","Update Docker information for atHeaderService","The Docker configuration for the {{ATHeaderService}} is now defined in the repo  https://github.com/lsst-dm/sal_Dockerfiles    Moreover, the {{ATHeaderService}} now can run as a service on a docker container.",1
"DM-21995","10/28/2019 21:22:07","Plan the end-to-end build/deploy/test effort","The goal of this effort is create a Confluence page that outlines the steps needed to complete the end-to-end automated build/deploy/test infrastructure.  It will include estimates of the time and resources needed.  At the end, this will feed a Jira Epic and its Tasks that will track this effort.    After the initial page is created, I will meet with stakeholders ([~aclements], [~tribeiro], [~rhl], [~mareuter], etc) to get by-in and approval of the plan.",2
"DM-21999","10/29/2019 01:47:48","Jenkins fails to build macOS binaries","The Jenkins nightly and weekly release builds are failing to build binary packages for macOS (although Linux is fine).    For example, look at [nightly #792|https://ci.lsst.codes/blue/organizations/jenkins/release%2Fnightly-release/detail/nightly-release/792/pipeline] or [weekly #256|https://ci.lsst.codes/blue/organizations/jenkins/release%2Fweekly-release/detail/weekly-release/256/pipeline/176].    In both cases, the pipeline superficially appears to have completed successfully. However, in fact the “build eups tarballs” stage failed on {{osx-10.14.clang-1000.10.44.4.miniconda3-4.5.12-f032070}} with the (repeated) message:  {code}  Scheduling project: release » tarball  Starting building: release » tarball #4830  release » tarball #4830 completed with status FAILURE (propagate: false to ignore)  {code}  then eventually  {code}  giving up on build but suppressing error  {code}    This error is due to a failure to find a dylib in the astshim package:  {code}  ____________________ ERROR collecting tests/test_append.py _____________________  ImportError while importing test module '/Users/square/j/ws/release/tarball/osx/10.9/clang-1000.10.44.4/miniconda3-4.5.12-f032070/build/stack/miniconda3-4.5.12-f032070/EupsBuildDir/DarwinX86/astshim-18.0.0+6/astshim-18.0.0+6/tests/test_append.py'.  Hint: make sure your test modules/packages have valid Python names.  Traceback:  tests/test_append.py:5: in <module>      from astshim import Frame, SkyFrame, UnitMap, FrameSet, append  python/astshim/__init__.py:26: in <module>      from .base import *  E   ImportError: dlopen(/Users/square/j/ws/release/tarball/osx/10.9/clang-1000.10.44.4/miniconda3-4.5.12-f032070/build/stack/miniconda3-4.5.12-f032070/EupsBuildDir/DarwinX86/astshim-18.0.0+6/astshim-18.0.0+6/python/astshim/base.so, 2): Library not loaded: libastshim.dylib  E     Referenced from: /Users/square/j/ws/release/tarball/osx/10.9/clang-1000.10.44.4/miniconda3-4.5.12-f032070/build/stack/miniconda3-4.5.12-f032070/EupsBuildDir/DarwinX86/astshim-18.0.0+6/astshim-18.0.0+6/python/astshim/base.so  E     Reason: image not found  {code}    However:    * I am able to build astshim, using the same {{eups distrib}} incantation as Jenkins, on my macOS laptop with no problems;  * Jenkins has itself built this version of astshim in an equivalent Conda environment as part of the stack-os-matrix — this failure appears to be peculiar to the release pipelines.    ",3
"DM-22002","10/29/2019 15:42:56","ATHeaderService: Rename settingsApplied event","In order to implement the changes ratified in the [settingsApplied|https://confluence.lsstcorp.org/display/LSSTCOM/Commissioning+Activities+2019-05-14+Meeting+notes#CommissioningActivities2019-05-14Meetingnotes-settingsApplied] discussion, the similarly named CSC specific event needs to be renamed or removed. Please see the generic event attributes to make that decision.    This is a child of CAP-357",1
"DM-22003","10/29/2019 15:50:14","MTHeaderService: Rename settingsApplied event","In order to implement the changes ratified in the [settingsApplied|https://confluence.lsstcorp.org/display/LSSTCOM/Commissioning+Activities+2019-05-14+Meeting+notes#CommissioningActivities2019-05-14Meetingnotes-settingsApplied] discussion, the similarly named CSC specific event needs to be renamed or removed. Please see the generic event attributes to make that decision.    This is a child of CAP-365",1
"DM-22005","10/29/2019 16:22:01","Renew lsst-demo SSL certificate - Fall 2019","This ticket records that the lsst-demo server SSL certificate was renewed by [~loi].",1
"DM-22015","10/29/2019 22:02:22","Add <LanguageSupport> to SALSubsystems.xml","Add a new tag to SALSubsystems.xml to allow the selection of which languages are required to be supported per CSC.     Values allowed include ""c++,python, idl,java,labview""  ",3
"DM-22016","10/29/2019 22:04:41","Deploy xml 4.4 on Tucson Test Stand. ","Ticket to track work for deployment on Tucson Test Stand. ",1
"DM-22017","10/29/2019 22:53:08","Testing M1M3 Cell phase 5 part 1","Continuing testing the M1M3 Cell according Jira test plan for it.",2
"DM-22018","10/29/2019 22:56:18","Electrometer CSC Hardware integration Phase 2","Unit Tests and Jenkinsfile.",2
"DM-22019","10/29/2019 23:00:36","Electrometer CSC Hardware Integration Phase 3","Documentation.",1
"DM-22021","10/29/2019 23:23:15","Please allow unlimited Domain Participants in our OpenSplice configuration","Please add the following to the ospl.xml configuration file in the {{<DDSI2Service name=""ddsi2"">}} section:  {code}    <Discovery>       <ParticipantIndex>none</ParticipantIndex>    </Discovery>  {code}    so the ScriptQueue can load more than 9 scripts at one time.",1
"DM-22026","10/30/2019 18:09:46","Present LOY1 alerts strategy to Science Collaborations","Present the strategy outlined in LSE-459 to the science collaborations at a PST talk 20 Nov.   LSE-459 should be reviewed and agreed by the DM-SST/PST/SAC prior to giving this talk",1
"DM-22027","10/30/2019 19:01:40","Re-make Gaia refcat to fix epoch","[~eggl] just pointed out to me that the time epoch expressed in the gaia catalog are given in TCB, not UTC as I had thought. This results in a ~50s systematic offset of all of the epochs (all Gaia DR2 epochs are J2015.5). Although 50s probably doesn't matter for LSST parallax/PM calculations (which we're not doing yet anyway), it's easy enough to fix the ingest config and re-process the data. I can set that to run this weekend.    While I'm at it, I will correct the meas_algorithms ""how to make a refcat"" guide to have the updated config too.    Once it's generated and on lsst-dev, I'll have to re-request permission to copy it into place. Since the catalog is so new, I'll just overwrite the existing one entirely.",1
"DM-22034","10/31/2019 00:33:00","ATDomeTrajectory should wait for its remotes to start","The ATDomeTrajectory should wait for its remotes to start before trying to do anything else.",0
"DM-22037","10/31/2019 17:19:19","CSCs should automatically wait for their remotes to start","ts_salobj Controller, BaseCsc, etc. should wait for remotes to start in the {{start}} method. This reduces the danger of a CSC trying to use a remote before it is ready and eliminates the need for the CSC to explicitly wait for its remotes (a step that is easy to forget).    Note: automatic wait is impossible for remotes that are constructed with {{start=False}}; if you create one of those then you must handle starting it yourself. Fortunately the only use case (so far) is the SAL/Kafka producer.    Also fix a bug in handling the {{LSST_DDS_IP}} environment variable: the value was translated to an unsigned long which might be too large to fit into the {{private_host}} field of topics, which is a signed long. This resulted in mysterious failures to output any DDS samples. Fixed by casting the value to a signed long. I added a test for this casting.",1
"DM-22041","10/31/2019 20:52:29","Improvements to the deployments scripts to support the new Ingest system","The main goal of the effort is to improve the deployment & management scripts to support the new Ingest system at {{lsp-int}} (former {{PDAC}})  ",2
"DM-22044","10/31/2019 23:02:19","Upgrade ATMonochromator CSC to salobj 4 and make it configurable","Just realized that ATMonochromator needs to be upgraded to salobj 4 and to become a configurable CSC.",1
"DM-22061","11/01/2019 18:52:42","ts_xml another iteration","This task is to remove the developers that are no longer on the team from the ts_xml salsusbsystems table.     I imagine it will take a few iterations before completing the table, therefore it makes more sense to update the table now having no information rather than misinformation.    Along with removing old developers, I will focus on updating these rows.",1
"DM-22062","11/01/2019 20:47:25","Add parquet support to Gen3 Butler","Port formatters for Parquet files to Gen3, including support for lazy loading of columns.",2
"DM-22066","11/02/2019 00:08:00","Fix issues discovered in SLAC rotator testing","In SLAC rotator testing today we discovered several issues that need fixing:    * The {{counter}} field was the wrong type for Command and Header (should be uint, not ushort).  * The rotator and hexapod CSCs have no heartbeat output. This strongly suggests we should have a base class for both CSCs.  * The rotator commander outputs too much noise because of encoder jitter (not a surprise). This is likely an issue for the hexapod commander as well.    Note that we still were not able to command the rotator: the commands were ignored. Fixing that is a different ticket.    I have attached a file that shows some binary telemetry data we read from the rotator, and how the Python code parses it.",1
"DM-22067","11/02/2019 00:11:37","Diagnose failure to command rotator","In rotator testing at SLAC DM-21951 we discovered that commands from the Python CSC were being ignored.    This ticket is to diagnose the problem. I propose to make use the fake cRIO controller in the ts_rotator package to talk to the real vendor's CSC, monitor the data in the commands sent by that CSC, and compare it to the data in the commands sent by the Python CSC.",2
"DM-22068","11/02/2019 12:52:51","Add ABC, StorageClass, and Formatter for stray-light correction","In order to support butler I/O for HSC's bespoke stray-light correction files, we should:   * Add an abstract base class for camera-generic stray light correction information to ip_isr, and make the implementation in obs_subaru inherit from it.   * Add a StorageClass to daf_butler for the abstract base class.   * Add a formatter to obs_subaru for the concrete HSC class.   * Add code and possibly a script to ingest the stray light data into a Gen3 repository.",1
"DM-22069","11/02/2019 21:41:14","Add lazy-product BoundedField class","[~erykoff] would like a BoundedField implementation that simply multiplies two other BoundedFields, and I've long said I'll add it as soon as it becomes a pressing need - that kind of composition is something I've always had in mind for BoundedField but hadn't prioritized.     ",2
"DM-22070","11/03/2019 14:31:00","Add unnormalized (but continuous) version of PixelScaleBoundedField","PixelScaleBoundedField is a bit inconvenient to use in full-focal-plane contexts (the only contexts in which we expect to use it) because it normalizes by the area of the origin pixel in order to yield a unitless quantity, and that origin pixel is a per-CCD quantity, resulting in a discontinuous combined field over the focal plane.    Duplicate and adjust it to define a new PixelAreaBoundedField class; I'll RFC deprecating and removing the original in a separate RFC to avoid worrying about code duplication in the interim.  Also make sure the new class is persistable (the original is not).     ",2
"DM-22073","11/04/2019 14:34:52","Add matplotlib (output) support to Gen3 butler","Port Gen2's write-only support for writing matplotlib Figures to (at least) PNG to Gen3.",1
"DM-22076","11/04/2019 17:22:49","Setup the Linux Laptop","Setup the Linux laptop that will be used on the summit for the test.",0
"DM-22079","11/04/2019 18:18:37","Linearity input bug in DM-18610","An oversight in DM-18610 prevents linearity type and coefficients from being read correctly from FITS.  Fix is trivial.",1
"DM-22089","11/04/2019 19:29:15","Update LDM-503 to clarify requirements verification using available data","As discussed at DMLT F2F 2019-10-28, LDM-503 needs to be updated to reflect policy around which requirements can be verified within the DM subsystem, based on available data. ",1
"DM-22093","11/04/2019 22:28:48","Store begin/end times of ap_pipe in ap_verify","[~cmorrison] asked to get time stamps for the start and end of an {{ap_verify}} job (excluding metric calculation, if possible). Find a way to do so; probably the {{StartUtc}} and {{EndUtc}} values logged by {{pipe.base.timeMethod}}.",2
"DM-22102","11/05/2019 15:51:40","Camera Hex/Rot Software Setup","Any assembly/testing needed at the summit for the Camera Hex/Rot to be ready for the individual functional testing as well as the Camera Rot/CCW/Camera Cart integration testing.  Will be working in tandem with IT (IT-1145) in regards to this process.    Will probably need switches and/or UPS from IT to complete this task.",2
"DM-22103","11/05/2019 16:28:11","Please uncomment some logging statements in the MT Rotator cRIO","Please enable the logging statements at lines 400, 403 and 408 in rotator/targetx2/commanding.c: the commented-out syslog calls in:  {code}                // select who can command the rotator: GUI (default/0) or DDS (1)                  case CMDSOURCE:                      if (isOffline(state1) && isPublishOnly(offlineSubstate1))                      {                          syslog(LOG_ERR, ""command not allowed in Offline/PublishOnly state"");                          gCommandSourceDDS = 0;                          break;                      }                      if (cmdMsg.param1 > 0.5)                       {                         if (gCmdDDSServerConnected == 1)                         {                            //syslog(LOG_NOTICE, ""Command source now == DDS"");                            gCommandSourceDDS = 1;                         }                         //else syslog(LOG_NOTICE, ""Not connected to DDS Cmd socket"");                      }                                          else                      {                         //syslog(LOG_NOTICE, ""Command source now == GUI"");                         gCommandSourceDDS = 0;                      }                      break;  {code}  Please also look for and enable similar messages in the hexapod commanding.c    This will help us diagnose when the rotator or hexapod cRIO decides to ignore commands from the CSC.    Note that any command from the ""GUI"", which is what we call the Engineering User Interface (EUI), will reset gCommandSourceDDS to 0, making the CSC unable to command the cRIO. That is based on different code in rotator/targetx2/cmdClientSocket.c:  {code}              // IF DDS was in control, a cmd from GUI will revert to GUI control                if (gCommandSourceDDS)                 {                     ////printf(""Command source==GUI\n"");                   gCommandSourceDDS = 0;                }  {code}  This looks like a bug or misfeature to me (unless it also changes the state to Offline/PublishOnly, which I strongly doubt).    I would expect the CSC to always be able to control the rotator except in Offline/PublishOnly mode.   * This task will try to understand the logging mechanism and log the useful information. This will be helpful for the following debug.",0
"DM-22104","11/05/2019 16:36:43","Refactor ts_rotator and ts_hexapod to put more common code in ts_hexrotcomm","There is too much duplication between the ts_rotator and ts_hexapod CSC and mock controllers. Refactor more of the common code into ts_hexrotcomm. I will do this work as part of DM-22066.",2
"DM-22107","11/05/2019 18:04:20","ATHexapod CSC: Revamp for salobj 4/5","Revamp athexapod CSC for Salobj 4/5 and ConfigurableCSC support.",2
"DM-22109","11/05/2019 21:17:03","Updating LOVE and Script to use generics post-SAL-4.1","The LOVE_Events.xml currently defines the logMessage event, but this is now a Generic event.  It just needs to be removed from the LOVE_Events.xml file.",0
"DM-22110","11/05/2019 22:11:35","Update HeaderService docker image for ts_sal 4.0.0","SAL_VERSION=4.0.0  SALOBJ_VERSION=5.0.0  XML_VERSION=4.4.1  IDL_VERSION=1.0.0        ",1
"DM-22120","11/06/2019 20:18:02","ap_verify scales poorly to large runs","[~cmorrison] reports that {{ap_verify}} can time out when run over large datasets on {{lsst-dev}}. This is partly because of the time needed to run the pipeline itself, but also the time needed to iterate through all metrics (since {{MetricsControllerTask}} was never parallelized). {{ap_verify}} does not offer any error recovery options beyond rerunning the entire pipeline.    Both {{ap_verify}}'s current control system and {{MetricsControllerTask}} will become obsolete with Gen 3, where responsibility for workflow management (and any checkpointing) will lie with the pipeline activator. Rather than try to design proper restart behavior into {{ap_verify}} now, provide a {{\-\-skip-completed}} command-line flag that does the following:  * runs {{ap_pipe}} with the {{\-\-reuse-outputs-from all}} command-line argument, which skips completed pipeline steps (currently, up through {{association}}).  * makes {{MetricsControllerTask}} check for a job file associated with each data ID, and skips processing that data ID if the file already exists    This flag should be enough to let us retry large runs efficiently until Gen 2 is retired.",2
"DM-22122","11/06/2019 22:00:09","Add a showAlarms command to the watcher","The INRIA group requested a command that will show all currently active alarms (those not in the nominal state). This will allow the Watcher user interface to ""catch up"" when it starts.    They also requested that the ConfiguredSeverities cycle forever.",2
"DM-22128","11/07/2019 15:28:16","Remove salpytools dependency from the HeaderService","The transition to ts_salobj has been fully vetted and therefore there is no need to continue carrying salpytools and its dependencies into the HeaderService. Therefore all  salpytools dependencies need to be removed from the HeaderService.",2
"DM-22139","11/07/2019 22:23:52","AP association bug with >1000 diaSources","I'm seeing a bug in {{ap_pipe}} when running association if the number of diaSources is large (>1000). I've posted the log starting with image differencing for one observation where the bug appeared, though this was not the only observation that failed. Other observations that had only ~980 or fewer diaSources ran successfully, while every observation with more than 1000 diaSources that I looked at failed.    I took my best guess at the story points and epic, so please correct those if I got them wrong!    {code:}  apPipe.differencer INFO: Subtracting images  apPipe.differencer.subtract INFO: Template Wcs : 2.619998,0.050177 -> 2.614735,0.052809  apPipe.differencer.subtract INFO: Science Wcs : 2.614754,0.052794 -> 2.619984,0.050191  apPipe.differencer.subtract INFO: Astrometrically registering template to science image  apPipe.differencer.subtract INFO: templateFwhmPix: 4.78053065883556  apPipe.differencer.subtract INFO: scienceFwhmPix: 3.957219530945321  ip.diffim.generateAlardLuptonBasisList INFO: Target psf fwhm is the greater, deconvolution mode  apPipe.differencer.subtract.selectDetection INFO: Detected 1020 positive peaks in 898 footprints to 10 sigma  apPipe.differencer.subtract.selectMeasurement INFO: Measuring 898 sources (898 parents, 0 children)   apPipe.differencer.subtract INFO: Growing 898 kernel candidate stars by 21 pixels  apPipe.differencer.subtract INFO: Selected 272 / 898 sources for KernelCandidacy  apPipe.differencer.subtract INFO: Matching Psf FWHM 4.78 -> 3.96 pix  ip.diffim.generateAlardLuptonBasisList INFO: Target psf fwhm is the greater, deconvolution mode  apPipe.differencer.subtract INFO: Final spatial kernel sum 26.409  apPipe.differencer.subtract INFO: Spatial model condition number 6.205e+09  apPipe.differencer.subtract INFO: Doing stats of kernel candidates used in the spatial fit.  apPipe.differencer.subtract INFO: 272 candidates total, 16 rejected, 237 used  apPipe.differencer.subtract INFO: Spatial kernel model well constrained; 237 candidates, 3 terms, 30 bases  apPipe.differencer.subtract INFO: Spatial background model appears well constrained; 237 candidates, 6 terms  apPipe.differencer INFO: Computing diffim PSF  apPipe.differencer.decorrelate INFO: Running A&L decorrelation: spatiallyVarying=False  ip_diffim_decorrelateALKernel INFO: Using matching kernel computed at (1024, 2048)  ip_diffim_decorrelateALKernel INFO: Variance (science, template): (92.492551, 0.004940)  ip_diffim_decorrelateALKernel INFO: Variance (uncorrected diffim): 98.240851  ip_diffim_decorrelateALKernel INFO: Variance (corrected diffim): 97.975441  apPipe.differencer INFO: Running diaSource detection  apPipe.differencer.detection INFO: Detected 7429 positive peaks in 1708 footprints and 139 negative peaks in 56 footprints to 5 sigma  apPipe.differencer INFO: Merging detections into 1228 sources  apPipe.differencer INFO: Running diaSource measurement: newDipoleFitting=True  apPipe.differencer.measurement INFO: Measuring 1228 sources (1228 parents, 0 children)   apPipe.differencer.forcedMeasurement INFO: Performing forced measurement on 1228 sources  apPipe INFO: Running Association...  apPipe.associator.diaCalculation WARN: Input diaSourceCat is indexed on column(s) incompatible with this task. Should be indexed on 'multi-index, ['diaObjectId', 'filterName', 'diaSourceId'].  apPipe FATAL: Failed on dataId={'ccdnum': 5, 'filter': 'g', 'visit': 288970, 'date': '2014-03-01', 'hdu': 31, 'object': 'Blind14A_04'}: KeyError: 124094082061435674  /software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pex_config/18.1.0-5-gc286bb7+4/python/lsst/pex/config/config.py:1279: FutureWarning: Config field ccdProcessor.isr.doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.    FutureWarning)  /software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_association/18.1.0-14-g371438c+2/python/lsst/ap/association/mapApData.py:185: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.    table_list = list(yaml.load_all(yaml_stream))  /software/lsstsw/stack_20191101/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/astropy/units/function/logarithmic.py:46: RuntimeWarning: invalid value encountered in log10    return dex.to(self._function_unit, np.log10(x))  Traceback (most recent call last):    File ""/software/lsstsw/stack_20191101/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2657, in get_loc      return self._engine.get_loc(key)    File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc    File ""pandas/_libs/index.pyx"", line 127, in pandas._libs.index.IndexEngine.get_loc    File ""pandas/_libs/index.pyx"", line 153, in pandas._libs.index.IndexEngine._get_loc_duplicates    File ""pandas/_libs/index_class_helper.pxi"", line 122, in pandas._libs.index.Int64Engine._maybe_get_bool_indexer  KeyError: 124094082061435674    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/18.1.0-9-gee19f03+2/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/18.1.0-9-gee19f03+2/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/18.1.0-9-gee19f03+2/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_pipe/18.1.0-4-gcd8f52c+37/python/lsst/ap/pipe/ap_pipe.py"", line 218, in runDataRef      associationResults = self.runAssociation(calexpRef)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/18.1.0-9-gee19f03+2/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_pipe/18.1.0-4-gcd8f52c+37/python/lsst/ap/pipe/ap_pipe.py"", line 306, in runAssociation      results = self.associator.run(dia_sources, diffim, self.ppdb)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/18.1.0-9-gee19f03+2/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_association/18.1.0-14-g371438c+2/python/lsst/ap/association/association.py"", line 139, in run      ppdb)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/18.1.0-9-gee19f03+2/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_association/18.1.0-14-g371438c+2/python/lsst/ap/association/association.py"", line 315, in update_dia_objects      filter_name)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/18.1.0-9-gee19f03+2/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_association/18.1.0-14-g371438c+2/python/lsst/ap/association/diaCalculation.py"", line 331, in run      filterName)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_association/18.1.0-14-g371438c+2/python/lsst/ap/association/diaCalculation.py"", line 392, in callCompute      objDiaSources = diaSourceCat.loc[objId]    File ""/software/lsstsw/stack_20191101/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1500, in __getitem__      return self._getitem_axis(maybe_callable, axis=axis)    File ""/software/lsstsw/stack_20191101/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1913, in _getitem_axis      return self._get_label(key, axis=axis)    File ""/software/lsstsw/stack_20191101/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/indexing.py"", line 141, in _get_label      return self.obj._xs(label, axis=axis)    File ""/software/lsstsw/stack_20191101/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/generic.py"", line 3585, in xs      loc = self.index.get_loc(key)    File ""/software/lsstsw/stack_20191101/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2659, in get_loc      return self._engine.get_loc(self._maybe_cast_indexer(key))    File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc    File ""pandas/_libs/index.pyx"", line 127, in pandas._libs.index.IndexEngine.get_loc    File ""pandas/_libs/index.pyx"", line 153, in pandas._libs.index.IndexEngine._get_loc_duplicates    File ""pandas/_libs/index_class_helper.pxi"", line 122, in pandas._libs.index.Int64Engine._maybe_get_bool_indexer  KeyError: 124094082061435674  {code}  ",2
"DM-22140","11/07/2019 22:44:03","Add language support selection to SALSubsystems.xml","Add a tag to the SALSubsystems.xml to allow specification of which languages we need to build support libraries and tests for.  e.g.     <Languages>cpp,java,python,pydds,labview</Languages>  ",1
"DM-22141","11/07/2019 22:50:12","Backup m1m3 test campaign EFD data, and make available in Tucson EFD","Backup the data from the M1M3 test campaigns and copy to NCSA and   make available access to the  live M1M3 EFD instance (mysql) from Tucson",3
"DM-22144","11/08/2019 15:30:20","Upgrade the M2 Control System to SAL 4.0 in Phase 1","This task begins to update the M2 control system to the latest SAL version. The original SAL version is 3.5.1. On the summit, only the telemetry and event can work for M2 control system. It can not be commanded by the SAL commands. This task will try to fix this bug as well.",3
"DM-22145","11/08/2019 15:32:08","Upgrade the M2 Control System to SAL 4.0 in Phase 2","This is a continued task of phase 1: DM-22144.",3
"DM-22146","11/08/2019 15:36:41","Test the Rotator/ Hexapod CSC based on ts_salobj","This task will test the new CSC codes by Russell to see why the rotator cRIO in SLAC can not receive the command. This is a follow-up task of DM-21951, DM-22104, DM-22066, and DM-22067.",2
"DM-22147","11/08/2019 16:24:01","Add python-only function to ChebyshevBoundedField to approximate another BoundedField","At the current moment the {{PixelAreaBoundedField}} (and the {{PixelScaleBoundedField}} that it is replacing, see RFC-644) are too slow (by a couple orders of magnitude) to use in coaddition via a {{PhotoCalib}}.  [~jbosch] says that this is a non-trivial fix (DM-22071).    However, for many purposes, it is possible to use a {{ChebyshevBoundedField}} to approximate a {{PixelAreaBoundedField}} (or other {{BoundedField}}).  Provided the field to approximate is smooth, preliminary tests show that (e.g.) a typical HSC WCS can be approximated at the 1e-7 level with plenty of speed, which is sufficient for use in a {{PhotoCalib}}.      This ticket is to add a python-only {{approxBoundedField}} method to {{ChebyshevBoundedField}}.  Tests will be written by approximating {{PixelAreaBoundedField}} (the primary use case) so this requires DM-22070 to be completed first.",2
"DM-22149","11/08/2019 17:38:15","Draft the SAL Functional Test Plan of Hexapod","Draft the hexapod test plan in SAL level.    The followings are suggested by [~rowen]:       Informally I would say that the same tests we proposed as for the rotator would be relevant here:  * Check that the state transition commands work as we think they were coded to (not exactly as per our standards). In particular I think clearError will send the CSC to OFFLINE/PUBLISH_ONLY, though I would love to be wrong about that.  * Check that the motion commands work as expected — to within a tolerance we can reasonably measure. I think trying to measure with high accuracy will be beneficial at some point (to understand what the hexapod is truly capable of), but I think our initial focus should be on sanity checking motion rather than trying to measure precision of motion.",0
"DM-22150","11/08/2019 17:43:28","The Rotator and Hexapod CSCs should reject all commands when the device is not commandable","The Rotator and Hexapod CSCs should check the state of `commandableByDDS` and reject *all* commands if the CSC is not allowed to control the device. This will prevent a major source of confusion: commands that fail or are silently ignored for no obvious reason.    I hope I can do all the work in ts_hexrotcomm, but will update ts_rotator and ts_hexapod as well, if necessary.",1
"DM-22151","11/08/2019 17:48:04","Rewrite make_idl_files to take advantage of DM-21106 to speed up file generation","Rewrite the code that generates IDL files to take advantage of DM-21106 to speed up the code. Call the new salgenerator command that only generates IDL files instead of the old command that generates additional code. Note that the deletion code can probably be simplified as well -- there is no point deleting files that make_idl_files does not create.",0
"DM-22155","11/08/2019 18:45:54","MTVMS_Telemetry.xml does not conform to Topic Naming Conventions","After updating the XML tests, it was found that the MTVMS_Telemetry.xml file defines three topics that do not conform to naming standards as defined in    [SAL_Recommendations|https://ts-xml.lsst.io/sal_constraints_and_recommendations.html] page.    The topics MTVMS_M1M3, MTVMS_TMA and MTVMS_M2 should be renamed to MTVMS_m1m3, MTVMS_tma (or MTVMS_mtmount) and MTVMS_m2, respectively.",1
"DM-22157","11/08/2019 21:09:48","Convert XML RobotFramework tests in to pure python - part 2","This task covers the time needed to complete the XML test conversion.  See DM-22032.",5
"DM-22162","11/09/2019 00:27:41","Add metadata writing to PipelineTask execution logic","PipelineTask execution should automatically write a metadata dataset for each Quantum.  We want that to appear in the QuantumGraph as well, so downstream PipelineTasks can use these as inputs.     ",2
"DM-22166","11/09/2019 18:07:56","Fix pipetask --show=pipeline option","After all recent changes --show=pipeline does not work anymore:  {noformat}  $ pipetask build -p $PIPE_TASKS_DIR/pipelines/DataReleaseProduction.yaml --show=pipeline  Traceback (most recent call last):    File ""/project/salnikov/gen3-middleware/ctrl_mpexec/bin/pipetask"", line 26, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/project/salnikov/gen3-middleware/ctrl_mpexec/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 119, in parseAndRun      self.showInfo(args, pipeline)    File ""/project/salnikov/gen3-middleware/ctrl_mpexec/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 398, in showInfo      for taskDef in pipeline:  TypeError: 'Pipeline' object is not iterable  {noformat}  ",1
"DM-22170","11/11/2019 14:34:42","ATHexapod CSC: Controller Integration Phase 1","Finish implementing CSC. Add mock controller and add unit tests.",2
"DM-22171","11/11/2019 14:35:32","ATHexapod CSC: Controller Integration Phase 2","Add Jenkinsfile.",2
"DM-22172","11/11/2019 14:36:27","ATHexapod CSC: Controller Integration Phase 3","Add documentation support to repo. Fix docstrings to comply with numpydoc format.",2
"DM-22174","11/11/2019 15:44:03","Attend the DM Boot Camp 2019","Attend the DM boot camp 2019. The agenda is [here|https://community.lsst.org/t/dm-boot-camp-2019/3887].    I am interested in:    1. System Architecture (2:30 EST, 12:30 PM in Tucson) at 11/12    2. All courses at 11/13 (begin from 2:00 EST, 12:00 PM in Tucson)    The meeting room will be in the Workroom (229) for this meeting.         Lectures will be remotely streamed: [https://princeton.zoom.us/j/5752451825]",0
"DM-22177","11/11/2019 18:11:25","ctrl_mpexec calls non-existent Pipeline.addConfigOverrideFile method","Running pipetask commands with the w_2019_45 weekly results in:  `Failed to build pipeline: 'Pipeline' object has no attribute 'addConfigOverrideFile'`  This method does not exist, but a similar `addConfigFile` does.",1
"DM-22190","11/12/2019 11:20:35","Perform end-to-end analysis of a visit with Spectractor shim","Take a visit that we have, and which Jérémy has analysed, and perform end-to-end extraction, getting roughly comparable results, _i.e._ something which looks like a legit extracted spectrum.    It doesn't have to be perfect (or even very good) yet, but something that isn't garbage-in, garbage-out, showing that the interface is working properly.",5
"DM-22191","11/12/2019 15:37:01","isr for yaml cameras is broken due to zero value for suspectLevel","I think that DM-18610 changed the default value for {{amplifier.getSuspectLevel()}} from Nan to zero, and this means that isr now masks out all amps built from yaml cameras.    This could be fixed many ways - getting all yaml cameras to specify it, getting the yaml camera builder to default it to nan in obs_base, having a failsafe in isr, because zero is never a legit value (if all values above zero are suspect you've got some serious problems that we can't help with, after all).    Some of these are more sensible than others, and yet more probably exist. I'm about to go on vacation so can't sort this out right now, but it seems quite urgent to me.",2
"DM-22192","11/12/2019 16:36:01","Remove PixelScaleBoundedField","Will be deprecated on DM-22070, which will probably make it into 19.0, so we'll assume for now that this can be removed after 19.0 and must be removed before 20.0.",1
"DM-22193","11/12/2019 17:20:52","Updates DMTN-107 ","Following the LOY1 alerts meeting, DMTN should be updated to include:   * references to the official pre-operation data previews  * a clear statement that we will use templates from the DPs will be used to generated LOY1 alerts. ",1
"DM-22195","11/12/2019 18:23:31","Continue generation of HVAC XML files","The MQTT based generated list of HVAC parameters, will be verified for completeness by comparing with the Windows server list and, then proceed with the final generation of XML telemetry, alarms and commands files.",8
"DM-22207","11/13/2019 16:21:11","Update dev guide and deprecate_pybind11 message to reflect the fact that it works on classes, top","See [https://lsstc.slack.com/archives/C2JPL2DGD/p1573657708086900]    More pair-coding between [~lguy] and [~jbosch].",1
"DM-22209","11/13/2019 20:10:53","Fix pipelines_lsst_io bug from d_2019_11_06","{code}    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx/environment/__init__.py"", line 785, in get_doctree      with open(doctree_filename, 'rb') as f:  FileNotFoundError: [Errno 2] No such file or directory: '/j/ws/sqre/infra/documenteer/doc_template/_build/doctree/py-api/lsst.afw.geom.IntervalD.doctree'  Exception occurred:    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx/environment/__init__.py"", line 785, in get_doctree      with open(doctree_filename, 'rb') as f:  FileNotFoundError: [Errno 2] No such file or directory: '/j/ws/sqre/infra/documenteer/doc_template/_build/doctree/py-api/lsst.afw.geom.IntervalD.doctree'  {code}    https://ci.lsst.codes/job/sqre/job/infra/job/documenteer/668/display/redirect",2
"DM-22217","11/14/2019 00:29:44","Do not over-fix obs_lsst detector bbox","This is related to DM-18610.  While updating that, I mistakenly reset the detector BBox to the sum of the amplifier RawBBox values.  This is unneeded, and creates the over-padded images.",1
"DM-22218","11/14/2019 02:14:43","Fix a bug in a configuration of the multi-master container","Host name substitution in file:  {code:java}admin/tools/docker/configured/scripts/configure.sh     {code}  doesn't follow changes in:  {code:java}admin/templates/installation/qserv-meta.conf {code}   As a result of this the *{{mysql-proxy}}* service at *{{NCSA PDAC}}*  (*{{lsp-int}}*)  failed to start. This was also followed by this messages reported in the *{{xrootd}}* log file:  {code:java}  % fgrep bind /qserv/log/xrootd.log  [2019-11-14T01:14:13.100Z] {{LWP,294}} INFO  xrdssi.msgs (cmsd:0) - XrdOpen: Unable to bind socket to port 1094; address already in use   {code}  The further analysis has revealed that the host name of the master service in the template-based configuration file of the *{{xrootd}}* service was set incorrectly as:  {code:java}  cat /qserv/run/etc/lsp.cf  ..  # host:port of manager instance (2131 is default)  all.manager master.localdomain:2131 {code}  In stead of the name of an actual host where the Qserv master container was being run (in a specific deployment at *{{NCSA}}*):  {code:java}  all.manager lsst-qserv-master01:2131{code}                  ",1
"DM-22219","11/14/2019 02:40:51","Add vscode configuration notes to dev guide","Like the title says.  Hopefully in time for bootcamp.",1
"DM-22229","11/14/2019 21:20:32","Write XML schema validation tests","Testing the XML files as well-formatted and against a schema file requires an additional python tool, probably lxml.  This task covers the work necessary to update the docker image to install this tool and write the tests and test generator scripts, as well as implement those tests in the build.",2
"DM-22231","11/14/2019 21:39:37","Update XML tests for the new InfluxDB limitations","With the move to InfluxDB, the column number and byte size limitations imposed by MySQL are removed.  This means the current suite of tests to very topics don't exceed those limitation are no longer necessary.  Additionally, InfluxDB introduces a new set of Reserved Words, see DM-22216.    This ticket tracks the work needed to remove the topic size tests, update the Reserved Words tests, as well as acts as documentation for this change.         The SAL Constraints documentation will also need to be updated:    [https://ts-xml.lsst.io/v/tickets-dm-20234/sal_constraints_and_recommendations.html#sal-topic-sizes]",1
"DM-22235","11/15/2019 21:15:22","Fix deprecated collections imports","A couple of our packages are importing Mapping or Iterable from collections rather than collections.abc. Fix these.",0.5
"DM-22236","11/15/2019 22:50:51","Test whitelight CSC with real KiloArc","Connect the Kiloarc lamp to ADAM    read the status lights on the KiloArc, which will be published as telemetry    verify that hitting the e-stop button results in an error status signal from Kiloarc, and this sends the CSC to fault state.",3
"DM-22253","11/18/2019 18:14:33","Audit usage of butler.get() in pipe_analysis scripts","As an initial setup towards Gen3-ification of the {{pipe_analysis}} scripts, audit all usage of {{butler.get()}} to get a feeling for how easy it will be to convert them to Gen3-land.",2
"DM-22254","11/18/2019 19:25:38","File LCR to address LIT-292, removal of ""LSSTC Board"" language in LSR and OSS; flowdown to DMSR","Refer to LIT-292 for the substance.",2
"DM-22258","11/18/2019 22:23:41","Monthly November ap_pipe HiTS rerun","It is November? Time to reprocess the full HiTS dataset with ap_pipe. Stick with the higher-res templates until DM-21330 is complete.",1
"DM-22260","11/19/2019 15:41:45","Metadata is not persisted when persisting an ExposureCatalog to fits","When writing an {{ExposureCatalog}} to FITS, the metadata property list is not persisted.  When writing a test case for this I ran into the problem that the case of the property list values is not always round-trippable ( https://community.lsst.org/t/fits-and-lowercase-header-keys/1184/3 ) but by looking at the header of the persisted file I see that this is unrelated to the current issue.  That is, the metadata is not persisted to the FITS file for an {{ExposureCatalog}} with any case.      {code:python}  import lsst.afw.table as afwTable  import lsst.daf.base as dafBase  import lsst.geom    name = 'TEST'    schema = afwTable.ExposureTable.makeMinimalSchema()  cat = afwTable.ExposureCatalog(schema)  cat.reserve(1)  rec = cat.addNew()    plist = dafBase.PropertyList()  plist.addDouble(name, 1.0)    cat.setMetadata(plist)    print(cat.getMetadata()[name])    cat.writeFits('%s_expcatalog_metadata.fits' % (name))    cat2 = afwTable.ExposureCatalog.readFits('%s_expcatalog_metadata.fits' % (name))    print(cat2.getMetadata()[name])    {code}  This results in a {{KeyError: 'TEST not found'}}.  ",1
"DM-22265","11/19/2019 20:00:03","BaseMockController CLEAR_ERROR command broken","Tiago reports:    The rotator CSC went to FAULT state... when trying to run the clearError  command I get the following:  ""Failed: module 'lsst.ts.hexrotcomm.enums' has no attribute 'CommandCode'""",0
"DM-22269","11/19/2019 20:36:20","Add S3 support to salobj","Add S3 support to salobj.    My plan to add the code I wrote as part of DM-21974 to salobj more or less ""as is"" and update the documentation.",0
"DM-22275","11/19/2019 22:25:34","Filter telemetry in command_hexapod.py","Ignore telemetry that change only slightly in {{command_hexapod.py}}. Add some jitter to the mock controller to exercise this change.",1
"DM-22278","11/20/2019 00:19:11","ATDomeTrajectory: Rename settingsApplied event","Implement CAP-356",0
"DM-22284","11/20/2019 17:42:48","Implement RFC-639: Stop supporting changing simulation mode after a CSC is running","Drop support for the {{setSimulationMode}} command.    * Add {{BaseCsc}} constructor argument {{simulation_mode}} and deprecate the old argument  {{initial_simulation_mode}}, but allow it to be specified for backwards compatibility.  * Update the documentation and unit tests accordingly.  * Update {{Controller}} to allow {{do_setSimulationMode}} to be missing This will allow me to remove the command from XML generics without breaking any code.  * File tickets to switch to the new argument in existing packages.  * File a ticket to remove the {{setSimulationMode}} command from the generic XML after the deprecation period expires  * File a ticket to remove the {{initial_simulation_mode}} argument from {{BaseCsc}} and a few subclasses after the deprecation period expires.",2
"DM-22285","11/20/2019 21:26:00","Update ups table in pipe_analysis to no longer require meas_mosaic","{{meas_mosaic}} is not required to be setup for the {{pipe_analysis}} scripts. At present, one can still explicitly ask for {{meas_mosaic}} calibrations to be applied with the config {{useMeasMosaic=True}} (default is {{False}}), but should give you a helpful error message if you do ask for it and it’s not setup (i.e. {{import}} is in a {{try:}}).  The {{ups}} table needs to be updated to reflect this.",1
"DM-22293","11/21/2019 19:49:38","Make units and description available in salobj topics.","SAL 4 includes units and description metadata for each field (as a comment appended to the line), as well as SAL and XML versions.    Parse these in salobj and:  * Make the units and description available as metadata from topics  * Make the SAL and XML versions available from SalInfo",2
"DM-22304","11/22/2019 17:40:11","Delete the char0 field from Test arrays topics","The char0 field should never have been part of the arrays topics for the Test component because char is a synonym for string and arrays of strings are not allowed.    I have slowly removed all code that requires char0 to be present. At this point I believe there is just one unit test in ts_salkafka that needs it, and I will fix that on the same ticket.    There is also some code in ts_salobj that handles char0 if present, but should work if the field is absent.",0
"DM-22307","11/22/2019 18:20:46","Test descriptions for arrays topics once DM-22303 is fixed","test_idl_parser.py has a test for topic metadata with a code block that is disabled because description data is missing for arrays topics DM-22303.    The fix made it into ts_sal 4.1. Once that is accepted as the current version then this ticket can safely be implemented.",0
"DM-22334","11/25/2019 16:07:07","TMA Preparation Week 1","This task is to capture the work done in preparation for the TMA integration on December 2. ",3
"DM-22340","11/25/2019 18:41:04","Change ATDome and ATMCSSimulator to not need scons","Change ts_ATDome and ts_ATMCSSimulator to not need scons.    Also fix a bug in ts_ATMCSSimulator: the TPVAJ.pva method mis-computes velocity and acceleration.",0
"DM-22363","11/25/2019 20:02:23","Add ability for defefredDatasetHandles to retrieve dataset components","If a task uses a deferredDatasetHandle, it should have the ability to fetch a component of the dataset if the handle wraps a composite object. Add that ability.",1
"DM-22364","11/25/2019 20:17:54","Gen 3 Butler cannot be created using daf.butler.Config","Trying to create a Gen 3 repository and a Butler to it using the idiom  {code:py}  config = Butler.makeRepo(root)  butler = Butler(config, run=""test"")  {code}  fails with an {{sqlalchemy.exc.OperationalError}} complaining that a {{run}} table does not exist. This behavior is surprising, as {{Butler.makeRepo}} is documented to return ""The updated {{Config}} instance written to the repo."", while the {{Butler}} constructor is documented to take a {{Config}}.    In contrast, calling the {{Butler}} constructor with {{root}} in the above example does work.",1
"DM-22367","11/25/2019 22:05:53","Create tests for the <RuntimeLanguages> tag in SALSubsystems.xml","Add tests for the new tag in SALSubsystems.xml to allow specification of which languages we need to build support libraries and tests for.   e.g.    <RuntimeLanguages>cpp,java,python,pydds,labview</RuntimeLanguages>",1
"DM-22378","11/26/2019 18:49:34","Allow rc version numbers in astropy eups package","The check version script in the eups package of astropy does not understand release candidate version numbers. Fix it so that  trailing characters are ignored.",0.5
"DM-22379","11/26/2019 20:19:29","Implement RFC-643: make Version and Alias tags option in our XML","Implement RFC-643: make the Version and Alias tags optional in ts_sal.    If possible please continue to check them as they are checked now, if present (in particular the Alias must match the topic name).",1
"DM-22382","11/26/2019 21:43:59","Add Configurable attribute to SALSubsystems XML","A new Configuration attribute should be added to the SALSubsystems.xml file. The values of the attribute should be a single character of Y if a CSC is configurable and N if not.",1
"DM-22385","11/26/2019 22:58:57","Some tests are failing in ts_hexapod v0.2.0","4 unit tests are failing in ts_hexapod v0.2.0. The cause is that I added jitter to the measured positions and didn't relax the comparison tolerances in the tests to accommodate.",0
"DM-22386","11/27/2019 16:37:06","Jointcal fails unit tests with Astropy 4","I tested a stack build with Astropy 4.0rc1 and that resulted in jointcal failing unit tests.    To do the install I:    * Made a fresh lsstsw deployment  * Removed astropy  * Updated pytest and pytest-* packages to get v5.3  * Followed instructions on https://github.com/astropy/astropy/wiki/v4.0-RC-testing to install astropy 4.0rc1  * Ran rebuild lsst_distrib    The failures in jointcal all relate to numerical changes in the final chi2 for astrometry and photometry, e.g:    {code}  AssertionError: 3292.076960636778 != 3292.08 : photometry_final_chi2  {code}    Jointcal passes with the default conda and pytest 5.3.",1
"DM-22388","11/27/2019 17:26:41","Investigate Astropy 4","We have been asked to test Astropy 4 with the stack. Do some builds and see if anything breaks.    To do the install I:  * Made a fresh lsstsw deployment  * Removed astropy  * Updated pytest and pytest-* packages to get v5.3  * Followed instructions on https://github.com/astropy/astropy/wiki/v4.0-RC-testing to install astropy 4.0rc1  * Ran rebuild lsst_distrib",2
"DM-22389","11/27/2019 17:29:41","verify fails with Astropy 4","Doing a build with Astropy4 I now get failures from {{verify}} package relating to YAML serialization.     This works fine with the same version of YAML on Astropy 3 but fails on Astropy 4.    {code}  self = <yaml.loader.SafeLoader object at 0x7fbb0063aba8>  node = SequenceNode(tag='tag:yaml.org,2002:python/object/apply:numpy.core.multiarray.scalar', value=[MappingNode(tag='tag:yam...Node(tag='tag:yaml.org,2002:int', value='0')]))]), ScalarNode(tag='tag:yaml.org,2002:binary', value='AAAAAAAAFEA=\n')])        def construct_undefined(self, node):          raise ConstructorError(None, None,                  ""could not determine a constructor for the tag %r"" % node.tag,  >               node.start_mark)  E       yaml.constructor.ConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:numpy.core.multiarray.scalar'  E         in ""<unicode string>"", line 9, column 14:  E                 value: !!python/object/apply:numpy.core ...   E                        ^    ../../stack/DarwinX86/pyyaml/5.1+2/lib/python/yaml/constructor.py:420: ConstructorError  {code}",2
"DM-22392","11/27/2019 18:18:49","Support the Hexapod Test on Summit","Support the hexapod Test on summit.",2
"DM-22404","11/28/2019 00:25:32","More efficient implementation of the replica management algorithms in Qserv Replication system","The current implementation of algorithm *{{FixUpJob}}* puts too much stress on the underlying infrastructure by launching all required _fix-ups_ (request objects of class *{{FixUpRequest}}*) at once. This overloads the persistent database of the Replication/Ingest system where the state of the system is maintained. This also results in a significant (and unneeded) traffic between the Replication Controllers and Replication workers.    Hence, the first goal of this development is to implement a {{flow control}} within the algorithm, so that only a limited number of request were {{in-fight}} at a time.    The second task is to make the algorithm more stable if more than one catalog needed rebalancing. The current implementation's _iterative_ approach results in excessive number of replicas created by the job.    A similar problem exists with the replica purging algorithm *{{PurgeJob}}*. This needs to be addressed as well.",8
"DM-22406","11/28/2019 16:32:45","Assist in mass IP changes","There is a rework going on with IT that will require the IP changes of many machines. [~ecoughlin] and I have split the machines on what is most relevant to our work. The table of machines that need IP changes is here [https://confluence.lsstcorp.org/pages/viewpage.action?spaceKey=LTS&title=Auxiliary+Telescope+Temporary+Deployment]",2
"DM-22407","11/28/2019 16:34:51","Document Labview Steps & other steps","Begin the documentation on the ts_sal repository. Currently there only exists PDF's outside of the repo. This is to create the SAL User Guide which will exist as a tech note. ",2
"DM-22409","11/29/2019 22:14:52","Include meas_extensions_scarlet in pipelines.lsst.io","As summary.",1
"DM-22412","12/02/2019 16:26:52","Bump OpenSplice version from 6.4.1 to 6.9","Just to change makefiles from OpenSplice version from 6.4.1 to 6.9 to match the current version of ts_sal@v4.0",1
"DM-22413","12/02/2019 16:31:27","Add error handling to XML parsing when loading setting files","When loading XML setting files there is no check if the file is correctly loaded. This caused me to debug the code for a good amount of time, because it was causing a segmentation fault. I propose to add some error handling everywhere this files are loaded so that end users can easily notice this kind of issues, of example if files are corrupted or moved to another location.",1
"DM-22417","12/02/2019 17:41:41","Remove python future from obs_base","In DM-22234 I knew that obs_base used {{from past}} but it seems that I lost track of fixing it. Do the fix (again) here.",1
"DM-22426","12/02/2019 21:11:41","Improve CSC startup and shutdown","Improve CSC startup by not starting the heartbeat pulse until after the CSC is ready to receive commands. Make sure that the initial summary state is delayed until then, as well (but I think it already is).    Improve close methods on Domain, SalInfo and Controller so if close is called multiple times the later calls wait for closing to finish. This avoids nasty messages from asyncio in unit tests. Also improve SalInfo.close by giving time for the read loop to terminate.    Reduce duplication in ReadTopic.close by making a new private method to cancel callbacks.",1
"DM-22427","12/02/2019 21:17:57","latitude and longitude are transposed in AUXTEL_LOCATION in obs_lsst","Similar to DM-21990, the auxtel longitude and latitude are swapped in latiss.py:   [https://github.com/lsst/obs_lsst/blob/74b9cd0735010b908c28591630d1b55b21330d12/python/lsst/obs/lsst/translators/latiss.py#L30]",0.5
"DM-22453","12/03/2019 17:57:16","Update load to safe_load for yaml reading in validate_drp","Update YAML loading to use {{safe_load}} in {{validate_drp}}.    1. Make trivial change to {{python/lsst/validate/drp/util.py}}.  Two lines.  2. Actually make sure the {{safe_load}} works beyond just making sure the Jenkins run passes.  Check to make sure the the yaml loading didn't change the logic of what validate_drp actually tries to do -- it still might succeed but it might skip half the tests.   Read the outputs, etc. to really make sure it's the same.  ",1
"DM-22455","12/03/2019 18:15:05","Create a proper release of the Portal application","Use the new Firefly build script feature in release-2019.3.2 to create a Portal application (""suit"" package) release as agreed with [~gcomoretto].    Fine-tune messages if there is a problem constructing the LSST TAP service URL.",2
"DM-22460","12/03/2019 19:08:53","Review test cases in S19 science pipelines acceptance test","Review test cases proposed for the S19 science pipelines acceptance test campaign",1
"DM-22469","12/04/2019 20:06:48","Add fgcmcal documentation tree","{{fgcmcal}} does not currently build documentation that will go on https://pipelines.lsst.io . Add this directory and template files.",3
"DM-22473","12/04/2019 22:43:19","Landing page message note","I know you’ve been looking at the landing page. Can you put something equivalent to this:    {code}  Check [here](https://github.com/lsst-dm/lsp-landing-page/blob/master/motd/integration.md)  for a complete list of messages.  {code}    into the framing material of the page so that I don’t have to replicate it in each new MOTD text?  No rush at all on this.",2
"DM-22474","12/04/2019 22:49:25","run_atmcs_simulator.py is not executable","bin/run_atmcs_simulator.py in ts_ATMCSSimulator is not marked as executable. Fix that with chmod +x.",0
"DM-22479","12/04/2019 23:35:11","SQuaSH being spammed with timestamps","DM-22093 added a ""Task finished"" timestamp to the metadata for all {{verify}} timing metrics. This inadvertantly caused the timestamps to be treated as tags in SQuaSH (see attached image), and will eventually bog down the server.    Change the timestamps from metadata to extras, which are not uploaded to SQuaSH. This is a breaking change, and will make accessing the data a bit clunkier:  {code:py}  measurement.notes[""end""]  measurement.extras[""end""].quantity  # yes, still a `str`  {code}",2
"DM-22481","12/05/2019 00:00:45","Do not delete datastore directory on error","[~dinob] discovered that two things go wrong when the datastore runs out of disk space:    1. The file partially written by the datastore does not get deleted.  2. If a directory was created by the put it tries to delete the directory but can't because the file is still there.    Item 1 should be fixed since we do not want partially written files left over in the datastore when a put fails.  Item 2 should be removed since it is possible that some other process is actively writing to the datastore and using that directory. The implication is that we should never try to remove directories if a transaction fails, solely files that are only under our control.  ",2
"DM-22485","12/05/2019 13:46:46","Fix docstring heading to make example appear","The example in the documentation for Butler.export doesn't appear in HTML because the heading is ""Example"", not ""Examples"".",0.5
"DM-22487","12/05/2019 16:39:58","Prototype Registry architecture for schema changes and (eventually) multiple layers","Work is done (see [https://confluence.lsstcorp.org/display/DM/Architectural+Prototype+for+the+New+Gen3+Registry]); creating this ticket belatedly to record effort involved.     ",8
"DM-22490","12/05/2019 18:02:51","Please allow the Alias tag to be missing in ts_xml unit tests","SAL 4.1 will allow the Alias tag to be missing from command and events. Please update the ts_xml unit tests to permit this.",1
"DM-22493","12/05/2019 20:09:29","Release new version of LSE-61 (LCR-1933/LCR-1664)","Make new release of LSE-61 with LCR-1933 and LCR-1664 included.",1
"DM-22504","12/05/2019 22:53:38","Support for lsstDebug functionality in Gen3 middleware","Following up the zoom session today, this ticket is about the support of {{--debug}} Gen2 command line option in the Gen3 middleware.    While it may not be a good idea to have interactive, execution blocking displays in pipeline code in the long term, there are some display implementations around. We need at least one execution mode (laptop activator; single process; via Jupyter?) when the existing lsstDebug based code can be run.",1
"DM-22517","12/06/2019 18:51:30","Please add metadata for the <name>ID field (i.e. a description).","If it's not too much work, I would appreciate having descriptive metadata for the <name>ID field (e.g. TestID or ScriptID) in topics in the IDL file. It is conspicuous as the only field with no @Metadata comment.    We know what it is for, but I think many readers might not, as the name is not self-descriptive.",1
"DM-22520","12/06/2019 21:09:55","Document topic support for pipeline tasks","Documenteer has a {{lsst-pipelinetasks}} directive that works analogously to {{lsst-cmdlinetasks}}. However, it is not mentioned in either the [module topic documentation|https://developer.lsst.io/stack/module-homepage-topic-type.html] nor in the [package templates|https://raw.githubusercontent.com/lsst/templates/master/project_templates/stack_package/example/doc/lsst.example/index.rst]. As a result, tasks that get converted to {{PipelineTask}} will drop off the module documentation.    Add a {{lsst-pipelinetasks}} section to the template (possibly before {{lsst-cmdlinetasks}}, which is still relevant until the migration is complete) and update the developer guide to match. Updating the {{index.rst}} files of individual packages is out of scope for this issue.",1
"DM-22529","12/09/2019 18:58:50","Dome Software Deliverables document II","This is continuation of DM-22451",5
"DM-22537","12/09/2019 21:52:07","Update pyyaml.load usage in documenteer 0.4.x and 0.5.x","[documenteer#63|https://github.com/lsst-sqre/documenteer/issues/63]:    {quote}  When running documenteer 0.4.5 with PyYAML 5.1 I get the following warning:    lsstsw3/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/documenteer/sphinxconfig/technoteconf.py:65:      YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.    _metadata = yaml.load(meta_stream)  Please update Documenteer's YAML usage accordingly (assuming that doing so wouldn't cause a PyYAML version conflict, of course).  {quote}    We'll release 0.4.7 and 0.5.5 with update {{yaml}} usage.",0.5
"DM-22543","12/10/2019 17:10:14","Create xml for CCHeaderService","The xml code for ComCam HeaderService (CCHeaderService) needs to be revised to be compatible with ComCam",2
"DM-22554","12/10/2019 21:27:59","Update ts_MTAOS to use simulation_mode instead of initial_simulation_mode","Update ts_MTAOS to use simulation_mode instead of initial_simulation_mode because the latter is deprecated in ts_salobj 5.2. I am happy to do the work if you prefer; just reassign the ticket to me.    This ticket will also update some changes from the xml update (DM-22618).",1
"DM-22555","12/10/2019 21:30:25","Update ts_salobjATHexapod to use simulation_mode instead of initial_simulation_mode","Update ts_salobjATHexapod to use simulation_mode instead of initial_simulation_mode because the latter is deprecated in ts_salobj 5.2.  ",0
"DM-22557","12/10/2019 21:37:07","Remove the setSimulationMode command from SALGenerics","Implement RFC-639 by removing the generic setSimulationMode command.    Support for the setSimulationMode command was only implemented by ts_salobj and it was removed in 5.2, with a deprecation warning.  After a suitable deprecation period remove the setSimulationMode command from SALGenerics.    At this time ts_xml 4.5.1 is current so I suggest waiting for at least ts_xml 4.7 to be released before making this change.    ts_salobj 5.2 must be also released before making this change, but I expect that to occur shortly.",0
"DM-22558","12/10/2019 21:39:48","Remove the initial_simulation_mode argument from ts_salobj","Remove the initial_simulation_mode argument from ts_salobj (constructors of BaseCsc, ConfigurableCsc and TestCsc) and delete existing unit tests that exercise it.    This argument is deprecated in ts_salobj 5.2 so I suggest not removing it until a few months after that is released.",0
"DM-22559","12/10/2019 22:53:02","Complete milestone DM-SUIT-8 - SUIT Portal integrated with workspace","Verify that the SUIT Portal application has been integrated with the WebDAV-based workspace interface.",2
"DM-22565","12/11/2019 00:31:19","Further cleanups in ts_simactuators","I missed some cleanups in ts_simactuators 0.1. Use ""position"", ""velocity"" and ""acceleration"" reliably and replace ""direction"" with ""direction_factor"".",0
"DM-22566","12/11/2019 01:29:49","Create SAL artifact yum repo on nexus server","A yum repository for SAL artifacts needs to be created on the new nexus server. The yum repo should be able to receive release (master) and develop artifacts.",1
"DM-22567","12/11/2019 14:13:41","Make GenericCamera Pip installable","Make GenericCamera package pip installable.",0
"DM-22575","12/11/2019 20:52:46","MTAlignment CSC: Mock T2SA Server Skeleton","simple tcp server that will emulate the T2SA application for testing",1
"DM-22576","12/11/2019 21:26:24","Add programmatic header fixups to astro_metadata_translator","In DM-22550 a bunch of files had the wrong filter. This could be fixed by the current scheme of per-file fixups but during that I realized that LSST_NUM is missing for the entirety of November. Providing a fixup file seems like the wrong answer to fix a month of data.    This ticket experiments with adding programmatic fixups as well as the existing scheme. The downside of programmatic fixups is that we might end up with every header having date strings parsed twice -- once to see if a date of interest is in play for header fix up, and again when the fixed header is translated.",2
"DM-22579","12/11/2019 22:27:28","Implement and Test Curtis's IOD Iterator","In testing and validating Gauss's method for IOD we found that the initial guess of the orbit could be off by as much as ~10,000 km (idealized inputs with no astrometric error). Gauss's method is fundamentally not closed-form and uses an approximation to compute a potential target's state vector.  To improve the initial orbit, some astrodynamics textbooks suggest iterating over the initial orbit (for example, using the guess of the state to compute the 'exact' equations for the Lagrange coefficients). We selected Howard Curtis' algorithm for iterative improvement and need to test if it will work as an additional component of IOD. ",2
"DM-22584","12/12/2019 01:15:04","Update developer guide to refer to “milestone” issue type","We seem to have grown a new issue type. Let's make sure people don't use it.",1
"DM-22599","12/12/2019 22:25:39","Develop PipelineTask unit test framework","Most of our new pipeline tasks' functionality can be tested by writing unit tests against {{run}} or more specific methods (in theory, these tests should be identical to those from the Gen 2 era). However, such tests do not verify:   * whether a task's {{Connections}} are correctly written and whether they match the inputs and outputs of the {{run}} method   * any logic in a custom {{runQuantum}} method   * configuration logic, such as optional or alternative inputs or outputs    Since the Gen 3 API is unfamiliar to us, these aspects of a {{PipelineTask}} are the ones that are most likely to have bugs.    Currently, the only way to test these features is in large-scale runs on Gen 3 repositories (e.g., HSC). Such tests, while valuable, can only exercise a small subset of conditions (e.g., configs), can be expensive to debug (e.g., due to cascading failures), and do not protect against regressions (no CI). A {{pytest}}-compatible framework that lets us test those parts of a {{PipelineTask}} that lie _outside_ {{run}} will let us catch problems much faster.    As part of DM-21875, I created a prototype [test framework for direct Butler I/O|https://github.com/lsst/verify/blob/b3f960befa179cee8050cdf380fd16c8a2acbafd/tests/butler_utils.py] and used it to verify that datasets could be stored to and retrieved from a dummy, obs-agnostic repository. I believe the same approach can be used to test {{PipelineTask}} functionality without the need to simulate a ""realistic"" Butler or depend on {{obs}} packages.    Desired features:   * a natural way for the test author to provide mock data IDs for the repository. The appropriate IDs will depend on the task being tested. It should be possible to simplify this from the prototype code, since most of the complexity of the Gen 3 Dimensions system is not needed for most tests; an exception may be {{ImageDifferenceTask}}'s mix of detector-level and patch-level inputs.   * a simple activator that calls {{runQuantum}} without modifications other than mocking {{run}}   * a way to test that the desired inputs get passed to {{run}}, including self-consistent use of config flags -and templates-. This will probably involve mocking {{run}} and may involve mock datasets, which are more technically challenging.   * a way to verify the output of a (real) {{run}} call against a configured connections object   * -analogous support for {{\_\_init\_\_}} inputs and outputs, which I'm less familiar with-",8
"DM-22600","12/12/2019 22:27:04","Mock T2SA, docstrings and expand set of supported commands","now we have a skeleton, flesh out the documentation a bit and add support for more of the API",2
"DM-22601","12/12/2019 22:31:09","MTAlignment Model skeleton","build a skeleton model for the alignment CSC",1
"DM-22602","12/12/2019 22:37:45","Alignment model, add support for basic commands","add support for commands to model",2
"DM-22603","12/12/2019 22:38:30","unit tests for mtalignment","unit tests for MTAlignment Model, and the mock controller underneath. ",2
"DM-22618","12/13/2019 21:46:41","MTAOS - use of InfluxDB Reserved Words","The MTAOS Telemetry XML file uses the InfluxDB reserved word DURATION, one or more times.",0
"DM-22621","12/13/2019 21:51:40","MTM1M3 - use of InfluxDB Reserved Words","The MTM1M3 Telemetry XML file uses the InfluxDB reserved word MEASUREMENT, one or more times.",1
"DM-22641","12/16/2019 18:45:54","ap_verify CI broken","Recent Jenkins runs such as [scipipe/ap_verify#438|https://ci.lsst.codes/blue/organizations/jenkins/scipipe%2Fap_verify/detail/ap_verify/438/] have been failing with the following error:  {noformat}  ap.verify.pipeline_driver.runApPipe INFO: Pipeline complete  Traceback (most recent call last):    File ""/opt/lsst/software/stack/stack/miniconda3-4.7.10-4d7b902/Linux64/ap_verify/19.0.0-4-gfb40179+1/bin/ap_verify.py"", line 29, in <module>      result = runApVerify()    File ""/opt/lsst/software/stack/stack/miniconda3-4.7.10-4d7b902/Linux64/ap_verify/19.0.0-4-gfb40179+1/python/lsst/ap/verify/ap_verify.py"", line 162, in runApVerify      computeMetrics(workspace, apPipeResults.parsedCmd.id, args)    File ""/opt/lsst/software/stack/stack/miniconda3-4.7.10-4d7b902/Linux64/ap_verify/19.0.0-4-gfb40179+1/python/lsst/ap/verify/metrics.py"", line 102, in computeMetrics      metricsFile)    File ""/opt/lsst/software/stack/stack/miniconda3-4.7.10-4d7b902/Linux64/ap_verify/19.0.0-4-gfb40179+1/python/lsst/ap/verify/metrics.py"", line 136, in _getMetricsConfig      timingConfig.load(os.path.join(lsst.utils.getPackageDir(""ap_verify""), ""config"", defaultFile))    File ""/opt/lsst/software/stack/stack/miniconda3-4.7.10-4d7b902/Linux64/pex_config/19.0.0+2/python/lsst/pex/config/config.py"", line 984, in load      self.loadFromStream(stream=code, root=root)    File ""/opt/lsst/software/stack/stack/miniconda3-4.7.10-4d7b902/Linux64/pex_config/19.0.0+2/python/lsst/pex/config/config.py"", line 1022, in loadFromStream      exec(stream, {}, local)    File ""/opt/lsst/software/stack/stack/miniconda3-4.7.10-4d7b902/Linux64/ap_verify/19.0.0-4-gfb40179+1/config/default_image_metrics.py"", line 44, in <module>      subConfig.connections.taskName = ""apPipe""    File ""/opt/lsst/software/stack/stack/miniconda3-4.7.10-4d7b902/Linux64/pex_config/19.0.0+2/python/lsst/pex/config/config.py"", line 1302, in __setattr__      raise AttributeError(""%s has no attribute %s"" % (_typeStr(self), attr))  AttributeError: lsst.pipe.base.config.Connections has no attribute taskName  {noformat}    This bug was most likely introduced by DM-21911, specifically by not testing {{ap_verify}} after [lsst/verify@e1015bc|https://github.com/lsst/verify/commit/e1015bc09cccfe6ddbaed01d077b604cddc7609b]. Update the {{ap_verify}} default configs to match.",1
"DM-22643","12/16/2019 19:56:05","convert visualizeVisit to gen3","The current gen2 visualizeVisit.py is not a valid pipelineTask, and does not conform to gen3 defaults.  Implementing this as a pipelineTask allows binned focal plane images to be appended to other pipelines.",2
"DM-22644","12/16/2019 20:58:33","Add torr unit to Astropy","CAP-396 requires torr unit support in Astropy. File a one line pull request with Astropy.",1
"DM-22647","12/16/2019 22:25:10","Bug in isrMock.getCamera's use of CameraWrapper","There's a bug in {{IsrMock}}'s use of {{CameraWrapper}}: CameraWrapper takes three kwargs, the last of which is {{isLsstLike}}, but it's only passing {{isLsstLike}} as a single arg. If we ever set that to be False, this would fail immediately. Instead, it's always forcing the plateScale to be 1.0.    See isrMock.py line 502 and compare that with {{CameraWrapper.\_\_init\_\_}}.",1
"DM-22656","12/17/2019 02:35:51","Improved Configuration service of the Replication System","The current API of the Configuration system has a number of the property *set*(-ing) methods which modify both the transient state of the configuration of a process and the persistent state of the system-wide configuration. For example:  {code:C++}  class Configuration: public ConfigurationIFace {  public:      ...      unsigned int controllerRequestTimeoutSec() const final;      void setControllerRequestTimeoutSec(unsigned int val) final;      ...  };  {code}  While this method works well for the *{{Master Replication Controller}}* it would result in unexpected (and undesired) changes in the system-wide configuration in those cases when a {{Controller}} needs just a temporary change in its transient state, thus affecting the behavior of all components of a distributed system.    Hence, a goal of this development is to extend the API with an option allowing to prevent propagating the change of the transient state down to the persistent one. The proposed API would look like:  {code:C++}  class Configuration: public ConfigurationIFace {  public:      ...      unsigned int controllerRequestTimeoutSec() const final;      void setControllerRequestTimeoutSec(unsigned int val,                                          bool updatePersistentState=true) final;      ...  };  {code}  The default value of the parameter would leave the behavior of the existing applications intact. Only those applications which are not supposed to modify the persistent state will need to be migrated to explicitly avoid updating that state.    Migrate the following controller applications to the extended interface:  {code}  qserv-replica-job-chunks  qserv-replica-job-index  qserv-replica-job-sql  {code}",3
"DM-22661","12/17/2019 17:04:08","isrTask failed to find master flat due to filter difference","I'm working on analyzing the spot data from the BOT. I have a notebook at /home/cslage/BOT/notebooks/Ingest_BOT_ITL_Spots_16Dec19.ipynb that ingests the data, creates the calibration products and then does the ISR on a spot image. Everything is working except when isrConfig.doFlat=True. Then I get an error that says:  {code}  RuntimeError: Unable to retrieve flat for \{'raftName': 'R02', 'detectorName': 'S02', 'visit': 3019103102292, 'run': '6869D', 'detector': 11, 'snap': 0}: No locations for get: datasetType:flat dataId:DataId(initialdata=\{'raftName': 'R02', 'detectorName': 'S02', 'visit': 3019103102292, 'run': '6869D', 'detector': 11, 'snap': 0}, tag=set()).  {code}  The master flat was successfully created, and is there in the CALIB directory, but it can't find it.  Apparently the problem is that the filters used for the flats include neutral density filters, which aren't used for the spots.  The filter for the flat appears to be:  SDSSi+ND_OD0.5, and the filter for the spots should be SDSSi, although it appears that it wasn't set so it returns NONE.  I tried setting isrConfig.fallbackFilterName='SDSSi+ND_OD0.5', but it still couldnt find the master flat.     ",2
"DM-22662","12/17/2019 17:14:19","Fix error in hexapod telemetry structure","The Telemetry struct in ts_hexapod has an error: the {{application_status}} should be of type {{uint32}} but is defined as {{uint16}}.",0
"DM-22673","12/17/2019 20:39:24","Format ts_m1m3support code to DM guidelines","The code should be formatted in agreement with https://developer.lsst.io/cpp/style.html, and therefore the Google Style Guide (https://google.github.io/styleguide/cppguide.html). This will make it easier to work with this codebase.",2
"DM-22676","12/17/2019 20:46:55","Create gaia dr2 refcat for ap_verify_hits2015","I'm using {{ap_verify_ci_hits2015}} to test my gen2->gen3 repo converter code, and I ran into a problem due to the use of the gaia dr1 refcat, which uses htm depth=8. Gen3 repos all have to use depth=7 for the moment. The easiest solution is to just extract the necessary pixels from the new gaia dr2 refcat and replace the old one in ap_verify_hits. That also gains us an upgraded reference catalog.    This should be an easy change to {{scripts/gaia_HiTS_2015.py}} and then a bit of processing on lsst-dev.",2
"DM-22677","12/17/2019 21:52:06","Modernize python scripts in daf_butler","The three python scripts in daf_butler use the old style approach of parsing arguments directly.  This is now deprecated and we should instead move all the code to the python module and have one line in the scripts themselves.    This allows for:    1. Unit testing of more of the code  2. Inclusion of scripts in sphinx documentation.",1
"DM-22688","12/18/2019 17:15:26","CLONE - MTM1M3 - use of InfluxDB Reserved Words","The MTM1M3 Telemetry XML file uses the InfluxDB reserved word MEASUREMENT, one or more times.",1
"DM-22700","12/18/2019 21:45:43","Include command to add offsets to the lookup tables in ATAOS. ","One of the things we figured we may want to do with the ATAOS is to be able to apply offsets on the current look up tables. This task is to include a command on ATAOS to enable that capability.",1
"DM-22702","12/18/2019 21:47:42","Deploy T&S Components at NCSA.","Task to finalize deployment of T&S components at NCSA",1
"DM-22704","12/18/2019 22:06:11","filter 'diffuser' cannot be read","For the gen3 middleware demo, I needed to add     {code:python}  if self.observationInfo.physical_filter == 'diffuser':                                         lsst.afw.image.utils.defineFilter(""diffuser"", 555.0)  {code}  to lsst.obs.base.fitsRawFormatterBase.FitsRawFormatterBase.makeFilter() to make reading raw data work.",1
"DM-22708","12/18/2019 23:36:31","Fix decam gen3 ingest","It turns out the work I did on decam for DM-20763 was not complete: I only tested that one dataId could be retrieved from the ingested raw data, but there are two ccds in that file in two different HDUs and the second one is not actually getting ingested. The gen2 CameraMapper path for a decam raw looks like {{decam%(visit)07d.fits.fz[%(hdu)d]}}, so the hdu number is baked in there.  As far as I can tell, gen3 doesn't have any way to encode hdu number. Maybe this has to be a further specialization in the RawFormatter. File paths that go into butler.ingest have to actually exist, so we will have to do something to the raw paths that we pass into `butler.ingest`.    The gen2 butler registry has an `hdu` field that encodes which hdu to get that raw from. It doesn't look like there's an equivalent `hdu` field anywhere in the gen3 registry: could we add an extra `hdu` field to the `posix_datastore_records` table? That's my best guess as to where it should live.",8
"DM-22714","12/19/2019 17:25:07","Fix warnings.warning","Adopt improvements from DM-22627.",0
"DM-22726","12/19/2019 22:48:56","Please allow empty topics in our SAL XML files","Please allow our SAL XML files to define topics with no fields.    This is much cleaner than defining a field whose sole purpose is to be ignored. Furthermore [~tjohnson] has a use case for ignoring such fields and it takes specially coding to handle it. If the field was simply absent he could presumably drop that code.",1
"DM-22727","12/19/2019 22:57:39","Add numpy warnings catch to DiaCalculationPlugins","Some DiaCalculation plugins throw numpy warnings while calculating results that are unnecessary. But such clauses in a context manager to catch and ignore these warnings.",2
"DM-22729","12/20/2019 01:47:20","Monthly December ap_pipe rerun","This is the ~monthly ap_pipe rerun of the HiTS 2015 dataset (all 3 fields). For the first time, I will use templates with the DECam instrumental pixel scale (0.26""/pixel). The rerun will be in {{/project/mrawls/hits2015/rerun/cw_2019_12}} and an accompanying ""quick look"" analysis notebook will be made.",8
"DM-22731","12/20/2019 06:15:48","Redeployment of summit components","After the last run with the AT there where some bug fixes and configuration changes needed. Most of the work was done on DM-22531. This task is to clean up all the components that where deployed with non-tagged repositories and also to switch up the kafka producers to publish data to the new EFD deployment. ",1
"DM-22747","12/23/2019 16:45:02","Support the Log File for MTAOS","The MTAOS does not support the logging file now. I realized this should be helpful to debug in the test with multiple components.",2
"DM-22748","12/23/2019 16:46:35","Restrict the Command in Specific State for MTAOS","Need to restrict the command to be only available on the specific state. For example, some commands are only allowed in Enabled state.",1
"DM-22764","12/24/2019 22:00:26","Watcher event timestamps must be double, not float","The Watcher event's timestamps are float, which does not have enough precision to do the job. Use double instead.",0
"DM-22766","12/24/2019 23:45:08","Add an unacknowledge command to the Watcher","Add an ""unacknowledge"" command to the Watcher CSC.    This is easy because the Model already supports it.    LOVE will have to be updated in order to actually use it, but there's no rush on that.",1
"DM-22770","12/29/2019 17:57:56","Remove duplication of BaseMapper","pipe_tasks' {{tests/test_read_CuratedCalibs.py}} contains a copy of {{BaseMapper}} from obs_base's {{tests/test_cameraMapper.py}}. Not only is that ugly, it's also fragile (see DM-22769).    Rearrange things so that pipe_tasks can import {{BaseMapper}} from obs_base directly.",1
"DM-22773","12/31/2019 15:09:28","Review the Document of Simulink Model by Moog","Review the document of Simulink to have the understanding of model for the debug of rotator track issue.",1
"DM-22774","12/31/2019 19:53:47","Store APDB in ap_verify Jenkins artifacts","The {{ap_verify}} CI job creates SQLite databases in {{<dataset-dir>/association.db}}. These databases contain valuable debugging information that can't be represented by metrics (e.g., individual source IDs and locations). It would be useful if these database files were stored for later analysis.    [~krughoff] pointed to [ap_verify.groovy|https://github.com/lsst-dm/jenkins-dm-jobs/blob/master/pipelines/scipipe/ap_verify.groovy#L391-L394] as the code that needs changing.",1
"DM-22779","01/03/2020 15:28:31","Organize and Clean the Code of Rotator GUI","Organize and clean the code of rotator GUI. The code delivered by Moog contains everything (middleware, hexapod and rotator GUIs, simulink model, and hexapod and rotator wrappers in PXI) and hard to update the code. This task will separate the rotator GUI code to a single repository and organize the VIs in the project. This will help for the future's update.",1
"DM-22780","01/03/2020 19:07:48","ts_sal IDL generation mis-handles topics whose names differ only in case","I modified Rotator_Telemetry.xml to have both the old {{Application}} topic, with fields names that started with uppercase, and a new {{application}} topic, whose field names start with lowercase:  {code}<SALTelemetry>      <Subsystem>Rotator</Subsystem>      <Version>3.5.1</Version>      <Author></Author>      <EFDB_Topic>Rotator_application</EFDB_Topic>      <Description>Commanded and actual rotator position.</Description>      <item>          <EFDB_Name>demand</EFDB_Name>          <Description>Commanded rotator position.</Description>          <IDL_Type>double</IDL_Type>          <Units>deg</Units>          <Count>1</Count>      </item>      <item>          <EFDB_Name>position</EFDB_Name>          <Description>Actual rotator position.</Description>          <IDL_Type>double</IDL_Type>          <Units>deg</Units>          <Count>1</Count>      </item>      <item>          <EFDB_Name>error</EFDB_Name>          <Description>Rotator following error.</Description>          <IDL_Type>double</IDL_Type>          <Units>deg</Units>          <Count>1</Count>      </item>  </SALTelemetry>  <SALTelemetry>      <Subsystem>Rotator</Subsystem>      <Version>3.5.1</Version>      <Author></Author>      <EFDB_Topic>Rotator_Application</EFDB_Topic>      <Description>Deprecated version of application</Description>      <item>          <EFDB_Name>Demand</EFDB_Name>          <Description>Commanded rotator position.</Description>          <IDL_Type>double</IDL_Type>          <Units>deg</Units>          <Count>1</Count>      </item>      <item>          <EFDB_Name>Position</EFDB_Name>          <Description>Actual rotator position.</Description>          <IDL_Type>double</IDL_Type>          <Units>deg</Units>          <Count>1</Count>      </item>      <item>          <EFDB_Name>Error</EFDB_Name>          <Description>Rotator following error.</Description>          <IDL_Type>double</IDL_Type>          <Units>deg</Units>          <Count>1</Count>      </item>  </SALTelemetry>  {code}    The resulting IDL file combined these two topics into a single topic:  {code}  struct application_c894253b {  // @Metadata=(Description=""Commanded and actual rotator position."")     string<8> private_revCode; //private // @Metadata=(Units=""unitless"",Description=""Revision code of topic"")     double private_sndStamp; //private // @Metadata=(Units=""second"",Description=""TAI at sender"")     double private_rcvStamp; //private // @Metadata=(Units=""second"",Description=""TAI at receiver"")     long  private_seqNum; //private // @Metadata=(Units=""unitless"",Description=""Sequence number"")     long  private_origin; //private // @Metadata=(Units=""unitless"",Description=""PID code of sender"")     long  private_host; //private // @Metadata=(Units=""unitless"",Description=""IP of sender"")     double Demand;     double Position;     double Error;   };  #pragma keylist application_c894253b   {code}  which, strangely, has the field names from one and the topic name from the other, thus matching neither.    I realize that having two topics that differ by case is rare, and I'm not convinced we should allow it at all. But if we prohibit it, then could you please reject the XML? Or if you think we should support it, then please fix the IDL generator.",1
"DM-22782","01/05/2020 19:54:27","Organize and Clean the Code of Hexapod GUI","Organize and clean the code of hexapod GUI. The code delivered by Moog contains everything (middleware, hexapod and rotator GUIs, simulink model, and hexapod and rotator wrappers in PXI) and hard to update the code. This task will separate the hexapod GUI code to a single repository and organize the VIs in the project. This will help for the future's update.",1
"DM-22783","01/05/2020 19:56:44","Fix the M2 Control Code for SAL Command Phase 1","Fix the M2 control code to be able to feedback the SAL command. It looks like the M2 control code will reject all received SAL command. Need to figure out the reason. This is the phase 1.",3
"DM-22784","01/05/2020 20:02:10","Take the Onramp Course of Simulink on MathWork Website","Take the onramp Simulink course on MathWork website to have the initial understanding of the use of Simulink. This should be helpful to review the model by Moog for the control algorithm. ",0
"DM-22788","01/06/2020 19:51:46","Responses tests fail with modern responses and old requests","The test_squash.py is failing for me locally because I have responses installed on my stack but this does not play well with the old EUPS requests package. If I unsetup requests and let the code use my conda requests the tests pass without a problem.    I think [~gpdf] has been worrying about our old {{requests}} package for a while now. Since {{requests}} is formally listed in the conda baseline and since no other package in lsst_distrib uses requests,  I think the easiest fix is to remove requests from the table file for verify.",1
"DM-22789","01/06/2020 19:55:51","End-to-End Build/Deploy/Test","Clean up and update the SAL/XML Jenkins jobs and processes for a full build/test/deploy process.    for more details, please see [https://confluence.lsstcorp.org/pages/viewpage.action?pageId=120783026]     ",40
"DM-22790","01/06/2020 20:06:45","pex_config FutureWarning reports wrong line number","The {{warnings.warn}} call in pex_config that warns about a deprecated config item, reports a line number inside pex_config when it should be reporting a line number in user code. Add stacklevel=2 to the call.",0.5
"DM-22791","01/06/2020 20:07:08","Update Jenkins jobs to point to the Nexus3 production server","Various Jenkins jobs were pushing to the temporary Nexus3 repo.  Now that the production server, [https://repo-nexus.lsst.org/nexus/], has been created, update the jobs to use this server.    This task covers the work to update the Jobs and run them to ensure they work.",1
"DM-22792","01/06/2020 20:22:47","Define the AuxTel CSC build and packaging process","I need to meet with Tiago and discuss exactly the process for building and packagin the AT CSCs.   * What is pulled from Nexus   * Build steps   * Unit testing   * Packaging details (format)",2
"DM-22794","01/06/2020 20:37:52","obs_base tests should not use daf_butler test configs","[~swinbank] has noted that obs_base tests depend on a private butler config test file in DAF_BUTLER_DIR/tests/config.  Those configs are not meant to be used outside of daf_butler since they contain unexpected formatters and storage classes designed specifically for internal daf_butler tests.  obs_base should not be relying on those configurations.  It should be sufficient for obs_base to use a default configuration made by {{Butler.makeRepo}}.    I'm not entirely sure whether I should fix this or whether this should be handled by science pipelines.",1
"DM-22796","01/06/2020 21:17:36","pipe_tasks installs 200MB of temporary test output","When pipe_tasks runs its tests it generates a single temporary output directory containing 200MB of files. These are installed. They serve no useful purpose in the installation.    I propose that we clean up the temp directory when the tests complete successfully and leave it around if the tests fail (much like we do with lsst_ci; see DM-22305).",1
"DM-22798","01/07/2020 15:18:37","Remove (unused) ability for fgcmcal to run on a full repo without specifying any ids","In the course of finishing DM-20163, I realized that the functionality in {{fgcmcal}} to run on a full repo without specifying any {{--id}} on the command line was only being used by the test code, and not in general use (or by the cookbook).  This removal of functionality and clarification of the cookbook and documentation has been split out from DM-20163 since it is logically distinct.  This update will also make it easier to convert {{fgcmcal}} to a {{PipelineTask}}.",1
"DM-22803","01/07/2020 20:30:02","validate_drp and numpy1.17/astropy4/matplotlib3.1 fails","I'm testing the science pipelines with numpy1.17/astropy4/matplotlib3.1 and validate_drp fails:    {code}  ======================================================================  ERROR: testPlotMetricsFromJsonJob (__main__.ParseJsonJob)  Does plotting the metrics run and produce the correct filenames?  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/matplotlib/axis.py"", line 1550, in convert_units      ret = self.converter.convert(x, self.units, self)    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/astropy/visualization/units.py"", line 101, in convert      elif isinstance(val, list) and isinstance(val[0], u.Quantity):  IndexError: list index out of range    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""tests/test_load_json.py"", line 93, in testPlotMetricsFromJsonJob      plot_metrics(job, filterName)    File ""/Users/timj/work/lsst/tmp/lsstsw/build/validate_drp/python/lsst/validate/drp/validate.py"", line 455, in plot_metrics      outputPrefix=outputPrefix)    File ""/Users/timj/work/lsst/tmp/lsstsw/build/validate_drp/python/lsst/validate/drp/plot.py"", line 314, in plotPhotometryErrorModel      histtype='stepfilled', orientation='horizontal')    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1601, in inner      return func(ax, *map(sanitize_sequence, args), **kwargs)    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 6694, in hist      x = [self.convert_xunits(xi) for xi in x]    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 6694, in <listcomp>      x = [self.convert_xunits(xi) for xi in x]    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/matplotlib/artist.py"", line 180, in convert_xunits      return ax.xaxis.convert_units(x)    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/matplotlib/axis.py"", line 1553, in convert_units      f'units: {x!r}') from e  matplotlib.units.ConversionError: Failed to convert value(s) to axis units: []  {code}    The root cause seems to be that in {{plotPhotometryErrorModel}} the {{mmagRmsHighSnr}} variable is empty because there are no high signal to noise measurements.  For the current astropy/numpy/matplotlib this triggers:    {code}  /Users/timj/work/lsstsw3/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.    out=out, **kwargs)  /Users/timj/work/lsstsw3/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars    ret = ret.dtype.type(ret / rcount)  {code}  but continues regardless. With newer versions it fails badly in Astropy with an index lookup gone bad.  This is arguably a bug in Astropy that should be a bit more defensive, but it also seems to me that we shouldn't be asking matplotlib to calculate a histogram with no data in it.    The quick fix would seem to me to be to not try to plot the histogram if there are no data. Is that acceptable?  Are we expecting there to be any numbers from the bright mask?    This does make the tests pass:  {code:diff}  diff --git a/python/lsst/validate/drp/plot.py b/python/lsst/validate/drp/plot.py  index 36a3cff..3ab6c58 100644  --- a/python/lsst/validate/drp/plot.py  +++ b/python/lsst/validate/drp/plot.py  @@ -118,8 +118,9 @@ def plotAstrometryErrorModel(dataset, astromModel, outputPrefix=''):          ax[0].hist(dist, bins=100, color=color['all'],                  histtype='stepfilled', orientation='horizontal')  -    ax[0].hist(dist[bright], bins=100, color=color['bright'],  -               histtype='stepfilled', orientation='horizontal')  +    if len(dist[bright]):  +        ax[0].hist(dist[bright], bins=100, color=color['bright'],  +                   histtype='stepfilled', orientation='horizontal')          ax[0].set_ylim([0., 500.])       ax[0].set_ylabel(""Distance [{unit:latex}]"".format(unit=dist.unit))  @@ -307,10 +308,11 @@ def plotPhotometryErrorModel(dataset, photomModel,       ax[0][0].hist(mmagRms,                     bins=100, range=(0, 500), color=color['all'],                     histtype='stepfilled', orientation='horizontal')  -    ax[0][0].hist(mmagRmsHighSnr,  -                  bins=100, range=(0, 500),  -                  color=color['bright'],  -                  histtype='stepfilled', orientation='horizontal')  +    if len(mmagRmsHighSnr):  +        ax[0][0].hist(mmagRmsHighSnr,  +                      bins=100, range=(0, 500),  +                      color=color['bright'],  +                      histtype='stepfilled', orientation='horizontal')       plotOutlinedAxline(           ax[0][0].axhline,           mmagrms_median.value,  {code}",1
"DM-22810","01/07/2020 23:52:51","Run bokeh app from within nublado","Since JupyterLab proxies ports out of the container, it should be possible to access bokeh apps running from within a JL container.",2
"DM-22818","01/08/2020 19:17:00","Matplotlib 3.1 bug triggered by fgcmcal on macOS","I'm testing an updated conda environment. fgcmcal didn't work because it got into an infinite cycle of C++ exceptions.    {code}  fgcmCalibrateTract INFO: Flagging 0 of 472 stars with TOO_FEW_OBS  libc++abi.dylib: terminating with uncaught exception of type std::runtime_error: Couldn't close file  Caught signal 6, backtrace follows:  0   libutils.dylib                      0x000000010e657c9f lsst::utils::(anonymous namespace)::signalHandler(int) + 79  1   libsystem_platform.dylib            0x00007fff6f1b942d (null) + 29  1   libsystem_platform.dylib            0x00007fff6f1b942d _sigtramp + 29  2   ???                                 0x075ce2d000000400 0x0 + 530548239103951872  3   libsystem_c.dylib                   0x00007fff6f08ea1c abort + 120  4   libc++abi.1.0.dylib                 0x000000010e5e9fa3 abort_message + 195  5   libc++abi.1.0.dylib                 0x000000010e5b4bcd default_terminate_handler() + 237  6   libc++abi.1.0.dylib                 0x000000010e5e8ad8 std::__terminate(void (*)()) + 8  7   libc++abi.1.0.dylib                 0x000000010e5ebbe9 (null) + 121  7   libc++abi.1.0.dylib                 0x000000010e5ebbe9 __cxa_throw + 121  8   ft2font.cpython-37m-darwin.so       0x000000010e3e2ced close_file_callback(FT_StreamRec_*) + 573  9   libfreetype.6.dylib                 0x000000010e3fb846 destroy_face + 582  10  libfreetype.6.dylib                 0x000000010e3fb57a FT_Done_Face + 170  11  ft2font.cpython-37m-darwin.so       0x000000010e3da8ce FT2Font::~FT2Font() + 94  12  ft2font.cpython-37m-darwin.so       0x000000010e3da84f FT2Font::~FT2Font() + 15  13  ft2font.cpython-37m-darwin.so       0x000000010e3e2218 PyFT2Font_dealloc(PyFT2Font*) + 24  14  python                              0x0000000103b72d43 lru_list_elem_dealloc + 51  15  python                              0x0000000103b72e19 lru_cache_cache_clear + 105  16  python                              0x00000001039757b7 (null) + 135  16  python                              0x00000001039757b7 _PyMethodDef_RawFastCallDict + 135  17  python                              0x000000010397506f (null) + 111  17  python                              0x000000010397506f _PyObject_FastCallDict + 111  18  python                              0x0000000103b39707 run_at_forkers + 87  19  python                              0x0000000103b42985 os_fork + 293  20  python                              0x00000001039766f3 (null) + 131  20  python                              0x00000001039766f3 _PyMethodDef_RawFastCallKeywords + 131  21  python                              0x0000000103ab2c82 call_function + 306  22  python                              0x0000000103aafa36 (null) + 42246  22  python                              0x0000000103aafa36 _PyEval_EvalFrameDefault + 42246  23  python                              0x0000000103975ef5 function_code_fastcall + 117  24  python                              0x0000000103ab2c07 call_function + 183  25  python                              0x0000000103aaf9a0 (null) + 42096  25  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  26  python                              0x0000000103975ef5 function_code_fastcall + 117  27  python                              0x00000001039f7591 slot_tp_init + 193  28  python                              0x0000000103a01691 type_call + 241  29  python                              0x0000000103976113 (null) + 179  29  python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  30  python                              0x0000000103ab2d15 call_function + 453  31  python                              0x0000000103ab0978 (null) + 46152  31  python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  32  python                              0x0000000103975ef5 function_code_fastcall + 117  33  python                              0x0000000103ab2c07 call_function + 183  34  python                              0x0000000103aafa36 (null) + 42246  34  python                              0x0000000103aafa36 _PyEval_EvalFrameDefault + 42246  35  python                              0x0000000103975ef5 function_code_fastcall + 117  36  python                              0x0000000103ab2c07 call_function + 183  37  python                              0x0000000103aaf9a0 (null) + 42096  37  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  38  python                              0x0000000103975ef5 function_code_fastcall + 117  39  python                              0x0000000103ab2c07 call_function + 183  40  python                              0x0000000103aaf9a0 (null) + 42096  40  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  41  python                              0x0000000103aa42ee (null) + 414  41  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  42  python                              0x00000001039753e7 (null) + 231  42  python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  43  python                              0x00000001039f7591 slot_tp_init + 193  44  python                              0x0000000103a01691 type_call + 241  45  python                              0x0000000103976113 (null) + 179  45  python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  46  python                              0x0000000103ab2d15 call_function + 453  47  python                              0x0000000103ab0a31 (null) + 46337  47  python                              0x0000000103ab0a31 _PyEval_EvalFrameDefault + 46337  48  python                              0x0000000103aa42ee (null) + 414  48  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  49  python                              0x0000000103976623 (null) + 195  49  python                              0x0000000103976623 _PyFunction_FastCallKeywords + 195  50  python                              0x0000000103ab2c07 call_function + 183  51  python                              0x0000000103ab0a31 (null) + 46337  51  python                              0x0000000103ab0a31 _PyEval_EvalFrameDefault + 46337  52  python                              0x0000000103aa42ee (null) + 414  52  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  53  python                              0x0000000103976623 (null) + 195  53  python                              0x0000000103976623 _PyFunction_FastCallKeywords + 195  54  python                              0x0000000103ab2c07 call_function + 183  55  python                              0x0000000103ab0a31 (null) + 46337  55  python                              0x0000000103ab0a31 _PyEval_EvalFrameDefault + 46337  56  python                              0x0000000103975ef5 function_code_fastcall + 117  57  python                              0x0000000103ab2c07 call_function + 183  58  python                              0x0000000103aaf9a0 (null) + 42096  58  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  59  python                              0x0000000103975ef5 function_code_fastcall + 117  60  python                              0x0000000103ab0ba5 (null) + 46709  60  python                              0x0000000103ab0ba5 _PyEval_EvalFrameDefault + 46709  61  python                              0x0000000103aa42ee (null) + 414  61  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  62  python                              0x0000000103976623 (null) + 195  62  python                              0x0000000103976623 _PyFunction_FastCallKeywords + 195  63  python                              0x0000000103ab2c07 call_function + 183  64  python                              0x0000000103aaf9a0 (null) + 42096  64  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  65  python                              0x0000000103975ef5 function_code_fastcall + 117  66  python                              0x00000001039f567d slot_tp_call + 189  67  python                              0x0000000103976113 (null) + 179  67  python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  68  python                              0x0000000103ab2d15 call_function + 453  69  python                              0x0000000103ab0978 (null) + 46152  69  python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  70  python                              0x0000000103975ef5 function_code_fastcall + 117  71  python                              0x0000000103ab2c07 call_function + 183  72  python                              0x0000000103aaf9a0 (null) + 42096  72  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  73  python                              0x0000000103aa42ee (null) + 414  73  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  74  python                              0x0000000103976623 (null) + 195  74  python                              0x0000000103976623 _PyFunction_FastCallKeywords + 195  75  python                              0x0000000103ab2c07 call_function + 183  76  python                              0x0000000103ab0a31 (null) + 46337  76  python                              0x0000000103ab0a31 _PyEval_EvalFrameDefault + 46337  77  python                              0x0000000103975ef5 function_code_fastcall + 117  78  python                              0x0000000103ab2c07 call_function + 183  79  python                              0x0000000103aaf9a0 (null) + 42096  79  python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  80  python                              0x0000000103975ef5 function_code_fastcall + 117  81  python                              0x0000000103ab2c07 call_function + 183  82  python                              0x0000000103ab0978 (null) + 46152  82  python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  83  python                              0x0000000103aa42ee (null) + 414  83  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  84  python                              0x00000001039753e7 (null) + 231  84  python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  85  python                              0x0000000103979312 method_call + 130  86  python                              0x0000000103976d92 PyObject_Call + 130  87  python                              0x0000000103ab0ba5 (null) + 46709  87  python                              0x0000000103ab0ba5 _PyEval_EvalFrameDefault + 46709  88  python                              0x0000000103aa42ee (null) + 414  88  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  89  python                              0x00000001039753e7 (null) + 231  89  python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  90  python                              0x00000001039f567d slot_tp_call + 189  91  python                              0x0000000103976113 (null) + 179  91  python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  92  python                              0x0000000103ab2d15 call_function + 453  93  python                              0x0000000103ab0978 (null) + 46152  93  python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  94  python                              0x0000000103aa42ee (null) + 414  94  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  95  python                              0x00000001039753e7 (null) + 231  95  python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  96  python                              0x0000000103979312 method_call + 130  97  python                              0x0000000103976d92 PyObject_Call + 130  98  python                              0x0000000103ab0ba5 (null) + 46709  98  python                              0x0000000103ab0ba5 _PyEval_EvalFrameDefault + 46709  99  python                              0x0000000103aa42ee (null) + 414  99  python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  100 python                              0x00000001039753e7 (null) + 231  100 python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  101 python                              0x00000001039f567d slot_tp_call + 189  102 python                              0x0000000103976113 (null) + 179  102 python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  103 python                              0x0000000103ab2d15 call_function + 453  104 python                              0x0000000103ab0978 (null) + 46152  104 python                              0x0000000103ab0978 _PyEval_EvalFrameDefault + 46152  105 python                              0x0000000103975ef5 function_code_fastcall + 117  106 python                              0x0000000103ab2c07 call_function + 183  107 python                              0x0000000103aaf9a0 (null) + 42096  107 python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  108 python                              0x0000000103975ef5 function_code_fastcall + 117  109 python                              0x0000000103ab2c07 call_function + 183  110 python                              0x0000000103aaf9a0 (null) + 42096  110 python                              0x0000000103aaf9a0 _PyEval_EvalFrameDefault + 42096  111 python                              0x0000000103aa42ee (null) + 414  111 python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  112 python                              0x00000001039753e7 (null) + 231  112 python                              0x00000001039753e7 _PyFunction_FastCallDict + 231  113 python                              0x00000001039f7591 slot_tp_init + 193  114 python                              0x0000000103a01691 type_call + 241  115 python                              0x0000000103976113 (null) + 179  115 python                              0x0000000103976113 _PyObject_FastCallKeywords + 179  116 python                              0x0000000103ab2d15 call_function + 453  117 python                              0x0000000103aafa36 (null) + 42246  117 python                              0x0000000103aafa36 _PyEval_EvalFrameDefault + 42246  118 python                              0x0000000103aa42ee (null) + 414  118 python                              0x0000000103aa42ee _PyEval_EvalCodeWithName + 414  119 python                              0x0000000103b07b30 PyRun_FileExFlags + 256  120 python                              0x0000000103b06fa7 PyRun_SimpleFileExFlags + 391  121 python                              0x0000000103b34c03 pymain_main + 9635  122 python                              0x0000000103948b4d main + 125  123 libdyld.dylib                       0x00007fff6efc07fd start + 1  124 ???                                 0x0000000000000002 0x0 + 2  {code}  This was when running {{tests/test_fgcmcal_hsc.py}}.",1
"DM-22819","01/08/2020 20:00:37","Add logLevel event to LOVE XML","The LOVE XML needs a logLevel event in order for INRIA to create a LOVE Controller.",0
"DM-22821","01/08/2020 20:24:14","Cannot create a Controller for a component with no commands","It turns out not to be possible to create a salobj Controller for a SAL component that has no commands. salobj complains that there is no ackcmd topic (and indeed there is not).    LOVE is the component that showed this problem.",1
"DM-22824","01/08/2020 21:24:39","Add QE, gain, read noise to as built camera.","This requires two things.      First, moving QE curves measured in the ts8 to the appropriate location for the as installed rafts.    Second, updating the camera geometry files with the as built electronic properties.    This issue will implement this for one raft and document the process.  Then, it should then be repeated for the other rafts as they become available.",2
"DM-22851","01/08/2020 23:14:20","Remove extraneous line breaks in pipeline --show=config output","The output from {{pipeline ... --show=config}} occasionally can contain some extra newlines in it, here are examples.    This is an expected output:  {noformat}  $ pipetask build -t lsst.ctrl.mpexec.examples.calexpToCoaddTask.CalexpToCoaddTask --show=config  ### Configuration for task `CalexpToCoaddTask'  import lsst.ctrl.mpexec.examples.calexpToCoaddTask  assert type(config)==lsst.ctrl.mpexec.examples.calexpToCoaddTask.CalexpToCoaddTaskConfig, 'config is of type %s.%s instead of lsst.ctrl.mpexec.examples.calexpToCoaddTask.CalexpToCoaddTaskConfig' % (type(config).__module__, type(config).__name__)  import lsst.pipe.base.config  # Flag to enable/disable metadata saving for a task, enabled by default.  config.saveMetadata=True    # name for connection calexp  config.connections.calexp='calexp'    # name for connection coadd  config.connections.coadd='deepCoadd_calexp'    {noformat}    and this one shows those extra line breaks:  {noformat}  $ pipetask build -t lsst.ctrl.mpexec.examples.calexpToCoaddTask.CalexpToCoaddTask --show=config='*'  ### Configuration for task `CalexpToCoaddTask'    import lsst.ctrl.mpexec.examples.calexpToCoaddTask    assert type(config)==lsst.ctrl.mpexec.examples.calexpToCoaddTask.CalexpToCoaddTaskConfig, 'config is of type %s.%s    instead of lsst.ctrl.mpexec.examples.calexpToCoaddTask.CalexpToCoaddTaskConfig' % (type(config).__module__, type(config).__name__)    import lsst.pipe.base.config    # Flag to enable/disable metadata saving for a task, enabled by default.  config.saveMetadata=True    # name for connection calexp  config.connections.calexp='calexp'    # name for connection coadd  config.connections.coadd='deepCoadd_calexp'  {noformat}    Difference between two commands is a filter added to --show=config option, I guess there is a code that is broken when tat filtering option is used.",1
"DM-22955","01/09/2020 17:50:04","Add spatially varying HSC NB filters to obs_subaru","We recently received spatially varying filter scans for the HSC NB filters.  These will be necessary to run {{fgcmcal}} on HSC NB filters.  The raw files will be formatted to put into {{obs_subaru}} {{filterTraces.py}}",1
"DM-23008","01/10/2020 17:15:33","Add DCR model subfilters to Gen3 registry","Follow the procedure that was done for registering the SkyMap, and do the same thing for subfilters. Need to figure out exactly that procedure is!",5
"DM-23009","01/10/2020 17:58:56","Update ATHeaderService for changes on ts_xml 4.6.0","The new ts_xml 4.6.0 changed the {{ATSpectrograph_logevent_settingsApplied}} to {{ATSpectrograph_logevent_settingsAppliedValues}} and therefore the ATHeaderService configuration and simulator need to be updated.",2
"DM-23023","01/10/2020 21:00:22","Simplify linearity corrections","The default linearityType is PROPORTIONAL, which is not implemented anywhere.  This should be fixed, so that detectors that do not require linearity correction have a clear NOOP.    A related issue is to allow the linearity defined by the detector to be overridden.  This would make testing new corrections easier.",8
"DM-23025","01/10/2020 21:23:16","Clarify prompt processing-related definitions in the DM glossary","The DM glossary has (what I think is) a slightly confusing discussion of the relationship between Prompt Processing, Alert Production and Solar System Processing. Please clarify it.",1
"DM-23034","01/13/2020 01:30:06","Update cbp for changes in how Cameras and Detectors are built","Please update the cbp package for change in how Detectors and Cameras are built.    I suggest you use the tickets/DM-20488 branch or base your branch off of that one. Or, if you prefer, cherry-pick those changes.    I think the main class that needs updating is SampleCoordinateConverter in testUtils.py.  I had a try at this myself (see attached file), and was able to make a list of Detectors and get rid of the no-longer-needed AmpInfoTable (the test code doesn't care about amplifiers), but could not figure out how to create a Camera (nor verify that the Detectors I made were correct).",2
"DM-23036","01/13/2020 17:09:13","Add ability for fgcmcal to do calibrations on local background-corrected fluxes","Investigation of calibration errors has shown that improvements can be made by performing local background subtraction on the aperture fluxes.  These will bring the faint stars more in line with the bright stars, and should only improve calibrations (decreasing bias and scatter for bright stars), and this ticket does not imply that these corrections should necessarily be made outside of calibration, or to fluxes other than aperture fluxes used for calibration.",2
"DM-23044","01/13/2020 21:10:05","PTC task should persist usable linearity models","Store the output of DM-21221 in some meaningful form using the Butler, and demonstrate that it can be used in downstream processing.",2
"DM-23056","01/14/2020 02:15:20","Suppress FutureWarnings from LSST code","Since DM-17566 landed, we're generating a bunch of FutureWarnings from inside DM code. Please suppress them.",2
"DM-23057","01/14/2020 02:33:11","Fix the M2 Cell System in Phase 1","Fix the bugs found in the M2 cell system. This task is the phase 1.    Founded bugs:   # The log file can not be generated after rebooting the cRIO. (Done)   # The state can not transit after rebooting the cRIO.   # TCP/IP connection is easy to break. (Partially done.)",3
"DM-23058","01/14/2020 14:00:19","HVAC XML to MQTT","Create XML SAL bindings for the MQTT telemetry subset.",2
"DM-23069","01/14/2020 20:49:28","Monthly January ap_pipe rerun","This is the ~monthly ap_pipe rerun of the HiTS 2015 dataset (all 3 fields). This time we should see if setting {{nImagesMax}} in the best seeing selector will work with the new slurm template generation. To be clear, this rerun should include generating new templates as well as running ap_pipe.    Optional bonus level and/or spinoff ticket: adopt the templates generated as part of this rerun as the new default in the ap_verify datasets.",8
"DM-23077","01/15/2020 17:10:40","Update default interpolation for Curve classes","Currently, the behavior is to throw an exception if interpolation outside the curve boundaries is attempted.  It seems like a better default, would be to clamp to zero outside the boundary.  I can imagine this would not be the case for all possible curves, so I will try to make this configurable.",2
"DM-23079","01/15/2020 21:49:34","Move opaque table Registry code into helper classes","This implements the OpaqueTableRecords and OpaqueTableManager classes described in the [prototype document|https://confluence.lsstcorp.org/display/DM/Architectural+Prototype+for+the+New+Gen3+Registry].",1
"DM-23080","01/15/2020 21:52:56","Move dimension Registry code into helper classes","This implements the DimensionTableRecords and DimensionTableManager classes described in the [prototype document|https://confluence.lsstcorp.org/display/DM/Architectural+Prototype+for+the+New+Gen3+Registry].",2
"DM-23082","01/15/2020 22:54:27","Document EfdClient classes and methods","Implement the docstrings for the {{lsst-efd-client}} classes and functions.",1
"DM-23084","01/15/2020 22:56:24","Fix Qserv container builds","Qerve container build scripts are busted downstream of pytest no-longer being published via eups distrib.  Remove explicit eups distrib install from scripts, since pytest is now made available in the build environment via conda.",0.5
"DM-23085","01/15/2020 23:11:46","Deprecate SourceDetectionTask.makeSourceCatalog","Per [discussion at the DM-CCB|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=125240021], {{SourceDetectionTask.makeSourceCatalog}} is to be removed in Pipelines release 21.0.0. Please add the appropriate deprecation notices to release 20.0.0.",1
"DM-23087","01/16/2020 01:39:06","Batch mode for allocating chunks during catalog ingests in Qserv","The current implementation of the Ingest system's {{REST}} service only allows single chunk allocations. This mode of operation has a noticeable overhead which translates into significant delays when ingesting large numbers (up to a few hours for catalogs comprising 150,000) of chunks . Each such chunk allocation requires executing a number of the relatively expensive queries against the persistent state of the Replication system.     Hence, a goal of this effort is to introduce the _batch_ mode for allocating chunks, which would significantly reduce the above mentioned overhead. A new REST service will be added to the Web server of the *Master Replication Controller*:  || method || url || input || output ||  | POST | /ingest/v1/chunk | {chunk,transaction_id} | {worker,port} |  | POST | /ingest/v1/chunks | a list of {chunk ,transaction_id} | a list of {chunk,worker,port} |",8
"DM-23090","01/16/2020 16:13:53","Update LATISS filters in obs_lsst to match commissioning filters","The filter list from DM-23075 must be added to obs_lsst to allow ingest of LATISS data by butler.",1
"DM-23092","01/16/2020 17:17:02","Check position limits for Hexapod move and offset commands","The main telescope Hexapod should check limits for position and offset commands and reject the command if out of bounds.",1
"DM-23095","01/16/2020 20:02:52","Remove pytest-runner from Documenteer 0.5.x","Create a patch release in the Documenteer 0.5 line that eliminates any use of pytest-runner. This is intended to avoid difficulties getting Documenteer built on Conda.    pytest-runner is already being removed from the Documenteer 0.6 release via DM-22957.",0.5
"DM-23106","01/17/2020 17:24:56","Use Jenkins parallel stage functionality to thread the salgenerator jobs","In looking for ways to speed up the generation of the RPMs, I found that Jenkins pipelines can run stages in parallel.  See if this functionality can work.    https://jenkins.io/blog/2017/09/25/declarative-1/",2
"DM-23107","01/17/2020 17:28:03","Create an IDL conda package job","Similar to the salobj conda package job, the next step is to create a job to create a conda package of the IDL files and the ts_idl repo.  See DM-18175 for details on the salobj conda package job for guidance on creating this job.     ",3
"DM-23108","01/17/2020 17:31:44","Create a CI build for a CSC","See DM-18175 for the original intent of that task.",3
"DM-23115","01/17/2020 22:05:06","TMA Work Phase 3","TMA Training & Review of Labview-based vendor software in Tucson & Spain.    TMA CSC design, creation & implementation.    Also any on-going work & support regarding the TMA.    This is a continuation of -DM-19848,- DM-21537",40
"DM-23117","01/18/2020 00:05:05","Fix the M2 TCP/IP Connectioin","TCP/IP connection between M2 control and cell systems is easy to break.",2
"DM-23129","01/21/2020 16:13:39","Update obs_base ingest RawFileData for multi-dataId files","In DM-23024 I changed FileDataset to support a list of DatasetRef and updated obs_base ingest to work with that.  What I did not do is update RawFileData to allow it to be populated with something that can be converted to a FileDataset with multiple DatasetRef.    The plan in this ticket is to create a new class that contains the dataId, obsInfo, and region and change RawFileData to accept a list of those.",2
"DM-23130","01/21/2020 16:45:09","Testing of Camera Hexapod SAL CSC","Testing the Camera Hexapod for the week of the 13th to the 17th. Completed on the 17th. Also helped Roberto and Mario finish hardware tests.",5
"DM-23131","01/21/2020 17:07:40","Fix ""unordered"" map documentation in DetectorCollection getters","from [~sogo.mineo] on Slack  {quote}  The comment just above this method says ""Get an unordered map"", but it returns a binary-tree map instead:  [https://github.com/lsst/afw/blob/6637c4fb6fa5813268496d76dcd9e97cbd417bcc/include/lsst/afw/cameraGeom/DetectorCollection.h#L62]  {quote}    We should just fix the docs; we probably have code that now depends on these being ordered, and it's not unreasonable for them to be ordered.",1
"DM-23132","01/21/2020 17:21:42","Update ts_xml schema to permit 0 occurrences of Author in topics","Implement RFC-657 by updating the schema in ts_xml to allow the Author tag to be omitted from topics.",0
"DM-23139","01/22/2020 00:41:04","Check whether the gen3 butler pre-filter template inputs and defer their loading if not","As of the currently available limited ci_hsc gen3 repo we have a very limited number of template images, however a production environment gen3 collection may contain many templates, covering large sky areas.         As a follow up of the initial gen3 support DM-22541 in {{ImageDifferenceTask}}, check whether the number of template inputs can potentially be huge in a production environment or not because of gen3 automatic filtering on patch spatial information and science image coordinate matching.         If all available template images are potentially loaded as task input, add support for deferred loading of the template coadd images to avoid unnecessary I/O and save resources. Load only at a later stage, if input belongs to a relevant skymap patch. If the coadd images loaded as inputs are pre-filtered at the middleware level then we perhaps do not need to implement any cautious approach in {{ImageDifferenceTask}}.     ",1
"DM-23141","01/22/2020 00:50:30","Update the cbp README to include a link to the online docs","Update the README file in the cpb package to link to the online documentation.",0
"DM-23146","01/22/2020 13:28:31","Write section 2: Configure your Repo for Docstrings","Write section on configuring your docstrings for tstn-005.",1
"DM-23148","01/22/2020 14:49:53","EAS Work Phase 4","This epic is used to hold all stories associated with the Environmental Awareness System (EAS).   This is to continue work from:   * DM-17215   * DM-18732   * DM-20197   * DM-21536",20
"DM-23152","01/22/2020 19:04:14","Helping debug Camera Hexapod Fault","On Friday the 17th, The Camera Hexapod faulted into a so-far unrecoverable state. This task outlines the effort in helping to debug this problem.",3
"DM-23153","01/22/2020 19:13:46","Outline Construction paper PSTN-017","Start the outline of the construction paper DM outline ..",2
"DM-23155","01/22/2020 20:25:44","Add XML tests to verify a topic does not contain special characters","The XML topic and attribute names should only contain letters, underscores and maybe numbers.  Create a test to validate this is true.",1
"DM-23158","01/22/2020 23:23:29","Update ts_ATDome for changes to vendor code","The vendor updated the low level ATDome code and the CSC can no longer parse full status. The error is:  {code}  Status request failed: Cound not parse 'Top Comm Link OK: 1' as SCB radio link OK: +(\d)  {code}  [~tribeiro] kindly got me an example of the new full status output:  {code}  MAIN AJAR 000  DROP AJAR 000  [OFF] 00  POSN 22.90  RR 000  Emergency Stop Active: 0  Top Comm Link OK: 0  Home Azimuth:  0.00  High Speed (degrees): 5.00  Coast (degrees): 0.50  Tolerance (degrees): 1.00  Encoder Counts per 360: 4018143232  Encoder Counts: 81630126151  Last Azimuth GoTo: 20.00  Azimuth Move Timeout (secs): 120  Rain-Snow enabled: 0  Cloud Sensor enabled: 1  Watchdog Reset Time: 600  Dropout Timer: 5  Reverse Delay: 5  Main Door Encoder Closed: 0  Main Door Encoder Opened: 0  Dropout Encoder Closed: 0  Dropout Encoder Opened: 0  Door Move Timeout (secs): 10  {code}",0
"DM-23162","01/23/2020 16:50:37","Change python style guide to match RFC-650","Following the adoption of RFC-650, change the developer guide to put binary operators at start of the next line when having to break a line.",1
"DM-23163","01/23/2020 16:51:36","Fix Attribute Description test","When a description tag is empty (no blank spaces): <Description></Description>, the test is not handling the fact the description.text is None in this case. The test throws this exception:       {code:java}  for description in root.findall(f""./{saltype}/item/Description""):  > assert description.text.replace("" "", """") is not None  E AttributeError: 'NoneType' object has no attribute 'replace'{code}     Also, the test does not handle the case where the description might be blank spaces. In this case the test passes, but there's no actual content in the description.    I found this when I was testing to see if we could lift DM-20971 from all the tests. It showed that the Hexapod XML still had one missing description, but the test failed it an unexpected manner.",1
"DM-23164","01/23/2020 16:56:54","Fix the Error Handling in M2 Control System","The error code and message are hard-coded in a string in the M2 control system. This part should be moved into the LabVIEW native error file for the extension and maintenance. In addition, there is no scroll bar for the list of error in the M2 control system. This needs to be fixed as well.",3
"DM-23165","01/23/2020 17:42:23","Hexapod_logevent_commandableByDDS state attribute missing description","The attribute in the title is missing a description. This can be seen by removing the exemptions on DM-20971 in all the tests and running pytest. Once this issue is fixed, we can remove the exemption.",0
"DM-23166","01/23/2020 17:45:01","Add __all__ to lsst.utils.deprecated module.","The {{__all__}} is needed for the documentation build. See failure log: https://ci.lsst.codes/blue/organizations/jenkins/sqre%2Finfra%2Fdocumenteer/detail/documenteer/761/pipeline/",0.5
"DM-23167","01/23/2020 18:20:29","test_NoDupcliateAttributes.py is misspelled","Please correct the spelling on the test name.",1
"DM-23171","01/23/2020 19:02:06","Add exposure group to metadata translator","To support the GROUPID header for grouping related exposures in LSSTCam data we need to add a new translation to astro_metadata_translator.     The proposal is to call this ""exposure_group"" and for most instruments to default it to be a string form of the ""exposure_id"". For LSSTCam and LATISS it will be the value of the GROUPID header.",2
"DM-23172","01/23/2020 19:19:12","Fix flake8 violations in astshim","Fix comments that are too long and modernize the flake8 configuration in setup.cfg",0
"DM-23178","01/24/2020 04:12:44","Convert some of afw to use f strings","Much of afw's use of % formatting would be clearer using f strings.    I would like to take a pass to change those. I propose to leave omit:  * Tricky formatting  * Code that is clearer with % (that is rare but not unheard of)  * Examples, because they are not unit tested",0
"DM-23180","01/24/2020 15:33:04","Investigate and fix problems with Butler vs. read-only SQLite databases","Problems were reported on Slack #stack-club, [here|https://lsstc.slack.com/archives/C9YRAS4HM/p1579842619007000] and [here|https://lsstc.slack.com/archives/C9YRAS4HM/p1571424285032300].    While we've got unit tests for Registry helper code operating on a read-only database, we don't have them for Butler, and it's possible we just need to pass a flag down to the helper code telling it to make a read-only connection.",2
"DM-23182","01/24/2020 16:42:21","Add basic IDL Type test","Currently there is no test to verify that the IDL Type is present, nor if it contains a valid value if it is.",2
"DM-23186","01/24/2020 18:11:46","Format my T&S packages with black as time permits","This trivial change will save me development time in the future by eliminating the need to manually format code. Changes:  * Change W504 to W503 in setup.cfg  * Fix any line continuation warnings that result  * Add E203 to work around a bug in flake8  * Run black",0
"DM-23188","01/24/2020 19:56:12","select_top_n in lsst_efd_client must support indexed components","The _select_top_n_ function in the EFD client library must support indexed components just as _select_time_series_ does.",1
"DM-23195","01/27/2020 16:30:10","DM-21221 broke cp_pipe due to lack of tests","The changing of {{...Nl}} to {{...nonLinearity}} is an API change which breaks things, including but not limited to {{makeBrighterFatterKernel}} due to a lack of tests (_i.e._ it's not really [~plazas]'s fault.    Also, the API change where that method doesn't {{return dataset}} and now just does {{return}} is breaking too (also due to lack of tests).    It is deliberately ambiguous whether this ticket is for fixing the breakages, or doing that and adding tests.",2
"DM-23203","01/27/2020 19:20:04","Add tabular output to translate_header","In order to simplify the listing of headers from multiple files, add a tabular option to translate_header.    I think the default will be that for one file we use the verbose listing and for multiple files we use the tabular form.",1
"DM-23204","01/27/2020 19:31:37","Dome software interface description","Describe the detailed content of the interface between low level and high level dome control software components, as mentioned in DM-23059",5
"DM-23206","01/27/2020 21:44:28","validate_drp crashes when trying to apply external skyWcs","RC2 processing on DM-23121 crashed when trying to apply jointcal wcs in validate_drp.  https://lsstc.slack.com/archives/C4JQP6FRS/p1580160187055200    Fix was illustrated by [~price]:    {code:java}  diff --git a/python/lsst/validate/drp/matchreduce.py b/python/lsst/validate/drp/matchreduce.py  index b2e9e34..5ab9804 100644  --- a/python/lsst/validate/drp/matchreduce.py  +++ b/python/lsst/validate/drp/matchreduce.py  @@ -313,7 +313,7 @@ def _loadAndMatchCatalogs(repo, dataIds, matchRadius,               / tmpCat['base_PsfFlux_instFluxErr']           if doApplyExternalSkyWcs:  -            afwTable.wcsUtils.updateSourceCoords(wcs, tmpCat)  +            afwTable.updateSourceCoords(wcs, tmpCat)           photoCalib.instFluxToMagnitude(tmpCat, ""base_PsfFlux"", ""base_PsfFlux"")           if not skipNonSrd:               tmpCat['slot_ModelFlux_snr'][:] = (tmpCat['slot_ModelFlux_instFlux'] /  {code}  ",0.5
"DM-23207","01/27/2020 22:02:15","Clean up the cat package","The current thinking of the cat package is to house the declarative yaml schemas in the [felis|https://github.com/lsst-dm/felis] format.  Those felis files are (or will be) used to instantiate TAP_SCHEMA tables, Qserv tables, etc.     Currently the cat repo has some old files and sql scripts that are no longer in use. This ticket is to clean them up.  This will remove unnecessary dependencies, and facilitate including the cat package in different builds (Sci Pi or Qserv) for validation. ",1
"DM-23208","01/27/2020 22:19:52","Add exposure group to gen3 registry","Now that DM-23171 add exposure_group to the translator, we can add it to the exposure metadata.",2
"DM-23212","01/28/2020 04:00:11","pipetask run with multiple ""-i"" command line arguments fails","This ticket follows up the discussion of the gen3 middleware telecon on 2020-01-23.    Specifying multiple {{-i}} command line options for input connection locations or one {{-i}} option with a comma separated list of connection locations should be equivalent. Please find example pipeline on the ci_hsc_gen3 data attached.       {code:java}# This one fails   $ pipetask run -j 1 -b ../ci_hsc_gen3/DATA/butler.yaml -i raw/hsc -i calib/hsc -i ref_cats -o sPccd3_2020-01-27 --register-dataset-types -p simpleProcessCccd.yaml -d ""detector = 22 AND visit = 903334 AND abstract_filter='r'"" |& tee -a sPccd3_2020-01-27.log    # This one works as expected   $ pipetask run -j 1 -b ../ci_hsc_gen3/DATA/butler.yaml -i raw/hsc,calib/hsc,ref_cats -o sPccd4_2020-01-27 --register-dataset-types -p simpleProcessCccd.yaml -d ""detector = 22 AND visit = 903334 AND abstract_filter='r'"" |& tee -a sPccd4_2020-01-27.log     {code}   ",0.5
"DM-23214","01/28/2020 05:34:23","Migrate Cassandra development branch to APDB ","Cassandra development branch ({{u/andy-slac/cassandra}}) for dax_ppdb and l1dbproto needs an update after dax_ppdb was renamed to dax_apdb. ",2
"DM-23221","01/28/2020 21:11:55","Change supplemental group separator in nextVisit","In DM-18126 we added nextVisit to ScriptQueue but [~ktl] pointed out that the {{+}} separator for the supplemental group numbers was the same separator that ISO8601 uses for time zones so this might be confusing.   Please replace with a {{#}} to indicate the supplemental number.",0
"DM-23222","01/28/2020 21:35:10","Fix OBJECT ENGTEST date and RADEC ","[~pingraham] tells me that they started using ENGTEST for real yesterday so I should stop converting OBJECT to ENGTEST from that date.    Also consider fixing the RA/DEC scaling before it is fixed tonight.",1
"DM-23223","01/28/2020 22:44:36","Allow translate_header to dump the fixed header","Currently translate_header has a dumphdr option to list the file header. This is the actual header and there is no option to provide a fixed up header after astro_metadata_translator has patched things up.    Add a new option to allow the fixed header to be seen. It's possible that we could do this with the mode switch and make mode=fixed dump the fixed header and dumphdr could alias to mode=header.",1
"DM-23231","01/29/2020 20:10:02","Sort out visit vs exposure ID in gen 2 butler","In DM-23171 I changed how visit is calculated such that it respects the GROUPID header.  This has led to some confusion because we have been using gen2 butlers as if visit_id and exposure ID were the same thing, and to make things worse exposure_id is not available and obsid is not declared to be a unique header so can't be used on its own to retrieve data.    In this ticket I will:    * Add expId as the exposure_id  * Declare that obsid and not visit is the unique constraint.  * change file templates that use visit to instead use obsid.    ",2
"DM-23234","01/29/2020 21:51:56","Create containers for ts_xml 4.4.1 and HeaderService 1.2.1","New version of header service needs a container to run on the summit asap.",1
"DM-23235","01/29/2020 22:10:24","Create containers for ts_xml 4.6.0 and HeaderService 1.3.0","New version of header service needs a container to run on the summit asap.",1
"DM-23237","01/30/2020 01:00:57","Strange image types ingested for LATISS images","The LATISS images that have been ingested at NCSA have a wide range of {{imageType}} values.  Many of these appear to be in error, but they are one-offs.    {code:shell}  $ sqlite3 /lsstdata/offline/teststand/auxTel/L1Archiver/gen2repo/registry.sqlite3  SQLite version 3.27.2 2019-02-25 16:06:06  Enter "".help"" for usage hints.  sqlite> select imageType, count(*) from raw group by imageType;  ARC|18  BIAS|409  BIAS_0001_0010|21  BIAS_0002_0010|19  BIAS_0002_0100|1  BIAS_0003_0010|19  BIAS_0003_0100|1  BIAS_0004_0010|19  BIAS_0005_0010|19  BIAS_0005_0100|1  BIAS_0006_0010|19  BIAS_0006_0100|1  BIAS_0007_0010|19  BIAS_0008_0010|19  BIAS_0008_0100|1  BIAS_0009_0010|19  BIAS_0009_0100|1  BIAS_0010_0010|19  BIAS_0011_0100|1  BIAS_0012_0100|1  BIAS_0014_0100|1  BIAS_0015_0100|1  BIAS_0017_0100|1  BIAS_0018_0100|1  BIAS_0020_0100|1  BIAS_0021_0100|1  BIAS_0023_0100|1  BIAS_0024_0100|1  BIAS_0025_0100|1  BIAS_0027_0100|1  BIAS_0029_0100|1  BIAS_0030_0100|1  BIAS_0032_0100|1  BIAS_0033_0100|1  BIAS_0035_0100|1  BIAS_TEST|2  DARK|96  DARK_0001_0010|9  DARK_0002_0010|8  DARK_0003_0010|8  DARK_0004_0010|8  DARK_0005_0010|7  DARK_0006_0010|8  DARK_0007_0010|8  DARK_0008_0010|8  DARK_0009_0010|7  DARK_0010_0010|7  ENGTEST|300  FLAT|450  FLAT_0001|52  FLAT_0001_0008_0001_0002|2  FLAT_0001_0008_0002_0002|2  FLAT_0002|51  FLAT_0002_0008_0001_0002|2  FLAT_0002_0008_0002_0002|2  FLAT_0003_0008_0001_0002|2  FLAT_0003_0008_0002_0002|2  FLAT_0004_0008_0001_0002|2  FLAT_0004_0008_0002_0002|2  FLAT_0005_0008_0001_0002|2  FLAT_0005_0008_0002_0002|2  FLAT_0006_0008_0001_0002|2  FLAT_0006_0008_0002_0002|2  FLAT_0007_0008_0001_0002|2  FLAT_0007_0008_0002_0002|2  FLAT_0008_0008_0001_0002|2  FLAT_0008_0008_0002_0002|2  FOCUS|22  FOCUS42086.85|1  FOCUS430|1  FOCUS4550.0|14  FOCUS4550.5|1  FOCUS45522.5|2  FOCUS45524.0|1  FOCUS45525.5|1  FOCUS45527.0|1  FOCUS45527.5|1  FOCUS45528.5|1  FOCUS45530.0|1  FOCUS45531.5|1  FOCUS45554.5|1  FOCUS45568.5|1  FOCUS45568.7|1  FOCUS45568.9|1  FOCUS45569.10000000000001|1  FOCUS45569.30000000000001|1  FOCUS45569.50000000000001|1  FOCUS45569.70000000000002|1  FOCUS45569.8|5  FOCUS45569.90000000000002|1  FOCUS45570.10000000000002|1  FOCUS45570.30000000000003|1  FOCUS45575.59|60  FOCUS467|2  FOCUS46786.85|1  FOCUS46786.92|3  FOCUS46786.95|1  FOCUS46787.04|1  FOCUS46787.25|1  FOCUS47868.4|2  FOCUS47868.7|1  FOCUS47868.80000000000001|1  FOCUS47869.0|2  FOCUS47869.1|1  FOCUS47869.19999999999999|1  FOCUS47869.20000000000002|1  FOCUS47869.29999999999998|1  FOCUS47869.3|1  FOCUS47869.39999999999998|1  FOCUS47869.4|1  FOCUS47869.49999999999997|1  FOCUS47869.59999999999997|1  FOCUS47869.6|1  FOCUS47869.60000000000002|1  FOCUS47869.69999999999996|1  FOCUS47869.79999999999995|1  FOCUS47869.8|35  FOCUS47869.80000000000001|1  FOCUS47869.89999999999995|1  FOCUS47869.89999999999999|1  FOCUS47870.00000000000003|1  FOCUS47870.19999999999999|1  FOCUS47870.20000000000002|1  FOCUS47870.49999999999999|1  FOCUS47870.79999999999998|1  FOCUS47871.09999999999998|1  FOCUS47886.85|6  FOCUS480|7  FOCUS540|1  FOCUS60086.7|1  FOCUS60086.76|1  FOCUS60086.81|1  FOCUS60086.84|1  FOCUS60086.92|1  FOCUS60086.96|1  FOCUS60087.25|2  FOCUS63239.4|1  FOCUS63239.699999999999996|1  FOCUS63239.8|1  FOCUS63239.9|1  FOCUS63239.99999999999999|1  FOCUS63240.0|1  FOCUS63240.099999999999994|1  FOCUS63240.1|1  FOCUS63240.199999999999996|1  FOCUS63240.2|2  FOCUS63240.29999999999999|1  FOCUS63240.3|1  FOCUS63240.300000000000004|2  FOCUS63240.39999999999999|1  FOCUS63240.400000000000006|2  FOCUS63240.46|21  FOCUS63240.49999999999999|1  FOCUS63240.5|6  FOCUS63240.50000000000001|2  FOCUS63240.59999999999999|1  FOCUS63240.60000000000001|2  FOCUS63240.69999999999999|1  FOCUS63240.70000000000001|2  FOCUS63240.79999999999999|1  FOCUS63240.80000000000001|1  FOCUS63240.9|2  FOCUS63240.999999999999986|1  FOCUS63241.09999999999999|1  FOCUS63241.3|1  FOCUS63258|1  FOCUS63258.874|9  FOCUS63258.9|1  FOCUS63275.55|1  FOCUS63277.03399999999996|2  FOCUS75086.41|1  FOCUS75086.77|1  FOCUS75086.81|1  FOCUS75086.83|1  FOCUS75086.95|1  FOCUS75087.02|1  FOCUS75087.04|1  FOCUS75087.15|1  FOCUS75087.27|1  FOCUS75087.36|1  FOCUS840|10  FOCUS84087.02|1  FOCUS84087.11|1  FOCUS84087.17|2  FOCUS84087.27|1  FOCUS84087.39|1  FOCUS98086.85|1  FOCUS98086.92|1  FOCUS98087.01|1  FOCUS98087.11|2  FOCUS98087.18|1  FOCUS98087.29|1  FOCUS98087.35|1  FOCUS98087.36|1  FOCUS98087.56|3  FOCUS98087.68|1  FOCUSTEST|2  FOCUS_632NM|26  MONOFLAT|10  OBJECT|51  QUEUE_TEST|11  TEST|402  TEST_QUEUE|9  UNKNOWN|180  WAVE,FOCUS,632,73.5905|1  {code}  ",1
"DM-23242","01/30/2020 17:31:17","Add HeaderService dynamical timeout depending of image exposure time","Update the ATHeaderService to have a dynamically set timeout that is adjusted by the requested exposure time. For example timeout=requested_exptime+N (in seconds).",2
"DM-23246","01/30/2020 18:22:07","Provide basic English Description based on Spanish MQTT Topic names","The MQTT topic names are all in Spanish which may be confusing for non-Spanish speaking people. It was decided to keep the Spanish topic names since mechanics generally are Chilean so it will be easier for them to identify the hardware components when they are referred to in Spanish. In addition English Descriptions will be added for those who don't understand Spanish well enough. The Descriptions will be direct translations of the Spanish words into English without an attempt to form proper sentences.",3
"DM-23248","01/30/2020 18:35:21","Draft changes to DPDD to bring in line with current DRP plans","Create a branch of the DPDD that incorporates DRP leaderships current best guesses at what we will actually do.  This is primarily the removal of multifit and galaxy Monte Carlo sampling, along with what we expect to replace them; it will not attempt to address more fine-grained issues with particular columns, as I believe others have already made progress addressing those elsewhere.",8
"DM-23249","01/30/2020 19:58:43","New decam ingest tests need skipif for testdata_decam","The gen3 ingest tests that were added as part of DM-22708 all need to be protected with skipif for installations that don't have testdata_decam.",0.5
"DM-23255","01/30/2020 22:03:06","Report memory leak in OpenSplice","INRIA reports a slow memory leak in salobj. Using the attached script they record a loss of 50Mb in 12 hours. Running profiling software suggests the leak is in {{ddsutil.py}} (part of ADLink OpenSplice).    I studied the leak and found it appears to be in OpenSplice dds (even the licensed version in lsstts/salobj:b48_lic). I reported it to ADLink as case 00020396 with two simple ""how to reproduce"" self-contained archives (both also attached here), one using asyncio and one using threads. I see the same leak either way:  * memory_tester_asyncio.zip  * memory_tester_threaded.zip    These write samples in one subprocess and read them in another. The reader simply counts the samples but does nothing else with them. That is as simple a procedure as I could think of.    So far ADLink has not responded to the ticket. Once they do I will file a new ticket for what we have to do (if there is anything we can do until they fix the problem at their end).",2
"DM-23256","01/31/2020 09:00:21","Migrate Qserv documentation to LSST-the-doc","Documentation is now available here:    http://qserv.lsst.io    https://qserv-operator.lsst.io",8
"DM-23257","01/31/2020 14:12:31","Create presentation for SST on deblending in the stack","Create a presentation for the LSST System Science Team (SST) on deblending in the stack. It has been requested that this presentation includes a general introduction to blending in the science pipelines as well as an update on the latest developments.",2
"DM-23258","01/31/2020 14:56:08","matchedVisitMetricsTask will crash if any visits have a missing source catalog","Currently, {{matchedVisitMetricsTask}} in {{validate_drp}} crashes if any ccd specified on the command line has a raw but no source catalog (if {{processCcd}} failed for any reason).  The fix is trivial to add a {{datasetExists}} to {{matchreduce.py}}.",0.5
"DM-23262","01/31/2020 18:03:16","Bug in RA/DEC calculation from ALT/AZ","the arguments to from_geodetic are reversed:   {{location = EarthLocation.from_geodetic(lat*u.deg, lon*u.deg, height*u.m)}}    and {{RA/DEC/END}} should use {{DATE_END}} and the moment it uses {{DATA_BEG}} instead.",1
"DM-23266","01/31/2020 18:47:13","Please simplify write_command in ts_salobjATHexapod controller.py","Suggestions for improving the write_command method in controller.py of ts_salobjATHexapod:  * Decode the data before returning it. That way only {{write_command}} has to deal with bytes; everything else can deal with strings.  * Use {[readuntil}} instead of reading character by character. This is far more efficient and simplifies the code. You wait until all the data you asked for arrives.  * Specify the number of lines of reply that you expect as an argument. This simplifies the method because it does not have to be clever. It also simplifies error handling in code that calls the method because the correct # of replies is guaranteed to be returned (else an exception is raised).  * Strip the final \n from the reply. That way callers don't have to do that.  * Return the replies as a list of strings instead of making the caller do that.    I am not sure who to assign this to. I am happy to do the work myself, if desired.    Note: these are changes I requested in DM-22170 but [~ecoughlin] suggested that they be handled in a different ticket.",1
"DM-23267","01/31/2020 20:12:14","Jenkins training - video tutorials","Task to cover time taking the LinkedIn Learning Jenkins course(s).",2
"DM-23275","01/31/2020 21:46:02","Restore outfile option for butler makeRepo","Butler.makeRepo takes a parameter to specify the output location of the config file but at some point in time that functionality was removed. Restore it and add a test.",2
"DM-23276","01/31/2020 23:13:31","Fix repr for gen3 Registry to output SQLite path","The str and repr output for the gen3 Registry objects could be cleaned up:    {code}  (Pdb++) str(gen3Butler.registry)  'SQLite3@/tmp/tmp8jbxfh7k/gen3.sqlite3'  (Pdb++) gen3Butler.registry  Registry(<lsst.daf.butler.registry.databases.sqlite.SqliteDatabase object at 0x7fec973f04a8>, DimensionUniverse({htm7, htm9, abstract_filter, instrument, skymap, calibration_label, detector, physical_filter, tract, patch, visit, exposure}))  {code}    The DimensionUniverse would be useful to see in the `str` output, while the `repr` should show the sqlite path instead of the Object.",1
"DM-23278","02/01/2020 05:01:18","Fix RADEC for LATISS data on 27th Jan","In CAP-422 I noticed that RASTART/DECSTART were completely wrong. These are the headers that are used for the RADEC calculation in the translator. The translator needs to know that those headers on the 2020-01-27  and -28 should not be trusted (before that date we knew not to trust them because the date was completely wrong).",1
"DM-23281","02/02/2020 01:15:20","FILTER and GRATING not concatenated in LATISS data","When in Tucson, the disperser was written with the FILTER2 key (note the 2), and this was concatenated (using a ""+"") with FILTER on ingest, so that we had a compound filter.    Now that we're on-sky, we're writing that as GRATING (which is good) but {{exp.getFilter()}} seems to only return the physical filter which is only the first part (though we know the registry is out of date), but moreover, doing       {noformat}  translate_header.py -p lsst.obs.lsst.translators /project/shared/auxTel/_parent/raw/2020-01-28/2020012800335-det000.fits -d{noformat}  shows that the filter is just {{blank_bk7_wg05}} rather than {{blank_bk7_wg05+ronchi90lpmm}} as I'd have expected.          ",3
"DM-23289","02/03/2020 15:44:27","Check the Script by Harris","Harris provided the function of script used in M2 control system to assign the actuator steps or forces. This task will evaluate the use of script and test it on the M2 simulator.",3
"DM-23290","02/03/2020 15:47:55","Check the SAL Event and Telemetry in M2 Control System Phase 1","This task will check the SAL published event and telemetry in M2 control system. From the previous tests, I only know the telemetry data is not zero. However, I am not sure the values are reasonable or not. I also noticed the published data of summary state event is always 0. Therefore, this task will try to make sure the published data is reasonable.",3
"DM-23310","02/04/2020 00:34:44","Some ComCam images do not have LSST_NUM header","Some of the ComCam images present at NCSA at {{/lsstdata/offline/teststand/comcam/CCS/storage}} appear to be lacking the {{LSST_NUM}} header that identifies the detector serial number.  As a result, they fail to ingest properly, as that header is used to compute the {{detector_serial}} metadata item that becomes the {{lsstSerial}} column in the registry database.    Please update the {{comCam.py}} translators to fix this header.    The canonical mapping from sensor position to serial number is:  {code}  sensor='S00' ccdserial='ITL-3800C-229'  sensor='S01' ccdserial='ITL-3800C-251'  sensor='S02' ccdserial='ITL-3800C-215'  sensor='S10' ccdserial='ITL-3800C-326'  sensor='S11' ccdserial='ITL-3800C-283'  sensor='S12' ccdserial='ITL-3800C-243'  sensor='S20' ccdserial='ITL-3800C-319'  sensor='S21' ccdserial='ITL-3800C-209'  sensor='S22' ccdserial='ITL-3800C-206'  {code}",1
"DM-23313","02/04/2020 13:35:31","Fix dominant failure mode in deblending sources with 2-components","The dominant failure mode in `meas_extensions_scarlet` is a parentheses that is outside, rather than inside a set of parentheses. Correcting this should fix the large number of errors that we are seeing when using 2-component models and bring the failure rate down to the same rate as the other deblending methods at a small fraction of a percent.",1
"DM-23324","02/04/2020 19:52:35","Update AT ops procedure (tstn-004)","Update TSTN-004 to the latest operations procedure. ",1
"DM-23329","02/04/2020 21:48:04","Fix controller code in phosim and imsim data","When I added controller to obs_lsst translator in DM-21204 I didn't realize that S is meant to be used for simulated data and so I used O. Fix that.",0.5
"DM-23331","02/05/2020 00:54:36","Add default fgcmcal configuration files for HSC processing","Now that {{fgcmcal}} is ready for full production, a set of recommended default config files can be added to {{obs_subaru}}, including look-up-table building, chromatic correction information, mirror coating dates, etc.",2
"DM-23343","02/05/2020 16:25:08","Mark Synpipe as deprecated","Following RFC-661, ensure that Synpipe is marked as deprecated before the next major release (20.0.0).",1
"DM-23349","02/05/2020 17:43:55","Build new shared stack with updated conda env","Once the conda env is updated in DM-22817, a new shared stack needs to be created.",1
"DM-23351","02/05/2020 18:22:03","BaseCscTestCase.check_bin_script checks for the wrong initial summary state","BaseCscTestCase.check_bin_script checks for an initial summary state of OFFLINE instead of STANDBY. Fix this. (Better yet, make it an argument that defaults to STANDBY).",0
"DM-23353","02/05/2020 20:46:12","Jenkins training - video tutorials - part 2","Task to cover time taking the LinkedIn Learning Jenkins Essentials course.    https://www.linkedin.com/learning/jenkins-essential-training/next-steps?u=37426900",1
"DM-23359","02/06/2020 00:01:18","Allow pex_config configs to use __file__","[~erykoff] notes that {{\_\_file\_\_}} is not supported by config files in pex_config. Looking at the code it seems trivial to add support for this.",0.5
"DM-23362","02/06/2020 16:18:23","Old LATISS data has missing OBSID/DAYOBS","[~ktl] reports that old LATISS data has no OBSID or DAYOBS headers so ingest fails.    The fix is to add them from the IMGNAME header during header fixup phase.    see eg /lsstdata/offline/teststand/auxTel/L1Archiver/storage/2019-03-06/AT_O_20190306_000014-ats-wfs_ccd.fits",1
"DM-23368","02/06/2020 20:08:05","Use masterPriority=0 for scripts to speed up script launch","As explained in DM-22382 it would speed up script launching significantly if we launch scripts with a lower masterPriority than some other existing SAL component. The obvious choice is to use a value lower than the masterPriority of the ScriptQueue itself.    Unfortunately, it appears that master priority cannot be set programmatically. However, it suffices to set the value when the Domain Participant is created. So I propose the following:  * When the ScriptQueue starts up it reads the OpenSplice configuration file pointed to by OSPL_URI.  * If it finds ""masterPriority=<number>"" then it writes a copy of the file to a temporary directory with masterPriority=0 and changes OSPL_URI to point to the copy.  * Thus we only have to maintain a single copy of the configuration file and any scripts that are launched will use masterPriority=0.      Note: it turns out that the following bit of configuration is not relevant or useful for speeding up script startup (ADLINK reported this on a recent ticket and I have confirmed it with the speed test I sent to ADLink). Apparently it only affects a more persistent form of durability than we use:  {code}  <Persistent>      <StoreDirectory>/home/saluser/dds_cache/</StoreDirectory>      <StoreMode>XML</StoreMode>  </Persistent>  {code}",1
"DM-23369","02/06/2020 22:05:31","Remove lsst.ip.isr.addDistortionModel","{{lsst.ip.isr.addDistortionModel}} was announced as deprecated in release 19.0.0 on DM-20154. Remove it before release 20.0.0.",1
"DM-23371","02/06/2020 22:10:05","Remove lsst.obs.base.CameraMapper._extractAmpId","This method has been marked as deprecated since release 11, and its deprecation was announced with release 19.0.0. Remove it.",1
"DM-23408","02/07/2020 19:31:38","Attempt to compile Moog Hexapod Simulator","Following recent events, there has been a desire to have a simulator for the Hexapod's built by Moog. However, the simulators have not been tested nor documented by us or Moog. I attempted to build the simulator and after many hours was able to successfully build it. ",1
"DM-23414","02/07/2020 22:26:30","lsst_ci fails with astropy 4 and numpy >=1.17","In testing DM-22817 I discovered that lsst_ci doesn't like Astropy 4 because it seems that Astropy with numpy >=1.17 now preserves units inside numpy functions: (resulting in us applying the unit twice):  {code}  /Users/square/j/ws/stack-os-matrix/osx.clang.py3/lsstsw/stack/DarwinX86/validate_drp/19.0.0-7-g12b746e+17/python/lsst/validate/drp/photerrmodel.py:71: RuntimeWarning: invalid value encountered in sqrt  return np.sqrt(sigmaSq)  Traceback (most recent call last):  File ""/Users/square/j/ws/stack-os-matrix/osx.clang.py3/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/astropy/units/quantity_helper/helpers.py"", line 32, in get_converter  scale = from_unit._to(to_unit)  File ""/Users/square/j/ws/stack-os-matrix/osx.clang.py3/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/astropy/units/core.py"", line 951, in _to  f""'{self!r}' is not a scaled version of '{other!r}'"")  astropy.units.core.UnitConversionError: 'Unit(""marcsec"")' is not a scaled version of 'Unit(""marcsec2"")'  {code}    (from https://ci.lsst.codes/blue/organizations/jenkins/stack-os-matrix/detail/stack-os-matrix/31182/tests/ )    This fails for testObsCfhtQuick  and testObsDecamQuick",1
"DM-23420","02/10/2020 16:34:52","ap_association does not work with numpy 1.18 and pandas 1.0","I'm testing the new conda environment (DM-22817) and ap_association is failing one test:  {code}  $ python tests/test_association_task.py   /Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/indexes/range.py:708: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.    return super().__getitem__(key)  .E/Users/timj/work/lsst/tmp/lsstsw/build/ap_association/python/lsst/ap/association/association.py:284: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.    obj_idxs[matched_src_idxs]]  ../Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/indexes/range.py:708: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.    return super().__getitem__(key)  ..  ======================================================================  ERROR: test_remove_nan_dia_sources (__main__.TestAssociationTask)  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""tests/test_association_task.py"", line 589, in test_remove_nan_dia_sources      out_dia_sources = assoc_task.check_dia_source_radec(dia_sources)    File ""/Users/timj/work/lsst/tmp/lsstsw/build/ap_association/python/lsst/ap/association/association.py"", line 179, in check_dia_source_radec      nan_idxs = np.argwhere(nan_mask).flatten()    File ""<__array_function__ internals>"", line 6, in argwhere    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/numpy/core/numeric.py"", line 584, in argwhere      return transpose(nonzero(a))    File ""<__array_function__ internals>"", line 6, in nonzero    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 1896, in nonzero      return _wrapfunc(a, 'nonzero')    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 58, in _wrapfunc      return _wrapit(obj, method, *args, **kwds)    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 51, in _wrapit      result = wrap(result)    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/generic.py"", line 1917, in __array_wrap__      return self._constructor(result, **d).__finalize__(self)    File ""/Users/timj/work/lsst/tmp/lsstsw/miniconda/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/series.py"", line 292, in __init__      f""Length of passed values is {len(data)}, ""  ValueError: Length of passed values is 1, index implies 6.    ----------------------------------------------------------------------  Ran 6 tests in 2.447s    FAILED (errors=1)  {code}  It's the {{np.argwhere}} that is falling over.  This is numpy 1.18.2 and pandas 1.0.0.    The nan_mask has a value of:  {code}  0    False  1    False  2     True  3     True  4     True  5    False  {code}",1
"DM-23422","02/10/2020 19:30:21","Try to speed up ts_salobj","At one time I believe ts_salobj tests/test_speed.py reported transmitting and receiving several thousand messages per second. With the current version of ts_sal tests/test_speed.py is reporting 600-700 messages/second on my Mac. Try to figure out what changed and improve the speed.    Note that v5.0.0 also runs at the same speed and running older versions requires an older Test IDL file because the ackcmd topic has changed.",2
"DM-23424","02/10/2020 20:14:06","Remove ddsutil from ts_salobj","We worked around a bug in OpenSplice 6.9 by importing including a patched ddsutils.py in ts_salobj. That is no longer necessary with OpenSplice 6.10 and it is time to use the standard module.",0
"DM-23425","02/10/2020 22:28:25","Monthly February ap_pipe rerun","Reprocess the three HiTS 2015 fields using ap_pipe with the same good-seeing CompareWarp coadd templates as used for January (DM-23069). Make a summary notebook and verify that nothing significant has changed.",8
"DM-23435","02/11/2020 16:27:21","Implement redis installation using kubedb inside Qserv operator","Qserv operator will install redis using KubeDB CRD. This might be a first track for prototyping redis DB.    A redis section will be added to Qserv CRD, with the number of redis masters.",8
"DM-23445","02/11/2020 21:43:25","Move visit definition out of raw ingest for Gen3","At present, RawIngestTask is responsible for adding three different types of dimension records:   - exposure   - visit   - visit_detector_region    We should move the latter two to a separate script that is run after RawIngestTask and (at least at present) only after all associated raws for a visit have been ingested.  That will open the door to (later) being able to define different visit definitions for the same exposure (DM-15536).    This will not on its own allow individual raws to be ingested incrementally - RawIngestTask still needs all raws for an exposure to be present in a single invocation so we can add the exposure dimension record once without needing to see if it already exists in the Registry.  That could be fixed by using Database.sync in a new high-level counterpart to {{Registry.insertDimensionData}} that does perform such a check, but we'll first need to figure out how to handle transactions in ingest - Database.sync is not and probably cannot be part of a larger transaction.    But even that is an easier problem to solve than the corresponding one for visits, because (in addition to the synchronization problem) the visit dimension record needs to contain a region that is currently computed from the regions of all of the detectors that went into it (but could be computed from cameraGeom in the future).",0
"DM-23450","02/12/2020 15:46:54","Add a FACILITY header to LATISS data","We need to add a FACILITY header to LATISS data with a fixed value of ""Vera C. Rubin Observatory"". ",1
"DM-23462","02/12/2020 17:13:31","Please use an env variable to specify the masterPriority level in the OSPL configuration","We want to run Scripts with masterPriority=0 in order to make sure that they never become ""master"", which would trigger an expensive system-wide DDS realignment.    ADLink knows of no programmatic way to do this, but we can easily use env variable substitution in the {{ospl.xml}} configuration file and that variable can be optional. I suggest the following, where {{masterPriority=""$\{OSPL_MASTER_PRIORITY:\-1\}""}} sets the master priority to the env variable OSPL_MASTER_PRIORITY, if defined, else 1. Yes the minus sign is needed (I checked that): the default delimiter really is "":-"".    {code}  <DurabilityService name=""durability"">      <Network>          <Alignment>              <TimeAlignment>false</TimeAlignment>              <RequestCombinePeriod>                  <Initial>2.5</Initial>                  <Operational>0.1</Operational>              </RequestCombinePeriod>          </Alignment>          <WaitForAttachment maxWaitCount=""100"">              <ServiceName>ddsi2</ServiceName>          </WaitForAttachment>      </Network>      <NameSpaces>          <NameSpace name=""defaultNamespace"">              <Partition>*</Partition>          </NameSpace>          <Policy alignee=""Initial""                  aligner=""true""                  durability=""Durable""                  masterPriority=""${OSPL_MASTER_PRIORITY:-1}""                  nameSpace=""defaultNamespace""/>      </NameSpaces>  </DurabilityService>  {code}    This will make it trivial for scripts to assign themselves a masterPriority of 0 and we can also easily assign an early CSC, such as a SAL/Kafka producer, a higher masterPriority.",1
"DM-23470","02/12/2020 18:36:34","Correct the usage of GIT_TAG_NAME in pipeline","We want several jobs to use the Git Tag name in the job, since the tag contains the application version information.  Currently, the GIT_TAG_NAME variable, provided by the [git-tag-message|https://plugins.jenkins.io/git-tag-message/] plugin, does not work in pipelines.  Therefore, a different implementation is needed until the plugin is updated.",1
"DM-23477","02/12/2020 19:54:06","pipe_base ScalarError can't be pickled","Testing the new conda env I had a failure in ci_hsc_gen3 and the failure triggered a failure in the multiprocessing module reporting the failure.  The {{ScalarError}} exception is a subclass of TypeError that takes two parameters. Unfortunately when a {{ScalarError}} is pickled only a single parameter (the message string) is stored and passed to the constructor so reporting the error state fails.    The fix is to make the second parameter optional and to assume that the first parameter is the error message if that's all that is given.",1
"DM-23489","02/13/2020 17:20:21","ci_hsc_gen3 fails with new conda env (sqlite v3.31.1)","With the new conda env from DM-22817 ci_hsc_gen3 fails because multiple flats match when only one should match.    {code}  bin/pipeline.sh 1  py.warnings WARN: /Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/obs_subaru/19.0.0-20-g5e540f88+1/config/hsc/isr.py:119: FutureWarning: Config field doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.    config.doAddDistortionModel = True  py.warnings WARN: /Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/obs_subaru/19.0.0-20-g5e540f88+1/config/hsc/isr.py:119: FutureWarning: Config field doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.    config.doAddDistortionModel = True  ctrl.mpexec.cmdLineFwk INFO: QuantumGraph contains 155 quanta for 12 tasks  Traceback (most recent call last):    File ""/Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/ctrl_mpexec/19.0.0-8-g354b538+3/bin/pipetask"", line 26, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/ctrl_mpexec/19.0.0-8-g354b538+3/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 175, in parseAndRun      return self.runPipeline(qgraph, taskFactory, args)    File ""/Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/ctrl_mpexec/19.0.0-8-g354b538+3/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 415, in runPipeline      executor.execute(graph, butler, taskFactory)    File ""/Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/ctrl_mpexec/19.0.0-8-g354b538+3/python/lsst/ctrl/mpexec/mpGraphExecutor.py"", line 76, in execute      self._executeQuantaInProcess(graph.traverse(), butler, taskFactory)    File ""/Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/ctrl_mpexec/19.0.0-8-g354b538+3/python/lsst/ctrl/mpexec/mpGraphExecutor.py"", line 97, in _executeQuantaInProcess      enableLsstDebug=self.enableLsstDebug)    File ""/Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/ctrl_mpexec/19.0.0-8-g354b538+3/python/lsst/ctrl/mpexec/mpGraphExecutor.py"", line 179, in _executePipelineTask      return executor.execute(taskDef, quantum)    File ""/Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/ctrl_mpexec/19.0.0-8-g354b538+3/python/lsst/ctrl/mpexec/singleQuantumExecutor.py"", line 96, in execute      self.runQuantum(task, quantum, taskDef)    File ""/Users/timj/work/lsst/tmp/lsstsw/stack/DarwinX86/ctrl_mpexec/19.0.0-8-g354b538+3/python/lsst/ctrl/mpexec/singleQuantumExecutor.py"", line 247, in runQuantum      inputRefs, outputRefs = connectionInstance.buildDatasetRefs(quantum)    File ""/Users/timj/work/lsst/tmp/lsstsw/build/pipe_base/python/lsst/pipe/base/connections.py"", line 440, in buildDatasetRefs      raise ScalarError(attributeName, len(quantumInputRefs))  lsst.pipe.base.connections.ScalarError: Expected scalar for output dataset field flat, received 2 DataIds  {code}    The two dataIds are:    {code}  flat@{'instrument': 'HSC', 'calibration_label': 'gen2/flat_2013-11-03_023_HSC-I', 'detector': 23, 'physical_filter': 'HSC-I'} (id=1476)  flat@{'instrument': 'HSC', 'calibration_label': 'gen2/flat_2013-06-17_023_HSC-R', 'detector': 23, 'physical_filter': 'HSC-I'} (id=1496)  {code}    This is with sqlalchemy 1.3.13 but also fails in the same way with 1.3.1 (current conda env) and 1.3.8 (current eups package).  ",0.5
"DM-23491","02/13/2020 18:02:29","Parameterize XML version in IDL conda job","Currently in the IDL Conda packaging process the XML version is hardcoded into the Jenkinsfile.conda recipe.  This is bad, as each XML release requires a ts_idl release.  Figure out the best way to parameterize this value.  Build parameter?  Docker image Env Var (prompting a new image with each XML release)?  Other?",2
"DM-23494","02/13/2020 19:10:35","Update the ts_hexapod internal simulator with the correct geometry for the hexapod","Update the simulator in ts_hexapod with the correct geometry for the hexapod. That way when the CSC is run in simulation mode the commanded actuator lengths will be at at least approximately correct.    One known reason that the simulator may still differ from the Moog code is that the simulator assumes that pivot rotations are applied in order: x, y, z. For small rotations it doesn't matter much, but for larger motions the order is important.    Also change the simulator so length = 0 at nominal position (with the position set to 0,0,0 0,0,0).",1
"DM-23495","02/13/2020 19:21:36","Make QueueModel.next_group_id a static method","Make QueueModel.next_group_id a static method",0
"DM-23503","02/13/2020 21:51:41","Butler gen3 datastore templates should handle slashes in data Ids","It was noted today that files in the ci_hsc_gen3 datastore are not unique and instead look like {{DATA/raw/hsc/raw/i/HSC-I/903986/raw_i_HSC-I_903986_903986_22_HSC_raw/hsc.fits}} -- this is because ""run"" has a value of {{raw/hsc}} and ""run"" is at the end of the template.    The templating system should replace / with _. Optionally we could also introduce a ""/"" format modifier to allow ""/"" to be retained.",1
"DM-23504","02/13/2020 21:52:51","Create containers for ts_xml 4.6.0 and HeaderService 1.4.0","New version of header service needs a container to run on the summit asap.    ",1
"DM-23509","02/14/2020 01:20:35","obs_lsst failing LATISS plate scale test","In DM-23490 the plate scale of LATISS was changed but the tests were not run before merging. This is breaking master:    {code}  self = <test_generateCamera.PhosimToRaftsTestCase testMethod=testGenerateCameraLatiss>      def testGenerateCameraLatiss(self):          """"""Test with LATISS in a test directory.""""""          content = self.runGenerateCamera([""latiss"", ""lsstCam"", os.path.curdir])          self.assertEqual(content[""name""], ""LATISS"")  >       self.assertEqual(content[""plateScale""], 20.0)  E       AssertionError: 10.112 != 20.0  {code}    I will fix the plate scale in the tests.",0.5
"DM-23523","02/14/2020 20:55:54","Check the SAL Event and Telemetry in M2 Control System in Phase 2","This task will check the SAL published event and telemetry in M2 control system as the phase 2 . This is a following task of DM-23290.",1
"DM-23524","02/14/2020 21:02:55","Support M2 Test","Support M2 test.",2
"DM-23526","02/14/2020 23:25:30","Fix fgcmcal issues exposed during PDR2 run","During RC2 and PDR2 processing a few issues with {{fgcmcal}} were exposed, and this ticket has consolidated these fixes into one.    First, visit level tests by [~lauren] show that the errors reported in {{fgcmcal}} {{photoCalib}} objects are much larger than for {{jointcal}}.  This is because the errors are taking into account the full calibration error, including uniformity, but this is not what the stack expects.  I do not believe these errors affect coaddition.  Instead, {{fgcmcal}} should be reporting just the ""gray"" error estimated locally on each ccd.  Second, there are some visits in PDR2 that have an airmass as large as 2.95 (zenith angle >70deg), which was beyond the range of the {{fgcmcal}} look-up table.  A new atmosphere table and LUT will be generated which goes to zenith angle of 75 deg which is the limit of the Subaru telescope.  Third, a couple of default {{fgcmcal}} config changes for {{obs_subaru}} including increasing the number of iterations in each fit cycle for better convergence, and fix an index error on the default {{fgcmcal}} aperture correction parameters.  Related, a bug in {{fgcm}} was fixed that did not properly persist internal aperture corrections for bands that did not have a lot of data (namely, N387).",2
"DM-23527","02/14/2020 23:29:38","change fits:tunit: nmgy to nanojanskys","In cat yml/hsc.yaml   we need to change all those {{fits:tunit: nmgy}} to nanojanskys",1
"DM-23529","02/14/2020 23:41:36","Add cat to lsst_distrib (as sdm_schemas) ","Add cat to lsst_distrib so it gets weekly version tags",2
"DM-23533","02/17/2020 13:56:55","Create XML files with proposed command protocol","For the commanding from the upper level DCS to the lower level components a proposal needs to be made for both the communications and the command protocols. The basis for the commands will be XML files that will be used by both the Rubin Observastory and EIE. This story is for collecting the necessary commands and creating an XML file to share between both parties.",1
"DM-23534","02/17/2020 14:06:27","Create mock controler framework for communications between upper and lower level DCS","For the communications between the upper level DCS and lower level a mock lower level controller will be created based on the ts_ATDome code.",2
"DM-23535","02/17/2020 14:09:32","Implement replies to command details in mock controller","After having created the mock controller framework, the commanding details need to be implemented which this story is for.",2
"DM-23537","02/17/2020 16:24:40","Draft the M2 Test Plan","Review the M2 requirement document and draft the test plan.",2
"DM-23549","02/17/2020 19:43:33","Dome Work Phase 3","This epic is to contain all the work for the Dome.  It is a continuation of epics:   * DM-18309   * DM-20198",40
"DM-23554","02/17/2020 21:38:02","Add document obsolescence facility to lsst-texmf","RFC-660 requires that LDM-472 be deprecated. Add an ""obsolete"" flag to the lsstdoc latex class to add deprecation warnings on each page.  This allows us to declare a document to be obsolete whilst also leaving the text available.",0.5
"DM-23558","02/18/2020 04:36:52","DCR coadds are missing PhotoCalib","{{ap_pipe}} fails for DCR coadds with the following error. This failure started in {{w_2020_07}} but was fine in {{w_2020_06}}. The error below was from a run with a coadd made using {{w.2020.06-6-g81841ad}} or pipe_tasks, while the failed {{ap_pipe}} run used {{w.2020.07}}       {code:java}  // apPipe INFO: Running ImageDifference...  apPipe.differencer INFO: Processing DataId(initialdata={'ccdnum': 23, 'filter': 'g', 'visit': 411037, 'date': '2015-02-17', 'object': 'Blind15A_42'}, tag=set())  apPipe.differencer.getTemplate INFO: Using skyMap tract 0  apPipe.differencer.getTemplate INFO: Assembling 4 coadd patches  apPipe.differencer.getTemplate INFO: exposure dimensions=(2048, 4096); coadd dimensions=(4175, 2116)  apPipe.differencer.getTemplate INFO: Constructing DCR-matched template for patch {'datasetType': 'dcrCoadd_sub', 'bbox': Box2I(minimum=Point2I(46369, 55694), dimensions=Extent2I(1731, 406)), 'tract': 0, 'patch': '11,13', 'numSubfil  ters': 3}  apPipe.differencer.getTemplate INFO: Constructing DCR-matched template for patch {'datasetType': 'dcrCoadd_sub', 'bbox': Box2I(minimum=Point2I(46369, 55900), dimensions=Extent2I(1731, 1910)), 'tract': 0, 'patch': '11,14', 'numSubfi  lters': 3}  apPipe.differencer.getTemplate WARN: dcrCoadd_sub, tract=0, patch=12,13, numSubfilters=3, subfilter=0 does not exist  apPipe.differencer.getTemplate WARN: dcrCoadd_sub, tract=0, patch=12,14, numSubfilters=3, subfilter=0 does not exist  apPipe FATAL: Failed on dataId={'ccdnum': 23, 'filter': 'g', 'visit': 411037, 'date': '2015-02-17', 'hdu': 21, 'object': 'Blind15A_42'}: RuntimeError: No coadd PhotoCalib found!  /software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/obs_decam/19.0.0-6-gb5bb71e+9/config/isr.py:95: FutureWarning: Config field ccdProcessor.isr.doAddDistortionModel is deprecated: Camera geometry is incorporate  d when reading the raw files. This option no longer is used, and will be removed after v19.    config.doAddDistortionModel = False  # rely on the TPV terms instead  /software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_association/19.0.0-13-g744ec97+1/python/lsst/ap/association/mapApData.py:185: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as   the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.    table_list = list(yaml.load_all(yaml_stream))  /software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pex_config/19.0.0-5-g9aa49c1/python/lsst/pex/config/configurableField.py:150: FutureWarning: Config field ccdProcessor.isr.doAddDistortionModel is deprecated:   Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.    self._value.__setattr__(name, value, at=at, label=label)  /software/lsstsw/stack_20191101/python/miniconda3-4.5.12/envs/lsst-scipipe/lib/python3.7/site-packages/astropy/units/function/logarithmic.py:46: RuntimeWarning: invalid value encountered in log10    return dex.to(self._function_unit, np.log10(x))  Traceback (most recent call last):    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-9-g0ae078d/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-9-g0ae078d/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-9-g0ae078d/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_pipe/19.0.0-6-g8dfed85+1/python/lsst/ap/pipe/ap_pipe.py"", line 165, in runDataRef      diffImResults = self.runDiffIm(calexpRef, templateIds)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-9-g0ae078d/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ap_pipe/19.0.0-6-g8dfed85+1/python/lsst/ap/pipe/ap_pipe.py"", line 238, in runDiffIm      return self.differencer.runDataRef(sensorRef, templateIdList=templateIds)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_base/19.0.0-9-g0ae078d/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/pipe_tasks/19.0.0-27-g2f87fd56/python/lsst/pipe/tasks/imageDifference.py"", line 388, in runDataRef      template = self.getTemplate.run(exposure, sensorRef, templateIdList=templateIdList)    File ""/software/lsstsw/stack_20191101/stack/miniconda3-4.5.12-4d7b902/Linux64/ip_diffim/19.0.0-7-g2f7a0e4+1/python/lsst/ip/diffim/getTemplate.py"", line 186, in run      raise RuntimeError(""No coadd PhotoCalib found!"")  RuntimeError: No coadd PhotoCalib found!  {code}",2
"DM-23589","02/20/2020 04:57:17","Update LATISS filters in obs_lsst ","Tonight the LATISS filters were changed. A corresponding change has to be made to lsst.obs.lsst.filters.  The updated list can be found at https://confluence.lsstcorp.org/display/LTS/AuxTel+Filters",0.5
"DM-23594","02/20/2020 19:10:39","Improved performance monitoring tools for the Replication system","Minor improvements of the existing performance monitoring tools:   * verbosity control for reporting results/errors   * added a multi-threaded testing option   * added a simple flow-control testing option",2
"DM-23607","02/24/2020 14:03:22","Model deconvolution kernel using scarlet","Before DFTs were integrated into scarlet, the earliest version of the code calculated the difference kernel needed to do partial convolutions from the narrow model PSF into the wider observed PSFs in each band using scarlet itself.     The basic algorithm is as follows:  * In scarlet we typically define a model PSF {{M}} that is a nyquist sampled gaussian with {{sigma=1/sqrt(2)}}. Fitting sources in the model to the observed images in each band requires convolving the PSF in each band ({{P_b}} with a difference kernel, the kernel needed to match the PSF in the model seeing to the PSF in the observed seeing. So the difference kernel {{D}} is given by {{P=M*D}}.  * The current version of scarlet uses the same technique as the one described in the galsim paper ([Rowe 2014|https://arxiv.org/abs/1407.7676] to calculate the difference kernel in k-space, however the first version of scarlet made this calculation in real space.  * In real space we defined the observed image as the observed PSF and the ""difference kernel"" used to convolve the model was the model PSF. This works because convolution commutes, so {{P=M*D=D*M}}. In other words, if we define our _model_ to be the difference kernel and convolve it with the model PSF, we can fit the result to the observed PSF and use the same algorithm used in scarlet to fit the model to find the best fit observed PSF without ever going into k-space.  * This worked quite well but was not as fast as matching the kernel in k-space, since the real space calculation required an iterative solution with the k-space version took only one pass. However, the problem with the k-space solution is that performing deconvolution, going from a wider PSF to a narrower PSF is ill-defined in k-space and leads to divergences in the difference kernel, making it numerically unstable.  * Earlier work in DM-20127 attempted to see if we could live with the numerical artifacts produced by the k-space deconvolution but the results were dependent on how different the model PSF and observed PSF were, with limitations on the upper-bound of the observed PSF before numerical artifacts dominated and the initialization was made worse.  * This ticket is to explore using the same real space PSF matching used in the previous version of scarlet, using the improved optimization algorithms of scarlet 1.0, to calculate the deconvolution kernel (the kernel to go from model PSF to observed PSF) to see if it can produce a more numerically stable result. ",5
"DM-23616","02/24/2020 23:17:33","Run converted ap_verify testdata through gen3 pipeline","To more fully test DM-22655, we should run as much of the gen3 pipeline as we can on a converted ap_verify test dataset. As a first test, lets run on ap_verify_ci_hits, instead of the much larger non-ci dataset.    I'll provide the command to run in a comment. You'll need to get three packages on branch {{tickets/DM-22655}} and scons'd: daf_butler, obs_base, and obs_decam. -If you setup ap_verify_ci_hits2015 before sconsing obs_decam, it will run a few extra tests.-",2
"DM-23623","02/25/2020 01:47:24","Measure crosstalk coefficients for AuxTel chip and add for use.","Dual purpose: test out the crosstalk measurement code and ensure it works well for this kind of data, and add a crosstalk matrix so that the AuxTel can use it.    If it doesn't work well for some reason, either fix it, or work out what kind of input data would make it happy, and if that's on-sky, add it to the observing plan for the next run.    Also, compare to nominal ITL values as provided by camera team, and see which works better for correction.",8
"DM-23627","02/25/2020 05:34:33","Missing psfMatched_nImage definition","{{assembleCoadd}} currently fails if {{warpType=psfMatched}} and {{doNImage=True}} using the Gen 2 butler. We just need to add the missing dataset definition to exposures.yaml, and add one for DCR coadds as well.",1
"DM-23630","02/25/2020 19:24:39","fgcmcal failure Ubuntu","fgcmcal fails on master with the latest conda environment on Ubuntu 18.04.4. The failure is in one of the new tests comparing the `photoCalib.getCalibrationErr()` with the `fgcmZptGrayErr`:    {code}  tests/test_fgcmcal_hsc.py:154:   _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   tests/fgcmcalTestBase.py:407: in _testFgcmOutputProducts      (np.log(10.0)/2.5)*testCal.getCalibrationMean()*fgcmZptGrayErr)  ../../stack/Linux64/utils/19.0.0-7-g686a884+2/python/lsst/utils/tests.py:656: in assertFloatsAlmostEqual      testCase.assertFalse(failed, msg=""\n"".join(errMsg))  E   AssertionError: True is not false : 1/1 elements differ with rtol=2.220446049250313e-16, atol=2.220446049250313e-16  E   0.00223844323512219 != 0.0022384434 (diff=2.3283064e-10/0.0022384434=1.0401453e-07)  {code}    We are not seeing this on other machines (CentOS, macOS), so there's probably a ""fun"" interaction with system libraries going on here. Adding {{rtol=2e-7, atol=None}} to the above test prevents the error, but may not be the actual solution we want. This suggests to me that there's a float32 calculation going on under the hood somewhere.    I can provide an Ubuntu account to test on.",1
"DM-23631","02/25/2020 19:25:34","Create simple python script to create and insert fakes for the HiTS2015 AP processing.","Create a simple script to insert fakes for AP using the HiTS2015 dataset. Likely this will be on an un-merged ticket of ap_verify.    This script will be fairly hacky as it will copy data around in the ap_verify_hists2015 dataset and edit the DecamMapper on the fly.",8
"DM-23643","02/26/2020 16:43:18","Create containers for ts_xml 4.7.0 and HeaderService 1.4.0","New version of header service needs a container to run on the summit asap.    ",2
"DM-23649","02/26/2020 19:27:03","Relax dev guide wording on post-review squashing","Add the text agreed in RFC-670 to the developer guide, to allow for post-review commits when squashing them is impractical and when the commits meet our existing quality standards.",1
"DM-23651","02/26/2020 22:28:59","ap_pipe calls some deprecated things","I noticed when reviewing recent ap_pipe logs that there are some deprecation warnings we should probably deal with.  {code}  /software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/obs_decam/19.0.0-6-gb5bb71e+12/config/isr.py:95: FutureWarning: Config field ccdProcessor.isr.doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.  /software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ap_association/19.0.0-13-g744ec97+3/python/lsst/ap/association/mapApData.py:185: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.  /software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pex_config/19.0.0-5-g9aa49c1+1/python/lsst/pex/config/configurableField.py:150: FutureWarning: Config field ccdProcessor.isr.doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.  /software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ip_diffim/19.0.0-7-g2f7a0e4+3/python/lsst/ip/diffim/imagePsfMatch.py:785: FutureWarning: Call to deprecated method getImageF(). (Zero-argument overload is deprecated; use one that takes an ``interpStyle`` instead. To be removed after 20.0.0.)  /software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ip_diffim/19.0.0-7-g2f7a0e4+3/python/lsst/ip/diffim/imageDecorrelation.py:421: FutureWarning: Call to deprecated method setCtrX. (Use `setCtr` instead. To be removed after 20.0.0.)  /software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ip_diffim/19.0.0-7-g2f7a0e4+3/python/lsst/ip/diffim/imageDecorrelation.py:422: FutureWarning: Call to deprecated method setCtrY. (Use `setCtr` instead. To be removed after 20.0.0.)  /software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_tasks/19.0.0-28-g53bcf5f6+2/python/lsst/pipe/tasks/imageDifference.py:759: FutureWarning: Call to deprecated method makeSourceCatalog. (Replaced by SourceDetectionTask.run(). Will be removed after v20.) {code}",2
"DM-23670","02/28/2020 14:30:51","Update ConfigurableCsc to output the settingsApplied and softwareVersions events","Update {{ConfigurableCsc}} to output the {{settingsApplied}} event. The logical place to do this is in {{begin_start}}, which calls {{configure}}. Also update {{BaseCsc}} to output the {{softwareVersions}} event    See https://confluence.lsstcorp.org/pages/viewpage.action?pageId=120786640 for details.  I believe the format of the {{settingsApplied}} field should be {{config_file}}:{{version}}, though that is a bit unclear from the Description field of the XML. The current default version is available as {{self.evt_settingVersions.data.settingsVersion}}.    Note that CSCs which want to set the {{otherSettingsEvents}} field should do that in the constructor, by setting {{self.evt_settingsApplied.data.otherSettingsEvent}}. Document that in ""How to write a CSC"".    Finally try to figure out how to output {{settingsVersions}} and {{settingsApplied}} in {{BaseCsc}}. These will be nearly empty. The trick is doing this in a way that does not interfere with {{ConfigurableCsc}}.",2
"DM-23672","02/28/2020 16:27:24","Prepare ts_dcs for SALObj","Add SALObj dependency to the project and implement a skeleton dcs CSC.",2
"DM-23678","02/28/2020 18:43:04","Bug in s3Datastore when using temporary file","On macOS high sierra builds daf_butler is falling over in S3 tests with the following error:    {code}  >       return fileDescriptor.storageClass.pytype(fileDescriptor.location.path, **parameters)  E       lsst.pex.exceptions.wrappers.FitsError:   E         File ""src/fits.cc"", line 1561, in lsst::afw::fits::Fits::Fits(const std::string &, const std::string &, int)  E           cfitsio error: tried to move past end of file (107) : Opening file '/var/folders/d4/hc0hbfss6_d5vtlcchxpdx5h0000gn/T/tmpg5x4hnge.fits' with mode 'r'  E       cfitsio error stack:  E         ffopen could not interpret primary array header of file:   E         /var/folders/d4/hc0hbfss6_d5vtlcchxpdx5h0000gn/T/tmpg5x4hnge.fits  E        {0}  E       lsst::afw::fits::FitsError: 'cfitsio error: tried to move past end of file (107) : Opening file '/var/folders/d4/hc0hbfss6_d5vtlcchxpdx5h0000gn/T/tmpg5x4hnge.fits' with mode 'r'  E       cfitsio error stack:  E         ffopen could not interpret primary array header of file:   E         /var/folders/d4/hc0hbfss6_d5vtlcchxpdx5h0000gn/T/tmpg5x4hnge.fits  {code}    We are seeing these now because the new conda env includes moto/boto3.    The relevant code in s3Datastore.py is:    {code:python}  with tempfile.NamedTemporaryFile(suffix=formatter.extension) as tmpFile:      tmpFile.file.write(serializedDataset)      formatter._fileDescriptor.location = Location(*os.path.split(tmpFile.name))      result = formatter.read(component=getInfo.component)  {code}    The problem is that between the write and the read the file is not closed. Closing the file would delete the temporary file so the minimalist fix is to add a {{flush()}} between the write and the read.    It seems like linux and macOS mojave have different buffering allowing the same process to read from something that is still open for write without flushing.",0.5
"DM-23679","02/28/2020 19:03:54","Update M1M3 cRIO with SAL4 compatible code","Travel to Chile and update and test the M1M3 Controller code and Monitor process to   use SAL 4 and DDS 6.9",5
"DM-23685","02/28/2020 21:56:55","Test ADLink's fix for the OpenSplice memory leak","ADLink claims to have fixed the memory leak discussed in DM-23255. Test their fix and make sure neither the reader nor writer are leaking.",1
"DM-23698","03/02/2020 14:55:23","Investigate N387 fgcmcal failures in PDR2 rerun","In DM-23394, [~hchiang2] reports that there are a number of PDR2 visits that did not produce {{photoCalib}} outputs from {{fgcmcal}}.  This will happen when a visit is deemed to be unsolvable (but this should be logged clearly).  However, there are a number of deep-field N387 visits in the deep fields that were deemed to be unsolvable but from cursory inspection should have been usable.",1
"DM-23699","03/02/2020 15:00:37","Update fgcmcal default config format to remove possibility of index errors","The configuration format for {{FgcmFitCycleTask}} has some elements that must be specified in band order.  While this was fine (such as it was) with 5 bands (g,r,i,z,y), with the addition of several narrow-bands this has become more obviously unwieldy.  It would be easy to change these configs to use dictionary mappings instead of ordered lists which would remove ambiguity and make mis-configurations much less likely.",2
"DM-23700","03/02/2020 16:57:47","Update and improve various testing documentation.","Basically, some of the testing documentation is out of date (bad links, old data) and is missing some recent developments, like the CI/CD process.    Attached in an email from Andy with his thoughts.  I will break out the points in that email into bulleted items in this ticket or make new tickets, depending on required effort.",2
"DM-23701","03/02/2020 17:41:59","pipetask-produced DOT for pipelines should show prerequisite inputs","the DOT graph produced now for pipelines does not include prerequisite inputs, would be nice to add them to the graph but also make them distinguishable from regular inputs.",1
"DM-23702","03/02/2020 18:45:46","IsrTask shoud use regular Input for raw data","Currently IsrTask connection are defined with all inputs being of PrerequisiteInput type. It should instead be using regular Input type for at least a ""raw"" type top constrain its inputs to only existing inputs, otherwise it will result in all possible combinations of visits/detectors being used.    Slack link: https://lsstc.slack.com/archives/C2JPT1KB7/p1582938474109700",1
"DM-23703","03/02/2020 21:55:09","ap_association fails if a new visit has 0 new diaObjects","When {{association.py}} is called on a new visit, {{new_dia_objects}} is initialized as an empty list. That list only receives properly formatted entries if there are in fact any new DIA objects, but it is used to create the {{new_dia_objects}} pandas dataframe regardless. This results in an error on the next line when diaObjectId is set as the index: [https://github.com/lsst/ap_association/blob/744ec977e52da689ac3639ffcbb519abeedabcd2/python/lsst/ap/association/association.py#L416]         Example error message:  {code:python}  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+4/python/lsst/pipe/base/cmdLineTask.py"", line 388, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+4/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+4/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ap_pipe/19.0.0-6-g8dfed85+6/python/lsst/ap/pipe/ap_pipe.py"", line 180, in runDataRef      diaPipeResults = self.runAssociation(calexpRef)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+4/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ap_pipe/19.0.0-6-g8dfed85+6/python/lsst/ap/pipe/ap_pipe.py"", line 267, in runAssociation      ccdExposureIdBits=sensorRef.get(""ccdExposureId_bits""))    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+4/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ap_association/19.0.0-13-g744ec97+5/python/lsst/ap/association/diaPipe.py"", line 212, in run      loaderResult.diaSources)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+4/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ap_association/19.0.0-13-g744ec97+5/python/lsst/ap/association/association.py"", line 128, in run      matchResult = self.associate_sources(diaObjects, diaSources)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+4/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ap_association/19.0.0-13-g744ec97+5/python/lsst/ap/association/association.py"", line 222, in associate_sources      match_result = self.match(dia_objects, dia_sources, scores)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+4/python/lsst/pipe/base/timer.py"", line 150, in wrapper      res = func(self, *args, **keyArgs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/ap_association/19.0.0-13-g744ec97+5/python/lsst/ap/association/association.py"", line 416, in match      new_dia_objects.set_index(""diaObjectId"", inplace=True, drop=False)    File ""/software/lsstsw/stack_20200220/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/pandas/core/frame.py"", line 4303, in set_index      raise KeyError(f""None of {missing} are in the columns"")  KeyError: ""None of ['diaObjectId'] are in the columns""    {code}",1
"DM-23707","03/02/2020 23:38:14","Add JS9 to the stable LDF deployment","In order to do our visualization bake-off, I'll need access to a cluster local JS9 server.  I'm still unclear whether it would be better to have a singleton, or to spawn a server in everyone's namespaces, but I guess we'll go with the former first.    Eric Mandel has supplied an image that can be used to spin up a server using docker.    {{docker run -dit -p 8080:80 -p 2718:2718 ericmandel/js9server}}",2
"DM-23711","03/03/2020 17:22:13","Allow butler configs to use environment variables to find other configs","We have been discussing on Slack how to allow gen3 configs to be combined from multiple sources.  One option is for the EUPS table file to add a search directory to DAF_BUTLER_CONFIG_PATH.  This works fine but can be unpredictable since the contents of the configuration can depend on precisely which packages have been set up.    On this ticket we will extend {{includeConfigs}} support to allow an environment variable to be specified in addition to a full path or relative path.",0.5
"DM-23713","03/03/2020 18:13:28","Create CSC deploy-env Jenkins build","To continue with the CSC build process, we need to start with the deploy-env image.  This needs a Jenkins build to ensure consistency.",2
"DM-23717","03/03/2020 19:48:19","FitsError building obs_decam on Jenkins mojave-1","[Build #31303|https://ci.lsst.codes/blue/organizations/jenkins/stack-os-matrix/detail/stack-os-matrix/31303/pipeline] failed on node mojave-1 in obs_decam {{test_ingest.py}}. Four test cases failed; example error is:    {code}  from lsst.afw.image import makeExposure, makeMaskedImage  > full = makeExposure(makeMaskedImage(self.readImage()))  ../../stack/DarwinX86/obs_base/19.0.0-21-gaaa92db+7/python/lsst/obs/base/fitsRawFormatterBase.py:338:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self = lsst.obs.decam.rawFormatter.DarkEnergyCameraRawFormatter(FileDescriptor(Location('file:///Users/square/j/ws/stack-os-m...mage.TransmissionCurve'), 'metadata': StorageClassPropertyList('PropertyList', pytype='lsst.daf.base.PropertyList')})))  def readImage(self):  index, metadata = self._determineHDU(self.dataId['detector'])  > return lsst.afw.image.ImageI(self.fileDescriptor.location.path, index)  E lsst.pex.exceptions.wrappers.FitsError:  E File ""src/fits.cc"", line 1402, in void lsst::afw::fits::Fits::readImageImpl(int, T *, long *, long *, long *) [T = int]  E cfitsio error (/Users/square/j/ws/stack-os-matrix/6ab9d5fd5e/lsstsw/build/obs_decam/tests/tmpbmqgsnz8/raw/raw/r/r DECam SDSS c0002 6415.0 1480.0/415282/raw_r_r DECam SDSS c0002 6415.0 1480.fits): non-CFITSIO program error (1) : Reading image  E cfitsio error stack:  E decompression error: hit end of compressed byte stream  E {0}  E lsst::afw::fits::FitsError: 'cfitsio error (/Users/square/j/ws/stack-os-matrix/6ab9d5fd5e/lsstsw/build/obs_decam/tests/tmpbmqgsnz8/raw/raw/r/r DECam SDSS c0002 6415.0 1480.0/415282/raw_r_r DECam SDSS c0002 6415.0 1480.fits): non-CFITSIO program error (1) : Reading image  E cfitsio error stack:  E decompression error: hit end of compressed byte stream  E '  {code}    ",0.5
"DM-23718","03/03/2020 19:55:26"," Replace dots in gen3 file names","-In DM-23717 we were reminded that filters can have spaces in them and these end up in file names.-    Dots in the file part of templates cause confusion when extension manipulation occurs in the {{Location}} class. To be safe replace dots in the file name with underscores. Do not change dots in directory parts of path though.    This fix is essentially the same fix as was done in DM-23503 for ""/"".",0.5
"DM-23719","03/03/2020 19:59:19","Write camera cable wrap CSC","Write the portion of the MTMount CSC that controls the camera cable wrap.    I will probably do this work before finishing the PXI simulator DM-23118 because we should have an existing hardware simulator to talk to, as well as the real hardware, and I consider it higher priority.",3
"DM-23722","03/03/2020 20:09:34","Validate dataset type definitions in pipeline task connections","In debugging DM-22655 we realized again that the dataset type definitions for input datasets in pipeline task connectors are not validated against dataset type definitions already understood by the registry. This led to a strange bug where cpBias had a ExposureF when the file was ingested but the isrTask definition said it was an ImageF.  This led to the pipeline creating a DatasetType object that was at odds with the registry definition and caused an error on get.    At the very least we should warn if there is an inconsistency between the two when the pipeline runs.",1
"DM-23728","03/03/2020 21:15:31","Cleanup ci_hsc_gen2 to use new convert script instead of custom one","In order to help get DM-22655 merged sooner, I'm going to move the work of converting ci_hsc to use the new script onto this ticket, instead of doing it there. I'd already started on that process, and will move that work onto this ticket.",2
"DM-23735","03/03/2020 23:44:30","Fix ""name"" variable in roundtable_aiohttp_bot template","The lsst-templatebot-aide service expects projects to have a variable called ""name"" that it turns into the name of the repository. Unfortunately in roundtable_aiohttp_bot, the {{name}} variable is instead {{repo_name}}. This ticket will fix that.",0.5
"DM-23750","03/04/2020 13:49:43","Update VMS software and install on summit machines","Update the VMS software for SAL 4 and OpenSplice 6.9 compatabilty.  Install on summit cRIO and PC and test (no real telemetry yet though)  basic function and logging of telemetry to summit EFD",2
"DM-23751","03/04/2020 13:53:56","Create feature releases in GitHub repos for new MTM1M3 and MTVMS","Create feature branches and then release for the new SAL4 compatible   versions of the M1M3MT and MTVMS packages as tested in Chile",3
"DM-23753","03/04/2020 14:12:05","Train Andrew the Use of Hexapod","Train Andrew the use of hexapod.",1
"DM-23759","03/04/2020 17:12:35","Add JS9 JupyterLab extension to nublado","I have tested out the process for building the extension and it seems to work using {{jupyter-lab}} version 1.2.6 on my laptop.    I followed the instructions [here|https://github.com/mgckind/jjs9] with the exception that I also needed to install {{pandas}}.",2
"DM-23769","03/04/2020 22:12:28","Add dome information to LATISS headers","Information values in:  https://confluence.lsstcorp.org/display/SYSENG/Auxiliary+Telescope+Header+Information+Topic+Mapping      {noformat}  DOMEAZ         ATDome ATDome.position.azimuthPosition                      SHUTLOWR ATDome ATDome.position.dropoutDoorOpeningPercentage   SHUTUPPR ATDome ATDome.position.mainDoorOpeningPercentage  SEEING         DIMM   DIMM.logevent_dimmMeasurement   {noformat}      ",2
"DM-23771","03/05/2020 00:53:57","Add a CscCommander class to salobj","Add a CscCommander class to salobj, much like the one in ts_hexrotcomm. This should allow making trivial scripts to exercise a CSC. It should handle simple commands automatically and allow overrides for more complex commands.",2
"DM-23772","03/05/2020 11:56:53","Fix detailed states for TunableLaser CSC","Currently the TunableLaser CSC's detailed states duplicate the summary states and only include a propagating detailed state. The duplicated summary states should be removed and a nonpropagating state should be added.",1
"DM-23773","03/05/2020 12:00:04","Refactor TunableLaser CSC and upgrade to latest salobj(5.x)","Fix outstanding issues with TunableLaser CSC and perform upgrade to TunableLaser CSC",2
"DM-23778","03/05/2020 21:38:11","Write config and tests for obs_lsst gen2-gen3 convert","[~tjenness] pointed out during the review of DM-22655 that we have gen2 repos to test obs_lsst gen2-3 conversion. We will also need to specify some configs specific to this package. Implementing `test_convert2to3.py` would be an easy start, once the right test data is identified.    [~tjenness]: In your comment you said ""This repo even comes with a bunch of gen2 repos..."": where are they, and what to they contain? We do have `testdata_lsst`, but that's not in lsst_distrib (how big is it? The readme isn't helpful.) and it doesn't contain premade gen2 repos.",2
"DM-23780","03/05/2020 22:44:16","Refresh jupyterlabutils cluster classes","The {{LSSTClusterClient}} doesn't work with the version of {{dask.distributed}} that ships with the most recent weekly.  Specifically the way timeouts work has changed in {{dask.distributed}}, so the exponential back off loop in {{LSSTClusterClient}} does not work as expected.    Anecdotally, the problem with {{dask.distributed}} that we were working around with that backoff loop seems to not be a problem anymore, so we can probably take that out.    It may now be that {{LSSTClusterClient}} can be slimmed down to be just a subclass of {{Client}} that overrides the {{__repr__}}.    The {{ClusterProxy}} seems to have bit rotted.  The link to the scheduler status page works, but the links to the status page for the individual workers end up in 500s.",1
"DM-23799","03/06/2020 18:09:29","Review the Rotator PXI Code","Review the rotator PXI code to have some understanding of it. This task will try to compile the code by the autotool as well.",3
"DM-23800","03/06/2020 18:12:16","Develop and Test the Hexapod and Rotator Software Phase 2","This epic will maintain, develop, and test the software of Hexapod and Rotator contains the middleware, GUI, and wrapper code in cRIO.    This will include testing of the CCW and Camera Hex/Rot on the Camera Cart at the Summit.  This testing is currently slated for the first two weeks of December.    This is a continuation of epic: DM-21931",40
"DM-23801","03/06/2020 18:51:52","Fix ""Contributing"" entry for ts_ATDome and my other T&S packages","The ""Contributing"" entry in index.rst is wrong for ts_ATDomeTrajectory and possibly others of my packages as well. T&S Jira tickets should be specified by label, not component (unlike DM).",0
"DM-23802","03/06/2020 18:57:18","Improve DDS queue warnings in salobj","ts_salobj warns if the DDS queue starts to fill up, but the warning is too crude. At present it warns if it reads >= 10 messages and never again, until it resets the warning when it reads only 1 message.    What we want to know is something on the order of:  - reached 10%  - reached 50%  - reached 90%  - reached 100% (error: data likely lost)    Also the warning should be reset in some fine-grained fashion, e.g. if we reach 10% then report that we have improved, and warn at 50% again.    Use the {{QueueLengthChecker}} for this (it is already used for messages about the Python queue).    Also there are two warnings about queues: one for the DDS queue and one for the Python queue. Improve the text for each so it is completely clear which warning applies to which cache.",1
"DM-23808","03/06/2020 21:57:19","Make azimuth encoder counts long long in ATDome position telemetry","Change the type of the azimuthEncoderPosition field of the position telemetry topic for ATDome from {{unsigned long}} to {{long long}}. The current field is too short.",0
"DM-23809","03/06/2020 22:23:17","Add weather station headers to header service","We do not currently write any weather station values to the header. LSE-400 lists:    AIRTEMP  PRESSURE  HUMIDITY  WINDSPD  WINDDIR  SEEING - DIMM seeing value  Please add them in their own section of the header.    Updated information values in     https://confluence.lsstcorp.org/display/SYSENG/Auxiliary+Telescope+Header+Information+Topic+Mapping",2
"DM-23812","03/07/2020 18:21:26","Add log name to the logMessage topic","Add log name to the logMessage topic. I originally thought that since logMessage is associated with a particular CSC the information was redundant, but I am wrong because components of the CSC may use child logs with component-specific names and presently that information is lost.    Update ts_salobj to set this field -- probably conditionally on it existing, so that salobj is compatible with older XML.",0
"DM-23815","03/09/2020 00:06:00","Update the M2 xml","Update the M2 xml based on the M2 test result at Mar., 2020. The ICD is LTS-162. After the [M2 test|https://confluence.lsstcorp.org/display/LTS/M2+Testing+on+Summit+at+March+2nd+-+6th%2C+2020], we realized that we may need to update the xml to fulfill the condition now and extend the original xml for the need.",3
"DM-23820","03/09/2020 17:52:54","Clean up the Subsystem.xml file ","The Subsystem.xml file is woefully out of date and/or just wrong.  Hoping to fix it a bit.",0
"DM-23822","03/09/2020 18:29:19","Create containers for ts_xml 4.7.0 and HeaderService 1.4.1","New version of header service needs a container to run on the summit asap.    ",1
"DM-23826","03/09/2020 19:39:36","The test of getCurrentTime ts_sal is failing","[~athornton] reports that the test of getCurrentTime is failing on two different systems. Adam helped me get credentials on a system that showed the problem and it turns out that it is due to a recent change to astropy: astropy now downloads a leap second table at need, and this was occurring in the middle of the test, making measured time differences much larger than expected.    I looked at both the 4.0 and 4.1 branches of ts_sal and concluded that only the 4.0 branch needs a fix, which is to get astropy time once before running the time-critical portion of the test.    The test is radically different in SAL 4.1 (it uses the new getLeapSeconds function) and is immune to this problem.",0
"DM-23827","03/09/2020 20:09:41","wrong python type for matchVisits_config","I encountered the following error:   {noformat}  CameraMapper INFO: Loading calib registry from /datasets/hsc/calib/20200115/calibRegistry.sqlite3  root INFO: Running: /home/hchiang2/PDR2/pa_stack/qa_explorer/bin/matchVisits.py /datasets/hsc/repo/rerun/DM-23243/ANALYSIS/DEEP/ --output /datasets/hsc/repo/rerun/DM-23243/ANALYSIS/DEEP/ --doraise --id tract=08524 filter=HSC-Z  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 680, in writeConfig      oldConfig = butler.get(configName, immediate=True)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_persistence/19.0.0-1-g6fe20d0+7/python/lsst/daf/persistence/butler.py"", line 1392, in get      return callback()    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_persistence/19.0.0-1-g6fe20d0+7/python/lsst/daf/persistence/butler.py"", line 1385, in callback      return self._read(location)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_persistence/19.0.0-1-g6fe20d0+7/python/lsst/daf/persistence/butler.py"", line 1586, in _read      results = location.repository.read(location)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_persistence/19.0.0-1-g6fe20d0+7/python/lsst/daf/persistence/repository.py"", line 194, in read      return butlerLocationStorage.read(butlerLocation)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_persistence/19.0.0-1-g6fe20d0+7/python/lsst/daf/persistence/posixStorage.py"", line 279, in read      return readFormatter(butlerLocation)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_persistence/19.0.0-1-g6fe20d0+7/python/lsst/daf/persistence/posixStorage.py"", line 531, in readConfigStorage      pythonType = doImport(pythonType)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_persistence/19.0.0-1-g6fe20d0+7/python/lsst/daf/persistence/utils.py"", line 122, in doImport      importedClass = doImport(importClassString)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_persistence/19.0.0-1-g6fe20d0+7/python/lsst/daf/persistence/utils.py"", line 115, in doImport      pythonType = getattr(importType, importClassString)  AttributeError: module 'lsst.pipe.tasks' has no attribute 'matchVisits'    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/home/hchiang2/PDR2/pa_stack/qa_explorer/bin/matchVisits.py"", line 3, in <module>      MatchVisitsTask.parseAndRun()    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 610, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 214, in run      if self.precall(parsedCmd):    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 330, in precall      self._precallImpl(task, parsedCmd)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 309, in _precallImpl      task.writeConfig(parsedCmd.butler, clobber=self.clobberConfig, doBackup=self.doBackup)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 683, in writeConfig      (configName, exc))  AttributeError: Unable to read stored config file matchVisits_config (module 'lsst.pipe.tasks' has no attribute 'matchVisits'); consider using --clobber-config    {noformat}    Somehow somewhere it thought {{matchVisits}} is from {{lsst.pipe.tasks}}; it should be from {{lsst.qa.explorer}}.   I found an entry in obs_base that has it wrong.     I don't understand why this bug wasn't causing problems before.  ",1
"DM-23828","03/09/2020 22:32:32","Create small test decam gen2 repo with calibs for test_convert2to3.py","The decam convert tests in DM-22655 rely on a gen2 repo that does not exist in our CI system. To make those tests able to be run as part of CI, we need to shrink the calibration data that lives in ap_verify_ci_hits2015, and make a gen2 repo with it. If the data is small enough, that data could even live in obs_decam.    [~tjenness]'s suggestion is to not zero all the data, but to set each data array to the HDU number, so that we can still test that gen3 and gen2 read the same pixel data.",2
"DM-23830","03/09/2020 23:48:20","Add cbp package to lsst_distrib","Implement RFC-658.",1
"DM-23836","03/10/2020 19:50:00","DCR templates have incorrect variance","The DCR templates for image differencing incorrectly set the variance using only the variance plane of the first subfilter that is loaded. The final variance plane is too low by a factor equal to the actual number of subfilters, resulting in an incorrect threshold for detecting sources in image differencing.",1
"DM-23839","03/10/2020 21:40:00","Fix MTM1M3 Telemetry name","Fix MTM1M3 telemetry topic powerData to along with code  ie powerSupplyData  ",1
"DM-23842","03/11/2020 16:17:29","Add additional SALOBJ functions to the Dome CSC","Now that SALOBJ has been added to the project, it is time to implement the azimuth motion control commands.",2
"DM-23844","03/11/2020 17:35:30","Make sure that the reply to the STATUS command is in line with the proposed communications protocol","The current implementation of the mock controller doesn't exactly follow the proposed communications protocol. Fix please.",1
"DM-23846","03/11/2020 19:21:54","YAML files with python/object/apply fail in pyyaml>5.2.1","python/object/apply is considered unsafe. PyYAML 5.2.1+ (at least in 5.3 for sure) seems like they will not load YAML files with python/object/apply using the FullLoader. This seems to make, at a minimum, astro_metadata_translator unhappy in some inute tests for obs_lsst test data.    Here is a test failure    {code}  /tmp/sss/stack/lsstsw/stack/DarwinX86/astro_metadata_translator/tickets.DM-23835-g592071df3b+406fba300f/python/astro_metadata_translator/tests.py:143: in assertObservationInfoFromYaml      header = read_test_file(file, dir=dir)  /tmp/sss/stack/lsstsw/stack/DarwinX86/astro_metadata_translator/tickets.DM-23835-g592071df3b+406fba300f/python/astro_metadata_translator/tests.py:78: in read_test_file      header = yaml.load(fd, Loader=Loader)  ../miniconda/envs/lsst-scipipe-05fa363/lib/python3.7/site-packages/yaml/__init__.py:114: in load      return loader.get_single_data()  ../miniconda/envs/lsst-scipipe-05fa363/lib/python3.7/site-packages/yaml/constructor.py:43: in get_single_data      return self.construct_document(node)  ../miniconda/envs/lsst-scipipe-05fa363/lib/python3.7/site-packages/yaml/constructor.py:47: in construct_document      data = self.construct_object(node)  ../miniconda/envs/lsst-scipipe-05fa363/lib/python3.7/site-packages/yaml/constructor.py:92: in construct_object      data = constructor(self, node)  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _     self = <yaml.loader.FullLoader object at 0x7f8590ac8e10>  node = SequenceNode(tag='tag:yaml.org,2002:python/object/apply:collections.OrderedDict', value=[SequenceNode(tag='tag:yaml.or...e=[ScalarNode(tag='tag:yaml.org,2002:str', value='BZERO'), ScalarNode(tag='tag:yaml.org,2002:float', value='0.0')])])])        def construct_undefined(self, node):          raise ConstructorError(None, None,                  ""could not determine a constructor for the tag %r"" % node.tag,  >               node.start_mark)  E       yaml.constructor.ConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:collections.OrderedDict'  E         in ""/private/tmp/sss/stack/lsstsw/obs_lsst/tests/headers/latiss-2018-09-20-05700065-det000.yaml"", line 1, column 1  {code}",0.5
"DM-23848","03/11/2020 21:28:11","Put the Rotator into Standby State after the clearError Command","Put the rotator PXI into the standby state after clearing the error. This task will need to update the Simulink model for the state machine transition. This task also needs to write the related test in Simulink to testify this change. After this, I will need to generate the C/C++ code and put back to ts_rotator_controller and test the new built. I need to think I can test this in C/C++ level in additional to the Simulink level or not.",3
"DM-23865","03/13/2020 01:27:37","Create an LCR proposing that the LSP be renamed VERA","This ticket simply captures the outcome of RFC-673; no further action is expected.",1
"DM-23873","03/13/2020 15:20:26","Typo in cp_pipe makeBrighterFatterKernel.py","Two issues:    (1) Function name in line 585 is mis-spelled.  Is spelled fitPtcAndNonlinearity and should be fitPtcAndNonLinearity.    (2) Same function in line 585 is missing  the lookupTableArray input variable.    I have verified that with these two fixes, it all runs OK.    As an aside, I'm surprised there are not automated checks to check for function calls to functions that do not exist!",1
"DM-23874","03/13/2020 16:36:11","Add clearError command to NewMTMount","Add a clearError command to NewMTMount that includes a field for a bitmask of which errors to clear.    Also fix moveToTarget command to eliminate the unwanted azimuth direction flag and clean up the axis state enums a bit.",0
"DM-23875","03/13/2020 17:27:18","Update logEvent message for Script and LOVE","The logMessage topic is defined in several places: generics, Script and LOVE, at least.    I recently updated the generic version but forgot to update the other versions.    Note: we can get rid of the copies once SAL 4.1 is released. It will require an update to SALSubsystems.xml to list which generic topics Script and LOVE use.",0
"DM-23885","03/16/2020 15:32:36","cRIO FPGA operation","Learn how are command passed from ts_m1m3support C/C++ controller to FPGA, and replies passed back.",2
"DM-23894","03/16/2020 19:55:12","coaddAnalysis: RuntimeError: No good data points to plot for sample labelled: star","In one edge tract/patch of the HSC-PDR2 DEEP+UDEEP,  coaddAnalysis gave this error:     {noformat}  coaddAnalysis.py /datasets/hsc/repo/ --calib /datasets/hsc/calib/20200115/ --rerun DM-23243/MULTIBAND/DEEP:private/hchiang2/testCoaddAna20200316 --doraise --config doWriteParquetTables=True --id tract=09572 filter=HSC-G  {noformat}      {noformat}  root INFO: Running: /home/hchiang2/PDR2/pa_stack_2/pipe_analysis/bin/coaddAnalysis.py /datasets/hsc/repo/ --calib /datasets/hsc/calib/20200115/ --rerun DM-23243/MULTIBAND/DEEP:private/hchiang2/testCoaddAna20200316 --doraise --config doWriteParquetTables=True --id tract=09572 filter=HSC-G  coaddAnalysis INFO: patchList size: 3  coaddAnalysis INFO: External calibration(s) used: photoCal: FGCM  wcs: JOINTCAL  coaddAnalysis INFO: shortName = overlap_modelfit_CModel_forced  coaddAnalysis INFO: No data for dataset: unknown  coaddAnalysis INFO: Statistics from DataId(initialdata={'tract': 9572, 'filter': 'HSC-G', 'patch': '8,6'}, tag=set()) of   Overlap mag difference (CModel) (mmag): {'galaxy': Stats(mean=nan; stdev=nan; num=0; total=0; median=nan; clip=nan; forcedMean=nan; thresholdType=S/N; thresholdValue=300.0), 'split': Stats(mean=nan; stdev=nan; num=0; total=0; median=nan; clip=nan; forcedMean=nan; thresholdType=S/N; thresholdValue=300.0), 'star': Stats(mean=0.0000; stdev=127.7816; num=2; total=2; median=0.0000; clip=378.2334; forcedMean=None; thresholdType=S/N; thresholdValue=300.0), 'unknown': Stats(mean=nan; stdev=nan; num=0; total=0; median=nan; clip=nan; forcedMean=nan; thresholdType=S/N; thresholdValue=300.0)}  coaddAnalysis WARN:   Overlap mag difference (CModel) (mmag) stdev = 127.78 exceeds maximum limit of 3.00: DataId(initialdata={'tract': 9572, 'filter': 'HSC-G', 'patch': '8,6'}, tag=set())  coaddAnalysis INFO: shortName = overlap_distance_forced  coaddAnalysis INFO: No data for dataset: unknown  coaddAnalysis INFO: Statistics from DataId(initialdata={'tract': 9572, 'filter': 'HSC-G', 'patch': '8,6'}, tag=set()) of Distance (mas): {'galaxy': Stats(mean=nan; stdev=nan; num=0; total=0; median=nan; clip=nan; forcedMean=nan; thresholdType=S/N; thresholdValue=300.0), 'split': Stats(mean=nan; stdev=nan; num=0; total=0; median=nan; clip=nan; forcedMean=nan; thresholdType=S/N; thresholdValue=300.0), 'star': Stats(mean=458.1745; stdev=458.1745; num=2; total=2; median=458.1745; clip=0.0000; forcedMean=0.0; thresholdType=S/N; thresholdValue=300.0), 'unknown': Stats(mean=nan; stdev=nan; num=0; total=0; median=nan; clip=nan; forcedMean=nan; thresholdType=S/N; thresholdValue=300.0)}  coaddAnalysis WARN: Distance (mas) stdev = 458.17 exceeds maximum limit of 5.00: DataId(initialdata={'tract': 9572, 'filter': 'HSC-G', 'patch': '8,6'}, tag=set())  coaddAnalysis INFO: Number of forced overlap objects matched = 170  coaddAnalysis INFO: shortName = overlap_modelfit_CModel_unforced  Traceback (most recent call last):    File ""/home/hchiang2/PDR2/pa_stack_2/pipe_analysis/bin/coaddAnalysis.py"", line 3, in <module>      CoaddAnalysisTask.parseAndRun()    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 610, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 221, in run      resultList = list(mapFunc(self, targetList))    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 385, in __call__      result = self.runTask(task, dataRef, kwargs)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/pipe_base/19.0.0-9-g0ae078d+2/python/lsst/pipe/base/cmdLineTask.py"", line 447, in runTask      return task.runDataRef(dataRef, **kwargs)    File ""/home/hchiang2/PDR2/pa_stack_2/pipe_analysis/python/lsst/pipe/analysis/coaddAnalysis.py"", line 318, in runDataRef      highlightList=highlightList, **plotKwargs)    File ""/home/hchiang2/PDR2/pa_stack_2/pipe_analysis/python/lsst/pipe/analysis/coaddAnalysis.py"", line 1051, in plotOverlaps      unitScale=self.unitScale,    File ""/home/hchiang2/PDR2/pa_stack_2/pipe_analysis/python/lsst/pipe/analysis/analysis.py"", line 229, in __init__      format(labeller.plot[0]))  RuntimeError: No good data points to plot for sample labelled: star  {noformat}      Could this edge case be handled more gracefully, such as a softer warning and skipping plotting the patch rather than an error, or other approaches?     The {{pipe__analysis}} version is at commit {{c11be5b}} with stack w_2020_08. ",3
"DM-23898","03/16/2020 21:25:37","Test MTMount CSC with the real camera cable wrap and/or the PXI simulator","Test the new MTMount CSC with the real camera cable wrap and/or the PXI simulator (preferably both) and fix any problems that show up.",3
"DM-23903","03/17/2020 15:49:52","Update OpenSpliceDDS repo and rpms with bug fix","Update the community editioh of OpenSplice  V6.9 with the pydds memory leak fix provided by ADlink. Build and distrubute RPM and update Nexus and github repos",2
"DM-23904","03/17/2020 15:52:18","SAL release 4,1 candidate","Test and create SAL 4.1.1 release candidate branch in github. Test with python 3.7.6 and bug fixed version of OpenSplice 6.9 , and XML 4.8",3
"DM-23920","03/18/2020 16:30:08","M1M3 On-going Learning and Updates","This epic is to capture the on-going learning effort of Petr for the M1M3 and capture any work he does on the M1M3 Support system.",20
"DM-23922","03/18/2020 17:35:34","Add Dome _labels.yaml","In order for the unit tests of ts_Dome to pass, an empty _labels.yaml is needed.",1
"DM-23925","03/18/2020 23:37:09","Implement Jenkins parallel stage functionality to the SAL Release job","As demonstrated in the work on DM-23106, the parallel stage functionality is a real time saver.  Implement this same strategy in the SAL release job, also parallelizing the messaging tests.",2
"DM-23931","03/19/2020 17:46:09","Allow butler.makeRepo to complain if a config already exists","There are many times where we can call makeRepo in cases where a butler repository already exists (or at least the config file exists). It would be useful for makeRepo to be able to complain if that is the case.",1
"DM-23933","03/19/2020 18:05:49","Produce and consume communication strings with YAML","We are still reviewing the use of YAML in stead of using plain semi-colon separated strings for the communication between the upper and lower level components but I am going ahead and will start implementing YAML already since this is the way we will go.    EDIT: YAML has been accepted as the preferred communications protocol.",3
"DM-23940","03/19/2020 22:12:50","Add CLOCK_TAI support to time in Python","I filed https://github.com/astropy/astropy/issues/10052 to request CLOCK_TAI support in the time module in Python and was told they would likely accept a patch. Provide one.",1
"DM-23946","03/20/2020 02:43:58","Change localWcs plugins to use a localGnomonicWcs transform.","[~jbosch] found that the current functor for localWcs would not return a valid transform when far from the Wcs center as it uses getCdMatrix/getTANWcs which behave poorly when far from the Wcs origin/do not utilize larger distortions. This ticket will change the plugins and functors to use linearizePixelToSky instead.",8
"DM-23949","03/20/2020 12:45:17","Changes FPGA to singleton","FPGA object is passed as parameter to multiple classes constructors. Its much better to implement FPGA as Singleton (as it's completely unexpected we will ever run M1M3 support on cRIOs with multiple FPGAs) and use it this way, removing multiple passing of pointer. This will also facilitate transaction from FPGA class to IFPGA, its abstract parent, which can be either real FPGA or simulated FPGA.",2
"DM-23950","03/20/2020 12:47:23","Change configurations path","Change M1M3 support to use configured path instead of hardcoded.",2
"DM-23952","03/20/2020 14:01:51","Remove scarlet and proxmin from meas_deblender","When we  upgraded to scarlet 0.5 in early 2019 we decided that rather than try to place more bandaids on {{meas_deblender}} we would create the new {{meas_extensions_scarlet}} package. In doing so {{meas_deblender}} was left alone, which still included dependencies on {{scarlet}} and {{proxmin}} with code that likely has bit rotted. So this ticket is just to remove all references to {{scarlet}} and {{proxmin}} from {{meas_deblender}} and delete the dependencies, as they be related to an obscure failure in the build system.",1
"DM-23953","03/20/2020 14:35:03","Allow logging into file(s)","Change M1M3 support logging to allow for logging into a file - the current logging assumes it's logging to stdout, and prints out escapes for color logging.",2
"DM-23956","03/20/2020 17:14:45","Audit sample-avro-alert & alert_stream repositories","Take a look through the lsst-dm/sample-avro-alert & lsst-dm/alert_stream repos. Understand what's there, and think about whether you understand it and would be able to maintain it. File tickets for any changes that you'd like to make in the short term (and think about longer term evolution, which we should probably discuss in person). Take a look at the ticket backlog on those repos, and see if those tickets seem relevant/obsolete/unclear/etc.",2
"DM-23958","03/20/2020 18:09:44","Install OpenSplice Enterprise at NCSA","Obtain license for the install OpenSliceDDS Enterprise and tools on a dedicated node   in the NCSA test stand cluster.",1
"DM-23959","03/20/2020 18:16:37","CameraMapper._standardizeExposure should patch header","When reading a {{""raw""}} dataset from the butler, we get an {{Exposure}} back. However, the metadata contained within that {{Exposure}} has not been patched. I believe {{CameraMapper._standardizeExposure}} should call {{fix_header}}.",1
"DM-23960","03/20/2020 18:48:41","Investigate options for TMA use of EOL'd NI software","Review the use of EOL'd software components by the TMA control software.  In particular NI SoftMotion module.",2
"DM-23976","03/23/2020 18:07:25","Move gen3 generic curated calibrations ingest code to obs_base","In DM-23778 I updated writeCuratedCalibrations to support QE curves as well as defects. This is generic code that uses the pipe_tasks read_all function to read standardized calibration data from {{obs_x_data}} packages.  Since by definition these calibrations are standardized we should move this code to obs_base and ensure that all gen3 instruments call it if they have defined an _data package.  The relevant dataset types and curated calibrations would only be created/ingested if they exist in the _data package (which by definition means they are standardized).  The instrument subclasses should only be required to define their non-standard curated calibrations.",2
"DM-23977","03/23/2020 18:37:48","Move Python code from ts_m1m3supporteui into own repository","Move Python code from labview ts_m1m3supporteui feature/Python branch into develop branch of ts_m1m3supportgui.",1
"DM-23980","03/23/2020 21:03:43","Standardize Gen3 instrument class names and location","Currently the gen3 instrument classes have no consistency in naming or module path. This adds to the confusion. Currently we have:    * lsst.obs.subaru.gen3.hsc.instrument.HyperSuprimeCam  * lsst.obs.decam.instrument.DarkEnergyCamera  * lsst.obs.lsst.gen3.instrument.LatissInstrument  * lsst.obs.lsst.gen3.instrument.Ts8Instrument  * lsst.obs.lsst.gen3.instrument.Ts3Instrument  * lsst.obs.lsst.gen3.instrument.UcdCamInstrument  * lsst.obs.lsst.gen3.instrument.PhosimInstrument  * lsst.obs.lsst.gen3.instrument.ImsimInstrument  * lsst.obs.lsst.gen3.instrument.LsstComCamInstrument  * lsst.obs.lsst.gen3.instrument.LsstCamInstrument    corresponding to instrument names of HSC, DECam, LATISS, LSST-TS8, LSST-TS3, LSST-UCDCam, PhoSim, LSST-ImSim, LSST-ComCam, lsstCam.    Instrument names have to be unique in Gen3 since telescope is not part of the data model. This is why TS8 is not TS8. lsstCam should probably be LSSTCam.    This ticket will try to clean things up a bit.    obs_cfht has no gen3 support at this time.",1
"DM-23983","03/23/2020 22:45:05","Cannot apply crosstalk in Gen 3 DECam processing","{{IsrTask}} delegates complex crosstalk corrections to {{CrosstalkTask}}. However, the Gen 3 call to {{CrosstalkTask}} is [disabled|https://github.com/lsst/ip_isr/blob/5673ca6e3699f4db46772b3e01c59e44131b93bf/python/lsst/ip/isr/isrTask.py#L870-L874], citing DM-17169. Because {{CrosstalkTask}} does not run, Gen 3 calls to {{IsrTask}} will fail unless the configuration includes {{doCrosstalk=False}}.    Please make it possible to run {{CrosstalkTask}} in Gen 3 and re-enable it.",20
"DM-23991","03/24/2020 17:04:41","JUPYTERHUB_SERVICE_PREFIX environment variable not passed to workflow environment","Trying to run the all sky gaia dask notebook fails in the workflow system because the {{JUPYTERHUB_SERVICE_PREFIX}} environment variable is not set in the workflow environment.  This causes dask to throw an exception when it tries to construct the path to the scheduler dashboard interface.    I have confirmed that variable is set in an interactive JupyterLab running {{w_2020_12}} on stable.",5
"DM-23992","03/24/2020 17:53:17"," Cannot load refcats in Gen 3 DECam processing","Attempting to run the Gen 3 single-frame pipeline as described in [DM-23616|https://jira.lsstcorp.org/browse/DM-23616?focusedCommentId=240326#comment-240326] leads to {{AstrometryTask}} crashing with the error ""No reference tables could be found for input region"". Running with {{--show graph}} reports that no photometric or astrometric reference catalog shards are included in the quanta for {{CalibrateTask}}. However, they can be retrieved from the Butler using {{Registry.queryDatasets}} (only tested with no data ID constraints).    The working hypothesis from [#dm-middleware|https://lsstc.slack.com/archives/C2JPT1KB7/p1585010272157800] is that this is a problem in ingestion (i.e., in {{convert_gen2_repo_to_gen3.py}}), so I'm filing this as a bug in {{obs_base}}.    Please fix the conversion\(?) so that a repository created as described in DM-23616 can correctly match catalogs to image data IDs.",2
"DM-23993","03/24/2020 18:11:27","Correct sending of OK reply in mock_controller","The OK reply sent from the mock_controller lacks a space right before the Timeout keyword.",1
"DM-23997","03/24/2020 18:42:58","Consolidate assertComponent and assertOK functions into one assertReply function","Since the structure of each reply received from the mock controller is similar, it is possible to have one assert method for each kind of reply instead of dedicated ones.",1
"DM-23998","03/24/2020 19:29:54","Update ts_MTMount based on new information from Tekniker","Update the ts_MTMount code based on my conversation with Alberto 2020-03-23. Changes include:  * Enforce only one command at a time.  * The source field of errors and warnings is a string of the form {{f""\{subsystem_id\}. \{text\}""}}, rather than a simple integer.  * The CSC should not command the azimuth cable wrap; let the azimuth axis take care of it. Remove the azimuth cable wrap from the simulator.  * Use the suggested startup sequence.  * Add the suggested shutdown sequence.  * Corrected the source-ish field for the error and warning replies. I had assumed it was an integer, but it turns out to be a string (that starts with an integer; it's ugly).",3
"DM-24003","03/24/2020 22:15:24","Refactor the Autotool Files in ts_rotator_controller","This task will reorganize the files in ts_rotator_controller and update the related configuration.ac and Makefile.ac. This is to improve the readability and maintainability of repository.",2
"DM-24012","03/25/2020 18:14:56","Prepare for SAL v4.1.x release - Part 1","Catch-all task to cover various activities needed to prepare for the SAL v4.1.x release.   * V4.1.1 requires Python 3.7.6, so update the Docker image accordingly.   * Cut release candidate branches and run tests.   * Implement Jenkins parallel stages in the SAL Release job   * Update SAL tests with SAL v4.1.0 changes",3
"DM-24013","03/25/2020 18:28:07","Fix bug in --output-run handling introduced in DM-21849","See [https://lsstc.slack.com/archives/C2JPT1KB7/p1585160382064500]     ",0.5
"DM-24015","03/25/2020 23:08:26","Review the Rotator Simulink Model","Review the rotator Simulink model by MOOG. This is to have an initial understanding how the code works and have the idea how to modify it. This task will remove the hard-coded paths in the model for the evaluation as well.",2
"DM-24018","03/26/2020 01:12:55","Failure to flatten or filter chained collections in queries","https://lsstc.slack.com/archives/C2JPT1KB7/p1585184879129400",0.5
"DM-24021","03/26/2020 16:14:18","Update docker containers for headerservice with ComCam","Now that we also have HeaderService for ComCam which uses the same code base as LATISS, we need to have a generic naming for the headerservice.",2
"DM-24026","03/26/2020 20:31:28","Update M1M3 simulator dockerfile to run current simulator in Docker","Dockerize ts_m1m3simulator.",2
"DM-24027","03/26/2020 21:07:30","CalibDate misinterpreted in curated calibration ingest","The metadata checking code that runs during read of curated calibrations from obs_x_data packages incorrectly assumes that CALIBDATE should match the validity start.  It turns out that CALIBDATE is meant to be a label for the calibration and is distinct from validity. Please remove the check that requires them to match.",1
"DM-24034","03/26/2020 23:11:45","Prepare for SAL v4.1.x release - Part 2","Catch-all task to cover various activities needed to prepare for the SAL v4.1.x release.  This is a continuation of DM-24012 into the next sprint   * V4.1.1 requires Python 3.7.6, so update the Docker image accordingly.   * Cut release candidate branches and run tests.   * Implement Jenkins parallel stages in the SAL Release job   * Update SAL tests with SAL v4.1.0 changes",3
"DM-24043","03/27/2020 17:52:44","Build the Lastest MTAOS Docker Image","Build the latest docker image of MTAOS. The environment of `b41` and ts_xml v4.7.0 is used.",0
"DM-24045","03/27/2020 18:59:43","Add unit tests for mock devices and the mock controller to MTMount","Add unit tests for the mock devices and the mock controller to ts_MTMount.",2
"DM-24046","03/27/2020 19:19:05","Try to use NCSA's LFA S3 service","[~felipe] gave me credentials for NCSA's S3 LFA service. Try to use these credentials from home to figure out what will be needed for our CSCs.",1
"DM-24047","03/27/2020 19:26:56","Implement S3 LFA image upload for the Fiber Spectrograph","The Fiber Spectrograph does not yet try to upload the images it takes to the LFA. Update the code to make that happen.",2
"DM-24051","03/27/2020 22:54:56","Remove workarounds for an older version of the ATMCS trackTarget command","We had to put some ugly code in ts_standardscripts in order for it to work with the version of ATMCS trackTarget command in ts_xml 4.8 and the next version. Remove that when we no longer need ts_xml 4.8.",0
"DM-24055","03/28/2020 00:11:41","Speed up table parsing in Defects","Reading defects is taking a very long time. Some of the larger defects files from DECam take over 2 seconds to read in. The bulk of this is in afw Table processing. It seems I was doing it wrong and it can be significantly optimized by preselecting keys.",1
"DM-24060","03/28/2020 18:19:51","Improve table creation efficiency in Defects","DM-24055 made a significant improvement in defect reading speed. Now the profiler is showing that there are some reasonable gains that can be made by adjusting table creation to use columns rather than per row assignment.",1
"DM-24234","03/30/2020 19:33:20","Review the Draft of LTS-162 v5","Review the draft of LTS-162 v5:    [https://www.dropbox.com/s/zgoq9zmcqtogyyz/LTS-162_v5.docx?dl=0]",0
"DM-24235","03/30/2020 19:36:27","Assist CI test and release SAL 4.1 RC2","Assist SAL 4.1 release CI process with any required tweaks anf fixes revealed during CI  testing ",3
"DM-24236","03/30/2020 19:39:19","Assist EAS software contractor and update XML","Assist the EAS contractor with building and testing the software against latest  SAL and DDS versions, rpi firmware upgrades, XML definition",1
"DM-24237","03/30/2020 19:45:39","Review proposed replacement for NI softmotion module in TMA software","Review the porposed solution (Tekniker) to replace the use of NI Softmotion Axis  calls with substitute/direct calls to OpenPLC equivalents. Assist engineers in  establishing a local TMA controller simulation environment with PXI capable PCs'",3
"DM-24238","03/30/2020 19:52:34","Check M1M3 command input parameters","M1M3 controller lack (almost any?) parameter checking. At least running M1M3 start_commander without proper Default (configuration name) will throw an error on m1m3 support. That needs to be rectified, verified and sensible messages returned.",3
"DM-24239","03/30/2020 20:00:52","M1M3 Support System Improvements","On-going list of improvements to the M1M3 support system.",40
"DM-24244","03/30/2020 22:42:20","w_2020_13 makeButlerRepo.py missing left parens error with Oracle","Doing weekly run of ci_hsc_gen3 using weekly stack and Oracle for w_2020_13.   makeButlerRepo.py dies with:    {noformat}  python /usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/bin/makeButlerRepo.py /work/mgower/gen3work/weeklyCItest/git/ci_hsc_gen3/DATA  Traceback (most recent call last):    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1249, in _execute_context      cursor, statement, parameters, context    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 552, in do_execute      cursor.execute(statement, parameters)  cx_Oracle.DatabaseError: ORA-00906: missing left parenthesisThe above exception was the direct cause of the following exception:Traceback (most recent call last):    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/bin/makeButlerRepo.py"", line 28, in <module>      sys.exit(main())    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/script/makeButlerRepo.py"", line 91, in main      makeButlerRepo(args.root, args.config, args.standalone, args.override, args.outfile)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/script/makeButlerRepo.py"", line 82, in makeButlerRepo      outfile=outfile)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/_butler.py"", line 381, in makeRepo      Registry.fromConfig(config, create=createRegistry, butlerRoot=root)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/registry/_registry.py"", line 178, in fromConfig      create=create)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/registry/_registry.py"", line 191, in __init__      self._opaque = opaque.initialize(self._db, context)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-984c9f7/lib/python3.7/contextlib.py"", line 119, in __exit__      next(self.gen)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/registry/interfaces/_database.py"", line 384, in declareStaticTables      self._metadata.create_all(self._connection)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/sql/schema.py"", line 4294, in create_all      ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1615, in _run_visitor      visitorcallable(self.dialect, self, **kwargs).traverse_single(element)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/sql/visitors.py"", line 132, in traverse_single      return meth(obj, **kw)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 781, in visit_metadata      _is_metadata_operation=True,    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/sql/visitors.py"", line 132, in traverse_single      return meth(obj, **kw)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 826, in visit_table      include_foreign_key_constraints,    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 988, in execute      return meth(self, multiparams, params)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/sql/ddl.py"", line 72, in _execute_on_connection      return connection._execute_ddl(self, multiparams, params)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1050, in _execute_ddl      compiled,    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1253, in _execute_context      e, statement, parameters, cursor, context    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1473, in _handle_dbapi_exception      util.raise_from_cause(sqlalchemy_exception, exc_info)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 398, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb, cause=cause)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/util/compat.py"", line 152, in reraise      raise value.with_traceback(tb)    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/base.py"", line 1249, in _execute_context      cursor, statement, parameters, context    File ""/usr/local/lsst_stack/w_2020_13/stack/miniconda3-4.7.12-984c9f7/Linux64/sqlalchemy/1.3.8/lib/python/SQLAlchemy-1.3.8-py3.7-linux-x86_64.egg/sqlalchemy/engine/default.py"", line 552, in do_execute      cursor.execute(statement, parameters)  sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-00906: missing left parenthesis  [SQL:   CREATE TABLE quantum (   id NUMBER(19),    task VARCHAR2(256 CHAR),    start_time NUMBER(19),    end_time NUMBER(19),    host VARCHAR2(64 CHAR),    run_name VARCHAR2 NOT NULL,    PRIMARY KEY (id),    CONSTRAINT fkey_quantum_run_name_run_name FOREIGN KEY(run_name) REFERENCES run (name) ON DELETE CASCADE  )]  (Background on this error at: http://sqlalche.me/e/4xp6)  scons: *** [DATA/butler.yaml] Error 1  scons: building terminated because of errors.  {noformat}",1
"DM-24247","03/30/2020 23:56:38","butler validation error in ci_hsc_gen3","[~npease] ran the butler config validation on the ci_hsc_gen3 output repo and got the following report:  {code}  Template failure with key 'default': Template '{run:/}/{datasetType}.{component:?}/{tract:?}/{patch:?}/{label:?}/{abstract_filter:?}/{subfilter:?}/{physical_filter:?}/{visit:?}/{datasetType}_{component:?}_{tract:?}_{patch:?}_{label:?}_{abstract_filter:?}_{physical_filter:?}_{calibration_label:?}_{visit:?}_{exposure:?}_{detector:?}_{instrument:?}_{skymap:?}_{skypix:?}_{run}' is inconsistent with DatasetType(ps1_pv3_3pi_20170110, {htm7}, SimpleCatalog): {'exposure', 'patch', 'skypix', 'abstract_filter', 'tract', 'label', 'physical_filter', 'detector', 'instrument', 'calibration_label', 'skymap', 'subfilter', 'visit'} is not a superset of {'htm7'}.  {code}    Fix the template.",0.5
"DM-24250","03/31/2020 00:31:17","Improve startup speed of fgcmcal and add checkpointing for restarts","The recent large runs of {{fgcmcal}} on HSC data have shown that (a) in certain use cases the startup time is a factor of a few slower than it could/should be, and (b) certain files can be checkpointed to allow faster restart in case of transient file system failures, out of memory issues, etc.  I have a branch {{/u/erykoff/fasterstart}} that implements these changes and will be cleaned up.",2
"DM-24262","03/31/2020 17:52:08","Run HSC AP processing in CI using Gen 3","Add an HSC Gen 3 dataset to the {{scipipe/ap_verify}} Jenkins job. The job should already be designed to let us plug in more datasets, so the main work would be specifying a Gen 3 run (requires adding an internal flag to the Jenkins job?) and working with Groovy.",8
"DM-24276","04/01/2020 15:29:36","update LPM-251 ","Perhaps include the usage table from [PSTN-003.lsst.io|http://pstn-003.lsst.io/]  perhaps one of the sizing cost tables - With a ref to DMTN-135 ..     We could also add the Check LIst for iDacs .. discussed in CEC meeting..",1
"DM-24277","04/01/2020 17:45:37","Apply proper motion and parallax while loading refcats in Jointcal","There's existing code in the stack to apply proper motions and parallaxes from the reference catalog. Find it, dust it off, and get it going in Jointcal. It may require some modification. If it requires substantial overhauls (e.g. rewriting completely because it's too slow), let's make a new ticket to capture that.    For now, we will correct the reference catalog to the mean epoch of the loaded data.",8
"DM-24285","04/01/2020 20:57:08","fitsExposureFormatter fails to read ""Exposure"" entries correctly","Copying from slack so I don't get confused in the future:  {code}  [...]    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/formatters/fitsExposureFormatter.py"", line 138, in readFull      return fileDescriptor.storageClass.pytype(fileDescriptor.location.path, **parameters)  File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/utils/19.0.0-7-g686a884+4/python/lsst/utils/wrappers.py"", line 313, in __call__      raise TypeError(""No registered subclass for {}."".format(d))  TypeError: No registered subclass for {'dtype': None}.  [...]    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/_butler.py"", line 693, in getDirect      return self.datastore.get(ref, parameters=parameters)    File ""/software/lsstsw/stack_20200220/stack/miniconda3-4.7.12-984c9f7/Linux64/daf_butler/19.0.0-41-g64a679d4/python/lsst/daf/butler/datastores/posixDatastore.py"", line 151, in get      raise ValueError(f""Failure from formatter '{formatter.name()}' for Dataset {ref.id}"") from e  ValueError: Failure from formatter 'lsst.daf.butler.formatters.fitsExposureFormatter.FitsExposureFormatter' for Dataset 945  ￼{code}  {quote}  Oh, wait, that's just invoking storageClass.pytype(...) (which will never work with just Exposure sans suffix) instead of using ExposureFitsReader.  15:30  This is a bug in the formatter, I think.  {quote}",1
"DM-24292","04/02/2020 00:12:39","Review the Configuration Manager by MOOG","Review the configuration manager by MOOG. This is a LabVIEW project.",2
"DM-24319","04/03/2020 00:18:41","Add support for new parquet source tables in fgcmcal","With new parquet source tables soon to be added to production, these should be supported in {{fgcmcal}} which should greatly speed up the i/o part of the operations.  The plan is that new methods will be written in {{fgcmBuildStarsTask}} that will make use of the source tables, meaning there will be some slight code duplication.  However, since the old method of reading source catalog fits files will be deprecated by this ticket, when the deprecation period is complete the old code can be removed much more cleanly.",8
"DM-24330","04/03/2020 05:27:09","add ability to run an obs_base command via the butler command","Make the butler command in daf_butler able to call the register command in obs_base (at python/lsst/obs/base/instrument.py).    To do this, when obs_base is setup, it should add a colon separated list of CLI python code modules or functions. For example set it to {{lsst.obs.base.cli}}.    Then in butler command itself do something like:  {code:java}  for module in os.environ[""DAF_BUTLER_PLUGINS""].split("":""):   blah = doImport(cls) {code}  and maybe have the imported class import the subcommands to click somehow   ...it may help to be explicit and have the env var contain the actual command function (so lsst.obs.base.cli.register or somesuch). ",8
"DM-24333","04/03/2020 16:21:29","Update MTMount to not send TCS_sequence_id","It turns out that the TCS sequence ID field is only used by the Operation Manager communicating with the PXI. It is not used by the CSC, HHD and EUI connecting to the Operation Manager. It also strips that field from replies before sending them to the CSC, HHD or EUI.    Update ts_MTMount accordingly.    Also update the mirror cover open/close commands to the sequence provided by Alberto:    To deploy the mirror covers (protect the mirror):  * Deploy mirror cover locking pins (1506 = MCL_MOVE_ALL, drive=-1, on=1)  * Deploy mirror covers (905 = MC_OPEN, drive=-1)  To retract the mirror covers (for observing):  * Retract mirror covers (906 = MC_CLOSE, drive=-1)  * Retract mirror cover locking pins (1506 = MCL_MOVE_ALL, drive=-1, on=0  ",1
"DM-24337","04/03/2020 20:37:29","Fix raw formatter gen3 breakage","In DM-24285 we made reading components from exposures very efficient but inadvertently broke reading of components from raw files because the API was changed. Nothing in lsst_distrib or lsst_ci noticed and it wasn't until ci_hsc_gen2 that the breakage was discovered.    In that test we showed that reading raw.wcs does not give you the same thing as reading the raw and then calling {{getWcs}}.  This is because the previous implementation did a full read and component extraction but now components are all handled by ExposureFitsReader.",1
"DM-24347","04/03/2020 22:37:46","Allow component gets in gen3 butler to be None","When a component is being requested from a dataset type, it is entirely possible for that particular dataset that the component will be None. At the moment we do not allow that and it is an error to return None because the python types do not match.    Change datastore such that if a component is being requested then None is an acceptable value.  Once this is supported fix the ingest test in obs_base that skips Wcs component test.",1
"DM-24350","04/03/2020 23:11:24","Add EFD notebooks to CI","We currently have a few example efd notebooks.  These should be converted to use the modern utility classes and added to the list of CI'd notebooks.  Now that there is EFD data available from the EFD on int and visible from stable, this is now something we can do.    I would also like to promote the notebooks to the prod branch so that they show up automatically.  That may be thwarted by the fact that the correct data may not be available everywhere.    Note: We did not promote to prod on this ticket.  That will be handled in DM-24410.",2
"DM-24352","04/04/2020 00:04:37","Add auto transfer mode to gen3 ingest","When converting gen2 repositories to gen3 we currently default to using symlink mode. This means that in-place conversions can't work and also means that a user can't convert to an S3 bucket.    I think we need a couple of changes that would help a lot with the right decision being made:    # Add a new ""link"" option that tries a hardlink and falls back to symlink  # Add an auto mode to posix datastore that will check to see if the file is within the repository and use None if it is, else it will use ""link"".  # Make ""auto"" an alias for ""copy"" in S3 datastore.    With these changes the 2to3 conversion can use ""auto"" mode and that should do the right thing most of the time.",2
"DM-24361","04/06/2020 16:49:47","Allow the Connection Message in the MT Hexapod PXI Controller","Allow the connection message (GUI or DDS) in hexapod PXI code.",0
"DM-24364","04/06/2020 21:45:27","Please simplify the way ts_sal finds PYTHON includes","Presently ts_sal salgenerator appears to find Python.h by using {{/include/python$PYTHON_BUILD_VERSION}}. This is a bit of a problem for several reasons:  * It assumes that /include exists  * It is a bit confusing as it's not clear what PYTHON_BUILD_VERSION is being used for.    I suggest offering a new env variable PYTHON_INCLUDE (or similar) that is the full path to the directory containing Python.h. I think it would be much clearer what the env variable was used for.    You could fall back to PYTHON_BUILD_VERSION if PYTHON_INCLUDE was not defined, for backwards compatibility.    Two more small requests to consider (in an attempt to rationalize the env vars):  * Rename LSST_SDK_INSTALL to something a bit clearer. It appears to be the root directory of ts_sal, so perhaps TS_SAL_ROOT? If you do that then do you need SAL_HOME? It is a known subdirectory of ts_sal.  * Move your bin scripts (such as salgenerator) to the bin/ directory. That's what it's there for. Then we just have to get ts_sal/bin on our PATH and they can be found. The current location is surprising and deeply buried. At that point you can probably lose SAL_DIR.",1
"DM-24365","04/06/2020 21:58:17","Add relative symbolic link transfer mode to Gen 3 ingest","When converting gen 2 to gen 3 repositories, the current options are in-place conversion, hard links, or absolute symbolic links. There are cases (e.g., Git repositories) where links are desirable but neither hard links nor absolute paths are portable enough. A ""relsymlink"" mode would be useful for these cases.    Because it's intended only for when very specific behavior is desired, ""relsymlink"" probably does not make sense as an option in either the ""auto"" or ""link"" transfer modes.",1
"DM-24370","04/06/2020 23:33:31","Support extensible scheduling in pipetask","AP pipeline want to have order of visit processing processing to be stable, in particular association steps for one visit needs to wait until the same step for previous visit is finished. Adding this sort of dependency for quantum graph would be difficult. It may be easier to implement special scheduling logic while executing the graph instead. I want to explore how this can be done in current pipetask and try to make it scheduling more configurable/extensible. Some refactoring (that we always wanted to do) of the internals will probably be needed.",2
"DM-24371","04/07/2020 05:40:28","Implement fixed correction fixed PSF support decorrelation afterburner","This is the first code piece in the decorrelation afterburner rewrite.    For helping the review, the whitening concept and the whitening kernel are demonstrated in DM-24316.     ",20
"DM-24375","04/07/2020 17:30:58","Check time round trip issues with new time format","Jim reported issues with round-tripping and comparison of timestamps in slack: https://lsstc.slack.com/archives/C2JPT1KB7/p1586273799139100  Need to understand what's causing this and to fix it.",2
"DM-24378","04/07/2020 21:23:43","Store instrument class with gen3 instrument registration and add API","When designing the butler command line tools it is clear that life would be much simpler if an instrument could be referred to by name rather than by instrument class. It should only be instrument registration that requires the full class name. All other operations should work by specifying the name, looking up the name in registry and retrieving the class name, using doImport on that class, and instantiating the class for downstream usage.    For this to work we need two changes:    # Add the instrument class full name text string to registry.  # Have an API ({{butler.registry.getInstrument}}?) that will return an instantiated gen3 Instrument (they don't need parameters) given the name of the instrument.    ",1
"DM-24383","04/08/2020 01:01:53","Update Firefly proxy container image to ACMEv2 compliance","The Apache proxy for Firefly provided by IPAC to LSST has fallen out of date with the ACME protocol used for certificate transactions.    It must be upgraded to ACMEv2 in order to continue to function.    Please generate an updated container image on DockerHub at {{ipac/proxy}}.  To support future configuration control activities, please apply the tags ""latest"" and ""1.1.0"" to this image, and, if possible, apply the ""1.0.0"" tag to the previously available image.",1
"DM-24385","04/08/2020 15:45:24","Optimize posixStorage.search with relative path","It turns out that a lot of the overhead in running {{dataRef.datasetExists()}} is extraneous path checking when the template is a relative path.  A quick change to the logic in {{posixStorage.search}} can speed up this operation by ~60% in tests, which is useful when finding large numbers of files (especially, but not limited to, global calibration in {{fgcmcal}}).",0.5
"DM-24387","04/08/2020 17:20:09","Refactor duplicate code and add a task scheduler","There is some duplicate code in particularly the test code. Review and refactor. Also a task scheduler is needed to poll the status command on a regular basis. Add it.",1
"DM-24392","04/08/2020 19:50:46","Update testdata_jointcal to include Gaia+PS1 refcats","Preparatory to DM-17597, we need Gaia+PS1 refcats (sub-selected from the current refcats) for each of the datasets in testdata_jointcal. This ticket is to extract the necessary shards from the refcats in {{lsst-dev:/datasets}} and put them into the relevant subdirectories in testdata_jointcal.    This ticket does not involve any reprocessing of the test data, nor does it involve changing jointcal or fgcmcal to use the new refcats (since that would change the measured metrics).",2
"DM-24393","04/08/2020 20:47:33","Write test for salgenerator IDL step","The salgenerator has an IDL step that needs to be tested.  Add this in the [https://github.com/lsst-ts/robotframework_salgenerator] repo and get it running on Jenkins.",2
"DM-24394","04/08/2020 20:49:10","Write test for salgenerator GENERATE step","The salgenerator now has the GENERATE option, that runs validate, html, cpp and idl processes in one command line option.  Write a test to validate this step.",2
"DM-24397","04/08/2020 23:52:52","fix db2authors for SPIE","Thought I had this but it needs a bit more work ..",1
"DM-24404","04/09/2020 02:33:44","Squash astropy ecsv read warnings","I'd certainly believe that these warnings mask a real problem, but it's already been reported upstream (https://github.com/astropy/astropy/issues/8673), and they make it extremely difficult to develop against tests in obs_packages - {{test_convert2to3.py}} in obs_subaru emits 1700 lines of these, for example.    {{warnings.catch_warnings}} + {{simplefilter}} seems to take care of the problem.     ",1
"DM-24406","04/09/2020 04:36:05","M1M3 hardware simulator","Run M1M3 cRIO with ICL hardware simulator.",3
"DM-24410","04/09/2020 17:41:35","Promote EFD notebooks to prod","In DM-24350 we sanitized some EFD notebooks to run on nublado hosted at the LDF.  We intended to promote those notebooks to the standard env that all users see, but I hadn't thought through how authentication would be handled in that context.  In order to do that, we need to make a read only user and inject those credentials into user pods at spawn time.  This ticket is to figure that workflow out.",1
"DM-24414","04/09/2020 19:37:29","Implement --prune-replaced option in ctrl_mpexec","This option was added to the argument parser on DM-21849 but not implemented due to problems with dataset deletion.",2
"DM-24421","04/09/2020 21:15:32","Adapt the Change of ts_xml and ts_salobj","Adapt the change of ts_xml 5.0, especially for the MTM2. This task will need to prepare the appropriate software environment for the test. This task will also use the test framework by ts_salobj.",2
"DM-24422","04/09/2020 21:29:10","Support the DOF Correction from Multiple Visits Instead of Single One","Support the DOF (degree of freedom) correction from multiple visits instead of single one. The initial idea now is to use the queue in Model class in ts_MTAOS.",3
"DM-24423","04/09/2020 21:44:49","Use the Latest Bending Mode And Sensitivity Matrix for ts_ofc","Bo updated the bending modes and sensitivity matrix for the phosim:    [https://github.com/lsst-ts/phosim_syseng4/pull/1]    He tried to update the related part in ts_ofc:    [https://github.com/lsst-ts/ts_ofc/pull/19]    This task will take over this PR and finish it. This task will fix some minor bugs as well. It is noted that some test cases in ts_MTAOS will need to update as well.",3
"DM-24428","04/10/2020 00:51:25","Update fgcmcal tests to use new PS1 refcats","When DM-17597 is complete, a new PS1 reference catalog will be available in {{testdata_jointcal}}.  The {{fgcmcal}} tests will be updated to make use of the improved reference catalog (and updated catalog format).",2
"DM-24429","04/10/2020 01:20:09","Investigate change in fracDiaSourcesToSciSources in ap_verify CI","Between 26 & 27 March, the {{fracDiaSourcesToSciSources}} (“Ratio of DIASources to Direct Image Sources per CCD”) being tracked by Chronograf very ap_verify_ci_hits2015 changed for unknown reasons. What happened?",1
"DM-24434","04/10/2020 17:49:53","symlink bug in posixDatastore.py on Ubuntu","I received an error when rebuilding the stack, related to the new ""smart"" symlink mode:    {code}  E       FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpxhaie4yi/raw/DECam/raw/g/g DECam SDSS c0001 4720.0 1520.0/411371/raw_g_g DECam SDSS c0001 4720_0 1520_0_411371_411371_56_DECam_raw_DECam.fits'  ../../lsstsw/stack/Linux64/daf_butler/19.0.0-50-g09080728+1/python/lsst/daf/butler/datastores/posixDatastore.py:400: FileNotFoundError  {code}    This was apparently due to attempting to hard link a symlink in the `transfer=='link'` mode in posixDatastore. [~tjenness] suggested a fix by using `os.path.realpath(fullPath)` at all calls to `link`.",1
"DM-24435","04/10/2020 19:26:45","Freezing a config locks the registry(ies) of other instances of that config","Config field instances that correspond to a {{RegistryField}} have an internal member, {{_field}}, pointing back to the original field. {{_field}} points to the same object for all config instances corresponding to the same config. This causes a rare bug where calling {{ConfigChoiceField.freeze}} locks the registry for all config instances using that {{RegistryField}}, including instances that did not exist at the time of the call. See my [post on #dm|https://lsstc.slack.com/archives/C2JPL2DGD/p1586539537027000] for a specific example.    Please change the code so that independently created config instances no longer affect each other. This issue appears to be a side effect of the fix for DM-17757, and may be difficult to fix without introducing still other side effects.",8
"DM-24446","04/13/2020 18:12:24","Complete the Java versioning implementation","Most of the work is done to get the versioning for Java correct.  But now those files need to be pushed to Nexus.  The jobs will need to be update accordingly.  This task covers that work.",3
"DM-24450","04/13/2020 20:09:08","Fix parsing of wavefront sensors","The wavefront sensors need to be defined as a separate kind of raft since the corner rafts contain multiple kinds of sensors, and currently yaml camera assumes all rafts are homogenous in their detector types.  To do this the yaml files append a 'W' to mark wavefront sensor rafts, but since the in memory camera doesn't care about this distinction, we can strip it out when building the full camera yaml.    [This|https://github.com/lsst/obs_lsst/blob/master/python/lsst/obs/lsst/script/generateCamera.py#L236] is where the stripping happens.  Instead of assuming a delimiter, I think I'd like to go to an explicit name translation mapping: e.g. {{output_name = nameTranslator.get(input_name, input_name)}}.",2
"DM-24459","04/13/2020 23:16:34","Remove ignored fields from my CSCs","SAL 4.2 will allow topics to have no user-added fields. Update the XML for my CSCs accordingly, especially Test, because that will exercise this capability.    This will have to wait until we have switched our test stands and site to SAL 4.2    Consider ditching the Alias tags at the same time.",0
"DM-24461","04/13/2020 23:48:56","Segfault in salobj unit test using SALPY_Test","The following segfaults in ts_salobj v5.8 using SALPY_Test generated by ts_sal v4.0:  {code}  pytest -v tests/*_to_either.py  {code}  I suspect an issue in ts_sal, but I am not positive.    Here is a log:  {code}  ================================================================================ test session starts ================================================================================  platform linux -- Python 3.7.2, pytest-4.5.0, py-1.8.0, pluggy-0.9.0 -- /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/bin/python  cachedir: .pytest_cache  rootdir: /home/saluser/tsrepos/ts_salobj, inifile: setup.cfg  plugins: session2file-0.1.9, flake8-1.0.4, xdist-1.29.0, subtests-0.3.0, remotedata-0.3.1, openfiles-0.3.2, forked-1.0.2, doctestplus-0.3.0, cov-2.7.1, asyncio-0.10.0, arraydiff-0.3, aiomisc-9.6.36  collected 6 items                                                                                                                                                                       tests/test_salobj_to_either.py::FLAKE8 SKIPPED                                                                                                                                [ 16%]  tests/test_salobj_to_either.py::SALPYTestCase::test_salobj_remote_salobj_controller PASSED                                                                                    [ 33%]  tests/test_salobj_to_either.py::SALPYTestCase::test_salobj_remote_salpy_controller PASSED                                                                                     [ 50%]  tests/test_salpy_to_either.py::FLAKE8 SKIPPED                                                                                                                                 [ 66%]  tests/test_salpy_to_either.py::SALPYTestCase::test_salpy_remote_salobj_controller PASSED                                                                                      [ 83%]  tests/test_salpy_to_either.py::SALPYTestCase::test_salpy_remote_salpy_controller PASSED                                                                                       [100%]    ================================================================================= warnings summary ==================================================================================  /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/lib/python3.7/site-packages/socks.py:58    /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/lib/python3.7/site-packages/socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working      from collections import Callable    /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/lib/python3.7/site-packages/moto/cloudformation/parsing.py:407    /opt/lsst/software/stack/python/miniconda3-4.7.10/envs/lsst-scipipe-4d7b902/lib/python3.7/site-packages/moto/cloudformation/parsing.py:407: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working      class ResourceMap(collections.Mapping):    -- Docs: https://docs.pytest.org/en/latest/warnings.html  ================================================================= 4 passed, 2 skipped, 2 warnings in 63.66 seconds ==================================================================  Segmentation fault  {code}",2
"DM-24467","04/14/2020 00:57:24","Review the M2 Wish List","Review the M2 wish list by Bo:  [https://docs.google.com/document/d/1z_UoaHy32q6yP8ebxfQwK8SL3ThtH0KS8Se--eNcX5A/edit]",0
"DM-24468","04/14/2020 14:46:09","Poll lower level status and send telemetry","Introduce a polling mechanism for the lower level statuses and emit telemetry.",3
"DM-24469","04/14/2020 14:49:11","Review Dome Software Timeline","Review the Dome Software Timeline and bring it in line with the current status and the expected the development progress.",1
"DM-24472","04/14/2020 18:58:20","Regenerate Gaia DR2 catalogs to correct coordinate error fields","While checking out the new Gaia DR2 refcats in testdata_jointcal with jointcal, the astrometry tests were failing to converge. After some exploration, I discovered that the coordinate error fields were incorrect: the ingest code was assuming they were radian, when they were actually milliarcsecond.    I've already written most of the code to test this as part of verifying that it exists. Once I'm satisfied, I will re-run the Gaia DR2 ingest, and then replace the testdata_jointcal refcats (which is now trivial, with the new scripts from DM-24392).",5
"DM-24476","04/14/2020 23:37:40","Add a timeout parameter to BaseCscTestCase.check_standard_state_transitions","Add a timeout parameter to {{BaseCscTestCase.check_standard_state_transitions}} so it can be used with CSCs that require a long time to change to certain states.    It is probably getting too fancy support timeouts that are specific to a given state or a given state transition command. That would really complicate the API.",0
"DM-24479","04/15/2020 00:30:56","Make a matched catalog pipeline task","In order to use the Gen3 {{MetricTask}} pipeline task, we need to make tasks that can produce the data products used by the metric measurements.    This is to produce the product most commonly used by currently implemented {{validate_drp}} measurements, the per band matched catalog.    Results will be check into [this repository|https://github.com/lsst-dmsst/metric-pipeline-tasks].",5
"DM-24483","04/15/2020 17:13:38","Prepare the MTAOS Docker Image","Prepare the MTAOS docker image that adapt the ts_xml v5.0.",0
"DM-24489","04/15/2020 21:25:56","Unit tests involving SALPY are much less reliable with ts_sal 4.1","ts_sal has a few test for communication between salobj and SALPY. At least one of these has become much less reliable.    The possibilities I have thought of are:  * The memory leak fix to OpenSplice. I can test this by running an older Docker image.  * Changes in ts_sal 4.1.  * Whatever is causing the new segfaults that I reported in DM-24461, which is most likely one of the items above.  * Change in salobj. This seems unlikely, because none of the changes seems even remotely relevant.    I have no idea how long this will take to resolve, so I have arbitrarily assigned 2 story points. Given the possible association with DM-24461 I am making this a priority and adding it to the current sprint. I hope to resolve it before releasing salobj v5.9",2
"DM-24493","04/15/2020 23:05:45","salgenerator operation order can delete required assets","The salgenerator can delete necessary assets if done in the wrong order.   * *salgenerator <CSC> sal idl* will delete or overwrite certain C++ artifacts.  This results in *sal <CSC> python* build failures   * if two CSCs have similar roots (e.g. ATDome and ATDomeTrajectory), the second CSC build can completely wipe out the first.    Also, *salgenerator <CSC> idl* is not the correct format, but is still allowed.  It should cause an error instead of executing a build.",1
"DM-24495","04/16/2020 01:18:50","Convert config overrides to use file in several obs packages","As per https://community.lsst.org/t/pex-config-configs-now-know-where-they-are/4023,  use {{os.path.dirname(\_\_file\_\_)}} instead of {{getPackageDir}} in obs-package config files.    I'm doing just obs_lsst, obs_decam, and obs_subaru on this ticket because I already did the work before deciding it didn't belong as a side-commit on the ticket I was working on.  Others are welcome to take care of other packages on other tickets.    ",2
"DM-24507","04/17/2020 17:48:46","Syntax error in ts_CBP","In ts_CBP/lsst/ts/cbp/statemachine.py there a syntax error at line 262 (a missing {{_}} in the f-string):  {code}  raise salobj.ExpectedError(f""{simulation mode} is not a valid value"")  {code}    Please fix it",0
"DM-24508","04/17/2020 18:20:18","Add lsst log message init to Click CLI tools","the lsst log tool needs to be initialized by the Click command line interface scripts, similar to     [https://github.com/lsst/pipe_base/blob/master/python/lsst/pipe/base/argumentParser.py#L497-L508]          ",1
"DM-24513","04/18/2020 00:34:50","Fix a bug in the REST server of the Qserv Replication/Ingest system and refactor its implementation","A refactoring of the REST service made in the previous ticket https://jira.lsstcorp.org/browse/DM-24053 introduced a bug in the implementation of the service. The bug was observed at https://jira.lsstcorp.org/browse/DM-24396. Further analysis of the problem suggested that a code which is routing REST requests to the corresponding processing modules needs to be revisited.    The new implementation of the {{HttpPocessor}} (where the request routing to the corresponding modules and submodules is happening) will no longer instantiate the modules directly. This task will be delegated to the static method of the modules, for example:  {code:c++}  HttpIngestModule::process(...)  {code}  This adds extra flexibility to modules on how they would be instantiated and implemented.",2
"DM-24515","04/18/2020 05:13:52","Refactor gen3 butler.prune","In DM-23671 we refactored datastore deletion into a two pass trash and empty. This allows us to simplify the logic in prune.",2
"DM-24521","04/20/2020 17:17:20","Fix test failures in ts_MTMount test_mock_devices.py with salobj v5.10","Some ts_MTMount unit tests in test_mock_devices.py are failing with salobj v5.10 that were not failing with 5.8 and 5.9. There are 3 errors, all like this:  {code}  tests/test_mock_devices.py:460: in check_axis_device      self.assertAlmostEqual(device.actuator.target.tai, tai)  E   AssertionError: 1587399318.8612864 != 1587399318.861286 within 7 places (4.76837158203125e-07 difference)  {code}  Perhaps a tolerance simply needs to be larger, but first understand how the breakage occurs. I hope it's not a bug in salobj 5.10!",0
"DM-24523","04/20/2020 17:59:49","ap.verify.ingestion._findMatchingFiles excludes directories","As reported on [Community|https://community.lsst.org/t/4107/], the unit tests for {{ap_verify}} include a test where the {{_findMatchingFiles}} function searches for files that don't contain the string ""fr"". This test causes a false positive when run in a directory containing ""fr"". Fix {{_findMatchingFiles}} so that exclude patterns are matched only against the filename, as implied by the documentation.",2
"DM-24534","04/21/2020 18:36:07","Add source table pipelines to ci_hsc_gen2 & add its cat schema","DM-24062 added the pipelines of generating the source tables in parquet.   Add the pipelines to {{ci_hsc_gen2}}.  ",5
"DM-24546","04/22/2020 17:05:47","Improve explanation of calibration and fluxCalibRadius in insertFakes.py code.","Improve the documentation in the code comments in {{pipe_tasks}} {{insertFakes.py}}.  Explain the use of {{fluxCalibRadius}} and the need for it to be synchronized with the radius used to define {{photoCalib.magnitudeToInstFlux}}.    See discussion at    https://community.lsst.org/t/why-are-fake-sources-corrected-to-mean-the-integrated-flux-within-calibfluxradius/4110/12",0.5
"DM-24555","04/22/2020 20:12:54","Add ability to retrieve various Exposure components in gen3 butler.","There are some components that were left out of the {{FitsExposureFormatter}}.  These are defined [here|https://github.com/lsst/daf_butler/blob/master/python/lsst/daf/butler/formatters/fitsExposureFormatter.py#L100].  Basically any function defined on the {{ExposreFitsReader}} can be defined there.  See the [header|https://github.com/lsst/afw/blob/f570990def94c74c425d919df9f56a40e06715f2/include/lsst/afw/image/ExposureFitsReader.h#L120] for the method to read the {{photoCalib}} object for example.",2
"DM-24556","04/22/2020 20:38:09","Add normalize method to Defects","There have recently been discussions regarding {{Defects}} where new defects are appended to the existing list of defects. This works fine but can result in a situation where lots of tiny bounding boxes could be represented by a few large bounding boxes. Many things become more efficient if the minimal set of bounding boxes is used by downstream processing.    This ticket will add a normalize method to {{Defects}} that will recalculate the defect bounding boxes. One implementation could be to generate a mask from the defects and then recalculate the defects from the mask. This would have the side effect of allowing direct comparison of two different Defects instances.",2
"DM-24557","04/22/2020 21:26:15","Trim matched catalog to output patch coordinates","Currently the task just matches any source that is close to the output patch.  We need to trim to the patch so we aren't lying to downstream tasks.",2
"DM-24558","04/22/2020 21:32:32","Record salobj topic read/write speed in SQuaSH","Record a measurement in SQuaSH of how quickly salobj can read and write writing DDS samples. Ideally we would record separate values for writing and reading. Note that the existing speed test testss/test_speed.py only measures the combined speed of reading and writing.    See https://github.com/lsst-sqre/notebook-demo/blob/master/lsst-verify-squash/LSSTVerificationSQuaSHDemo.ipynb for instructions.",3
"DM-24560","04/22/2020 21:55:58","make 'repo' an argument, by convention always the first.","change repo from a required ""option"" to a required ""argument"" (former has flags e.g. {{--repo foo}}, the latter is positional).",3
"DM-24564","04/23/2020 00:08:06","Produce a runnable pipeline description for the metrics tasks","Now that we have two tasks we can make a pipeline that is not completely trivial.  Write runnable pipeline description that links catalog construction with metric measurement.",1
"DM-24576","04/23/2020 18:20:36","Update ts_FiberSpectrograph S3 writing for changes in salobj v5.9","Update ts_FiberSpectrograph code that writes images to S3 for changes in ts_salobj v5.9.0. This should be a trivial change.    Add a revision history.",0
"DM-24587","04/24/2020 11:54:30","Load DC2 data inside CC-IN2P3 cluster","Load a small set of chunk inside CC-IN2P3 Qserv cluster using replication system.    The procedure is based on: [https://confluence.lsstcorp.org/display/DM/Live+demo%3A+test+ingest+of+a+subset+of+one+track+of+the+HSC+Object+catalog]",20
"DM-24590","04/24/2020 17:01:25","Abstract the interface of deblending code in ts_wep","This task will use the factory pattern for the deblending algorithm to let UW team experiment multiple deblending algorithm without the affection of code body.",2
"DM-24591","04/24/2020 17:04:25","Organize the hexapod code by MOOG","This task will put the haxapod code by MOOG to a new repo and reorganize the code structure. The autotool related code will be rewritten to fit the new structure as well.",2
"DM-24592","04/24/2020 18:26:11","Get astrometry working for the AuxTel","Get astrometry working for the AuxTel now that we're on sky.",40
"DM-24593","04/24/2020 19:40:35","Fill out general CSC overview","Fill out general_csv_overview with more detailed information.",3
"DM-24601","04/25/2020 00:37:41","Qserv master replication controller crashes on the misformed requests to its REST services","The {{REST}} server of the {{Master Replication Controller}} throws an unhandled exception when parsing the improperly formed {{body}} of the HTTP requests where a JOSN-fied string is expected.    A solution is to catch the exception and to report its reason back to a user.",0.5
"DM-24605","04/27/2020 02:05:28","Qserv Replication and Ingest services crash due to an unhandled exception","Both services crash when attempting to parse improperly formed messages into a Google Protobuf object. This is reported in the log stream of the Replication/Ingest system workers as:  {code}  terminate called after throwing an instance of 'std::overflow_error'    what():  ProtocolBuffer::_extend  requested capacity 1195725856 exceeds the hard limit of Google Protobuf 64000000  {code}",1
"DM-24610","04/27/2020 17:41:41","Assign the OFC state0 in ts_MTAOS by Configuration File ","Assign the OFC (Optical Feedback Control) state0 in ts_MTAOS by configuration file. The *ConfigurableCsc* class in ts_salobj is the basic CSC (Commendable SAL Component) class for the subsystem such as *MtaosCsc* in ts_MTAOS to use. The configuration file is located at MTAOS directory of ts_config_mttcs. The following document explains the details of this: [TSTN-017|https://tstn-017.lsst.io/]. When writing the configuration file, a schema is needed to verify the configuration file based on: [Write a CSC|https://ts-salobj.lsst.io/salobj_cscs.html#writing-a-csc].    OFC is the component that calculates the correction of mirror bending mode and hexapod position. The control algorithm in use can follow [Real time wavefront control system for the Large Synoptic Survey Telescope (LSST)|https://drive.google.com/file/d/1RyMQWeXWRZLtxM8KkSwtB3STzMM9fCvX/view?usp=sharing] and [Control Algorithm in Optical Feedback Control|https://confluence.lsstcorp.org/display/LTS/Control+Algorithm+in+Optical+Feedback+Control]. The state0 (*s*~*0*~ in the [Control Algorithm in Optical Feedback Control|https://confluence.lsstcorp.org/display/LTS/Control+Algorithm+in+Optical+Feedback+Control]) is an intentional parameter used in *x00* reference, which is explained in the above links.    The AOS software contains the simulation code and control code. For the simulation, the user needs to use the repos: ts_wep, ts_ofc, and ts_phosim. For the control code, the user needs to use the repos: ts_wep, ts_ofc, and ts_MTAOS. The ts_phosim is not needed in the control and the ts_MTAOS is not needed in the simulation. In this task, we focus on the control code.    The followings are the steps in this task:   # Update the [default.yaml|https://github.com/lsst-ts/ts_config_mttcs/blob/develop/MTAOS/v1/default.yaml] in ts_config_mttcs to contain the state0 information as the following: [state0inDof.yaml|https://github.com/lsst-ts/ts_ofc/blob/master/policy/comcam/state0inDof.yaml]. I prefer to have two yaml files (default.yaml + state0inDof.yaml) instead of one (default.yaml) in ts_config_mttcs. However, we need to check ts_salobj can support this or not.   # Provide the related schema: [MTAOS.yaml|https://github.com/lsst-ts/ts_MTAOS/blob/master/schema/MTAOS.yaml] in ts_MTAOS/schema directory. Prefer to have two files instead of one. But this depends on the implementation of ts_salobj.   # Update *MtaosCsc* class in ts_MTAOS to read the configuration of state0. *MtaosCsc.configure()* will read the configuration file from SAL. *ConfigByObj* and *ConfigByFile* classes inherit from *ConfigDefault* to read the data. We need to update the related test cases to make sure the *MtaosCsc* can change the state0 by SAL or file.   # Override the default state0 when *OFCCalculation* is instantiated by *OFCCalculationFactory.getCalculator()* in *Model.ofc*. The state0 is stored in the internal data of *Model.ofc.ztaac.optCtrl*. The developer can do *OFCCalculation.getZtaac()* to get the *ztaac* attribute. After this, the developer can use *ZTAAC.setState0()* to set the state0 values. The related *get()* functions are provided for the test. By default, the state0 is set by *OFCCalculation()* --> *OFCCalculation._configZTAAC()* --> *ZTAAC.setState0FromFile()*. It is noted that we will keep the default state0 file in ts_ofc for the simulation (the AOS simulation does not know the ts_MTAOS. The UW team is using the AOS simulation to help the algorithm development)   # Update the test cases and [versionHistory.rst|https://github.com/lsst-ts/ts_MTAOS/blob/master/doc/versionHistory.rst] to reflect the change.",3
"DM-24611","04/27/2020 17:42:20","Update nublado docs to clarify NCSA VPN profile to uees","Neither our docs nor the NCSA VPN docs say specifically which profile to use when logging in to the VPN for use with the Science Platform.  I'll add a note that {{ncsa-vpn-default}} is the one to use unless you know better.",0.5
"DM-24612","04/27/2020 17:57:20","Add indexes to dataset_collection tables","Adding the right indexes - once we determine what they are - should be easy after DM-21764.  Stay tuned for a follow-up post on that here.",1
"DM-24616","04/27/2020 19:54:40","Stop writing to gen2 butler directory during 2to3 conversion","We recently started to see some registry files turn up in obs_lsst after running the tests:  {code}  data/input/phosim/calibRegistry.sqlite3  data/input/ts3/calibRegistry.sqlite3  data/input/ucd/calibRegistry.sqlite3  {code}  Making the {{data}} directory read only tells me that it's the 2to3 tests that are causing the problem.    {code}  self = <lsst.obs.base.gen2to3.calibRepoConverter.CalibRepoConverter object at 0x7f96729fea90>        def insertDimensionData(self):          # Docstring inherited from RepoConverter.          # This has only been tested on HSC, and it's not clear how general it          # is.  The catch is that it needs to generate calibration_label strings          # consistent with those produced by the Translator system.  >       db = sqlite3.connect(os.path.join(self.root, ""calibRegistry.sqlite3""))  E       sqlite3.OperationalError: unable to open database file  {code}    I'm not sure if this is a bug in running the converter or if the routine should return without error if the sqlite3 file is not present (I guess sqlite3 creates the file if it does not exist).      ",1
"DM-24619","04/27/2020 22:04:06","salgenerator operation for similarly named CSCs will delete build products","The salgenerator will delete necessary assets if done in the wrong order.  If two CSCs have similar roots (e.g. ATDome and ATDomeTrajectory), the second CSC build can completely wipe out the first.  In this case, if ATDome is run after ATDomeTrajectory, then the C++ and Java directories inside ts_sal/test/ATDomeTrajectory/ will be deleted.  I'm concerned this could be an issue in the future, if not resolved.",1
"DM-24637","04/28/2020 17:02:29","Run CscCommander commands asynchronously","CscCommander presently waits for each command to finish before allowing another command. That is unnecessarily restrictive and really annoying if a task hangs. Modify the commander to run commands asynchronously.",0
"DM-24641","04/28/2020 18:58:09","Update Opensplice RPM","Update Opensplice 6.9 RPM to include master priority change to ospl.xml",1
"DM-24642","04/28/2020 19:20:25","Implement simple single-pass data pacer for alert stream simulator","As laid out in [DMTN-149|https://dmtn-149.lsst.io/#injector-and-data-rates], implement a single-pass pacer for injecting alerts from disk into a Kafka broker using the timestamps in the files themselves.",2
"DM-24643","04/28/2020 19:21:54","Implement infinite-mode data pacer for alert stream simulator","As laid out in DMTN-149, implement a looping infinite-mode pacer for injecting alerts from disk into a Kafka broker which attempts to simulate a ""realtistic rate."" Make it configurable as described in DMTN-149, too.",5
"DM-24644","04/28/2020 19:23:32","Provide a distributable package for the alert stream simulator","Package up the alert stream simulator to make it straightforward to install on a Linux host. Some code duplication with (for example) sample-avro-alert is acceptable.",3
"DM-24645","04/28/2020 19:24:36","Provide a sample consumer application for the alert stream simulator","Provide a consumer application which simply counts the number of alerts it has received and prints the count to stdout every second. This is intended to be scaffolding code for users to work with.",1
"DM-24646","04/28/2020 19:25:23","Document alert stream simulator usage and setup","Write enough docs that a software engineer could install, use, and make sense of the alert stream in a box.",2
"DM-24649","04/28/2020 19:28:34","Add integration tests to alert stream simulator","Add an integration test, triggered with Github Actions, which tests that the alert stream simulator is installable and works with the basic tools we are providing.",1
"DM-24650","04/28/2020 19:30:09","Write docker infrastructure to create an alert broker locally","Write docker scripts to be able to create the basic infrastructure laid out in DMTN-149.",8
"DM-24653","04/28/2020 20:09:26","Make a pip-installable script to send data to the alert stream simulator broker","Make a script which is pip-installable which can send data to the alert stream simulator broker. The script should just take a file and write it into the broker.",1
"DM-24658","04/28/2020 22:36:51","Improve error reporting in connection string","Connection string *sometimes* uses DbAuth to insert username and passwords read from .lsst/dbAuth.yaml into the connection string and ignores it most of the time, trusting that db connection string provided in config is what the user really wants to use.    Error capturing in DbAuth raises DbAuthError from ... to make this happen. It was noticed today, by [~mgower], that leads to confusing error much later on in the code if the YAML is badly formatted. The following is enough to cause the error:         correct:  {code:java}  - url: ""postgresql://user@uri""    password: newpasswd  {code}  incorrect:  {code:java}  - url: ""postgresql://user@uri""      password: newpasswd{code}       I would like to add an DbAuthNotFound error to separate the allowed DbAuth failure case from the ones we want to raise early.",1
"DM-24667","04/29/2020 17:13:31","Determine if the Python MTMount CSC can keep up with telemetry from the low level controller","Figure out if the new Python MTMount CSC could keep up with telemetry from the low level controller: read the data, parse it and output it as SAL while tracking.    Test at the current rate of 20Hz tracking updates and 20Hz telemetry output per topic.  If that works, try speeding things up the telemetry and tracking rates until the system fails, to get a sense of how close to the edge we will be..",2
"DM-24671","04/29/2020 20:11:03","Create Jenkins build for the ATHexapod Conda package","The ATHexapod Conda package job should use the DeployEnv docker image and the necessary RPMs/IDLs, all from Nexus3.  It should create a conda package and then run the unit tests.  Must-have unit tests include instantiating the topics and running through the state-machine workflow.  The goal here is to catch unsupported interface changes before a release occurs.",3
"DM-24672","04/29/2020 20:11:35","Create Jenkins build for the ATDome Conda package","The ATDome Conda package job should use the DeployEnv docker image and the necessary RPMs/IDLs, all from Nexus3.  It should create a conda package and then run the unit tests.  Must-have unit tests include instantiating the topics and running through the state-machine workflow.  The goal here is to catch unsupported interface changes before a release occurs.",3
"DM-24674","04/29/2020 20:19:02","Create Jenkins build for the ATSpectrograph Conda package","The ATSpectrograph Conda package job should use the DeployEnv docker image and the necessary RPMs/IDLs, all from Nexus3.  It should create a conda package and then run the unit tests.  Must-have unit tests include instantiating the topics and running through the state-machine workflow.  The goal here is to catch unsupported interface changes before a release occurs.",1
"DM-24675","04/29/2020 20:35:01","Determine a way to have the SalObj conda job trigger a build off new tags","The [SalObj Conda Jenkins job|https://tssw-ci.lsst.org/job/SalObj%20Conda%20package/] will create an item when new tags appear, but no jobs runs automatically.  The job can obviously be started manually, but it would be nice to have it trigger automatically.  Determine a way to do this.",1
"DM-24681","04/30/2020 00:13:30","Format alerts published into the alert stream according to Confluent Wire Format","Prefixing alert messages with a magic byte and schema ID will let us interoperate with the Confluent Schema Registry in the future, if we should choose to use it. I've heard good things about the CSR from teams using Kafka in similar ways; it would help manage schema updates much more easily.    ",0.5
"DM-24683","04/30/2020 01:12:54","Use PRODUCT_DIR instead of OBS_BASE_DIR in obs_base eups table file","obs_base table file was updated to setup DAF_BUTLER_PLUGINS envvar, but it uses $OBS_BASE_DIR instead of $\{PRODUCT_DIR} in the value and this causes issues when package is setup twice. It looks like to ""unsetup"" this envvar, the variable needs to be defined with $\{PRODUCT_DIR}.    See also Slack discussion: [https://lsstc.slack.com/archives/C2JPT1KB7/p1588203833017400]    I do not know if there are other packages defining DAF_BUTLER_PLUGINS, if there is they need this fix too.",1
"DM-24684","04/30/2020 02:44:24","Extend the Replication/Ingest framework and REST API with algorithms and tools for indexing catalogs","The current implementation of the Replication/Ingest doesn't provide any mechanism for managing (inspecting, creating or dropping) indexes on the data tables within Qserv. Only a few essential (for Qserv to function) indexes get automatically created. A lack of indexes may affect a performance of many user queries. Hence a goal of this development is to add the following capabilities to the system:  * extending the Replication Framework with a set of _request_ and _job_ classes for managing indexes at workers.  * adding a command-line version of the Replication _controller_ tool for managing the indexes  * extending the REST services of the *Master Replication Controller* with the index management services    Other notes:  * the REST services need to be protected with the _auth_key_ mechanism as explained in: https://jira.lsstcorp.org/browse/DM-22745  * it may be necessary to consider putting a cap on the number of indexes which were created by the system due to disk space constraints at a particular Qserv deployment. For example, the system may need to evaluate the amount of the available disk space on the coresponding file system (at Qserv workers) before attempting to create a new index.  ",8
"DM-24694","04/30/2020 23:03:18","Update the mock hexapod and rotator to transition from fault to standby","Update the mock controller code in ts_hexrotcomm to transition from FAULT to STANDBY instead of OFFLINE, in order to match improvements that [~ttsai] has made to the rotator low level controller, and will make to the hexapod.",0
"DM-24697","05/01/2020 01:01:12","Update ts_hexrotcomm for ts_xml 5.2","Update ts_hexrotcomm to be forward-compatible with the pending ts_xml 5.2 release and to make it compatible with explicitly listing the generic topics used.    The issue is that BaseCsc implements several generic commands that it does not actually support (by raising an exception if they are called). The fix is to remove that code. Then those commands can be removed from the XML without affecting ts_hexrotcomm, ts_hexapod or ts_rotator.",0
"DM-24698","05/01/2020 14:29:31","Clean up (at least) doc bug introduced in DM-21764","See https://lsstc.slack.com/archives/C2JPT1KB7/p1588310584058000",1
"DM-24700","05/01/2020 17:42:16","Assist with streamlining SAL CI testing","Assist Jenkins CI testers in speeding up  / parallelizing / streamlining the test runs",3
"DM-24703","05/01/2020 17:59:47","Make linearity a subclass of lsst.ip.isr.IsrCalib","With DM-24537 done, ISR ""user curated calibrations"" can be made into subclasses of the new IsrCalib, ensuring we treat all of these products in a uniform manner.  This will help the transition to Gen3, as we will be able to use fewer mapper tricks.    This ticket covers the linearity correction.",8
"DM-24705","05/01/2020 18:13:52","Remove afwGeom aliases for geom from pipe_analysis","Many of the {{afwGeom}} aliases (e.g. [Sphere]Point, Extent, angular units) are now deprecated in favor of those defined in {{geom}}.  Please replace all instances in the \{\{pipe_analysis}} scripts.",1
"DM-24706","05/01/2020 18:48:54","M1M3cli","Develop M1M3 command line tool, so testing of communication to a single actuators can be reasonably commanded. This is just engineering tool, with purpose similar to unit tests (which are missing in M1M3 support as well).",3
"DM-24713","05/04/2020 14:13:22","Investigate poor scarlet performance on crowded fields","The Rubin Observatory LSST-VTS Blending Task Force has attempted to run scarlet on the globular cluster NGC 6569 to test it's effectiveness on crowded fields. The attached image shows that things did not go so well at all, but possibly because of very poor object detection and/or a poorly modeled PSF. This ticket is to investigate the source of the error and at least ensure that scarlet works as well on this field as can be expected on a first pass without detections on the residuals, which is left for a future scarlet ticket.",3
"DM-24718","05/04/2020 17:24:24","Fix license header to refer to project correctly","As pointed out in https://github.com/lsst-dm/alert-stream-simulator/pull/3#issuecomment-623492975, alert-stream-simulator's license headers call the project ""rubin-alert-stream."" This should be corrected.",1
"DM-24721","05/04/2020 18:47:09","w_2020_18 butler create does not work with Oracle","Trying to run latest weekly (w_2020_18) with Oracle:  {noformat}  python /usr/local/lsst_stack/w_2020_18/stack/miniconda3-4.7.12-2deae7a/Linux64/daf_butler/19.0.0-65-ga8942968/bin/butler create --repo /work/mgower/gen3work/weeklyCItest/git/ci_hsc_gen3/DATA  /work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/dialects/oracle/base.py:1381: SAWarning: Oracle version (19, 4, 0, 0, 0) is known to have a maximum identifier length of 128, rather than the historical default of 30. SQLAlchemy 1.4 will use 128 for this database; please set max_identifier_length=128 in create_engine() in order to test the application with this new length, or set to 30 in order to assure that 30 continues to be used.  In particular, pay close attention to the behavior of database migrations as dynamically generated names may change. See the section 'Max Identifier Lengths' in the SQLAlchemy Oracle dialect documentation for background.    % ((self.server_version_info,))  Traceback (most recent call last):    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context      cursor, statement, parameters, context    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 590, in do_execute      cursor.execute(statement, parameters)  cx_Oracle.DatabaseError: ORA-01408: such column list already indexed    The above exception was the direct cause of the following exception:  Traceback (most recent call last):    File ""/usr/local/lsst_stack/w_2020_18/stack/miniconda3-4.7.12-2deae7a/Linux64/daf_butler/19.0.0-65-ga8942968/bin/butler"", line 28, in <module>      sys.exit(main())    File ""/usr/local/lsst_stack/w_2020_18/stack/miniconda3-4.7.12-2deae7a/Linux64/daf_butler/19.0.0-65-ga8942968/python/lsst/daf/butler/cli/butler.py"", line 252, in main      return cli()    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/click/core.py"", line 764, in __call__      return self.main(*args, **kwargs)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/click/core.py"", line 717, in main      rv = self.invoke(ctx)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke      return _process_result(sub_ctx.command.invoke(sub_ctx))    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/click/core.py"", line 956, in invoke      return ctx.invoke(self.callback, **ctx.params)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/click/core.py"", line 555, in invoke      return callback(*args, **kwargs)    File ""/usr/local/lsst_stack/w_2020_18/stack/miniconda3-4.7.12-2deae7a/Linux64/daf_butler/19.0.0-65-ga8942968/python/lsst/daf/butler/cli/cmd/create.py"", line 42, in create      outfile=outfile)    File ""/usr/local/lsst_stack/w_2020_18/stack/miniconda3-4.7.12-2deae7a/Linux64/daf_butler/19.0.0-65-ga8942968/python/lsst/daf/butler/_butler.py"", line 382, in makeRepo      Registry.fromConfig(config, create=createRegistry, butlerRoot=root)    File ""/usr/local/lsst_stack/w_2020_18/stack/miniconda3-4.7.12-2deae7a/Linux64/daf_butler/19.0.0-65-ga8942968/python/lsst/daf/butler/registry/_registry.py"", line 204, in fromConfig      datasets=datasets, create=create)    File ""/usr/local/lsst_stack/w_2020_18/stack/miniconda3-4.7.12-2deae7a/Linux64/daf_butler/19.0.0-65-ga8942968/python/lsst/daf/butler/registry/_registry.py"", line 223, in __init__      self._opaque = opaque.initialize(self._db, context)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/contextlib.py"", line 119, in __exit__      next(self.gen)    File ""/usr/local/lsst_stack/w_2020_18/stack/miniconda3-4.7.12-2deae7a/Linux64/daf_butler/19.0.0-65-ga8942968/python/lsst/daf/butler/registry/interfaces/_database.py"", line 387, in declareStaticTables      self._metadata.create_all(self._connection)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/sql/schema.py"", line 4321, in create_all      ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1627, in _run_visitor      visitorcallable(self.dialect, self, **kwargs).traverse_single(element)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/sql/visitors.py"", line 144, in traverse_single      return meth(obj, **kw)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/sql/ddl.py"", line 781, in visit_metadata      _is_metadata_operation=True,    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/sql/visitors.py"", line 144, in traverse_single      return meth(obj, **kw)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/sql/ddl.py"", line 833, in visit_table      self.traverse_single(index)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/sql/visitors.py"", line 144, in traverse_single      return meth(obj, **kw)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/sql/ddl.py"", line 862, in visit_index      self.connection.execute(CreateIndex(index))    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 984, in execute      return meth(self, multiparams, params)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/sql/ddl.py"", line 72, in _execute_on_connection      return connection._execute_ddl(self, multiparams, params)    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1046, in _execute_ddl      compiled,    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1288, in _execute_context      e, statement, parameters, cursor, context    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1482, in _handle_dbapi_exception      sqlalchemy_exception, with_traceback=exc_info[2], from_=e    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_      raise exception    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context      cursor, statement, parameters, context    File ""/work/mgower/miniconda/envs/bps-lsst-scipipe-2deae7a/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 590, in do_execute      cursor.execute(statement, parameters)  sqlalchemy.exc.DatabaseError: (cx_Oracle.DatabaseError) ORA-01408: such column list already indexed  [SQL: CREATE INDEX run_fkidx_name ON run (name)]  (Background on this error at: http://sqlalche.me/e/4xp6)  scons: *** [DATA/butler.yaml] Error 1  scons: building terminated because of errors.    {noformat}  Got the create table and index SQL by setting SQLAlchemy's echo=True.   Can repeat the error message.",1
"DM-24734","05/05/2020 14:09:14","Revive and profile RC2 QuantumGraph generation","DM-24664 should have provided everything we need in terms of a starting data repository, but since we last ran on RC2 we've switched to YAML pipeline definitions, and neither the generic DRP one in pipe_tasks nor the custom one in ci_hsc_gen3 seem quite right.  Ideally we'll be able either make the pipe_tasks one viable with just instrument overrides or define a generic HSC+DRP pipeline in RC2, but if necessary we can put one in the Gen3+RC2 script repo.  After that, time and profile QG generation on RC2 and create tickets to optimize as needed.",2
"DM-24741","05/05/2020 19:24:43","Make output of verify.Job.write() more readable","Currently, the {{Job}} object writes using `json.dump` with no keyword arguments.  This means there are no carriage returns and that the file is hard to read without reformatting.  Adding a little formatting makes it much more readable at relatively small cost to overall file size.",1
"DM-24760","05/06/2020 19:30:27","Migrate measureCrosstalk.py to cp_pipe","measureCrosstalk has historically been in ip_isr, but more logically belongs in cp_pipe.  This ticket will move it, and ensure it is fully updated to use the new CrosstalkCalib.",20
"DM-24761","05/06/2020 19:34:08","Fix Firefly notebook","When the {{obs_lsstSim}} repository was removed from the distributed stack, it broke the repository being used by the demo Firefly notebook.  This is probably a good thing since that repository was several years old and should probably be deprecated.    This meant moving to an HSC rerun in {{/datasets}} instead.  We will need to move again when we go to gen3, but this seems like a reasonable thing for the time being.",2
"DM-24766","05/06/2020 22:01:17","Update ts_ATDome to use port=0 instead of a port generator when talking to the mock controller","At present ts_ATDome uses a generator to pick a likely-free port when running unit tests. This is not really safe. It is much better to specify port=0 and let the operating system pick a free port (as ts_hexrotcomm does in OneClientServer). Update ts_ATDome accordingly.",1
"DM-24768","05/07/2020 01:00:48","build race condition in daf_persistence","I received a build failure in daf_persistence when rebuilding lsstsw, due to the `testLib.so` file not being produced before pytest started, resulting in this error:    {code}  [2020-05-06T17:18:36.046873Z] tests/test_butlerProxy.py:32: in <module>  [2020-05-06T17:18:36.046878Z]     from testLib import isValidDateTime, TypeWithProxy, TypeWithoutProxy  [2020-05-06T17:18:36.046882Z] tests/testLib/__init__.py:3: in <module>  [2020-05-06T17:18:36.046887Z]     from .testLib import *  [2020-05-06T17:18:36.046897Z] E   ModuleNotFoundError: No module named 'testLib.testLib'  {code}    It appears this was due to the SConscript having the `pybind11` and `tests` entries out of order. I'm fixing it in daf_persistence, and I will check whether this is an issue in any other packages. I will also update the dev guide to mention the correct ordering of these statements.    I don't know why I hadn't encountered this before, but I could consistently trigger it on my 2013 Macbook Pro, so my guess is a change in build time with the new conda environment.",2
"DM-24769","05/07/2020 01:02:49","Improve connection string matching.","Michelle Gower noticed that when dialect and driver are specified in the config but not so in the db-auth.yaml the match will not succeed.    Example, the db key in registry config:         db: ""oracle+cx_oracle://host""     will not match db-auth.yaml entry         - url: oracle://host...     and vice-versa. Specifying the driver has an impact on functionality of the registry since it changes the functionality of the underlying database, but the driver has no direct impact on DB authentication. The same user should be able to connect with any appropriate driver they have available regardless of whether db-auth entry matches the full dialect+driver string or not.         I think it's most natural to allow, and preserve, specified driver in the butler.yaml, but ignore it in db-auth.yaml. I am wary of making it raise an error when it does, but that can be changed.",1
"DM-24777","05/07/2020 17:19:29","Confirm EFD client is not broken by InfluxDB 1.8.0","Confirm in CI that the efd helper classes work with the new version of InfluxDB (1.8.0).",1
"DM-24780","05/07/2020 18:55:59","Initial mypy configuration for daf_butler","This ticket adds:   # configuration that tells mypy to run on just lsst.daf.butler.registry.interfaces   # fixes for the type annotations in just that package   # a non-default scons target to run mypy   # travis configuration to run mypy    As mypy isn't part of the stack's usual conda environment, users will have to take care of installing it (I used {{conda install --no-update-deps mypy}}) before trying {{scons mypy}}.    I figure we'll expand the configuration to other daf.butler subpackages as we get them to run clean under mypy.    Work for this ticket is almost entirely done in prep for a Science Pipelines team meeting on type annotations; all that remains is review.",1
"DM-24786","05/07/2020 20:29:14","New component column in datastore is too small","In DM-24288 the component name was added to the internal posix datastore record. Unfortunately it was length 16 and sqlite did not complain -- this breaks databases that do check the length of strings. In particular ""TransmissionCurve"" exceeds the limit. Change the limit to 32 to fix this.    Additionally we might want to add constraints back. Constraints were added previously in DM-18438 but subsequently removed when DatabaseDict was replaced with OpaqueData.",2
"DM-24788","05/07/2020 20:58:03","Improved error reporting in the REST services of the Replication System's Master Controller","There are a few problems with the current design of the HTTP modules (subclasses of {{lsst::qserv::replica::HttpModule}}):  * it's impossible to return an extended description of errors in addition to a simple string sent back to the services' requestors. It would be useful for many operations to let a client have more insight into origins of problems reported by some services.  * the general design for reporting both results and errors is unsafe, and it leaves a room for mistakes.    In the current model a module's implementation has to use the following mechanism for reporting results/errors:  {code:c++}  class HttpSomeModule: public HttpModule {  ...  protected:      virtual void executeImpl() override {          ...          if (error condition) {              sendError(""error string"");              return;          }          ....          json result = ... ;          sendData(result);          return;      }      ....  };  {code}  This model won't force the module to submit either result or error code which is required by the protocol. The method may end w/o reporting either of those, or it may send both (if a developer misses to {{return}} right after calling the corresponding method.    Hence, the proposed improvement is to redesign the modules to require the overloaded method {{HttpModule::executeImpl}} to return its normal result as a {{JSON}} object (instead of calling {{sendData()}}), and use exception for reporting errors. Besides that, there will be a new exception allowing to return a {{JSON}} object as an optional parameter for reporting extended conditions of errors:  {code:c++}      virtual json executeImpl() override {          ...          if (error condition) {              json errorExt = ... ;              throw HttpError(""error string"", errorExt);          }          ....          json result = ... ;          return result;      }  {code}",3
"DM-24789","05/07/2020 21:07:00","Make sure that the DCS mock controller uses radians instead of degrees","The EIE lower level components express angles in radians while SAL uses degrees. Make sure that the mock controller for the lower level components use radians as well.",1
"DM-24790","05/07/2020 21:46:57","Add documentation for DCS","Update index.rst with a description of DCS.",2
"DM-24796","05/08/2020 20:16:05","pipetask's graphviz dot files need to quote component dataset type names","Strings with periods apparently aren't allowed as the names of vertices or edges in graphviz unless they're quoted.  At least the pipeline-level dot files (and I assume the QG-level ones) run afoul of this when a dataset type name refers to a component, as in ""{{calexp.wcs}}"".",0.5
"DM-24797","05/08/2020 21:20:18","Store per-run information (configs, software versions) in butler repo","As part of gen2 deprecation we are required to store run details into the butler repository. These are expected to be per-run and so have blank dataIds.    * The config used by the pipeline.  * The pipeline definition.  * The software versions.    ",2
"DM-24798","05/08/2020 21:21:08","Put the Hexapod into Standby State after the clearError Command","Put the hexapod PXI into the standby state after clearing the error. This task will need to update the Simulink model for the state machine transition. After this, I will need to generate the C/C++ code and put back to ts_hexapod_controller and test the new built. This task will need to learn and test and hexapod simulink model. This task will also try to understand the logic of look-up table (LUT) in code.",3
"DM-24799","05/08/2020 21:23:54","Review the Control Algorithm of Rotator in Phase 1","This task will review the control algorithm of rotator in Simulink model and try to translate to the math equations. This is try to understand the behavior of stop command in the tracking. In the previous test on summit, we learned that the path generator has the weird behavior if stop command was issued in the tracking. We suspect the stop command results in the wrong behavior of path generator.",3
"DM-24800","05/08/2020 21:33:36","Add #define to provide CLOCK_TAI if not present (temporary)","For the latest LSST stack the CLOCK_TAI symbol is not defined due touse of an older glibc  version. Implement a temporary worlaround to provide the #define in this situation.",0.5
"DM-24802","05/08/2020 21:46:16","Adjust SAL to accomodate changes to the XML recommendations","The https://confluence.lsstcorp.org/pages/viewpage.action?pageId=120786640&src=contextnavpagetreemode  proposes some changes which   will require small mods to the SAL parser",1
"DM-24804","05/08/2020 22:28:32","create a 'convert' butler command","make a {{convert}} command that implements the {{the obs_base convert_gen2_to_gen3}} command.    existing tests are in {{lsst.obs.base.gen2to3.convertTests}}, and {{ci_hsc_gen2}} has a test script {{tests/test_gen2to3.py}}",8
"DM-24805","05/08/2020 22:30:41","Build the Latest MTAOS Docker Image to Deploy on NCSA","Build the latest MTAOS docker image to contain the update of DM-24422, DM-24423, and DM-24610 to deploy on NCSA. AOS needs the upstream packages of *lsst-distrib* and *lsst-sims*. The available version (tag) can follow [tags|https://eups.lsst.codes/stack/src/tags/]. The T&S team uses the weekly built in the most case. For the *lsst-distrib*, the latest available tag is *w_2020_18*. For the *lsst-sims*, the latest available tag is *sims_w_2020_16*. To install these two packages together, we need to make sure to use the same weekly built tag.    The T&S team releases the available development image ([develop-env|https://hub.docker.com/repository/docker/lsstts/develop-env]) regularly. The latest formal tag is *b68*, which has the *lsst-dev* *w_2020_16* installed already. It is noted that we do not use the *latest* tag usually. In this task, we will begin from this docker image to build the image of MTAOS (the latest one now is using *w_2020_15*). The followings are the steps:   # Update the [aos_sal|https://github.com/lsst-ts/ts_Dockerfiles/tree/develop/aos_sal] image to use the *lsstts/develop-env:b68* and publish as [aos_sal|https://hub.docker.com/repository/docker/lsstts/aos_sal] on Docker hub. This image installs the *lsst-sims* and other needed packages for AOS to use.   # Update the [aos_aoclc|https://github.com/lsst-ts/ts_Dockerfiles/tree/develop/aos_aoclc] image to use the latest AOS packages and publish as [aos_aoclc|https://hub.docker.com/repository/docker/lsstts/aos_aoclc] on Docker hub.   # Update the [mtaos_sim|https://github.com/lsst-ts/ts_Dockerfiles/tree/develop/mtaos_sim] image to use the latest ts_MTAOS package and publish as [mtaos_sim|https://hub.docker.com/r/lsstts/mtaos_sim] on Docker hub.   # Deliver to Tiago to deploy the image on NCSA for the test.",1
"DM-24811","05/09/2020 00:52:47","Fix the track target command in MTMount","Tekniker's ""crude simulator"" is rejecting the track target command from MTMountCsc. I have asked Tekniker to explain what is wrong. Once I find out update ts_MTMount (assuming the fault is there, which seems likely).",1
"DM-24813","05/09/2020 01:11:34","Finish specifying changes to Tekniker's TMA code","This ticket is to capture the time I will spend on final (I hope) changes to LTS-103 and the change request for Tekniker's TMA code.    Others have also put a lot of time into this, but if I understand the time accounting system we each need our own ticket.",1
"DM-24824","05/11/2020 15:39:09","Modify event used to populate AT focus header value.","With CAP-468 completed, the ATAOS now tracks the different focus offsets applied to the lookup tables.     What should be in the header is the user-defined offset from the LUT.    This ticket is to update the FOCUSZ header to point to:    ATAOS_logevent_focusOffsetSummary.userApplied",2
"DM-24833","05/11/2020 18:50:21","Dome software meetings","Prepare and coordinate weekly meetings on dome control software with EIE contractor.",5
"DM-24834","05/11/2020 18:57:20","Create the M2 GUI/DDS Startup Procedure in Jira Test Manager","Create the M2 GUI/DDS startup procedure in Jira test manager. This is for the system engineer team to start up the system. The focus is the use of M2 control system (GUI) and SAL. For example, how to check the DDS communication, open the GUI, run the software, commend the system by SAL, etc.. The link of ""LSST Verification and Validation"" is [here|https://jira.lsstcorp.org/secure/Tests.jspa#/design?projectId=12800].",1
"DM-24835","05/11/2020 18:58:50","Generate document on dome coditioning contract","As a component of the Environment Awareness System EAS ([https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873),|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873)] it's necessary to develop a dome environment conditioning system, to control te temperature during daytime. For that purpose, a document will be generated to lead to a contract for the implementation of this system.",3
"DM-24838","05/11/2020 19:32:53","Replace boot and shutdown commands with enter/exitControl","M1M3 SS uses boot and shutdown commands for what nowadays shall be done with enterControl and exitControl generic SAL commands. Replace boot and shutdown commands with generic enter/exitControl. Enforce their functionality (if possible) by actually stopping and restarting HW threads on exit/enter.",3
"DM-24840","05/11/2020 21:36:33","Please allow enumerations with specified values in the Enumeration field of SALSubsystems.xml","I think it would make SALSubsystems.xml a bit clearer if the <Enumeration> fields could have values specified, e.g.:    For example I would argue that this:  {code}  <Enumeration>Blue=1, Red=2, Broad=3</Enumeration>  {code}  is a bit clearer than this:  {code}  <Enumeration>Blue, Red, Broad</Enumeration>  {code}    It also supports an unusual existing use case: ts_FiberSpectrograph uses special index -1 to mean ""UNKNOWN: use the only connected spectrograph"" (though that could easily be changed to another value).    If you decide to make this change, please also:  * Reject the XML if a value is 0, since for topic readers 0 means ""read from all indices"" (a great feature!).  * Please consider allowing optional whitespace around the = and after the comma.    This is a minor suggestion and I do not consider it important enough to justify a lot of work. Once we have good central documentation for SALSubsystems.xml the current system should be clear enough.",0.5
"DM-24841","05/11/2020 21:45:12","Please support non-empty specification of a non-indexed component","For the <Enumeration> field, non-indexed components should be identified by the content ""no"" instead of an empty tag. This removes the ambiguity between it is not indexed or it was not noted down properly.",0.5
"DM-24842","05/11/2020 22:25:44","Generate Opensplice RPM/DEB for raspberry pi","Build runitme support archives (DEB/RPM) for the raspberry pi version  of OpenSplice",0.5
"DM-24843","05/11/2020 22:30:50","Make <IndexEnumeration> values available in SAL-generated IDL files and libraries","Please make the SAL index enumeration values available in the IDL files and libraries generated by ts_sal. These are the values for the field called <Enumeration> in SALSubsystems.xml (and which is likely to be renamed to <IndexEnumeration>).    Not a high priority, but it would be nice to have.",2
"DM-24851","05/11/2020 23:55:05","Change Datastore.getUri to Datastore.getURIs","When we started to allow datasets to be disassembled this resulted in a single dataset_id corresdponding to multiple disassembled files. This breaks the paradigm for getUri which assumes a single file.    On this ticket we propose to replace getUri with getURIs that will return a tuple of the URI of the primary dataset and a dict mapping component name to component URIs.    For normal disassembly the primary dataset would be None. In the future for virtual datasets this may no longer be true and the primary dataset could be defined with URIs to override components also being returned.    Predicted file will still assume no disassembly.",2
"DM-24855","05/12/2020 00:48:35","Exclude sky sources from Ratio of DIASources to Direct Image Sources metric","DM-23078 added [Sky Sources|https://community.lsst.org/t/sky-sources-added-to-single-frame-processing/4137] to Single-Frame Processing.  This created an unwanted adjustment in the ""Ratio of DIASources to Direct Image Sources"" metric.      This ticket is to exclude sky sources from the metric calculation (we should continue to compute and store them because we are likely to use them in the future.)",2
"DM-24862","05/12/2020 17:53:44","Update with latest scarlet and proxmin and test for performance","The API for scarlet changed slightly, with some improvements in performance and in memory usage. So this ticket is to update {{meas_extensions_scarlet}} to properly wrap and propagate those changes and run on the fake patches to measure the gains in performance from the previously tested version.",5
"DM-24892","05/12/2020 19:40:26","Fix bug in gen2to3 when only special dataset types are being converted","RepoWalker raises when initialized when there is nothing for it to look for.  This is useful information that should be propagated up to avoid unnecessary filesystem scanning, but it shouldn't be allowed to propagate _all_ the way up and kill conversion, as this isn't all of the work that might need to be done.",1
"DM-24912","05/13/2020 01:46:35","Fix packages broken by ts_salobj 5.12","DM-21196 introduces a change to ts_salobj that is not completely backwards compatible. Fix code that is broken by this change.    The new version of ts_salobj will probably be called v5.12.0.    So far the main issue I have seen is one or two unit tests in a few packages. I am not expecting anything worse, but I want to verify that before releasing the new salobj.",2
"DM-24915","05/13/2020 14:48:49","Fix broken Travis CI","Testing in scarlet started failing on Travis CI yesterday, meaning that the docs are no longer being built. Tests pass locally, and the first test to fail is the current master: https://travis-ci.org/github/pmelchior/scarlet/builds/686233127 . Executing the previous master commit, which was previously passing, is now failing as well: https://travis-ci.org/github/pmelchior/scarlet/builds/686224195 .    This indicates thats something on Travis changed, but it is not clear what this could have been. I checked all of the dependencies and they use the same versions now as they did before:    numpy 1.18.1  scipy 1.4.1  astropy 4.0.1  nbsphinx 0.7.0  matplotlib 3.2.1  numpydoc 0.9.2  jupyter 1.0.0  pybind11 2.4.3  proxmin 0.6.10 (loaded from github, has not been updated)  peigen 0.0.9  prompt_toolkit 3.0.4  sep (loaded from github but has not had a new commit since November)  autograd 1.3",3
"DM-24924","05/13/2020 19:43:19","Add new telescope name for ComCam and LSSTCam to obs_lsst","obs_lsst should be able to allow the Simonyi Survey Telescope as a valid telescope name in order to read files for ComCam and LSSTCam.",1
"DM-24925","05/13/2020 19:45:13","Merge SAL/XML Jenkinsfiles","There are two Jenkinsfiles for SAL/XML builds; one for releases and one for daily builds.  They are heavily duplicated, but build different branches and set variables differently.  In theory, with some smart conditionals, these two files can be merged into one.  This task covers the time needed to attempt this consolidation.",3
"DM-24932","05/14/2020 15:01:08","Improve output from SALPY tests in salobj","Print how long it takes to get historical data in the salpy<->salobj tests in ts_salobj.    This may help diagnose DM-24931",0
"DM-24934","05/14/2020 17:40:59","Clarify the meaning of CW/CCW/UP/DOWN and whether a velocity may be negative","Make sure that it is clear what CW/CCW/UP/DOWN in the Dome commands XML in SAL mean and make sure that it is stated in the description of the crawl commands. Also make sure that negative velocities are rejected?",1
"DM-24935","05/14/2020 18:05:12","Add support for DataFrame and Table in ScienceSourceSelectorTask and ReferenceSourceSelectorTask","As of DM-24062, HSC calexp sources (at least) can be consolidated into visit level {{sourceTable_visit}} parquet tables.  While reading this in is extremely fast relative to reading in the individual source catalogs (especially when not reading the full width) [about 95-98% faster in some testing], these full catalogs are {{DataFrame}} objects and cannot be used with our source selectors.    It should be relatively easy to add {{DataFrame}} support to {{ScienceSourceSelectorTask}} and {{ReferenceSourceSelectorTask}} which I believe are the most common down-stream source selectors used for science analysis (and, for example, in {{fgcmcal}} which will be one of the first consumers of {{sourceTable_visit}}, DM-24319.",2
"DM-24937","05/14/2020 18:08:45","move implementation of remaining butler commands to script folder","the butler command functions config_dump and config_validate need to be refactored so that their implementations are in the scripts folder, similar to the other command functions.",1
"DM-24940","05/14/2020 18:44:21","Move NamedKeyDict and NamedValueSet out of utils and clean up typing","An early attempt to do some of DM-24938 on DM-24734 ran afoul of messiness involving regular dictionaries with dimension element names vs. NamedKeyDict objects holding real DimensionElement instances.  This is exactly the sort of stuff mypy is designed to fix, but the type annotations for NamedKeyDict and NamedValueSet are tricky and in some cases incorrect, and need to be fixed first.    We're also regularly using these containers in downstream packages because they're the natural way to pass around and group by DatasetTypes, and that means we should be exporting them at ""lsst.daf.butler"" level, instead of requiring imports from ""lsst.daf.butler.core.utils"", because the last two entries in that path should be considered internal daf_butler details.",1
"DM-24941","05/14/2020 18:55:11","Compile a good sample dataset for alert stream simulator","Take a bunch of the data from ap_verify_hits2015 and compile it into a single .avro data file which covers multiple CCD visits. Publish it through lsst-dev web directory, and add a Make directive for downloading it.",1
"DM-24942","05/14/2020 19:35:43","HVAC defines a command and telemetry topic with the same name","The HVAC interface defines a command and a telemetry topic with, effectively, the same name:    lsstBarraoblPiso04BarraoblManejadoraBarraoblSblanca    This string appears in both HVAC_Commands.xml and HVAC_Telemetry.xml.  The SAL is having an issue building the HVAC libraries, as a result.  There's no error in the salgenerator output, as far as I can see, but there's an error during the build looking for an output file:    [https://tssw-ci.lsst.org/job/test_linking/158/robot/report/rpm_report.html#suites?s1-s3-s6]         All the libraries seem to generate just fine; there are no other reported errors.  But this may indicate a bigger issue with the libraries.",1
"DM-24970","05/18/2020 18:47:28","XML_VERSION=1.0.0 in IDL files","The reported XML version is 1.0.0 in generated IDL files, instead of the git-tagged version of ts_xml. Here is the first line from an IDL file generated by ts_sal 4.1.1:  {code}  // SAL_VERSION=4.1.0 XML_VERSION=1.0.0  {code}",1
"DM-24976","05/19/2020 01:05:37","Rework testdata_jointcal dependencies","The new data in DM-17597 will grow the size of a testdata_jointcal checkout significantly. Because testdata_jointcal has dependencies on each of the obs packages that it contains data for, it is re-installed during every build, resulting in potentially significant disk usage.    This ticket is to split out the obs dependencies so that testdata_jointcal has no dependencies itself. jointcal and fgcmcal would then gain optional dependencies on the obs packages whose repos they use, in addition to testdata_jointcal. Those unittests would also need to trip for whether the appropriate obs packages was `setup` and skip the tests if not.    As part of this, we should add a note to `fgcmcal.table` to explain why it has a dependency on jointcal (it needs the gen2 tract-grouping code, which will be unnecessary in gen3).",1
"DM-24978","05/19/2020 01:48:50","Update butler create config option in ci_hsc_gen3","DM-24584 changed the butler command line argument. In particular for passing in a config file to {{butler create}} in {{w_2020_19}}:     {noformat}  $ butler create --help  Usage: butler create [OPTIONS] [REPO]      Create an empty Gen3 Butler repository.      REPO is the URI or path to the new repository. Will be created if it does    not exist.    Options:    -c, --config TEXT   Path to an existing YAML config file to apply (on top of                        defaults).    --standalone        Include all defaults in the config file in the repo,                        insulating the repo from changes in package defaults.    -o, --override      Allow values in the supplied config to override any repo                        settings.    -f, --outfile TEXT  Name of output file to receive repository configuration.                        Default is to write butler.yaml into the specified repo.    --help              Show this message and exit.  {noformat}      In w_2020_20  {noformat}  $ butler create --help  Usage: butler create [OPTIONS] [REPO]      Create an empty Gen3 Butler repository.      REPO is the URI or path to the new repository. Will be created if it does    not exist.    Options:    -C, --config-file TEXT  The path to the config file.    --standalone            Include all defaults in the config file in the repo,                            insulating the repo from changes in package                            defaults.    -o, --override          Allow values in the supplied config to override any                            repo settings.    -f, --outfile TEXT      Name of output file to receive repository                            configuration. Default is to write butler.yaml into                            the specified repo.    --help                  Show this message and exit.  {noformat}    ci_hsc_gen3 uses the {{-c}} option but wasn't updated. ",0.5
"DM-24979","05/19/2020 02:05:58","config_file_option does not save the 'help' input argument","in [https://github.com/lsst/daf_butler/blob/master/python/lsst/daf/butler/cli/opt/config_file.py#L27]         the 'help' input argument should be stored as a member var and passed to click.option in __call__",1
"DM-24982","05/19/2020 15:02:38","Install raspis and sensors at home and flash HD card","Two raspberry pi's are installed at the Recinto with temperature and cloud sensors attached to them. Pick them up, set them up at home and flash the SD card of the raspberry pi 4 with the latest image provided by Garry.",1
"DM-24985","05/19/2020 16:50:59","'file' needs to be an @property when it uses a testdata pacakge","in DM-24923 `IngestTestBase.file` was changed from a member variable to a class property. Some subclasses use their `testdata` package to define the value of `file`. These tests are skipped when the testdata package is not set up, but class properties are loaded import time and this fails when performing e.g. `os.path.join`. These class properties must be property functions with the @property decorator to prevent this error.     ",1
"DM-24995","05/19/2020 22:00:24","Create chronograf organizations for people to use at the base.","The commissioning and T&S people would like to use organizations to handle their dashboards.  It makes sense to provide that by making organizations with an admin per organization.  I will also hook up github orgs with each organization so people can self serve membership.",1
"DM-25000","05/20/2020 00:17:32","Using components in PipelineTaskConnections no longer seems to work.","I have a [pipeline task|https://github.com/lsst-dmsst/metric-pipeline-tasks/blob/master/tasks/MatchedCatalogs.py] that worked with w_2020_17 with {{ci_hsc_gen3}}.    With w_2020_19, it now returns {{UserWarning: QuantumGraph is empty}} whether I ask for `calexp.photoCalib` or `calexp.wcs`.  If I switch to using a stand alone dataset, `jointcal_photoCalib` the quantum graph looks as I expect and it allows my pipeline to run to completion.",2
"DM-25003","05/20/2020 17:49:12","RemoteCommand.set_start should not use data from the previous command","In ts_salobj {{RemoteCommand.set_start}} presently uses data from the previous command as default values for fields. This is dangerous. [~pingraham] reports he has been bitten by this a few times.    I propose to fix it by modifying {{RemoteCommand.set}} to create a new sample before setting the specified fields. That will also fix {{RemoteCommand.set_start}} and makes the two methods self-consistent.    I have tried that and found:  * ts_salobj unit tests all pass. Thus the old behavior was not tested, which is unfortunate but it was never intended to be a useful feature. The new behavior definitely deserves to be tested.  * ts_scriptqueue unit tests all pass. I was expecting some failure there, but apparently I wrote the tests to not rely on the old behavior. Good!  * ts_standard_scripts unit tests: one test fails so that script may need an update  * ts_observatory_control unit tests all pass",1
"DM-25006","05/20/2020 19:07:36","Update the efd client to use segwarides as the first source of credentials","Currently the efd client looks for a special file, but now we need to query the segwarides service first.",2
"DM-25007","05/20/2020 19:10:07","convert metrics pipeline tasks to use MetricTask","In putting together a prototype metric pipeline, we re-implemented some of the same things implemented in {{MetricTask}}.  We will just switch over to subclassing that.",2
"DM-25010","05/20/2020 21:14:58","Make Alert serialization optional in diaPipe.","Create a configurable setting in diaPipe to optionally enable or disable seralizing alert packets.",1
"DM-25014","05/20/2020 23:32:01","Coadds converted to Gen 3 without abstract_filter"," I have a Gen 3 repository that I created from https://github.com/lsst/ap_verify_ci_hits2015/tree/master/templates by running  {noformat}  convert_gen2_repo_to_gen3.py lsst.obs.decam.DarkEnergyCamera --gen2root ${AP_VERIFY_CI_HITS2015_DIR}/templates/ --gen3root `pwd`/hits2015_gen3/ --config ${AP_VERIFY_CI_HITS2015_DIR}/config/convertRepo_templates.py  {noformat}  using the {{w_2020_20}} stack. Trying to run image differencing using {{w_2020_20}} and these coadds gives  {noformat}  ValueError: Supplied dataset type (DatasetType(deepCoadd, {abstract_filter, skymap, tract, patch}, ExposureF)) inconsistent with registry definition (DatasetType(deepCoadd, {skymap, tract, patch}, ExposureF))  {noformat}  Since coadds should be parametrized by some kind of filter, this is presumably an error in the conversion.",3
"DM-25016","05/21/2020 01:51:07","DM-21724 unpickling error appears again ","Saving a QuantumGraph and run it with pipetask doesn't work. This is the same problem as DM-21724 but the fix got unfixed.     When trying to run a stored  QuantumGraph pickle with   {{pipetask run --qgraph name.pickle  -b  butler.yaml}}  the error is   {noformat}  _pickle.UnpicklingError: DimensionUniverse with version '0' not found.  Note that DimensionUniverse objects are not truly serialized; when using pickle to transfer them between processes, an equivalent instance with the same version must already exist in the receiving process.  {noformat}      Slack: https://lsstc.slack.com/archives/C2JPT1KB7/p1590018536265400 ",2
"DM-25020","05/21/2020 17:18:42","Update Sources.yaml with correct local background","In the current {{Sources.yaml}} which defines the conversion of {{src}} catalogs to {{sourceTable_visit}} parquet tables, the {{base_LocalBackground_instFlux}} is converted to {{sky}}.  However, as pointed out in DM-25019, the {{sky}} value should be the overall background value, and not the residual.  Therefore, {{base_LocalBackground_instFlux}} should be recorded non-transformed, while {{sky}} should have a temporary placeholder that is flagged with a DM-25019 TODO.",0.5
"DM-25028","05/21/2020 23:03:19","change butler create --config-file to --seed-config","change the {{butler create}} option {{--config-file}} to {{--seed-config}}. Keep the existing help text ""Path to an existing YAML config file to apply (on top of defaults).""    Change the shared {{--config-file}} option's default help text to be more specific: ""Path to a pex config override to be included after the Instrument config overrides are applied.""    TODO verify the new config-file help text.",2
"DM-25029","05/21/2020 23:51:12","make define-visits butler command","replace defineVisits from ci_hsc_gen3 with a butler subcommand.",8
"DM-25030","05/21/2020 23:53:36","make an import butler subcommand","replaces ingestExternalData from ci_hsc_gen3 with a butler subcommand; it should be a straightforward wrapper around {{Butler.import_}}    define the command in daf_butler    it should be able to read whatever the butler export generates  and we will consider (later) whether butler export | butler import works (with associated parameters)",8
"DM-25031","05/22/2020 00:18:52","Remove AuxTel specific keywords from ComCam/MainCam headers","The following keywords are specific to AuxTel and should be removed for ComCam/MainCam   *   INSTPORT   *   ATM3PORT   *   GRATING   *   GRATPOS",1
"DM-25032","05/22/2020 00:22:24","Fix comments in ComCam/MainCam header keywords that currently reference AuxTel","The following keyword comments need to be fixed to refer to MainTel systems in the ComCam/MainCam headers:     This should reference MTCS  TRACKSYS=                      / Tracking coordinate system from ATMCS       For MainTel, this is from the Camera Hexapod  FOCUSZ  =                   0. / Focus Z position from ATAOS/ATHexapod (mm)",1
"DM-25037","05/22/2020 18:02:23","Present DM Status at  LSSTC Board meeting "," LSSTC Board requested an update on DM status at their 4 June board meeting",1
"DM-25039","05/22/2020 18:43:30","Please allow Jenkins authentication when visiting any URL","The T&S Jenkins system https://tssw-ci.lsst.org/ gives a 404 error if one tries to go to a link and one's login has expired. This is a headache when trying to see the status of a job started on github, or even just visit a frequently-wanted page such as https://tssw-ci.lsst.org/job/LSST_Telescope-and-Site. I have to open a new browser to https://tssw-ci.lsst.org, log in there, wait for that to complete, and only then can I go to the other page and try the link again.    Please if at all possible enable login when visiting any page. Also please consider a longer time limit on login or some way to ""remember this browser"" (for those of us on secure computers, e.g. desktops).",1
"DM-25040","05/22/2020 21:28:55","ap_association uses physical filter in Gen 3","{{ap_association}} uses the Alert Production database, which has columns named by abstract filter (e.g., {{gPSFluxLinearSlope}}). We have tested this system extensively in Gen 2 with DECam and, to a lesser extent, HSC.    However, attempting to run {{ap_association}} on HSC in Gen 3 gives database errors such as  {noformat}  table DiaObject has no column named HSC-GPSFluxLinearSlope  {noformat}    I believe this bug can be fixed by consistently calling {{Filter.getCanonicalName}} instead of {{Filter.getName}} throughout the {{ap_association}} package. Unfortunately, it may be difficult to test the in Gen 3, since we don't yet have all the infrastructure in place (DM-21939).",2
"DM-25043","05/22/2020 22:55:51","Add AMX and TEX measurements to the new metrics pipeline","We have a proof of concept for a verification pipeline in the works on DM-25007.  It looks very promising.  Adding AMX and TEX will get it almost to the place where validate_drp is.",3
"DM-25045","05/23/2020 00:29:32","Review the Control Algorithm of Rotator in Phase 2","This task will review the rotator Simulink model and try to reproduce the error we had on the summit test that the demand rotator position can be -2000 peak. Another error we have is the peak between tracking targets. We will also investigate the stop profile in model. This is the phase 2 of DM-24799.",3
"DM-25046","05/23/2020 00:34:26","Replace Cython by Pybind11 in WEP","Replace the cython by pybind11 in cwfs module of ts_wep to follow the DM convention.",3
"DM-25047","05/23/2020 00:37:15","Reformat AOS Modules by Black","Reformat the AOS packages to black with the same coding style. There are four packages: ts_wep, ts_ofc, ts_phosim, and ts_MTAOS. The task is a place holder for DM-25062, DM-25063, DM-25065, and DM-25066 to calculate the story point.",1
"DM-25049","05/23/2020 02:40:07","Streamlined naming convention for the REST API resources in the Qserv Replication/Ingest System","The current implementation of the REST API provided by the Qserv Replication/Ingest system for exploring and managing Qserv may need to be improved. Presently, the API provides two groups of resources:  {code:}  /replication/v1/<service>  /ingest/v1/<service>  {code}  This naming scheme is based on a non-realistic (from the practical standpoint) assumption that multiple versions of the services would co-exist within the same Qserv deployment. This doesn't seem to be possible due to various technical difficulties in keeping and maintaining multiple versions in the code of the system.    The scheme also complicates the overall development and testing process of the services as the system evolves (gets improved and more refined) over time.    Besides, it's not possible for client application to figure out which version is presently deployed.    Hence, the current ticket represents a proposal to introduce a better service versioning mechanism which is meant to address the above stated issues. The idea is to introduce a special service for retrieving metadata on the service itself. This would include (at least) the following service:  {code}  /meta/version  {code}  The service would return a {{JSON}} object with the current version of system. A schema of the object will be finalized later. More services in this group could be added later if needed.    The second proposal is to drop explicit versioning (like: *{{v1}}*) from the resource names. After that the resource names would look like this:  {code:}  /replication/<service>  /ingest/<service>  {code}  In this case availability of services and their behavior will be completely driven by the current version of the services. Another benefit of this schema is that it would with documenting the REST API since each version of the documentation could refer to specific numbers returned by the proposed metadata service.",2
"DM-25057","05/25/2020 20:50:06","SAL's topic hash code generator not consistent","The code in ts_sal v4.1.2 that generates the has code for DDS topics seems to have a flaw. When I switched the Script SAL component to explicitly list the generic topics it uses (ts_xml tickets/DM-22109), the hash code for {{logevent_checkpoints}} changed (compared to ts_xml develop 133d378) even though {{logevent_checkpoints}} is not a generic topic and did not change at all. I see the same issue in ts_sal v4.1.1    This is quite worrisome for using the new ability to list generic topics as it means the new IDL will not be compatible with the old IDL even though the schemas have not changed.    I have attached the two IDL files.  ",1
"DM-25062","05/26/2020 16:03:52","Reformat ts_wep Python Repository by Black","Reformat ts_wep Python repository by black.",0
"DM-25063","05/26/2020 16:06:22","Reformat ts_ofc Python Repository by Black","Reformat ts_ofc Python repository by black.",0
"DM-25064","05/26/2020 16:07:54","Tests for PID in M1M3","Tests for PID code inside M1M3 SS. C++ code to verify PID functionality, look for possible runaways scenarios.",3
"DM-25065","05/26/2020 16:08:55","Reformat ts_phosim Python Repository by Black","Reformat ts_phosim Python repository by black.",0
"DM-25066","05/26/2020 16:10:19","Reformat ts_MTAOS Python Repository by Black","Reformat ts_MTAOS Python repository by black.",0
"DM-25067","05/26/2020 16:52:52","Dome software meetings","Prepare and coordinate weekly meetings on dome control software with EIE contractor.",5
"DM-25068","05/26/2020 17:01:54","Generate document on dome coditioning contract","As a component of the Environment Awareness System EAS ([https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873),|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873)] it's necessary to develop a dome environment conditioning system, to control the temperature during daytime. This is a continuation of DM-24835 to compose the technical support documents for the implementation contract.",3
"DM-25073","05/26/2020 20:27:59","Make AsyncS3Bucket capable of constructing the bucket","There are situations where S3 upload may not have the bucket we are trying to write to. Enhance AsyncS3Bucket to add the ability to create the bucket if it does not exist.    That might also be useful for the mock mode -- try it.",1
"DM-25074","05/26/2020 20:32:55","Add more useful info to the documentation","The current Dome documentation is pretty thin. Add more useful info, add version info and create a release.    Also, make sure that the flake8 line length is set to 79 characters.",2
"DM-25080","05/26/2020 22:19:09","Finish static typing in daf.butler.registry","This was mostly done over the weekend, originally intended to be a side-commit on DM-24613.  Might as well get it out for review on its own branch earlier.",1
"DM-25095","05/27/2020 04:16:24","Fix Jenkinsfile","Jenkinsfile introduced in DM-24610 breaks build on branch. Partly expected, the fix (due to unclear nature of env.CHANGE_TARGET) would be to use BRANCH_NAME if CHANGE_TARGET is null",0
"DM-25102","05/27/2020 19:03:16","Distribute lsst/alert_stream on PyPI","Write a setup.py script which describes lsst/alert_stream, and put it in the root of that repository.  Write a Makefile with directives for building and uploading to PyPI.  Upload an initial release. Use version v0.1.0, handled in the same manner as {{astro_metadata_translator}} in https://github.com/lsst/astro_metadata_translator/blob/master/setup.py.",2
"DM-25109","05/27/2020 19:42:40","Wrap up SAL changes for SALSubsystems.xml for a 4.1 hotfix","Make a hotfix patch to implement the recommended changes to SALSubsystems.xml in SAL",2
"DM-25110","05/27/2020 19:54:34","EAS Work Phase 5","This epic is used to hold all stories associated with the Environmental Awareness System.  Previous epics are:   * DM-17215   * DM-18732   * DM-20197   * DM-21536   * DM-23148",40
"DM-25112","05/27/2020 22:38:43","Update ci_lsst for bitrot","In trying to validate some changes (on DM-24858) in {{obs_lsst}} involving all detector ""flavors"" associated with that obs-package and requiring adaptations to tests, it was suggested that I try running {{ci_lsst}} (currently on lsst-dm github). It ran without failures for all the default cameras except {{latiss}}, which has suffered some bitrot of a sort. While we await the fully fleshed out ci integrations currently being worked on in cpp-land (e.g. DM-23302 & DM-23622), this ticket is to fix the bitrot so that at least some testing of {{obs_lsst}} changes not possible via unittests can be done.    Of note, the {{ci_lsst}} scripts are being tested and adapted here based on the data that currently resides on {{lsst-dev}} at {{/project/shared/data/test_data/testdata_lsst/raws}}.",3
"DM-25117","05/28/2020 14:40:46","Install ESS software in RPi4 and make sure it runs at system start up","Take the ESS sources from GitHub and install them in a RPi4 that is prepared with the SAL software stack. Also make sure that the software gets started at system start up.",2
"DM-25121","05/28/2020 17:48:18","Modify SAL rpm naming convention ","Change the SAL generated rpm naming convention to support the scheme outlined in https://confluence.lsstcorp.org/display/LTS/Proposal%3A+RPM+naming+convention+change",2
"DM-25130","05/28/2020 20:33:26","Replace YAML with JSON when sending commands from and receiving replies in the Dome CSC.","In the weekly Dome Software Meeting of 2020-05-28 it was decided to replace YAML with JSON when sending commands from and receiving replies in the Dome CSC. This ticket will take care of that and will also make sure that there will be a central function to construct the commands (in the Dome CSC) and replies (in the mock_controller simulator) that does this so a possible future technology switch could be done more easily.",2
"DM-25133","05/28/2020 21:06:07","Fix the race condition between the RPM daily job and the IDL Conda job","There is only about a minute between the end of the RPM Daily Build and the start of the IDL_Conda job.  This isn't quite enough time for the yum repo to finish indexing all the new RPMs, so the IDL job often fails as a result.  Figure out a way to have the IDL job wait until the desired RPM is available before trying to install it.",2
"DM-25134","05/28/2020 21:48:41","Test ADLink's fix for segfaults with the community edition of OpenSplice","ADLink provided a fix for the segfaults we see in CAP-529. This fix is for the licensed version. Apply the same change to the community edition of OpenSplice and see if it works there. If it does then submit a pull request for the community edition.    Also document the build and install procedure.",2
"DM-25137","05/28/2020 23:06:37","current_tai may return a numpy.float64 instead of a float","The functions {{tai_from_utc}} and {{tai_from_utc_unix}} return a {{numpy.float64}} instead of a float, and {{current_tai}} will also do so if not using {{CLOCK_TAI}}. This is a nuisance for encoding the data with yaml (and, probably, json). Fix these to always return float. This can be done by casting the value returned by {{tai_from_utc_unix}}. Test the result.    ",0
"DM-25152","05/29/2020 16:34:43","butler ingest raws does not register translators","[~spietrowicz] has discovered that butler ingest-raws does not work with LSST data because the metadata translators have not been registered. It seems the tests are passing because other tests import the translators.    I think a generic fix is to preimport all registered instrument classes before running the ingest itself.  This would cause all metadata translators to be imported. All these classes should exist.",2
"DM-25153","05/29/2020 17:00:29","Make it clear that gen3 instrument class paths don't need .instrument","In DM-23980 I standardized the instrument class names for Gen3. Doing so means that both {{lsst.obs.lsst.LsstCam}} and {{lsst.obs.lsst.instrument.LsstCam}} work. Since we don't want multiple ways of specifying the same instrument and we already know of cases where people think they need to include the {{.instrument}}, rename the internal {{instrument.py}} to {{_instrument.py}} to make it clearer that this name should not be used.    One caveat is that when we register the instrument we call getFullTypeName so if you specify the short name the registry still sees the full name and we therefore have to promise not to move that class later inside the package. One solution to that is for the instrument class to not use getFullTypeName but to ""know"" the public name of itself.",0.5
"DM-25156","05/29/2020 19:31:01","Gen 2->3 conversion of DECam repositories can give duplicate defects","If I convert a DECam Gen 2 repository containing defects, the conversion succeeds, but running {{ProcessCcd}} on the result gives an error:  {noformat}  lsst.pipe.base.connections.ScalarError: Found multiple datasets {instrument: DECam, calibration_label: defects/2015-01-05T01:15:00/60, detector: 60}, {instrument: DECam, calibration_label: gen2/defects_2015-01-05T01:15:00_060, detector: 60} for scalar connection defects (defects).  {noformat}  The immediate cause is that one copy of the defects is from the Gen 2 repository, the other copy was ingested during the conversion.    Discussion on [#dm-middleware|https://lsstc.slack.com/archives/C2JPT1KB7/p1590719387081900] suggests that this is due to conflicting responsibilities:  * {{obs.base.Instrument}} has some [hard-coded definitions|https://github.com/lsst/obs_base/blob/a51665517dd0ecacd8d72fbcead186475fcf66d3/python/lsst/obs/base/instrument.py#L38-L45] of curated files, which are processed any time {{ConvertRepoConfig.doWriteCuratedCalibrations}} is set. Defects are included.  * {{ConvertRepoConfig}} also has a [{{curatedCalibrations}} field|https://github.com/lsst/obs_base/blob/7c071951e367c4bef83840b75921f3a2f3af29cb/python/lsst/obs/base/gen2to3/convertRepo.py#L252-L263], which presumably allows other datasets to be handled using {{writeCuratedCalibrations}}. This list does not include defects by default.  * HSC [adds defects to {{ConvertRepoConfig.curatedCalibrations}}|https://github.com/lsst/obs_subaru/blob/c641360ad764cf8db63ecf1d14af6c93660160ff/config/hsc/convertRepo.py#L35-L36]. The bug described above *does not* occur.  * DECam makes no changes to {{curatedCalibrations}}, and its [config override file|https://github.com/lsst/obs_decam/blob/8567ae41add358da7b9af65fc81c39e39078b975/config/convertRepo.py] makes no mention of defects. The bug *does* occur.    The above behavior implies that DECam repositories with defects cannot be converted using the default configs, which goes against our general aim to provide working defaults for all Tasks.    Possible solutions:  * Do away with {{ConvertRepoConfig.curatedCalibrations}}, as proposed by [~tjenness]. However, this does raise the question of how to handle observatory-specific curated calibrations.  * Have DECam explicitly register defects in {{curatedCalibrations}}, as HSC does. May be hard to scale to other instruments.  * Duplicate the dataset types hardcoded into {{Instrument}} in the default {{ConvertRepoConfig.curatedCalibrations}}. Won't benefit configs that overwrite this field instead of appending to it.",2
"DM-25170","05/29/2020 23:59:47","Fix to get predicted butler URIs","Getting predicted butler URIs crashed with     {noformat}      primary, components = butler.getURIs(thisRef, predict=True, run=""TBD"")    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-83-gf58ce13b/python/lsst/daf/butler/_butler.py"", line 830, in getURIs      ref = ref.resolved(id=0, run=self.run)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-83-gf58ce13b/python/lsst/daf/butler/core/datasets/ref.py"", line 182, in resolved      id=id, run=run, hash=self.hash, conform=False)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-83-gf58ce13b/python/lsst/daf/butler/core/datasets/ref.py"", line 106, in __new__      raise ValueError(f""Cannot provide id without run for dataset with id={id}, ""  ValueError: Cannot provide id without run for dataset with id=0, type=DatasetType(postISRCCD, {abstract_filter, instrument, detector, physical_filter, exposure}, ExposureF), and dataId={instrument: HSC, detector: 18, exposure: 903990}.  {noformat}      I think [this line|https://github.com/lsst/daf_butler/blob/master/python/lsst/daf/butler/_butler.py#L826]  means to use {{run}}, not {{self.run}}, in the arguments. ",0.5
"DM-25171","05/30/2020 01:01:22","Update translator for phosim to use nominal weather defaults","When ingesting/processing {{phosim}} data, one encounters the following messages:  {code:python}astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'temperature' using translator <class 'lsst.obs.lsst.translators.phosim.LsstPhoSimTranslator'>: ""Could not find ['TEMPERA'] in header""  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'pressure' using translator <class 'lsst.obs.lsst.translators.phosim.LsstPhoSimTranslator'>: ""Could not find ['PRESS'] in header""  astro_metadata_translator.observationInfo WARN: Ignoring Error calculating property 'boresight_airmass' using translator <class 'lsst.obs.lsst.translators.phosim.LsstPhoSimTranslator'>: ""Could not find ['AIRMASS'] in header""  LsstCamAssembler WARN: argDict[boresightAirmass] is None; stripping  {code}  If {{phosim}} is never going to give us weather it would be good make the translator use nominal defaults to and not complain about it.",1
"DM-25177","06/01/2020 16:18:04","pipelines.lsst.io broken by missing safeFileIo in daf_butler","e.g. https://ci.lsst.codes/job/sqre/job/infra/job/documenteer/883/console:    {code}  [autosummary] generating autosummary for: getting-started/coaddition.rst, getting-started/data-setup.rst, getting-started/display.rst, getting-started/index.rst, getting-started/multiband-analysis.rst, getting-started/photometry.rst, getting-started/processccd.rst, index.rst, install/demo.rst, install/docker.rst, ..., releases/v13_0_qserv_dax.rst, releases/v13_0_sui.rst, releases/v14_0.rst, releases/v15_0.rst, releases/v16_0.rst, releases/v17_0.rst, releases/v18_0_0.rst, releases/v18_1_0.rst, releases/v19_0_0.rst, tasks.rst    Traceback (most recent call last):    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/documenteer/sphinxrunner.py"", line 84, in run_sphinx      warningiserror, tags, verbosity, jobs)    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx/application.py"", line 233, in __init__      self._init_builder()    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx/application.py"", line 311, in _init_builder      self.emit('builder-inited')    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx/application.py"", line 444, in emit      return self.events.emit(event, self, *args)    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx/events.py"", line 79, in emit      results.append(callback(*args))    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx_automodapi/automodsumm.py"", line 255, in process_automodsumm_generation      lines = automodsumm_to_autosummary_lines(sfn, app)    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx_automodapi/automodsumm.py"", line 327, in automodsumm_to_autosummary_lines      filestr = automodapi.automodapi_replace(fr.read(), app, True, docname, False)    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx_automodapi/automodapi.py"", line 277, in automodapi_replace      modnm, toskip, onlylocals=onlylocals)    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx_automodapi/automodapi.py"", line 388, in _mod_info      for localnm, fqnm, obj in zip(*find_mod_objs(modname, onlylocals=onlylocals)):    File ""/j/ws/sqre/infra/documenteer/doc_template/home/.local/lib/python3.7/site-packages/sphinx_automodapi/utils.py"", line 78, in find_mod_objs      __import__(modname)  ModuleNotFoundError: No module named 'lsst.daf.butler.core.safeFileIo'  {code}",1
"DM-25178","06/01/2020 17:45:18","Add parameter support to formatter configuration","In order to implement DM-13353 we need to be able to specify parameters in datastore configs associated with the write formatters.    We must assume that parameters are never required for reading with a formatter since that would break our contract with the user that any change to configuration of a datastore should not break the ability to read a file.",3
"DM-25179","06/01/2020 17:56:26","Add circular mock actuators to ts_simactuators","Add circular versions of PointToPoint and TrackingActuator to to ts_simactautors. By ""circular"" I mean an a rotation with no limits and the ability to take the shortest path to a given target or go in the positive or negative direction.    It will always report position in the range [0, 360) degrees but accept any angle for the commanded angle.    Note on implementation: the most naive implementation is to increment or decrement the internal angle as required. By my calculations it would take a millions of wraps before we lose 0.1"" accuracy due to roundoff errors. One could get fancier and check if the absolute value of position is > 720 and reduce all elements of the path if so.",3
"DM-25182","06/01/2020 21:17:06","Write procedure for adding credentials to segwarides","Documentation is needed to describe adding credentials to segwarides.  It is essentially just executing {{vault kv patch}} with the correct paths and data, but still needs to be written down concretely.",1
"DM-25184","06/02/2020 00:05:54","Make obs_subaru config overrides play nice with new jointcal filterMap config","Monika Adamow  3:37 PM:  When I try to run   {code} jointcal.py /datasets/hsc/repo --calib /datasets/hsc/repo/CALIB --rerun RC/w_2020_22/DM-25176-sfm:RC/w_2020_22/DM-25176 --id ccd=0..8^10..103 visit=26024^26028^26032^26036^26044^26046^26048^26050^26058^26060^26062^26070^26072^26074^26080^26084^26094 filter=HSC-G tract=9615  {code}    with w_2020_22 I get  {code}  jointcal.Associations INFO: Unmatched objects: 2  jointcal.Associations INFO: Matched 49 objects in 26094_103  jointcal.Associations INFO: Unmatched objects: 2  jointcal FATAL: Failed processing tract 9615, RuntimeError: Unknown reference filter phot_g_mean_flux  {code}    Lauren MacArthur  3:45 PM  I *think* that’s the gaia flux column, but is HSC still using ps1 for jointcal (astrom)?    Lauren MacArthur  3:57 PM  The filterMap is now getting loaded with the gaia entries:  in /datasets/hsc/repo/rerun/RC/w_2020_22/DM-25176/config/jointcal.py  # Mapping of camera filter name: reference catalog filter name; each reference filter must exist  config.astrometryRefObjLoader.filterMap={'u': 'phot_g_mean', 'g': 'phot_g_mean', 'r': 'phot_g_mean', 'i': 'phot  _g_mean', 'z': 'phot_g_mean', 'y': 'phot_g_mean', 'B': 'g', 'V': 'r', 'R': 'r', 'I': 'i', 'r2': 'r', 'i2': 'i',   'N387': 'g', 'N468': 'g', 'N515': 'g', 'N527': 'g', 'N656': 'r', 'N718': 'i', 'N816': 'i', 'N921': 'z', 'N926'  : 'z', 'N973': 'y', 'N1010': 'y', 'I945': 'z', 'HSC-G': 'g', 'HSC-R': 'r', 'HSC-R2': 'r', 'HSC-I': 'i', 'HSC-I2  ': 'i', 'HSC-Z': 'z', 'HSC-Y': 'y', 'NB0387': 'g', 'NB0468': 'g', 'NB0515': 'g', 'NB0527': 'g', 'NB0656': 'r',   'NB0718': 'i', 'NB0816': 'i', 'NB0921': 'z', 'NB0926': 'z', 'NB0973': 'y', 'NB1010': 'y', 'IB0945': 'z'}  # Name of the ingested reference dataset  config.astrometryRefObjLoader.ref_dataset_name='ps1_pv3_3pi_20170110'  vs. /datasets/hsc/repo/rerun/RC/w_2020_19/DM-24822/config/jointcal.py  # Mapping of camera filter name: reference catalog filter name; each reference filter must exist  config.photometryRefObjLoader.filterMap={'B': 'g', 'V': 'r', 'R': 'r', 'I': 'i', 'r2': 'r', 'i2': 'i', 'N387':   'g', 'N468': 'g', 'N515': 'g', 'N527': 'g', 'N656': 'r', 'N718': 'i', 'N816': 'i', 'N921': 'z', 'N926': 'z', 'N  973': 'y', 'N1010': 'y', 'I945': 'z', 'HSC-G': 'g', 'HSC-R': 'r', 'HSC-R2': 'r', 'HSC-I': 'i', 'HSC-I2': 'i', '  HSC-Z': 'z', 'HSC-Y': 'y', 'NB0387': 'g', 'NB0468': 'g', 'NB0515': 'g', 'NB0527': 'g', 'NB0656': 'r', 'NB0718':   'i', 'NB0816': 'i', 'NB0921': 'z', 'NB0926': 'z', 'NB0973': 'y', 'NB1010': 'y', 'IB0945': 'z'}  # Name of the ingested reference dataset  config.photometryRefObjLoader.ref_dataset_name='ps1_pv3_3pi_20170110'    3:57  Ah, I think it’s appending to this (now added by default) list:  https://github.com/lsst/jointcal/commit/41e890198605efd737f5d0b3035ce6271dad9c1c    Lauren MacArthur  4:02 PM  ^^ @parejkoj: it looks like that default is not safe if astrometryRefObjLoader.ref_dataset_name gets overridden (or perhaps obs_* packages have to be careful about appending vs. setting?    John Parejko  1 hour ago  [~madamow]: I haven’t tested it yet, but I think you can add this at obs_subaru/config/jointcal.py:10:  config.astrometryRefObjLoader.filterMap = {}  (edited)",1
"DM-25187","06/02/2020 14:16:21","daf_butler registry EllipsisType breaks pipelines.lsst.io doc build","The pipelines.lsst.io build failed this morning with  {code}  FileNotFoundError: [Errno 2] No such file or directory: '/j/ws/sqre/infra/documenteer/doc_template/_build/doctree/py-api/lsst.daf.butler.registry.wildcards.EllipsisType.doctree'  {code}    This is presumably due to the lack of documentation for this type despite its presence in {{\_\_all\_\_}}.",1
"DM-25188","06/02/2020 14:46:26","Sends modbus commands from M1M3cli","Add the ability to M1M3cli app to send Modbus commands to ILC. That will enable execution and debugging of connections with ILCs.",1
"DM-25192","06/02/2020 17:53:30","Switch from Travis to GitHub Actions in daf_butler","We have recently been noticing delays in Travis communiction. We have been given permission to switch daf_butler to GitHub actions as a test bed.    To simplify dev guide documentation later we have been advised to do flake8 in one workflow and mypy in a second.",1
"DM-25203","06/02/2020 23:18:30","Process various releases and make build harness changes as necessary","This sprint includes 3+ releases, SAL v4.1.3, SAL v4.1.4 and XML v6.0.  This task covers the time necessary to process the releases and deal with any fallout or issues that result.     Also, the XML 6.0 release will be built against SAL v4.1.1.  The current build harness isn't quite ready to build against an older version, so this task will also cover the work to update the build accordingly.",3
"DM-25204","06/02/2020 23:19:18","Update code for changes to simactautors","DM-25179 introduces some breaking changes to ts_simactuators. Update other code accordingly.",2
"DM-25207","06/02/2020 23:53:38","Update SAL/XML test suite generators to use the Generics field","The Generics field of ts_xml/sal_interfaces/SALSubsystems.xml, will define which Generic topics the CSC is using.  It will either be ""yes"" if it uses them all, or a comma-delimited list of specific topics.  This requires adding some infrastructure to the suite generators.    Also, there should be an XML unit test to verify this field is either yes, or contains valid topic values.",2
"DM-25208","06/03/2020 00:38:49","Fix broken test in lsst/alert_packet master","{{python -m pytest .}}  does not succeed on the master branch of github.com/lsst/alert_packet, at least for me working locally. We should fix this.    The error I get is this:    {code}  ======================== test session starts =========================  platform linux -- Python 3.8.2, pytest-5.4.3, py-1.8.1, pluggy-0.13.1  rootdir: /home/swnelson/code/rubin/alert_packet  collected 6 items                                                        test/test_io.py .                                              [ 16%]  test/test_schema.py ..                                         [ 50%]  test/test_schemaRegistry.py .                                  [ 66%]  test/test_schemas.py F.                                        [100%]    ============================== FAILURES ==============================  ______________ SchemaValidityTestCase.test_example_avro ______________    self = <test.test_schemas.SchemaValidityTestCase testMethod=test_example_avro>        def test_example_avro(self):          """"""Test that example data in Avro format can be loaded by the schema.          """"""          no_data = (""1.0"",)  # No example data is available.          bad_versions = (""2.0"",)  # This data is known not to parse.                for version in self.registry.known_versions:              path = path_to_sample_data(get_schema_root(), version,                                         ""fakeAlert.avro"")              schema = self.registry.get_by_version(version)                    if version in no_data:                  self.assertFalse(os.path.exists(path))              else:                  with open(path, ""rb"") as f:                      if version in bad_versions:                          with self.assertRaises(RuntimeError):                              schema.retrieve_alerts(f)                      else:                          retrieved_schema, alerts = schema.retrieve_alerts(f)  >                       self.assertEqual(retrieved_schema, schema)  E                       AssertionError: <lsst.alert.packet.schema.Schema object at 0x7f189cd8c940> != <lsst.alert.packet.schema.Schema object at 0x7f189cd14b50>    test/test_schemas.py:77: AssertionError  ====================== short test summary info =======================  FAILED test/test_schemas.py::SchemaValidityTestCase::test_example_avro  ==================== 1 failed, 5 passed in 0.29s =====================  {code}     ",2
"DM-25216","06/03/2020 16:20:10","ap_verify failure: ap_verify.py: error: no config field: diaPipe.doSerializeAlerts","In Jenkins overnight: https://ci.lsst.codes/blue/organizations/jenkins/scipipe%2Fap_verify/detail/ap_verify/596/pipeline",1
"DM-25217","06/03/2020 16:40:09","Reinstall CentOS and document all steps","Now that CentOS is running on the RPi4, reinstall it and document all steps. The document can be found at    [https://confluence.lsstcorp.org/display/LTS/Installation+instructions+CentOS+7+on+Raspberry+Pi+4]",2
"DM-25222","06/03/2020 18:10:43","Error with --init-only --skip-existing","What lead up to this problem is that I am back to trying to make ctrl_bps work with current version of the stack with as minimal effort as possible (i.e., it works then start making bigger changes).  I am testing with a sqlite3 ci_hsc_gen3 repo created with this week's stack (w_2020_22).   With [~jbosch]'s help I had made enough changes (e.g. --output-run --extend-run) that it ran through all of isr, but failed when it tried to run the --init-only quanta for the next step, charImage with the following constraint error message:   {noformat}  sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: dataset_collection_0000.dataset_type_id, dataset_collection_0000.collection_id  [SQL: INSERT INTO dataset_collection_0000 (dataset_type_id, dataset_id, collection_id) VALUES (?, ?, ?)]  [parameters: (208, 1318, 13)]  {noformat}  Then I remembered the discussion about needing to do something extra with the --init-only calls.  Per the discussion in DM-24797, I added --skip-existing to my pipetask calls with --init-only.   Now it fails on the isr --init-only (i.e., the first quanta executed).    command line:  {noformat}  /software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/19.0.0-20-gab3bd24+4/bin/pipetask run -b /scratch/mgower/Summer2020_bps_cihsc/git/ci_hsc_gen3/DATA/butler.yaml -i calib/hsc,raw/hsc,masks/hsc,ref_cats,skymaps --output-run G3W22_000002_004 --extend-run --init-only --skip-existing --register-dataset-types --qgraph quantum000001.pickle{noformat}  output:  {noformat}  ctrl.mpexec.cmdLineFwk INFO: QuantumGraph contains 1 quanta for 1 tasks  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/19.0.0-20-gab3bd24+4/bin/pipetask"", line 26, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/19.0.0-20-gab3bd24+4/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 493, in parseAndRun      return self.runPipeline(qgraph, taskFactory, args)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/19.0.0-20-gab3bd24+4/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 680, in runPipeline      saveVersions=not args.no_versions)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/19.0.0-20-gab3bd24+4/python/lsst/ctrl/mpexec/preExecInit.py"", line 90, in initialize      self.saveConfigs(graph)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/19.0.0-20-gab3bd24+4/python/lsst/ctrl/mpexec/preExecInit.py"", line 228, in saveConfigs      oldConfig = self.butler.get(configName, {})    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-88-gc8bbe74d/python/lsst/daf/butler/_butler.py"", line 773, in get      ref = self._findDatasetRef(datasetRefOrType, dataId, collections=collections, **kwds)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-88-gc8bbe74d/python/lsst/daf/butler/_butler.py"", line 569, in _findDatasetRef      raise LookupError(f""Dataset {datasetType.name} with data ID {dataId} ""  LookupError: Dataset isr_config with data ID {} could not be found in collections [G3W22_000002_004: ..., calib/hsc: ..., raw/hsc: ..., masks/hsc: ..., ref_cats: ..., skymaps: ...].  {noformat}  I have not changed ctrl_bps to do all init's in a single pipetask call, but from the ticket it didn't sound like that was a requirement.",1
"DM-25232","06/04/2020 06:11:46","Minor refactoring in Qserv due to migration to Protobuf version 3 API","After migrating the Qserv dependencies to Protobuf version 3 the following warnings showed during compilation stage (this is just one example):  {code}  core/modules/replica/ProtocolBuffer.h:111:24: warning: 'int google::protobuf::MessageLite::ByteSize() const' is deprecated: Please use ByteSizeLong() instead [-Wdeprecated-declarations]           uint32_t const bytes = message.ByteSize();  {code}  There are three sources of the warnings in Qserv modules:  {code:bash}  % fgrep 'ByteSize()' core/modules/*/*  core/modules/proto/FrameBuffer.h:        uint32_t const messageLength = message.ByteSize();  core/modules/replica/ProtocolBuffer.h:        uint32_t const bytes = message.ByteSize();  core/modules/wdb/QueryRunner.cc:        tSize += rawRow->ByteSize();  {code}  Hence, a goal of the development is to migrate to the newer API.",0.5
"DM-25236","06/04/2020 17:20:17","Temporarily remove quantum tables from Registry","Getting the quantum database representation stable and future-proof requires more thought than we want to put into it before gen2 deprecation, so this ticket is a near-term replacement for DM-24613, which we'll pick up again later.  See also https://lsstc.slack.com/archives/C2JPT1KB7/p1591284060371700    Preliminary plan is to remove the quantum and dataset_consumer tables, along with the dataset.quantum_id column.  It'll be easier to add those back in than alter them to match what we do in the future.",1
"DM-25243","06/04/2020 21:39:16","Fix the Bugs of Trajectory If No Track Command Received In the Tracking","The path generator will continuously update the demand position even if there is no new track command received in the SlewAndTrack sub-state. The initial suspect is that this comes from the non-zero target velocity in the final received track command. This will continue to update the position command. The initial idea to fix this is to reset the velocity command to be zero if no new track command received.    The following figure (x-axis: time in seconds, y-axis: value such as position in degree) shows this problem. There are two targets (positions = 1.2 degree and 3 degree. velocity = 0.01 deg/sec and -0.01 deg/sec). The track command does not continue after the time equals 6 seconds. We could see the rotator position is at 3 degree when time is at 8 seconds but the demand position is decreasing after time=8 seconds (rotator's position keeps the same) because of the second target has the velocity of -0.01 deg/sec.    !track_v0_1_long_enterFault.png|thumbnail!     ",3
"DM-25245","06/04/2020 21:56:15","Put the Rotator into Fault State if no Track Command for a Long Time","If the rotator does not receive the track command for a long time, the rotator should be put into the Fault state. This could be 0.5 second (by Tiago) or 1 second (by Russell). At this moment, we do see this in the Simulink model by the simulation experiment, but we do not know how that is decided. In addition, we did not see this when we did the test on summit. This task needs to figure out what is the existed logic in model and how to update it.    The following is the figure to show the simulation result (detail is at DM-25243):    !track_v0_1_long_enterFault.png|thumbnail!    The system is at the Fault state around time equals 20.2 seconds (no new track command after time=6 seconds).",2
"DM-25246","06/04/2020 22:07:02","Make version of RC2 bootstrap script that can convert reruns as well","Modify [a copy of] the gen3-hsc-rc2 bootstrap.py script that can convert the outputs of one or more standard RC2 processing runs.    This should just be a matter of updating the configured dataset types and passing Rerun arguments to ConvertRepoTask.run, and then inventing an interface that allows the user to specify which processing runs to convert.  ",2
"DM-25249","06/04/2020 22:30:18","Track the Multiple Targets","The rotator Simulink model should be able to track multiple targets. The system will fail/ crash if there are targets that have different directions. In addition, sometimes, the simulation will fail even though all targets are in the same direction. This task will dig into the Simulink model to see how is fitting of multiple targets.    The following figure demonstrates the failure. The x-axis is the time in second and the y-axis is the value such as the rotation position in degree. There are two targets with positions of +1.5 and -1.5 degree. The simulation time is 30.1 seconds and the simulation crashes in the middle with zero-crossing error.      !trackErrorForTargetsWithDifferentDirections.png|thumbnail!",2
"DM-25250","06/04/2020 22:39:28","Adjust DCS and the mock controller to the new JSON protocols and LLC states and statuses","In the Dome Software Meeting of 2020-06-04 new JSON-based command and configuration protocols were agreed upon, as well as new LLC states and statuses. This ticket is to make the necessary changes to the Dome CSC and mock controller to adopt those changes.    Protocol details, with mention to the response codes, can be found here    https://confluence.lsstcorp.org/display/LTS/Command+and+Configuration+Protocols",2
"DM-25263","06/05/2020 01:38:42","Document non-partitioned table ingest ","Include description of non-partitioned table support in documentation for the new Qserv ingest system",1
"DM-25280","06/05/2020 05:38:21","ap_association broken by changes to alert_packet","https://ci.lsst.codes/blue/organizations/jenkins/scipipe%2Flsst_distrib/detail/lsst_distrib/1021/pipeline    {code}  ../../miniconda/envs/lsst-scipipe-1a1d771/lib/python3.7/site-packages/py/_path/local.py:701:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  from .version import *  from .association import *  from .diaForcedSource import *  from .mapApData import *  from .afwUtils import *  from .diaCalculation import *  from .diaCalculationPlugins import *  from .loadDiaCatalogs import *  > from .packageAlerts import *  python/lsst/ap/association/__init__.py:30:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  __all__ = (""PackageAlertsConfig"", ""PackageAlertsTask"")  import os  import lsst.afw.fits as afwFits  > import lsst.alert.packet as alertPack  E ModuleNotFoundError: No module named 'lsst.alert'  {code}    This will break tonight's nightly build unless we get a quick fix in soon.",1
"DM-25286","06/05/2020 18:05:06","Update OpenSplice builds with latest community edition","Update the builds of OpenSplice (rpm's and ts_opensplice) to the   latest version from the community edition github repo",2
"DM-25288","06/05/2020 18:07:12","Update setup.py to include Header Templates","The naming of the folder with the templates for the HeaderService changed when adding ComCam support, but the {{setup.py}} failed to capture this and therefore the conda packages are now faulty.",1
"DM-25292","06/05/2020 19:36:11","ComCam Instrument.name and translator instrument name differ","[~spietrowicz] has discovered that I messed up the comCam instrument name when I normalized them in DM-23980. I ended up with LSSTComCam in the gen3 instrument and LSST-ComCam in the metadata translator.    Standardize on LSSTComCam to match LSSTCam.",0.5
"DM-25295","06/05/2020 20:58:46","Introduce a more intelligent HVAC naming scheme to avoid overly long topic names","The HVAC SAL topic names are deduced from the Spanish MQTT topic names where special characters have been replaced with Spanish descriptions of those special characters. This leads to overly long topic names like    HVAC_lsstBarraoblPiso01BarraoblTccGuionP1GuionSalaGuionMaquinas    Come up with a naming convention such that those long topic names can be made much shorter and still can uniquely be converted from the SAL topic names to the MQTT topic names and back. Then implement that in both ts_xml and ts_hvac.",3
"DM-25307","06/08/2020 05:37:12","Create docker images HeaderService for salobj 5.14.0 and v2.2.1","New docker images are needed for the HS using   HeaderService: 2.2.1  salobj: v5.14.0  idl:v1.2.0  xml:v5.1.0   sal: 4.1.1",2
"DM-25308","06/08/2020 14:45:46","Change Health & Status data to direct FIFO","Health & Status data are routed in FPGA through the Request queue. The 64bit data are then squeezed into 4 16bit Response FIFO values. This design is suboptimal for two reasons:   * response queue is also used to send Modbus responses from ILCs, the mix effectively prevents multi-threading on CPU (Host) side to query H&S data and responses at the same time   * as H&S data are mixed with responses, it's hard to tell which data on reposne queue are from H&S and which are from ModBus - that is probably reason why vital H&S collection was implemented in FPGA, but not in C/C++ controller (where it most likely never worked)",2
"DM-25311","06/08/2020 16:59:01","Document the Track Simulation in Rotator Simulink Model","Document the track simulation in rotator Simulink model. This is a follow-up task of DM-25045 to summarize and analyze the test results.",2
"DM-25312","06/08/2020 17:16:13","Dome software meetings","Prepare and coordinate weekly meetings on dome control software with EIE contractor.",5
"DM-25313","06/08/2020 17:19:08","Generate document on dome coditioning contract","As a component of the Environment Awareness System EAS ([https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873),|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873)] it's necessary to develop a dome environment conditioning system, to control the temperature during daytime.",3
"DM-25323","06/08/2020 20:05:34","Switch PropertySet in gen3 to YAML formatter","We seem to have defaulted to writing PropertySet/List in pickle format in gen3 even though it's using YAML in gen2. Switch to YAML and add a quick test in obs_base.",1
"DM-25325","06/08/2020 20:50:11","Write Packages object as YAML","We currently write Packages out as Pickle but we would prefer to write them out as YAML.",1
"DM-25327","06/08/2020 22:28:22","Change gen3 to write Packages in YAML format","In DM-25325 I updated base.Packages to allow YAML format to be written. Now I need to change daf_butler to write them out in YAML and obs_base to check that butler can read them properly.    I've added YAML representers so that a simple change to the formatters config yaml to use YamlFormatter should do it. We could also use a specialist formatter that knows how to do read/write with writeParameters controlling the file format...",3
"DM-25333","06/09/2020 00:59:53","Please add target events to Dome","Whenever the Dome CSC is commanded to a new azimuth or elevation, if the command is accepted then it should output a ""target"" event that gives the details. That way other code, such as the MTDomeTrajectory CSC, can see what's going on.    I suggest these two events (rename as you see fit):  {code}  targetAzimuth:  * position (deg)  * velocity (deg/sec)  * taiTime (unix seconds)    targetElevation:  * position (deg)  * velocity (deg/sec)  * taiTime (unix seconds)  {code}    If we cannot get EIE to give us ""move position velocity time"" then you may have to ditch the ""tai"" fields. But I am guessing we can either get that, or something close enough that you can fill out the tai field with at least a reasonable guess.    Once you know what the louver commands will be, I suggest the same sort of event for that data -- and similarly for other systems the Dome controls.",0
"DM-25339","06/09/2020 18:02:34","Please clean up the Dome events.","The existing Dome {{inPosition}} event has several serious problems:  * It has no boolean flag indicating whether or not the specified device is in position. We need to know ""is in position"" and ""is not in position"".  * It has a {{deviceId}} attribute, which badly breaks salobj's ability to automatically output new events only when data has changed. The most recent output is likely to be for a different device! Thus we will get far too many of these events, even when nothing has changed.  * The {{devicePosition}} field is not not useful (especially if you implement DM-25333) and is inadequate for the azimuth (without an associated velocity and time).    I suggest:  * A different inPosition event for each device/axis, e.g.:  * azimuthInPosition(inPosition=True/False)  * elevationInPosition(inPosition=True/False)  * etc.    An alternative is to have one or several events with multiple fields, e.g.:  * inPosition(azimuth=True/False, elevation=True/False, ...)  but it only makes sense to combine data that you get in a single status call. Otherwise you imply information that you don't have!    The same basic problem applies to most other events, including {{motionEnabled}} and {{driveFault}}, {{overTemp}}, {{speedLimitReached}}, {{accelerationLimitReached}} (why do those last two even exist?).  * It would be much better to have a separate event for each device  * We need a flag or other simple way to tell ""this condition is present"" and ""this condition is absent"".    Other changes to consider:  * Remove the word {{Change}} from all events that have that suffix. ""Change"" is redundant. The event is being output *because* the state changed.  * Combine {{lockingPinEngaged}} and {{lockingPinDisengaged}} as {{lockingPin}} with a boolean {{engaged}} flag.  * Combine {{brakeEngaged}} and {{brakeDisengaged}} as {{brake}} with a boolean {{engaged}} flag.  * Combine {{motionEnabled}} and {{driveFault}} into {{driveState}}, with an enum field for enabled/not enabled/fault and another field for the fault code (which is null if enabled/disabled).  * Rename {{interlockAlarm}} field {{interlockAlarmCode}} to {{code}} and look for similar redundancies in other topics. There are a lot of fields that seem needlessly verbose (why ""devicePosition"" instead of ""position"", etc.)  ",1
"DM-25345","06/09/2020 19:51:14","base fails to build standalone","After YAML support was added to base Packages in DM-25325 we started to get failures in standalone builds of {{base}} the first time scons was typed. It seems that the YAML constructor assumes that {{Packages}} full name is {{base.packages.Packages}} rather than {{lsst.base.packages.Packages}}.  This causes the {{isinstance}} test to fail. Running the tests a second time fixes the problem for reasons we aren't entirely sure about.    The fundamental problem seems to be that we are using namespace packages in {{base}} and do not have a {{\_\_init\_\_.py}}. Adding one fixes the problem.",0.5
"DM-25346","06/09/2020 19:52:58","Modernize ts_ATDomeTrajectory","Update the code in ts_ATDomeTrajectory based on lessons learned from ts_MTDomeTrajectory.    The main change is to support more sophisticated algorithms by sending the next target event to the algorithm.    Additional changes wanted:  * Eliminate use of astropy angles; we aren't gaining any benefit and they add clutter.  * Use {{handle_summary_state}} instead of {{report_summary_state}}.",1
"DM-25347","06/09/2020 20:24:21","Integer out of range when trying to convert RC2 rerun using postgresql","While running gen3-hsc-rc2/bootstrap.py on recent RC2 rerun (w_2020_22) using PG, it failed with the following.  {noformat}  INFO  2020-06-09T14:03:29.926-0500 convertRepo - Ingesting 241 deepCoadd_ref datasets into run RC2/w_2020_22/DM-25176/remainder.  INFO  2020-06-09T14:03:31.115-0500 convertRepo - Ingesting 241 objectTable datasets into run RC2/w_2020_22/DM-25176/remainder.  INFO  2020-06-09T14:03:32.539-0500 convertRepo - Ingesting 3 objectTable_tract datasets into run RC2/w_2020_22/DM-25176/remainder.  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1228, in _execute_context      cursor, statement, parameters, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py"", line 857, in do_executemany      cursor.executemany(statement, parameters)  psycopg2.errors.NumericValueOutOfRange: integer out of range  The above exception was the direct cause of the following exception:Traceback (most recent call last):    File ""./gen3-hsc-rc2/bootstrap.py"", line 269, in <module>      main()    File ""./gen3-hsc-rc2/bootstrap.py"", line 265, in main      continue_=options.continue_, reruns=reruns)    File ""./gen3-hsc-rc2/bootstrap.py"", line 216, in run      visits=makeVisitList(tracts, filters)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-72-g37abf38+2/python/lsst/obs/base/gen2to3/convertRepo.py"", line 572, in run      converter.ingest()    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-72-g37abf38+2/python/lsst/obs/base/gen2to3/repoConverter.py"", line 495, in ingest      self.task.butler3.ingest(*datasetsForType, transfer=self.task.config.transfer, run=run)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/core/utils.py"", line 249, in inner      return func(self, *args, **kwargs)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/_butler.py"", line 1214, in ingest      self.datastore.ingest(*datasets, transfer=transfer)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/core/datastore.py"", line 559, in ingest      self._finishIngest(prepData, transfer=transfer)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/core/utils.py"", line 249, in inner      return func(self, *args, **kwargs)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/datastores/fileLikeDatastore.py"", line 736, in _finishIngest      self._register_datasets(refsAndInfos)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/datastores/genericDatastore.py"", line 155, in _register_datasets      self.addStoredItemInfo(expandedRefs, expandedItemInfos)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/datastores/fileLikeDatastore.py"", line 327, in addStoredItemInfo      self._table.insert(*records)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/registry/opaque.py"", line 67, in insert      self._db.insert(self._table, *data)    File ""/scratch/mgower/rc2_convert_w19/daf_butler/python/lsst/daf/butler/registry/interfaces/_database.py"", line 955, in insert      self._connection.execute(table.insert(), *rows)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 984, in execute      return meth(self, multiparams, params)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/sql/elements.py"", line 293, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1103, in _execute_clauseelement      distilled_params,    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1288, in _execute_context      e, statement, parameters, cursor, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1482, in _handle_dbapi_exception      sqlalchemy_exception, with_traceback=exc_info[2], from_=e    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_      raise exception    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1228, in _execute_context      cursor, statement, parameters, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py"", line 857, in do_executemany      cursor.executemany(statement, parameters)  sqlalchemy.exc.DataError: (psycopg2.errors.NumericValueOutOfRange) integer out of range[SQL: INSERT INTO mgower_4.posix_datastore_records (dataset_id, path, formatter, storage_class, component, checksum, file_size) VALUES (%(dataset_id)s, %(path)s, %(formatter)s, %(storage_class)s, %(component)s, %(checksum)s, %(file_size)s)]  [parameters: ({'dataset_id': 605010, 'path': 'RC2/w_2020_22/DM-25176/remainder/objectTable_tract/9615/objectTable_tract_9615_hsc_rings_v1_RC2_w_2020_22_DM-25176_remainder.parq', 'formatter': 'lsst.daf.butler.formatters.parquetFormatter.ParquetFormatter', 'storage_class': 'DataFrame', 'component': '__NULL_STRING__', 'checksum': None, 'file_size': 2591970489}, {'dataset_id': 605011, 'path': 'RC2/w_2020_22/DM-25176/remainder/objectTable_tract/9813/objectTable_tract_9813_hsc_rings_v1_RC2_w_2020_22_DM-25176_remainder.parq', 'formatter': 'lsst.daf.butler.formatters.parquetFormatter.ParquetFormatter', 'storage_class': 'DataFrame', 'component': '__NULL_STRING__', 'checksum': None, 'file_size': 2911408427}, {'dataset_id': 605012, 'path': 'RC2/w_2020_22/DM-25176/remainder/objectTable_tract/9697/objectTable_tract_9697_hsc_rings_v1_RC2_w_2020_22_DM-25176_remainder.parq', 'formatter': 'lsst.daf.butler.formatters.parquetFormatter.ParquetFormatter', 'storage_class': 'DataFrame', 'component': '__NULL_STRING__', 'checksum': None, 'file_size': 2236160139})]  (Background on this error at: http://sqlalche.me/e/9h9h)  {noformat}",1
"DM-25352","06/10/2020 16:45:07","Update the M2 xml Based on the Latest LTS-162","Update the M2 xml based on the latest LTS-162. This will also add some interfaces needed to support the level-3 integration test (integration test in software/ simulator level to check the communication of components).",1
"DM-25353","06/10/2020 17:31:51","Update build scripts for the new RPM naming format","This task covers the work to update the build scripts after the SAL is updated with the new RPM naming format.         The Java/Maven name format was also updated.  The build scripts will also need to be updated.",2
"DM-25354","06/10/2020 17:55:35","Extend registry schema to support metadata/configuration","For schema stability we need a place where we can store identifying information for schema itself and various configuration-related info as well. That will server as a sort of metadata for the rest of the schema and data in registry. To be usable from many releases it itself needs to be super-stable but we also should be be able to store all sorts of metadata in it, exact structure of that is not possible predict today. I think the most generic solution for that is a simple key-value store with arbitrary keys and values encoded as strings. All other structures can be mapped to key/value using reasonable rules, though some care is certainly needed to avoid name collision.",2
"DM-25356","06/10/2020 19:28:11","Update ts_salobj unit tests to accept a private_identity field","ts_sal 4.2 will add a {{private_identity}} field to DDS topics, while maintaining and using the {{private_host}} and {{private_origin}} fields.    This breaks a few unit tests in salobj. Update the tests to optionally allow {{private_identity}}.",0
"DM-25357","06/10/2020 19:37:48","Create CentOS SD image with SAL for Raspberry Pi 4","Create CentOS SD image with SAL for Raspberry Pi 4 for future use.",1
"DM-25376","06/11/2020 19:09:36","Please retain the current behavior of private_origin and private_host in ts_sal 4.2","ts_sal 4.2 pre-release (develop commit 62503396) appears to change how private_origin is set. Please restore the existing behavior (in 4.1) until version 5.0, when we will switch to using private_identity for some of that.    Also for SAL 5.0 we will need some way of differentiating the same user running multiple processes (e.g. multiple Jupyter notebooks or logins) so command acknowledgements can be reliably disambiguated.    The question is whether the distinction between a user on one notebook or another is of interest to the authorization system. My guess is ""no"" -- all logins are treated the same. In that case the additional information belongs in a separate field -- and I suggest using private_origin the way it is used now, since we already have it. But if the answer is ""yes"" -- different logins are authorized independently -- then any additional identifying information clearly belongs in the {{private_identity}} field.",1
"DM-25377","06/11/2020 19:47:46","Update base.Packages serialization to support bytes","In DM-25325 support was added for YAML serialization but the serialization still requires a file name to be given.    It would be helpful for butler gen3 s3 datastore to support a serialization to from bytes by updating the PackagesFormatter (created in DM-25327).",1
"DM-25379","06/11/2020 20:52:53","psycopg2.OperationalError: SSL when running ci_hsc_gen3","While running ci_hsc_gen3 against the PostgreSQL server, I've been seeing the following error message (Usually > 100 times per ci_hsc_gen3 run when it happens):  {noformat}  sqlalchemy.pool.impl.QueuePool ERROR: Exception during reset or similar  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/pool/base.py"", line 693, in _finalize_fairy      fairy._reset(pool)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/pool/base.py"", line 880, in _reset      pool._dialect.do_rollback(self)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 540, in do_rollback      dbapi_connection.rollback()  psycopg2.OperationalError: SSL SYSCALL error: EOF detected  {noformat}  I've only seen the following once:  {noformat}  sqlalchemy.pool.impl.QueuePool ERROR: Exception during reset or similar  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/pool/base.py"", line 693, in _finalize_fairy      fairy._reset(pool)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/pool/base.py"", line 880, in _reset      pool._dialect.do_rollback(self)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 540, in do_rollback      dbapi_connection.rollback()  psycopg2.OperationalError: SSL error: decryption failed or bad record mac  {noformat}    This doesn't seem to actually affect the run.  This doesn't happen 100% of the runs.  It only seems to happen when ci_hsc_gen3 runs pipetask (i.e., never butler commands).  Here are counts from a few runs (from grep of stdout/stderr):  {noformat}  1591627439/cihsc_w_2020_23_1591627439_postgres.out:149  1591633412/cihsc_w_2020_23_1591633412_postgres.out:136  1591819226/cihsc_w_2020_23_1591819226_postgres.out:0  1591886082/cihsc_w_2020_23_1591886082_postgres.out:140  1591888106/cihsc_w_2020_23_1591888106_postgres.out:0  1591889656/cihsc_w_2020_23_1591889656_postgres.out:136  {noformat}    Google searches of the error messages makes it sound like a multiprocessing code issue (e.g., https://virtualandy.wordpress.com/2019/09/04/a-fix-for-operationalerror-psycopg2-operationalerror-ssl-error-decryption-failed-or-bad-record-mac/).   I had found result that talked about TCP keepalive so I tried  pool_pre_ping=True in the create_engine call, but that didn't seem to make any difference.",1
"DM-25391","06/12/2020 00:23:16","Refactor metric measurement code to reduce duplication","There is some duplication that I think we can reduce by factoring things out into base classes that can be inherited in multiple place.  I also thing we may find we can push things up into {{MetricTask}}.",2
"DM-25403","06/12/2020 16:40:17","Clean up naming of gen3 formatters","Currently obs_base formatters are all in the top level python/lsst/obs/base directory with long names. As we get more of them it makes sense to move them into a formatters directory as is done in daf_butler.  Similarly in daf_butler we use names like {{lsst.daf.butler.formatters.yamlFormatter.YamlFormatter}} and this has a lot of repetition. Following a discussion on Slack [~ktl] agreed that the formatter in the file name could be removed without resulting in confusion since we always use the full names in configs.    Therefore in this ticket I will:    # Move obs_base formatters to a formatters directory and use the new name scheme.  # Shorten the names of the formatter python files to drop Formatter in daf_butler.    Note this will break existing repositories but the fix is simple and other backwards compatibility breakage this week means that we can make this change without any additional impact.",0.5
"DM-25407","06/12/2020 19:31:48","ap_verify cannot handle curated crosstalk data in Gen 2","Following the merge of DM-23983, {{ap_verify}} crashes because it only ingests defects, and not other types of curated calibrations (in particular, crosstalk coefficients). Modify {{ap.verify.ingestion.DatasetIngestTask}} to process 0 or more curated calibrations in the format expected by {{IngestCuratedCalibsTask}}.    In addition to updating the (obs_lsst, obs_decam, obs_subaru) package override files for {{DatasetIngestConfig}}, double-check that this field is not overridden in the four existing datasets.    Since this will break all configs for {{DatasetIngestTask}} anyway, also rename {{textDefectPath}} and {{defectIngester}} to {{curatedCalibPaths}} and {{curatedCalibIngester}}, respectively.",1
"DM-25408","06/12/2020 19:58:10","A more efficient implementation of the empty chunks list generator","This development effort was triggered by https://jira.lsstcorp.org/browse/DM-24307. In the original version of the _empty chunk list_ generator a hard-wired fixed number {{1000000}} is used as an upper limit for chunk numbers to be evaluated by the algorithm. It turns out there is a better (and also - more efficient) way to limit a set of chunks to be evaluated. A complete list of such chunks could be obtained by calling this method:  {code:java}  namespace lsst {  namespace sphgeom {    class Chunker {  public:        /// `getAllChunks` returns the complete set of chunk IDs for the unit      /// sphere.      std::vector<int32_t> getAllChunks() const;    {code}  Also, the new implementation of the generator will reduce the total number of entries to be stored in either form of the list (a file or a database table).",1
"DM-25411","06/12/2020 21:38:46","Fix broken postgres test due to attribute manager","My DM-25354 merge have broken unit test for Postgres (and I think for Oracle too):  {noformat}          # cannot store empty key or value          with self.assertRaises(Exception):  >           attributes.set("""", ""value"")  E           AssertionError: Exception not raised  {noformat}    The issue is with the handling handling of the empty strings, our sqlite backend adds constraint to the schema which disables empty strings, that was done to work around Oracle issue that treats empty strings as NULLS. as a result the code now raises integrity exception when saving empty string to database in sqlite case but there is no similar check for oracle/sqlite so that works OK.    Simplest fix for my case would be either to suppress this check in the test completely or to add a new check in attribute manager that raises exception independently of a backend.",1
"DM-25416","06/13/2020 17:10:23","Fix sphinx build for daf_butler","In DM-25327 I forgot to check the sphinx build in daf_butler before merging. This broke the weekly.",0.5
"DM-25419","06/15/2020 16:13:35","Adopt DCS and mock controller to ts_xml changes","Due to work on MTDomeTrajectory the command, telemetry and event XML files for the Dome have been updated. Adopt the DCS and mock controller to these changes.",2
"DM-25420","06/15/2020 16:41:49","Update XML for MTDomeTrajectory","Update the XML for MTDomeTrajectory in ts_xml.    This was implemented, reviewed and merged as part of DM-24058.  However, I wanted a separate ticket for CAP-563 that could be closed before ts_MTDomeTrajectory was merged.",0
"DM-25429","06/15/2020 21:37:04","Implement an example measurement","Implement a simple example visit level measurement.",1
"DM-25431","06/15/2020 22:23:32","Add conda env to base.Packages","Examining the package versions we store in butler repository when running pipelines, it is clear that we are not including the details of our conda environment.  We need to add code that will read the conda env and include all the versions (possibly overwriting version information determined from the current technique). This will have the advantage of giving us details of {{boost}} and related packages that are not visible to python.    One wrinkle is that I do not yet see an obvious way to get this information without having {{base}} run {{conda list -e}}.",2
"DM-25437","06/16/2020 21:11:20","Convert metrics pipeline repository to use eups","We currently set up a few environment variables up by hand when running the pipelines.  This is somewhat error prone, so I'd like to convert to using eups to handle that more automatically.  This could also include adding proper Python modules.",2
"DM-25456","06/18/2020 01:31:12","Add authList support to Script","The Script component needs authorization list support: the generic setAuthList command and authList event.    I will implement this as part of DM-25126 but merge it sooner.",0
"DM-25465","06/18/2020 21:14:56","Thoroughly inspect ESS code and form API proposal","In order to be able to use the ESS CSC as a generic CSC for all kinds of sensors, the existing ESS code needs to be inspected so an API can be extracted. This issue is for doing the inspection and for forming an API proposal.",1
"DM-25468","06/18/2020 21:58:22","Add protection for repeated schema initialization","In our code we support repeated schema initialization (in PPDatabase.declareStaticTables}} method) but it is not very safe and can potentially cause loss of data if it happens on a populated database. We want a reasonable protection to avoid disaster, schema should only be initialized once and all later attempts should raise an exception.  ",2
"DM-25471","06/18/2020 22:56:47","Please move salgenerator and other crucial bin scripts to the bin/ directory","If you are willing, please consider moving {{salgenerator}} and any other important executable scripts from {{lsstsal/scripts}} to {{bin/}}.    Also please make sure {{lsstsal/bin}} is *not* added to $PATH. I find it on my $PATH when I run [~tribeiro]'s dev Docker image and there is no such directory.    I think this would be an improvement because:  * ""bin/"" is where all other T&S and DM packages put executable scripts.  * {{lsstsal/scripts}} has 170 or so scripts, most of which are executable. It seems unfortunate to have all of those on the $PATH unless we really need them to be there.    Note that the current eups table file adds {{bin/}} to the PATH but not {{lsstsal/scripts}}. If you prefer to leave {{lsstsal/scripts}} on the PATH then I would like to change that so both are added to PATH.",1
"DM-25473","06/18/2020 23:08:32","Please echo private_identity from commands as identity in ackcmd messages.","I am implementing the authList support in salobj and realized that ackcmd replies from SALPY_Test do not have the {{identity}} field correctly set to the {{private_identity}} field from the command being acknowledged.    If practical it would be a help to have this fixed in SAL 4.2, because it will allow ts_salobj 6 to fully handle authorization and operate with SAL 4.2 (as well as SAL 5). It is likely to also benefit running a combination of SAL 5 and SAL 4.2 (which I strongly suspect will work), though without authorization in the SAL 4.2 CSCs.    All that said, a simple workaround will allow ts_salobj 6 to work without this, so it's not critical. DM-25474.",1
"DM-25476","06/19/2020 00:17:48","Remove misleading dax_imgserv ups directory","I believe the {{ups}} directory in {{dax_imgserv}} is actually unused, as the package is installed using {{pip}} and {{setup.py}} rather than {{eups}}.  The real dependencies appear to be in {{requirements.txt}} and the {{Dockerfile}}.    The presence of this misleading information is confusing automatic dependency tooling for the product tree.  Accordingly, it would help if it were removed.  Other unused files (like {{SConstruct}}?) could be removed at the same time.",2
"DM-25480","06/19/2020 02:36:09","Remove SEQNAME from PRIMARY common template for ComCam","The {{SEQNAME}} is defined in both the:  * primary_hdu.header  * primary_sensor_hdu.header    However it should only be present on the {{primary_sensor_hdu.header}}    Also add FILTER=r to the ComCam template.",1
"DM-25481","06/19/2020 07:13:00","Repair busted Qserv container builds","Use of Qserv dependency tarballs in Qserv container builds results in broken paths being baked into the containers (we end up with tarball build-time paths instead of Qserv container build-time paths for components such as mariadb).    Short-term fix is to disable use of tarballs in the Qserv container builds (these builds are already scheduled for demolition, and finding and implementing all the necessary path fixups is deemed not worth the effort at this time.)",1
"DM-25482","06/19/2020 09:08:45","Investigate and fix a possible bug in the chunk allocator","The current implementation of the Ingest system may have a bug in the single chunk allocator. The problem was reported at:  https://jira.lsstcorp.org/browse/DM-24587?focusedCommentId=248700&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-248700    Extend the error reporting of the allocator to return a collection of replicas which may be causing this problem. Fix a bug if any exists.",3
"DM-25491","06/19/2020 18:25:51","Command all sky camera using GPhoto","Use the Python bindings for libgphoto2 and the GenericCamera CSC to command the all sky camera.",2
"DM-25492","06/19/2020 18:31:24","Integrate ESS code in ESS CSC","Take the ESS code that reads the sensors and integrate it into the ESS CSC code.",2
"DM-25503","06/19/2020 23:17:51","Port metric AD2 to metric task framework","Metric AD1 implemented in the validate_drp framework should be ported to the new framework.    ",1
"DM-25514","06/20/2020 06:48:10","REST API for building the secondary index in Qserv","In the current implementation of the new Ingest system the _secondary index_ of a database is normally populated in the end of the _super-transactions_. The algorithm harvests contributions from the corresponding MySQL _partitions_ of the director table's chunks and loads them into the index table at Qserv ""czar""'s database . Depending on the amount of data ingested into that table during a transaction the index building operation may noticeably slow down the commit phase of the transaction. After gaining the first experience with using the new Ingest system it has become apparent that some workflows may benefit from having the _secondary index_ construction as an explicit stage deferred till a moment when all director table's contributions were loaded into Qserv.    Hence, a goal of the current development is to refine and extend the REST API of the Master Replication Controller to allow more options in building the index. Specifically, the following changes are proposed:  * add an optional parameter *build_secondary_index=<val>* to existing database creation service to indicate an intent to defer the index creation till the very last stage. The default value of the parameter will be 0 which would preserve the current behavior.  * extend the database information structure DatabaseInfo with the corresponding flag.  * modify the transaction commit service not to make contributions into the index if the deferred index construction for the database was requested.  * add a new POST service for constructing indexes as a separate operation to be performed before (or with extra controls) after publishing a database. The service should also allow an optional parameter to allow rebuilding the index from scratch.  * update the documentation on the Ingest System accordingly.  * evaluate if the version of the Ingest API needs to be incremented as well.",8
"DM-25515","06/20/2020 14:08:24","Fix references to https://lsst-web.ncsa.illinois.edu/~buildbot/doxygen","The doxygen output has not lived at this location for a long time.  The proper URL should be {{https://doxygen.lsst.codes/doxygen/}}",1
"DM-25516","06/21/2020 03:35:04","numpydoc warnings in obs_base test utilities","The following problems are causing warnings in doc builds.  (There are more in the geom and afw test utilities, but those appear to still be in doxygen form.)   * [https://github.com/lsst/obs_base/blob/master/python/lsst/obs/base/butler_tests.py#L82] needs quoting around the code (or {{**}} is misinterpreted)   * [https://github.com/lsst/obs_base/blob/master/python/lsst/obs/base/camera_tests.py#L52-L57] indented an extra space   * [https://github.com/lsst/obs_base/blob/master/python/lsst/obs/base/tests.py#L80] indented an extra space",1
"DM-25518","06/22/2020 04:49:55","Update my packages for ts_xml 6, SAL 4.2, and salobj 6","Update my packages for SAL 4.2 (the new private_identity field), ts_xml 6 (the new setAuthList command) and salobj 6 (stop using deprecated code -- primarily {{BaseCsc.main}}).    Packages that need work include:  * ts_ATMCSSimulator: bin script calls BaseCsc.main  * ts_hexrotcomm: setAuthList needs to be added to good commands for checking state transitions  * ts_salfafka: schema tests do not expect private_identity and should be modified to allow private_host to be missing  * ts_watcher: bin script calls BaseCsc.main    Also investigate warnings in ts_scriptqueue unit tests.  ",1
"DM-25519","06/22/2020 14:30:53","clang-format M1M3 support code","M1M3 C++ code is written in Eclipse, using Eclipse tab-based formatting. That contradicts to DM recommended clang-format. I am reformatting single files if I do significant changes, but time has come to reformat the whole.    Lets couple that with Github action to check for clang-format.",1
"DM-25526","06/22/2020 18:30:24","Fix array handling in ADLink's community edition of OpenSplice","Examine the differences between our OpenSplice code and ADLink's community edition and try to determine what fixes the array handling. Then submit a pull request.",1
"DM-25621","06/22/2020 19:11:46","Jenkins maintenance - install plugins, apply updates and bug fixes","Please, add {{docker-compose}} to the Jenkins nodes so we can experiment using it to manage the build of deployment containers. ",2
"DM-25627","06/22/2020 20:45:15","make qgraph and run subcommands for pipetask","Implement {{qgraph}} and {{run}} using click (in the {{pipetask2}} command, which will eventually replace {{pipetask)}}    Add the ""chained"" subcommands feature, write a Community post about it describing the differences from the existing {{pipetask}} command, request feedback.",8
"DM-25633","06/22/2020 22:23:03","Fix warnings in test_queue_model","Two tests in test_queue_model are giving warnings because they don't wait until some scripts have finished.",0
"DM-25634","06/22/2020 23:31:16","Dome software meetings","Prepare and coordinate weekly meetings on dome control software with EIE contractor.",5
"DM-25635","06/22/2020 23:34:35","Generate document on dome coditioning contract","As a component of the Environment Awareness System EAS ([https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873),|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=73574873)] it's necessary to develop a dome environment conditioning system, to control the temperature during daytime.",3
"DM-25636","06/22/2020 23:59:29","Write a simple authentication controller","Write a simple authentication controller that accepts and processes requests for authentication.    This will be the foundation on which the LOVE interface for authentication will be built, so it will use the LOVE component name. The long-term plan is for LOVE to show each such request to the operators, who will accept or reject the request. In the meantime (having no telescope operators and no GUI for authentication) the simple authentication controller will simply accept each request.    For now I will put the code into a new package ts_authenticate. We may well move it when INRIA starts implementing the operator interface for approving authentication requests.",2
"DM-25638","06/23/2020 00:55:33","Update MTMount simulator to use motion parameters from specification","Michael just noticed that the MTMount simulator is using values for Elevation and Azimuth motion that are not the same as the specification. We agreed that the best would be to use the same parameters we use for survey simulations, which are actually based on the minimum requirements.       {noformat}altitude_max = 86.5 deg  altitude_min = 20 deg  altitude_maxspeed = 3.5 deg/sec  altitude_accel    = 3.5 deg/sec**2    azimith_max = 270 deg  azimuth_min = -270 deg  azimuth_maxspeed  = 7.0 deg/sec  azimuth_accel     = 7.0 deg/sec**2{noformat}  For the camera cable wrap the (minimum) requirements are:       {noformat}position_limits = +/- 90 degrees (soft limit)  velocity = 3.5 deg/sec  acceleration = 1 deg/sec**2{noformat}  Note that there's a +/- 4 degrees of hard-limit on the CCW, probably not on the simulator though.     ",0
"DM-25640","06/23/2020 01:19:58","Add documentation for ctrl_mpexec to pipelines.lsst.io","ctrl_mpexec has a perfectly serviceable {{doc/}} directory, which builds without errors or warnings. It doesn't provide much useful information beyond automatically generated API docs yet, but it at least provides something convenient to link to. We should include it on pipelines.lsst.io.",1
"DM-25654","06/24/2020 00:52:26","Audit notebook aspect requirements","Have a look through the requirements on the notebook aspect of the science platform presented in the [DMSR|https://docushare.lsst.org/docushare/dsweb/Get/LSE-61] and the [LSP requirements|https://docushare.lsst.org/docushare/dsweb/Get/LDM-554].  The intent is to perform an audit to check for self consistence and completeness.    A special effort should be made to identify missing or incomplete requirements in order to improve our ability to verify the system.",5
"DM-25658","06/24/2020 19:25:21","Fix the Following Error in the Rotator Simulink Model","Fix the following error in the rotator Simulink model. There is the problem of physical model in plant that the rotator's position will differ from the related position command gradually until the trigger of *followingError* (the difference >= threshold) and this stops the motion of rotator. According to the observation, this happens if the rotator moves in the negative direction.    In the following figure, there are two targets: (1) position=1.2 deg and velocity=0.01 deg/sec and (2) position=1.5 deg and velocity=-0.01 deg/sec. The total simulation time is 20.1 sec. There is no big difference between the position command and position. The error of *noNewTrackCmdError* after the slew of the second target is triggered as expected.   !trackAdapNoNewTrackCmd.png|thumbnail!    However, if the second target has position=-1.5 deg and velocity=-0.01 deg/sec, the following error is triggered while the rotator turns the direction to slew to the second target:   !trackSolverAdap.png|thumbnail!",3
"DM-25675","06/25/2020 16:09:20","Implement OBSANNOT keyword in HeaderService","Add OBSANNOT keyword needs to be implemented in the headers.    {{CCCamera_logevent_imageReadoutParameters.daqAnnotation}}    {{ATCamera_logevent_imageReadoutParameters.daqAnnotation}}      This applies to all camera flavors.",1
"DM-25685","06/25/2020 23:14:41","Investigate the memory leak in lsstts/deploy-env:salobj_5.15.0","[~mareuter] reports that the SAL/kafka producers are leaking memory. Investigate the problem. Determine if it's DM-23255, which we have a fix for, or something else. See that ticket for simple dds scripts to measure memory leakage. For this ticket I will use https://jira.lsstcorp.org/secure/attachment/42424/memory_test_asyncio.zip",1
"DM-25689","06/26/2020 09:10:37","Problems converting RC2 with w_2020_25","{noformat}     19.0.0-109-g029026b7  sims_w_2020_25 current w_2020_25 setup  INFO  2020-06-25T19:16:02.828-0500 convertRepo - Finding raws in root /datasets/hsc/repo.  INFO  2020-06-25T19:17:23.807-0500 convertRepo - Ingesting raws from root /datasets/hsc/repo into run HSC/raw/all.  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1228, in _execute_context      cursor, statement, parameters, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py"", line 857, in do_executemany      cursor.executemany(statement, parameters)  psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""dataset_collection_08cc_unq_dataset_type_id_collection_b1d439b9""  DETAIL:  Key (dataset_type_id, collection_id, instrument, detector, exposure)=(1, 2, HSC, 38, 34674) already exists.  The above exception was the direct cause of the following exception:  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/registry/_registry.py"", line 587, in insertDatasets      refs = list(storage.insert(runRecord, expandedDataIds))    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/registry/datasets/byDimensions/_storage.py"", line 78, in insert      self._db.insert(self._dynamic, *dynamicRows)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/registry/interfaces/_database.py"", line 955, in insert      self._connection.execute(table.insert(), *rows)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 984, in execute      return meth(self, multiparams, params)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/sql/elements.py"", line 293, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1103, in _execute_clauseelement      distilled_params,    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1288, in _execute_context      e, statement, parameters, cursor, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1482, in _handle_dbapi_exception      sqlalchemy_exception, with_traceback=exc_info[2], from_=e    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_      raise exception    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1228, in _execute_context      cursor, statement, parameters, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py"", line 857, in do_executemany      cursor.executemany(statement, parameters)  sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""dataset_collection_08cc_unq_dataset_type_id_collection_b1d439b9""  DETAIL:  Key (dataset_type_id, collection_id, instrument, detector, exposure)=(1, 2, HSC, 38, 34674) already exists.  [SQL: INSERT INTO rc2w22_ssw25.dataset_collection_08cc (dataset_type_id, dataset_id, collection_id, instrument, detector, exposure) VALUES (%(dataset_type_id)s, %(dataset_id)s, %(collection_id)s, %(instrument)s, %(detector)s, %(exposure)s)]  [parameters: ({'dataset_type_id': 1, 'dataset_id': 46225, 'collection_id': 2, 'instrument': 'HSC', 'detector': 38, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46226, 'collection_id': 2, 'instrument': 'HSC', 'detector': 42, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46227, 'collection_id': 2, 'instrument': 'HSC', 'detector': 18, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46228, 'collection_id': 2, 'instrument': 'HSC', 'detector': 20, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46229, 'collection_id': 2, 'instrument': 'HSC', 'detector': 31, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46230, 'collection_id': 2, 'instrument': 'HSC', 'detector': 89, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46231, 'collection_id': 2, 'instrument': 'HSC', 'detector': 22, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46232, 'collection_id': 2, 'instrument': 'HSC', 'detector': 25, 'exposure': 34674}  ... displaying 10 of 112 total bound parameter sets ...  {'dataset_type_id': 1, 'dataset_id': 46335, 'collection_id': 2, 'instrument': 'HSC', 'detector': 107, 'exposure': 34674}, {'dataset_type_id': 1, 'dataset_id': 46336, 'collection_id': 2, 'instrument': 'HSC', 'detector': 106, 'exposure': 34674})]  (Background on this error at: http://sqlalche.me/e/gkpj)  The above exception was the direct cause of the following exception:  Traceback (most recent call last):    File ""./gen3-hsc-rc2/bootstrap.py"", line 274, in <module>      main()    File ""./gen3-hsc-rc2/bootstrap.py"", line 270, in main      continue_=options.continue_, reruns=reruns)    File ""./gen3-hsc-rc2/bootstrap.py"", line 220, in run      visits=makeVisitList(tracts, filters)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-80-g3c4dce0/python/lsst/obs/base/gen2to3/convertRepo.py"", line 520, in run      rootConverter.runRawIngest()    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-80-g3c4dce0/python/lsst/obs/base/gen2to3/rootRepoConverter.py"", line 123, in runRawIngest      self._rawRefs.extend(self.task.raws.run(dataPaths))    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-80-g3c4dce0/python/lsst/obs/base/ingest.py"", line 472, in run      refs.extend(self.ingestExposureDatasets(exposure, run=this_run))    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/19.0.0-80-g3c4dce0/python/lsst/obs/base/ingest.py"", line 408, in ingestExposureDatasets      self.butler.ingest(*datasets, transfer=self.config.transfer, run=run)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/core/utils.py"", line 249, in inner      return func(self, *args, **kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/_butler.py"", line 1195, in ingest      run=run)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/core/utils.py"", line 249, in inner      return func(self, *args, **kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-109-g029026b7/python/lsst/daf/butler/registry/_registry.py"", line 594, in insertDatasets      f""dimension row is missing."") from err  lsst.daf.butler.registry._exceptions.ConflictingDefinitionError: A database constraint failure was triggered by inserting one or more datasets of type DatasetType(raw, {abstract_filter, instrument, detector, physical_filter, exposure}, Exposure) into collection 'HSC/raw/all'. This probably means a dataset with the same data ID and dataset type already exists, but it may also mean a dimension row is missing.  {noformat}",1
"DM-25692","06/26/2020 10:08:40","Write gen3 formatter for Exposure Filter","In DM-25447 I discovered that a read-only Filter component for Exposure can't work because the information is derived from metadata that is stripped and a Filter can not be persisted since Filter.writeFits is broken.    I propose that we write a very simple formatter that bypasses the FITS persistence system and simply writes a yaml file that includes the filter information. On read it will translate the filter name to a Filter object by looking it up in the filters singleton. This can then be set in the exposure using {{setFilter}}.    This will turn filter into a normal component and should be quick to implement.",1
"DM-25698","06/26/2020 11:58:58","Fix missing obs_decam dependencies","The obs_decam tests do not depend on the decam/crosstalk and decam/defects being built, and almost certainly should",1
"DM-25705","06/26/2020 15:49:57","Add tests for stray light in gen3","In DM-25689 we found some problems with stray light files.  We need to add a test to obs_subaru to ingest an example file and then be able to retrieve it.",1
"DM-25707","06/26/2020 16:13:24","ts_salkafka seems to be ignoring ackcmd topic messages","The salkafka producers seem to not be sending out ackcmd Kafka messages when the SAL topic messages are received.",0
"DM-25710","06/29/2020 06:59:20","Update dev guide to clarify rule on .py extensions for scripts","See RFC-698.",1
"DM-25712","06/29/2020 09:41:27","Write stop scripts for the main and aux telescope","Write simple scripts to stop the aux tel and main telescope. These will call the stop_all functions in ts_observatory_control.",1
"DM-25713","06/29/2020 09:44:19","Update docker images and  conda for Headerservice 2.3.1","HS need to be deployed at the base using version 2.3.0  ",2
"DM-25719","06/29/2020 11:29:50","Cherry-pick the change to salobj 5 needed by salkakfa for DM-25707","Cherry-pick the salobj 6 change that allows disabling filtering when reading ackcmd topics to salobj 5, so we can release a new ts_salkafka DM-25707.    Also update the requirements in ts_salkafka accordingly.",0
"DM-25738","06/30/2020 12:03:20","Update MTAOS to Adapt the Latest xml Version","Update *MTAOS* to adapt the latest xml version, especially for the *M2* xml update (DM-25475) and *Hexapod* xml update (DM-25856). Maybe need to check there is the update of *M1M3* or not. This task will update the CSC test as well for the [timeout issue|https://github.com/lsst-ts/ts_MTAOS/issues/27] and use the *Controller* by *ts_salobj* to help for the test (receive the subsystem correction). For the use of *Controller*, the CSC test in *ts_m2* is a good example. After this task is done, need to prepare a new docker image of *MTAOS* simulator to deploy in NCSA for the integration test.",1
"DM-25739","06/30/2020 12:08:34","Update SALSubsystems for SAL components rowen has some responsibility for","Update SALSubsystems for the following SAL components:  * ATDome  * ATDomeTrajectory  * ATMCS  * ATPneumatics  * FiberSpectrograph  * Hexapod  * LOVE  * MTDomeTrajectory  * MTMount  * NewMTMount  * Rotator  * Script  * ScriptQueue  * Test  * Watcher",0
"DM-25742","06/30/2020 14:30:03","Add a REST service to report chunk-to-worker allocation maps during catalog ingest","Some large scale catalog ingest workflows may benefit from seeing  a status of the {{chunk->worker}} allocation map. Specifically, this could be used for the monitoring purposes. Hence, a goal of the proposed development is to add the following service to the REST service of the *Master Replication Controller*:  || method || service || request query || result ||  | GET | /ingest/chunks |?database=<name> | JSON |    Where:  * the name of a database is passed in the query string of a request  * the service returns a JSON object (see details in the documentation linked below)    Notes:  * the service should *not* be used as a reliable source of information to decide on the chunk allocation/placements during catalog ingest. Use the corresponding *POST* method instead.  * the service can be used for databases in any state (_published_ or not).    Other action items:  * update the documentation on the *Ingest System* at: https://confluence.lsstcorp.org/pages/viewpage.action?pageId=133333850",2
"DM-25743","06/30/2020 14:56:07","Document the Algorithm of Interpolation for Rotator","Document the algorithm of interpolation for rotator. At this moment, the rotator Simulink model uses the extrapolation for *track* command. This task tries to define the question clearly and provides the analytical solution of interpolation. The update of Simulink model will be a separate ticket.",2
"DM-25744","06/30/2020 14:59:56","Test scarlet on a single full patch of HSC data","Previous versions of scarlet have suffered from memory issues that prevented them from running on a full HSC patch, causing us to create small 1k x 1k patches to test the stack implementation of scarlet. This ticket is just to run scarlet on a full HSC patch to verify that all of those issues have been taken care of or generate new tickets to fix any issues that come up.",3
"DM-25745","06/30/2020 15:12:42","Update the Simulator entry for MTMount in SALSubsystems","Update the Simulator entry in SALSubsystems.xml for MTMount with one or more links to the vendor's simulator code.",0
"DM-25746","06/30/2020 15:18:59","filterName in APDB needs to be one of g, r, i, z, or y","Before DM-21333 (filter overhaul) happens, we need an interim quick fix for making it possible to run ap_pipe on HSC data in the R2 and I2 filters. Presently, `getCanonicalFilter` retrieves `afw_name` if available and `abstract_filter` otherwise (e.g., https://github.com/lsst/obs_subaru/blob/master/python/lsst/obs/hsc/hscFilters.py#L81). The problem is that `afw_name` only exists for R2 and I2 (as `r2` and `i2`), but the APDB schema uses the filter name to construct a myriad of columns that assume all filters are correctly represented as one of g, r, i, z, or y. Presumably `getCanonicalFilter` is used in other places where `afw_name` is the desired thing to return, and I shouldn't mess with it at this time.",1
"DM-25750","06/30/2020 17:44:48","gen3 queryCollections does not work with regexes","[~krzys] noted that {{registry.queryCollections}} does not work with regexes despite being documented to do so.    This patch helps a little:  {code:diff}  diff --git a/python/lsst/daf/butler/registry/wildcards.py b/python/lsst/daf/butler/registry/wildcards.py  index c416105f..52efe8f2 100644  --- a/python/lsst/daf/butler/registry/wildcards.py  +++ b/python/lsst/daf/butler/registry/wildcards.py  @@ -739,7 +739,7 @@ class CollectionQuery:           if wildcard is Ellipsis:               return cls.any           assert not wildcard.strings  -        return cls(search=CollectionSearch.fromExpression(wildcard),  +        return cls(search=CollectionSearch.fromExpression(wildcard.items),                      patterns=tuple(wildcard.patterns))          def iterPairs(  {code}    We did not notice this previously because there are no tests using regexes.",0.5
"DM-25754","06/30/2020 19:48:41","UnboundLocalError: local variable 'n' referenced before assignment","Came across this with some pipetask run commands, and it looks like a bug at https://github.com/lsst/pipe_base/blob/master/python/lsst/pipe/base/graphBuilder.py#L556    {code:java}  Failed to build graph: local variable 'n' referenced before assignment  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-3-g6d6cb62+2/bin/pipetask"", line 26, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-3-g6d6cb62+2/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 474, in parseAndRun      qgraph = self.makeGraph(pipeline, args)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-3-g6d6cb62+2/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 620, in makeGraph      qgraph = graphBuilder.makeGraph(pipeline, collections, run, args.data_query)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/pipe_base/20.0.0+3/python/lsst/pipe/base/graphBuilder.py"", line 788, in makeGraph      scaffolding.connectDataIds(self.registry, collections, userQuery)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/pipe_base/20.0.0+3/python/lsst/pipe/base/graphBuilder.py"", line 556, in connectDataIds      _LOG.debug(""Finished processing %d rows from data ID query."", n)  UnboundLocalError: local variable 'n' referenced before assignment{code}",0.5
"DM-25755","06/30/2020 20:01:15","Define minimal set of image-query features to replace legacy PDAC Portal","Document in one place the specific minimal set of work that would be required to fully replace the PDAC Portal's image-search features with an IVOA-services-oriented equivalent.  This was in-progress work at the time of the DM-10 freeze, and is the minimal piece of new development that would facilitate the use of the Portal during DP0 and commissioning activities.    This amounts to writing a specification for DM-18692, and involves collecting various sketches together for concreteness.",2
"DM-25756","06/30/2020 20:48:58","Refresh lsst-demo certificate - June 2020","Refresh the certificate for the HTTPS service on the {{lsst-demo.ncsa.illinois.edu/firefly}} server.",1
"DM-25758","07/01/2020 04:20:10","Adjust DCS and the mock controller to a change in the configuration protocol","A small change has been introduced in the configuration protocol: all values need to be arrays.",1
"DM-25761","07/01/2020 09:20:43","Compare performance of ts_opensplice with the community edition","Compare Python performance of ADLink's official community edition of OpenSplice (with my array fix and segfault fix) against what we are using. If the performance is similar and all tests pass, I hope we will consider using it -- though it still has to be checked with the other languages.",1
"DM-25762","07/01/2020 09:52:21","Move Dome CSC documentation on Confluence to .rst files","There is a lot of documentation related to the Dome CSC and the communication with the Lower Level Components on Confluence. This ticket is to migrate the relevant documents to .rst files.",2
"DM-25764","07/01/2020 10:25:40","Review ESS XML and add timestamps if necessary","Review the ESS telemetry topics and add a timestamp if there isn't already one.",1
"DM-25767","07/01/2020 12:44:13","Add checking for duplicate keys in pipeline definitions","If a pipeline definition document has the same label for two or more tasks, the last one will clobber the rest silently.  Tasks should not have the same label, but if they mistakenly do, it can lead to confusing behavior.  It would be nice if the quantum graph builder would throw an exception in the case of duplicate labels in the same pipeline definition document.    I believe it already does throw an exception for duplicate keys in separate pipeline definition documents inherited in a single one.",1
"DM-25774","07/02/2020 04:41:58","M1M3SSPublisher as singleton","Instead of passing M1M3SSPublisher (OpenSplice class, created once) pointer to zillion of classes, it will be much easier to have M1M3SSPublisher as singleton _(see similar refactoring we finished for FPGA)_.         Various other classes might be singletonized as well, with SafetyController one of the prominent example.",3
"DM-25777","07/02/2020 10:11:51","Add ts_simactuators to ts_ATDomeTrajectory dependencies","List ts_simactuators as a dependency of ts_ATDomeTrajectory",0
"DM-25779","07/02/2020 12:16:27","Investigate jointcal chi2 changes in a large HSC run","Before trying to implement DM-25159, I need to check that jointcal does not have cases where the chi2 gets dramatically worse in one step but then recovers. It should be relatively simple to write a script that scans a log and generates a plot of the chi2 for that jointcal run.    I'll need access to the logs from jointcal runs on a big HSC run (RC2? Whatever the biggest one is).",2
"DM-25784","07/02/2020 14:28:46","Update documentation for ts_ATDome to match our new template","Try out the new documentation format on a small package: ts_ATDome.",1
"DM-25786","07/02/2020 15:16:22","Cannot import sphgeom objects in Gen 3","I've run into an issue when trying to import the Gen 3 portions of {{ap_verify_ci_hits2015}} and {{ap_verify_ci_cosmos_pdr2}}. Both datasets' export files were created using the call  {noformat}  with butler.export(directory=<dir>, format=""yaml"") as contents:      contents.saveDatasets(butler.registry.queryDatasets(datasetType=..., collections=..., expand=True))  {noformat}    This creates patch/tract nodes in the {{export.yaml}} that look like:  {noformat}  - type: dimension    element: tract    records:    - skymap: deepCoadd_skyMap      id: 0      region: !!python/object/new:lsst.sphgeom.convexPolygon.ConvexPolygon        state: !!binary |          cFXvAKxKVuq/RvRf2oeC4T8FPPNfpn/DvyDNA7Oynu2/OLzXNgIs1j9xm1N9pn/Dv8YNITrW4O2/          YJqZWJtv1j8ULM5xUauyPz0LuTZumOq/F3pwY1Sk4T/+N9NnUauyPw==  {noformat}    When I try to import the {{export.yaml}}, I get the following stack trace:  {noformat}  File ""/scratch/krzys001/daf_butler/python/lsst/daf/butler/_butler.py"", line 1339, in import_      doImport(stream)    File ""/scratch/krzys001/daf_butler/python/lsst/daf/butler/_butler.py"", line 1332, in doImport      backend = BackendClass(importStream, self.registry)    File ""/scratch/krzys001/daf_butler/python/lsst/daf/butler/core/repoTransfers.py"", line 396, in __init__      wrapper = yaml.safe_load(stream)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/__init__.py"", line 162, in safe_load      return load(stream, SafeLoader)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/__init__.py"", line 114, in load      return loader.get_single_data()    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/constructor.py"", line 51, in get_single_data      return self.construct_document(node)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/constructor.py"", line 60, in construct_document      for dummy in generator:    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/constructor.py"", line 413, in construct_yaml_map      value = self.construct_mapping(node)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/constructor.py"", line 218, in construct_mapping      return super().construct_mapping(node, deep=deep)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/constructor.py"", line 143, in construct_mapping      value = self.construct_object(value_node, deep=deep)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/constructor.py"", line 100, in construct_object      data = constructor(self, node)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/yaml/constructor.py"", line 429, in construct_undefined      node.start_mark)  yaml.constructor.ConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/new:lsst.sphgeom.convexPolygon.ConvexPolygon'    in ""/project/krzys001/ap_verify_ci_cosmos_pdr2/config/export.yaml"", line 4163, column 13  {noformat}    [~tjenness] recommends adding serialization support to {{sphgeom}} that makes these types compatible with {{yaml.safe_load}}. This is related to DM-25448, but distinct in that there's no {{LookupError}} with the registry.",2
"DM-25788","07/02/2020 17:51:18","BaseCscTestCase.check_bin_script does not set LSST_DDS_DOMAIN","BaseCscTestCase.check_bin_script should set the LSST_DDS_DOMAIN environment variable to a random value, just like make_csc does.    Thanks to [~tribeiro] for reporting this.",0
"DM-25790","07/02/2020 18:28:12","Support the Configuration File in Rotator Low-Level Controller Code","Support the use of configuration file in rotator low-level controller. MOOG provided the middleware wrapper orginally that allows the user to update the configuration file by SAL. T&S team replaced it and used the Python CSC based on *ts_salobj* instead. This new CSC does not allow to do so. Therefore, this task will update the low-level controller code to let the user can configure the rotator by the configuration file.",3
"DM-25792","07/03/2020 12:29:47","Forward units on to JSON serialization of verify.Job even when NaN","For some reason, when a {{Measurement}} is NaN (specifically {{numpy.nan}} the units are set to the empty string in the serialized JSON document of the {{Job}} to which the {{Measurement}} belongs.  {{astropy.Quantity}} allows a NaN to have a unit, so it does not appear to be dropped at that stage.    This is to figure out which phase is dropping the unit: adding the measurement to the {{Job}} or serialization to JSON.  Then find a way to make sure the unit is forwarded on.  This may require a special JSON serializer for {{Measurement}} objects or {{astropy.Quantity}} objects.",1
"DM-25795","07/04/2020 19:23:28","Fix the Bugs of Transition of Tracking","Fix the transition of tracking in deciding the tracking trajectory. In the previous update (DM-25243), I made two mistakes. One is that I do not generate a smooth velocity command in the tracking (I forgot the 4000 Hz internal calculation). The other one is that I do not stop the rotator's velocity smoothly if there is no new tracking command for some time. This ticket is to fix these two bugs.",3
"DM-25806","07/06/2020 14:54:42","Support parallel ap_verify ingestion in Gen 3","The Gen 3 {{lsst.obs.base.RawIngestTask}} can be passed the number of processes to use for ingesting in parallel. This could potentially greatly speed up the processing of large datasets like HiTS2015. Propagate the existing {{ap_verify.py -j}} flag to Gen 3 ingestion.",2
"DM-25812","07/06/2020 16:15:06","Enable camera cable wrap tracking rotator by default","The MTMount CSC should start up with the camera cable wrap tracking the camera rotator.",1
"DM-25813","07/06/2020 16:31:31","Dome software meetings","Prepare and coordinate weekly meetings on dome control software with EIE contractor.",5
"DM-25825","07/07/2020 19:00:08","Add a function to ts_salobj that returns the OpenSplice version","Add a function to ts_salobj that returns the OpenSplice version",0
"DM-25855","07/08/2020 17:35:21","Fix missing obs_decam dependencies, pt 2","""Looks like https://github.com/lsst/obs_decam/blob/master/decam/SConscript#L71 needs to also depend on makeCrosstalk or decam/crosstalk""",1
"DM-25856","07/08/2020 17:54:01","Add compensation support to the MT Hexapod CSC","Add LUT correction to the Hexapod CSC.",3
"DM-25858","07/08/2020 18:13:37","Update the file SALSubsystems for HeaderService components","Update the file SALSubsystems.xml for HeaderService components for AT/CC and MT following the definitions from:  [https://ts-xml.lsst.io/salsubsystems_attributes.html]",1
"DM-25863","07/08/2020 19:24:42","Fix and update metric generation in colorAnalysis.py","As of DM-22255, it appears the metrics defined in {{colorAnalysis.py}} are no longer being added to the verify {{job}}, so the persisted {{json}} file does not included any actual measurements (e.g. they do exist in {{datasets/hsc/repo/rerun/RC/w_2020_19/DM-24822/verify/color/tract-9813/colorAnalysis_verify_job.json}}, but are not present in {{datasets/hsc/repo/rerun/RC/w_2020_22/DM-25176/verify/color/tract-9813/colorAnalysis_verify_job.json}}).  This needs to be fixed.    Additionally, to allow tracking the metrics per-tract when dispatched to the [SQuaSH metrics dashboard|https://sqr-009.lsst.io], a tract tag needs to be added to the metric metadata.",2
"DM-25867","07/08/2020 22:20:53","Make sphgeom pip installable","It would be helpful for Google POC developers if they could do daf_butler development using pypi dependencies. For this to work sphgeom needs to be buildable using pip.",2
"DM-25868","07/09/2020 02:34:18","Fix a bug in a script starting XRootD services at Qserv deployments","There is a bug In the current implementation of the startup script:  {code}  /qserv/run/etc/init.d/xrootd  {code}  The script would incorrectly redirect the log streams of both {{xrootd}} and {{cmsd}} services into the same file:  {code}  /qserv/run/var/logs/xrootd.log  {code}  As a result of this all logging information of the {{cmsd}} service would be completely lost.    The problem affects any multi-node installation of Qserv in both the bare OS or Docker-based installations, including the ""integration test"". It also affects both the master and worker components of Qserv.    A goal of this development is to ensure the output of each service goes into a separate log file based whose name is based on the name of the corresponding service ( {{xrootd}} and {{cmsd}}). After that there should be 2 separate log files in the corresponding deployments:  {code}  /qserv/run/var/logs/cmsd.log  /qserv/run/var/logs/xrootd.log  {code}",1
"DM-25871","07/09/2020 11:24:29","Replace list-table with csv-table","The tables in the Dome CSC documentation are marked up with list-table directives but it is clearer with csv-tables. This ticket is to replace all list-table markup with csv-table.",0
"DM-25880","07/09/2020 19:13:32","Write the User Document of MTAOS","Write the user document of MTAOS. The target reader is the operator or scientist. The [ts_athexapod|https://github.com/lsst-ts/ts_athexapod] can be used as an example.",2
"DM-25881","07/09/2020 20:54:53","Add extra logging to S3 datastore","During some test processing on Google we are running into an odd situation where a file exists in S3 that should not exist and an attempt is made to overwrite it. To help debug this add more debug logging messages into s3datastore.",0.5
"DM-25885","07/09/2020 23:53:43","Export/Import of some datasets prints time warnings","Exporting, then importing certain dataset types with timestamps results in the warning:  {noformat}  '1969-12-31 23:59:51.999918' is earlier than epoch time '1970-01-01 00:00:00.000', epoch time will be used instead  {noformat}  While the warning is harmless, some repositories produce dozens of copies, which is distracting.    Dataset types known to trigger this problem:  * DECam {{defects}}  * DECam {{crosstalk}}  * HSC {{unbounded}}    Discussion on [#dm-middleware|https://lsstc.slack.com/archives/C2JPT1KB7/p1594327976459000] concluded that the problem is likely limited numerical precision in the process of converting TAI to UTC, exporting it, importing it, and converting back to TAI.    While we could work around the warnings by not labeling datasets with such an (arbitrarily) early date, [~salnikov] believes the conversion must be made more precise to support general storage of epochs.",2
"DM-25892","07/10/2020 13:04:56","Modernize GenericCamera code and unit tests","The unit tests of GenericCamera still make use of the old Harness infrastructure. This is not needed anymore. This ticket aims to replace that with a more modern CSC test approach.",1
"DM-25895","07/10/2020 17:02:21","Updates to the developer guide","Small portions of the developer guide are out of date - updating the guide to match current reality. ",0
"DM-25899","07/10/2020 19:18:08","Move the body of the command-line scripts in doc/ to library modules.","There are two command-line scripts in the doc/ directory that contain a lot of implementation. Move the implementation to library modules. Also consider adding unit tests.    Also add an auto black formatter and a unit test for black formatted code.",1
"DM-25902","07/10/2020 21:37:39","Environment CSC defines 'string' attributes with units, should be unitless","In Environment_Telemetry.xml, the *Environment_windSpeed* and *Environment_airPressure* topics each have a *sensorName* string attribute, but the Units are incorrectly defined.  One is Pa and the other is degree.  They should both be unitless.",0
"DM-25903","07/10/2020 21:46:02","CALIB_ID written by findDefects.py is wrong","Sort out ambiguity between the ""slot name"" which is the butler-queired detectorName, _i.e._ just S12, and the fully qualified name reported by {{detector.getName()}}.",1
"DM-25912","07/13/2020 00:48:59","Stop automatically sending InProgress ack for commands","Stop automatically sending an InProgress ack when starting a background do_xxx callback. Instead allow the user to send the ack for slow commands, and include an estimate of duration.    Get this into v6.0.0 since it is a slightly backward incompatible change.",0
"DM-25920","07/13/2020 17:18:26","Support dome coditioning contract","Support activities to develop a dome environment conditioning contract, to control the temperature during daytime. The activities include reviewing proposals, provide technical information to potential contractors, meetings with Rubin Observatory reviewers and contractors.",3
"DM-25921","07/13/2020 17:20:35","Liaise with camera team about CTI correction","[~snyder18] has written lots of great CTI correction code, and there was talk with [~roodman] in March about getting some of this DM-ified.    Ticket is to get this conversation restarted, and put somewhere more appropriate than in private messages.",2
"DM-25922","07/13/2020 17:27:49","Ensure getVersionFromPythonModule() returns a string","I encountered the below error while building a fresh stack. This is likely due to one of my {{pip}}-installed packages pulling in {{fancycompleter}}, which has a different kind of {{version}} than we are expecting. [~tjenness] suggested forcing a cast to {{str}}, which got {{base}} to build.    {code}  =================================== FAILURES ===================================  ________________________ PackagesTestCase.testPackages _________________________  [gw2] darwin -- Python 3.7.6 /Users/parejkoj/lsst/lsstsw/miniconda/envs/lsst-scipipe-448abc6/bin/python3.7  self = <test_packages.PackagesTestCase testMethod=testPackages>      def testPackages(self):          """"""Test the Packages class""""""          packages = lsst.base.Packages.fromSystem()          # Test pickling and YAML          new = self._writeTempFile(packages, "".pickle"")          new_pkl = self._writeTempFile(packages, "".pkl"")  >       new_yaml = self._writeTempFile(packages, "".yaml"")  tests/test_packages.py:87:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  tests/test_packages.py:75: in _writeTempFile      new = lsst.base.Packages.read(tempName)  python/lsst/base/packages.py:329: in read      return cls.fromBytes(data, cls.formats[ext])  python/lsst/base/packages.py:300: in fromBytes      new = yaml.load(data, Loader=yaml.SafeLoader)  ../../miniconda/envs/lsst-scipipe-448abc6/lib/python3.7/site-packages/yaml/__init__.py:114: in load      return loader.get_single_data()  ../../miniconda/envs/lsst-scipipe-448abc6/lib/python3.7/site-packages/yaml/constructor.py:51: in get_single_data      return self.construct_document(node)  ../../miniconda/envs/lsst-scipipe-448abc6/lib/python3.7/site-packages/yaml/constructor.py:55: in construct_document      data = self.construct_object(node)  ../../miniconda/envs/lsst-scipipe-448abc6/lib/python3.7/site-packages/yaml/constructor.py:105: in construct_object      data = next(generator)  python/lsst/base/packages.py:474: in pkg_constructor      yield Packages(loader.construct_mapping(node, deep=True))  ../../miniconda/envs/lsst-scipipe-448abc6/lib/python3.7/site-packages/yaml/constructor.py:218: in construct_mapping      return super().construct_mapping(node, deep=deep)  ../../miniconda/envs/lsst-scipipe-448abc6/lib/python3.7/site-packages/yaml/constructor.py:143: in construct_mapping      value = self.construct_object(value_node, deep=deep)  ../../miniconda/envs/lsst-scipipe-448abc6/lib/python3.7/site-packages/yaml/constructor.py:100: in construct_object      data = constructor(self, node)  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self = <yaml.loader.SafeLoader object at 0x1125d7110>  node = MappingNode(tag='tag:yaml.org,2002:python/object:fancycompleter.LazyVersion', value=[(ScalarNode(tag='tag:yaml.org,200...calarNode(tag='tag:yaml.org,2002:str', value='pkg'), ScalarNode(tag='tag:yaml.org,2002:str', value='fancycompleter'))])      def construct_undefined(self, node):          raise ConstructorError(None, None,                  ""could not determine a constructor for the tag %r"" % node.tag,  >               node.start_mark)  E       yaml.constructor.ConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object:fancycompleter.LazyVersion'  E         in ""<byte string>"", line 29, column 17:  E           fancycompleter: !!python/object:fancycompleter.L ...  E                           ^  ../../miniconda/envs/lsst-scipipe-448abc6/lib/python3.7/site-packages/yaml/constructor.py:429: ConstructorError  {code}",0.5
"DM-25923","07/13/2020 18:07:42","Add cache to yamlCamera.makeCamera","Instantiating a yaml camera takes a long time (10 seconds for imsim on my Mac). This can cause significant slow downs if there are repeat calls to yamlCamera.makeCamera with the same yaml file.  This is particularly noticeable in tests.  Since the camera returned by makeCamera should be immutable, it makes sense to add an lru_cache decorator around that call.   This should also allow the local caching to be removed from obs_lsst.",0.5
"DM-25934","07/13/2020 20:26:31","MeasurePhotonTransferCurveTask appears to ignore the defect mask","I was testing MeasurePhotonTransferCurveTask with isr.doDefect=True.  The code reads in and applies the defects and correctly applies edge mask pixels when isr.numEdgeSuspect=N.  However, when calculating the mean/variance pairs, it appears that the defect mask is ignored.  I think the andMask need to be set in afwMath.StatisticsControl to correct this.",2
"DM-25936","07/13/2020 20:54:48","Fix pickle bug","There is currently a bug that prevents some models from saving correctly due to an integer that is wrapped by autograd. This ticket is to fix that bug.",1
"DM-25954","07/14/2020 12:17:48","Follow-up on Robert's comment on ""Sensor Characterization and ISR"" page following DM-25793","Robert Lupton provided comments and feedback on the ""Sensor Characterization and ISR"" confluence page: https://confluence.lsstcorp.org/display/DM/Sensor+Characterization+and+ISR#/    This ticket is to follow-up on those comments, and file new tickets for some of them if needed. ",3
"DM-25955","07/14/2020 16:49:52","Prep for presentation to Strong Lensing SCs","Presentation will involve a few overview slides and a lot of Q&A.     ",2
"DM-25957","07/14/2020 18:54:35","Remove unnecessary numpy usage from daf_butler","There are two places in daf_butler where we use numpy without needing to. Both of them involve random numbers that can be replaced with calls to {{Random}} instead. There is one usage in a test which is protected and then one other test that requires a numpy integer.    Remove the unnecessary usage and protect the remaining one.    numpy is going to be pulled in by sphgeom but that's not reason to use numpy in butler when it doesn't give us anything.",0.5
"DM-25966","07/15/2020 16:24:00","Update puppet HeaderService at the NTS","Update HeaderService version to 2.3.1",1
"DM-25970","07/15/2020 17:37:40","ap_verify CI command line broken","After merging DM-21915, {{ap_verify}} CI runs fail with the following error:  {noformat}  ap_verify.py: error: the following arguments are required: --output  {noformat}    We believe this is from the addition of an extra argument ({{--gen2}}), with a shell comment that got parsed as an argument instead. Moving the comment to its own line should fix the problem.",1
"DM-25979","07/16/2020 00:59:12","IngestIndexReferenceTask throws if coord_err_unit is None","See [this report on Slack|https://lsstc.slack.com/archives/C2JPMCF5X/p1594855003499800] from [~kadrlica]:  {code}  ingestReferenceCatalog.py repo reference_catalog_v479028-i.txt --configfile ImageProcessingPipelines/config/DC2_run22_ingestRefConfig.py   LsstCamMapper WARN: Unable to find valid calib root directory  CameraMapper INFO: Loading exposure registry from /global/u2/k/kadrlica/projects/sims_ci_pipe/repo/registry.sqlite3  root INFO: Running: /cvmfs/sw.lsst.eu/linux-x86_64/lsst_distrib/w_2020_24/stack/miniconda3-4.7.12-1a1d771/Linux64/pipe_tasks/19.0.0-63-gd25ffe2a/bin/ingestReferenceCatalog.py repo reference_catalog_v479028-i.txt --configfile ImageProcessingPipelines/config/DC2_run22_ingestRefConfig.py  Traceback (most recent call last):    File ""/cvmfs/sw.lsst.eu/linux-x86_64/lsst_distrib/w_2020_24/stack/miniconda3-4.7.12-1a1d771/Linux64/pipe_tasks/19.0.0-63-gd25ffe2a/bin/ingestReferenceCatalog.py"", line 3, in <module>      IngestIndexedReferenceTask.parseAndRun()    File ""/cvmfs/sw.lsst.eu/linux-x86_64/lsst_distrib/w_2020_24/stack/miniconda3-4.7.12-1a1d771/Linux64/pipe_base/19.0.0-24-g878c510+9/python/lsst/pipe/base/cmdLineTask.py"", line 610, in parseAndRun      resultList = taskRunner.run(parsedCmd)    File ""/cvmfs/sw.lsst.eu/linux-x86_64/lsst_distrib/w_2020_24/stack/miniconda3-4.7.12-1a1d771/Linux64/meas_algorithms/19.0.0-21-gf63060fa+1/python/lsst/meas/algorithms/ingestIndexReferenceTask.py"", line 88, in run      task.createIndexedCatalog(files)    File ""/cvmfs/sw.lsst.eu/linux-x86_64/lsst_distrib/w_2020_24/stack/miniconda3-4.7.12-1a1d771/Linux64/meas_algorithms/19.0.0-21-gf63060fa+1/python/lsst/meas/algorithms/ingestIndexReferenceTask.py"", line 354, in createIndexedCatalog      self.log)    File ""/cvmfs/sw.lsst.eu/linux-x86_64/lsst_distrib/w_2020_24/stack/miniconda3-4.7.12-1a1d771/Linux64/meas_algorithms/19.0.0-21-gf63060fa+1/python/lsst/meas/algorithms/ingestIndexManager.py"", line 84, in __init__      self.coord_err_unit = u.Unit(self.config.coord_err_unit)    File ""/cvmfs/sw.lsst.eu/linux-x86_64/lsst_distrib/w_2020_24/conda/miniconda3-4.7.12/envs/lsst-scipipe-1a1d771/lib/python3.7/site-packages/astropy/units/core.py"", line 1886, in __call__      raise TypeError(""None is not a valid Unit"")  TypeError: None is not a valid Unit  {code}",1
"DM-25980","07/16/2020 02:11:56","Increased connection timeout for the backend service in a configuration of mysql-proxy","During a stress testing of Qserv it was observed that {{mysql-proxy}} may report failures to connect to its _backend_ MySQL server and report the following to its clients:  {code}  ERROR 1105 (HY000): (proxy) all backends are down  {code}  Typically, this is seen when the number of clients exceeds a certain limit. Specifically, it has been observed (and reproduced on many occasions) that 2 out of 40 simultaneously launched query processing requests would instantly fail with the above shown error.    Further analysis of the log file (done by [~salnikov]) has revealed the following message in the proxy's log file:  {code}  2020-07-11 01:59:18: (debug) proxy-plugin.c:229: connecting to 127.0.0.1:3306 timed out after 2.00 seconds. Trying another backend.  2020-07-11 01:59:18: (critical) proxy-plugin.c.1869: Cannot connect, all backends are down.  {code}    According to the documentation portal for the proxy, there is a command line option which allows increasing the timeout:  {code}  --proxy-connect-timeout=<seconds>  {code}  See: https://downloads.mysql.com/docs/mysql-proxy-relnotes-en.pdf    Hence, a goal of this development is to investigate if setting the following value of this parameter when starting the proxy would solve the problem:  {code}  --proxy-connect-timeout=30  {code}  Deploy the updated version of Qserv at the development cluster at NCSA and test it. Report results and limitations in this ticket.",2
"DM-25984","07/16/2020 21:18:33","Investigate why the afw means of flat images are NANs for several amps of BOT data after DM-25934","Craig Lage reports that after implementing masks to calculate clipped statistics DM-25934, the mean values of several amplifiers are now NAN (they had a reasonable value before the DM-25934). Investigate the cause and fix it. ",3
"DM-25986","07/17/2020 03:03:10","Master replication controller crashes in the Kubernetes environment","Repeated crashes of the Master Replication Controller have been reported at IN2P3 in the Kubernetes-based Qserv installation which also includes the Replication/Ingest system. The crashes are seen in two scenarios:  * when starting the Qserv instance the Controller may crash a few times before getting into a stable state  * when ingesting new catalogs (typically when committing {{super-transactions}})    In both cases the following messages were reported by the Controller:  {code}  terminate called after throwing an instance of 'boost::wrapexcept<boost::system::system_error>'    what():  cancel: Bad file descriptor  /config-start/start.sh: line 83:    10 Aborted                 (core dumped) qserv-replica-master-http ${PARAMETERS} --config=""${CONFIG}"" --qserv-db-password=""${MYSQL_ROOT_PASSWORD}""  {code}  A goal of this effort is to investigate a reason of the crash and fix/reinforce the Controller's implementation.",2
"DM-25994","07/17/2020 15:09:38","Publish the Velocity or Needed Telemetry to DDS for Rotator","Publish the velocity or needed telemetry to DDS for rotator.",3
"DM-26001","07/17/2020 21:07:49","Fix buttons in www.lsst.io to use the correct pointer","{quote}The cursor doesn’t become the pointing-finger over the blue buttons, and the buttons don’t display any transient “I’m being pressed” feedback.  Combined with the other things below it creates some uncertainty about whether the buttons are “live” or not.  {quote}",0.5
"DM-26003","07/17/2020 22:02:22","Post-release cleanup of www.lsst.io codebase","Some maintenance chores now that we've launched www_lsst_io:   * Upgrade the node version we're using locally and in GitHub Actions, and thus update more packages (locally this means adopting the node version manager).   * Cleaning up the components, namely moving components into the basics/ and algolia/ directories.   * More consistent documentation of components",3
"DM-26004","07/17/2020 23:01:20","Clean up qgraph show-workflow implementations ","Currently {{pipetask qgraph --show workflow}} figures out the dependencies in its own method, but the {{QuantumGraph}} class can provide such information already.  I don't know why I didn't use it but I'll clean it up here.  For this ticket the output format would stay the same, just improving the implementations. ",1
"DM-26007","07/18/2020 00:24:37","defaultFilter is not used if a filterName is given to loadSkyCircle","If {{defaultFilter}} is supplied without a {{filterMap}}, and the user specifies {{filter=something}} in {{loadSkyCircle()}} (as one normally does, because when you load the refcat you don't know how its filter mappings are configured), {{getRefFluxField()}} will fail:    {code}  RuntimeError: Could not find flux field(s) r_camFlux, r_flux  {code}    I believe the solution is to add {{camFlux}} to the {{fluxFieldList}} in {{getRefFluxField()}}, so that if {{defaultFilter}} is specified, you'll still get a fluxField of some kind. The behavior of how {{defaultFilter}} and {{filterMap}} should interact is undefined in the docs that I can see, so although this is a behavior change, I think it is more self-consistent. We also do not use {{defaultFilter}} anywhere in the stack currently that I've found, and this change should make it useable for RFC-697.",2
"DM-26008","07/18/2020 00:29:42","Add YAML representers to pex_config","pex_config serializes to pickle by converting to text. This should be easy to also do as a yaml representer.  Doing this will give us the option of switching quantum graphs over to yaml.",1
"DM-26010","07/18/2020 15:17:04","Simplify mypy configuration after ending use of namespace packages","Now that [~tjenness] has removed our usage of namespace packages, we can simplify our mypy configuration by running it in a more standard manner - from the root directory, on a directory, instead of from the python subdirectory, on a package.    At least in my (VSCode) case, this will also make in-editor configuration vastly better.",0.5
"DM-26011","07/18/2020 23:49:15","Intermittent failures in obs_lsst tests","Since I merged DM-25923 we've been getting some intermittent failures in obs_lsst tests that seem to be related to the camera caching returning the wrong object.  This only seems to happen in some environments with many cores. The TS3 ingest test fails because it ends up registering a camera that looks like LSSTCam and not TS3.  There is also a failure in writeCuratedCalibrations that is related where the detector in the data package does not exist in the camera despite the correct data package being used.  ",1
"DM-26013","07/20/2020 16:24:36","Check SALSubsystems.XML file for attributes for several subsystems","This page    [https://confluence.lsstcorp.org/display/LTS/SALSubsystem+Attributes]    describes the attributes of all subsystems in SAL. Check the attributes for the following projects and correct them if necessary:   * Dome   * EAS   * ESS   * HVAC",1
"DM-26015","07/20/2020 17:16:47","Validate PipelineTaskConnections dimensions are iterables other than str","if a connection's (or task's) dimensions is specified without a comma and there is only one dimension a weird error is generated as the system attempts to iterate over the string. This ticket will add validation to catch this case",1
"DM-26020","07/20/2020 19:00:18","Rotator Plant Model Analysis","Tasks capturing Mike's work on the Rotator Plant Model for the last couple of weeks.  Mike has entered 24 hours into his time sheets, so 3 SP will used for this task.",3
"DM-26027","07/20/2020 21:50:21","Update docker images and conda for Headerservice 2.3.2","HS need to be deployed using version 2.3.2",1
"DM-26036","07/21/2020 01:01:22","Add missing unit test","In DM-25712 I forgot to add one unit test (which doesn't do very much, so not a big loss). Add it.",0
"DM-26040","07/21/2020 18:00:08","Add AP timing metrics for DiaPipelineTask and all subtasks","Currently, {{ap_verify}} times only {{AssociationTask}} from {{ap_association}}. Add timing measurements for {{DiaPipelineTask}} and its other first-level subtasks to bring the level of SQuaSH instrumentation in line with image differencing and single-CCD processing.",2
"DM-26047","07/21/2020 20:22:37","Defect calibration product filename collision","I have been creating defect calibration products to use when running measurePhotonTransferCurve.py.  After ingestion, the defect calibration product filenames have the sensor number (Sxx) but not the raft number (Rxx) or detector number, and so are not unique.  An example filename is {{CALIB/defects/2019-10-12T15\:02\:16.071000/defects-2019-10-12T15\:02\:16.071000-S11.fits}}.  When running BOT data, both detector 40 (R11_S11) and detector 85 (R21_S11) ended up with the same filename.      The defect files were created with:  {code}  findDefects.py /project/shared/BOT --calib /project/shared/BOT/rerun/cslage/CALIB --rerun /project/shared/BOT/rerun/cslage/E2V_6790D_40^85 --id detector=40^85 --visitList=3019101200480 -c mode=MASTER writeAs=BOTH badOnAndOffPixelColumnThreshold=500  {code}  and ingested with:  {code}  ingestCuratedCalibs.py /project/shared/BOT /project/shared/BOT/rerun/cslage/E2V_6790D_40^85/calibrations/lsstCam/defects --calib /project/shared/BOT/rerun/cslage/CALIB --config clobber=True --ignore-ingested  {code}",0.5
"DM-26048","07/21/2020 22:31:30","Dome software meetings","Prepare and coordinate weekly meetings on dome control software with EIE contractor.",5
"DM-26052","07/22/2020 00:59:48","Please document private fields and the ackcmd topic in the HTML for ts_xml","Please add documentation for the private fields present for all topics in the HTML generated from ts_xml. Please also document the ackcmd topic. None of these are defined in the XML, so this documentation will have to be manually created and maintained.    The following descriptions are my own understanding and are how ts_salobj uses them (though users can set the public fields of the ackcmd topic as they like). Anyone should feel free to correct me if I am mistaken. I have added [~dmills], [~tribeiro], and [~rbovill] as watchers for this reason.    I suggest that the private fields be documented in one place (even though they are present for all topics) to avoid needless repetition. But do as you think best. The private fields, which are present on all topics are:    * private_sndStamp: time at which the sample was sent (TAI, unix seconds).  * private_rcvStamp: time at which the sample was received (TAI, unix seconds).  * private_identity: For a CSC this has the form <SAL component name>[:index] e.g. ""Script:5"" or ""ATDome"". For a user this has the form username@host. This field is used for authorization. New in ts_sal 4.2.  * private_origin: the process ID of the writer.  * private_host: the host address of the writer. This is deprecated, and due to be removed in ts_sal 5.  * private_seqNum: for commands this uniquely identifies the command (within a reasonable timespan, since the value must eventually wrap around). For other kinds of topics this may be incremented for each sample, though that is optional.  * <SAL component name>ID e.g. ScriptID (only present for indexed SAL components): the SAL index of the writer.    The ackcmd topic is how commands are acknowledged. Note that there is just one ackcmd topic that is used to acknowledge all commands. The public fields and one private field of the ackcmd topic are as follows:    * private_seqNum: the private_seqNum of the command being acknowledged.  * ack: the acknowledgement code, one of the CMD_ constants, such as CMD_COMPLETE = 303 or CMD_FAILED = -302.  * error: the error code. Set to a nonzero value if ack=CMD_FAILED.  * result: salobj sets this to an error message if ack=CMD_FAILED. I am not sure if it is used for anything else.  * identity: the private_identity of the command being acknowledged (see below).  * host: the private_host of the command being acknowledged. private_host is deprecated, so this field is also deprecated, and both should be gone in ts_sal 5.  * origin: the private_origin of the command being acknowledged, which is a process ID.  * cmdtype: command identifier: I believe this is supposed to be the 0-based index of the command in an alphabetical list of all topics for that SAL component. That is surprising to me as the list includes all topics, not just command topics. I hope I am wrong, but if so, I need to fix it in ts_salobj.  * timeout: the approximate expected duration of the command. Only set if ack=CMD_INPROGRESS.    I am not sure if you want to also explain the acknowledgement sequence here. It should certainly be documented somewhere, and this might be a nice place for it. As a quick reminder:    * The initial ack code is CMD_ACK when the command is read.  * If the command will take a long time to run then the CSC should respond with a CMD_INPROGRESS ack that includes an estimate of the command duration. This should only happen after the command has been approved.  * Usually the next ack is the final one for this command and is typically one of:  ** CMD_COMPLETE if the command finishes successfully.  ** CMD_FAILED if the command fails.  ** CMD_NOPERM if the command issuer is not authorized to send the command.  ** CMD_ABORTED if the command is superseded.  * CMD_TIMEOUT is returned if the command _issuer_ times out waiting for command completion. Note that it is the issuer, not the CSC, that returns this. If the command times out inside the CSC then the ack code is CMD_FAILED. In particular note that if the issuer gives up waiting for command completion and then the command finishes, the user will only see the CMD_TIMEOUT ack, but the DDS system will see the final ack from the CSC.  * There are at least two other CMD_ codes as well:  ** CMD_NOACK: ts_salobj sets the ackcmd field of AckTimeoutError to this value if no CMD_ACK was seen for the command before the command timed out. I am not sure how non-salobj systems use this code.  ** CMD_STALLED: I think the intent is to indicate ""this is going more slowly than I expected, but I am still working on it."" I am not sure which systems use this code.    Here is how the ackcmd topic shows up in an IDL file. This also shows the data types of the private fields:    {code}  struct ackcmd_51be61c7 {  // @Metadata=(Description="""")        string<8> private_revCode;        double  private_sndStamp;        double  private_rcvStamp;        string            private_identity;        long  private_origin;        long   private_host;        long  private_seqNum;        long TestID; // @Metadata=(Description=""Index number for CSC with multiple instances"")        long   ack;        long   error;        string<256> result;        string            identity;        long  host;        long  origin;        long  cmdtype;        double  timeout;   };  {code}    ",1
"DM-26055","07/22/2020 10:47:15","Install GenericCamera CSC on raspberry pi3 and make the camera work with it","Now that the raspberry pi 3 with the Canon all-sky camera is up and running at the Recinto, it is time to install the CSC on it and get it working with the camera.",2
"DM-26061","07/22/2020 18:50:43","Fix missing seek_to_beginning method in alert-stream-simulator","As pointed out by @ebellm, there's a missing method from the internal Kafka client.",1
"DM-26064","07/22/2020 19:50:24","Change alert-stream-simulator tests to use latest alert packet version","Tests currently fail because they specifically ask for v2.1 of the alert schema, but that was deleted recently. They should use the latest schema.",1
"DM-26066","07/22/2020 21:36:02","Use proper versioning in scarlet","Use setuptools_scm to properly calculate the scarlet version, and create a scarlet 1.0 tag.",1
"DM-26069","07/22/2020 22:39:43","Create requirements list for new Filter implementation","After discussion with [~krzys] about approaching implementation of the new Filter design, we wanted to write down a list of requirements for the new system. RFC-541 was more focused on technical requirements, but some of the comments there can be built on to help with use case requirements. RFC-624 provides a design, but discussion since that has led to some questions about how exactly that design maps to how this system would be used.    For this ticket, I will write up (probably not a DMTN?) a list of use cases and requirements for Filters in the stack, based on the above RFCs, my own experiences with the {{Instrument}} {{FilterDefinitions}}, and expectations for how filter names are used for photometry.",2
"DM-26070","07/22/2020 23:42:12","Add visit definition to ap_verify","DM-21915 implemented ingestion of {{ap_verify}} datasets in {{Gen3DatasetIngestTask}}, but neglected to include visit definition. This means that the repository, following ingestion, is not in a state where the AP pipeline can be run on it.    Add visit definition to {{Gen3DatasetIngestTask}}, so that users running {{ingest_dataset.py}} from the command line get a repository that is usable immediately. Because {{ap_verify}} datasets are necessarily complete and self-contained, the usual complications to doing visit definition (e.g., partially ingested visits) do not apply.",3
"DM-26081","07/23/2020 18:36:40","Make Hexapod and Rotator configurable CSCs","At present the Hexapod and Rotator are not configurable. Update the base class to make them configurable. Rotator will offer TCP/IP configuration. Hexapod will also offer compensation configuration once DM-25856 is implemented.",3
"DM-26084","07/23/2020 22:50:33","Add JenkinsTestResults entries for ATMCS, Test, MTDomeTrajectory and Watcher to SALSubsystems.xml","Add the missing JenkinsTestResults entries to SALSubsystems.xml for Test, MTDomeTrajectory and Watcher.",0
"DM-26088","07/24/2020 04:01:33","Unmask LSST_LOG_CONFIG for setting log config","lsst.log allows its config to be set via a configuration file at the LSST_LOG_CONFIG environment variable. However currently this feature cannot be used with the PipelineTask framework, because it resets the logging properties.      It'd be convenient if this reset can be skipped while a configuration file is provided.   ",1
"DM-26089","07/24/2020 15:07:09","Install ESS CSC on the raspberry pi 4","Install the ESS CSC on the raspberry pi 4 running CentOS 7 and make sure that it can be started. Also update the installation guidelines with the necessary steps.",1
"DM-26091","07/24/2020 17:19:12","Review the Trajectory Document of TMA","Review the trajectory document of TMA: [^TrajectoryGenerationAlgorithmForLSST.docx].",1
"DM-26094","07/24/2020 19:38:28","Support LSST_DDS_HISTORYSYNC<0 to disable wait_for_historical_data in ts_salobj","Support LSST_DDS_HISTORYSYNC<0 to disable wait_for_historical_data in ts_salobj.    This is for use on nodes that have no durability service running. Some or all CSCs that do not control other CSCs may be run in this way.",0
"DM-26095","07/24/2020 19:39:51","Support LSST_DDS_HISTORYSYNC<0 disables wait_for_historical_data in ts_sal","Support LSST_DDS_HISTORYSYNC<0 disables wait_for_historical_data in ts_sal.    This is for use on nodes that have no durability service running. Some or all CSCs that do not control other CSCs may be run in this way.",1
"DM-26100","07/24/2020 23:31:50","Review/update orchestration harness for KPM50 tests","For KPM50 we want to check the state of the existing test harness that was used for previous tests and understand its limitations and bottlenecks.",8
"DM-26102","07/25/2020 16:56:31","Enable auto-tuning for the XRootD ""spread"" in Qserv","This ticket is based on [DM-25891].    The new version of XRootD allows to auto-tune the client parameter ""spread"" by setting its value to 0. Hence, an objective of this effort is to test the effect of auto-tuning when processing multiple shared scan queries at a large-scale Qserv installation. If it works, and if it doesn't have any side effects on the performance of Qserv then make auto-tuning the default parameter of Qserv.",2
"DM-26104","07/26/2020 21:17:49","Do the Coverage Test for Rotator PXI Code","Do the coverage test for rotator PXI code by *gcovr*.",1
"DM-26110","07/27/2020 18:42:37","Add conda build to ts_authorize","Add the conda build files to ts_authorize. This may have to wait until ts_sal 5 is released -- at least to run it.",1
"DM-26112","07/27/2020 18:53:59","Create Jenkins Conda build for the Environment CSC","Task to track the work to create the Conda recipe and Jenkinsfile.conda for the Environment CSC.",2
"DM-26113","07/27/2020 19:38:37","Cherry pick setting a random LSST_DDS_DOMAIN in BaseCscTestCase.check_bin_script to salobj 5","Release a salobj 5 that sets a random LSST_DDS_DOMAIN in {{BaseCscTestCase.check_bin_script}} by cherry-picking the change from salobj 6",0
"DM-26119","07/28/2020 00:03:16","Test dataset disassembly with ci_hsc_gen3","Composite disassembly is not properly tested anywhere for afw Exposures. Run ci_hsc_gen3 with disassembly turned on and see what breaks.",2
"DM-26120","07/28/2020 01:02:20","[XML] Update IOTA entry in SALSubsystems.xml","These fields for the IOTA entry in ts_xml/sal_interfaces/SALSubsystems.xml need to be updated:    <ActiveDevelopers> text must not be empty    <Github> text must not be empty    <JenkinsTestResults> text must not be empty    <Simulator> have a URL or one of the following: Internal to CSC, Not Required, Not Provided",0
"DM-26122","07/28/2020 01:13:47","[XML] Update MTEEC entry in SALSubsystems.xml","These fields for the MTEEC entry in ts_xml/sal_interfaces/SALSubsystems.xml need to be updated:    <Github> text must not be empty    <JenkinsTestResults> text must not be empty    <Simulator> have a URL or one of the following: Internal to CSC, Not Required, Not Provided",0
"DM-26128","07/28/2020 01:32:29","[XML] Update MTM2 entry in SALSubsystems.xml","These fields for the MTM2 entry in ts_xml/sal_interfaces/SALSubsystems.xml need to be updated:    <JenkinsTestResults> text must not be empty    * Note: Work with DM-26120.",0
"DM-26129","07/28/2020 01:35:18","[XML] Update MTAOS entry in SALSubsystems.xml","These fields for the MTAOS entry in ts_xml/sal_interfaces/SALSubsystems.xml need to be updated:    <VendorContact> text must not be empty",0
"DM-26133","07/28/2020 15:42:16","generateAcronyms not robust for misformated glossary","A recent addition to glossaries missed one quote - it passes travis anyway of course. Finding the offender required adding a print to the script. There should really be a try catch on the reader to indicate the offending line.    It may also be good to add a check format option which could be invoked by travis for a badly formated glossarydefs ffile. ",2
"DM-26142","07/29/2020 00:47:45","Add the columns separator option to the file ingest tool","Add the following option to the command line tool:  {code}  qserv-replica-file-ingest --columns-separator={COMMA|TAB} ...  {code}  The default value of the option should be {{COMMA}}.    Update section 11.2.2 of the [User guide for the Qserv Ingest system|https://confluence.lsstcorp.org/pages/viewpage.action?pageId=133333850] as well.",1
"DM-26144","07/29/2020 16:38:12","Allow Pipelines to inherit configs","Currently if a task is declared in a Pipeline task list, and the task is already declared in an inherited pipeline (with the same label), the task and its configs replace the inherited versions meaning all configs need to be declared in a pipeline, even if only one config is to be added.    This ticket will change this behavior. If a task is declared in a pipeline that inherits the same task with the same label, the new config options will be added to the existing config overrides. If a label is declared with a different task than what exists in the inherited pipeline, it will completely replace the existing definition, as the config options likely no longer apply.",1
"DM-26159","07/30/2020 07:52:38","Fix teardown crash in qhttp unit test","There has been for some time an intermittent crasher that occurs sometimes during teardown of the boost::unit unit test for qhttp (testqhttp).    Andy Salnikov recently obtained a repeatable failure and extracted a core; investigation of this indicated the problem is occurs as a bad object dereference in a race between the asio service teardown in one thread and a socket accept handler executing on the service handler thread.",0.5
"DM-26160","07/30/2020 16:36:13","Fix test failure where URI special characters are in build directory","Following DM-24475 the obs_base build failed because there is a ""+"" in the build path and somewhere a double URI encoding is happening so that the + is encoded and then the resulting % is encoded.",1
"DM-26161","07/30/2020 16:41:23","Setup the Software Environment to Update M2 Software","This task is to setup the needed software environment (by virtual box) on my MacBook Pro to update the M2 software. The original linux laptop and desktop are in Chile and Tucson office individually. Therefore, I need to setup the environment on my personal laptop to do the development of M2 software.",1
"DM-26166","07/30/2020 18:59:57","Recent tests of ptc.py show strange results","I've been running ptc.py on the BOT data for some time with good results.  Recently, I've been working with the defect calibration files to mask out the edge pixels.  On 17-Jul, I ran ptc.py on the BOT data for detector 94.  This was with   {noformat}w_2020_28{noformat}  {{  You can see the python script that ran that at }}  {noformat}/project/cslage/BOT_lspdev/standalone/Run_PTC_Edge_15Jul20.py{noformat}  {{and the output repo is at}}  {noformat}/project/cslage/BOT_lspdev/E2V_6790D_Gain_Edge7_R22S11{noformat}  {{Everything looked completely normal, and the gains agreed better with the Eotest Fe55 results.  So I set up to run all of the CCDs that way.  This took a while to get everything running, but it is now finished with all 81 CCDs.  The python script is at }}  {noformat}/project/cslage/BOT_lspdev/standalone/Run_PTC_All_29Jul20.py{noformat}  {{and the output repo is at }}  {noformat}/project/shared/BOT/rerun/cslage/PTC_6790D_All{noformat}  {{This ran with}}  {noformat}w_2020_30{noformat}  The  {noformat}PTC_det94.pdf{noformat}  plot looks OK at first glance, but the values are all messed up.  The gain comes out ~1.8 instead of ~1.0.  The PTC curve, instead of turning over at ~100,000 ADU like it should, turns over at ~60,000 ADU. I'm not sure what went wrong.  I don't know if it is something I did or if the code has changed.  Note that both the good run above and the bad run are after the refactoring of ptc.py",1
"DM-26168","07/30/2020 19:49:16","Test wavelet detection in crowded field","The TVS crowded field working group is working on testing scarlet on DECam images of the globular cluster NGC-6569. One of the issues is detecting all of the sources in the field, since the high stellar density and wide PSF in redder bands makes detection more ambiguous.    This ticket is to test out the wavelet detection algorithm of Remy Joseph. The idea is that because the field is dominated by point sources, we should be able to use wavelets to create a high pass filter for the image and only allow the highest frequency coefficents to pass through, removing the wings of PSFs and lowering the detection threshold.",2
"DM-26170","07/30/2020 20:37:53","Migrate ci_cpp from lsst-dm to lsst","Upon acceptance of the associated RFC, ci_cpp, ci_cpp_gen2, ci_cpp_gen3, and testdata_latiss_cpp will be moved from the lsst-dm github organization to lsst.",1
"DM-26171","07/30/2020 21:15:06","calibrations cannot be retrieved on the last day of a validity range","The validity ranges in the calibration database are currently set as days with no time included, which means that they implicitly round down.   In consequence, a calibration that should be valid on the last day of a validity range can never be looked up.    The simplest fix is to make validEnd equal to validStart for the next interval.  There is a problem for calibs taken exactly at the roll-over, but this window is only 1ms wide, and providing we don't take calibs at midnight we will be OK.  For gen2, I think this is OK, but we should handle it properly in gen3 (especially as people like to take calibrations on dark and stormy nights)    [~czw] points out that the place to fix the code is in the {{fixSubsetValidity}} method in ingestCalibs.py",1
"DM-26174","07/30/2020 21:55:49","Optimize the Tool that Generates the SAL LabVIEW APIs","Optimize the tool that generates the SAL LabVIEW APIs. The [ts_SALLabVIEW|https://github.com/lsst-ts/ts_SALLabVIEW] is used now to transform the data type from SAL IDL. The execution speed is slow now. This task will try to optimize the performance.    * I noticed some type definition do not work as expected. This task will try to fix this as well.",3
"DM-26177","07/30/2020 23:15:40","Write the Test Framework of SAL LabVIEW API in Phase 1","Write the test framework of SAL LabVIEW API. This task begins to construct a framework to test the LabVIEW API for each new SAL update. In the past, we used the component CSC in LabVIEW to test this. Now, we need a framework to test this automatically. The SAL LabVIEW APIs rely on the shared memory, which is not needed for the C++ and JAVA APIs. Therefore, this kind of automatic test framework should be helpful for the maintenance of CSC in LabVIEW. This task is in the phase 1.",3
"DM-26181","07/31/2020 01:02:01","Ensure that filters are defined in pipetask multiprocessing","In DM-26119 I learned that when pipetask uses multiprocessing we never instantiate an {{Instrument}} and therefore we never define the filters for that instrument in the global singleton.    In single process mode it's fine because at some point an Instrument is created.    Modify pipetask multiprocessing such that the dataIds are scanned for the ""instrument"" dimension and we call {{Instrument.fromName(dataId[""instrument""], registry)}}.  Currently we only expect one instrument.    When the singleton is removed it's likely that some related initialization will be needed to register the filters but we assume that would allow the same initialization for multiple instruments.    CC/ [~krzys], [~Parejkoj] in case they have come across this problem before.",1
"DM-26185","07/31/2020 16:52:55","PCW 2020","This epic is for tracking the time spent at the PCW 2020",40
"DM-26187","07/31/2020 17:16:49","Rename deblend.py in meas_deblender and meas_extensions_scarlet","To {{sourceDeblendTask.py}} and {{scarletDeblendTask.py}} respectively, thereby implementing RFC-717. ",1
"DM-26188","07/31/2020 17:56:06","Coordinate the Jenkins Conda build hackathon","This task covers the time necessary to prepare for and then lead the hackathon to create the various CSC Conda Jenkins builds.",3
"DM-26189","07/31/2020 18:00:08","Various build script updates","As SAL and XML are released, the build scripts need to be updated accordingly.  Normally, there are specific events that I use to create these tasks (a release, for example).  However, this sprint had no such event, but there was significant work to be done on the scripts.  Namely, activating the SALSubsystems.xml unit tests and writing the Jira tickets for outstanding work on that file.",2
"DM-26191","07/31/2020 18:36:45","Improve documentation for CscCommander.amain","Improve the doc string for CscCommander.amain to document the parameters and clarify that index is a required argument.",0
"DM-26197","07/31/2020 22:13:46","Replace hexrotcomm.CscCommander with salobj.CscCommander","When time permits rewrite the Hexapod and Rotator CSC commanders to use the standard salobjlCscCommander and delete the hexrotcomm.CscCommander.    I wrote the hexrotcomm version first, and the salobj version based on that. The salobj version is now much better.",1
"DM-26198","07/31/2020 22:16:59","Allow butler import to skip some dimensions","As discussed on DM-26195 it would be useful to skip instrument dimensions on import if the instrument has already been registered.    One quick work around is to allow the user to specify a list of dimensions to skip and use them in the {{Backend.load()}} method.    It looks like this is a trivial fix as such and it does work.",1
"DM-26199","07/31/2020 22:31:57","Support dome coditioning contract","Support activities to develop a dome environment conditioning contract, to control the temperature during daytime. The activities include reviewing proposals, provide technical information to potential contractors, meetings with Rubin Observatory reviewers and contractors.",3
"DM-26200","07/31/2020 23:53:17","ATHexapod should set itself to FAULT state if start command fails","The ATHexapod is not setting its internal state to be FAULT when the start command fails. It does issue the proper summaryState info to SAL, but then its internal state is inconsistent with what is presented by the EFD.",0
"DM-26208","08/03/2020 15:19:46","Please update the version of Python in lsstts/robot:latest to 3.7.x and put black on the PATH","Please update the version of Python in lsstts/robot:latest so we can use features in Python 3.7.    Another option to consider is running ts_xml Jenkins with lsstts/salobj instead, if that would work.",2
"DM-26209","08/03/2020 17:54:55","Attend 2020 RubenObs PCW","Attend the 2020 Project & Community Workshop 8/10-8/14",2
"DM-26210","08/03/2020 18:49:48","Attend PCW ","Attend the virtual PCW meetings",2
"DM-26211","08/03/2020 18:51:32","Generate list of essential ts_sal files","Generate a list of essential components of ts_sal that are required  in order for Jenkins to be able to create the CSC runtime assets  and run the tests",1
"DM-26225","08/04/2020 11:28:12","Integrate ESS USB code in ESS CSC","The ESS code has been adjusted to support a single USB device. Integrate that code into the CSC.",1
"DM-26229","08/04/2020 17:43:04","Investigate failure in daf_butler test in nightly build","In the nightly release build https://ci.lsst.codes/blue/organizations/jenkins/release%2Ftarball/detail/tarball/6041/tests one of the ingest symlink tests failed:    {code}  self.assertTrue(not uri.scheme or uri.scheme == ""file"", f""Check {uri.scheme}"")  > self.assertTrue(os.path.islink(uri.path))  E AssertionError: False is not true  {code}    Normal Jenkins is passing and the job worked the previous night.    Only two tickets were merged yesterday: DM-26119 and DM-25985; and neither look like they should affect ingest.  ",1
"DM-26230","08/04/2020 17:54:48","Improve pipetask dignostics on multiprocessing timeouts.","When a task timeouts the disgnostics that pipetask produces is only a standard traceback mentioning TimeoutError:  {noformat}  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-7-g518c986/bin/pipetask"", line 26, in <module>      sys.exit(CmdLineFwk().parseAndRun())    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-7-g518c986/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 494, in parseAndRun      return self.runPipeline(qgraph, taskFactory, args)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-7-g518c986/python/lsst/ctrl/mpexec/cmdLineFwk.py"", line 694, in runPipeline      executor.execute(graph, butler)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-7-g518c986/python/lsst/ctrl/mpexec/mpGraphExecutor.py"", line 71, in execute      self._executeQuantaMP(quantaIter, butler)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-7-g518c986/python/lsst/ctrl/mpexec/mpGraphExecutor.py"", line 175, in _executeQuantaMP      results[dep].get(self.timeout)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/multiprocessing/pool.py"", line 653, in get      raise TimeoutError  multiprocessing.context.TimeoutError  scons: *** [DATA/shared/ci_hsc_output] Error 1  scons: building terminated because of errors.  {noformat}    but it does not say which task timed out or for which quanta. Need to add that info.    And a bug that [~ktl] noticed is that there exists timeout command line option which is supposed to change timeout value but it's not used by CmdLineFwk.  ",1
"DM-26232","08/04/2020 18:17:17","Command ESS CSC using an external python script","Now that the basic ESS CSC is done, it needs to be commanded as well. In order to test that functionality, create an additional python script and command the CSC with that.",1
"DM-26233","08/04/2020 18:19:20","Command all-sky CSC using an external python script","Now that the basic all-sky CSC and driver are done, they need to be commanded as well. In order to test that functionality, create an additional python script and command the CSC with that.",1
"DM-26235","08/04/2020 19:24:08","Dome software meetings","Prepare and coordinate weekly meetings on dome control software with EIE contractor.",5
"DM-26236","08/04/2020 19:26:55","Support dome conditioning contract","Support activities to develop a dome environment conditioning contract, to control the temperature during daytime. The activities include reviewing proposals, provide technical information to potential contractors, meetings with Rubin Observatory reviewers and contractors.",3
"DM-26250","08/05/2020 15:40:41","Modernize MTEEC GitHub repo, code and unit tests","The MTEEC GitHub repo is very much outdated and needs to be modernized. This also is true for the code and unit tests. Please fix this all.",2
"DM-26251","08/05/2020 15:42:07","Modernize EAS GitHub repo, code and unit tests","The EAS GitHub repo is very much outdated and needs to be modernized. This also is true for the code and unit tests. Please fix this all.",2
"DM-26252","08/05/2020 15:51:34","Create conda build recipe for MTEEC","Once the MTECC GitHub repo, code and unit tests have been modernized, the conda recipe can be created as well.",1
"DM-26253","08/05/2020 15:52:37","Create conda build recipe for EAS","Once the EAS GitHub repo, code and unit tests have been modernized, the conda recipe can be created as well.",1
"DM-26254","08/05/2020 15:55:26","Create HVAC CSC","Create the CSC for HVAC that at least publishes the HVAC telemetry based on what it receives via MQTT.",3
"DM-26255","08/05/2020 15:57:49","Create conda build recipe for HVAC","Once the HVAC CSC has been created, the conda recipe can be created as well.",2
"DM-26257","08/05/2020 17:57:53","Update docker images and conda for Headerservice 2.3.3 and new ts_xml/ts_salobj/ts_idl","HS need to be deployed using version 2.3.2",1
"DM-26258","08/05/2020 18:32:53","Fix problem with daf_butler tests failing with escaped characters in path","A test failed in the nightly (again) which is the same problem but in a different test. This time I will run the tests locally in a similar directory.",0.5
"DM-26260","08/05/2020 21:41:38","ExposureFitsReader can't read compressed darks/biases","Working on the new pipeline_check package I realized that the 33MB dark and bias files can trivially be compressed using fpack to be 5MB each.    Compressing these single data array FITS files (GZIP_1 or GZIP_2) then breaks ExposureFitsReader with a SEGV when trying to read the mask that does not exist:    {code}  afw.image.MaskedImageFitsReader WARN: Mask unreadable (FitsReader not initialized; desired HDU is probably missing.); using default  Process 58746 stopped  * thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x8)      frame #0: 0x0000000110ee02fd libafw.dylib`lsst::afw::image::MaskedImage<float, int, float> lsst::afw::image::MaskedImageFitsReader::read<float, int, float>(this=0x00007ffeefbf1628, bbox=0x0000000121f54770, origin=PARENT, conformMasks=false, needAllHdus=false, allowUnsafe=false) at MaskedImageFitsReader.cc:216:43     213              }     214              LOGL_WARN(_log, ""Mask unreadable (%s); using default"", e.what());     215              // By resetting the status we are able to read the next HDU (the variance).  -> 216              _maskReader._fitsFile->status = 0;     217          }     218          try {     219              variance = std::make_shared<Image<VariancePixelT>>(  {code}    The compression confuses the reader because compression creates an extra hdu binary table. MaskedImageFitsReader does not look at the EXTTYPE header and so tries to read the next extension as if it's a mask.    I'm not entirely sure why we write FITS files with EXTTYPE headers but then don't use them when we read.    The most important issue here is the SEGV.    To reproduce run {{fpack -g2}} on the {{CALIB/DARK/2013-11-03/NONE/DARK-2013-11-03-010.fits}} from {{testdata_ci_hsc}}.  Then try to read the .fits.fz file with ExposureFitsReader:    {code:python}  from lsst.afw.image import ExposureFitsReader  reader = ExposureFitsReader(""dark.fits.fz"")  e = reader.read()  {code}    [~ktl] points out that we probably should be reading these as Image and not Exposure but that might require invasive changes to pipelines.    A more generic solution would be for MaskedImageFitsReader to query for EXTTYPE and if there are no MASK and VARIANCE extensions, skip trying to read them regardless of the number of extensions found.    ",1
"DM-26263","08/06/2020 00:13:30","Add “scientist-facing” release notes to pipelines.lsst.io","[~jbosch] & [~ebellm] have prepared notes on how the changes made in release 20.0.0 impact on the data products which will be consumed by end users. Please ensure these notes are included on the pipelines.lsst.io site.",1
"DM-26264","08/06/2020 01:59:19","Fix qhttp ajax unit test","qhttp ajax unit test sometimes fails in Jenkins.  Troubleshoot and fix.",0.5
"DM-26265","08/06/2020 15:39:15","ap_verify tests use fixed temp directories","The two sets of {{TestWorkspace}} tests use a hardcoded temporary directory ({{_temp}}) for some tests that don't use the standard setup, causing a conflict between the tests. Replace {{_temp}} with a normal temporary directory.",1
"DM-26266","08/06/2020 16:11:04","Attend 2020 RubenObs PCW","Attend 2020 RubenObs PCW.",2
"DM-26267","08/06/2020 17:47:13","Disable the fault method in hexrotcomm.BaseCsc","hexrotcomm.BaseCsc handles state transitions differently than salobj, since the state is maintained in the low-level controller. One problem with this is that it does not make sense for a hexrotcomm CSC to send itself into a FAULT state.    Thus the fault method should be disabled and avoided.",0
"DM-26270","08/06/2020 18:50:29","Support the New Enum Syntax in IDL File","Support the new enum syntax in IDL file: (1) shared enum such as the *SummaryState*, and (2) assign the enum value in <Enumeration> of xml. Use the *Test* subsystem as an example, the structure of summary state looks like this:    {color:#4c9aff}struct logevent_summaryState \{{color}  {color:#4c9aff}    long summaryState;{color}  {color:#4c9aff}    long priority;{color}  {color:#4c9aff} };{color}    There is no information to say summaryState is a enum (e.g. *// enum :*).    In addition, the following part in *Test* xml does not reflect to IDL file:  {color:#4c9aff}<Enumeration>Int0ValueEnum_Zero=0, Int0ValueEnum_Two=2, Int0ValueEnum_Four=04, Int0ValueEnum_Five=0x05</Enumeration>{color}",2
"DM-26283","08/08/2020 00:14:21","Determine the compensation models for the Hexapod, including coefficients","Determine a suitable model for Hexapod compensation, with coefficients.",2
"DM-26286","08/10/2020 12:40:52","Document M1M3 SS Context, Model,..","Provide more Doxygen docs for Context, Model and related (now singletons). Publish build Doxygen doc on https://ts-m1m3support.lsst.io",3
"DM-26289","08/10/2020 16:23:22","Update lsst-texmf build and Docker image for Python 3.7/GitHub Actions","The lsst-texmf build process hasn't been updated in several years, and it's now worth making several updates to improve the stability and performance of both CI and the Docker image product going forwards:   * Switch to GitHub Actions for better performance   * Target Python 3.7 for both the test environment and the Docker image   * Use a multi-stage docker build and intermediate image caching to create a compact docker image that no longer relies on https://github.com/lsst-sqre/lsst-texlive",5
"DM-26291","08/10/2020 19:46:08","Fix URL redirects on the document series pages (www.lsst.io)","The document series pages on [www.lsst.io|http://www.lsst.io/] are creating infinite redirects based on the search state cached in the URL. The advanced search page had a similar issue and we fixed that in  DM-24955.",1
"DM-26294","08/11/2020 11:44:30","Merge Gsoc student PR about networkpolicies","PR is here: https://github.com/lsst/qserv-operator/pull/13",1
"DM-26304","08/11/2020 22:54:04","Move PexConfigFormatter to obs_base","PexConfigFormatter is anomalous in that it's defined in daf_butler but can't be used by daf_butler, shouldn't be used by daf_butler and can't be tested in daf_butler.    It should move to obs_base where the other lsst-specific formatters exist. It can then be tested properly in obs_base.",1
"DM-26305","08/11/2020 23:30:27","Improve documentation of DataType in salobj topic classes","Try to clarify the documentation for data types in salobj topic classes, and what the callback function receives.",0
"DM-26315","08/13/2020 00:20:06","Support the Update of Sensitivity Matrix","Support the update of sensitivity matrix. Bo updated the sensitivity matrix and bending mode in the following two PRs:   1. [https://github.com/lsst-ts/phosim_syseng4/pull/3]   2. [https://github.com/lsst-ts/ts_ofc/pull/26]    Because this update will break the unit tests in *ts_ofc*, I will support the fix of them. In addition, the scientific pipeline uses the *gcc/g++ 7.3*. The [phosim_syseng4|https://github.com/lsst-ts/phosim_syseng4] has the problem of compilation. This task will try to upgrade it and do the related test.    The update of M2 force file in *ts_phosim* should break the unit tests as well. Need to fix them.",1
"DM-26316","08/13/2020 19:03:34","Allow bias generation to retain overscan signal","ComCam data has patterns in the biases along the x-axis (perpendicular to parallel transfers).  The fitted overscan is therefore not removing all of the signal it should.  Reversing the application of the overscan and bias correction should allow these patterns to be removed by an initial bias pass, allowing the overscan correction to remove the row-by-row signals.",1
"DM-26317","08/13/2020 22:51:58","Add camera caching to obs_decam and obs_subaru Gen3 Instrument","It takes half a second for getCamera to read the HSC camera in HyperSuprimeCam instrument class every time we call it. This adds up really quickly when we are looping over exposures and calling {{loadCamera}}.    Add a simple lru_cache as was done for YAMLCamera in DM-25923",1
"DM-26318","08/14/2020 00:15:05","Fix WebDAV failures on Jenkins","DM-26310 is breaking Jenkins because www.lsst.org is not visible from Jenkins servers. We are using responses package so this shouldn't be an issue but it seems that we are failing to mock the {{requests.options}} call.  ",1
"DM-26320","08/14/2020 01:47:33","Please clarify some parts of the TMA simulator docs","I found some parts of https://ts-tma.lsst.io/v/feature-tickets-DM-25520/virtual_machines.html confusing and would appreciate some clarifications.    The title ""Virtual Machines"" does not make it clear that these are for running simulation code. Perhaps ""Simulators"" or ""Simulators and Virtual Machines"" or...?    The instructions for dealing with IP addresses could be clearer. I did not realize I was supposed to pick a different address than the one my machine already had. Consider something like this for macOS (though you'll have to replace my IP numbers with yours for consistency):    {code}  To select an IP address for the mtmount_default_... virtual machine:  • Find the IP address of your computer (the internal IP address, not the one visible to the rest of the world).  • On macOS:      • Open System Preferences -> Network      • For each network interface that you are using, select it in the list on the left        to view the network address. On my computer Ethernet is 10.0.1.9 and Wi-Fi is 10.0.1.24      • Another option is to run the ifconfig command in Terminal, but there are a lot of them,        so it can be confusing. en0 is probably wired and en1 is probably wifi.  • Pick a different address in the same subnet to use with the remaining instructions. I chose 10.0.1.95.  {code}    I had no idea how to set the IP address of the mtmount_default_... Windows virtual machine. Please consider something like the following (unless you can recommend a better way to get to the properties editor; this seems quite roundabout). Also this could be on a separate page if you expect mostly people familiar with Windows to be reading the document:  {code}  Set the IP address for the Windows Ethernet 2 adapter in the mtmount_default_... virtual machine to the address you chose:  • Push the windows icon at the lower left corner.  • Select ""Settings"" in the pop-up menu.  • Click the ""Network & Internet"" button.  • Select ""Ethernet in the contents bar along the left.  • Click ""Change adapter options"" beneath ""Related settings"".  • Double-click ""Ethernet 2"".  • Click the ""Properties"" button.  • Allow the operation to proceed by clicking ""Yes"".  • Double-click ""Internet Protocol Version 4..."" in the list of items.  • Edit the IP address.  • Push the Apply button.    Test that you have succeeded by pinging the address from your computer.  {code}.    The section about setting up a Host Network Manager for VirtualBox seemed pretty clear, but I'm a bit surprised it didn't appear at the beginning, before importing the Windows VM. I thought it perhaps had to be done for each of the PXI virtual machines individually, but it clearly is a global settings. You might consider clarifying that (or moving it to before importing any of the VMs, which would make it self-evident). Here are the notes I wrote to myself:  {code}  In VirtualBox's Manager window (Oracle VM VirtualBox Manager).  Note: you do not have to have a virtual machine selected; this setting is global to VirtualBox.  • Select File>Host Network Manager  • Select vboxnet0  • Make sure ""Adapter"" is selected.  • Select ""Configure Adapter Manually""  • Enter your computer's address in ""IPv4 Address""  • Click Apply  {code}    I also found that I wasn't sure about importing any of the virtual machines. It turned out I could just use the defaults, but I was not at all clear on that. It would help to make it clear whether or not the user is expected to change anything during the import (and under what circumstances). In my case I found that all the instructions were for changing things after import.    I realize all this detail just adds to an already long document. I'm not sure what to say except for one not familiar with either Windows or VirtualBox it can be pretty painful trying to figure out what's going on. And if VirtualBox evolves I suppose too much detail may end up being a problem.",3
"DM-26327","08/14/2020 20:27:27","Add support for x-flipped WCS in gen3 formatters","I might be missing something but it looks like the DECam WCS uses the flipX parameter in gen2 for raw data but it doesn't in gen3. I'm not entirely sure how that can be true.    Regardless LATISS will need flipping (DM-24592) so I need to add a class property to raw formatter base to indicate whether flipX should be applied. Should be quick.",1
"DM-26335","08/17/2020 16:56:48","Test RPI Centos7 container with hardware","Build a 64bit CentOS 7 (aarch) container and test it with hardware setup  (temp sensors and POE hat)",2
"DM-26343","08/17/2020 18:08:38","Fix extension usage in ButlerURI and Butler Ingest","In DM-26138 we ended up with a filter name that includes {{1.4}}.  This breaks butler ingest because gen2 puts the 1.4 in the file names and ButlerURI assumes that 4 is part of a file extension.  Fix ButlerURI.getExtension so that it only returns a single extension unless there is a special case of .gz, .bz2, .xz, and .fz (in which case it returns two).  This should fix ingest for filters with a dot in them and allow us to use the filter name we want for imsim.",1
"DM-26363","08/17/2020 23:25:11","Test the Jenkins with LabVIEW","Test the Jenkins with LabVIEW on my desktop to see it can do the unit test, generate the Junit xml reports, and publish or not. I think it might be easier to use the virtual box to do this test (install the Jenkins in the virtual box).",3
"DM-26365","08/18/2020 00:31:21","Linkage analysis tool abstract for 2020 DPS meeting","Submit an abstract for a poster covering the linkage analysis tool as a possible QA tool for small body linking algorithms. ",1
"DM-26379","08/19/2020 00:14:22","Missing Control in SAL LabVIEW VI and IDL Information","The SAL LabVIEW VI failed because the topic is allowed to have no attributes. Take the [^sal_Test.idl] as an example. The fault command is:    -----------------------------------------------------------------------    struct command_fault \{   };   #pragma keylist command_fault TestID    -----------------------------------------------------------------------    This will fail the *ts_SALLabVIEW* for the empty string. If the *ts_salobj* does not need this, it is better to remove this from the IDL file. In addition, this will break the SAL LabVIEW API like the following:    !failedAPI.png|width=400,height=274!    The API itself is broken already for the empty structure:   !failedAPI_mag.png|width=346,height=136!",1
"DM-26391","08/19/2020 19:33:08","Assess status of Conda job completion","Determine which CSCs need Conda Jenkins jobs, which currently have them and how many are left to work on.  Then coordinate finishing any remaining work.",2
"DM-26398","08/20/2020 09:14:36","SourceDetectionTask alters input exposure image values in place","In connection to the test run of DM-26182, the output exposure written to storage does not match with the result of {{ZogyTask}}. It was narrowed down to {{lsst.meas.algorithms.SourceDetectionTask}} that alters the input exposure in place.       {code:java}imageDifference INFO: Running diaSource detection  imageDifference INFO: subtractedExposure before detection zero: 5106037  imageDifference.detection INFO: Detected 135 positive peaks in 39 footprints and 224 negative peaks in 92 footprints to 5.5 sigma  imageDifference INFO: subtractedExposure after detection zero: 0{code}     {code:java}self.log.info(""subtractedExposure before detection zero: ""   f""{numpy.sum(subtractedExposure.image.array == 0)}"")     results = self.detection.run(   table=table,   exposure=subtractedExposure,   doSmooth=not self.config.doPreConvolve   )     self.log.info(""subtractedExposure after detection zero: ""   f""{numpy.sum(subtractedExposure.image.array == 0)}""){code}       In this case, {{doSmooth = False}}  i.e. the input exposure is a likelihood image. The variance plane seems to be unaffected at least the constant value block that is the result of ZogyTask is preserved.",2
"DM-26399","08/20/2020 17:07:43","Support dome coditioning contract","Support activities to develop a dome environment conditioning contract, to control the temperature during daytime. The activities include reviewing proposals, provide technical information to potential contractors, meetings with Rubin Observatory reviewers and contractors.",3
"DM-26400","08/20/2020 17:18:28","Dome Control Software Development Support","Support activities for the development of low level dome control software development by EIE contractor. The activities include  organization and attendance to technical meetings, review of documentacion, enforce compliance to requirements of software implementations, plan timeline of deliverables, provide technical support and guidance during development.",5
"DM-26414","08/21/2020 15:57:52","Handle masked pixels in ip_isr's MEDIAN_PER_ROW","If there are masked pixels in the overscan, the \{\{MEDIAN_PER_ROW}} overscan correction code generates a warning:    {code}  python/lsst/ip/isr/overscan.py:344: UserWarning: Warning: converting a masked element to nan.   rowMedian = afwMath.makeStatistics(row, fitType, self.statControl).getValue()  {code}    Please remove this.  I think the simplest fix is \{\{row = row[~row.mask].data[0]}} in collapseArrayMedian, but there may be other occurrences.  A better solution might be to stop using {{np.ma}} in favour of using stack-native code everywhere, which understands {{Mask}} planes.     ",1
"DM-26419","08/21/2020 18:22:17","Update CI repositories for latest Butler","DM-26331 added a change to the Gen 3 registry schema that is likely to break {{ap_verify}} datasets despite our use of repository cloning. Update {{ap_verify_ci_hits2015}} and {{ap_verify_ci_cosmos_pdr2}} to conform to the new schema.",1
"DM-26421","08/21/2020 18:33:47","Add instructions for using an access token outside of nublado","Logging in to nublado provides an access token to the user that can be used to access services in the API aspect.  This is to provide an extension to nb.lsst.io to provide instructions on how to do that in a specific example case.  I'd like to use access of the TAP service via the TOPCAT tool as my exemplar.",3
"DM-26423","08/21/2020 19:34:00","Make SAL/kafka producers shut down gracefully for SIGTERM","Make the SAL/Kafka producers shut down gracefully for SIGTERM (and SIGINT).",1
"DM-26426","08/21/2020 21:30:28","Upgrade Qserv image inside Qserv-operator","use image `deps_20200729_0234` whose git SHA is `3130bd0`",2
"DM-26429","08/21/2020 22:16:01","Improve DateSystem enum docs to clarify EPOCH","I was not able to find documentation about the EPOCH value of {{daf.base.DateTime.DateSystem}}. From the C++ code, it looks like it means ""Julian Epoch Year"", like the [astropy time format|https://docs.astropy.org/en/stable/api/astropy.time.TimeJulianEpoch.html#astropy.time.TimeJulianEpoch], but ""epoch"" is also used elsewhere in the same code to refer to ""nanoseconds since the unix epoch"", which is quite a different thing.    We need a short docstring on that enum (or elsewhere, if enum docstrings don't work) to make the values explicitly clear.    Also, the private `_getEpoch` could be similarly updated to specify that it is the Julian Epoch Year.",0.5
"DM-26445","08/24/2020 17:29:26","w34 ingest_raws fails with ci_hsc_gen3 and PostgreSQL with Timespan error","scons --butler-config=/scratch/mgower/weekly_ci_hsc_gen3/postgres/output/1598279875/butler-seed.yaml -j 8 --enable-profile    With butler-seed.yaml looking like   {noformat}  registry:     db: postgresql://lsst-pg-devel1.ncsa.illinois.edu:5432/lsstdevdb1     namespace: svclsstdbmgower1  datastore:    composites:      default: true  {noformat}    Leads to:  {noformat}  python -m cProfile -o profile-004-butler.pstats /software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-145-g6ed37199+01e3669f2c/bin/butler ingest-raws /scratch/mgower/weekly_ci_hsc_gen3/postgres/git/ci_hsc_gen3/DATA -d /project/shared/data/test_data/testdata_ci_hsc/raw  Error: An error occurred during command execution:  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-145-g6ed37199+01e3669f2c/python/lsst/daf/butler/cli/utils.py"", line 444, in cli_handle_exception      return func(*args, **kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/20.0.0-36-g2de6156+58b4951e8a/python/lsst/obs/base/script/ingestRaws.py"", line 75, in ingestRaws      ingester.run(files, run=output_run)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/20.0.0-36-g2de6156+58b4951e8a/python/lsst/obs/base/ingest.py"", line 456, in run      self.butler.registry.syncDimensionData(""exposure"", exposure.record)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-145-g6ed37199+01e3669f2c/python/lsst/daf/butler/registry/_registry.py"", line 953, in syncDimensionData      return storage.sync(record)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-145-g6ed37199+01e3669f2c/python/lsst/daf/butler/registry/dimensions/table.py"", line 157, in sync      compared=compared,    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-145-g6ed37199+01e3669f2c/python/lsst/daf/butler/registry/interfaces/_database.py"", line 1097, in sync      f""Conflict ({bad}) in sync after successful insert; this is ""  RuntimeError: Conflict ([""timespan: Timespan(begin=astropy.time.Time('2013-06-17 13:34:45.775000', scale='tai', format='iso'), end=astropy.time.Time('2013-06-17 13:35:17.947000', scale='tai', format='iso')) != Timespan(begin=astropy.time.Time('2013-06-17T13:34:10.775', scale='utc', format='isot'), end=astropy.time.Time('2013-06-17T13:34:42.947', scale='utc', format='isot'))""]) in sync after successful insert; this is possible if the same table is being updated by a concurrent process that isn't using sync, but it may also be a bug in daf_butler.  {noformat}",1
"DM-26447","08/24/2020 19:18:23","ScriptQueueCommander: support setting logLevel, etc. in the add command","Enhance the ScriptQueue commander to allow the user to specify logLevel and other options for the add command. Preferably make these optional arguments that use keywords, e.g. logLevel=40.",1
"DM-26452","08/24/2020 22:48:09","Fix fringe filter inconsistency","The tests here:  [https://github.com/lsst/ip_isr/blob/master/python/lsst/ip/isr/isrTask.py#L1300]  and here:  [https://github.com/lsst/ip_isr/blob/master/python/lsst/ip/isr/fringe.py#L251]  should match.",1
"DM-26475","08/26/2020 07:57:42","Update 'suit' package build procedure to new online help system","From [~gpdf] - [~loi] discussion this morning: update the {{suit}} package's build scripts to     # use the new Firefly online help system, building the help from the suit-specific fork [lsst/suit-help|https://github.com/lsst/suit-help] of the core online help text at [Caltech-IPAC/firefly-help|https://github.com/Caltech-IPAC/firefly-help], and   # place the generated pages into the .war file output of the build, and therefore into the resulting Docker image, for service from Tomcat under a sub-endpoint of the {{suit}} / Portal Aspect application.    This feature should be backported to the rc-2.0 branch of {{suit}}.",3
"DM-26480","08/26/2020 19:03:38","Fix ts_sal unit tests for volatile telemetry","A few ts_sal unit tests are now failing, as expected, because telemetry topics are now volatile and so have no historical (late joiner) data. Update the tests on the version 5 ticket branch.",0
"DM-26485","08/27/2020 00:20:45","Add vectorized pure-python interface to convert ra/dec to and from x/y for SkyWcs","Currently, there is no direct way to convert numpy arrays of ra/dec to and from x/y with a {{SkyWcs}}, which leads to inefficiencies when computing {{SpherePoint}} for large numbers of positions.  Some of this would be fixed with DM-19235 which I believe is tricky because of the memory ordering.    As it turns out, AST supports using numpy arrays without any memory finagling, for example in {{suprême}}: https://github.com/LSSTDESC/supreme/blob/75a2008d203e28b3d42738374823f1f5e42d9db1/supreme/utils.py#L157    This ticket will add {{wcs.xyToRaDec}} and {{wcs.raDecToXY}} (or some suitable name) to {{wcsContinued.py}} to allow general access to these very fast ways of interfacing with the AST code.  I don't believe the names {{skyToPixel}} and {{pixelToSky}} can be used because pure-python methods can't do the overloading (plus these don't return or take quite the same type of data structures).",1
"DM-26486","08/27/2020 03:06:24","Qserv Replication worker service crashes after receiving spurious packets","The server crashes if a clients sends an incorrect or a random) request. Here is what's reported in the Replication/Ingest/Export worker's log file:  {code}  ...  2020-08-22T06:14:29.927Z  LWP 528   DEBUG  PROCESSOR  _fetchNextForProcessing  thread: 6  timeout: 1000  2020-08-22T06:14:29.961Z  LWP 512   DEBUG  PROCESSOR  _fetchNextForProcessing  thread: 1  timeout: 1000  2020-08-22T06:14:30.084Z  LWP 542   DEBUG  PROCESSOR  _fetchNextForProcessing  thread: 10  timeout: 1000  2020-08-22T06:14:30.094Z  LWP 508   DEBUG  CONNECTION  _received  terminate called after throwing an instance of 'std::overflow_error'    what():  ProtocolBuffer::_extend  requested capacity 1735489335 exceeds the hard limit of Google Protobuf 64000000  /home/qserv/lsst: line 7:   438 Aborted                 (core dumped) $@  {code}  The problem could be triggered by any random request like:  {code:bash}  curl http://qserv-db34:25001/meta/version -X GET  {code}  Note that the service has a proprietary (non-HTTP) binary protocol based on Protobuf.    A proposed solution is to reinforce the code to catch exception {{std::overflow_error}} and close a connection with the offending client.",1
"DM-26488","08/27/2020 19:41:47","Get the CSC Conda jobs running regularly and as part of the daily workflow","Now that DM-26391 is complete, we have a fairly accurate picture of the state of the CSC Conda jobs.  The next step is to get those jobs running regularly so we get the feedback on the status of the jobs.    We also want the wrap the Conda jobs into the daily workflow.  This will require a lot of coordination and logic.",3
"DM-26489","08/27/2020 20:00:49","Fix references to atdometrajectory in ts_mtdometrajectory","There are a few references to atdometrajectory in the ts_mtdometrajectory package (presumably holdovers from starting the package by copying ts_atdometrajectory). Fix them.",0
"DM-26493","08/27/2020 22:12:32","Support the Shared Enum in LabVIEW","Support the shared enum in LabVIEW. Based on Dave, the bug described in DM-26270 can not be fixed in his side. Therefore, the ts_SALLabVIEW may need to be updated to support this.    To do this, maybe I will need to pass ""const"" and ""enum : xxxx"" in the IDL file.    This ticket will try to fix the *space/tab* and *private* in the IDL file.",3
"DM-26499","08/27/2020 23:20:34","Get conda job running for ts_MTDomeTrajectory","The conda job for ts_MTDomeTrajectory is failing. Looks like missing dependencies. https://tssw-ci.lsst.org/job/MTDomeTrajectory%20Conda%20package/",0
"DM-26502","08/28/2020 00:19:53","Support the LSST FAM in AOS Modules in Phase 1","Support the LSST full-array mode (FAM) in AOS modules in phase 1. The SE team will provide the needed parameters such as the off-axis correction, sensitivity matrix, etc. This task is to update the *ts_wep*, *ts_ofc*, and *ts_phosim* to support the following evaluation of convergence. In the mean time, the SE team will check the calculated wavefront error by *cwfs* with the *ZEMAX* test data. The field points on 21 rafts will be checked and evaluated. After this cwfs test is done, the related test data will be added into *ts_wep*.    Bo verified the cwfs with ZEMAX in the following: [fam.ipynb|https://github.com/bxin/cwfs/blob/master/examples/fam.ipynb].    The sensitivity matrix file is [^senM_189_19_50.yaml]. The field numbers start from lower left, then go up: [^senM_189_200812.pdf].       This task is the phase 1.",3
"DM-26517","08/28/2020 15:25:46","Simplify simulation mode support in BaseCsc","At present supporting simulation modes in CSCs requires overriding 3 methods: BaseCsc.implement_simulation_mode, BaseCsc.add_arguments, and BaseCsc.add_kwargs_from_args. Simplify this as follows:    Add a valid_simulation_modes argument to BaseCsc. Default it to None for backwards compatibility. If not None then do the following:  * Test the {{simulation_mode}} argument in the {{BaseCsc}} constructor.  * Make {{BaseCsc.handle_simulation_mode}} a no-op.  * Add a {{--simulate}} argument to {{BaseCsc.amain}}'s argument parser, if there is more than one valid simulation mode.    This will greatly reduce the amount of extra work needed by CSC authors to add a simulation mode.    Also convert at least one other package to use this new technique, to check the design.",1
"DM-26521","08/28/2020 20:34:12","Requests processor in the Qserv Replication worker crashes after receiving spurious packets","Fix a problem in the replication worker processing requests from the master controller. It was overlooked at [DM-26486].    *TODO*: As a side note, plan (in a separate ticket) for refactoring the worker services to have a common base class which would provide communication services. ",0.5
"DM-26530","08/29/2020 03:02:48","Improved batch mode of ingesting table contributions into Qserv","The current implementation of the Ingest system has a binary tool *{{qserv-replica-file-ingest}}* providing the _batch_ mode for loading a list of contributions (read from input files) into various tables:  {code}  qserv-replica-file-ingest FILE-LIST <contributions> [--auth-key=<key>] ...  {code}  Where, the mandatory parameter *{{<contributions>}}* requires a *{{JSON}}*-formatted text file with an array of file contributions into potentially any tables using multiple transactions. Here is an example of such file:  {code:yaml}  [{""worker-host"":""qserv-db01"",""worker-port"":25002,""transaction-id"":123,""table"":""Object"",""type"":""P"",""path"":""input/chunk_187107_overlap.txt""},   {""worker-host"":""qserv-db01"",""worker-port"":25002,""transaction-id"":123,""table"":""Object"",""type"":""P"",""path"":""input/chunk_187107.txt""},   {""worker-host"":""qserv-db02"",""worker-port"":25002,""transaction-id"":123,""table"":""Object"",""type"":""P"",""path"":""input/chunk_187108_overlap.txt""},   {""worker-host"":""qserv-db02"",""worker-port"":25002,""transaction-id"":123,""table"":""Object"",""type"":""P"",""path"":""input/chunk_187108.txt""},   {""worker-host"":""qserv-db01"",""worker-port"":25002,""transaction-id"":123,""table"":""Object"",""type"":""P"",""path"":""input/chunk_187109_overlap.txt""},   {""worker-host"":""qserv-db01"",""worker-port"":25002,""transaction-id"":123,""table"":""Object"",""type"":""P"",""path"":""input/chunk_187109.txt""},   {""worker-host"":""qserv-db02"",""worker-port"":25002,""transaction-id"":123,""table"":""Object"",""type"":""P"",""path"":""input/chunk_187110_overlap.txt""},   {""worker-host"":""qserv-db02"",""worker-port"":25002,""transaction-id"":123,""table"":""Object"",""type"":""P"",""path"":""input/chunk_187110.txt""}  ]  {code}  Although this method provides a lot of flexibility in mixing contributions into any tables, using different transactions, it creates a few inconveniences for implementing ingest workflows. One of those is cased by encoding transaction identifiers into the file. If, for some reason, a transaction fails ad has to be restarted, the previous version of the table contributions files becomes invalid and it needs to be regenerated to encompass a new transaction. Another problem of mixing transactions in the same file would require aborting all transactions mentioned in the file if any contribution fails to be ingested. And finally, it's impractical in any realistic workflows to mix contributions into different tables in the same file.    The proposed effort is meant to keep the current batch method, and to add another one which addresses the above mentioned issues:  {code}  qserv-replica-file-ingest FILE-LIST-TRANS <transaction-id> <table> <type> <contributions> [--auth-key=<key>] ...  {code}  Where the *{{<contributions>}}* file will have a fewer number of attributes in each element of the {{JSON}} array. For example:  {code:yaml}  [{""worker-host"":""qserv-db01"",""worker-port"":25002,""path"":""input/chunk_187107_overlap.txt""},   {""worker-host"":""qserv-db01"",""worker-port"":25002,""path"":""input/chunk_187107.txt""},   {""worker-host"":""qserv-db02"",""worker-port"":25002,""path"":""input/chunk_187108_overlap.txt""},   {""worker-host"":""qserv-db02"",""worker-port"":25002,""path"":""input/chunk_187108.txt""},   {""worker-host"":""qserv-db01"",""worker-port"":25002,""path"":""input/chunk_187109_overlap.txt""},   {""worker-host"":""qserv-db01"",""worker-port"":25002,""path"":""input/chunk_187109.txt""},   {""worker-host"":""qserv-db02"",""worker-port"":25002,""path"":""input/chunk_187110_overlap.txt""},   {""worker-host"":""qserv-db02"",""worker-port"":25002,""path"":""input/chunk_187110.txt""}  ]  {code}  This file would produce the same yield as the existing one if used like illustrated below:  {code:bash}  qserv-replica-file-ingest FILE-LIST-TRANS 123 Object P <contributions>  {code}    Advantages of the new batch mode:  * the file of contributions is reusable across transactions  * a scope of failures during ingest is limited to a single transactions    (!) Destinations where the tables contributions are meant to be ingested do not change after aborting and restarting transactions.  ",3
"DM-26539","08/31/2020 18:51:24","w_35 ci_hsc_gen pipeline.sh dying due to SSL Error","w_35 of the software stack + ci_hsc_gen3 dies in pipeline.sh with  {noformat}  psycopg2.OperationalError: SSL error: decryption failed or bad record mac  {noformat}    [~madamow] first generated the error and I was able to duplicate it.  While a full run has not yet completed using -j 1, it was run far enough to get through the first few PipelineTasks without having this error.  No error messages from previous steps in scons.  Full output of pipeline.sh:  {noformat}    bin/pipeline.sh 8 /scratch/mgower/weekly_ci_hsc_gen3/postgres/git/ci_hsc_gen3/DATA  ctrl.mpexec.cmdLineFwk INFO: QuantumGraph contains 155 quanta for 12 tasks  ctrl.mpexec.cmdLineFwk INFO: QuantumGraph contains 155 quanta for 12 tasks  Process task-0:  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context      cursor, statement, parameters, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 590, in do_execute      cursor.execute(statement, parameters)  psycopg2.OperationalError: SSL error: decryption failed or bad record mac      The above exception was the direct cause of the following exception:  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap      self.run()    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/multiprocessing/process.py"", line 99, in run      self._target(*self._args, **self._kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-13-g37abb6e+24c9b31241/python/lsst/ctrl/mpexec/singleQuantumExecutor.py"", line 83, in execute      self.initGlobals(quantum, butler)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-13-g37abb6e+24c9b31241/python/lsst/ctrl/mpexec/singleQuantumExecutor.py"", line 273, in initGlobals      Instrument.fromName(instrument, butler.registry)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/20.0.0-37-g38a3e24+ce95e46117/python/lsst/obs/base/_instrument.py"", line 219, in fromName      records = list(registry.queryDimensionRecords(""instrument"", instrument=name))    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/_registry.py"", line 1402, in queryDimensionRecords      where=where, components=components, **kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/_registry.py"", line 1324, in queryDataIds      standardizedDataId = self.expandDataId(dataId, **kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/_registry.py"", line 851, in expandDataId      fetched = tuple(storage.fetch(dataIdSet))    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/dimensions/caching.py"", line 111, in fetch      for record in self._nested.fetch(toFetch):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/dimensions/table.py"", line 127, in fetch      for row in self._db.query(query.combine()):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/interfaces/_database.py"", line 1373, in query      return self._connection.execute(sql, *args, **kwds)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 984, in execute      return meth(self, multiparams, params)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/sql/elements.py"", line 293, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1103, in _execute_clauseelement      distilled_params,    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1288, in _execute_context      e, statement, parameters, cursor, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1482, in _handle_dbapi_exception      sqlalchemy_exception, with_traceback=exc_info[2], from_=e    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_      raise exception    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context      cursor, statement, parameters, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 590, in do_execute      cursor.execute(statement, parameters)  sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) SSL error: decryption failed or bad record mac    [SQL: SELECT svclsstdbmgower1.instrument.name, svclsstdbmgower1.instrument.visit_max, svclsstdbmgower1.instrument.exposure_max, svclsstdbmgower1.instrument.detector_max, svclsstdbmgower1.instrument.class_name  FROM svclsstdbmgower1.instrument  WHERE svclsstdbmgower1.instrument.name = %(name_1)s]  [parameters: {'name_1': 'HSC'}]  (Background on this error at: http://sqlalche.me/e/e3q8)  Process task-1:  Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context      cursor, statement, parameters, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 590, in do_execute      cursor.execute(statement, parameters)  psycopg2.OperationalError: SSL error: decryption failed or bad record mac      The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap      self.run()    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/multiprocessing/process.py"", line 99, in run      self._target(*self._args, **self._kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-13-g37abb6e+24c9b31241/python/lsst/ctrl/mpexec/singleQuantumExecutor.py"", line 83, in execute      self.initGlobals(quantum, butler)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/ctrl_mpexec/20.0.0-13-g37abb6e+24c9b31241/python/lsst/ctrl/mpexec/singleQuantumExecutor.py"", line 273, in initGlobals      Instrument.fromName(instrument, butler.registry)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/obs_base/20.0.0-37-g38a3e24+ce95e46117/python/lsst/obs/base/_instrument.py"", line 219, in fromName      records = list(registry.queryDimensionRecords(""instrument"", instrument=name))    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/_registry.py"", line 1402, in queryDimensionRecords      where=where, components=components, **kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/_registry.py"", line 1324, in queryDataIds      standardizedDataId = self.expandDataId(dataId, **kwargs)    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/_registry.py"", line 851, in expandDataId      fetched = tuple(storage.fetch(dataIdSet))    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/dimensions/caching.py"", line 111, in fetch      for record in self._nested.fetch(toFetch):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/dimensions/table.py"", line 127, in fetch      for row in self._db.query(query.combine()):    File ""/software/lsstsw/stack_20200515/stack/miniconda3-4.7.12-46b24e8/Linux64/daf_butler/19.0.0-148-g38150800+de92faa26f/python/lsst/daf/butler/registry/interfaces/_database.py"", line 1373, in query      return self._connection.execute(sql, *args, **kwds)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 984, in execute      return meth(self, multiparams, params)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/sql/elements.py"", line 293, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1103, in _execute_clauseelement      distilled_params,    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1288, in _execute_context      e, statement, parameters, cursor, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1482, in _handle_dbapi_exception      sqlalchemy_exception, with_traceback=exc_info[2], from_=e    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_      raise exception    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context      cursor, statement, parameters, context    File ""/software/lsstsw/stack_20200515/python/miniconda3-4.7.12/envs/lsst-scipipe/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 590, in do_execute      cursor.execute(statement, parameters)  sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) SSL error: decryption failed or bad record mac  {noformat}  The error seems to repeat multiple times and then we get lines like:   {noformat}  trl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(MakeWarpTask, label=makeWarpTask) dataId={abstract_filter: r, instrument: HSC, skymap: discrete/ci_hsc, physical_filter: HSC-R, tract: 0, visit_system: 0, patch: 69, visit: 903342}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(MakeWarpTask, label=makeWarpTask) dataId={abstract_filter: i, instrument: HSC, skymap: discrete/ci_hsc, physical_filter: HSC-I, tract: 0, visit_system: 0, patch: 69, visit: 903986}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(MakeWarpTask, label=makeWarpTask) dataId={abstract_filter: r, instrument: HSC, skymap: discrete/ci_hsc, physical_filter: HSC-R, tract: 0, visit_system: 0, patch: 69, visit: 903344}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(CompareWarpAssembleCoaddTask, label=assembleCoadd) dataId={abstract_filter: r, skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(CompareWarpAssembleCoaddTask, label=assembleCoadd) dataId={abstract_filter: i, skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(DetectCoaddSourcesTask, label=detection) dataId={abstract_filter: r, skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(DetectCoaddSourcesTask, label=detection) dataId={abstract_filter: i, skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(MergeDetectionsTask, label=mergeDetections) dataId={skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(DeblendCoaddSourcesSingleTask, label=deblend) dataId={abstract_filter: r, skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(DeblendCoaddSourcesSingleTask, label=deblend) dataId={abstract_filter: i, skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(MeasureMergedCoaddSourcesTask, label=measure) dataId={abstract_filter: r, skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(MeasureMergedCoaddSourcesTask, label=measure) dataId={abstract_filter: i, skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  ctrl.mpexec.mpGraphExecutor ERROR: Upstream job failed for task <TaskDef(MergeMeasurementsTask, label=mergeMeasurements) dataId={skymap: discrete/ci_hsc, tract: 0, patch: 69}>, skipping this task.  {noformat}  ",1
"DM-26540","08/31/2020 19:06:48","NCSA Test stand DDS debug 2","Continue testing DDS deployments with the NCSA test stand.   Explore the impact of seperating subsystems into dds ""partitions""",5
"DM-26546","08/31/2020 22:08:55","Change --simulate to true/false if there are only two valid states","Change the --simulate argument in BaseCsc.amain to be true/false if there are only two valid simulation modes.",0
"DM-26550","08/31/2020 23:26:52","Add observation_reason to ObservationInfo","In DM-26476 we discuss a need for an observation reason to be added to gen3 registry. This requires that we add the concept to ObservationInfo. This will be a string that can have any value.    Provide a default implementation that returns a reason of ""science"" for observation_type of science, and ""calibration"" for all other cases. Then provide obs_lsst implementation that looks for TESTTYPE header and overrides it with that value. We do not yet know the name of the header that will be used for LSSTCam and ComCam.    Adding reason suggests that using ""science"" for observation_type might be too generic since that was previously trying to serve both roles. Code already relies on ""science"" for observation type though so it will be difficult to switch those to using reason instead.",2
