"issuekey","title","description","storypoint"
"MESOS-934","'Logging and Debugging' document is out-of-date.","The following is no longer correct: http://mesos.apache.org/documentation/latest/logging-and-debugging/  We should either delete this document or re-write it entirely.",1
"MESOS-998","Slave should wait until Containerizer::update() completes successfully","Container resources are updated in several places in the slave and we don't check the update was successful or even wait until it completes.",5
"MESOS-1081","Master should not deactivate authenticated framework/slave on new AuthenticateMessage unless new authentication succeeds.","Master should not deactivate an authenticated framework/slave upon receiving a new AuthenticateMessage unless new authentication succeeds. As it stands now, a malicious user could spoof the pid of an authenticated framework/slave and send an AuthenticateMessage to knock a valid framework/slave off the authenticated list, forcing the valid framework/slave to re-authenticate and re-register. This could be used in a DoS attack. But how should we handle the scenario when the actual authenticated framework/slave sends an AuthenticateMessage that fails authentication?",1
"MESOS-1119","Allocator should make an allocation decision per slave instead of per framework/role.","Currently the Allocator::allocate() code loops through roles and frameworks (based on DRF sort) and allocates *all* slaves resources to the first framework.  This logic should be a bit inversed. Instead, the slave should go through each slave, allocate it a role/framework and update the DRF shares.",2
"MESOS-1143","Add a TASK_ERROR task status.","During task validation we drop tasks that have errors and send TASK_LOST status updates. In most circumstances a framework will want to relaunch a task that has gone lost, and in the event the task is actually malformed (thus invalid) this will result in an infinite loop of sending a task and having it go lost.",2
"MESOS-1148","Add support for rate limiting slave removal","To safeguard against unforeseen bugs leading to widespread slave removal, it would be nice to allow for rate limiting of the decision to remove slaves and/or send TASK_LOST messages for tasks on those slaves.  Ideally this would allow an operator to be notified soon enough to intervene before causing cluster impact.",3
"MESOS-1236","stout's os module uses a mix of Try<Nothing> and bool returns","stout's os module should use Try<Nothing> for return values throughout.",2
"MESOS-1237","stout's os::ls should return a Try<>","stout's os::ls returns a list that can be empty - instead it should return a Try<list...> to be consistent.",2
"MESOS-1303","ExamplesTest.{TestFramework, NoExecutorFramework} flaky","I'm having trouble reproducing this but I did observe it once on my OSX system:  {noformat} [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from ExamplesTest [ RUN      ] ExamplesTest.TestFramework ../../src/tests/script.cpp:81: Failure Failed test_framework_test.sh terminated with signal 'Abort trap: 6' [  FAILED  ] ExamplesTest.TestFramework (953 ms) [ RUN      ] ExamplesTest.NoExecutorFramework [       OK ] ExamplesTest.NoExecutorFramework (10162 ms) [----------] 2 tests from ExamplesTest (11115 ms total)  [----------] Global test environment tear-down [==========] 2 tests from 1 test case ran. (11121 ms total) [  PASSED  ] 1 test. [  FAILED  ] 1 test, listed below: [  FAILED  ] ExamplesTest.TestFramework {noformat}  when investigating a failed make check for https://reviews.apache.org/r/20971/ {noformat} [----------] 6 tests from ExamplesTest [ RUN      ] ExamplesTest.TestFramework [       OK ] ExamplesTest.TestFramework (8643 ms) [ RUN      ] ExamplesTest.NoExecutorFramework tests/script.cpp:81: Failure Failed no_executor_framework_test.sh terminated with signal 'Aborted' [  FAILED  ] ExamplesTest.NoExecutorFramework (7220 ms) [ RUN      ] ExamplesTest.JavaFramework [       OK ] ExamplesTest.JavaFramework (11181 ms) [ RUN      ] ExamplesTest.JavaException [       OK ] ExamplesTest.JavaException (5624 ms) [ RUN      ] ExamplesTest.JavaLog [       OK ] ExamplesTest.JavaLog (6472 ms) [ RUN      ] ExamplesTest.PythonFramework [       OK ] ExamplesTest.PythonFramework (14467 ms) [----------] 6 tests from ExamplesTest (53607 ms total) {noformat}",1
"MESOS-1316","Implement decent unit test coverage for the mesos-fetcher tool","There are current no tests that cover the {{mesos-fetcher}} tool itself, and hence bugs like MESOS-1313 have accidentally slipped though.",2
"MESOS-1332","Improve Master and Slave metric names","As we move the metrics to a new endpoint, we should consider revisiting the names of some of the current metrics to make them clearer.  It may also be worth considering changing some existing counter-style metrics to gauges. ",3
"MESOS-1358","Show when the leading master was elected in the webui","This would be nice to have during debugging.",1
"MESOS-1365","SlaveRecoveryTest/0.MultipleFrameworks is flaky","--gtest_repeat=-1 --gtest_shuffle --gtest_break_on_failure  {noformat} [ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks WARNING: Logging before InitGoogleLogging() is written to STDERR I0513 15:42:05.931761  4320 exec.cpp:131] Version: 0.19.0 I0513 15:42:05.936698  4340 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0 Registered executor on artoo Starting task 51991f97-f5fd-4905-ad0f-02668083af7c Forked command at 4367 sh -c 'sleep 1000' WARNING: Logging before InitGoogleLogging() is written to STDERR I0513 15:42:06.915061  4408 exec.cpp:131] Version: 0.19.0 I0513 15:42:06.931149  4435 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0 Registered executor on artoo Starting task eaf5d8d6-3a6c-4ee1-84c1-fae20fb1df83 sh -c 'sleep 1000' Forked command at 4439 I0513 15:42:06.998332  4340 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0 I0513 15:42:06.998414  4436 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0 I0513 15:42:07.006350  4437 exec.cpp:228] Executor re-registered on slave 20140513-154204-16842879-51872-13062-0 Re-registered executor on artoo I0513 15:42:07.027039  4337 exec.cpp:378] Executor asked to shutdown Shutting down Sending SIGTERM to process tree at pid 4367 Killing the following process trees: [  -+- 4367 sh -c sleep 1000   \--- 4368 sleep 1000  ] ../../src/tests/slave_recovery_tests.cpp:2807: Failure Value of: status1.get().state()   Actual: TASK_FAILED Expected: TASK_KILLED  Program received signal SIGSEGV, Segmentation fault. testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795 3795          *static_cast<volatile int*>(NULL) = 1; (gdb) bt #0  testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795 #1  0x0000000000df98b9 in testing::internal::AssertHelper::operator= (this=0x7fffffffb860, message=...) at gmock-1.6.0/gtest/src/gtest.cc:356 #2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807 #3  0x0000000000e22583 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2090 #4  0x0000000000e12467 in testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2126 #5  0x0000000000e010d5 in testing::Test::Run (this=0x1954db0) at gmock-1.6.0/gtest/src/gtest.cc:2161 #6  0x0000000000e01ceb in testing::TestInfo::Run (this=0x158cf80) at gmock-1.6.0/gtest/src/gtest.cc:2338 #7  0x0000000000e02387 in testing::TestCase::Run (this=0x158a880) at gmock-1.6.0/gtest/src/gtest.cc:2445 #8  0x0000000000e079ed in testing::internal::UnitTestImpl::RunAllTests (this=0x1558b40) at gmock-1.6.0/gtest/src/gtest.cc:4237 #9  0x0000000000e1ec83 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>,      location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2090 #10 0x0000000000e14217 in testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>,      location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2126 #11 0x0000000000e076d7 in testing::UnitTest::Run (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>) at gmock-1.6.0/gtest/src/gtest.cc:3872 #12 0x0000000000b99887 in main (argc=1, argv=0x7fffffffd9f8) at ../../src/tests/main.cpp:107 (gdb) frame 2 #2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807 2807      ASSERT_EQ(TASK_KILLED, status1.get().state()); (gdb) p status1 $1 = {data = {<std::__shared_ptr<process::Future<mesos::TaskStatus>::Data, 2>> = {_M_ptr = 0x1963140, _M_refcount = {_M_pi = 0x198a620}}, <No data fields>}} (gdb) p status1.get() $2 = (const mesos::TaskStatus &) @0x7fffdc5bf5f0: {<google::protobuf::Message> = {<google::protobuf::MessageLite> = {_vptr$MessageLite = 0x7ffff74bc940 <vtable for mesos::TaskStatus+16>}, <No data fields>}, static kTaskIdFieldNumber = 1, static kStateFieldNumber = 2, static kMessageFieldNumber = 4,    static kDataFieldNumber = 3, static kSlaveIdFieldNumber = 5, static kTimestampFieldNumber = 6, _unknown_fields_ = {fields_ = 0x0}, task_id_ = 0x7fffdc5ce9a0, message_ = 0x7fffdc5f5880, data_ = 0x154b4b0 <google::protobuf::internal::kEmptyString>, slave_id_ = 0x7fffdc59c4f0, timestamp_ = 1429688582.046252,    state_ = 3, _cached_size_ = 0, _has_bits_ = {55}, static default_instance_ = 0x0} (gdb) p status1.get().state() $3 = mesos::TASK_FAILED (gdb) list 2802      // Kill task 1. 2803      driver1.killTask(task1.task_id()); 2804 2805      // Wait for TASK_KILLED update. 2806      AWAIT_READY(status1); 2807      ASSERT_EQ(TASK_KILLED, status1.get().state()); 2808 2809      // Kill task 2. 2810      driver2.killTask(task2.task_id()); 2811 {noformat}",1
"MESOS-1371","Expose libprocess queue length from scheduler driver to metrics endpoint","We expose the master's event queue length and we should do the same for the scheduler driver.",1
"MESOS-1424","Mesos tests should not rely on echo","Triggered by MESOS-1413 I would like to propose changing our tests to not rely on {{echo}} but to use {{printf}} instead.  This seems to be useful as {{echo}} is introducing an extra linefeed after the supplied string whereas {{printf}} does not. The {{-n}} switch preventing that extra linefeed is unfortunately not portable - it is not supported by the builtin {{echo}} of the BSD / OSX {{/bin/sh}}. ",1
"MESOS-1459","Build failure: Ubuntu 13.10/clang due to missing virtual destructor","In file included from launcher/main.cpp:19: In file included from ./launcher/launcher.hpp:24: In file included from ../3rdparty/libprocess/include/process/future.hpp:23: ../3rdparty/libprocess/include/process/owned.hpp:188:5: error: delete called on 'mesos::internal::launcher::Operation' that is abstract but has non-virtual destructor [-Werror,-Wdelete-non-virtual-dtor]     delete t;     ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:456:8: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Data::~Data' requested here               delete __p;               ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:768:24: note: in instantiation of function template specialization 'std::__shared_count<2>::__shared_count<process::Owned<mesos::internal::launcher::Operation>::Data *>' requested here         : _M_ptr(__p), _M_refcount(__p)                        ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:919:4: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here           __shared_ptr(__p).swap(*this);           ^ ../3rdparty/libprocess/include/process/owned.hpp:68:10: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::reset<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here     data.reset(new Data(t));          ^ ./launcher/launcher.hpp:101:7: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Owned' requested here   add(process::Owned<Operation>(new T()));       ^ launcher/main.cpp:26:3: note: in instantiation of function template specialization 'mesos::internal::launcher::add<mesos::internal::launcher::ShellOperation>' requested here   launcher::add<launcher::ShellOperation>();   ^ 1 error generated.",1
"MESOS-1469","No output from review bot on timeout","When the mesos review build times out, likely due to a long-running failing test, we have no output to debug. We should find a way to stream the output from the build instead of waiting for the build to finish.",2
"MESOS-1472","Improve child exit if slave dies during executor launch in MC","When restarting many slaves there's a reasonable chance that a slave will be restarted between the fork and exec stages of launching an executor in the MesosContainerizer.  The forked child correctly detects this however rather than abort it should safely log and then exit non-zero cleanly.",1
"MESOS-1545","SlaveRecoveryTest/0.MultipleFrameworks is flaky","{code} [ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks Using temporary directory '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_6dJqxr' I0626 00:04:39.557339  5450 leveldb.cpp:176] Opened db in 179.857593ms I0626 00:04:39.565433  5450 leveldb.cpp:183] Compacted db in 8.071041ms I0626 00:04:39.565457  5450 leveldb.cpp:198] Created db iterator in 4065ns I0626 00:04:39.565466  5450 leveldb.cpp:204] Seeked to beginning of db in 596ns I0626 00:04:39.565474  5450 leveldb.cpp:273] Iterated through 0 keys in the db in 396ns I0626 00:04:39.565490  5450 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0626 00:04:39.565827  5476 recover.cpp:425] Starting replica recovery I0626 00:04:39.566033  5474 recover.cpp:451] Replica is in EMPTY status I0626 00:04:39.566504  5474 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I0626 00:04:39.566686  5477 recover.cpp:188] Received a recover response from a replica in EMPTY status I0626 00:04:39.566905  5472 recover.cpp:542] Updating replica status to STARTING I0626 00:04:39.568307  5471 master.cpp:288] Master 20140626-000439-1032504131-55423-5450 (juno.apache.org) started on 67.195.138.61:55423 I0626 00:04:39.568332  5471 master.cpp:325] Master only allowing authenticated frameworks to register I0626 00:04:39.568339  5471 master.cpp:330] Master only allowing authenticated slaves to register I0626 00:04:39.568348  5471 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_6dJqxr/credentials' I0626 00:04:39.568461  5471 master.cpp:356] Authorization enabled I0626 00:04:39.568739  5478 master.cpp:122] No whitelist given. Advertising offers for all slaves I0626 00:04:39.568814  5475 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@67.195.138.61:55423 I0626 00:04:39.569206  5478 master.cpp:1122] The newly elected leader is master@67.195.138.61:55423 with id 20140626-000439-1032504131-55423-5450 I0626 00:04:39.569223  5478 master.cpp:1135] Elected as the leading master! I0626 00:04:39.569231  5478 master.cpp:953] Recovering from registrar I0626 00:04:39.569286  5475 registrar.cpp:313] Recovering registrar I0626 00:04:39.600639  5477 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 33.682136ms I0626 00:04:39.600661  5477 replica.cpp:320] Persisted replica status to STARTING I0626 00:04:39.600790  5476 recover.cpp:451] Replica is in STARTING status I0626 00:04:39.601184  5474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I0626 00:04:39.601274  5477 recover.cpp:188] Received a recover response from a replica in STARTING status I0626 00:04:39.601465  5471 recover.cpp:542] Updating replica status to VOTING I0626 00:04:39.610605  5471 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 9.076262ms I0626 00:04:39.610638  5471 replica.cpp:320] Persisted replica status to VOTING I0626 00:04:39.610683  5471 recover.cpp:556] Successfully joined the Paxos group I0626 00:04:39.610780  5471 recover.cpp:440] Recover process terminated I0626 00:04:39.610946  5474 log.cpp:656] Attempting to start the writer I0626 00:04:39.611486  5475 replica.cpp:474] Replica received implicit promise request with proposal 1 I0626 00:04:39.618924  5475 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 7.418789ms I0626 00:04:39.618942  5475 replica.cpp:342] Persisted promised to 1 I0626 00:04:39.619220  5476 coordinator.cpp:230] Coordinator attemping to fill missing position I0626 00:04:39.619763  5476 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I0626 00:04:39.627267  5476 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 7.485492ms I0626 00:04:39.627295  5476 replica.cpp:676] Persisted action at 0 I0626 00:04:39.627822  5473 replica.cpp:508] Replica received write request for position 0 I0626 00:04:39.627861  5473 leveldb.cpp:438] Reading position from leveldb took 17132ns I0626 00:04:39.635592  5473 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 7.714322ms I0626 00:04:39.635612  5473 replica.cpp:676] Persisted action at 0 I0626 00:04:39.635797  5473 replica.cpp:655] Replica received learned notice for position 0 I0626 00:04:39.643941  5473 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 8.129347ms I0626 00:04:39.643960  5473 replica.cpp:676] Persisted action at 0 I0626 00:04:39.643970  5473 replica.cpp:661] Replica learned NOP action at position 0 I0626 00:04:39.644207  5473 log.cpp:672] Writer started with ending position 0 I0626 00:04:39.644625  5471 leveldb.cpp:438] Reading position from leveldb took 9128ns I0626 00:04:39.646010  5476 registrar.cpp:346] Successfully fetched the registry (0B) I0626 00:04:39.646044  5476 registrar.cpp:422] Attempting to update the 'registry' I0626 00:04:39.647274  5471 log.cpp:680] Attempting to append 136 bytes to the log I0626 00:04:39.647337  5471 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0626 00:04:39.647687  5476 replica.cpp:508] Replica received write request for position 1 I0626 00:04:39.655206  5476 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 7.499736ms I0626 00:04:39.655225  5476 replica.cpp:676] Persisted action at 1 I0626 00:04:39.655467  5476 replica.cpp:655] Replica received learned notice for position 1 I0626 00:04:39.663534  5476 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 8.054929ms I0626 00:04:39.663554  5476 replica.cpp:676] Persisted action at 1 I0626 00:04:39.663563  5476 replica.cpp:661] Replica learned APPEND action at position 1 I0626 00:04:39.663890  5478 registrar.cpp:479] Successfully updated 'registry' I0626 00:04:39.663947  5478 registrar.cpp:372] Successfully recovered registrar I0626 00:04:39.663969  5476 log.cpp:699] Attempting to truncate the log to 1 I0626 00:04:39.664044  5478 master.cpp:980] Recovered 0 slaves from the Registry (98B) ; allowing 10mins for slaves to re-register I0626 00:04:39.664057  5476 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0626 00:04:39.664341  5476 replica.cpp:508] Replica received write request for position 2 I0626 00:04:39.664681  5450 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem I0626 00:04:39.666721  5471 slave.cpp:168] Slave started on 173)@67.195.138.61:55423 I0626 00:04:39.666741  5471 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/credential' I0626 00:04:39.666806  5471 slave.cpp:268] Slave using credential for: test-principal I0626 00:04:39.666936  5471 slave.cpp:281] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0626 00:04:39.667000  5471 slave.cpp:326] Slave hostname: juno.apache.org I0626 00:04:39.667009  5471 slave.cpp:327] Slave checkpoint: true I0626 00:04:39.667572  5478 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta' I0626 00:04:39.667703  5475 status_update_manager.cpp:193] Recovering status update manager I0626 00:04:39.667840  5475 containerizer.cpp:287] Recovering containerizer I0626 00:04:39.668478  5471 slave.cpp:3128] Finished recovery I0626 00:04:39.668712  5471 slave.cpp:601] New master detected at master@67.195.138.61:55423 I0626 00:04:39.668738  5471 slave.cpp:677] Authenticating with master master@67.195.138.61:55423 I0626 00:04:39.668802  5471 slave.cpp:650] Detecting new master I0626 00:04:39.668861  5471 status_update_manager.cpp:167] New master detected at master@67.195.138.61:55423 I0626 00:04:39.668916  5471 authenticatee.hpp:128] Creating new client SASL connection I0626 00:04:39.669087  5471 master.cpp:3499] Authenticating slave(173)@67.195.138.61:55423 I0626 00:04:39.669203  5471 authenticator.hpp:156] Creating new server SASL connection I0626 00:04:39.669340  5471 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0626 00:04:39.669359  5471 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0626 00:04:39.669386  5471 authenticator.hpp:262] Received SASL authentication start I0626 00:04:39.669414  5471 authenticator.hpp:384] Authentication requires more steps I0626 00:04:39.669457  5471 authenticatee.hpp:265] Received SASL authentication step I0626 00:04:39.669514  5471 authenticator.hpp:290] Received SASL authentication step I0626 00:04:39.669534  5471 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0626 00:04:39.669543  5471 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0626 00:04:39.669567  5471 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0626 00:04:39.669580  5471 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0626 00:04:39.669589  5471 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0626 00:04:39.669594  5471 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0626 00:04:39.669606  5471 authenticator.hpp:376] Authentication success I0626 00:04:39.669641  5471 authenticatee.hpp:305] Authentication success I0626 00:04:39.669669  5471 master.cpp:3539] Successfully authenticated principal 'test-principal' at slave(173)@67.195.138.61:55423 I0626 00:04:39.669761  5450 sched.cpp:139] Version: 0.20.0 I0626 00:04:39.669764  5478 slave.cpp:734] Successfully authenticated with master master@67.195.138.61:55423 I0626 00:04:39.669826  5478 slave.cpp:972] Will retry registration in 3.190666ms if necessary I0626 00:04:39.669950  5471 master.cpp:2781] Registering slave at slave(173)@67.195.138.61:55423 (juno.apache.org) with id 20140626-000439-1032504131-55423-5450-0 I0626 00:04:39.669960  5475 sched.cpp:235] New master detected at master@67.195.138.61:55423 I0626 00:04:39.669977  5475 sched.cpp:285] Authenticating with master master@67.195.138.61:55423 I0626 00:04:39.670073  5471 registrar.cpp:422] Attempting to update the 'registry' I0626 00:04:39.670114  5475 authenticatee.hpp:128] Creating new client SASL connection I0626 00:04:39.670263  5475 master.cpp:3499] Authenticating scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423 I0626 00:04:39.670361  5474 authenticator.hpp:156] Creating new server SASL connection I0626 00:04:39.670506  5475 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0626 00:04:39.670526  5475 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0626 00:04:39.670559  5475 authenticator.hpp:262] Received SASL authentication start I0626 00:04:39.670590  5475 authenticator.hpp:384] Authentication requires more steps I0626 00:04:39.670619  5475 authenticatee.hpp:265] Received SASL authentication step I0626 00:04:39.670650  5475 authenticator.hpp:290] Received SASL authentication step I0626 00:04:39.670670  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0626 00:04:39.670677  5475 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0626 00:04:39.670687  5475 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0626 00:04:39.670697  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0626 00:04:39.670706  5475 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0626 00:04:39.670712  5475 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0626 00:04:39.670723  5475 authenticator.hpp:376] Authentication success I0626 00:04:39.670749  5475 authenticatee.hpp:305] Authentication success I0626 00:04:39.670773  5475 master.cpp:3539] Successfully authenticated principal 'test-principal' at scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423 I0626 00:04:39.670845  5475 sched.cpp:359] Successfully authenticated with master master@67.195.138.61:55423 I0626 00:04:39.670858  5475 sched.cpp:478] Sending registration request to master@67.195.138.61:55423 I0626 00:04:39.670899  5475 master.cpp:1241] Received registration request from scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423 I0626 00:04:39.670922  5475 master.cpp:1201] Authorizing framework principal 'test-principal' to receive offers for role '*' I0626 00:04:39.671052  5475 master.cpp:1300] Registering framework 20140626-000439-1032504131-55423-5450-0000 at scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423 I0626 00:04:39.671159  5474 sched.cpp:409] Framework registered with 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:39.671185  5474 sched.cpp:423] Scheduler::registered took 10223ns I0626 00:04:39.671226  5474 hierarchical_allocator_process.hpp:331] Added framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:39.671241  5474 hierarchical_allocator_process.hpp:724] No resources available to allocate! I0626 00:04:39.671247  5474 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8574ns I0626 00:04:39.671879  5476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.48781ms I0626 00:04:39.671900  5476 replica.cpp:676] Persisted action at 2 I0626 00:04:39.672164  5471 replica.cpp:655] Replica received learned notice for position 2 I0626 00:04:39.674092  5472 slave.cpp:972] Will retry registration in 25.467893ms if necessary I0626 00:04:39.674108  5476 master.cpp:2769] Ignoring register slave message from slave(173)@67.195.138.61:55423 (juno.apache.org) as admission is already in progress I0626 00:04:39.680193  5471 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.01285ms I0626 00:04:39.680223  5471 leveldb.cpp:401] Deleting ~1 keys from leveldb took 11393ns I0626 00:04:39.680234  5471 replica.cpp:676] Persisted action at 2 I0626 00:04:39.680245  5471 replica.cpp:661] Replica learned TRUNCATE action at position 2 I0626 00:04:39.680585  5472 log.cpp:680] Attempting to append 326 bytes to the log I0626 00:04:39.680670  5477 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0626 00:04:39.680953  5474 replica.cpp:508] Replica received write request for position 3 I0626 00:04:39.688521  5474 leveldb.cpp:343] Persisting action (345 bytes) to leveldb took 7.548316ms I0626 00:04:39.688542  5474 replica.cpp:676] Persisted action at 3 I0626 00:04:39.688750  5474 replica.cpp:655] Replica received learned notice for position 3 I0626 00:04:39.696851  5474 leveldb.cpp:343] Persisting action (347 bytes) to leveldb took 8.088289ms I0626 00:04:39.696869  5474 replica.cpp:676] Persisted action at 3 I0626 00:04:39.696878  5474 replica.cpp:661] Replica learned APPEND action at position 3 I0626 00:04:39.697268  5474 registrar.cpp:479] Successfully updated 'registry' I0626 00:04:39.697350  5474 log.cpp:699] Attempting to truncate the log to 3 I0626 00:04:39.697412  5474 master.cpp:2821] Registered slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:39.697423  5474 master.cpp:3967] Adding slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0626 00:04:39.697535  5474 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0626 00:04:39.697618  5474 slave.cpp:768] Registered with master master@67.195.138.61:55423; given slave ID 20140626-000439-1032504131-55423-5450-0 I0626 00:04:39.697754  5474 slave.cpp:781] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/slave.info' I0626 00:04:39.697762  5471 hierarchical_allocator_process.hpp:444] Added slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0626 00:04:39.697845  5471 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:39.697854  5474 slave.cpp:2325] Received ping from slave-observer(142)@67.195.138.61:55423 I0626 00:04:39.698040  5471 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140626-000439-1032504131-55423-5450-0 in 231333ns I0626 00:04:39.698051  5474 replica.cpp:508] Replica received write request for position 4 I0626 00:04:39.698118  5471 master.hpp:794] Adding offer 20140626-000439-1032504131-55423-5450-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:39.698170  5471 master.cpp:3446] Sending 1 offers to framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:39.698318  5471 sched.cpp:546] Scheduler::resourceOffers took 24371ns I0626 00:04:39.699718  5477 master.hpp:804] Removing offer 20140626-000439-1032504131-55423-5450-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:39.699787  5477 master.cpp:2125] Processing reply for offers: [ 20140626-000439-1032504131-55423-5450-0 ] on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:39.699812  5477 master.cpp:2211] Authorizing framework principal 'test-principal' to launch task 897522cc-4ec5-4904-aed0-00b6b8c41028 as user 'jenkins' I0626 00:04:39.700160  5477 master.hpp:766] Adding task 897522cc-4ec5-4904-aed0-00b6b8c41028 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:39.700188  5477 master.cpp:2277] Launching task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:39.700392  5471 slave.cpp:1003] Got assigned task 897522cc-4ec5-4904-aed0-00b6b8c41028 for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:39.700479  5477 hierarchical_allocator_process.hpp:546] Framework 20140626-000439-1032504131-55423-5450-0000 left cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] unused on slave 20140626-000439-1032504131-55423-5450-0 I0626 00:04:39.700505  5471 slave.cpp:3400] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.info' I0626 00:04:39.700597  5477 hierarchical_allocator_process.hpp:588] Framework 20140626-000439-1032504131-55423-5450-0000 filtered slave 20140626-000439-1032504131-55423-5450-0 for 5secs I0626 00:04:39.700686  5471 slave.cpp:3407] Checkpointing framework pid 'scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.pid' I0626 00:04:39.700960  5471 slave.cpp:1113] Launching task 897522cc-4ec5-4904-aed0-00b6b8c41028 for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:39.702287  5471 slave.cpp:3722] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/executor.info' I0626 00:04:39.702738  5471 slave.cpp:3837] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/tasks/897522cc-4ec5-4904-aed0-00b6b8c41028/task.info' I0626 00:04:39.702744  5476 containerizer.cpp:427] Starting container '9ad3a5ac-3587-47df-96c2-df76ea09328c' for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework '20140626-000439-1032504131-55423-5450-0000' I0626 00:04:39.702987  5471 slave.cpp:1223] Queuing task '897522cc-4ec5-4904-aed0-00b6b8c41028' for executor 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework '20140626-000439-1032504131-55423-5450-0000 I0626 00:04:39.703039  5471 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c' I0626 00:04:39.704654  5477 launcher.cpp:137] Forked child with pid '7596' for container '9ad3a5ac-3587-47df-96c2-df76ea09328c' I0626 00:04:39.704891  5477 containerizer.cpp:705] Checkpointing executor's forked pid 7596 to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/pids/forked.pid' I0626 00:04:39.705301  5474 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.183865ms I0626 00:04:39.705343  5474 replica.cpp:676] Persisted action at 4 I0626 00:04:39.705912  5476 containerizer.cpp:537] Fetching URIs for container '9ad3a5ac-3587-47df-96c2-df76ea09328c' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher' I0626 00:04:39.706073  5471 replica.cpp:655] Replica received learned notice for position 4 I0626 00:04:39.713664  5471 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 6.238172ms I0626 00:04:39.713762  5471 leveldb.cpp:401] Deleting ~2 keys from leveldb took 42244ns I0626 00:04:39.713788  5471 replica.cpp:676] Persisted action at 4 I0626 00:04:39.713810  5471 replica.cpp:661] Replica learned TRUNCATE action at position 4 I0626 00:04:40.378677  5475 slave.cpp:2470] Monitoring executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework '20140626-000439-1032504131-55423-5450-0000' in container '9ad3a5ac-3587-47df-96c2-df76ea09328c' WARNING: Logging before InitGoogleLogging() is written to STDERR I0626 00:04:40.413177  7631 process.cpp:1671] libprocess is initialized on 67.195.138.61:40619 for 8 cpus I0626 00:04:40.414454  7631 exec.cpp:131] Version: 0.20.0 I0626 00:04:40.415856  7649 exec.cpp:181] Executor started at: executor(1)@67.195.138.61:40619 with pid 7631 I0626 00:04:40.416453  5471 slave.cpp:1734] Got registration for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.416527  5471 slave.cpp:1819] Checkpointing executor pid 'executor(1)@67.195.138.61:40619' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/pids/libprocess.pid' I0626 00:04:40.416998  5471 slave.cpp:1853] Flushing queued task 897522cc-4ec5-4904-aed0-00b6b8c41028 for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.417186  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:40.417322  7648 exec.cpp:205] Executor registered on slave 20140626-000439-1032504131-55423-5450-0 I0626 00:04:40.417368  7653 process.cpp:1037] Socket closed while receiving I0626 00:04:40.418385  7648 exec.cpp:217] Executor::registered took 115121ns Registered executor on juno.apache.org I0626 00:04:40.418544  7648 exec.cpp:292] Executor asked to run task '897522cc-4ec5-4904-aed0-00b6b8c41028' Starting task 897522cc-4ec5-4904-aed0-00b6b8c41028 I0626 00:04:40.418609  7648 exec.cpp:301] Executor::launchTask took 35936ns Forked command at 7654 sh -c 'sleep 1000' I0626 00:04:40.420611  7650 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.420953  5473 slave.cpp:2088] Handling status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from executor(1)@67.195.138.61:40619 I0626 00:04:40.421188  5474 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.421206  5474 status_update_manager.cpp:499] Creating StatusUpdate stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.421469  5474 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.525890  5474 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to master@67.195.138.61:55423 I0626 00:04:40.526053  5474 master.cpp:3107] Status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:40.526087  5474 slave.cpp:2246] Status update manager successfully handled status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.526100  5474 slave.cpp:2252] Sending acknowledgement for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to executor(1)@67.195.138.61:40619 I0626 00:04:40.526252  5474 sched.cpp:637] Scheduler::statusUpdate took 17393ns I0626 00:04:40.526294  5474 master.cpp:2631] Forwarding status update acknowledgement 6d952e6d-b7d7-4f40-9f44-f7c3f81757af for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:40.526371  5474 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.526384  5474 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.526468  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:40.526574  7651 exec.cpp:338] Executor received status update acknowledgement 6d952e6d-b7d7-4f40-9f44-f7c3f81757af for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.526679  7653 process.cpp:1037] Socket closed while receiving I0626 00:04:40.569715  5473 hierarchical_allocator_process.hpp:833] Filtered cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.569749  5473 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 105698ns I0626 00:04:40.576212  5477 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.578642  5450 sched.cpp:139] Version: 0.20.0 I0626 00:04:40.578886  5475 sched.cpp:235] New master detected at master@67.195.138.61:55423 I0626 00:04:40.578902  5475 sched.cpp:285] Authenticating with master master@67.195.138.61:55423 I0626 00:04:40.579040  5475 authenticatee.hpp:128] Creating new client SASL connection I0626 00:04:40.579202  5475 master.cpp:3499] Authenticating scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423 I0626 00:04:40.579313  5475 authenticator.hpp:156] Creating new server SASL connection I0626 00:04:40.579414  5475 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0626 00:04:40.579430  5475 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0626 00:04:40.579457  5475 authenticator.hpp:262] Received SASL authentication start I0626 00:04:40.579488  5475 authenticator.hpp:384] Authentication requires more steps I0626 00:04:40.579514  5475 authenticatee.hpp:265] Received SASL authentication step I0626 00:04:40.579551  5475 authenticator.hpp:290] Received SASL authentication step I0626 00:04:40.579573  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0626 00:04:40.579586  5475 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0626 00:04:40.579601  5475 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0626 00:04:40.579612  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0626 00:04:40.579619  5475 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0626 00:04:40.579624  5475 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0626 00:04:40.579638  5475 authenticator.hpp:376] Authentication success I0626 00:04:40.579664  5475 authenticatee.hpp:305] Authentication success I0626 00:04:40.579687  5475 master.cpp:3539] Successfully authenticated principal 'test-principal' at scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423 I0626 00:04:40.579768  5475 sched.cpp:359] Successfully authenticated with master master@67.195.138.61:55423 I0626 00:04:40.579781  5475 sched.cpp:478] Sending registration request to master@67.195.138.61:55423 I0626 00:04:40.579825  5475 master.cpp:1241] Received registration request from scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423 I0626 00:04:40.579845  5475 master.cpp:1201] Authorizing framework principal 'test-principal' to receive offers for role '*' I0626 00:04:40.579984  5475 master.cpp:1300] Registering framework 20140626-000439-1032504131-55423-5450-0001 at scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423 I0626 00:04:40.580056  5475 sched.cpp:409] Framework registered with 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:40.580075  5475 sched.cpp:423] Scheduler::registered took 8994ns I0626 00:04:40.580117  5475 hierarchical_allocator_process.hpp:331] Added framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:40.580173  5475 hierarchical_allocator_process.hpp:750] Offering cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:40.580366  5475 hierarchical_allocator_process.hpp:833] Filtered  on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:40.580378  5475 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 251520ns I0626 00:04:40.580454  5475 master.hpp:794] Adding offer 20140626-000439-1032504131-55423-5450-1 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:40.580509  5475 master.cpp:3446] Sending 1 offers to framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:40.580796  5476 sched.cpp:546] Scheduler::resourceOffers took 36436ns I0626 00:04:40.582280  5476 master.hpp:804] Removing offer 20140626-000439-1032504131-55423-5450-1 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:40.582362  5476 master.cpp:2125] Processing reply for offers: [ 20140626-000439-1032504131-55423-5450-1 ] on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) for framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:40.582402  5476 master.cpp:2211] Authorizing framework principal 'test-principal' to launch task b1f40647-a2ff-475d-a56b-d2a5db9c1229 as user 'jenkins' I0626 00:04:40.582823  5475 master.hpp:766] Adding task b1f40647-a2ff-475d-a56b-d2a5db9c1229 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:40.582892  5475 master.cpp:2277] Launching task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:40.583001  5474 slave.cpp:1003] Got assigned task b1f40647-a2ff-475d-a56b-d2a5db9c1229 for framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:40.583097  5474 slave.cpp:3400] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/framework.info' I0626 00:04:40.583204  5474 slave.cpp:3407] Checkpointing framework pid 'scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/framework.pid' I0626 00:04:40.583442  5474 slave.cpp:1113] Launching task b1f40647-a2ff-475d-a56b-d2a5db9c1229 for framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:40.584455  5474 slave.cpp:3722] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/executor.info' I0626 00:04:40.584846  5474 slave.cpp:3837] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3/tasks/b1f40647-a2ff-475d-a56b-d2a5db9c1229/task.info' I0626 00:04:40.584866  5476 containerizer.cpp:427] Starting container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' for executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework '20140626-000439-1032504131-55423-5450-0001' I0626 00:04:40.584976  5474 slave.cpp:1223] Queuing task 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' for executor b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework '20140626-000439-1032504131-55423-5450-0001 I0626 00:04:40.585026  5474 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' I0626 00:04:40.586937  5476 launcher.cpp:137] Forked child with pid '7656' for container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' I0626 00:04:40.587131  5476 containerizer.cpp:705] Checkpointing executor's forked pid 7656 to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3/pids/forked.pid' I0626 00:04:40.587872  5477 containerizer.cpp:537] Fetching URIs for container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher' I0626 00:04:41.384660  5472 slave.cpp:2470] Monitoring executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework '20140626-000439-1032504131-55423-5450-0001' in container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' WARNING: Logging before InitGoogleLogging() is written to STDERR I0626 00:04:41.417649  7691 process.cpp:1671] libprocess is initialized on 67.195.138.61:40524 for 8 cpus I0626 00:04:41.418674  7691 exec.cpp:131] Version: 0.20.0 I0626 00:04:41.420272  7712 exec.cpp:181] Executor started at: executor(1)@67.195.138.61:40524 with pid 7691 I0626 00:04:41.420771  5477 slave.cpp:1734] Got registration for executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.420871  5477 slave.cpp:1819] Checkpointing executor pid 'executor(1)@67.195.138.61:40524' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3/pids/libprocess.pid' I0626 00:04:41.421335  5477 slave.cpp:1853] Flushing queued task b1f40647-a2ff-475d-a56b-d2a5db9c1229 for executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.421401  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:41.421506  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:41.421622  7709 exec.cpp:205] Executor registered on slave 20140626-000439-1032504131-55423-5450-0 I0626 00:04:41.421701  7713 process.cpp:1037] Socket closed while receiving I0626 00:04:41.421891  7713 process.cpp:1037] Socket closed while receiving I0626 00:04:41.422695  7709 exec.cpp:217] Executor::registered took 116729ns Registered executor on juno.apache.org I0626 00:04:41.422817  7709 exec.cpp:292] Executor asked to run task 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' Starting task b1f40647-a2ff-475d-a56b-d2a5db9c1229 I0626 00:04:41.422878  7709 exec.cpp:301] Executor::launchTask took 44617ns Forked command at 7714 sh -c 'sleep 1000' I0626 00:04:41.424744  7710 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.425102  5473 slave.cpp:2088] Handling status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 from executor(1)@67.195.138.61:40524 I0626 00:04:41.425271  5472 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.425309  5472 status_update_manager.cpp:499] Creating StatusUpdate stream for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.425585  5472 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.517669  5472 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to master@67.195.138.61:55423 I0626 00:04:41.517848  5474 slave.cpp:2246] Status update manager successfully handled status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.517870  5474 slave.cpp:2252] Sending acknowledgement for status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to executor(1)@67.195.138.61:40524 I0626 00:04:41.517985  5471 master.cpp:3107] Status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 from slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:41.518061  5473 sched.cpp:637] Scheduler::statusUpdate took 30727ns I0626 00:04:41.518087  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:41.518188  5473 master.cpp:2631] Forwarding status update acknowledgement 7994ad88-77f5-45a5-91bf-b1f4957fba87 for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:41.518209  7705 exec.cpp:338] Executor received status update acknowledgement 7994ad88-77f5-45a5-91bf-b1f4957fba87 for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.518237  7713 process.cpp:1037] Socket closed while receiving I0626 00:04:41.518332  5477 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.518358  5477 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.565961  5477 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.566172  5450 slave.cpp:486] Slave terminating I0626 00:04:41.566315  5476 master.cpp:760] Slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) disconnected I0626 00:04:41.566337  5476 master.cpp:1602] Disconnecting slave 20140626-000439-1032504131-55423-5450-0 I0626 00:04:41.566411  5473 hierarchical_allocator_process.hpp:483] Slave 20140626-000439-1032504131-55423-5450-0 disconnected I0626 00:04:41.567461  5450 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem I0626 00:04:41.569854  5477 slave.cpp:168] Slave started on 174)@67.195.138.61:55423 I0626 00:04:41.569874  5477 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/credential' I0626 00:04:41.569941  5477 slave.cpp:268] Slave using credential for: test-principal I0626 00:04:41.570065  5477 slave.cpp:281] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0626 00:04:41.570139  5477 slave.cpp:326] Slave hostname: juno.apache.org I0626 00:04:41.570148  5477 slave.cpp:327] Slave checkpoint: true I0626 00:04:41.570361  5478 hierarchical_allocator_process.hpp:833] Filtered  on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.570382  5478 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 97062ns I0626 00:04:41.570710  5476 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta' I0626 00:04:41.572727  5475 slave.cpp:3196] Recovering framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.572752  5475 slave.cpp:3572] Recovering executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.572877  5475 slave.cpp:3196] Recovering framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.572904  5475 slave.cpp:3572] Recovering executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.573421  5478 status_update_manager.cpp:193] Recovering status update manager I0626 00:04:41.573436  5478 status_update_manager.cpp:201] Recovering executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.573470  5478 status_update_manager.cpp:499] Creating StatusUpdate stream for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.573627  5478 status_update_manager.hpp:306] Replaying status update stream for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 I0626 00:04:41.573662  5478 status_update_manager.cpp:201] Recovering executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.573689  5478 status_update_manager.cpp:499] Creating StatusUpdate stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.573804  5478 status_update_manager.hpp:306] Replaying status update stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028 I0626 00:04:41.573848  5475 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' I0626 00:04:41.573881  5475 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c' I0626 00:04:41.574369  5477 containerizer.cpp:287] Recovering containerizer I0626 00:04:41.574404  5477 containerizer.cpp:329] Recovering container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' for executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.574440  5477 containerizer.cpp:329] Recovering container '9ad3a5ac-3587-47df-96c2-df76ea09328c' for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.575889  5476 slave.cpp:3069] Sending reconnect request to executor 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 at executor(1)@67.195.138.61:40619 I0626 00:04:41.576014  5476 slave.cpp:3069] Sending reconnect request to executor b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 at executor(1)@67.195.138.61:40524 I0626 00:04:41.576128  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:41.576170  7645 exec.cpp:251] Received reconnect request from slave 20140626-000439-1032504131-55423-5450-0 I0626 00:04:41.576202  7653 process.cpp:1037] Socket closed while receiving I0626 00:04:41.576230  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:41.576308  7705 exec.cpp:251] Received reconnect request from slave 20140626-000439-1032504131-55423-5450-0 I0626 00:04:41.576328  7713 process.cpp:1037] Socket closed while receiving I0626 00:04:41.576519  5472 slave.cpp:1913] Re-registering executor 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.576658  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:41.576730  7653 process.cpp:1037] Socket closed while receiving I0626 00:04:41.576750  7650 exec.cpp:228] Executor re-registered on slave 20140626-000439-1032504131-55423-5450-0 IRe-registered executor on juno.apache.org 0626 00:04:41.577729  7650 exec.cpp:240] Executor::reregistered took 50146ns I0626 00:04:41.590677  5476 hierarchical_allocator_process.hpp:833] Filtered  on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.590695  5475 slave.cpp:2037] Cleaning up un-reregistered executors I0626 00:04:41.590701  5476 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 56695ns I0626 00:04:41.590706  5475 slave.cpp:2055] Killing un-reregistered executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:41.590744  5475 slave.cpp:3128] Finished recovery I0626 00:04:41.590900  5474 containerizer.cpp:903] Destroying container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' I0626 00:04:41.592074  5472 slave.cpp:601] New master detected at master@67.195.138.61:55423 I0626 00:04:41.592099  5472 slave.cpp:677] Authenticating with master master@67.195.138.61:55423 I0626 00:04:41.592154  5472 slave.cpp:650] Detecting new master I0626 00:04:41.592196  5472 status_update_manager.cpp:167] New master detected at master@67.195.138.61:55423 W0626 00:04:41.592607  5477 slave.cpp:1906] Shutting down executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 because the slave is not in recovery mode I0626 00:04:41.592816  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:41.592881  7711 exec.cpp:378] Executor asked to shutdown I0626 00:04:41.592921  7713 process.cpp:1037] Socket closed while receiving I0626 00:04:41.592954  7705 exec.cpp:77] Scheduling shutdown of the executor IShutting down 0626 00:04:41.592994  7711 exec.cpp:393] Executor::shutdown took 49357ns Sending SIGTERM to process tree at pid 7714 I0626 00:04:41.594029  5471 authenticatee.hpp:128] Creating new client SASL connection I0626 00:04:41.594419  5472 master.cpp:3499] Authenticating slave(174)@67.195.138.61:55423 I0626 00:04:41.594646  5476 authenticator.hpp:156] Creating new server SASL connection I0626 00:04:41.594898  5476 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0626 00:04:41.594923  5476 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0626 00:04:41.594960  5476 authenticator.hpp:262] Received SASL authentication start I0626 00:04:41.595002  5476 authenticator.hpp:384] Authentication requires more steps I0626 00:04:41.595039  5476 authenticatee.hpp:265] Received SASL authentication step I0626 00:04:41.595095  5476 authenticator.hpp:290] Received SASL authentication step I0626 00:04:41.595115  5476 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0626 00:04:41.595124  5476 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0626 00:04:41.595141  5476 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0626 00:04:41.595155  5476 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0626 00:04:41.595162  5476 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0626 00:04:41.595168  5476 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0626 00:04:41.595181  5476 authenticator.hpp:376] Authentication success I0626 00:04:41.595219  5476 authenticatee.hpp:305] Authentication success I0626 00:04:41.595252  5476 master.cpp:3539] Successfully authenticated principal 'test-principal' at slave(174)@67.195.138.61:55423 I0626 00:04:41.595978  5471 slave.cpp:734] Successfully authenticated with master master@67.195.138.61:55423 I0626 00:04:41.596087  5471 slave.cpp:972] Will retry registration in 5.904051ms if necessary W0626 00:04:41.596179  5476 master.cpp:2896] Slave at slave(174)@67.195.138.61:55423 (juno.apache.org) is being allowed to re-register with an already in use id (20140626-000439-1032504131-55423-5450-0) I0626 00:04:41.596371  5476 slave.cpp:818] Re-registered with master master@67.195.138.61:55423 I0626 00:04:41.596407  5476 slave.cpp:1584] Updating framework 20140626-000439-1032504131-55423-5450-0000 pid to scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423 I0626 00:04:41.596454  5476 slave.cpp:1592] Checkpointing framework pid 'scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.pid' I0626 00:04:41.596570  5476 slave.cpp:1584] Updating framework 20140626-000439-1032504131-55423-5450-0001 pid to scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423 I0626 00:04:41.596608  5476 slave.cpp:1592] Checkpointing framework pid 'scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/framework.pid' I0626 00:04:41.596710  5476 hierarchical_allocator_process.hpp:497] Slave 20140626-000439-1032504131-55423-5450-0 reconnected I0626 00:04:41.597498  5476 master.cpp:2461] Asked to kill task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.597523  5476 master.cpp:2562] Telling slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) to kill task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.597580  5476 slave.cpp:1279] Asked to kill task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:41.597724  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:41.597790  7645 exec.cpp:312] Executor asked to kill task '897522cc-4ec5-4904-aed0-00b6b8c41028' I0626 00:04:41.597796  7653 process.cpp:1037] Socket closed while receiving I0626 00:04:41.597843  7645 exec.cpp:321] Executor::killTask took 26639ns Shutting down Sending SIGTERM to process tree at pid 7654 Killing the following process trees: [  -+- 7654 sh -c sleep 1000   \--- 7655 sleep 1000  ] I0626 00:04:41.656000  5479 process.cpp:1037] Socket closed while receiving Command terminated with signal Terminated (pid: 7654) I0626 00:04:42.421964  7649 exec.cpp:524] Executor sending status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.422332  5477 slave.cpp:2088] Handling status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from executor(1)@67.195.138.61:40619 I0626 00:04:42.422384  5477 slave.cpp:3770] Terminating task 897522cc-4ec5-4904-aed0-00b6b8c41028 I0626 00:04:42.422912  5472 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.422946  5472 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.558498  5472 status_update_manager.cpp:373] Forwarding status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to master@67.195.138.61:55423 I0626 00:04:42.558712  5477 slave.cpp:2246] Status update manager successfully handled status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.558743  5477 slave.cpp:2252] Sending acknowledgement for status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to executor(1)@67.195.138.61:40619 I0626 00:04:42.558749  5476 master.cpp:3107] Status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:42.558820  5476 master.hpp:784] Removing task 897522cc-4ec5-4904-aed0-00b6b8c41028 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:42.558917  5478 sched.cpp:637] Scheduler::statusUpdate took 40786ns I0626 00:04:42.559017  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:42.559092  7653 process.cpp:1037] Socket closed while receiving I0626 00:04:42.559100  7650 exec.cpp:338] Executor received status update acknowledgement 3bd1b60e-8496-4254-8188-c160b6a7e498 for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.559386  5471 master.cpp:2631] Forwarding status update acknowledgement 3bd1b60e-8496-4254-8188-c160b6a7e498 for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:42.559453  5474 hierarchical_allocator_process.hpp:635] Recovered cpus(*):1; mem(*):512 (total allocatable: cpus(*):1; mem(*):512) on slave 20140626-000439-1032504131-55423-5450-0 from framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.559494  5471 master.cpp:2461] Asked to kill task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.559516  5471 master.cpp:2562] Telling slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) to kill task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.559541  5474 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.559577  5474 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.559608  5472 slave.cpp:1279] Asked to kill task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 W0626 00:04:42.559625  5472 slave.cpp:1364] Ignoring kill task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 because the executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' is terminating/terminated I0626 00:04:42.569269  5476 master.cpp:122] No whitelist given. Advertising offers for all slaves I0626 00:04:42.591553  5478 containerizer.cpp:1019] Executor for container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' has exited I0626 00:04:42.591665  5477 hierarchical_allocator_process.hpp:833] Filtered cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.591794  5477 hierarchical_allocator_process.hpp:750] Offering cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.591970  5477 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 352174ns I0626 00:04:42.592067  5471 master.hpp:794] Adding offer 20140626-000439-1032504131-55423-5450-2 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:42.592103  5471 master.cpp:3446] Sending 1 offers to framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.592118  5473 slave.cpp:2528] Executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 terminated with signal Killed E0626 00:04:42.592233  5477 slave.cpp:2796] Failed to unmonitor container for executor b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001: Not monitored I0626 00:04:42.592279  5472 sched.cpp:546] Scheduler::resourceOffers took 32048ns I0626 00:04:42.592439  5472 master.hpp:804] Removing offer 20140626-000439-1032504131-55423-5450-2 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) I0626 00:04:42.592495  5472 master.cpp:2125] Processing reply for offers: [ 20140626-000439-1032504131-55423-5450-2 ] on slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) for framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.592707  5475 hierarchical_allocator_process.hpp:546] Framework 20140626-000439-1032504131-55423-5450-0001 left cpus(*):1; mem(*):512 unused on slave 20140626-000439-1032504131-55423-5450-0 I0626 00:04:42.592865  5475 hierarchical_allocator_process.hpp:588] Framework 20140626-000439-1032504131-55423-5450-0001 filtered slave 20140626-000439-1032504131-55423-5450-0 for 5secs I0626 00:04:42.593211  5473 slave.cpp:2088] Handling status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 from @0.0.0.0:0 I0626 00:04:42.593237  5473 slave.cpp:3770] Terminating task b1f40647-a2ff-475d-a56b-d2a5db9c1229 W0626 00:04:42.593387  5472 containerizer.cpp:809] Ignoring update for unknown container: 44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3 I0626 00:04:42.600702  5474 status_update_manager.cpp:530] Cleaning up status update stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.600874  5473 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.600895  5473 slave.cpp:3812] Completing task 897522cc-4ec5-4904-aed0-00b6b8c41028 I0626 00:04:42.600913  5474 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.600939  5474 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.634199  5474 status_update_manager.cpp:373] Forwarding status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to master@67.195.138.61:55423 I0626 00:04:42.634354  5475 master.cpp:3107] Status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 from slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:42.634373  5477 slave.cpp:2246] Status update manager successfully handled status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.634428  5473 sched.cpp:637] Scheduler::statusUpdate took 22610ns I0626 00:04:42.634520  5475 master.hpp:784] Removing task b1f40647-a2ff-475d-a56b-d2a5db9c1229 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) ../../src/tests/slave_recovery_tests.cpp:2930: Failure Value of: status2.get().state()   Actual: TASK_FAILED Expected: TASK_KILLED I0626 00:04:42.634699  5475 master.cpp:2631] Forwarding status update acknowledgement 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) I0626 00:04:42.634778  5472 hierarchical_allocator_process.hpp:635] Recovered cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140626-000439-1032504131-55423-5450-0 from framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.634804  5475 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.634836  5475 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.634843  5472 master.cpp:710] Framework 20140626-000439-1032504131-55423-5450-0001 disconnected I0626 00:04:42.634857  5472 master.cpp:1577] Deactivating framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.634881  5472 master.cpp:732] Giving framework 20140626-000439-1032504131-55423-5450-0001 0ns to failover I0626 00:04:42.635025  5472 hierarchical_allocator_process.hpp:407] Deactivated framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.635056  5472 master.cpp:3362] Framework failover timeout, removing framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.635066  5472 master.cpp:3821] Removing framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.635154  5472 master.cpp:710] Framework 20140626-000439-1032504131-55423-5450-0000 disconnected I0626 00:04:42.635167  5472 master.cpp:1577] Deactivating framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.635226  5472 master.cpp:732] Giving framework 20140626-000439-1032504131-55423-5450-0000 0ns to failover I0626 00:04:42.635254  5478 hierarchical_allocator_process.hpp:362] Removed framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.635267  5476 slave.cpp:1407] Asked to shut down framework 20140626-000439-1032504131-55423-5450-0001 by master@67.195.138.61:55423 I0626 00:04:42.635285  5476 slave.cpp:1432] Shutting down framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.635301  5476 slave.cpp:2662] Cleaning up executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.635308  5478 hierarchical_allocator_process.hpp:407] Deactivated framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.635340  5478 master.cpp:3362] Framework failover timeout, removing framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.635349  5478 master.cpp:3821] Removing framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.635469  5478 hierarchical_allocator_process.hpp:362] Removed framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.635601  5450 master.cpp:619] Master terminating I0626 00:04:42.635840  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' for gc 6.99999264157333days in the future I0626 00:04:42.635916  5476 slave.cpp:2737] Cleaning up framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.635916  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229' for gc 6.99999264090074days in the future I0626 00:04:42.635960  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' for gc 6.99999264048593days in the future I0626 00:04:42.636015  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229' for gc 6.99999264009185days in the future I0626 00:04:42.636034  5476 slave.cpp:1407] Asked to shut down framework 20140626-000439-1032504131-55423-5450-0000 by master@67.195.138.61:55423 I0626 00:04:42.636049  5476 slave.cpp:1432] Shutting down framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.636061  5476 slave.cpp:2808] Shutting down executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:42.636064  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001' for gc 6.99999263944593days in the future I0626 00:04:42.636107  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001' for gc 6.9999926390963days in the future I0626 00:04:42.636207  5476 slave.cpp:2332] master@67.195.138.61:55423 exited W0626 00:04:42.636220  5476 slave.cpp:2335] Master disconnected! Waiting for a new master to be elected I0626 00:04:42.636307  5479 process.cpp:1098] Socket closed while receiving I0626 00:04:42.636379  7653 process.cpp:1037] Socket closed while receiving I0626 00:04:42.636382  7648 exec.cpp:378] Executor asked to shutdown I0626 00:04:42.636535  7648 exec.cpp:393] Executor::shutdown took 6684ns I0626 00:04:42.636545  7649 exec.cpp:77] Scheduling shutdown of the executor I0626 00:04:42.637948  5472 containerizer.cpp:903] Destroying container '9ad3a5ac-3587-47df-96c2-df76ea09328c' I0626 00:04:42.672613  5479 process.cpp:1037] Socket closed while receiving I0626 00:04:42.692251  5475 status_update_manager.cpp:530] Cleaning up status update stream for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.692435  5475 status_update_manager.cpp:282] Closing status update streams for framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:42.692450  5471 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 E0626 00:04:42.692477  5471 slave.cpp:1685] Status update acknowledgement (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of unknown framework 20140626-000439-1032504131-55423-5450-0001 I0626 00:04:43.592118  5473 containerizer.cpp:1019] Executor for container '9ad3a5ac-3587-47df-96c2-df76ea09328c' has exited I0626 00:04:43.592550  5475 slave.cpp:2528] Executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 terminated with signal Killed I0626 00:04:43.592599  5475 slave.cpp:2662] Cleaning up executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:43.592901  5475 slave.cpp:2737] Cleaning up framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:43.592900  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c' for gc 6.99999313928296days in the future I0626 00:04:43.592991  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028' for gc 6.99999313866963days in the future I0626 00:04:43.592985  5471 status_update_manager.cpp:282] Closing status update streams for framework 20140626-000439-1032504131-55423-5450-0000 I0626 00:04:43.593022  5475 slave.cpp:486] Slave terminating I0626 00:04:43.593040  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c' for gc 6.99999313827556days in the future I0626 00:04:43.593086  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028' for gc 6.99999313791704days in the future I0626 00:04:43.593125  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000' for gc 6.99999313702518days in the future I0626 00:04:43.593166  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000' for gc 6.99999313664296days in the future [  FAILED  ] SlaveRecoveryTest/0.MultipleFrameworks, where TypeParam = mesos::internal::slave::MesosContainerizer (4218 ms)  {code}",1
"MESOS-1559","Allow jenkins build machine to dump stack traces of all threads when timeout","Many of the time, when jenkins build times out, we know that some test freezes at some place. However, most of the time, it's very hard to reproduce the deadlock on dev machines.  I would be cool if we can dump the stack traces of all threads when jenkins build times out. Some command like the following:  {noformat} echo thread apply all bt > tmp; gdb attach `pgrep lt-mesos-tests` < tmp {noformat}",5
"MESOS-1586","Isolate system directories, e.g., per-container /tmp","Ideally, tasks should not write outside their sandbox (executor work directory) but pragmatically they may need to write to /tmp, /var/tmp, or some other directory.  1) We should include any such files in disk usage and quota. 2) We should make these ""shared"" directories private, i.e., each container has their own. 3) We should make the lifetime of any such files the same as the executor work directory.",3
"MESOS-1587","Report disk usage from MesosContainerizer","We should report disk usage for the executor work directory from MesosContainerizer and include in the ResourceStatistics protobuf.",5
"MESOS-1594","SlaveRecoveryTest/0.ReconcileKillTask is flaky","Observed this on Jenkins.  {code} [ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG' I0714 15:08:43.915114 27216 leveldb.cpp:176] Opened db in 474.695188ms I0714 15:08:43.933645 27216 leveldb.cpp:183] Compacted db in 18.068942ms I0714 15:08:43.934129 27216 leveldb.cpp:198] Created db iterator in 7860ns I0714 15:08:43.934439 27216 leveldb.cpp:204] Seeked to beginning of db in 2560ns I0714 15:08:43.934779 27216 leveldb.cpp:273] Iterated through 0 keys in the db in 1400ns I0714 15:08:43.935098 27216 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0714 15:08:43.936027 27238 recover.cpp:425] Starting replica recovery I0714 15:08:43.936225 27238 recover.cpp:451] Replica is in EMPTY status I0714 15:08:43.936867 27238 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I0714 15:08:43.937049 27238 recover.cpp:188] Received a recover response from a replica in EMPTY status I0714 15:08:43.937232 27238 recover.cpp:542] Updating replica status to STARTING I0714 15:08:43.945600 27235 master.cpp:288] Master 20140714-150843-16842879-55850-27216 (quantal) started on 127.0.1.1:55850 I0714 15:08:43.945643 27235 master.cpp:325] Master only allowing authenticated frameworks to register I0714 15:08:43.945651 27235 master.cpp:330] Master only allowing authenticated slaves to register I0714 15:08:43.945658 27235 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG/credentials' I0714 15:08:43.945808 27235 master.cpp:359] Authorization enabled I0714 15:08:43.946369 27235 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@127.0.1.1:55850 I0714 15:08:43.946419 27235 master.cpp:122] No whitelist given. Advertising offers for all slaves I0714 15:08:43.946614 27235 master.cpp:1128] The newly elected leader is master@127.0.1.1:55850 with id 20140714-150843-16842879-55850-27216 I0714 15:08:43.946630 27235 master.cpp:1141] Elected as the leading master! I0714 15:08:43.946637 27235 master.cpp:959] Recovering from registrar I0714 15:08:43.946707 27235 registrar.cpp:313] Recovering registrar I0714 15:08:43.957895 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 20.529301ms I0714 15:08:43.957978 27238 replica.cpp:320] Persisted replica status to STARTING I0714 15:08:43.958142 27238 recover.cpp:451] Replica is in STARTING status I0714 15:08:43.958664 27238 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I0714 15:08:43.958762 27238 recover.cpp:188] Received a recover response from a replica in STARTING status I0714 15:08:43.958945 27238 recover.cpp:542] Updating replica status to VOTING I0714 15:08:43.975685 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.646136ms I0714 15:08:43.976367 27238 replica.cpp:320] Persisted replica status to VOTING I0714 15:08:43.976824 27241 recover.cpp:556] Successfully joined the Paxos group I0714 15:08:43.977072 27242 recover.cpp:440] Recover process terminated I0714 15:08:43.980590 27236 log.cpp:656] Attempting to start the writer I0714 15:08:43.981385 27236 replica.cpp:474] Replica received implicit promise request with proposal 1 I0714 15:08:43.999141 27236 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.705787ms I0714 15:08:43.999222 27236 replica.cpp:342] Persisted promised to 1 I0714 15:08:44.004451 27240 coordinator.cpp:230] Coordinator attemping to fill missing position I0714 15:08:44.004914 27240 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I0714 15:08:44.021456 27240 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 16.499775ms I0714 15:08:44.021533 27240 replica.cpp:676] Persisted action at 0 I0714 15:08:44.022006 27240 replica.cpp:508] Replica received write request for position 0 I0714 15:08:44.022043 27240 leveldb.cpp:438] Reading position from leveldb took 21376ns I0714 15:08:44.035969 27240 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 13.885907ms I0714 15:08:44.036365 27240 replica.cpp:676] Persisted action at 0 I0714 15:08:44.040156 27238 replica.cpp:655] Replica received learned notice for position 0 I0714 15:08:44.058082 27238 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.860707ms I0714 15:08:44.058161 27238 replica.cpp:676] Persisted action at 0 I0714 15:08:44.058176 27238 replica.cpp:661] Replica learned NOP action at position 0 I0714 15:08:44.058526 27238 log.cpp:672] Writer started with ending position 0 I0714 15:08:44.058872 27238 leveldb.cpp:438] Reading position from leveldb took 25660ns I0714 15:08:44.060556 27238 registrar.cpp:346] Successfully fetched the registry (0B) I0714 15:08:44.060845 27238 registrar.cpp:422] Attempting to update the 'registry' I0714 15:08:44.062304 27238 log.cpp:680] Attempting to append 120 bytes to the log I0714 15:08:44.062866 27236 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0714 15:08:44.063154 27236 replica.cpp:508] Replica received write request for position 1 I0714 15:08:44.082813 27236 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 19.61683ms I0714 15:08:44.082890 27236 replica.cpp:676] Persisted action at 1 I0714 15:08:44.083256 27236 replica.cpp:655] Replica received learned notice for position 1 I0714 15:08:44.097398 27236 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 14.104796ms I0714 15:08:44.097475 27236 replica.cpp:676] Persisted action at 1 I0714 15:08:44.097488 27236 replica.cpp:661] Replica learned APPEND action at position 1 I0714 15:08:44.098569 27236 registrar.cpp:479] Successfully updated 'registry' I0714 15:08:44.098906 27240 log.cpp:699] Attempting to truncate the log to 1 I0714 15:08:44.099608 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0714 15:08:44.100005 27240 replica.cpp:508] Replica received write request for position 2 I0714 15:08:44.100566 27236 registrar.cpp:372] Successfully recovered registrar I0714 15:08:44.101227 27239 master.cpp:986] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register I0714 15:08:44.118376 27240 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 18.329495ms I0714 15:08:44.118455 27240 replica.cpp:676] Persisted action at 2 I0714 15:08:44.122258 27242 replica.cpp:655] Replica received learned notice for position 2 I0714 15:08:44.137336 27242 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.023553ms I0714 15:08:44.137460 27242 leveldb.cpp:401] Deleting ~1 keys from leveldb took 55049ns I0714 15:08:44.137480 27242 replica.cpp:676] Persisted action at 2 I0714 15:08:44.137492 27242 replica.cpp:661] Replica learned TRUNCATE action at position 2 I0714 15:08:44.143729 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem I0714 15:08:44.145934 27242 slave.cpp:168] Slave started on 43)@127.0.1.1:55850 I0714 15:08:44.145953 27242 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential' I0714 15:08:44.146040 27242 slave.cpp:266] Slave using credential for: test-principal I0714 15:08:44.146136 27242 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0714 15:08:44.146198 27242 slave.cpp:324] Slave hostname: quantal I0714 15:08:44.146209 27242 slave.cpp:325] Slave checkpoint: true I0714 15:08:44.146708 27242 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta' I0714 15:08:44.146824 27242 status_update_manager.cpp:193] Recovering status update manager I0714 15:08:44.146901 27242 containerizer.cpp:287] Recovering containerizer I0714 15:08:44.147228 27242 slave.cpp:3126] Finished recovery I0714 15:08:44.147531 27242 slave.cpp:599] New master detected at master@127.0.1.1:55850 I0714 15:08:44.147562 27242 slave.cpp:675] Authenticating with master master@127.0.1.1:55850 I0714 15:08:44.147614 27242 slave.cpp:648] Detecting new master I0714 15:08:44.147652 27242 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850 I0714 15:08:44.147691 27242 authenticatee.hpp:128] Creating new client SASL connection I0714 15:08:44.148533 27235 master.cpp:3507] Authenticating slave(43)@127.0.1.1:55850 I0714 15:08:44.148666 27235 authenticator.hpp:156] Creating new server SASL connection I0714 15:08:44.149054 27242 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0714 15:08:44.149447 27242 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0714 15:08:44.149917 27236 authenticator.hpp:262] Received SASL authentication start I0714 15:08:44.149974 27236 authenticator.hpp:384] Authentication requires more steps I0714 15:08:44.150208 27242 authenticatee.hpp:265] Received SASL authentication step I0714 15:08:44.150720 27239 authenticator.hpp:290] Received SASL authentication step I0714 15:08:44.150749 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0714 15:08:44.150758 27239 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0714 15:08:44.150771 27239 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0714 15:08:44.150781 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0714 15:08:44.150787 27239 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0714 15:08:44.150792 27239 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0714 15:08:44.150804 27239 authenticator.hpp:376] Authentication success I0714 15:08:44.150848 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(43)@127.0.1.1:55850 I0714 15:08:44.157696 27242 authenticatee.hpp:305] Authentication success I0714 15:08:44.158855 27242 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850 I0714 15:08:44.158936 27242 slave.cpp:970] Will retry registration in 10.352612ms if necessary I0714 15:08:44.161813 27216 sched.cpp:139] Version: 0.20.0 I0714 15:08:44.162608 27236 sched.cpp:235] New master detected at master@127.0.1.1:55850 I0714 15:08:44.162637 27236 sched.cpp:285] Authenticating with master master@127.0.1.1:55850 I0714 15:08:44.162747 27236 authenticatee.hpp:128] Creating new client SASL connection I0714 15:08:44.163506 27239 master.cpp:2789] Registering slave at slave(43)@127.0.1.1:55850 (quantal) with id 20140714-150843-16842879-55850-27216-0 I0714 15:08:44.164086 27238 registrar.cpp:422] Attempting to update the 'registry' I0714 15:08:44.165694 27238 log.cpp:680] Attempting to append 295 bytes to the log I0714 15:08:44.166231 27240 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0714 15:08:44.166517 27240 replica.cpp:508] Replica received write request for position 3 I0714 15:08:44.167199 27239 master.cpp:3507] Authenticating scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 I0714 15:08:44.167867 27241 authenticator.hpp:156] Creating new server SASL connection I0714 15:08:44.168058 27241 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0714 15:08:44.168081 27241 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0714 15:08:44.168107 27241 authenticator.hpp:262] Received SASL authentication start I0714 15:08:44.168149 27241 authenticator.hpp:384] Authentication requires more steps I0714 15:08:44.168176 27241 authenticatee.hpp:265] Received SASL authentication step I0714 15:08:44.168215 27241 authenticator.hpp:290] Received SASL authentication step I0714 15:08:44.168233 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0714 15:08:44.168793 27241 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0714 15:08:44.168820 27241 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0714 15:08:44.168834 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0714 15:08:44.168840 27241 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0714 15:08:44.168845 27241 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0714 15:08:44.168858 27241 authenticator.hpp:376] Authentication success I0714 15:08:44.168895 27241 authenticatee.hpp:305] Authentication success I0714 15:08:44.168970 27241 sched.cpp:359] Successfully authenticated with master master@127.0.1.1:55850 I0714 15:08:44.168987 27241 sched.cpp:478] Sending registration request to master@127.0.1.1:55850 I0714 15:08:44.169426 27239 master.cpp:1239] Queuing up registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 because authentication is still in progress I0714 15:08:44.169958 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 I0714 15:08:44.170440 27241 slave.cpp:970] Will retry registration in 8.76707ms if necessary I0714 15:08:44.175359 27239 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress I0714 15:08:44.175916 27239 master.cpp:1247] Received registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 I0714 15:08:44.176298 27239 master.cpp:1207] Authorizing framework principal 'test-principal' to receive offers for role '*' I0714 15:08:44.176858 27239 master.cpp:1306] Registering framework 20140714-150843-16842879-55850-27216-0000 at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 I0714 15:08:44.177408 27236 sched.cpp:409] Framework registered with 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.177443 27236 sched.cpp:423] Scheduler::registered took 12527ns I0714 15:08:44.177727 27241 hierarchical_allocator_process.hpp:331] Added framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.177747 27241 hierarchical_allocator_process.hpp:724] No resources available to allocate! I0714 15:08:44.177753 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8120ns I0714 15:08:44.179908 27241 slave.cpp:970] Will retry registration in 66.781028ms if necessary I0714 15:08:44.180007 27241 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress I0714 15:08:44.183082 27240 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 16.533189ms I0714 15:08:44.183125 27240 replica.cpp:676] Persisted action at 3 I0714 15:08:44.183465 27240 replica.cpp:655] Replica received learned notice for position 3 I0714 15:08:44.203276 27240 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 19.768951ms I0714 15:08:44.203376 27240 replica.cpp:676] Persisted action at 3 I0714 15:08:44.203392 27240 replica.cpp:661] Replica learned APPEND action at position 3 I0714 15:08:44.204033 27240 registrar.cpp:479] Successfully updated 'registry' I0714 15:08:44.204138 27240 log.cpp:699] Attempting to truncate the log to 3 I0714 15:08:44.204221 27240 master.cpp:2829] Registered slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) I0714 15:08:44.204241 27240 master.cpp:3975] Adding slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0714 15:08:44.204387 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0714 15:08:44.204489 27240 slave.cpp:766] Registered with master master@127.0.1.1:55850; given slave ID 20140714-150843-16842879-55850-27216-0 I0714 15:08:44.204745 27240 slave.cpp:779] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/slave.info' I0714 15:08:44.204954 27240 hierarchical_allocator_process.hpp:444] Added slave 20140714-150843-16842879-55850-27216-0 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0714 15:08:44.205023 27240 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.205122 27240 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140714-150843-16842879-55850-27216-0 in 131192ns I0714 15:08:44.205189 27240 slave.cpp:2323] Received ping from slave-observer(32)@127.0.1.1:55850 I0714 15:08:44.205258 27240 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal) I0714 15:08:44.205303 27240 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.205469 27240 sched.cpp:546] Scheduler::resourceOffers took 23591ns I0714 15:08:44.206351 27241 replica.cpp:508] Replica received write request for position 4 I0714 15:08:44.208353 27237 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal) I0714 15:08:44.208436 27237 master.cpp:2133] Processing reply for offers: [ 20140714-150843-16842879-55850-27216-0 ] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) for framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.208472 27237 master.cpp:2219] Authorizing framework principal 'test-principal' to launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 as user 'jenkins' I0714 15:08:44.208909 27237 master.hpp:773] Adding task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal) I0714 15:08:44.208947 27237 master.cpp:2285] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) I0714 15:08:44.209090 27237 slave.cpp:1001] Got assigned task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.209190 27237 slave.cpp:3398] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.info' I0714 15:08:44.209413 27237 slave.cpp:3405] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid' I0714 15:08:44.209710 27237 slave.cpp:1111] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.210978 27237 slave.cpp:3720] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/executor.info' I0714 15:08:44.211520 27237 slave.cpp:3835] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/tasks/4a6783aa-8d07-46e3-8399-2a5d047f0021/task.info' I0714 15:08:44.211714 27237 slave.cpp:1221] Queuing task '4a6783aa-8d07-46e3-8399-2a5d047f0021' for executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework '20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.211937 27236 containerizer.cpp:427] Starting container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000' I0714 15:08:44.212242 27236 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' I0714 15:08:44.216187 27236 launcher.cpp:137] Forked child with pid '28451' for container '19c466f8-bb5a-4842-a152-f585ff88762a' I0714 15:08:44.217281 27236 containerizer.cpp:705] Checkpointing executor's forked pid 28451 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/forked.pid' I0714 15:08:44.219408 27236 containerizer.cpp:537] Fetching URIs for container '19c466f8-bb5a-4842-a152-f585ff88762a' using command '/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/mesos-fetcher' I0714 15:08:44.223963 27241 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.554461ms I0714 15:08:44.224501 27241 replica.cpp:676] Persisted action at 4 I0714 15:08:44.225051 27241 replica.cpp:655] Replica received learned notice for position 4 I0714 15:08:44.242923 27241 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 17.806547ms I0714 15:08:44.243057 27241 leveldb.cpp:401] Deleting ~2 keys from leveldb took 57154ns I0714 15:08:44.243078 27241 replica.cpp:676] Persisted action at 4 I0714 15:08:44.243096 27241 replica.cpp:661] Replica learned TRUNCATE action at position 4 I0714 15:08:44.401140 27241 slave.cpp:2468] Monitoring executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000' in container '19c466f8-bb5a-4842-a152-f585ff88762a' WARNING: Logging before InitGoogleLogging() is written to STDERR I0714 15:08:44.434221 28486 process.cpp:1671] libprocess is initialized on 127.0.1.1:34669 for 8 cpus I0714 15:08:44.436146 28486 exec.cpp:131] Version: 0.20.0 I0714 15:08:44.438555 28500 exec.cpp:181] Executor started at: executor(1)@127.0.1.1:34669 with pid 28486 I0714 15:08:44.440846 27241 slave.cpp:1732] Got registration for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.440917 27241 slave.cpp:1817] Checkpointing executor pid 'executor(1)@127.0.1.1:34669' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/libprocess.pid' I0714 15:08:44.442373 27243 process.cpp:1098] Socket closed while receiving I0714 15:08:44.442790 27241 slave.cpp:1851] Flushing queued task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.443192 27243 process.cpp:1098] Socket closed while receiving I0714 15:08:44.443994 28508 process.cpp:1037] Socket closed while receiving I0714 15:08:44.444144 28508 process.cpp:1037] Socket closed while receiving I0714 15:08:44.444741 28500 exec.cpp:205] Executor registered on slave 20140714-150843-16842879-55850-27216-0 Registered executor on quantal I0714 15:08:44.446338 28500 exec.cpp:217] Executor::registered took 534236ns I0714 15:08:44.446715 28500 exec.cpp:292] Executor asked to run task '4a6783aa-8d07-46e3-8399-2a5d047f0021' Starting task 4a6783aa-8d07-46e3-8399-2a5d047f0021 I0714 15:08:44.447548 28500 exec.cpp:301] Executor::launchTask took 584306ns sh -c 'sleep 1000' Forked command at 28509 I0714 15:08:44.451202 28506 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.452327 27239 slave.cpp:2086] Handling status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669 I0714 15:08:44.452503 27239 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.452520 27239 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.452775 27239 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.472384 27239 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850 I0714 15:08:44.472764 27237 master.cpp:3115] Status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) I0714 15:08:44.472854 27237 sched.cpp:637] Scheduler::statusUpdate took 17656ns I0714 15:08:44.472920 27237 master.cpp:2639] Forwarding status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) I0714 15:08:44.473122 27239 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.473146 27239 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.473244 27237 slave.cpp:2244] Status update manager successfully handled status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.473258 27237 slave.cpp:2250] Sending acknowledgement for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669 I0714 15:08:44.473567 27243 process.cpp:1098] Socket closed while receiving I0714 15:08:44.474095 28508 process.cpp:1037] Socket closed while receiving I0714 15:08:44.474676 28502 exec.cpp:338] Executor received status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.491111 27239 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.491761 27216 slave.cpp:484] Slave terminating I0714 15:08:44.492559 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem I0714 15:08:44.494635 27237 master.cpp:766] Slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) disconnected I0714 15:08:44.494663 27237 master.cpp:1608] Disconnecting slave 20140714-150843-16842879-55850-27216-0 I0714 15:08:44.495120 27237 slave.cpp:168] Slave started on 44)@127.0.1.1:55850 I0714 15:08:44.495133 27237 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential' I0714 15:08:44.495226 27237 slave.cpp:266] Slave using credential for: test-principal I0714 15:08:44.495322 27237 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0714 15:08:44.495407 27237 slave.cpp:324] Slave hostname: quantal I0714 15:08:44.495419 27237 slave.cpp:325] Slave checkpoint: true I0714 15:08:44.495939 27242 master.cpp:2469] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.496207 27238 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta' I0714 15:08:44.498291 27240 slave.cpp:3194] Recovering framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.498325 27240 slave.cpp:3570] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.498940 27240 status_update_manager.cpp:193] Recovering status update manager I0714 15:08:44.498956 27240 status_update_manager.cpp:201] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.498975 27240 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.499092 27240 status_update_manager.hpp:306] Replaying status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 I0714 15:08:44.499241 27240 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' I0714 15:08:44.499433 27240 containerizer.cpp:287] Recovering containerizer I0714 15:08:44.499457 27240 containerizer.cpp:329] Recovering container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.495811 27237 hierarchical_allocator_process.hpp:483] Slave 20140714-150843-16842879-55850-27216-0 disconnected I0714 15:08:44.501255 27240 slave.cpp:3067] Sending reconnect request to executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 at executor(1)@127.0.1.1:34669 I0714 15:08:44.502030 28501 exec.cpp:251] Received reconnect request from slave 20140714-150843-16842879-55850-27216-0 I0714 15:08:44.502627 27243 process.cpp:1098] Socket closed while receiving I0714 15:08:44.502681 28508 process.cpp:1037] Socket closed while receiving I0714 15:08:44.503211 27240 slave.cpp:1911] Re-registering executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:44.504238 28501 exec.cpp:228] Executor re-registered on slave 20140714-150843-16842879-55850-27216-0 I0714 15:08:44.505033 28501 exec.cpp:240] Executor::reregistered took 45053ns Re-registered executor on quantal I0714 15:08:44.505507 27243 process.cpp:1098] Socket closed while receiving I0714 15:08:44.505558 28508 process.cpp:1037] Socket closed while receiving I0714 15:08:44.948043 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 124255ns I0714 15:08:45.948671 27237 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 61521ns I0714 15:08:46.503978 27238 slave.cpp:2035] Cleaning up un-reregistered executors I0714 15:08:46.504050 27238 slave.cpp:3126] Finished recovery I0714 15:08:46.504590 27238 slave.cpp:599] New master detected at master@127.0.1.1:55850 I0714 15:08:46.504639 27238 slave.cpp:675] Authenticating with master master@127.0.1.1:55850 I0714 15:08:46.504729 27238 slave.cpp:648] Detecting new master I0714 15:08:46.504772 27238 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850 I0714 15:08:46.504863 27238 authenticatee.hpp:128] Creating new client SASL connection I0714 15:08:46.505091 27238 master.cpp:3507] Authenticating slave(44)@127.0.1.1:55850 I0714 15:08:46.505239 27238 authenticator.hpp:156] Creating new server SASL connection I0714 15:08:46.505363 27238 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0714 15:08:46.505393 27238 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0714 15:08:46.505420 27238 authenticator.hpp:262] Received SASL authentication start I0714 15:08:46.505476 27238 authenticator.hpp:384] Authentication requires more steps I0714 15:08:46.505506 27238 authenticatee.hpp:265] Received SASL authentication step I0714 15:08:46.505542 27238 authenticator.hpp:290] Received SASL authentication step I0714 15:08:46.505558 27238 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0714 15:08:46.505566 27238 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0714 15:08:46.505584 27238 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0714 15:08:46.505595 27238 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0714 15:08:46.505601 27238 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0714 15:08:46.505606 27238 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0714 15:08:46.505616 27238 authenticator.hpp:376] Authentication success I0714 15:08:46.505646 27238 authenticatee.hpp:305] Authentication success I0714 15:08:46.505671 27238 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(44)@127.0.1.1:55850 I0714 15:08:46.505769 27238 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850 I0714 15:08:46.505873 27238 slave.cpp:970] Will retry registration in 17.903094ms if necessary W0714 15:08:46.505991 27238 master.cpp:2904] Slave at slave(44)@127.0.1.1:55850 (quantal) is being allowed to re-register with an already in use id (20140714-150843-16842879-55850-27216-0) W0714 15:08:46.506063 27238 master.cpp:3679]  Slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal) has non-terminal task 4a6783aa-8d07-46e3-8399-2a5d047f0021 that is supposed to be killed. Killing it now! I0714 15:08:46.506150 27238 slave.cpp:816] Re-registered with master master@127.0.1.1:55850 I0714 15:08:46.506186 27238 slave.cpp:1277] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.507275 27241 hierarchical_allocator_process.hpp:497] Slave 20140714-150843-16842879-55850-27216-0 reconnected I0714 15:08:46.508061 28504 exec.cpp:312] Executor asked to kill task '4a6783aa-8d07-46e3-8399-2a5d047f0021' I0714 15:08:46.508117 28504 exec.cpp:321] Executor::killTask took 24954ns Shutting down Sending SIGTERM to process tree at pid 28509 I0714 15:08:46.512238 27243 process.cpp:1098] Socket closed while receiving I0714 15:08:46.512508 27238 slave.cpp:1582] Updating framework 20140714-150843-16842879-55850-27216-0000 pid to scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 I0714 15:08:46.512846 27238 slave.cpp:1590] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid' I0714 15:08:46.513419 28508 process.cpp:1037] Socket closed while receiving Killing the following process trees: [  -+- 28509 sh -c sleep 1000   \--- 28510 sleep 1000  ] Command terminated with signal Terminated (pid: 28509) I0714 15:08:46.940232 28506 exec.cpp:524] Executor sending status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.940918 27240 slave.cpp:2086] Handling status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669 I0714 15:08:46.940979 27240 slave.cpp:3768] Terminating task 4a6783aa-8d07-46e3-8399-2a5d047f0021 I0714 15:08:46.941603 27240 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.941644 27240 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.949417 27236 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 63926ns I0714 15:08:46.965200 27240 status_update_manager.cpp:373] Forwarding status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850 I0714 15:08:46.965625 27239 master.cpp:3115] Status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal) I0714 15:08:46.965724 27239 master.hpp:791] Removing task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal) I0714 15:08:46.965903 27239 sched.cpp:637] Scheduler::statusUpdate took 39326ns I0714 15:08:46.966022 27239 hierarchical_allocator_process.hpp:635] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140714-150843-16842879-55850-27216-0 from framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.966120 27239 master.cpp:2639] Forwarding status update acknowledgement e3a5f8fd-eefc-42c6-94a7-086c93c01968 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal) I0714 15:08:46.966501 27241 slave.cpp:2244] Status update manager successfully handled status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.966519 27241 slave.cpp:2250] Sending acknowledgement for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669 I0714 15:08:46.966754 27240 status_update_manager.cpp:398] Received status update acknowledgement (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.966785 27240 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.967386 28500 exec.cpp:338] Executor received status update acknowledgement e3a5f8fd-eefc-42c6-94a7-086c93c01968 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.967562 27243 process.cpp:1098] Socket closed while receiving I0714 15:08:46.968147 28508 process.cpp:1037] Socket closed while receiving I0714 15:08:46.984608 27240 status_update_manager.cpp:530] Cleaning up status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.985239 27236 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:46.985280 27236 slave.cpp:3810] Completing task 4a6783aa-8d07-46e3-8399-2a5d047f0021 I0714 15:08:47.940703 27243 process.cpp:1037] Socket closed while receiving I0714 15:08:47.940984 27238 containerizer.cpp:1019] Executor for container '19c466f8-bb5a-4842-a152-f585ff88762a' has exited I0714 15:08:47.941007 27238 containerizer.cpp:903] Destroying container '19c466f8-bb5a-4842-a152-f585ff88762a' I0714 15:08:47.950192 27241 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:47.950405 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 320604ns I0714 15:08:47.950518 27241 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal) I0714 15:08:47.950572 27241 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:47.950774 27241 sched.cpp:546] Scheduler::resourceOffers took 37944ns I0714 15:08:47.951179 27216 master.cpp:625] Master terminating I0714 15:08:47.951263 27216 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal) I0714 15:08:47.953447 27242 sched.cpp:747] Stopping framework '20140714-150843-16842879-55850-27216-0000' I0714 15:08:47.953547 27242 slave.cpp:2330] master@127.0.1.1:55850 exited W0714 15:08:47.953567 27242 slave.cpp:2333] Master disconnected! Waiting for a new master to be elected I0714 15:08:47.964512 27238 slave.cpp:2526] Executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 exited with status 0 I0714 15:08:47.968690 27238 slave.cpp:2660] Cleaning up executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:47.969348 27236 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' for gc 6.99998878298667days in the future I0714 15:08:47.969751 27241 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021' for gc 6.99998877682963days in the future I0714 15:08:47.970082 27239 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' for gc 6.99998877336889days in the future I0714 15:08:47.970379 27242 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021' for gc 6.99998876968889days in the future I0714 15:08:47.970587 27238 slave.cpp:2735] Cleaning up framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:47.970960 27237 status_update_manager.cpp:282] Closing status update streams for framework 20140714-150843-16842879-55850-27216-0000 I0714 15:08:47.971225 27236 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000' for gc 6.99998875966519days in the future I0714 15:08:47.971549 27241 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000' for gc 6.99998875612148days in the future W0714 15:08:47.975971 27235 containerizer.cpp:893] Ignoring destroy of unknown container: 19c466f8-bb5a-4842-a152-f585ff88762a ./tests/cluster.hpp:530: Failure (wait).failure(): Unknown container: 19c466f8-bb5a-4842-a152-f585ff88762a # # A fatal error has been detected by the Java Runtime Environment: # #  SIGSEGV (0xb) at pc=0x00000000005a0299, pid=27216, tid=47907931709760 # # JRE version: OpenJDK Runtime Environment (7.0_55-b14) (build 1.7.0_55-b14) # Java VM: OpenJDK 64-Bit Server VM (24.51-b03 mixed mode linux-amd64 compressed oops) # Problematic frame: # C  [lt-mesos-tests+0x1a0299]  mlock@@GLIBC_2.2.5+0x1a0299 # # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again # # An error report file with more information is saved as: # /var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/hs_err_pid27216.log # # If you would like to submit a bug report, please include # instructions on how to reproduce the bug and visit: #   http://icedtea.classpath.org/bugzilla # The crash happened outside the Java Virtual Machine in native code. # See problematic frame for where to report the bug. # make[3]: *** [check-local] Aborted make[3]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src' make[2]: *** [check-am] Error 2 make[2]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src' make[1]: *** [check] Error 2 make[1]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src' make: *** [check-recursive] Error 1 Build step 'Execute shell' marked build as failure erreicht: 1854109 Sending e-mails to: kernel-test@twitter.com apache-mesos@twitter.com Finished: FAILURE  Help us localize this page Page generated: Jul 14, 2014 5:57:17 PMREST  {code}",1
"MESOS-1676","ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession is flaky","{noformat:title=} [ RUN      ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession I0806 01:18:37.648684 17458 zookeeper_test_server.cpp:158] Started ZooKeeperTestServer on port 42069 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1682db0 flags=0 2014-08-06 01:18:37,656:17458(0x2b468638b700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:37,669:17458(0x2b468638b700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0000, negotiated timeout=6000 I0806 01:18:37.671725 17486 group.cpp:313] Group process (group(37)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:37.671758 17486 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:37.671771 17486 group.cpp:385] Trying to create path '/mesos' in ZooKeeper 2014-08-06 01:18:39,101:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:42,441:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0806 01:18:42.656673 17481 contender.cpp:131] Joining the ZK group I0806 01:18:42.662484 17484 contender.cpp:247] New candidate (id='0') has entered the contest for leadership I0806 01:18:42.663754 17481 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:42.663884 17481 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:42.664788 17483 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x15c00f0 flags=0 2014-08-06 01:18:42,668:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:42,672:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000 I0806 01:18:42.673542 17485 group.cpp:313] Group process (group(38)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:42.673570 17485 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:42.673580 17485 group.cpp:385] Trying to create path '/mesos' in ZooKeeper 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 131ms) 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 115ms) 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms 2014-08-06 01:18:46,799:17458(0x2b4687394700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 1025ms 2014-08-06 01:18:46,800:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0806 01:18:46.806895 17486 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:46.807857 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:47.669064 17482 contender.cpp:131] Joining the ZK group 2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2989ms 2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:47,671:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000 I0806 01:18:47.682868 17485 contender.cpp:247] New candidate (id='1') has entered the contest for leadership I0806 01:18:47.683404 17482 group.cpp:313] Group process (group(38)@127.0.1.1:55561) reconnected to ZooKeeper I0806 01:18:47.683445 17482 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:47.685998 17482 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:47.686142 17482 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:47.687289 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c0421c0 flags=0 2014-08-06 01:18:47,699:17458(0x2b4687de6700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:47,712:17458(0x2b4687de6700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0002, negotiated timeout=6000 I0806 01:18:47.712846 17479 group.cpp:313] Group process (group(39)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:47.712873 17479 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:47.712882 17479 group.cpp:385] Trying to create path '/mesos' in ZooKeeper I0806 01:18:47.714648 17479 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:47.714759 17479 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:47.716130 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:47,718:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response I0806 01:18:47.718889 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... 2014-08-06 01:18:47,720:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response I0806 01:18:47.720788 17484 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:47.724663 17458 zookeeper_test_server.cpp:122] Shutdown ZooKeeperTestServer on port 42069 2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 4133ms 2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:49,720:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 33ms 2014-08-06 01:18:49,721:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:49,722:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:50,136:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:50,800:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:51,723:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:51,723:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:52,801:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:52.842553 17481 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0000) expiration I0806 01:18:52.842911 17481 group.cpp:472] ZooKeeper session expired I0806 01:18:52.843468 17485 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:52.843483 17485 detector.cpp:138] Detected a new leader: None I0806 01:18:52.843618 17485 contender.cpp:196] Membership cancelled: 0 2014-08-06 01:18:52,843:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0000  2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1349ad0 flags=0 2014-08-06 01:18:52,844:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:53,473:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:53.720684 17480 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0001) expiration I0806 01:18:53.721132 17480 group.cpp:472] ZooKeeper session expired I0806 01:18:53.721516 17479 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:53.721534 17479 detector.cpp:138] Detected a new leader: None I0806 01:18:53.721696 17479 contender.cpp:196] Membership cancelled: 1 2014-08-06 01:18:53,721:17458(0x2b46798a3700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0001  2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x16a0550 flags=0 2014-08-06 01:18:53,723:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:53,726:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:53.730258 17479 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0002) expiration I0806 01:18:53.730736 17479 group.cpp:472] ZooKeeper session expired I0806 01:18:53.731081 17481 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:53.731132 17481 detector.cpp:138] Detected a new leader: None 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0002  2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c035f30 flags=0 2014-08-06 01:18:53,733:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:54,512:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:55,393:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:55,403:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:56,301:17458(0x2b468698f700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 122ms 2014-08-06 01:18:56,302:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:56,809:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:57,939:17458(0x2b4686f92700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 879ms 2014-08-06 01:18:57,940:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 870ms 2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client tests/master_contender_detector_tests.cpp:574: Failure Failed to wait 10secs for leaderReconnecting 2014-08-06 01:18:57,941:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  I0806 01:18:57.949972 17458 contender.cpp:186] Now cancelling the membership: 1 2014-08-06 01:18:57,950:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  I0806 01:18:57.950731 17458 contender.cpp:186] Now cancelling the membership: 0 2014-08-06 01:18:57,951:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  ../3rdparty/libprocess/include/process/gmock.hpp:298: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const DispatchEvent&>()))...     Expected args: dispatch matcher (1, 16-byte object <50-20 4A-00 00-00 00-00 00-00 00-00 00-00 00-00>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession (20308 ms) {noformat}",1
"MESOS-1683","Create user doc for framework rate limiting feature","Create a Markdown doc under /docs",2
"MESOS-1695","The stats.json endpoint on the slave exposes ""registered"" as a string.","The slave is currently exposing a string value for the ""registered"" statistic, this should be a number:  {code} slave:5051/stats.json {   ""recovery_errors"": 0,   ""registered"": ""1"",   ""slave/executors_registering"": 0,   ... } {code}  Should be a pretty straightforward fix, looks like this first originated back in 2013:  {code} commit b8291304e1523eb67ea8dc5f195cdb0d8e7d8348 Author: Vinod Kone <vinod@twitter.com> Date:   Wed Jul 3 12:37:36 2013 -0700      Added a ""registered"" key/value pair to slave's stats.json.      Review: https://reviews.apache.org/r/12256  diff --git a/src/slave/http.cpp b/src/slave/http.cpp index dc2955f..dd51516 100644 --- a/src/slave/http.cpp +++ b/src/slave/http.cpp @@ -281,6 +281,8 @@ Future<Response> Slave::Http::stats(const Request& request)    object.values[""lost_tasks""] = slave.stats.tasks[TASK_LOST];    object.values[""valid_status_updates""] = slave.stats.validStatusUpdates;    object.values[""invalid_status_updates""] = slave.stats.invalidStatusUpdates; +  object.values[""registered""] = slave.master ? ""1"" : ""0""; +     return OK(object, request.query.get(""jsonp""));  } {code}",1
"MESOS-1698","make check segfaults","Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2331/consoleFull  It looks like the segfault happens before any tests are run. So I suspect somewhere in the setup phase of the tests.  {code} mv -f .deps/tests-time_tests.Tpo .deps/tests-time_tests.Po /bin/bash ./libtool  --tag=CXX   --mode=link g++  -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11   -o tests tests-decoder_tests.o tests-encoder_tests.o tests-http_tests.o tests-io_tests.o tests-main.o tests-mutex_tests.o tests-metrics_tests.o tests-owned_tests.o tests-process_tests.o tests-queue_tests.o tests-reap_tests.o tests-sequence_tests.o tests-shared_tests.o tests-statistics_tests.o tests-subprocess_tests.o tests-system_tests.o tests-timeseries_tests.o tests-time_tests.o 3rdparty/libgmock.la libprocess.la 3rdparty/glog-0.3.3/libglog.la 3rdparty/libry_http_parser.la 3rdparty/libev-4.15/libev.la -lz  -lrt libtool: link: g++ -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -o tests tests-decoder_tests.o tests-encoder_tests.o tests-http_tests.o tests-io_tests.o tests-main.o tests-mutex_tests.o tests-metrics_tests.o tests-owned_tests.o tests-process_tests.o tests-queue_tests.o tests-reap_tests.o tests-sequence_tests.o tests-shared_tests.o tests-statistics_tests.o tests-subprocess_tests.o tests-system_tests.o tests-timeseries_tests.o tests-time_tests.o  3rdparty/.libs/libgmock.a ./.libs/libprocess.a /home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess/3rdparty/glog-0.3.3/.libs/libglog.a /home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess/3rdparty/libev-4.15/.libs/libev.a 3rdparty/glog-0.3.3/.libs/libglog.a -lpthread 3rdparty/.libs/libry_http_parser.a 3rdparty/libev-4.15/.libs/libev.a -lm -lz -lrt make[5]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess' make  check-local make[5]: Entering directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess' ./tests Note: Google Test filter =  [==========] Running 0 tests from 0 test cases. [==========] 0 tests from 0 test cases ran. (0 ms total) [  PASSED  ] 0 tests.    YOU HAVE 3 DISABLED TESTS  make[5]: *** [check-local] Segmentation fault make[5]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess' make[4]: *** [check-am] Error 2 make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess' make[3]: *** [check-recursive] Error 1 make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess' make[2]: *** [check-recursive] Error 1 make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty' make[1]: *** [check] Error 2 make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty' make: *** [check-recursive] Error 1 Build step 'Execute shell' marked build as failure Sending e-mails to: dev@mesos.apache.org benjamin.hindman@gmail.com dhamon@twitter.com yujie.jay@gmail.com Finished: FAILURE {code}",2
"MESOS-1705","SubprocessTest.Status sometimes flakes out","It's a pretty rare event, but happened more then once.    [ RUN      ] SubprocessTest.Status *** Aborted at 1408023909 (unix time) try ""date -d @1408023909"" if you are using GNU date *** PC: @       0x35700094b1 (unknown) *** SIGTERM (@0x3e8000041d8) received by PID 16872 (TID 0x7fa9ea426780) from PID 16856; stack trace: ***     @       0x3570435cb0 (unknown)     @       0x35700094b1 (unknown)     @       0x3570009d9f (unknown)     @       0x357000e726 (unknown)     @       0x3570015185 (unknown)     @           0x5ead42 process::childMain()     @           0x5ece8d std::_Function_handler<>::_M_invoke()     @           0x5eac9c process::defaultClone()     @           0x5ebbd4 process::subprocess()     @           0x55a229 process::subprocess()     @           0x55a846 process::subprocess()     @           0x54224c SubprocessTest_Status_Test::TestBody()     @     0x7fa9ea460323 (unknown)     @     0x7fa9ea455b67 (unknown)     @     0x7fa9ea455c0e (unknown)     @     0x7fa9ea455d15 (unknown)     @     0x7fa9ea4593a8 (unknown)     @     0x7fa9ea459647 (unknown)     @           0x422466 main     @       0x3570421d65 (unknown)     @           0x4260bd (unknown) [       OK ] SubprocessTest.Status (153 ms)",2
"MESOS-1712","Automate disallowing of commits mixing mesos/libprocess/stout","For various reasons, we don't want to mix mesos/libprocess/stout changes into a single commit. Typically, it is up to the reviewee/reviewer to catch this.   It wold be nice to automate this via the pre-commit hook .",2
"MESOS-1727","Configure fails with ../configure: line 18439: syntax error near unexpected token `PROTOBUFPREFIX,'","I followed the ""Getting started"" documentation and did: {noformat} $ git clone http://git-wip-us.apache.org/repos/asf/mesos.git; cd mesos $ ./bootstrap $ mkdir build; cd build $ ../configure {noformat} which aborts with {noformat} .... .... checking whether we are using the GNU C compiler... (cached) yes checking whether gcc accepts -g... (cached) yes checking for gcc option to accept ISO C89... (cached) none needed checking dependency style of gcc... (cached) gcc3 ../configure: line 18439: syntax error near unexpected token `PROTOBUFPREFIX,' ../configure: line 18439: `  PKG_CHECK_MODULES(PROTOBUFPREFIX,' {noformat}",2
"MESOS-1728","Libprocess: report bind parameters on failure","When you attempt to start slave or master and there's another one already running there, it is nice to report what are the actual parameters to {{bind}} call that failed.",1
"MESOS-1748","MasterZooKeeperTest.LostZooKeeperCluster is flaky","{noformat:title=} tests/master_tests.cpp:1795: Failure Failed to wait 10secs for slaveRegisteredMessage {noformat}  Should have placed the FUTURE_MESSAGE that attempts to capture this messages before the slave starts...",1
"MESOS-1758","Freezer failure leads to lost task during container destruction.","In the past we've seen numerous issues around the freezer. Lately, on the 2.6.44 kernel, we've seen issues where we're unable to freeze the cgroup:  (1) An oom occurs. (2) No indication of oom in the kernel logs. (3) The slave is unable to freeze the cgroup. (4) The task is marked as lost.  {noformat} I0903 16:46:24.956040 25469 mem.cpp:575] Memory limit exceeded: Requested: 15488MB Maximum Used: 15488MB  MEMORY STATISTICS: cache 7958691840 rss 8281653248 mapped_file 9474048 pgpgin 4487861 pgpgout 522933 pgfault 2533780 pgmajfault 11 inactive_anon 0 active_anon 8281653248 inactive_file 7631708160 active_file 326852608 unevictable 0 hierarchical_memory_limit 16240345088 total_cache 7958691840 total_rss 8281653248 total_mapped_file 9474048 total_pgpgin 4487861 total_pgpgout 522933 total_pgfault 2533780 total_pgmajfault 11 total_inactive_anon 0 total_active_anon 8281653248 total_inactive_file 7631728640 total_active_file 326852608 total_unevictable 0 I0903 16:46:24.956848 25469 containerizer.cpp:1041] Container bbb9732a-d600-4c1b-b326-846338c608c3 has reached its limit for resource mem(*):1.62403e+10 and will be terminated I0903 16:46:24.957427 25469 containerizer.cpp:909] Destroying container 'bbb9732a-d600-4c1b-b326-846338c608c3' I0903 16:46:24.958664 25481 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:46:34.959529 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:46:34.962070 25482 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.710848ms I0903 16:46:34.962658 25479 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:46:44.963349 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:46:44.965631 25472 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.588224ms I0903 16:46:44.966356 25472 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:46:54.967254 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:46:56.008447 25475 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 2.15296ms I0903 16:46:56.009071 25466 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:47:06.010329 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:47:06.012538 25467 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.643008ms I0903 16:47:06.013216 25467 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:47:12.516348 25480 slave.cpp:3030] Current usage 9.57%. Max allowed age: 5.630238827780799days I0903 16:47:16.015192 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:47:16.017043 25486 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.511168ms I0903 16:47:16.017555 25480 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:47:19.862746 25483 http.cpp:245] HTTP request for '/slave(1)/stats.json' E0903 16:47:24.960055 25472 slave.cpp:2557] Termination of executor 'E' of framework '201104070004-0000002563-0000' failed: Failed to destroy container: discarded future I0903 16:47:24.962054 25472 slave.cpp:2087] Handling status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 from @0.0.0.0:0 I0903 16:47:24.963470 25469 mem.cpp:293] Updated 'memory.soft_limit_in_bytes' to 128MB for container bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:47:24.963541 25471 cpushare.cpp:338] Updated 'cpu.shares' to 256 (cpus 0.25) for container bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:47:24.964756 25471 cpushare.cpp:359] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 25ms (cpus 0.25) for container bbb9732a-d600-4c1b-b326-846338c608c3 I0903 16:47:43.406610 25476 status_update_manager.cpp:320] Received status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 I0903 16:47:43.406991 25476 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 I0903 16:47:43.410475 25476 status_update_manager.cpp:373] Forwarding status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 to master@<scrubbed_ip>:5050 I0903 16:47:43.439923 25480 status_update_manager.cpp:398] Received status update acknowledgement (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 I0903 16:47:43.440115 25480 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 I0903 16:47:43.443595 25480 slave.cpp:2709] Cleaning up executor 'E' of framework 201104070004-0000002563-0000 {noformat}  We should consider avoiding the freezer entirely in favor of a kill(2) loop. We don't have to wait for pid namespaces to remove the freezer dependency.  At the very least, when the freezer fails, we should proceed with a kill(2) loop to ensure that we destroy the cgroup.",2
"MESOS-1760","MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky","Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2355/changes  {code} [ RUN] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_0tw16Z' I0903 22:04:33.520237 25565 leveldb.cpp:176] Opened db in 49.073821ms I0903 22:04:33.538331 25565 leveldb.cpp:183] Compacted db in 18.065051ms I0903 22:04:33.538363 25565 leveldb.cpp:198] Created db iterator in 4826ns I0903 22:04:33.538377 25565 leveldb.cpp:204] Seeked to beginning of db in 682ns I0903 22:04:33.538385 25565 leveldb.cpp:273] Iterated through 0 keys in the db in 312ns I0903 22:04:33.538399 25565 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0903 22:04:33.538624 25593 recover.cpp:425] Starting replica recovery I0903 22:04:33.538707 25598 recover.cpp:451] Replica is in EMPTY status I0903 22:04:33.540909 25590 master.cpp:286] Master 20140903-220433-453759884-44122-25565 (hemera.apache.org) started on 140.211.11.27:44122 I0903 22:04:33.540932 25590 master.cpp:332] Master only allowing authenticated frameworks to register I0903 22:04:33.540936 25590 master.cpp:337] Master only allowing authenticated slaves to register I0903 22:04:33.540941 25590 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_0tw16Z/credentials' I0903 22:04:33.541337 25590 master.cpp:366] Authorization enabled I0903 22:04:33.541508 25597 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I0903 22:04:33.542343 25582 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@140.211.11.27:44122 I0903 22:04:33.542445 25592 master.cpp:120] No whitelist given. Advertising offers for all slaves I0903 22:04:33.543175 25602 recover.cpp:188] Received a recover response from a replica in EMPTY status I0903 22:04:33.543637 25587 recover.cpp:542] Updating replica status to STARTING I0903 22:04:33.544256 25579 master.cpp:1205] The newly elected leader is master@140.211.11.27:44122 with id 20140903-220433-453759884-44122-25565 I0903 22:04:33.544275 25579 master.cpp:1218] Elected as the leading master! I0903 22:04:33.544282 25579 master.cpp:1036] Recovering from registrar I0903 22:04:33.544401 25579 registrar.cpp:313] Recovering registrar I0903 22:04:33.558487 25593 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.678563ms I0903 22:04:33.558531 25593 replica.cpp:320] Persisted replica status to STARTING I0903 22:04:33.558653 25593 recover.cpp:451] Replica is in STARTING status I0903 22:04:33.559867 25588 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I0903 22:04:33.560057 25602 recover.cpp:188] Received a recover response from a replica in STARTING status I0903 22:04:33.561280 25584 recover.cpp:542] Updating replica status to VOTING I0903 22:04:33.576900 25581 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.712427ms I0903 22:04:33.576942 25581 replica.cpp:320] Persisted replica status to VOTING I0903 22:04:33.577018 25581 recover.cpp:556] Successfully joined the Paxos group I0903 22:04:33.577108 25581 recover.cpp:440] Recover process terminated I0903 22:04:33.577401 25581 log.cpp:656] Attempting to start the writer I0903 22:04:33.578559 25589 replica.cpp:474] Replica received implicit promise request with proposal 1 I0903 22:04:33.594611 25589 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.029152ms I0903 22:04:33.594640 25589 replica.cpp:342] Persisted promised to 1 I0903 22:04:33.595391 25584 coordinator.cpp:230] Coordinator attemping to fill missing position I0903 22:04:33.597512 25588 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I0903 22:04:33.613037 25588 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 15.502568ms I0903 22:04:33.613065 25588 replica.cpp:676] Persisted action at 0 I0903 22:04:33.615435 25585 replica.cpp:508] Replica received write request for position 0 I0903 22:04:33.615463 25585 leveldb.cpp:438] Reading position from leveldb took 10743ns I0903 22:04:33.630801 25585 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 15.320225ms I0903 22:04:33.630852 25585 replica.cpp:676] Persisted action at 0 I0903 22:04:33.631126 25585 replica.cpp:655] Replica received learned notice for position 0 I0903 22:04:33.647801 25585 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 16.652951ms I0903 22:04:33.647830 25585 replica.cpp:676] Persisted action at 0 I0903 22:04:33.647842 25585 replica.cpp:661] Replica learned NOP action at position 0 I0903 22:04:33.648548 25583 log.cpp:672] Writer started with ending position 0 I0903 22:04:33.649235 25583 leveldb.cpp:438] Reading position from leveldb took 25209ns I0903 22:04:33.650897 25591 registrar.cpp:346] Successfully fetched the registry (0B) I0903 22:04:33.650930 25591 registrar.cpp:422] Attempting to update the 'registry' I0903 22:04:33.652861 25601 log.cpp:680] Attempting to append 138 bytes to the log I0903 22:04:33.653097 25586 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0903 22:04:33.655225 25590 replica.cpp:508] Replica received write request for position 1 I0903 22:04:33.669618 25590 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 14.337486ms I0903 22:04:33.669663 25590 replica.cpp:676] Persisted action at 1 I0903 22:04:33.670045 25584 replica.cpp:655] Replica received learned notice for position 1 I0903 22:04:34.414243 25584 leveldb.cpp:343] Persisting action (159 bytes) to leveldb took 15.401247ms I0903 22:04:34.414300 25584 replica.cpp:676] Persisted action at 1 I0903 22:04:34.414316 25584 replica.cpp:661] Replica learned APPEND action at position 1 I0903 22:04:34.414937 25589 registrar.cpp:479] Successfully updated 'registry' I0903 22:04:34.415069 25585 log.cpp:699] Attempting to truncate the log to 1 I0903 22:04:34.415194 25589 registrar.cpp:372] Successfully recovered registrar I0903 22:04:34.415284 25589 master.cpp:1063] Recovered 0 slaves from the Registry (100B) ; allowing 10mins for slaves to re-register I0903 22:04:34.415362 25587 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0903 22:04:34.418926 25597 replica.cpp:508] Replica received write request for position 2 I0903 22:04:34.434321 25597 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 15.368147ms I0903 22:04:34.434352 25597 replica.cpp:676] Persisted action at 2 I0903 22:04:34.435022 25582 replica.cpp:655] Replica received learned notice for position 2 I0903 22:04:34.450331 25582 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.284486ms I0903 22:04:34.450387 25582 leveldb.cpp:401] Deleting ~1 keys from leveldb took 25774ns I0903 22:04:34.450402 25582 replica.cpp:676] Persisted action at 2 I0903 22:04:34.450412 25582 replica.cpp:661] Replica learned TRUNCATE action at position 2 I0903 22:04:34.460691 25565 sched.cpp:137] Version: 0.21.0 I0903 22:04:34.460927 25582 sched.cpp:233] New master detected at master@140.211.11.27:44122 I0903 22:04:34.460948 25582 sched.cpp:283] Authenticating with master master@140.211.11.27:44122 I0903 22:04:34.461359 25582 authenticatee.hpp:128] Creating new client SASL connection I0903 22:04:34.461647 25582 master.cpp:3637] Authenticating scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122 I0903 22:04:34.461801 25598 authenticator.hpp:156] Creating new server SASL connection I0903 22:04:34.462172 25598 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0903 22:04:34.462185 25598 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0903 22:04:34.462257 25598 authenticator.hpp:262] Received SASL authentication start I0903 22:04:34.462323 25598 authenticator.hpp:384] Authentication requires more steps I0903 22:04:34.462345 25598 authenticatee.hpp:265] Received SASL authentication step I0903 22:04:34.462417 25598 authenticator.hpp:290] Received SASL authentication step I0903 22:04:34.462522 25598 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false I0903 22:04:34.462529 25598 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0903 22:04:34.462538 25598 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0903 22:04:34.462543 25598 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true I0903 22:04:34.462548 25598 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0903 22:04:34.462550 25598 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0903 22:04:34.462558 25598 authenticator.hpp:376] Authentication success I0903 22:04:34.462635 25598 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122 I0903 22:04:34.462687 25590 authenticatee.hpp:305] Authentication success I0903 22:04:34.463219 25588 sched.cpp:357] Successfully authenticated with master master@140.211.11.27:44122 I0903 22:04:34.463243 25588 sched.cpp:476] Sending registration request to master@140.211.11.27:44122 I0903 22:04:34.463307 25588 master.cpp:1324] Received registration request from scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122 I0903 22:04:34.463330 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*' I0903 22:04:34.463412 25588 master.cpp:1383] Registering framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122 I0903 22:04:34.463577 25598 sched.cpp:407] Framework registered with 20140903-220433-453759884-44122-25565-0000 I0903 22:04:34.463728 25587 hierarchical_allocator_process.hpp:329] Added framework 20140903-220433-453759884-44122-25565-0000 I0903 22:04:34.463739 25587 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0903 22:04:34.463743 25587 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5016ns I0903 22:04:34.463755 25598 sched.cpp:421] Scheduler::registered took 165035ns I0903 22:04:34.465558 25583 sched.cpp:227] Scheduler::disconnected took 6254ns I0903 22:04:34.465566 25583 sched.cpp:233] New master detected at master@140.211.11.27:44122 I0903 22:04:34.465575 25583 sched.cpp:283] Authenticating with master master@140.211.11.27:44122 I0903 22:04:34.465642 25583 authenticatee.hpp:128] Creating new client SASL connection I0903 22:04:34.465790 25583 master.cpp:1680] Deactivating framework 20140903-220433-453759884-44122-25565-0000 I0903 22:04:34.465850 25583 master.cpp:3637] Authenticating scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122 I0903 22:04:34.465879 25601 hierarchical_allocator_process.hpp:405] Deactivated framework 20140903-220433-453759884-44122-25565-0000 I0903 22:04:34.466047 25600 authenticator.hpp:156] Creating new server SASL connection I0903 22:04:34.466315 25600 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0903 22:04:34.466326 25600 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0903 22:04:34.466346 25600 authenticator.hpp:262] Received SASL authentication start I0903 22:04:34.466418 25600 authenticator.hpp:384] Authentication requires more steps I0903 22:04:34.466436 25600 authenticatee.hpp:265] Received SASL authentication step I0903 22:04:34.466475 25600 authenticator.hpp:290] Received SASL authentication step I0903 22:04:34.466486 25600 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false I0903 22:04:34.466491 25600 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0903 22:04:34.466496 25600 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0903 22:04:34.466502 25600 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true I0903 22:04:34.466506 25600 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0903 22:04:34.466509 25600 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0903 22:04:34.466516 25600 authenticator.hpp:376] Authentication success I0903 22:04:34.466596 25588 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122 I0903 22:04:34.466629 25597 authenticatee.hpp:305] Authentication success I0903 22:04:34.467062 25594 sched.cpp:357] Successfully authenticated with master master@140.211.11.27:44122 I0903 22:04:34.467077 25594 sched.cpp:476] Sending registration request to master@140.211.11.27:44122 I0903 22:04:34.467190 25588 master.cpp:1448] Received re-registration request from framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122 I0903 22:04:36.368134 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*' I0903 22:04:34.542999 25594 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0903 22:04:35.463639 25582 sched.cpp:476] Sending registration request to master@140.211.11.27:44122 I0903 22:04:36.368185 25594 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.825177748secs I0903 22:04:36.368302 25588 master.cpp:1448] Received re-registration request from framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122 I0903 22:04:36.368330 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*' I0903 22:04:36.368388 25582 sched.cpp:476] Sending registration request to master@140.211.11.27:44122 : Failure Mock function called more times than expected - returning default value.     Function call: authorize(@0x2ba11964c1b0 40-byte object <D0-ED 39-16 A1-2B 00-00 00-00 00-00 00-00 00-00 00-6C 01-3C A1-2B 00-00 30-20 00-3C A1-2B 00-00 00-00 00-00 03-00 00-00>)     The mock function has no default action set, and its return type has no default value set. *** Aborted at 1409781876 (unix time) try ""date -d @1409781876"" if you are using GNU date *** I0903 22:04:36.368913 25598 sched.cpp:745] Stopping framework '20140903-220433-453759884-44122-25565-0000' PC: @     0x2ba117a990d5 (unknown) *** SIGABRT (@0x3ea000063dd) received by PID 25565 (TID 0x2ba11964d700) from PID 25565; stack trace: ***     @     0x2ba117854cb0 (unknown)     @     0x2ba117a990d5 (unknown)     @     0x2ba117a9c83b (unknown)     @           0x9cba9d testing::internal::GoogleTestFailureReporter::ReportFailure()     @           0x790091 testing::internal::FunctionMockerBase<>::PerformDefaultAction()     @           0x790166 testing::internal::FunctionMockerBase<>::UntypedPerformDefaultAction()     @           0x9c3daa testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()     @           0x787279 mesos::internal::tests::MockAuthorizer::authorize()     @     0x2ba1157c133d mesos::internal::master::Master::validate()     @     0x2ba1157c2b7a mesos::internal::master::Master::reregisterFramework()     @     0x2ba1157e0038 ProtobufProcess<>::handler2<>()     @     0x2ba1157dde89 std::tr1::_Function_handler<>::_M_invoke()     @     0x2ba1157b15f7 mesos::internal::master::Master::_visit()     @     0x2ba1157bfa3e mesos::internal::master::Master::visit()     @     0x2ba115caf5e7 process::ProcessManager::resume()     @     0x2ba115cb027c process::schedule()     @     0x2ba11784ce9a start_thread     @     0x2ba117b5731d (unknown) {code}",1
"MESOS-1765","Use PID namespace to avoid freezing cgroup","There is some known kernel issue when we freeze the whole cgroup upon OOM. Mesos probably can just use PID namespace so that we will only need to kill the ""init"" of the pid namespace, instead of freezing all the processes and killing them one by one. But I am not quite sure if this would break the existing code.",5
"MESOS-1766","MasterAuthorizationTest.DuplicateRegistration test is flaky","{code} [ RUN      ] MasterAuthorizationTest.DuplicateRegistration Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateRegistration_pVJg7m' I0905 15:53:16.398993 25769 leveldb.cpp:176] Opened db in 2.601036ms I0905 15:53:16.399566 25769 leveldb.cpp:183] Compacted db in 546216ns I0905 15:53:16.399590 25769 leveldb.cpp:198] Created db iterator in 2787ns I0905 15:53:16.399605 25769 leveldb.cpp:204] Seeked to beginning of db in 500ns I0905 15:53:16.399617 25769 leveldb.cpp:273] Iterated through 0 keys in the db in 185ns I0905 15:53:16.399633 25769 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0905 15:53:16.399817 25786 recover.cpp:425] Starting replica recovery I0905 15:53:16.399952 25793 recover.cpp:451] Replica is in EMPTY status I0905 15:53:16.400683 25795 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I0905 15:53:16.400795 25787 recover.cpp:188] Received a recover response from a replica in EMPTY status I0905 15:53:16.401005 25783 recover.cpp:542] Updating replica status to STARTING I0905 15:53:16.401470 25786 master.cpp:286] Master 20140905-155316-3125920579-49188-25769 (penates.apache.org) started on 67.195.81.186:49188 I0905 15:53:16.401521 25786 master.cpp:332] Master only allowing authenticated frameworks to register I0905 15:53:16.401533 25786 master.cpp:337] Master only allowing authenticated slaves to register I0905 15:53:16.401543 25786 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateRegistration_pVJg7m/credentials' I0905 15:53:16.401558 25793 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 474683ns I0905 15:53:16.401582 25793 replica.cpp:320] Persisted replica status to STARTING I0905 15:53:16.401667 25793 recover.cpp:451] Replica is in STARTING status I0905 15:53:16.401669 25786 master.cpp:366] Authorization enabled I0905 15:53:16.401898 25795 master.cpp:120] No whitelist given. Advertising offers for all slaves I0905 15:53:16.401936 25796 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.186:49188 I0905 15:53:16.402160 25784 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I0905 15:53:16.402333 25790 master.cpp:1205] The newly elected leader is master@67.195.81.186:49188 with id 20140905-155316-3125920579-49188-25769 I0905 15:53:16.402359 25790 master.cpp:1218] Elected as the leading master! I0905 15:53:16.402371 25790 master.cpp:1036] Recovering from registrar I0905 15:53:16.402472 25798 registrar.cpp:313] Recovering registrar I0905 15:53:16.402529 25791 recover.cpp:188] Received a recover response from a replica in STARTING status I0905 15:53:16.402782 25788 recover.cpp:542] Updating replica status to VOTING I0905 15:53:16.403002 25795 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 116403ns I0905 15:53:16.403020 25795 replica.cpp:320] Persisted replica status to VOTING I0905 15:53:16.403081 25791 recover.cpp:556] Successfully joined the Paxos group I0905 15:53:16.403197 25791 recover.cpp:440] Recover process terminated I0905 15:53:16.403388 25796 log.cpp:656] Attempting to start the writer I0905 15:53:16.403993 25784 replica.cpp:474] Replica received implicit promise request with proposal 1 I0905 15:53:16.404147 25784 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 132156ns I0905 15:53:16.404167 25784 replica.cpp:342] Persisted promised to 1 I0905 15:53:16.404542 25795 coordinator.cpp:230] Coordinator attemping to fill missing position I0905 15:53:16.405498 25787 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I0905 15:53:16.405868 25787 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 347231ns I0905 15:53:16.405886 25787 replica.cpp:676] Persisted action at 0 I0905 15:53:16.406553 25788 replica.cpp:508] Replica received write request for position 0 I0905 15:53:16.406582 25788 leveldb.cpp:438] Reading position from leveldb took 11402ns I0905 15:53:16.529067 25788 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 535803ns I0905 15:53:16.529088 25788 replica.cpp:676] Persisted action at 0 I0905 15:53:16.529355 25784 replica.cpp:655] Replica received learned notice for position 0 I0905 15:53:16.529784 25784 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 406036ns I0905 15:53:16.529806 25784 replica.cpp:676] Persisted action at 0 I0905 15:53:16.529817 25784 replica.cpp:661] Replica learned NOP action at position 0 I0905 15:53:16.530108 25783 log.cpp:672] Writer started with ending position 0 I0905 15:53:16.530597 25792 leveldb.cpp:438] Reading position from leveldb took 14594ns I0905 15:53:16.532060 25787 registrar.cpp:346] Successfully fetched the registry (0B) I0905 15:53:16.532091 25787 registrar.cpp:422] Attempting to update the 'registry' I0905 15:53:16.533537 25785 log.cpp:680] Attempting to append 140 bytes to the log I0905 15:53:16.533596 25785 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0905 15:53:16.533998 25798 replica.cpp:508] Replica received write request for position 1 I0905 15:53:16.534397 25798 leveldb.cpp:343] Persisting action (159 bytes) to leveldb took 372452ns I0905 15:53:16.534416 25798 replica.cpp:676] Persisted action at 1 I0905 15:53:16.534808 25793 replica.cpp:655] Replica received learned notice for position 1 I0905 15:53:16.534996 25793 leveldb.cpp:343] Persisting action (161 bytes) to leveldb took 164609ns I0905 15:53:16.535014 25793 replica.cpp:676] Persisted action at 1 I0905 15:53:16.535025 25793 replica.cpp:661] Replica learned APPEND action at position 1 I0905 15:53:16.535368 25784 registrar.cpp:479] Successfully updated 'registry' I0905 15:53:16.535419 25784 registrar.cpp:372] Successfully recovered registrar I0905 15:53:16.535452 25785 log.cpp:699] Attempting to truncate the log to 1 I0905 15:53:16.535555 25791 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0905 15:53:16.535553 25792 master.cpp:1063] Recovered 0 slaves from the Registry (102B) ; allowing 10mins for slaves to re-register I0905 15:53:16.536038 25784 replica.cpp:508] Replica received write request for position 2 I0905 15:53:16.536166 25784 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 101619ns I0905 15:53:16.536185 25784 replica.cpp:676] Persisted action at 2 I0905 15:53:16.536497 25791 replica.cpp:655] Replica received learned notice for position 2 I0905 15:53:16.536633 25791 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 109281ns I0905 15:53:16.536664 25791 leveldb.cpp:401] Deleting ~1 keys from leveldb took 14164ns I0905 15:53:16.536677 25791 replica.cpp:676] Persisted action at 2 I0905 15:53:16.536689 25791 replica.cpp:661] Replica learned TRUNCATE action at position 2 I0905 15:53:16.548408 25769 sched.cpp:137] Version: 0.21.0 I0905 15:53:16.548627 25792 sched.cpp:233] New master detected at master@67.195.81.186:49188 I0905 15:53:16.548653 25792 sched.cpp:283] Authenticating with master master@67.195.81.186:49188 I0905 15:53:16.548857 25797 authenticatee.hpp:128] Creating new client SASL connection I0905 15:53:16.548950 25797 master.cpp:3637] Authenticating scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188 I0905 15:53:16.549041 25797 authenticator.hpp:156] Creating new server SASL connection I0905 15:53:16.549120 25797 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0905 15:53:16.549141 25797 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0905 15:53:16.549180 25797 authenticator.hpp:262] Received SASL authentication start I0905 15:53:16.549229 25797 authenticator.hpp:384] Authentication requires more steps I0905 15:53:16.549268 25797 authenticatee.hpp:265] Received SASL authentication step I0905 15:53:16.549351 25787 authenticator.hpp:290] Received SASL authentication step I0905 15:53:16.549378 25787 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false I0905 15:53:16.549391 25787 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0905 15:53:16.549403 25787 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0905 15:53:16.549415 25787 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true I0905 15:53:16.549424 25787 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0905 15:53:16.549432 25787 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0905 15:53:16.549448 25787 authenticator.hpp:376] Authentication success I0905 15:53:16.549489 25787 authenticatee.hpp:305] Authentication success I0905 15:53:16.549525 25787 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188 I0905 15:53:16.549669 25783 sched.cpp:357] Successfully authenticated with master master@67.195.81.186:49188 I0905 15:53:16.549690 25783 sched.cpp:476] Sending registration request to master@67.195.81.186:49188 I0905 15:53:16.549751 25787 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188 I0905 15:53:16.549782 25787 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*' I0905 15:53:16.551250 25791 sched.cpp:233] New master detected at master@67.195.81.186:49188 I0905 15:53:16.551273 25791 sched.cpp:283] Authenticating with master master@67.195.81.186:49188 I0905 15:53:16.551357 25788 authenticatee.hpp:128] Creating new client SASL connection I0905 15:53:16.551456 25791 master.cpp:3637] Authenticating scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188 I0905 15:53:16.551553 25788 authenticator.hpp:156] Creating new server SASL connection I0905 15:53:16.551673 25786 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0905 15:53:16.551697 25786 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0905 15:53:16.551755 25792 authenticator.hpp:262] Received SASL authentication start I0905 15:53:16.551808 25792 authenticator.hpp:384] Authentication requires more steps I0905 15:53:16.551856 25792 authenticatee.hpp:265] Received SASL authentication step I0905 15:53:16.551920 25786 authenticator.hpp:290] Received SASL authentication step I0905 15:53:16.551949 25786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false I0905 15:53:16.551966 25786 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0905 15:53:16.551985 25786 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0905 15:53:16.551997 25786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true I0905 15:53:16.552006 25786 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0905 15:53:16.552014 25786 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0905 15:53:16.552031 25786 authenticator.hpp:376] Authentication success I0905 15:53:16.552081 25792 authenticatee.hpp:305] Authentication success I0905 15:53:16.552100 25786 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188 I0905 15:53:16.552249 25792 sched.cpp:357] Successfully authenticated with master master@67.195.81.186:49188 I0905 15:53:17.402861 25793 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0905 15:53:18.874348 25792 sched.cpp:476] Sending registration request to master@67.195.81.186:49188 I0905 15:53:18.874364 25793 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.471501003secs I0905 15:53:18.874420 25792 sched.cpp:476] Sending registration request to master@67.195.81.186:49188 I0905 15:53:18.874451 25793 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188 I0905 15:53:18.874480 25793 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*' I0905 15:53:18.874565 25793 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188 I0905 15:53:18.874588 25793 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*' : Failure Mock function called more times than expected - returning default value.     Function call: authorize(@0x2b9ed7fe9350 40-byte object <90-BA B4-D4 9E-2B 00-00 00-00 00-00 00-00 00-00 A0-FA 06-F4 9E-2B 00-00 80-17 09-F4 9E-2B 00-00 00-00 00-00 03-00 00-00>)     The mock function has no default action set, and its return type has no default value set. *** Aborted at 1409932398 (unix time) try ""date -d @1409932398"" if you are using GNU date *** PC: @     0x2b9ed6233f79 (unknown) *** SIGABRT (@0x95c000064a9) received by PID 25769 (TID 0x2b9ed7fea700) from PID 25769; stack trace: ***     @     0x2b9ed5fef340 (unknown)     @     0x2b9ed6233f79 (unknown)     @     0x2b9ed6237388 (unknown)     @           0x93a5ec testing::internal::GoogleTestFailureReporter::ReportFailure()     @           0x7296c5 testing::internal::FunctionMockerBase<>::UntypedPerformDefaultAction()     @           0x933094 testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()     @           0x71fbde mesos::internal::tests::MockAuthorizer::authorize()     @     0x2b9ed4038caf mesos::internal::master::Master::validate()     @     0x2b9ed4039763 mesos::internal::master::Master::registerFramework()     @     0x2b9ed40a0c0f ProtobufProcess<>::handler1<>()     @     0x2b9ed4050c57 std::_Function_handler<>::_M_invoke()     @     0x2b9ed407d202 ProtobufProcess<>::visit()     @     0x2b9ed402af1a mesos::internal::master::Master::_visit()     @     0x2b9ed4037eb8 mesos::internal::master::Master::visit()     @     0x2b9ed44cb792 process::ProcessManager::resume()     @     0x2b9ed44cba9c process::schedule()     @     0x2b9ed5fe7182 start_thread     @     0x2b9ed62f830d (unknown) {code}",2
"MESOS-1778","Provide an option to validate flag value in stout/flags. ","Currently we can provide the default value for a flag, but cannot check if the flag is set to a reasonable value and, e.g., issue a warning. Passing an optional lambda checker to {{FlagBase::add()}} can be a possible solution.",3
"MESOS-1782","AllocatorTest/0.FrameworkExited is flaky","{noformat:title=} [ RUN      ] AllocatorTest/0.FrameworkExited Using temporary directory '/tmp/AllocatorTest_0_FrameworkExited_B6WZng' I0909 08:02:35.116555 18112 leveldb.cpp:176] Opened db in 31.64686ms I0909 08:02:35.126065 18112 leveldb.cpp:183] Compacted db in 9.449823ms I0909 08:02:35.126118 18112 leveldb.cpp:198] Created db iterator in 5858ns I0909 08:02:35.126137 18112 leveldb.cpp:204] Seeked to beginning of db in 1136ns I0909 08:02:35.126150 18112 leveldb.cpp:273] Iterated through 0 keys in the db in 560ns I0909 08:02:35.126178 18112 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0909 08:02:35.126502 18133 recover.cpp:425] Starting replica recovery I0909 08:02:35.126601 18133 recover.cpp:451] Replica is in EMPTY status I0909 08:02:35.127012 18133 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I0909 08:02:35.127094 18133 recover.cpp:188] Received a recover response from a replica in EMPTY status I0909 08:02:35.127223 18133 recover.cpp:542] Updating replica status to STARTING I0909 08:02:35.226631 18133 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 99.308134ms I0909 08:02:35.226690 18133 replica.cpp:320] Persisted replica status to STARTING I0909 08:02:35.226812 18131 recover.cpp:451] Replica is in STARTING status I0909 08:02:35.227246 18131 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I0909 08:02:35.227308 18131 recover.cpp:188] Received a recover response from a replica in STARTING status I0909 08:02:35.227409 18131 recover.cpp:542] Updating replica status to VOTING I0909 08:02:35.228540 18129 master.cpp:286] Master 20140909-080235-16842879-44005-18112 (precise) started on 127.0.1.1:44005 I0909 08:02:35.228593 18129 master.cpp:332] Master only allowing authenticated frameworks to register I0909 08:02:35.228607 18129 master.cpp:337] Master only allowing authenticated slaves to register I0909 08:02:35.228620 18129 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_FrameworkExited_B6WZng/credentials' I0909 08:02:35.228754 18129 master.cpp:366] Authorization enabled I0909 08:02:35.229560 18129 master.cpp:120] No whitelist given. Advertising offers for all slaves I0909 08:02:35.229933 18129 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@127.0.1.1:44005 I0909 08:02:35.230057 18127 master.cpp:1212] The newly elected leader is master@127.0.1.1:44005 with id 20140909-080235-16842879-44005-18112 I0909 08:02:35.230129 18127 master.cpp:1225] Elected as the leading master! I0909 08:02:35.230144 18127 master.cpp:1043] Recovering from registrar I0909 08:02:35.230257 18127 registrar.cpp:313] Recovering registrar I0909 08:02:35.232461 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 4.999384ms I0909 08:02:35.232489 18131 replica.cpp:320] Persisted replica status to VOTING I0909 08:02:35.232544 18131 recover.cpp:556] Successfully joined the Paxos group I0909 08:02:35.232611 18131 recover.cpp:440] Recover process terminated I0909 08:02:35.232727 18131 log.cpp:656] Attempting to start the writer I0909 08:02:35.233012 18131 replica.cpp:474] Replica received implicit promise request with proposal 1 I0909 08:02:35.238785 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.749504ms I0909 08:02:35.238818 18131 replica.cpp:342] Persisted promised to 1 I0909 08:02:35.244056 18131 coordinator.cpp:230] Coordinator attemping to fill missing position I0909 08:02:35.244580 18131 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I0909 08:02:35.250143 18131 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.382351ms I0909 08:02:35.250319 18131 replica.cpp:676] Persisted action at 0 I0909 08:02:35.250901 18131 replica.cpp:508] Replica received write request for position 0 I0909 08:02:35.251137 18131 leveldb.cpp:438] Reading position from leveldb took 18689ns I0909 08:02:35.256597 18131 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.274169ms I0909 08:02:35.256764 18131 replica.cpp:676] Persisted action at 0 I0909 08:02:35.263712 18126 replica.cpp:655] Replica received learned notice for position 0 I0909 08:02:35.269613 18126 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.417225ms I0909 08:02:35.351641 18126 replica.cpp:676] Persisted action at 0 I0909 08:02:35.351655 18126 replica.cpp:661] Replica learned NOP action at position 0 I0909 08:02:35.351889 18126 log.cpp:672] Writer started with ending position 0 I0909 08:02:35.352165 18126 leveldb.cpp:438] Reading position from leveldb took 25215ns I0909 08:02:35.353163 18126 registrar.cpp:346] Successfully fetched the registry (0B) I0909 08:02:35.353185 18126 registrar.cpp:422] Attempting to update the 'registry' I0909 08:02:35.354152 18126 log.cpp:680] Attempting to append 120 bytes to the log I0909 08:02:35.354195 18126 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0909 08:02:35.354416 18126 replica.cpp:508] Replica received write request for position 1 I0909 08:02:35.351579 18127 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.354558 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 2.984795ms I0909 08:02:35.360254 18126 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.811986ms I0909 08:02:35.360285 18126 replica.cpp:676] Persisted action at 1 I0909 08:02:35.364126 18132 replica.cpp:655] Replica received learned notice for position 1 I0909 08:02:35.369856 18132 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 5.702756ms I0909 08:02:35.369899 18132 replica.cpp:676] Persisted action at 1 I0909 08:02:35.369910 18132 replica.cpp:661] Replica learned APPEND action at position 1 I0909 08:02:35.370209 18132 registrar.cpp:479] Successfully updated 'registry' I0909 08:02:35.370311 18132 registrar.cpp:372] Successfully recovered registrar I0909 08:02:35.370477 18132 log.cpp:699] Attempting to truncate the log to 1 I0909 08:02:35.370553 18132 master.cpp:1070] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register I0909 08:02:35.370594 18132 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0909 08:02:35.371201 18127 replica.cpp:508] Replica received write request for position 2 I0909 08:02:35.376760 18127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.264501ms I0909 08:02:35.377105 18127 replica.cpp:676] Persisted action at 2 I0909 08:02:35.377770 18127 replica.cpp:655] Replica received learned notice for position 2 I0909 08:02:35.383363 18127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.272769ms I0909 08:02:35.383818 18127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 28148ns I0909 08:02:35.384137 18127 replica.cpp:676] Persisted action at 2 I0909 08:02:35.384399 18127 replica.cpp:661] Replica learned TRUNCATE action at position 2 I0909 08:02:35.396512 18127 slave.cpp:167] Slave started on 64)@127.0.1.1:44005 I0909 08:02:35.654770 18131 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.654847 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 104933ns I0909 08:02:35.654974 18127 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/credential' I0909 08:02:35.655097 18127 slave.cpp:274] Slave using credential for: test-principal I0909 08:02:35.655203 18127 slave.cpp:287] Slave resources: cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] I0909 08:02:35.655274 18127 slave.cpp:315] Slave hostname: precise I0909 08:02:35.655285 18127 slave.cpp:316] Slave checkpoint: false I0909 08:02:35.655804 18127 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/meta' I0909 08:02:35.655913 18127 status_update_manager.cpp:193] Recovering status update manager I0909 08:02:35.656005 18127 slave.cpp:3202] Finished recovery I0909 08:02:35.656251 18127 slave.cpp:598] New master detected at master@127.0.1.1:44005 I0909 08:02:35.656285 18127 slave.cpp:672] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.656325 18127 slave.cpp:645] Detecting new master I0909 08:02:35.656358 18127 status_update_manager.cpp:167] New master detected at master@127.0.1.1:44005 I0909 08:02:35.656389 18127 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.656563 18127 master.cpp:3653] Authenticating slave(64)@127.0.1.1:44005 I0909 08:02:35.656651 18127 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.656770 18127 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.656796 18127 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.656822 18127 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.656858 18127 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.656883 18127 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.656924 18127 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.656960 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.656971 18127 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.656982 18127 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.656997 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.657004 18127 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.657008 18127 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.657019 18127 authenticator.hpp:376] Authentication success I0909 08:02:35.657047 18127 authenticatee.hpp:305] Authentication success I0909 08:02:35.657073 18127 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(64)@127.0.1.1:44005 I0909 08:02:35.657145 18127 slave.cpp:729] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.657183 18127 slave.cpp:980] Will retry registration in 19.238717ms if necessary I0909 08:02:35.657276 18128 master.cpp:2843] Registering slave at slave(64)@127.0.1.1:44005 (precise) with id 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.657389 18128 registrar.cpp:422] Attempting to update the 'registry' I0909 08:02:35.658382 18130 log.cpp:680] Attempting to append 295 bytes to the log I0909 08:02:35.658432 18130 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0909 08:02:35.658635 18130 replica.cpp:508] Replica received write request for position 3 I0909 08:02:35.660959 18112 sched.cpp:137] Version: 0.21.0 I0909 08:02:35.661093 18126 sched.cpp:233] New master detected at master@127.0.1.1:44005 I0909 08:02:35.661111 18126 sched.cpp:283] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.661175 18126 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.661306 18126 master.cpp:3653] Authenticating scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661376 18126 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.661466 18126 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.661483 18126 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.661504 18126 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.661530 18126 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.661552 18126 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.661579 18126 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.661592 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.661598 18126 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.661607 18126 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.661613 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.661619 18126 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.661623 18126 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.661633 18126 authenticator.hpp:376] Authentication success I0909 08:02:35.661653 18126 authenticatee.hpp:305] Authentication success I0909 08:02:35.661672 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661730 18126 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.661741 18126 sched.cpp:476] Sending registration request to master@127.0.1.1:44005 I0909 08:02:35.661782 18126 master.cpp:1331] Received registration request from scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661798 18126 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*' I0909 08:02:35.661917 18126 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0000 at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.662017 18126 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.662039 18126 sched.cpp:421] Scheduler::registered took 9070ns I0909 08:02:35.662119 18126 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.662130 18126 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.662135 18126 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5558ns I0909 08:02:35.672230 18130 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 13.567526ms I0909 08:02:35.672268 18130 replica.cpp:676] Persisted action at 3 I0909 08:02:35.672483 18130 replica.cpp:655] Replica received learned notice for position 3 I0909 08:02:35.677322 18132 slave.cpp:980] Will retry registration in 14.890338ms if necessary I0909 08:02:35.677399 18132 master.cpp:2831] Ignoring register slave message from slave(64)@127.0.1.1:44005 (precise) as admission is already in progress I0909 08:02:35.680881 18130 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 8.376798ms I0909 08:02:35.680908 18130 replica.cpp:676] Persisted action at 3 I0909 08:02:35.680917 18130 replica.cpp:661] Replica learned APPEND action at position 3 I0909 08:02:35.681252 18130 registrar.cpp:479] Successfully updated 'registry' I0909 08:02:35.681330 18130 log.cpp:699] Attempting to truncate the log to 3 I0909 08:02:35.681385 18130 master.cpp:2883] Registered slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.681399 18130 master.cpp:4126] Adding slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] I0909 08:02:35.681504 18130 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0909 08:02:35.681570 18130 slave.cpp:763] Registered with master master@127.0.1.1:44005; given slave ID 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.681689 18130 slave.cpp:2329] Received ping from slave-observer(50)@127.0.1.1:44005 I0909 08:02:35.681753 18130 hierarchical_allocator_process.hpp:442] Added slave 20140909-080235-16842879-44005-18112-0 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] (and cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] available) I0909 08:02:35.681808 18130 hierarchical_allocator_process.hpp:734] Offering cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.681892 18130 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140909-080235-16842879-44005-18112-0 in 109580ns I0909 08:02:35.681968 18130 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.682014 18130 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.682443 18130 sched.cpp:544] Scheduler::resourceOffers took 254258ns I0909 08:02:35.682633 18130 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.682684 18130 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-0 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.682708 18130 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I0909 08:02:35.682971 18130 replica.cpp:508] Replica received write request for position 4 I0909 08:02:35.683132 18132 master.hpp:833] Adding task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.683159 18132 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0000 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.683363 18132 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.683580 18132 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.684833 18133 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.684864 18133 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0000 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs I0909 08:02:35.686401 18132 exec.cpp:132] Version: 0.21.0 I0909 08:02:35.686848 18128 exec.cpp:182] Executor started at: executor(8)@127.0.1.1:44005 with pid 18112 I0909 08:02:35.687095 18132 slave.cpp:1231] Queuing task '0' for executor executor-1 of framework '20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.687302 18132 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-18112-0000/executors/executor-1/runs/c4458e43-94ee-4b5e-bd74-5d39a09deff6' I0909 08:02:35.687568 18132 slave.cpp:1741] Got registration for executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.687893 18127 exec.cpp:206] Executor registered on slave 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.688789 18127 exec.cpp:218] Executor::registered took 15015ns I0909 08:02:35.688977 18132 slave.cpp:1859] Flushing queued task 0 for executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.689260 18133 exec.cpp:293] Executor asked to run task '0' I0909 08:02:35.689441 18133 exec.cpp:302] Executor::launchTask took 24599ns I0909 08:02:35.691651 18112 sched.cpp:137] Version: 0.21.0 I0909 08:02:35.691946 18131 sched.cpp:233] New master detected at master@127.0.1.1:44005 I0909 08:02:35.692126 18131 sched.cpp:283] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.692399 18131 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.692791 18131 master.cpp:3653] Authenticating scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.693068 18131 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.693351 18131 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.693532 18131 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.693739 18131 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.693979 18131 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.694202 18131 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.694449 18131 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.694633 18131 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.694792 18131 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.694980 18131 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.695158 18131 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.695369 18131 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.695724 18131 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.695907 18131 authenticator.hpp:376] Authentication success I0909 08:02:35.696117 18128 authenticatee.hpp:305] Authentication success I0909 08:02:35.698509 18130 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 15.520863ms I0909 08:02:35.698698 18130 replica.cpp:676] Persisted action at 4 I0909 08:02:35.698940 18128 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.699095 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.699354 18130 replica.cpp:655] Replica received learned notice for position 4 I0909 08:02:35.699973 18128 sched.cpp:476] Sending registration request to master@127.0.1.1:44005 I0909 08:02:35.700265 18128 master.cpp:1331] Received registration request from scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.700515 18128 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*' I0909 08:02:35.700809 18128 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0001 at scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.701037 18133 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701211 18133 sched.cpp:421] Scheduler::registered took 11991ns I0909 08:02:35.701488 18131 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701728 18131 hierarchical_allocator_process.hpp:734] Offering cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701992 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 297969ns I0909 08:02:35.702229 18128 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-1 with resources cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.702481 18128 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.702901 18129 sched.cpp:544] Scheduler::resourceOffers took 127949ns I0909 08:02:35.703305 18128 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-1 with resources cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.703629 18128 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-1 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.703908 18128 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I0909 08:02:35.703789 18132 slave.cpp:2542] Monitoring executor 'executor-1' of framework '20140909-080235-16842879-44005-18112-0000' in container 'c4458e43-94ee-4b5e-bd74-5d39a09deff6' I0909 08:02:35.704763 18128 master.hpp:833] Adding task 0 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.704951 18128 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0001 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.705255 18129 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.705582 18129 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.707756 18129 exec.cpp:132] Version: 0.21.0 I0909 08:02:35.708035 18130 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.127072ms I0909 08:02:35.708281 18130 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28817ns I0909 08:02:35.708459 18130 replica.cpp:676] Persisted action at 4 I0909 08:02:35.708632 18130 replica.cpp:661] Replica learned TRUNCATE action at position 4 I0909 08:02:35.708869 18133 exec.cpp:182] Executor started at: executor(9)@127.0.1.1:44005 with pid 18112 I0909 08:02:35.709120 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 35083ns I0909 08:02:35.709511 18129 slave.cpp:1231] Queuing task '0' for executor executor-2 of framework '20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.709707 18129 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-18112-0001/executors/executor-2/runs/7654870b-fd36-40b2-aac7-37b1bcfa821e' I0909 08:02:35.709913 18129 slave.cpp:1741] Got registration for executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.710188 18129 slave.cpp:1859] Flushing queued task 0 for executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.710516 18129 slave.cpp:2542] Monitoring executor 'executor-2' of framework '20140909-080235-16842879-44005-18112-0001' in container '7654870b-fd36-40b2-aac7-37b1bcfa821e' I0909 08:02:35.710321 18130 exec.cpp:206] Executor registered on slave 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.711678 18130 exec.cpp:218] Executor::registered took 14355ns I0909 08:02:35.711987 18130 exec.cpp:293] Executor asked to run task '0' I0909 08:02:35.715551 18130 exec.cpp:302] Executor::launchTask took 3.40476ms I0909 08:02:35.716006 18131 sched.cpp:745] Stopping framework '20140909-080235-16842879-44005-18112-0000' I0909 08:02:35.716292 18128 master.cpp:1640] Asked to unregister framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.716490 18127 hierarchical_allocator_process.hpp:563] Recovered mem(*):256; disk(*):25116; ports(*):[31000-32000] (total allocatable: mem(*):256; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.716792 18127 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0001 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs I0909 08:02:35.717018 18128 master.cpp:3976] Removing framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.717269 18128 master.hpp:851] Removing task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise) W0909 08:02:35.717607 18128 master.cpp:4419] Removing task 0 of framework 20140909-080235-16842879-44005-18112-0000 and slave 20140909-080235-16842879-44005-18112-0 in non-terminal state TASK_STAGING I0909 08:02:35.717470 18131 hierarchical_allocator_process.hpp:405] Deactivated framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718065 18131 hierarchical_allocator_process.hpp:563] Recovered cpus(*):2; mem(*):512 (total allocatable: mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.717438 18132 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0000 by master@127.0.1.1:44005 I0909 08:02:35.718444 18132 slave.cpp:1439] Shutting down framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718621 18132 slave.cpp:2882] Shutting down executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718843 18133 exec.cpp:379] Executor asked to shutdown I0909 08:02:35.719022 18133 exec.cpp:394] Executor::shutdown took 13745ns I0909 08:02:35.722009 18128 hierarchical_allocator_process.hpp:360] Removed framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.830785 18131 hierarchical_allocator_process.hpp:734] Offering mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.830940 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 218030ns I0909 08:02:35.831056 18127 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-2 with resources mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.831115 18127 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.831248 18127 sched.cpp:544] Scheduler::resourceOffers took 18178ns I0909 08:02:35.831387 18112 master.cpp:650] Master terminating I0909 08:02:35.831441 18112 master.hpp:851] Removing task 0 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 (precise) W0909 08:02:35.831488 18112 master.cpp:4419] Removing task 0 of framework 20140909-080235-16842879-44005-18112-0001 and slave 20140909-080235-16842879-44005-18112-0 in non-terminal state TASK_STAGING I0909 08:02:35.831573 18112 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-2 with resources mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.832608 18112 slave.cpp:475] Slave terminating I0909 08:02:35.832630 18112 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0000 by @0.0.0.0:0 W0909 08:02:35.832643 18112 slave.cpp:1435] Ignoring shutdown framework 20140909-080235-16842879-44005-18112-0000 because it is terminating I0909 08:02:35.832648 18112 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0001 by @0.0.0.0:0 I0909 08:02:35.832654 18112 slave.cpp:1439] Shutting down framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.832664 18112 slave.cpp:2882] Shutting down executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 tests/allocator_tests.cpp:1444: Failure Actual function call count doesn't match EXPECT_CALL(this->allocator, resourcesRecovered(_, _, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] AllocatorTest/0.FrameworkExited, where TypeParam = mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> (756 ms) {noformat}",1
"MESOS-1813","Fail fast in example frameworks if task goes into unexpected state","Most of the example frameworks launch a bunch of tasks and exit if *all* of them reach FINISHED state. But if there is a bug in the code resulting in TASK_LOST, the framework waits forever. Instead the framework should abort if an un-expected task state is encountered.",1
"MESOS-1815","Create a guide to becoming a committer","We have a committer's guide, but the process by which one becomes a committer is unclear. We should set some guidelines and a process by which we can grow contributors into committers.",3
"MESOS-1830","Expose master stats differentiating between master-generated and slave-generated LOST tasks","The master exports a monotonically-increasing counter of tasks transitioned to TASK_LOST.  This loses fidelity of the source of the lost task.  A first step in exposing the source of lost tasks might be to just differentiate between TASK_LOST transitions initiated by the master vs the slave (and maybe bad input from the scheduler).",5
"MESOS-1844","AllocatorTest/0.SlaveLost is flaky","{code} [ RUN      ] AllocatorTest/0.SlaveLost Using temporary directory '/tmp/AllocatorTest_0_SlaveLost_Z2oazw' I0929 16:58:29.484141  3486 leveldb.cpp:176] Opened db in 604109ns I0929 16:58:29.484629  3486 leveldb.cpp:183] Compacted db in 172697ns I0929 16:58:29.484912  3486 leveldb.cpp:198] Created db iterator in 6429ns I0929 16:58:29.485133  3486 leveldb.cpp:204] Seeked to beginning of db in 1618ns I0929 16:58:29.485337  3486 leveldb.cpp:273] Iterated through 0 keys in the db in 752ns I0929 16:58:29.485595  3486 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0929 16:58:29.486017  3500 recover.cpp:425] Starting replica recovery I0929 16:58:29.486304  3500 recover.cpp:451] Replica is in EMPTY status I0929 16:58:29.486793  3500 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I0929 16:58:29.487205  3500 recover.cpp:188] Received a recover response from a replica in EMPTY status I0929 16:58:29.487540  3500 recover.cpp:542] Updating replica status to STARTING I0929 16:58:29.487911  3500 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 36629ns I0929 16:58:29.488173  3500 replica.cpp:320] Persisted replica status to STARTING I0929 16:58:29.488438  3500 recover.cpp:451] Replica is in STARTING status I0929 16:58:29.488891  3500 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I0929 16:58:29.489187  3500 recover.cpp:188] Received a recover response from a replica in STARTING status I0929 16:58:29.489516  3500 recover.cpp:542] Updating replica status to VOTING I0929 16:58:29.489887  3502 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 32099ns I0929 16:58:29.490124  3502 replica.cpp:320] Persisted replica status to VOTING I0929 16:58:29.490381  3500 recover.cpp:556] Successfully joined the Paxos group I0929 16:58:29.490713  3500 recover.cpp:440] Recover process terminated I0929 16:58:29.493401  3506 master.cpp:312] Master 20140929-165829-2759502016-55618-3486 (fedora-20) started on 192.168.122.164:55618 I0929 16:58:29.493700  3506 master.cpp:358] Master only allowing authenticated frameworks to register I0929 16:58:29.493921  3506 master.cpp:363] Master only allowing authenticated slaves to register I0929 16:58:29.494123  3506 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveLost_Z2oazw/credentials' I0929 16:58:29.494500  3506 master.cpp:392] Authorization enabled I0929 16:58:29.495249  3506 master.cpp:120] No whitelist given. Advertising offers for all slaves I0929 16:58:29.495728  3502 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@192.168.122.164:55618 I0929 16:58:29.496196  3506 master.cpp:1241] The newly elected leader is master@192.168.122.164:55618 with id 20140929-165829-2759502016-55618-3486 I0929 16:58:29.496469  3506 master.cpp:1254] Elected as the leading master! I0929 16:58:29.496713  3506 master.cpp:1072] Recovering from registrar I0929 16:58:29.497020  3506 registrar.cpp:312] Recovering registrar I0929 16:58:29.497486  3506 log.cpp:656] Attempting to start the writer I0929 16:58:29.498105  3506 replica.cpp:474] Replica received implicit promise request with proposal 1 I0929 16:58:29.498373  3506 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 27145ns I0929 16:58:29.498605  3506 replica.cpp:342] Persisted promised to 1 I0929 16:58:29.500880  3500 coordinator.cpp:230] Coordinator attemping to fill missing position I0929 16:58:29.501404  3500 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I0929 16:58:29.501687  3500 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 57971ns I0929 16:58:29.501935  3500 replica.cpp:676] Persisted action at 0 I0929 16:58:29.504905  3507 replica.cpp:508] Replica received write request for position 0 I0929 16:58:29.505130  3507 leveldb.cpp:438] Reading position from leveldb took 18418ns I0929 16:58:29.505377  3507 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 19998ns I0929 16:58:29.505571  3507 replica.cpp:676] Persisted action at 0 I0929 16:58:29.505957  3507 replica.cpp:655] Replica received learned notice for position 0 I0929 16:58:29.506186  3507 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 21648ns I0929 16:58:29.506433  3507 replica.cpp:676] Persisted action at 0 I0929 16:58:29.506767  3507 replica.cpp:661] Replica learned NOP action at position 0 I0929 16:58:29.507199  3507 log.cpp:672] Writer started with ending position 0 I0929 16:58:29.507730  3507 leveldb.cpp:438] Reading position from leveldb took 11532ns I0929 16:58:29.508915  3507 registrar.cpp:345] Successfully fetched the registry (0B) I0929 16:58:29.509230  3507 registrar.cpp:421] Attempting to update the 'registry' I0929 16:58:29.510516  3500 log.cpp:680] Attempting to append 130 bytes to the log I0929 16:58:29.510949  3500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0929 16:58:29.511363  3500 replica.cpp:508] Replica received write request for position 1 I0929 16:58:29.511697  3500 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 66530ns I0929 16:58:29.512039  3500 replica.cpp:676] Persisted action at 1 I0929 16:58:29.512460  3500 replica.cpp:655] Replica received learned notice for position 1 I0929 16:58:29.512778  3500 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 24121ns I0929 16:58:29.513013  3500 replica.cpp:676] Persisted action at 1 I0929 16:58:29.513239  3500 replica.cpp:661] Replica learned APPEND action at position 1 I0929 16:58:29.513674  3500 log.cpp:699] Attempting to truncate the log to 1 I0929 16:58:29.513954  3500 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0929 16:58:29.514385  3500 replica.cpp:508] Replica received write request for position 2 I0929 16:58:29.514680  3500 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 65014ns I0929 16:58:29.514991  3500 replica.cpp:676] Persisted action at 2 I0929 16:58:29.516978  3501 replica.cpp:655] Replica received learned notice for position 2 I0929 16:58:29.517319  3501 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 24103ns I0929 16:58:29.517546  3501 leveldb.cpp:401] Deleting ~1 keys from leveldb took 16533ns I0929 16:58:29.517801  3501 replica.cpp:676] Persisted action at 2 I0929 16:58:29.518039  3501 replica.cpp:661] Replica learned TRUNCATE action at position 2 I0929 16:58:29.518539  3507 registrar.cpp:478] Successfully updated 'registry' I0929 16:58:29.518885  3507 registrar.cpp:371] Successfully recovered registrar I0929 16:58:29.519201  3507 master.cpp:1099] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register I0929 16:58:29.533073  3505 slave.cpp:169] Slave started on 57)@192.168.122.164:55618 I0929 16:58:29.533500  3505 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/credential' I0929 16:58:29.533834  3505 slave.cpp:276] Slave using credential for: test-principal I0929 16:58:29.534168  3505 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] I0929 16:58:29.534751  3505 slave.cpp:317] Slave hostname: fedora-20 I0929 16:58:29.534965  3505 slave.cpp:318] Slave checkpoint: false I0929 16:58:29.535557  3505 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/meta' I0929 16:58:29.535951  3505 status_update_manager.cpp:193] Recovering status update manager I0929 16:58:29.536290  3505 slave.cpp:3271] Finished recovery I0929 16:58:29.536782  3505 slave.cpp:598] New master detected at master@192.168.122.164:55618 I0929 16:58:29.537122  3505 slave.cpp:672] Authenticating with master master@192.168.122.164:55618 I0929 16:58:29.537492  3505 slave.cpp:645] Detecting new master I0929 16:58:29.537294  3506 status_update_manager.cpp:167] New master detected at master@192.168.122.164:55618 I0929 16:58:29.537642  3507 authenticatee.hpp:128] Creating new client SASL connection I0929 16:58:29.538769  3502 master.cpp:3737] Authenticating slave(57)@192.168.122.164:55618 I0929 16:58:29.539091  3502 authenticator.hpp:156] Creating new server SASL connection I0929 16:58:29.539710  3503 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0929 16:58:29.539943  3503 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0929 16:58:29.540206  3502 authenticator.hpp:262] Received SASL authentication start I0929 16:58:29.540457  3502 authenticator.hpp:384] Authentication requires more steps I0929 16:58:29.540757  3502 authenticatee.hpp:265] Received SASL authentication step I0929 16:58:29.541121  3502 authenticator.hpp:290] Received SASL authentication step I0929 16:58:29.541368  3502 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0929 16:58:29.541599  3502 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0929 16:58:29.541874  3502 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0929 16:58:29.542129  3502 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0929 16:58:29.542333  3502 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0929 16:58:29.542553  3502 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0929 16:58:29.542785  3502 authenticator.hpp:376] Authentication success I0929 16:58:29.543047  3502 authenticatee.hpp:305] Authentication success I0929 16:58:29.543381  3502 slave.cpp:729] Successfully authenticated with master master@192.168.122.164:55618 I0929 16:58:29.543707  3502 slave.cpp:992] Will retry registration in 11.795692ms if necessary I0929 16:58:29.543179  3503 master.cpp:3777] Successfully authenticated principal 'test-principal' at slave(57)@192.168.122.164:55618 I0929 16:58:29.544255  3503 master.cpp:2930] Registering slave at slave(57)@192.168.122.164:55618 (fedora-20) with id 20140929-165829-2759502016-55618-3486-0 I0929 16:58:29.544587  3503 registrar.cpp:421] Attempting to update the 'registry' I0929 16:58:29.545816  3500 log.cpp:680] Attempting to append 299 bytes to the log I0929 16:58:29.546267  3500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0929 16:58:29.546749  3500 replica.cpp:508] Replica received write request for position 3 I0929 16:58:29.547030  3500 leveldb.cpp:343] Persisting action (318 bytes) to leveldb took 31759ns I0929 16:58:29.547236  3500 replica.cpp:676] Persisted action at 3 I0929 16:58:29.548902  3506 replica.cpp:655] Replica received learned notice for position 3 I0929 16:58:29.549139  3506 leveldb.cpp:343] Persisting action (320 bytes) to leveldb took 25595ns I0929 16:58:29.549343  3506 replica.cpp:676] Persisted action at 3 I0929 16:58:29.549607  3506 replica.cpp:661] Replica learned APPEND action at position 3 I0929 16:58:29.550081  3506 log.cpp:699] Attempting to truncate the log to 3 I0929 16:58:29.550497  3506 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0929 16:58:29.550943  3506 replica.cpp:508] Replica received write request for position 4 I0929 16:58:29.551198  3506 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 20852ns I0929 16:58:29.551409  3506 replica.cpp:676] Persisted action at 4 I0929 16:58:29.551795  3506 replica.cpp:655] Replica received learned notice for position 4 I0929 16:58:29.552094  3506 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 22182ns I0929 16:58:29.552320  3506 leveldb.cpp:401] Deleting ~2 keys from leveldb took 18503ns I0929 16:58:29.552525  3506 replica.cpp:676] Persisted action at 4 I0929 16:58:29.552781  3506 replica.cpp:661] Replica learned TRUNCATE action at position 4 I0929 16:58:29.550289  3503 registrar.cpp:478] Successfully updated 'registry' I0929 16:58:29.553553  3503 master.cpp:2970] Registered slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) I0929 16:58:29.553807  3503 master.cpp:4180] Adding slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) with cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] I0929 16:58:29.554152  3503 slave.cpp:763] Registered with master master@192.168.122.164:55618; given slave ID 20140929-165829-2759502016-55618-3486-0 I0929 16:58:29.554455  3503 slave.cpp:2345] Received ping from slave-observer(56)@192.168.122.164:55618 I0929 16:58:29.554707  3504 hierarchical_allocator_process.hpp:442] Added slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) with cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] available) I0929 16:58:29.555064  3504 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140929-165829-2759502016-55618-3486-0 in 13111ns I0929 16:58:29.558220  3486 sched.cpp:137] Version: 0.21.0 I0929 16:58:29.558821  3501 sched.cpp:233] New master detected at master@192.168.122.164:55618 I0929 16:58:29.559054  3501 sched.cpp:283] Authenticating with master master@192.168.122.164:55618 I0929 16:58:29.559360  3501 authenticatee.hpp:128] Creating new client SASL connection I0929 16:58:29.560096  3501 master.cpp:3737] Authenticating scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618 I0929 16:58:29.560430  3501 authenticator.hpp:156] Creating new server SASL connection I0929 16:58:29.561141  3501 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0929 16:58:29.561465  3501 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0929 16:58:29.561743  3501 authenticator.hpp:262] Received SASL authentication start I0929 16:58:29.562098  3501 authenticator.hpp:384] Authentication requires more steps I0929 16:58:29.562353  3501 authenticatee.hpp:265] Received SASL authentication step I0929 16:58:29.562721  3507 authenticator.hpp:290] Received SASL authentication step I0929 16:58:29.563022  3507 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0929 16:58:29.563254  3507 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0929 16:58:29.563484  3507 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0929 16:58:29.563736  3507 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0929 16:58:29.563976  3507 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0929 16:58:29.564188  3507 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0929 16:58:29.564415  3507 authenticator.hpp:376] Authentication success I0929 16:58:29.564673  3507 master.cpp:3777] Successfully authenticated principal 'test-principal' at scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618 I0929 16:58:29.568681  3501 authenticatee.hpp:305] Authentication success I0929 16:58:29.569046  3501 sched.cpp:357] Successfully authenticated with master master@192.168.122.164:55618 I0929 16:58:29.569286  3501 sched.cpp:476] Sending registration request to master@192.168.122.164:55618 I0929 16:58:29.569581  3507 master.cpp:1360] Received registration request from scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618 I0929 16:58:29.569846  3507 master.cpp:1320] Authorizing framework principal 'test-principal' to receive offers for role '*' I0929 16:58:29.570219  3507 master.cpp:1419] Registering framework 20140929-165829-2759502016-55618-3486-0000 at scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618 I0929 16:58:29.570543  3506 sched.cpp:407] Framework registered with 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.570811  3506 sched.cpp:421] Scheduler::registered took 13811ns I0929 16:58:29.571135  3502 hierarchical_allocator_process.hpp:329] Added framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.571393  3502 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 to framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.571723  3502 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 368547ns I0929 16:58:29.572125  3507 master.hpp:868] Adding offer 20140929-165829-2759502016-55618-3486-0 with resources cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) I0929 16:58:29.572374  3507 master.cpp:3679] Sending 1 offers to framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.572841  3503 sched.cpp:544] Scheduler::resourceOffers took 114306ns I0929 16:58:29.573197  3507 master.hpp:877] Removing offer 20140929-165829-2759502016-55618-3486-0 with resources cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) I0929 16:58:29.573457  3507 master.cpp:2274] Processing reply for offers: [ 20140929-165829-2759502016-55618-3486-0 ] on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) for framework 20140929-165829-2759502016-55618-3486-0000 W0929 16:58:29.573717  3507 master.cpp:1944] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0929 16:58:29.573953  3507 master.cpp:1955] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0929 16:58:29.574177  3507 master.cpp:2357] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I0929 16:58:29.574745  3507 master.hpp:845] Adding task 0 with resources cpus(*):2; mem(*):512 on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) I0929 16:58:29.574992  3507 master.cpp:2423] Launching task 0 of framework 20140929-165829-2759502016-55618-3486-0000 with resources cpus(*):2; mem(*):512 on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) I0929 16:58:29.575315  3503 slave.cpp:1023] Got assigned task 0 for framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.575724  3503 slave.cpp:1133] Launching task 0 for framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.578129  3503 exec.cpp:132] Version: 0.21.0 I0929 16:58:29.578505  3504 exec.cpp:182] Executor started at: executor(30)@192.168.122.164:55618 with pid 3486 I0929 16:58:29.578867  3503 slave.cpp:1246] Queuing task '0' for executor default of framework '20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.579144  3503 slave.cpp:554] Successfully attached file '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/slaves/20140929-165829-2759502016-55618-3486-0/frameworks/20140929-165829-2759502016-55618-3486-0000/executors/default/runs/b0de9759-7054-4763-90f4-889ddc3a8524' I0929 16:58:29.579401  3503 slave.cpp:1756] Got registration for executor 'default' of framework 20140929-165829-2759502016-55618-3486-0000 from executor(30)@192.168.122.164:55618 I0929 16:58:29.579879  3506 exec.cpp:206] Executor registered on slave 20140929-165829-2759502016-55618-3486-0 I0929 16:58:29.580921  3506 exec.cpp:218] Executor::registered took 17644ns I0929 16:58:29.581188  3503 slave.cpp:1875] Flushing queued task 0 for executor 'default' of framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.581526  3504 exec.cpp:293] Executor asked to run task '0' I0929 16:58:29.581807  3504 exec.cpp:302] Executor::launchTask took 42649ns I0929 16:58:29.583133  3504 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.586869  3503 slave.cpp:2611] Monitoring executor 'default' of framework '20140929-165829-2759502016-55618-3486-0000' in container 'b0de9759-7054-4763-90f4-889ddc3a8524' I0929 16:58:29.587252  3503 slave.cpp:2109] Handling status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 from executor(30)@192.168.122.164:55618 I0929 16:58:29.587723  3502 hierarchical_allocator_process.hpp:563] Recovered mem(*):512; disk(*):752; ports(*):[31000-32000] (total allocatable: mem(*):512; disk(*):752; ports(*):[31000-32000]) on slave 20140929-165829-2759502016-55618-3486-0 from framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.588127  3502 hierarchical_allocator_process.hpp:599] Framework 20140929-165829-2759502016-55618-3486-0000 filtered slave 20140929-165829-2759502016-55618-3486-0 for 5secs I0929 16:58:29.588433  3506 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.588767  3506 status_update_manager.cpp:499] Creating StatusUpdate stream for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.589054  3506 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 to master@192.168.122.164:55618 I0929 16:58:29.589400  3506 master.cpp:3301] Forwarding status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.589702  3506 master.cpp:3273] Status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 from slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) I0929 16:58:29.589923  3500 sched.cpp:635] Scheduler::statusUpdate took 36034ns I0929 16:58:29.590337  3500 master.cpp:2777] Forwarding status update acknowledgement 454bdb88-fd27-4201-b2c7-4ea03a6d00b3 for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 to slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) I0929 16:58:29.590643  3503 slave.cpp:477] Slave terminating I0929 16:58:29.590893  3503 slave.cpp:1429] Asked to shut down framework 20140929-165829-2759502016-55618-3486-0000 by @0.0.0.0:0 I0929 16:58:29.591136  3503 slave.cpp:1454] Shutting down framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.591367  3503 slave.cpp:2951] Shutting down executor 'default' of framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.591701  3501 master.cpp:817] Slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) disconnected I0929 16:58:29.591917  3501 master.cpp:821] Removing disconnected slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) because it is not checkpointing! I0929 16:58:29.592149  3501 master.cpp:4301] Removing slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) I0929 16:58:29.593868  3505 hierarchical_allocator_process.hpp:467] Removed slave 20140929-165829-2759502016-55618-3486-0 I0929 16:58:29.594907  3486 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem I0929 16:58:29.595091  3501 master.cpp:4485] Removing task 0 with resources cpus(*):2; mem(*):512 of framework 20140929-165829-2759502016-55618-3486-0000 on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) I0929 16:58:29.595960  3501 master.cpp:4514] Removing executor 'default' with resources  of framework 20140929-165829-2759502016-55618-3486-0000 on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) tests/allocator_tests.cpp:1552: Failure Mock function called more times than expected - taking default action specified at: ./tests/mesos.hpp:616:     Function call: resourcesRecovered(@0x7f958007f590 20140929-165829-2759502016-55618-3486-0000, @0x7f958007f5b0 20140929-165829-2759502016-55618-3486-0, @0x7f958007f5d0 {}, @0x7f958007f5e8 16-byte object <01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>)          Expected: to be called twice I0929 16:58:29.596640  3506 registrar.cpp:421] Attempting to update the 'registry'            Actual: called 3 times - over-saturated and active I0929 16:58:29.598697  3506 log.cpp:680] Attempting to append 133 bytes to the log I0929 16:58:29.598984  3500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5 I0929 16:58:29.599422  3500 replica.cpp:508] Replica received write request for position 5 I0929 16:58:29.599712  3500 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 65914ns I0929 16:58:29.599931  3500 replica.cpp:676] Persisted action at 5 I0929 16:58:29.600332  3500 replica.cpp:655] Replica received learned notice for position 5 I0929 16:58:29.600621  3500 leveldb.cpp:343] Persisting action (154 bytes) to leveldb took 24641ns I0929 16:58:29.600858  3500 replica.cpp:676] Persisted action at 5 I0929 16:58:29.601060  3500 replica.cpp:661] Replica learned APPEND action at position 5 I0929 16:58:29.601588  3506 registrar.cpp:478] Successfully updated 'registry' I0929 16:58:29.601765  3500 log.cpp:699] Attempting to truncate the log to 5 I0929 16:58:29.602308  3501 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6 I0929 16:58:29.602736  3505 replica.cpp:508] Replica received write request for position 6 I0929 16:58:29.602967  3505 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 22681ns I0929 16:58:29.603175  3505 replica.cpp:676] Persisted action at 6 I0929 16:58:29.603591  3501 replica.cpp:655] Replica received learned notice for position 6 I0929 16:58:29.603903  3501 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 23564ns I0929 16:58:29.604161  3501 leveldb.cpp:401] Deleting ~2 keys from leveldb took 18683ns I0929 16:58:29.604378  3501 replica.cpp:676] Persisted action at 6 I0929 16:58:29.604575  3501 replica.cpp:661] Replica learned TRUNCATE action at position 6 I0929 16:58:29.604970  3502 master.cpp:4393] Removed slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) I0929 16:58:29.605197  3502 master.cpp:3296] Sending status update TASK_LOST (UUID: cfc350bc-4ebf-4ea1-9fe4-27f53825c787) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 'Slave fedora-20 removed' I0929 16:58:29.605445  3502 master.cpp:4411] Notifying framework 20140929-165829-2759502016-55618-3486-0000 of lost slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) after recovering I0929 16:58:29.605756  3502 sched.cpp:635] Scheduler::statusUpdate took 9369ns I0929 16:58:29.605996  3502 sched.cpp:686] Lost slave 20140929-165829-2759502016-55618-3486-0 I0929 16:58:29.606210  3502 sched.cpp:697] Scheduler::slaveLost took 13761ns I0929 16:58:29.607326  3501 slave.cpp:169] Slave started on 58)@192.168.122.164:55618 I0929 16:58:29.607640  3501 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveLost_NcoJ6Z/credential' I0929 16:58:29.607975  3501 slave.cpp:276] Slave using credential for: test-principal I0929 16:58:29.608253  3501 slave.cpp:289] Slave resources: cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] I0929 16:58:29.608832  3501 slave.cpp:317] Slave hostname: fedora-20 I0929 16:58:29.608989  3501 slave.cpp:318] Slave checkpoint: false I0929 16:58:29.609542  3501 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveLost_NcoJ6Z/meta' I0929 16:58:29.609904  3500 status_update_manager.cpp:193] Recovering status update manager I0929 16:58:29.610119  3500 containerizer.cpp:252] Recovering containerizer I0929 16:58:29.610589  3507 slave.cpp:3271] Finished recovery I0929 16:58:29.611037  3507 slave.cpp:598] New master detected at master@192.168.122.164:55618 I0929 16:58:29.611264  3507 slave.cpp:672] Authenticating with master master@192.168.122.164:55618 I0929 16:58:29.611529  3507 slave.cpp:645] Detecting new master I0929 16:58:29.611385  3506 status_update_manager.cpp:167] New master detected at master@192.168.122.164:55618 I0929 16:58:29.611719  3503 authenticatee.hpp:128] Creating new client SASL connection I0929 16:58:29.612570  3503 master.cpp:3737] Authenticating slave(58)@192.168.122.164:55618 I0929 16:58:29.612843  3503 authenticator.hpp:156] Creating new server SASL connection I0929 16:58:29.613394  3503 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0929 16:58:29.613706  3503 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0929 16:58:29.614083  3503 authenticator.hpp:262] Received SASL authentication start I0929 16:58:29.614326  3503 authenticator.hpp:384] Authentication requires more steps I0929 16:58:29.614552  3503 authenticatee.hpp:265] Received SASL authentication step I0929 16:58:29.614828  3503 authenticator.hpp:290] Received SASL authentication step I0929 16:58:29.615067  3503 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0929 16:58:29.615314  3503 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0929 16:58:29.615562  3503 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0929 16:58:29.615766  3503 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0929 16:58:29.616060  3503 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0929 16:58:29.616387  3503 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0929 16:58:29.616631  3503 authenticator.hpp:376] Authentication success I0929 16:58:29.616929  3503 authenticatee.hpp:305] Authentication success I0929 16:58:29.617081  3501 master.cpp:3777] Successfully authenticated principal 'test-principal' at slave(58)@192.168.122.164:55618 I0929 16:58:29.620779  3500 slave.cpp:729] Successfully authenticated with master master@192.168.122.164:55618 I0929 16:58:29.621150  3500 slave.cpp:992] Will retry registration in 15.66596ms if necessary I0929 16:58:29.621526  3501 master.cpp:2930] Registering slave at slave(58)@192.168.122.164:55618 (fedora-20) with id 20140929-165829-2759502016-55618-3486-1 I0929 16:58:29.621976  3501 registrar.cpp:421] Attempting to update the 'registry' I0929 16:58:29.623364  3506 log.cpp:680] Attempting to append 299 bytes to the log I0929 16:58:29.623780  3506 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7 I0929 16:58:29.624407  3506 replica.cpp:508] Replica received write request for position 7 I0929 16:58:29.624712  3506 leveldb.cpp:343] Persisting action (318 bytes) to leveldb took 64462ns I0929 16:58:29.624984  3506 replica.cpp:676] Persisted action at 7 I0929 16:58:29.625460  3506 replica.cpp:655] Replica received learned notice for position 7 I0929 16:58:29.625838  3506 leveldb.cpp:343] Persisting action (320 bytes) to leveldb took 30316ns I0929 16:58:29.626093  3506 replica.cpp:676] Persisted action at 7 I0929 16:58:29.626382  3506 replica.cpp:661] Replica learned APPEND action at position 7 I0929 16:58:29.626832  3506 log.cpp:699] Attempting to truncate the log to 7 I0929 16:58:29.627231  3506 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8 I0929 16:58:29.627789  3506 replica.cpp:508] Replica received write request for position 8 I0929 16:58:29.628073  3506 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 26181ns I0929 16:58:29.628347  3506 replica.cpp:676] Persisted action at 8 I0929 16:58:29.628829  3506 replica.cpp:655] Replica received learned notice for position 8 I0929 16:58:29.629323  3506 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 28559ns I0929 16:58:29.629581  3506 leveldb.cpp:401] Deleting ~2 keys from leveldb took 22253ns I0929 16:58:29.629897  3506 replica.cpp:676] Persisted action at 8 I0929 16:58:29.630159  3506 replica.cpp:661] Replica learned TRUNCATE action at position 8 I0929 16:58:29.630910  3501 registrar.cpp:478] Successfully updated 'registry' I0929 16:58:29.631356  3501 master.cpp:2970] Registered slave 20140929-165829-2759502016-55618-3486-1 at slave(58)@192.168.122.164:55618 (fedora-20) I0929 16:58:29.631624  3501 master.cpp:4180] Adding slave 20140929-165829-2759502016-55618-3486-1 at slave(58)@192.168.122.164:55618 (fedora-20) with cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] I0929 16:58:29.632066  3501 slave.cpp:763] Registered with master master@192.168.122.164:55618; given slave ID 20140929-165829-2759502016-55618-3486-1 I0929 16:58:29.632493  3501 slave.cpp:2345] Received ping from slave-observer(57)@192.168.122.164:55618 I0929 16:58:29.632298  3506 hierarchical_allocator_process.hpp:442] Added slave 20140929-165829-2759502016-55618-3486-1 (fedora-20) with cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] (and cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] available) I0929 16:58:29.633102  3506 hierarchical_allocator_process.hpp:734] Offering cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-1 to framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.633496  3507 master.hpp:868] Adding offer 20140929-165829-2759502016-55618-3486-1 with resources cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-1 (fedora-20) I0929 16:58:29.633833  3507 master.cpp:3679] Sending 1 offers to framework 20140929-165829-2759502016-55618-3486-0000 I0929 16:58:29.634218  3507 sched.cpp:544] Scheduler::resourceOffers took 32550ns I0929 16:58:29.634784  3507 sched.cpp:745] Stopping framework '20140929-165829-2759502016-55618-3486-0000' I0929 16:58:29.634558  3486 master.cpp:676] Master terminating I0929 16:58:29.635319  3486 master.hpp:877] Removing offer 20140929-165829-2759502016-55618-3486-1 with resources cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-1 (fedora-20) I0929 16:58:29.635725  3506 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140929-165829-2759502016-55618-3486-1 in 2.656855ms I0929 16:58:29.644737  3503 slave.cpp:2430] master@192.168.122.164:55618 exited W0929 16:58:29.645407  3503 slave.cpp:2433] Master disconnected! Waiting for a new master to be elected I0929 16:58:29.656318  3486 slave.cpp:477] Slave terminating tests/allocator_tests.cpp:1532: Failure Actual function call count doesn't match EXPECT_CALL(this->allocator, resourcesRecovered(_, _, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] AllocatorTest/0.SlaveLost, where TypeParam = mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> (179 ms) {code}",1
"MESOS-1853","Remove /proc and /sys remounts from port_mapping isolator","/proc/net reflects a new network namespace regardless and remount doesn't actually do what we expected anyway, i.e., it's not sufficient for a new pid namespace and a new mount is required.",3
"MESOS-1855","Mesos 0.20.1 doesn't compile","The compilation of Mesos 0.20.1 fails on Ubuntu Trusty with the following error -  slave/containerizer/mesos/containerizer.cpp  -fPIC -DPIC -o slave/containerizer/mesos/.libs/libmesos_no_3rdparty_la-containerizer.o In file included from ./linux/routing/filter/ip.hpp:36:0,                  from ./slave/containerizer/isolators/network/port_mapping.hpp:42,                  from slave/containerizer/mesos/containerizer.cpp:44: ./linux/routing/filter/filter.hpp:29:43: fatal error: linux/routing/filter/handle.hpp: No such file or directory  #include ""linux/routing/filter/handle.hpp""                                            ^",1
"MESOS-1862","Performance regression in the Master's http metrics.","As part of the change to hold on to terminal unacknowledged tasks in the master, we introduced a performance regression during the following patch:  https://github.com/apache/mesos/commit/0760b007ad65bc91e8cea377339978c78d36d247 {noformat} commit 0760b007ad65bc91e8cea377339978c78d36d247 Author: Benjamin Mahler <bmahler@twitter.com> Date:   Thu Sep 11 10:48:20 2014 -0700      Minor cleanups to the Master code.      Review: https://reviews.apache.org/r/25566 {noformat}  Rather than keeping a running count of allocated resources, we now compute resources on-demand. This was done in order to ignore terminal task's resources.  As a result of this change, the /stats.json and /metrics/snapshot endpoints on the master have slowed down substantially on large clusters.  {noformat} $ time curl localhost:5050/health real 0m0.004s user 0m0.001s sys 0m0.002s  $ time curl localhost:5050/stats.json > /dev/null real 0m15.402s user 0m0.001s sys 0m0.003s  $ time curl localhost:5050/metrics/snapshot > /dev/null real 0m6.059s user 0m0.002s sys 0m0.002s {noformat}  {{perf top}} reveals some of the resource computation during a request to stats.json: {noformat: perf top} Events: 36K cycles  10.53%  libc-2.5.so             [.] _int_free   9.90%  libc-2.5.so             [.] malloc   8.56%  libmesos-0.21.0.so  [.] std::_Rb_tree<process::ProcessBase*, process::ProcessBase*, std::_Identity<process::ProcessBase*>, std::less<process::ProcessBase*>, std::allocator<process::ProcessBase*> >::   8.23%  libc-2.5.so             [.] _int_malloc   5.80%  libstdc++.so.6.0.8      [.] std::_Rb_tree_increment(std::_Rb_tree_node_base*)   5.33%  [kernel]                [k] _raw_spin_lock   3.13%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&)   2.95%  libmesos-0.21.0.so  [.] process::SocketManager::exited(process::ProcessBase*)   2.43%  libmesos-0.21.0.so  [.] mesos::Resource::MergeFrom(mesos::Resource const&)   1.88%  libmesos-0.21.0.so  [.] mesos::internal::master::Slave::used() const   1.48%  libstdc++.so.6.0.8      [.] __gnu_cxx::__atomic_add(int volatile*, int)   1.45%  [kernel]                [k] find_busiest_group   1.41%  libc-2.5.so             [.] free   1.38%  libmesos-0.21.0.so  [.] mesos::Value_Range::MergeFrom(mesos::Value_Range const&)   1.13%  libmesos-0.21.0.so  [.] mesos::Value_Scalar::MergeFrom(mesos::Value_Scalar const&)   1.12%  libmesos-0.21.0.so  [.] mesos::Resource::SharedDtor()   1.07%  libstdc++.so.6.0.8      [.] __gnu_cxx::__exchange_and_add(int volatile*, int)   0.94%  libmesos-0.21.0.so  [.] google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)   0.92%  libstdc++.so.6.0.8      [.] operator new(unsigned long)   0.88%  libmesos-0.21.0.so  [.] mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&)   0.75%  libmesos-0.21.0.so  [.] mesos::matches(mesos::Resource const&, mesos::Resource const&) {noformat}",3
"MESOS-1941","Make executor's user owner of executor's cgroup directory","Currently, when cgroups are enabled, and executor is spawned, it's mounted under, for ex: /sys/fs/cgroup/cpu/mesos/<mesos-id>. This directory in current implementation is only writable by root user. This prevents process launched by executor to mount its child processes under this cgroup, because the cgroup directory is only writable by root.  To enable a executor spawned process to mount it's child processes under it's cgroup directory, the cgroup directory should be made writable by the user which spawns the executor.",3
"MESOS-2007","AllocatorTest/0.SlaveReregistersFirst is flaky","{noformat:title=} [ RUN      ] AllocatorTest/0.SlaveReregistersFirst Using temporary directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d' I1028 23:48:22.360447 31190 leveldb.cpp:176] Opened db in 2.192575ms I1028 23:48:22.361253 31190 leveldb.cpp:183] Compacted db in 760753ns I1028 23:48:22.361320 31190 leveldb.cpp:198] Created db iterator in 22188ns I1028 23:48:22.361340 31190 leveldb.cpp:204] Seeked to beginning of db in 1950ns I1028 23:48:22.361351 31190 leveldb.cpp:273] Iterated through 0 keys in the db in 345ns I1028 23:48:22.361403 31190 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1028 23:48:22.362185 31217 recover.cpp:437] Starting replica recovery I1028 23:48:22.362764 31219 recover.cpp:463] Replica is in EMPTY status I1028 23:48:22.363955 31210 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1028 23:48:22.364320 31217 recover.cpp:188] Received a recover response from a replica in EMPTY status I1028 23:48:22.364820 31211 recover.cpp:554] Updating replica status to STARTING I1028 23:48:22.365365 31215 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418991ns I1028 23:48:22.365391 31215 replica.cpp:320] Persisted replica status to STARTING I1028 23:48:22.365617 31217 recover.cpp:463] Replica is in STARTING status I1028 23:48:22.366328 31206 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043 I1028 23:48:22.366377 31206 master.cpp:358] Master only allowing authenticated frameworks to register I1028 23:48:22.366391 31206 master.cpp:363] Master only allowing authenticated slaves to register I1028 23:48:22.366402 31206 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials' I1028 23:48:22.366708 31206 master.cpp:392] Authorization enabled I1028 23:48:22.366886 31209 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1028 23:48:22.367311 31208 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:22.367312 31207 recover.cpp:188] Received a recover response from a replica in STARTING status I1028 23:48:22.367686 31211 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043 I1028 23:48:22.367863 31212 recover.cpp:554] Updating replica status to VOTING I1028 23:48:22.368477 31218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 375527ns I1028 23:48:22.368505 31218 replica.cpp:320] Persisted replica status to VOTING I1028 23:48:22.368517 31204 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190 I1028 23:48:22.368549 31204 master.cpp:1255] Elected as the leading master! I1028 23:48:22.368567 31204 master.cpp:1073] Recovering from registrar I1028 23:48:22.368621 31215 recover.cpp:568] Successfully joined the Paxos group I1028 23:48:22.368716 31219 registrar.cpp:313] Recovering registrar I1028 23:48:22.369000 31215 recover.cpp:452] Recover process terminated I1028 23:48:22.369523 31208 log.cpp:656] Attempting to start the writer I1028 23:48:22.370909 31205 replica.cpp:474] Replica received implicit promise request with proposal 1 I1028 23:48:22.371266 31205 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 325016ns I1028 23:48:22.371290 31205 replica.cpp:342] Persisted promised to 1 I1028 23:48:22.371979 31218 coordinator.cpp:230] Coordinator attemping to fill missing position I1028 23:48:22.373378 31210 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1028 23:48:22.373746 31210 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 329018ns I1028 23:48:22.373772 31210 replica.cpp:676] Persisted action at 0 I1028 23:48:22.374897 31214 replica.cpp:508] Replica received write request for position 0 I1028 23:48:22.374951 31214 leveldb.cpp:438] Reading position from leveldb took 26002ns I1028 23:48:22.375272 31214 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 289094ns I1028 23:48:22.375298 31214 replica.cpp:676] Persisted action at 0 I1028 23:48:22.375886 31204 replica.cpp:655] Replica received learned notice for position 0 I1028 23:48:22.376258 31204 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 346650ns I1028 23:48:22.376277 31204 replica.cpp:676] Persisted action at 0 I1028 23:48:22.376298 31204 replica.cpp:661] Replica learned NOP action at position 0 I1028 23:48:22.376843 31215 log.cpp:672] Writer started with ending position 0 I1028 23:48:22.378056 31205 leveldb.cpp:438] Reading position from leveldb took 28265ns I1028 23:48:22.380323 31217 registrar.cpp:346] Successfully fetched the registry (0B) in 11.55584ms I1028 23:48:22.380466 31217 registrar.cpp:445] Applied 1 operations in 50632ns; attempting to update the 'registry' I1028 23:48:22.382472 31217 log.cpp:680] Attempting to append 139 bytes to the log I1028 23:48:22.382715 31210 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1028 23:48:22.383463 31210 replica.cpp:508] Replica received write request for position 1 I1028 23:48:22.383857 31210 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 363758ns I1028 23:48:22.383875 31210 replica.cpp:676] Persisted action at 1 I1028 23:48:22.384397 31218 replica.cpp:655] Replica received learned notice for position 1 I1028 23:48:22.384840 31218 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 420161ns I1028 23:48:22.384862 31218 replica.cpp:676] Persisted action at 1 I1028 23:48:22.384882 31218 replica.cpp:661] Replica learned APPEND action at position 1 I1028 23:48:22.385684 31211 registrar.cpp:490] Successfully updated the 'registry' in 5.158144ms I1028 23:48:22.385818 31211 registrar.cpp:376] Successfully recovered registrar I1028 23:48:22.385912 31214 log.cpp:699] Attempting to truncate the log to 1 I1028 23:48:22.386101 31218 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1028 23:48:22.386124 31211 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1028 23:48:22.387398 31209 replica.cpp:508] Replica received write request for position 2 I1028 23:48:22.387758 31209 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 334969ns I1028 23:48:22.387776 31209 replica.cpp:676] Persisted action at 2 I1028 23:48:22.388272 31204 replica.cpp:655] Replica received learned notice for position 2 I1028 23:48:22.388453 31204 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 159390ns I1028 23:48:22.388501 31204 leveldb.cpp:401] Deleting ~1 keys from leveldb took 30409ns I1028 23:48:22.388516 31204 replica.cpp:676] Persisted action at 2 I1028 23:48:22.388531 31204 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1028 23:48:22.400737 31207 slave.cpp:169] Slave started on 34)@67.195.81.190:50043 I1028 23:48:22.400786 31207 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/credential' I1028 23:48:22.400996 31207 slave.cpp:276] Slave using credential for: test-principal I1028 23:48:22.401304 31207 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:22.401413 31207 slave.cpp:318] Slave hostname: pietas.apache.org I1028 23:48:22.401520 31207 slave.cpp:319] Slave checkpoint: false W1028 23:48:22.401535 31207 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I1028 23:48:22.402349 31207 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/meta' I1028 23:48:22.402678 31207 status_update_manager.cpp:197] Recovering status update manager I1028 23:48:22.403048 31211 slave.cpp:3456] Finished recovery I1028 23:48:22.403815 31215 slave.cpp:602] New master detected at master@67.195.81.190:50043 I1028 23:48:22.403852 31215 slave.cpp:665] Authenticating with master master@67.195.81.190:50043 I1028 23:48:22.403875 31206 status_update_manager.cpp:171] Pausing sending status updates I1028 23:48:22.403961 31215 slave.cpp:638] Detecting new master I1028 23:48:22.404016 31211 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:22.404230 31204 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043 I1028 23:48:22.404464 31205 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:22.404613 31211 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:22.404649 31211 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:22.404734 31211 authenticator.hpp:267] Received SASL authentication start I1028 23:48:22.404783 31211 authenticator.hpp:389] Authentication requires more steps I1028 23:48:22.404898 31215 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:22.404999 31215 authenticator.hpp:295] Received SASL authentication step I1028 23:48:22.405030 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:22.405047 31215 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:22.405086 31215 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:22.405109 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:22.405122 31215 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.405129 31215 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.405146 31215 authenticator.hpp:381] Authentication success I1028 23:48:22.405243 31213 authenticatee.hpp:310] Authentication success I1028 23:48:22.405253 31214 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043 I1028 23:48:22.405505 31213 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:22.405619 31213 slave.cpp:1050] Will retry registration in 17.050994ms if necessary I1028 23:48:22.405819 31215 master.cpp:3032] Registering slave at slave(34)@67.195.81.190:50043 (pietas.apache.org) with id 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.406262 31216 registrar.cpp:445] Applied 1 operations in 52647ns; attempting to update the 'registry' I1028 23:48:22.406697 31190 sched.cpp:137] Version: 0.21.0 I1028 23:48:22.407083 31211 sched.cpp:233] New master detected at master@67.195.81.190:50043 I1028 23:48:22.407114 31211 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:22.407290 31214 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:22.407424 31214 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.407659 31207 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:22.407757 31207 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:22.407774 31207 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:22.407830 31207 authenticator.hpp:267] Received SASL authentication start I1028 23:48:22.407868 31207 authenticator.hpp:389] Authentication requires more steps I1028 23:48:22.407927 31207 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:22.408015 31212 authenticator.hpp:295] Received SASL authentication step I1028 23:48:22.408037 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:22.408046 31212 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:22.408072 31212 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:22.408092 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:22.408100 31212 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.408105 31212 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.408116 31212 authenticator.hpp:381] Authentication success I1028 23:48:22.408192 31210 authenticatee.hpp:310] Authentication success I1028 23:48:22.408210 31217 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.408419 31210 sched.cpp:357] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:22.408460 31210 sched.cpp:476] Sending registration request to master@67.195.81.190:50043 I1028 23:48:22.408568 31217 master.cpp:1362] Received registration request for framework 'default' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.408617 31217 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1028 23:48:22.408937 31214 master.cpp:1426] Registering framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.409265 31213 sched.cpp:407] Framework registered with 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.409267 31212 hierarchical_allocator_process.hpp:329] Added framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.409312 31212 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1028 23:48:22.409324 31215 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:22.409333 31213 sched.cpp:421] Scheduler::registered took 38591ns I1028 23:48:22.409327 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 24107ns I1028 23:48:22.409518 31205 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I1028 23:48:22.410127 31206 replica.cpp:508] Replica received write request for position 3 I1028 23:48:22.410706 31206 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554098ns I1028 23:48:22.410725 31206 replica.cpp:676] Persisted action at 3 I1028 23:48:22.411151 31217 replica.cpp:655] Replica received learned notice for position 3 I1028 23:48:22.411499 31217 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 326572ns I1028 23:48:22.411519 31217 replica.cpp:676] Persisted action at 3 I1028 23:48:22.411533 31217 replica.cpp:661] Replica learned APPEND action at position 3 I1028 23:48:22.412292 31219 registrar.cpp:490] Successfully updated the 'registry' in 5.972992ms I1028 23:48:22.412518 31218 log.cpp:699] Attempting to truncate the log to 3 I1028 23:48:22.412621 31213 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I1028 23:48:22.412734 31219 slave.cpp:2522] Received ping from slave-observer(38)@67.195.81.190:50043 I1028 23:48:22.412787 31206 master.cpp:3086] Registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:22.412858 31219 slave.cpp:756] Registered with master master@67.195.81.190:50043; given slave ID 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.412994 31210 status_update_manager.cpp:178] Resuming sending status updates I1028 23:48:22.413014 31211 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available) I1028 23:48:22.413159 31211 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] on slave 20141028-234822-3193029443-50043-31190-S0 to framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.413290 31208 replica.cpp:508] Replica received write request for position 4 I1028 23:48:22.413421 31211 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 346658ns I1028 23:48:22.413650 31208 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 336067ns I1028 23:48:22.413668 31208 replica.cpp:676] Persisted action at 4 I1028 23:48:22.413797 31216 master.cpp:3795] Sending 1 offers to framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.414077 31212 replica.cpp:655] Replica received learned notice for position 4 I1028 23:48:22.414356 31212 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260401ns I1028 23:48:22.414403 31212 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28541ns I1028 23:48:22.414417 31212 replica.cpp:676] Persisted action at 4 I1028 23:48:22.414446 31212 replica.cpp:661] Replica learned TRUNCATE action at position 4 I1028 23:48:22.414422 31207 sched.cpp:544] Scheduler::resourceOffers took 310278ns I1028 23:48:22.415086 31214 master.cpp:2321] Processing reply for offers: [ 20141028-234822-3193029443-50043-31190-O0 ] on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) for framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 W1028 23:48:22.415163 31214 master.cpp:1969] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W1028 23:48:22.415186 31214 master.cpp:1980] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I1028 23:48:22.415256 31214 master.cpp:2417] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I1028 23:48:22.416033 31219 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) I1028 23:48:22.416084 31219 master.cpp:2480] Launching task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.416317 31214 slave.cpp:1081] Got assigned task 0 for framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.416679 31215 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.416721 31215 hierarchical_allocator_process.hpp:599] Framework 20141028-234822-3193029443-50043-31190-0000 filtered slave 20141028-234822-3193029443-50043-31190-S0 for 5secs I1028 23:48:22.416724 31214 slave.cpp:1191] Launching task 0 for framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.418534 31214 slave.cpp:3871] Launching executor default of framework 20141028-234822-3193029443-50043-31190-0000 in work directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.420557 31214 exec.cpp:132] Version: 0.21.0 I1028 23:48:22.420755 31213 exec.cpp:182] Executor started at: executor(22)@67.195.81.190:50043 with pid 31190 I1028 23:48:22.420903 31214 slave.cpp:1317] Queuing task '0' for executor default of framework '20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.420997 31214 slave.cpp:555] Successfully attached file '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.421058 31214 slave.cpp:1849] Got registration for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043 I1028 23:48:22.421295 31214 slave.cpp:1968] Flushing queued task 0 for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.421391 31205 exec.cpp:206] Executor registered on slave 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.421495 31214 slave.cpp:2802] Monitoring executor 'default' of framework '20141028-234822-3193029443-50043-31190-0000' in container 'd593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.422873 31205 exec.cpp:218] Executor::registered took 19148ns I1028 23:48:22.422991 31205 exec.cpp:293] Executor asked to run task '0' I1028 23:48:22.423085 31205 exec.cpp:302] Executor::launchTask took 76519ns I1028 23:48:22.424541 31205 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.424724 31205 slave.cpp:2202] Handling status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043 I1028 23:48:22.424932 31213 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.424963 31213 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425122 31213 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to the slave I1028 23:48:22.425257 31205 slave.cpp:2442] Forwarding the update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to master@67.195.81.190:50043 I1028 23:48:22.425398 31205 slave.cpp:2369] Status update manager successfully handled status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425420 31205 slave.cpp:2375] Sending acknowledgement for status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to executor(22)@67.195.81.190:50043 I1028 23:48:22.425583 31212 master.cpp:3410] Forwarding status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425621 31206 exec.cpp:339] Executor received status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425786 31212 master.cpp:3382] Status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.425832 31212 master.cpp:4617] Updating the latest state of task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to TASK_RUNNING I1028 23:48:22.425885 31208 sched.cpp:635] Scheduler::statusUpdate took 49727ns I1028 23:48:22.426082 31208 master.cpp:2882] Forwarding status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 to slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.426360 31206 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.426623 31206 slave.cpp:1789] Status update manager successfully handled status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.426893 31210 master.cpp:677] Master terminating W1028 23:48:22.427028 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING I1028 23:48:22.427397 31209 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.427512 31210 master.cpp:4705] Removing executor 'default' with resources  of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.428129 31206 slave.cpp:2607] master@67.195.81.190:50043 exited W1028 23:48:22.428153 31206 slave.cpp:2610] Master disconnected! Waiting for a new master to be elected I1028 23:48:22.434645 31190 leveldb.cpp:176] Opened db in 2.551453ms I1028 23:48:22.437157 31190 leveldb.cpp:183] Compacted db in 2.484612ms I1028 23:48:22.437203 31190 leveldb.cpp:198] Created db iterator in 19171ns I1028 23:48:22.437235 31190 leveldb.cpp:204] Seeked to beginning of db in 18300ns I1028 23:48:22.437306 31190 leveldb.cpp:273] Iterated through 3 keys in the db in 59465ns I1028 23:48:22.437347 31190 replica.cpp:741] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned I1028 23:48:22.437827 31216 recover.cpp:437] Starting replica recovery I1028 23:48:22.438127 31216 recover.cpp:463] Replica is in VOTING status I1028 23:48:22.438443 31216 recover.cpp:452] Recover process terminated I1028 23:48:22.439877 31212 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043 I1028 23:48:22.439916 31212 master.cpp:358] Master only allowing authenticated frameworks to register I1028 23:48:22.439931 31212 master.cpp:363] Master only allowing authenticated slaves to register I1028 23:48:22.439946 31212 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials' I1028 23:48:22.440142 31212 master.cpp:392] Authorization enabled I1028 23:48:22.440439 31218 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:22.440901 31213 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043 I1028 23:48:22.441395 31206 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190 I1028 23:48:22.441421 31206 master.cpp:1255] Elected as the leading master! I1028 23:48:22.441457 31206 master.cpp:1073] Recovering from registrar I1028 23:48:22.441623 31205 registrar.cpp:313] Recovering registrar I1028 23:48:22.442172 31219 log.cpp:656] Attempting to start the writer I1028 23:48:22.443235 31219 replica.cpp:474] Replica received implicit promise request with proposal 2 I1028 23:48:22.443685 31219 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 427888ns I1028 23:48:22.443703 31219 replica.cpp:342] Persisted promised to 2 I1028 23:48:22.444371 31213 coordinator.cpp:230] Coordinator attemping to fill missing position I1028 23:48:22.444687 31209 log.cpp:672] Writer started with ending position 4 I1028 23:48:22.445754 31215 leveldb.cpp:438] Reading position from leveldb took 47909ns I1028 23:48:22.445826 31215 leveldb.cpp:438] Reading position from leveldb took 30611ns I1028 23:48:22.446941 31218 registrar.cpp:346] Successfully fetched the registry (277B) in 5.213184ms I1028 23:48:22.447118 31218 registrar.cpp:445] Applied 1 operations in 42362ns; attempting to update the 'registry' I1028 23:48:22.449329 31204 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:22.449477 31218 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5 I1028 23:48:22.450187 31215 replica.cpp:508] Replica received write request for position 5 I1028 23:48:22.450767 31215 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554400ns I1028 23:48:22.450788 31215 replica.cpp:676] Persisted action at 5 I1028 23:48:22.451561 31215 replica.cpp:655] Replica received learned notice for position 5 I1028 23:48:22.451979 31215 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 397219ns I1028 23:48:22.452000 31215 replica.cpp:676] Persisted action at 5 I1028 23:48:22.452020 31215 replica.cpp:661] Replica learned APPEND action at position 5 I1028 23:48:22.452993 31213 registrar.cpp:490] Successfully updated the 'registry' in 5.816832ms I1028 23:48:22.453136 31213 registrar.cpp:376] Successfully recovered registrar I1028 23:48:22.453238 31208 log.cpp:699] Attempting to truncate the log to 5 I1028 23:48:22.453384 31214 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6 I1028 23:48:22.453518 31215 master.cpp:1100] Recovered 1 slaves from the Registry (277B) ; allowing 10mins for slaves to re-register I1028 23:48:22.454116 31207 replica.cpp:508] Replica received write request for position 6 I1028 23:48:22.454570 31207 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 427424ns I1028 23:48:22.454589 31207 replica.cpp:676] Persisted action at 6 I1028 23:48:22.455095 31219 replica.cpp:655] Replica received learned notice for position 6 I1028 23:48:22.455399 31219 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 282466ns I1028 23:48:22.455462 31219 leveldb.cpp:401] Deleting ~2 keys from leveldb took 43939ns I1028 23:48:22.455478 31219 replica.cpp:676] Persisted action at 6 I1028 23:48:22.455494 31219 replica.cpp:661] Replica learned TRUNCATE action at position 6 I1028 23:48:22.465553 31213 status_update_manager.cpp:171] Pausing sending status updates I1028 23:48:22.465566 31216 slave.cpp:602] New master detected at master@67.195.81.190:50043 I1028 23:48:22.465612 31216 slave.cpp:665] Authenticating with master master@67.195.81.190:50043 I1028 23:48:23.441506 31206 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1028 23:48:27.441004 31214 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:30.101379 31206 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 6.659877806secs I1028 23:48:30.101568 31216 slave.cpp:638] Detecting new master I1028 23:48:30.101632 31214 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:30.102021 31218 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043 I1028 23:48:30.102329 31212 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:30.102505 31216 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:30.102545 31216 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:30.102638 31216 authenticator.hpp:267] Received SASL authentication start I1028 23:48:30.102709 31216 authenticator.hpp:389] Authentication requires more steps I1028 23:48:30.102812 31216 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:30.102957 31204 authenticator.hpp:295] Received SASL authentication step I1028 23:48:30.102982 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:30.102993 31204 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:30.103032 31204 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:30.103049 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:30.103056 31204 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:30.103061 31204 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:30.103073 31204 authenticator.hpp:381] Authentication success I1028 23:48:30.103149 31209 authenticatee.hpp:310] Authentication success I1028 23:48:30.103153 31204 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043 I1028 23:48:30.103371 31209 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:30.103773 31209 slave.cpp:1050] Will retry registration in 12.861518ms if necessary I1028 23:48:30.104068 31219 master.cpp:3210] Re-registering slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:30.104760 31216 registrar.cpp:445] Applied 1 operations in 71655ns; attempting to update the 'registry' I1028 23:48:30.107877 31205 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:30.108070 31219 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7 I1028 23:48:30.109110 31211 replica.cpp:508] Replica received write request for position 7 I1028 23:48:30.109434 31211 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 281545ns I1028 23:48:30.109484 31211 replica.cpp:676] Persisted action at 7 I1028 23:48:30.110124 31219 replica.cpp:655] Replica received learned notice for position 7 I1028 23:48:30.110903 31219 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 750414ns I1028 23:48:30.110927 31219 replica.cpp:676] Persisted action at 7 I1028 23:48:30.110950 31219 replica.cpp:661] Replica learned APPEND action at position 7 I1028 23:48:30.112160 31205 registrar.cpp:490] Successfully updated the 'registry' in 7.33824ms I1028 23:48:30.112529 31217 log.cpp:699] Attempting to truncate the log to 7 I1028 23:48:30.112714 31207 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8 I1028 23:48:30.112870 31210 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) W1028 23:48:30.113136 31210 master.cpp:4394] Possibly orphaned task 0 of framework 20141028-234822-3193029443-50043-31190-0000 running on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:30.113198 31219 slave.cpp:2522] Received ping from slave-observer(39)@67.195.81.190:50043 I1028 23:48:30.113340 31210 master.cpp:3278] Re-registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:30.113499 31219 slave.cpp:824] Re-registered with master master@67.195.81.190:50043 I1028 23:48:30.113636 31219 replica.cpp:508] Replica received write request for position 8 I1028 23:48:30.113652 31210 status_update_manager.cpp:178] Resuming sending status updates I1028 23:48:30.113759 31212 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] available) I1028 23:48:30.113904 31212 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 74698ns I1028 23:48:30.114116 31219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 452165ns I1028 23:48:30.114142 31219 replica.cpp:676] Persisted action at 8 I1028 23:48:30.114786 31213 replica.cpp:655] Replica received learned notice for position 8 I1028 23:48:30.115337 31213 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 525187ns I1028 23:48:30.115399 31213 leveldb.cpp:401] Deleting ~2 keys from leveldb took 37689ns I1028 23:48:30.115418 31213 replica.cpp:676] Persisted action at 8 I1028 23:48:30.115484 31213 replica.cpp:661] Replica learned TRUNCATE action at position 8 I1028 23:48:30.116603 31212 sched.cpp:227] Scheduler::disconnected took 16969ns I1028 23:48:30.116624 31212 sched.cpp:233] New master detected at master@67.195.81.190:50043 I1028 23:48:30.116657 31212 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:30.116870 31205 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:30.117084 31207 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:30.117279 31212 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:30.117410 31210 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:30.117507 31210 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:30.117604 31214 authenticator.hpp:267] Received SASL authentication start I1028 23:48:30.117652 31214 authenticator.hpp:389] Authentication requires more steps I1028 23:48:30.117738 31210 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:30.117905 31208 authenticator.hpp:295] Received SASL authentication step I1028 23:48:30.117935 31208 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:30.117947 31208 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:30.117979 31208 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:30.118001 31208 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I../../src/tests/allocator_tests.cpp:2405: Failure 1028 23:48:30.118013 31208 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true Failed to wait 10secs for resourceOffers2 I1028 23:48:31.101976 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 124354ns I1028 23:48:58.775811 31208 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true W1028 23:48:35.117725 31214 sched.cpp:378] Authentication timed out W1028 23:48:35.117784 31219 master.cpp:3911] Authentication timed out I1028 23:48:45.114322 31213 slave.cpp:2522] Received ping from slave-observer(39)@67.195.81.190:50043 I1028 23:48:35.102212 31206 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:58.775874 31208 authenticator.hpp:381] Authentication success I1028 23:48:58.776267 31214 sched.cpp:338] Failed to authenticate with master master@67.195.81.190:50043: Authentication discarded ../../src/tests/allocator_tests.cpp:2396: Failure Actual function call count doesn't match EXPECT_CALL(allocator2, frameworkAdded(_, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1028 23:48:58.776526 31204 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:58.776626 31214 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:58.776928 31204 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:58.777194 31210 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 W1028 23:48:58.777528 31210 master.cpp:3888] Failed to authenticate scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043: Failed to communicate with authenticatee ../../src/tests/allocator_tests.cpp:2399: Failure Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(&driver, _))...          Expected: to be called once            Actual: never called - unsatisfied and active ../../src/tests/allocator_tests.cpp:2394: Failure Actual function call count doesn't match EXPECT_CALL(sched, registered(&driver, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1028 23:48:58.778053 31205 slave.cpp:591] Re-detecting master I1028 23:48:58.778084 31205 slave.cpp:638] Detecting new master I1028 23:48:58.778115 31207 status_update_manager.cpp:171] Pausing sending status updates F1028 23:48:58.778115 31205 logging.cpp:57] RAW: Pure virtual method called I1028 23:48:58.778724 31210 master.cpp:677] Master terminating W1028 23:48:58.778919 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING *** Aborted at 1414540138 (unix time) try ""date -d @1414540138"" if you are using GNU date *** PC: @           0x91bc86 process::PID<>::PID() *** SIGSEGV (@0x0) received by PID 31190 (TID 0x2b20a6d95700) from PID 0; stack trace: ***     @     0x2b20a41ff340 (unknown)     @     0x2b20a1f2a188  google::LogMessage::Fail()     @     0x2b20a1f2f87c  google::RawLog__()     @           0x91bc86 process::PID<>::PID()     @           0x91bf24 process::Process<>::self()     @     0x2b20a15d5c06  __cxa_pure_virtual     @     0x2b20a1877752  mesos::internal::slave::Slave::detected()     @     0x2b20a1671f24 process::dispatch<>()     @     0x2b20a18b35f9  _ZZN7process8dispatchIN5mesos8internal5slave5SlaveERKNS_6FutureI6OptionINS1_10MasterInfoEEEES9_EEvRKNS_3PIDIT_EEMSD_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESM_     @     0x2b20a1663217 mesos::internal::master::allocator::Allocator::resourcesRecovered()     @     0x2b20a1650d01 mesos::internal::master::Master::removeTask()     @     0x2b20a162fb41 mesos::internal::master::Master::finalize()     @     0x2b20a1eb69a1 process::ProcessBase::visit()     @     0x2b20a1ec0464 process::TerminateEvent::visit()     @           0x8e0812 process::ProcessBase::serve()     @     0x2b20a18da89e  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal5slave5SlaveERKNS0_6FutureI6OptionINS5_10MasterInfoEEEESD_EEvRKNS0_3PIDIT_EEMSH_FvT0_ET1_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_     @     0x2b20a1eb1ca0 process::ProcessManager::resume()     @     0x2b20a1ea8365 process::schedule()     @     0x2b20a41f7182 start_thread     @     0x2b20a4507fbd (unknown) make[3]: *** [check-local] Segmentation fault {noformat}",2
"MESOS-2008","MasterAuthorizationTest.DuplicateReregistration is flaky","{noformat:title=} [ RUN      ] MasterAuthorizationTest.DuplicateReregistration Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX' I1029 08:25:26.021766 32232 leveldb.cpp:176] Opened db in 3.066621ms I1029 08:25:26.022734 32232 leveldb.cpp:183] Compacted db in 935019ns I1029 08:25:26.022766 32232 leveldb.cpp:198] Created db iterator in 4350ns I1029 08:25:26.022785 32232 leveldb.cpp:204] Seeked to beginning of db in 902ns I1029 08:25:26.022799 32232 leveldb.cpp:273] Iterated through 0 keys in the db in 387ns I1029 08:25:26.022831 32232 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1029 08:25:26.023305 32248 recover.cpp:437] Starting replica recovery I1029 08:25:26.023598 32248 recover.cpp:463] Replica is in EMPTY status I1029 08:25:26.025059 32260 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1029 08:25:26.025320 32247 recover.cpp:188] Received a recover response from a replica in EMPTY status I1029 08:25:26.025585 32256 recover.cpp:554] Updating replica status to STARTING I1029 08:25:26.026546 32249 master.cpp:312] Master 20141029-082526-3142697795-40696-32232 (pomona.apache.org) started on 67.195.81.187:40696 I1029 08:25:26.026561 32261 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 694444ns I1029 08:25:26.026592 32249 master.cpp:358] Master only allowing authenticated frameworks to register I1029 08:25:26.026592 32261 replica.cpp:320] Persisted replica status to STARTING I1029 08:25:26.026605 32249 master.cpp:363] Master only allowing authenticated slaves to register I1029 08:25:26.026639 32249 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX/credentials' I1029 08:25:26.026877 32249 master.cpp:392] Authorization enabled I1029 08:25:26.026901 32260 recover.cpp:463] Replica is in STARTING status I1029 08:25:26.027498 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:26.027541 32248 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40696 I1029 08:25:26.028055 32252 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1029 08:25:26.028451 32247 recover.cpp:188] Received a recover response from a replica in STARTING status I1029 08:25:26.028733 32249 master.cpp:1242] The newly elected leader is master@67.195.81.187:40696 with id 20141029-082526-3142697795-40696-32232 I1029 08:25:26.028764 32249 master.cpp:1255] Elected as the leading master! I1029 08:25:26.028781 32249 master.cpp:1073] Recovering from registrar I1029 08:25:26.028904 32246 recover.cpp:554] Updating replica status to VOTING I1029 08:25:26.029163 32257 registrar.cpp:313] Recovering registrar I1029 08:25:26.029556 32251 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 485711ns I1029 08:25:26.029588 32251 replica.cpp:320] Persisted replica status to VOTING I1029 08:25:26.029726 32253 recover.cpp:568] Successfully joined the Paxos group I1029 08:25:26.029932 32253 recover.cpp:452] Recover process terminated I1029 08:25:26.030436 32250 log.cpp:656] Attempting to start the writer I1029 08:25:26.032152 32248 replica.cpp:474] Replica received implicit promise request with proposal 1 I1029 08:25:26.032778 32248 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 597030ns I1029 08:25:26.032807 32248 replica.cpp:342] Persisted promised to 1 I1029 08:25:26.033481 32254 coordinator.cpp:230] Coordinator attemping to fill missing position I1029 08:25:26.035429 32247 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1029 08:25:26.036154 32247 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 690208ns I1029 08:25:26.036181 32247 replica.cpp:676] Persisted action at 0 I1029 08:25:26.037344 32249 replica.cpp:508] Replica received write request for position 0 I1029 08:25:26.037395 32249 leveldb.cpp:438] Reading position from leveldb took 22607ns I1029 08:25:26.038074 32249 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 647429ns I1029 08:25:26.038105 32249 replica.cpp:676] Persisted action at 0 I1029 08:25:26.038683 32247 replica.cpp:655] Replica received learned notice for position 0 I1029 08:25:26.039378 32247 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 664911ns I1029 08:25:26.039407 32247 replica.cpp:676] Persisted action at 0 I1029 08:25:26.039433 32247 replica.cpp:661] Replica learned NOP action at position 0 I1029 08:25:26.040045 32252 log.cpp:672] Writer started with ending position 0 I1029 08:25:26.041378 32251 leveldb.cpp:438] Reading position from leveldb took 25625ns I1029 08:25:26.044642 32246 registrar.cpp:346] Successfully fetched the registry (0B) in 15.433984ms I1029 08:25:26.044742 32246 registrar.cpp:445] Applied 1 operations in 16444ns; attempting to update the 'registry' I1029 08:25:26.047538 32256 log.cpp:680] Attempting to append 139 bytes to the log I1029 08:25:26.156330 32247 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1029 08:25:26.158460 32261 replica.cpp:508] Replica received write request for position 1 I1029 08:25:26.159277 32261 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 782308ns I1029 08:25:26.159328 32261 replica.cpp:676] Persisted action at 1 I1029 08:25:26.160267 32255 replica.cpp:655] Replica received learned notice for position 1 I1029 08:25:26.161070 32255 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 750259ns I1029 08:25:26.161100 32255 replica.cpp:676] Persisted action at 1 I1029 08:25:26.161125 32255 replica.cpp:661] Replica learned APPEND action at position 1 I1029 08:25:26.162199 32253 registrar.cpp:490] Successfully updated the 'registry' in 117.40416ms I1029 08:25:26.162400 32253 registrar.cpp:376] Successfully recovered registrar I1029 08:25:26.162724 32249 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1029 08:25:26.162757 32253 log.cpp:699] Attempting to truncate the log to 1 I1029 08:25:26.162919 32256 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1029 08:25:26.163949 32250 replica.cpp:508] Replica received write request for position 2 I1029 08:25:26.164589 32250 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 603175ns I1029 08:25:26.164618 32250 replica.cpp:676] Persisted action at 2 I1029 08:25:26.165385 32251 replica.cpp:655] Replica received learned notice for position 2 I1029 08:25:26.166007 32251 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 594003ns I1029 08:25:26.166056 32251 leveldb.cpp:401] Deleting ~1 keys from leveldb took 23309ns I1029 08:25:26.166077 32251 replica.cpp:676] Persisted action at 2 I1029 08:25:26.166100 32251 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1029 08:25:26.178493 32232 sched.cpp:137] Version: 0.21.0 I1029 08:25:26.179029 32256 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:26.179078 32256 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:26.179424 32246 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:26.179678 32259 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.179970 32250 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:26.180165 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:26.180191 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:26.180272 32250 authenticator.hpp:267] Received SASL authentication start I1029 08:25:26.180378 32250 authenticator.hpp:389] Authentication requires more steps I1029 08:25:26.180557 32260 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:26.180704 32254 authenticator.hpp:295] Received SASL authentication step I1029 08:25:26.180737 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:26.180748 32254 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:26.180780 32254 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:26.180804 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:26.180816 32254 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.180824 32254 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.180841 32254 authenticator.hpp:381] Authentication success I1029 08:25:26.180937 32259 authenticatee.hpp:310] Authentication success I1029 08:25:26.180991 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.181422 32259 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:26.181449 32259 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:26.181697 32260 master.cpp:1362] Received registration request for framework 'default' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.181758 32260 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:26.182063 32260 master.cpp:1426] Registering framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.182430 32248 hierarchical_allocator_process.hpp:329] Added framework 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:26.182462 32248 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:26.182462 32261 sched.cpp:407] Framework registered with 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:26.182473 32248 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 15372ns I1029 08:25:26.182554 32261 sched.cpp:421] Scheduler::registered took 60059ns I1029 08:25:26.185515 32260 sched.cpp:227] Scheduler::disconnected took 16607ns I1029 08:25:26.185538 32260 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:26.185567 32260 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:26.185783 32246 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:26.186218 32250 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.186456 32247 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:26.186594 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:26.186621 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:26.186745 32259 authenticator.hpp:267] Received SASL authentication start I1029 08:25:26.186800 32259 authenticator.hpp:389] Authentication requires more steps I1029 08:25:26.186936 32260 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:26.187062 32249 authenticator.hpp:295] Received SASL authentication step I1029 08:25:26.187095 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:26.187108 32249 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:26.187137 32249 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:26.187162 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:26.187175 32249 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.187182 32249 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.187199 32249 authenticator.hpp:381] Authentication success I1029 08:25:26.187327 32249 authenticatee.hpp:310] Authentication success I1029 08:25:26.187366 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.187631 32249 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:26.187659 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:27.028445 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:28.045682 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.017231941secs I1029 08:25:28.045760 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:28.045900 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.045989 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:28.046455 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.046529 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:28.050155 32247 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:28.050217 32247 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:28.050405 32252 master.cpp:1552] Re-registering framework 20141029-082526-3142697795-40696-32232-0000 (default)  at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.050509 32253 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:28.050566 32252 master.cpp:1592] Allowing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 to re-register with an already used id I1029 08:25:28.051084 32257 sched.cpp:449] Framework re-registered with 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:28.051151 32252 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.051167 32257 sched.cpp:463] Scheduler::reregistered took 52801ns I1029 08:25:28.051723 32261 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:28.052042 32249 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:28.052077 32249 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:28.052170 32249 master.cpp:1534] Dropping re-registration request of framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 because new authentication attempt is in progress I1029 08:25:28.052218 32257 authenticator.hpp:267] Received SASL authentication start I1029 08:25:28.052325 32257 authenticator.hpp:389] Authentication requires more steps I1029 08:25:28.052428 32257 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:28.052641 32246 authenticator.hpp:295] Received SASL authentication step I1029 08:25:28.052685 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:28.052701 32246 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:28.052739 32246 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:28.052767 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:28.052779 32246 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:28.052788 32246 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:28.052804 32246 authenticator.hpp:381] Authentication success I1029 08:25:28.052947 32252 authenticatee.hpp:310] Authentication success I1029 08:25:28.053020 32246 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.053462 32247 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:29.046855 32261 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:29.046880 32261 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 35632ns I1029 08:25:30.047458 32253 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:30.047487 32253 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 43031ns I1029 08:25:31.028373 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:31.048673 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:31.048702 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 44769ns I1029 08:25:32.049576 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:32.049604 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 51919ns I1029 08:25:33.050864 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:33.050896 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38019ns I1029 08:25:34.051961 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:34.051993 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 64619ns I1029 08:25:35.052196 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:35.052223 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 34475ns I1029 08:25:36.029101 32259 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:36.053067 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:36.053095 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38354ns I1029 08:25:37.053506 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:37.053536 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38249ns tests/master_authorization_tests.cpp:877: Failure Failed to wait 10secs for frameworkReregisteredMessage I1029 08:25:38.053241 32259 master.cpp:768] Framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 disconnected I1029 08:25:38.053375 32259 master.cpp:1731] Disconnecting framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.053426 32259 master.cpp:1747] Deactivating framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.053932 32259 master.cpp:790] Giving framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 0ns to failover I1029 08:25:38.054072 32257 hierarchical_allocator_process.hpp:405] Deactivated framework 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:38.054208 32257 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:38.054236 32257 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38534ns I1029 08:25:38.054508 32258 master.cpp:3665] Framework failover timeout, removing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.054549 32258 master.cpp:4201] Removing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.055179 32252 master.cpp:677] Master terminating I1029 08:25:38.055181 32254 hierarchical_allocator_process.hpp:360] Removed framework 20141029-082526-3142697795-40696-32232-0000 ../3rdparty/libprocess/include/process/gmock.hpp:345: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <B8-BD 01-88 4A-2B 00-00>, 1-byte object <95>, 1-byte object <30>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] MasterAuthorizationTest.DuplicateReregistration (12042 ms) {noformat}",2
"MESOS-2017","Segfault with ""Pure virtual method called"" when tests fail","The most recent one:  {noformat:title=DRFAllocatorTest.DRFAllocatorProcess} [ RUN      ] DRFAllocatorTest.DRFAllocatorProcess Using temporary directory '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j' I1030 05:55:06.934813 24459 leveldb.cpp:176] Opened db in 3.175202ms I1030 05:55:06.935925 24459 leveldb.cpp:183] Compacted db in 1.077924ms I1030 05:55:06.935976 24459 leveldb.cpp:198] Created db iterator in 16460ns I1030 05:55:06.935995 24459 leveldb.cpp:204] Seeked to beginning of db in 2018ns I1030 05:55:06.936005 24459 leveldb.cpp:273] Iterated through 0 keys in the db in 335ns I1030 05:55:06.936039 24459 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1030 05:55:06.936705 24480 recover.cpp:437] Starting replica recovery I1030 05:55:06.937023 24480 recover.cpp:463] Replica is in EMPTY status I1030 05:55:06.938158 24475 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1030 05:55:06.938859 24482 recover.cpp:188] Received a recover response from a replica in EMPTY status I1030 05:55:06.939486 24474 recover.cpp:554] Updating replica status to STARTING I1030 05:55:06.940249 24489 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 591981ns I1030 05:55:06.940274 24489 replica.cpp:320] Persisted replica status to STARTING I1030 05:55:06.940752 24481 recover.cpp:463] Replica is in STARTING status I1030 05:55:06.940820 24489 master.cpp:312] Master 20141030-055506-3142697795-40429-24459 (pomona.apache.org) started on 67.195.81.187:40429 I1030 05:55:06.940871 24489 master.cpp:358] Master only allowing authenticated frameworks to register I1030 05:55:06.940891 24489 master.cpp:363] Master only allowing authenticated slaves to register I1030 05:55:06.940908 24489 credentials.hpp:36] Loading credentials for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j/credentials' I1030 05:55:06.941215 24489 master.cpp:392] Authorization enabled I1030 05:55:06.941751 24475 master.cpp:120] No whitelist given. Advertising offers for all slaves I1030 05:55:06.942227 24474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1030 05:55:06.942401 24476 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40429 I1030 05:55:06.942895 24483 recover.cpp:188] Received a recover response from a replica in STARTING status I1030 05:55:06.943035 24474 master.cpp:1242] The newly elected leader is master@67.195.81.187:40429 with id 20141030-055506-3142697795-40429-24459 I1030 05:55:06.943063 24474 master.cpp:1255] Elected as the leading master! I1030 05:55:06.943079 24474 master.cpp:1073] Recovering from registrar I1030 05:55:06.943313 24480 registrar.cpp:313] Recovering registrar I1030 05:55:06.943455 24475 recover.cpp:554] Updating replica status to VOTING I1030 05:55:06.944144 24474 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 536365ns I1030 05:55:06.944172 24474 replica.cpp:320] Persisted replica status to VOTING I1030 05:55:06.944355 24489 recover.cpp:568] Successfully joined the Paxos group I1030 05:55:06.944576 24489 recover.cpp:452] Recover process terminated I1030 05:55:06.945155 24486 log.cpp:656] Attempting to start the writer I1030 05:55:06.947013 24473 replica.cpp:474] Replica received implicit promise request with proposal 1 I1030 05:55:06.947854 24473 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 806463ns I1030 05:55:06.947883 24473 replica.cpp:342] Persisted promised to 1 I1030 05:55:06.948547 24481 coordinator.cpp:230] Coordinator attemping to fill missing position I1030 05:55:06.950269 24479 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1030 05:55:06.950933 24479 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 603843ns I1030 05:55:06.950961 24479 replica.cpp:676] Persisted action at 0 I1030 05:55:06.952180 24476 replica.cpp:508] Replica received write request for position 0 I1030 05:55:06.952239 24476 leveldb.cpp:438] Reading position from leveldb took 28437ns I1030 05:55:06.952896 24476 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 623980ns I1030 05:55:06.952926 24476 replica.cpp:676] Persisted action at 0 I1030 05:55:06.953543 24485 replica.cpp:655] Replica received learned notice for position 0 I1030 05:55:06.954082 24485 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 511807ns I1030 05:55:06.954107 24485 replica.cpp:676] Persisted action at 0 I1030 05:55:06.954128 24485 replica.cpp:661] Replica learned NOP action at position 0 I1030 05:55:06.954710 24473 log.cpp:672] Writer started with ending position 0 I1030 05:55:06.956215 24478 leveldb.cpp:438] Reading position from leveldb took 33085ns I1030 05:55:06.959481 24475 registrar.cpp:346] Successfully fetched the registry (0B) in 16.11904ms I1030 05:55:06.959616 24475 registrar.cpp:445] Applied 1 operations in 28239ns; attempting to update the 'registry' I1030 05:55:06.962514 24487 log.cpp:680] Attempting to append 139 bytes to the log I1030 05:55:06.962646 24474 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1030 05:55:06.964146 24486 replica.cpp:508] Replica received write request for position 1 I1030 05:55:06.964962 24486 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 743389ns I1030 05:55:06.964993 24486 replica.cpp:676] Persisted action at 1 I1030 05:55:06.965895 24473 replica.cpp:655] Replica received learned notice for position 1 I1030 05:55:06.966531 24473 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 607242ns I1030 05:55:06.966555 24473 replica.cpp:676] Persisted action at 1 I1030 05:55:06.966578 24473 replica.cpp:661] Replica learned APPEND action at position 1 I1030 05:55:06.967706 24481 registrar.cpp:490] Successfully updated the 'registry' in 8.036096ms I1030 05:55:06.967895 24481 registrar.cpp:376] Successfully recovered registrar I1030 05:55:06.967993 24482 log.cpp:699] Attempting to truncate the log to 1 I1030 05:55:06.968258 24479 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1030 05:55:06.968268 24475 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1030 05:55:06.969156 24476 replica.cpp:508] Replica received write request for position 2 I1030 05:55:06.969678 24476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 491913ns I1030 05:55:06.969703 24476 replica.cpp:676] Persisted action at 2 I1030 05:55:06.970459 24478 replica.cpp:655] Replica received learned notice for position 2 I1030 05:55:06.971060 24478 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 573076ns I1030 05:55:06.971124 24478 leveldb.cpp:401] Deleting ~1 keys from leveldb took 35339ns I1030 05:55:06.971145 24478 replica.cpp:676] Persisted action at 2 I1030 05:55:06.971168 24478 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1030 05:55:06.980211 24459 containerizer.cpp:100] Using isolation: posix/cpu,posix/mem I1030 05:55:06.984153 24473 slave.cpp:169] Slave started on 203)@67.195.81.187:40429 I1030 05:55:07.055308 24473 credentials.hpp:84] Loading credential for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/credential' I1030 05:55:06.988750 24459 sched.cpp:137] Version: 0.21.0 I1030 05:55:07.055521 24473 slave.cpp:276] Slave using credential for: test-principal I1030 05:55:07.055726 24473 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):0; ports(*):[31000-32000] I1030 05:55:07.055865 24473 slave.cpp:318] Slave hostname: pomona.apache.org I1030 05:55:07.055881 24473 slave.cpp:319] Slave checkpoint: false W1030 05:55:07.055889 24473 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I1030 05:55:07.056172 24485 sched.cpp:233] New master detected at master@67.195.81.187:40429 I1030 05:55:07.056222 24485 sched.cpp:283] Authenticating with master master@67.195.81.187:40429 I1030 05:55:07.056717 24485 state.cpp:33] Recovering state from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/meta' I1030 05:55:07.056851 24475 authenticatee.hpp:133] Creating new client SASL connection I1030 05:55:07.057003 24473 status_update_manager.cpp:197] Recovering status update manager I1030 05:55:07.057252 24488 master.cpp:3853] Authenticating scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.057502 24489 containerizer.cpp:281] Recovering containerizer I1030 05:55:07.057524 24475 authenticator.hpp:161] Creating new server SASL connection I1030 05:55:07.057688 24475 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1030 05:55:07.057719 24475 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1030 05:55:07.057919 24481 authenticator.hpp:267] Received SASL authentication start I1030 05:55:07.057968 24481 authenticator.hpp:389] Authentication requires more steps I1030 05:55:07.058070 24473 authenticatee.hpp:270] Received SASL authentication step I1030 05:55:07.058199 24485 authenticator.hpp:295] Received SASL authentication step I1030 05:55:07.058223 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1030 05:55:07.058233 24485 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1030 05:55:07.058259 24485 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1030 05:55:07.058290 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1030 05:55:07.058302 24485 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.058307 24485 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.058320 24485 authenticator.hpp:381] Authentication success I1030 05:55:07.058467 24480 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.058493 24485 slave.cpp:3456] Finished recovery I1030 05:55:07.058593 24478 authenticatee.hpp:310] Authentication success I1030 05:55:07.058838 24478 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40429 I1030 05:55:07.058861 24478 sched.cpp:476] Sending registration request to master@67.195.81.187:40429 I1030 05:55:07.058969 24475 slave.cpp:602] New master detected at master@67.195.81.187:40429 I1030 05:55:07.058969 24487 status_update_manager.cpp:171] Pausing sending status updates I1030 05:55:07.059026 24475 slave.cpp:665] Authenticating with master master@67.195.81.187:40429 I1030 05:55:07.059061 24481 master.cpp:1362] Received registration request for framework 'framework1' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.059131 24481 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role 'role1' I1030 05:55:07.059171 24475 slave.cpp:638] Detecting new master I1030 05:55:07.059214 24482 authenticatee.hpp:133] Creating new client SASL connection I1030 05:55:07.059550 24481 master.cpp:3853] Authenticating slave(203)@67.195.81.187:40429 I1030 05:55:07.059787 24487 authenticator.hpp:161] Creating new server SASL connection I1030 05:55:07.059922 24481 master.cpp:1426] Registering framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.059996 24474 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1030 05:55:07.060034 24474 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1030 05:55:07.060117 24474 authenticator.hpp:267] Received SASL authentication start I1030 05:55:07.060165 24474 authenticator.hpp:389] Authentication requires more steps I1030 05:55:07.060377 24476 hierarchical_allocator_process.hpp:329] Added framework 20141030-055506-3142697795-40429-24459-0000 I1030 05:55:07.060394 24488 sched.cpp:407] Framework registered with 20141030-055506-3142697795-40429-24459-0000 I1030 05:55:07.060403 24476 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1030 05:55:07.060431 24476 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 29857ns I1030 05:55:07.060443 24488 sched.cpp:421] Scheduler::registered took 19407ns I1030 05:55:07.060545 24478 authenticatee.hpp:270] Received SASL authentication step I1030 05:55:07.060645 24478 authenticator.hpp:295] Received SASL authentication step I1030 05:55:07.060673 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1030 05:55:07.060685 24478 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1030 05:55:07.060714 24478 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1030 05:55:07.060740 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1030 05:55:07.060760 24478 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.060770 24478 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.060788 24478 authenticator.hpp:381] Authentication success I1030 05:55:07.060920 24474 authenticatee.hpp:310] Authentication success I1030 05:55:07.060945 24485 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(203)@67.195.81.187:40429 I1030 05:55:07.061388 24489 slave.cpp:722] Successfully authenticated with master master@67.195.81.187:40429 I1030 05:55:07.061504 24489 slave.cpp:1050] Will retry registration in 4.778336ms if necessary I1030 05:55:07.061718 24480 master.cpp:3032] Registering slave at slave(203)@67.195.81.187:40429 (pomona.apache.org) with id 20141030-055506-3142697795-40429-24459-S0 I1030 05:55:07.062119 24489 registrar.cpp:445] Applied 1 operations in 53691ns; attempting to update the 'registry' I1030 05:55:07.065182 24479 log.cpp:680] Attempting to append 316 bytes to the log I1030 05:55:07.065337 24487 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I1030 05:55:07.066359 24474 replica.cpp:508] Replica received write request for position 3 I1030 05:55:07.066643 24474 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 249579ns I1030 05:55:07.066671 24474 replica.cpp:676] Persisted action at 3 I../../src/tests/allocator_tests.cpp:120: Failure Failed to wait 10secs for offers1 1030 05:55:07.067101 24477 slave.cpp:1050] Will retry registration in 24.08243ms if necessary I1030 05:55:07.067140 24473 master.cpp:3020] Ignoring register slave message from slave(203)@67.195.81.187:40429 (pomona.apache.org) as admission is already in progress I1030 05:55:07.067395 24488 replica.cpp:655] Replica received learned notice for position 3 I1030 05:55:07.943416 24478 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1030 05:55:19.804687 24478 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 11.861261123secs I1030 05:55:11.942713 24474 master.cpp:120] No whitelist given. Advertising offers for all slaves I1030 05:55:19.805850 24488 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 1.067224ms I1030 05:55:19.806012 24488 replica.cpp:676] Persisted action at 3 ../../src/tests/allocator_tests.cpp:115: Failure Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(_, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1030 05:55:19.806144 24488 replica.cpp:661] Replica learned APPEND action at position 3 I1030 05:55:19.806695 24473 master.cpp:768] Framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 disconnected I1030 05:55:19.806726 24473 master.cpp:1731] Disconnecting framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.806751 24473 master.cpp:1747] Deactivating framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.806967 24473 master.cpp:790] Giving framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 0ns to failover ../../src/tests/allocator_tests.cpp:94: Failure Actual function call count doesn't match EXPECT_CALL(allocator, slaveAdded(_, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active F1030 05:55:19.806967 24480 logging.cpp:57] RAW: Pure virtual method called I1030 05:55:19.807348 24488 master.cpp:3665] Framework failover timeout, removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.807370 24488 master.cpp:4201] Removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 *** Aborted at 1414648519 (unix time) try ""date -d @1414648519"" if you are using GNU date *** PC: @           0x91bc86 process::PID<>::PID() *** SIGSEGV (@0x0) received by PID 24459 (TID 0x2b86c919a700) from PID 0; stack trace: *** I1030 05:55:19.808631 24489 registrar.cpp:490] Successfully updated the 'registry' in 12.746377984secs     @     0x2b86c55fc340 (unknown) I1030 05:55:19.808938 24473 log.cpp:699] Attempting to truncate the log to 3     @     0x2b86c3327174  google::LogMessage::Fail() I1030 05:55:19.809084 24481 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4     @           0x91bc86 process::PID<>::PID()     @     0x2b86c332c868  google::RawLog__() I1030 05:55:19.810191 24479 replica.cpp:508] Replica received write request for position 4 I1030 05:55:19.810899 24479 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 678090ns I1030 05:55:19.810919 24479 replica.cpp:676] Persisted action at 4     @           0x91bf24 process::Process<>::self() I1030 05:55:19.811635 24485 replica.cpp:655] Replica received learned notice for position 4 I1030 05:55:19.812180 24485 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 523927ns I1030 05:55:19.812228 24485 leveldb.cpp:401] Deleting ~2 keys from leveldb took 29523ns I1030 05:55:19.812242 24485 replica.cpp:676] Persisted action at 4 I    @     0x2b86c29d2a36  __cxa_pure_virtual 1030 05:55:19.812258 24485 replica.cpp:661] Replica learned TRUNCATE action at position 4     @          0x1046936  testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith() I1030 05:55:19.829655 24474 slave.cpp:1050] Will retry registration in 31.785967ms if necessary     @           0x9c0633  testing::internal::FunctionMockerBase<>::InvokeWith()     @           0x9b6152  testing::internal::FunctionMocker<>::Invoke()     @           0x9abdeb  mesos::internal::tests::MockAllocatorProcess<>::frameworkDeactivated()     @           0x91c78f  _ZZN7process8dispatchIN5mesos8internal6master9allocator16AllocatorProcessERKNS1_11FrameworkIDES6_EEvRKNS_3PIDIT_EEMSA_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESJ_     @           0x959ad7  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal6master9allocator16AllocatorProcessERKNS5_11FrameworkIDESA_EEvRKNS0_3PIDIT_EEMSE_FvT0_ET1_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_     @     0x2b86c32d174f  std::function<>::operator()()     @     0x2b86c32b2a17  process::ProcessBase::visit()     @     0x2b86c32bd34c  process::DispatchEvent::visit()     @           0x8e0812  process::ProcessBase::serve()     @     0x2b86c32aec8c  process::ProcessManager::resume() I1030 05:55:22.050081 24478 slave.cpp:1050] Will retry registration in 25.327301ms if necessary     @     0x2b86c32a5351  process::schedule()     @     0x2b86c55f4182  start_thread     @     0x2b86c5904fbd  (unknown) {noformat}",5
"MESOS-2032","Update Maintenance design to account for persistent resources.","With persistent resources and dynamic reservations, frameworks need to know how long the resources will be unavailable for maintenance operations.  This is because for persistent resources, the framework needs to understand how long the persistent resource will be unavailable. For example, if there will be a 10 minute reboot for a kernel upgrade, the framework will not want to re-replicate all of it's persistent data on the machine. Rather, tolerating one unavailable replica for the maintenance window would be preferred.  I'd like to do a revisit of the design to ensure it works well for persistent resources as well.",13
"MESOS-2052","RunState::recover should always recover 'completed'","RunState::recover() will return partial state if it cannot find or open the libprocess pid file. Specifically, it does not recover the 'completed' flag.  However, if the slave has removed the executor (because launch failed or the executor failed to register) the sentinel flag will be set and this fact should be recovered. This ensures that container recovery is not attempted later.  This was discovered when the LinuxLauncher failed to recover because it was asked to recover two containers with the same forkedPid. Investigation showed the executors both OOM'ed before registering, i.e., no libprocess pid file was present. However, the containerizer had detected the OOM, destroyed the container, and notified the slave which cleaned everything up: failing the task and calling removeExecutor (which writes the completed sentinel file.)",1
"MESOS-2056","Refactor fetcher code in preparation for fetcher cache","Refactor/rearrange fetcher-related code so that cache functionality can be dropped in. One could do both together in one go. This is splitting up reviews into smaller chunks. It will not immediately be obvious how this change will be used later, but it will look better-factored and still do the exact same thing as before. In particular, a download routine to be reused several times in launcher/fetcher will be factored out and the remainder of fetcher-related code can be moved from the containerizer realm into fetcher.cpp.",1
"MESOS-2058","Deprecate stats.json endpoints for Master and Slave","With the introduction of the libprocess {{/metrics/snapshot}} endpoint, metrics are now duplicated in the Master and Slave between this and {{stats.json}}. We should deprecate the {{stats.json}} endpoints.  Manual inspection of {{stats.json}} shows that all metrics are now covered by the new endpoint for Master and Slave.",1
"MESOS-2062","Add InverseOffer to Event/Call API.","The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add this is to tack it on to the OFFERS Event:  {code} message Offers {   repeated Offer offers = 1;   repeated InverseOffer inverse_offers = 2; } {code}",3
"MESOS-2070","Implement simple slave recovery behavior for fetcher cache","Clean the fetcher cache completely upon slave restart/recovery. This implements correct, albeit not ideal behavior. More efficient schemes that restore knowledge about cached files or even resume downloads can be added later. ",2
"MESOS-2072","Fetcher cache eviction","Delete files from the fetcher cache so that a given cache size is never exceeded. Succeed in doing so while concurrent downloads are on their way and new requests are pouring in.  Idea: measure the size of each download before it begins, make enough room before the download. This means that only download mechanisms that divulge the size before the main download will be supported. AFAWK, those in use so far have this property.   The calculation of how much space to free needs to be under concurrency control, accumulating all space needed for competing, incomplete download requests. (The Python script that performs fetcher caching for Aurora does not seem to implement this. See https://gist.github.com/zmanji/f41df77510ef9d00265a, imagine several of these programs running concurrently, each one's _cache_eviction() call succeeding, each perceiving the SAME free space being available.)  Ultimately, a conflict resolution strategy is needed if just the downloads underway already exceed the cache capacity. Then, as a fallback, direct download into the work directory will be used for some tasks. TBD how to pick which task gets treated how.   At first, only support copying of any downloaded files to the work directory for task execution. This isolates the task life cycle after starting a task from cache eviction considerations.   (Later, we can add symbolic links that avoid copying. But then eviction of fetched files used by ongoing tasks must be blocked, which adds complexity. another future extension is MESOS-1667 ""Extract from URI while downloading into work dir""). ",8
"MESOS-2074","Fetcher cache test fixture","To accelerate providing good test coverage for the fetcher cache (MESOS-336), we can provide a framework that canonicalizes creating and running a number of tasks and allows easy parametrization with combinations of the following: - whether to cache or not - whether make what has been downloaded executable or not - whether to extract from an archive or not - whether to download from a file system, http, or...  We can create a simple HHTP server in the test fixture to support the latter.  Furthermore, the tests need to be robust wrt. varying numbers of StatusUpdate messages. An accumulating update message sink that reports the final state is needed.  All this has already been programmed in this patch, just needs to be rebased: https://reviews.apache.org/r/21316/",5
"MESOS-2083","Add documentation for maintenance primitives.","We should provide some guiding documentation around the upcoming maintenance primitives in Mesos.  Specifically, we should ensure that general users, framework developers, and operators understand the notion of maintenance in Mesos. Some guidance and recommendations for the latter two audiences will be necessary.",8
"MESOS-2099","Support acquiring/releasing resources with DiskInfo in allocator.","The allocator needs to be changed because the resources are changing while we acquiring or releasing persistent disk resources (resources with DiskInfo). For example, when we release a persistent disk resource, we are changing the release with DiskInfo to a resource with the DiskInfo.",8
"MESOS-2100","Implement master to slave protocol for persistent disk resources.","We need to do the following: 1) Slave needs to send persisted resources when registering (or re-registering). 2) Master needs to send total persisted resources to slave by either re-using RunTask/UpdateFrameworkInfo or introduce new type of messages (like UpdateResources).",8
"MESOS-2104","Correct naming of cgroup memory statistics","mem_rss_bytes is *not* RSS but is the total memory usage (memory.usage_in_bytes) of the cgroup, including file cache etc. Actual RSS is reported as mem_anon_bytes. These, and others, should be consistently named.",3
"MESOS-2128","Turning on cgroups_limit_swap effectively disables memory isolation","Our test runs show that enabling cgroups_limit_swap effectively disables memory isolation altogether.  Per: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html  ""It is important to set the memory.limit_in_bytes parameter before setting the memory.memsw.limit_in_bytes parameter: attempting to do so in the reverse order results in an error. This is because memory.memsw.limit_in_bytes becomes available only after all memory limitations (previously set in memory.limit_in_bytes) are exhausted.""  Looks like the flag sets ""memory.memsw.limit_in_bytes"" if true and ""memory.limit_in_bytes"" if false, but should always set ""memory.limit_in_bytes"" and in addition set ""memory.memsw.limit_in_bytes"" if true. Otherwise the limits won't be set and enforced.  See: https://github.com/apache/mesos/blob/c8598f7f5a24a01b6a68e0f060b79662ee97af89/src/slave/containerizer/isolators/cgroups/mem.cpp#L365 ",2
"MESOS-2144","Segmentation Fault in ExamplesTest.LowLevelSchedulerPthread","Occured on review bot review of: https://reviews.apache.org/r/28262/#review62333  The review doesn't touch code related to the test (And doesn't break libprocess in general)  [ RUN      ] ExamplesTest.LowLevelSchedulerPthread ../../src/tests/script.cpp:83: Failure Failed low_level_scheduler_pthread_test.sh terminated with signal Segmentation fault [  FAILED  ] ExamplesTest.LowLevelSchedulerPthread (7561 ms)  The test ",8
"MESOS-2176","Hierarchical allocator inconsistently accounts for reserved resources. ","Looking through the allocator code for MESOS-2099, I see an issue with respect to accounting reserved resources in the sorters:  Within {{HierarchicalAllocatorProcess::allocate}}, only unreserved resources are accounted for in the sorters, whereas everywhere else (add/remove framework, add/remove slave) we account for both reserved and unreserved.  From git blame, it looks like this issue was introduced over a long course of refactoring and fixes to the allocator. My guess is that this was never caught due to the lack of unit-testability of the allocator (unnecessarily requires a master PID to use an allocator).  From my understanding, the two levels of the hierarchical sorter should have the following semantics:  # Level 1 sorts across roles. Only unreserved resources are shared across roles, and therefore the ""role sorter"" for level 1 should only account for the unreserved resource pool. # Level 2 sorts across frameworks, within a role. Both unreserved and reserved resources are shared across frameworks within a role, and therefore the ""framework sorters"" for level 2 should each account for the reserved resource pool for the role, as well as the unreserved resources _allocated_ inside the role.",5
"MESOS-2182","Performance issue in libprocess SocketManager.","Noticed an issue in production under which the master is slow to respond after failover for ~15 minutes.  After looking at some perf data, the top offender is:  {noformat}     12.02%  mesos-master  libmesos-0.21.0-rc3.so  [.] std::_Rb_tree<process::ProcessBase*, process::ProcessBase*, std::_Identity<process::ProcessBase*>, std::less<process::ProcessBase*>, std::allocator<process::ProcessBase*> >::erase(process::ProcessBase* const&) ...      3.29%  mesos-master  libmesos-0.21.0-rc3.so  [.] process::SocketManager::exited(process::ProcessBase*) {noformat}  It appears that in the SocketManager, whenever an internal Process exits, we loop over all the links unnecessarily:  {code} void SocketManager::exited(ProcessBase* process) {   // An exited event is enough to cause the process to get deleted   // (e.g., by the garbage collector), which means we can't   // dereference process (or even use the address) after we enqueue at   // least one exited event. Thus, we save the process pid.   const UPID pid = process->pid;    // Likewise, we need to save the current time of the process so we   // can update the clocks of linked processes as appropriate.   const Time time = Clock::now(process);    synchronized (this) {     // Iterate through the links, removing any links the process might     // have had and creating exited events for any linked processes.     foreachpair (const UPID& linkee, set<ProcessBase*>& processes, links) {       processes.erase(process);        if (linkee == pid) {         foreach (ProcessBase* linker, processes) {           CHECK(linker != process) << ""Process linked with itself"";           synchronized (timeouts) {             if (Clock::paused()) {               Clock::update(linker, time);             }           }           linker->enqueue(new ExitedEvent(linkee));         }       }     }      links.erase(pid);   } } {code}  On clusters with 10,000s of slaves, this means we hold the socket manager lock for a very expensive loop erasing nothing from a set! This is because, the master contains links from the Master Process to each slave. However, when a random ephemeral Process terminates, we don't need to loop over each slave link.  While we hold this lock, the following calls will block:  {code} class SocketManager { public:   Socket accepted(int s);   void link(ProcessBase* process, const UPID& to);   PID<HttpProxy> proxy(const Socket& socket);   void send(Encoder* encoder, bool persist);   void send(const Response& response,             const Request& request,             const Socket& socket);   void send(Message* message);   Encoder* next(int s);   void close(int s);   void exited(const Node& node);   void exited(ProcessBase* process); ... {code}  As a result, the slave observers and the master can block calling send()!  Short term, we will try to fix this issue by removing the unnecessary looping. Longer term, it would be nice to avoid all this locking when sending on independent sockets.",3
"MESOS-2184","deprecate unused flag 'cgroups_subsystems'","cgroups_subsystems is a slave flag that is no longer used and should be deprecated.",1
"MESOS-2225","FaultToleranceTest.ReregisterFrameworkExitedExecutor is flaky","Observed this on internal CI.  {code} [ RUN      ] FaultToleranceTest.ReregisterFrameworkExitedExecutor Using temporary directory '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_yNprKi' I0114 18:50:51.461186  4720 leveldb.cpp:176] Opened db in 4.866948ms I0114 18:50:51.462057  4720 leveldb.cpp:183] Compacted db in 472256ns I0114 18:50:51.462514  4720 leveldb.cpp:198] Created db iterator in 42905ns I0114 18:50:51.462784  4720 leveldb.cpp:204] Seeked to beginning of db in 21630ns I0114 18:50:51.463068  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 19967ns I0114 18:50:51.463485  4720 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0114 18:50:51.464555  4737 recover.cpp:449] Starting replica recovery I0114 18:50:51.465188  4737 recover.cpp:475] Replica is in EMPTY status I0114 18:50:51.467324  4741 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0114 18:50:51.470118  4736 recover.cpp:195] Received a recover response from a replica in EMPTY status I0114 18:50:51.475424  4739 recover.cpp:566] Updating replica status to STARTING I0114 18:50:51.476553  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 107545ns I0114 18:50:51.476862  4739 replica.cpp:323] Persisted replica status to STARTING I0114 18:50:51.477309  4739 recover.cpp:475] Replica is in STARTING status I0114 18:50:51.479109  4734 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0114 18:50:51.481274  4738 recover.cpp:195] Received a recover response from a replica in STARTING status I0114 18:50:51.482324  4738 recover.cpp:566] Updating replica status to VOTING I0114 18:50:51.482913  4738 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 66011ns I0114 18:50:51.483186  4738 replica.cpp:323] Persisted replica status to VOTING I0114 18:50:51.483608  4738 recover.cpp:580] Successfully joined the Paxos group I0114 18:50:51.484031  4738 recover.cpp:464] Recover process terminated I0114 18:50:51.554949  4734 master.cpp:262] Master 20150114-185051-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018 I0114 18:50:51.555785  4734 master.cpp:308] Master only allowing authenticated frameworks to register I0114 18:50:51.556046  4734 master.cpp:313] Master only allowing authenticated slaves to register I0114 18:50:51.556426  4734 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_yNprKi/credentials' I0114 18:50:51.557003  4734 master.cpp:357] Authorization enabled I0114 18:50:51.558007  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process I0114 18:50:51.558521  4741 whitelist_watcher.cpp:65] No whitelist given I0114 18:50:51.562185  4734 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185051-2272962752-57018-4720 I0114 18:50:51.562680  4734 master.cpp:1232] Elected as the leading master! I0114 18:50:51.562950  4734 master.cpp:1050] Recovering from registrar I0114 18:50:51.564506  4736 registrar.cpp:313] Recovering registrar I0114 18:50:51.566162  4737 log.cpp:660] Attempting to start the writer I0114 18:50:51.568691  4741 replica.cpp:477] Replica received implicit promise request with proposal 1 I0114 18:50:51.569154  4741 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 106885ns I0114 18:50:51.569504  4741 replica.cpp:345] Persisted promised to 1 I0114 18:50:51.573277  4740 coordinator.cpp:230] Coordinator attemping to fill missing position I0114 18:50:51.575623  4739 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0114 18:50:51.576133  4739 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 86360ns I0114 18:50:51.576449  4739 replica.cpp:679] Persisted action at 0 I0114 18:50:51.586966  4736 replica.cpp:511] Replica received write request for position 0 I0114 18:50:51.587666  4736 leveldb.cpp:438] Reading position from leveldb took 60621ns I0114 18:50:51.588043  4736 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 81094ns I0114 18:50:51.588374  4736 replica.cpp:679] Persisted action at 0 I0114 18:50:51.589418  4736 replica.cpp:658] Replica received learned notice for position 0 I0114 18:50:51.590428  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 106648ns I0114 18:50:51.590840  4736 replica.cpp:679] Persisted action at 0 I0114 18:50:51.591104  4736 replica.cpp:664] Replica learned NOP action at position 0 I0114 18:50:51.592260  4734 log.cpp:676] Writer started with ending position 0 I0114 18:50:51.594172  4739 leveldb.cpp:438] Reading position from leveldb took 52163ns I0114 18:50:51.600744  4736 registrar.cpp:346] Successfully fetched the registry (0B) in 35968us I0114 18:50:51.601646  4736 registrar.cpp:445] Applied 1 operations in 184502ns; attempting to update the 'registry' I0114 18:50:51.604329  4737 log.cpp:684] Attempting to append 130 bytes to the log I0114 18:50:51.604966  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0114 18:50:51.606449  4737 replica.cpp:511] Replica received write request for position 1 I0114 18:50:51.606937  4737 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 84877ns I0114 18:50:51.607199  4737 replica.cpp:679] Persisted action at 1 I0114 18:50:51.611934  4741 replica.cpp:658] Replica received learned notice for position 1 I0114 18:50:51.612423  4741 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 113059ns I0114 18:50:51.612794  4741 replica.cpp:679] Persisted action at 1 I0114 18:50:51.613056  4741 replica.cpp:664] Replica learned APPEND action at position 1 I0114 18:50:51.614598  4741 log.cpp:703] Attempting to truncate the log to 1 I0114 18:50:51.615157  4741 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0114 18:50:51.616458  4737 replica.cpp:511] Replica received write request for position 2 I0114 18:50:51.616902  4737 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71716ns I0114 18:50:51.617168  4737 replica.cpp:679] Persisted action at 2 I0114 18:50:51.618505  4740 replica.cpp:658] Replica received learned notice for position 2 I0114 18:50:51.619031  4740 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 78481ns I0114 18:50:51.619567  4740 leveldb.cpp:401] Deleting ~1 keys from leveldb took 59638ns I0114 18:50:51.619832  4740 replica.cpp:679] Persisted action at 2 I0114 18:50:51.620101  4740 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0114 18:50:51.621757  4736 registrar.cpp:490] Successfully updated the 'registry' in 19.78496ms I0114 18:50:51.622658  4736 registrar.cpp:376] Successfully recovered registrar I0114 18:50:51.623261  4736 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register I0114 18:50:51.670349  4739 slave.cpp:173] Slave started on 115)@192.168.122.135:57018 I0114 18:50:51.671133  4739 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/credential' I0114 18:50:51.671685  4739 slave.cpp:282] Slave using credential for: test-principal I0114 18:50:51.672245  4739 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0114 18:50:51.673360  4739 slave.cpp:329] Slave hostname: fedora-19 I0114 18:50:51.673660  4739 slave.cpp:330] Slave checkpoint: false W0114 18:50:51.674052  4739 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0114 18:50:51.677234  4737 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/meta' I0114 18:50:51.684973  4739 status_update_manager.cpp:197] Recovering status update manager I0114 18:50:51.687644  4739 slave.cpp:3519] Finished recovery I0114 18:50:51.688698  4737 slave.cpp:613] New master detected at master@192.168.122.135:57018 I0114 18:50:51.688902  4734 status_update_manager.cpp:171] Pausing sending status updates I0114 18:50:51.689482  4737 slave.cpp:676] Authenticating with master master@192.168.122.135:57018 I0114 18:50:51.689910  4737 slave.cpp:681] Using default CRAM-MD5 authenticatee I0114 18:50:51.690577  4741 authenticatee.hpp:138] Creating new client SASL connection I0114 18:50:51.691453  4737 slave.cpp:649] Detecting new master I0114 18:50:51.691864  4741 master.cpp:4130] Authenticating slave(115)@192.168.122.135:57018 I0114 18:50:51.692369  4741 master.cpp:4141] Using default CRAM-MD5 authenticator I0114 18:50:51.693208  4741 authenticator.hpp:170] Creating new server SASL connection I0114 18:50:51.694598  4738 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0114 18:50:51.694893  4738 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0114 18:50:51.695329  4741 authenticator.hpp:276] Received SASL authentication start I0114 18:50:51.695641  4741 authenticator.hpp:398] Authentication requires more steps I0114 18:50:51.696028  4736 authenticatee.hpp:275] Received SASL authentication step I0114 18:50:51.696486  4741 authenticator.hpp:304] Received SASL authentication step I0114 18:50:51.696753  4741 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0114 18:50:51.697041  4741 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0114 18:50:51.697343  4741 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0114 18:50:51.697685  4741 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0114 18:50:51.697998  4741 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0114 18:50:51.698251  4741 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0114 18:50:51.698580  4741 authenticator.hpp:390] Authentication success I0114 18:50:51.698927  4735 authenticatee.hpp:315] Authentication success I0114 18:50:51.705123  4741 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(115)@192.168.122.135:57018 I0114 18:50:51.705847  4720 sched.cpp:151] Version: 0.22.0 I0114 18:50:51.707159  4736 sched.cpp:248] New master detected at master@192.168.122.135:57018 I0114 18:50:51.707523  4736 sched.cpp:304] Authenticating with master master@192.168.122.135:57018 I0114 18:50:51.707792  4736 sched.cpp:311] Using default CRAM-MD5 authenticatee I0114 18:50:51.708412  4736 authenticatee.hpp:138] Creating new client SASL connection I0114 18:50:51.709316  4735 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018 I0114 18:50:51.709723  4737 master.cpp:4130] Authenticating scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:51.710274  4737 master.cpp:4141] Using default CRAM-MD5 authenticator I0114 18:50:51.710739  4735 slave.cpp:1075] Will retry registration in 17.028024ms if necessary I0114 18:50:51.711304  4737 master.cpp:3276] Registering slave at slave(115)@192.168.122.135:57018 (fedora-19) with id 20150114-185051-2272962752-57018-4720-S0 I0114 18:50:51.711459  4738 authenticator.hpp:170] Creating new server SASL connection I0114 18:50:51.713142  4739 registrar.cpp:445] Applied 1 operations in 100530ns; attempting to update the 'registry' I0114 18:50:51.713465  4738 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0114 18:50:51.715435  4738 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0114 18:50:51.715963  4740 authenticator.hpp:276] Received SASL authentication start I0114 18:50:51.716258  4740 authenticator.hpp:398] Authentication requires more steps I0114 18:50:51.716524  4740 authenticatee.hpp:275] Received SASL authentication step I0114 18:50:51.716784  4740 authenticator.hpp:304] Received SASL authentication step I0114 18:50:51.716979  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0114 18:50:51.717139  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0114 18:50:51.717315  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0114 18:50:51.717542  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0114 18:50:51.717703  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0114 18:50:51.717864  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0114 18:50:51.718040  4740 authenticator.hpp:390] Authentication success I0114 18:50:51.718292  4740 authenticatee.hpp:315] Authentication success I0114 18:50:51.718454  4738 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:51.719012  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018 I0114 18:50:51.719364  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018 I0114 18:50:51.719702  4740 sched.cpp:548] Will retry registration in 746.539282ms if necessary I0114 18:50:51.719902  4735 master.cpp:1417] Received registration request for framework 'default' at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:51.720232  4735 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0114 18:50:51.722206  4735 master.cpp:1481] Registering framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:51.720927  4737 log.cpp:684] Attempting to append 300 bytes to the log I0114 18:50:51.722924  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0114 18:50:51.724269  4737 replica.cpp:511] Replica received write request for position 3 I0114 18:50:51.724817  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 116638ns I0114 18:50:51.728560  4737 replica.cpp:679] Persisted action at 3 I0114 18:50:51.726066  4736 sched.cpp:442] Framework registered with 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.728879  4736 sched.cpp:456] Scheduler::registered took 34885ns I0114 18:50:51.725520  4735 hierarchical_allocator_process.hpp:319] Added framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.731864  4735 hierarchical_allocator_process.hpp:839] No resources available to allocate! I0114 18:50:51.732038  4735 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 214728ns I0114 18:50:51.733106  4738 replica.cpp:658] Replica received learned notice for position 3 I0114 18:50:51.733340  4738 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 83165ns I0114 18:50:51.733538  4738 replica.cpp:679] Persisted action at 3 I0114 18:50:51.733705  4738 replica.cpp:664] Replica learned APPEND action at position 3 I0114 18:50:51.735610  4738 registrar.cpp:490] Successfully updated the 'registry' in 21.936128ms I0114 18:50:51.735805  4739 log.cpp:703] Attempting to truncate the log to 3 I0114 18:50:51.736445  4739 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0114 18:50:51.737664  4739 replica.cpp:511] Replica received write request for position 4 I0114 18:50:51.738013  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 72906ns I0114 18:50:51.738255  4739 replica.cpp:679] Persisted action at 4 I0114 18:50:51.743397  4734 replica.cpp:658] Replica received learned notice for position 4 I0114 18:50:51.743628  4734 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 78832ns I0114 18:50:51.743837  4734 leveldb.cpp:401] Deleting ~2 keys from leveldb took 63991ns I0114 18:50:51.744004  4734 replica.cpp:679] Persisted action at 4 I0114 18:50:51.744168  4734 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0114 18:50:51.745537  4738 master.cpp:3330] Registered slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0114 18:50:51.745968  4734 hierarchical_allocator_process.hpp:453] Added slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0114 18:50:51.746070  4735 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185051-2272962752-57018-4720-S0 I0114 18:50:51.751437  4741 status_update_manager.cpp:178] Resuming sending status updates I0114 18:50:51.752428  4740 master.cpp:4072] Sending 1 offers to framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:51.753764  4740 sched.cpp:605] Scheduler::resourceOffers took 751714ns I0114 18:50:51.754812  4740 master.cpp:2541] Processing reply for offers: [ 20150114-185051-2272962752-57018-4720-O0 ] on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) for framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:51.755040  4740 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' W0114 18:50:51.756431  4741 master.cpp:2124] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0114 18:50:51.756652  4741 master.cpp:2136] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0114 18:50:51.757284  4741 master.hpp:766] Adding task 0 with resources cpus(*):1; mem(*):16 on slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19) I0114 18:50:51.757733  4734 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185051-2272962752-57018-4720-S0 in 9.535066ms I0114 18:50:51.758117  4735 slave.cpp:2588] Received ping from slave-observer(95)@192.168.122.135:57018 I0114 18:50:51.758630  4741 master.cpp:2897] Launching task 0 of framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 with resources cpus(*):1; mem(*):16 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) I0114 18:50:51.759526  4741 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0114 18:50:51.759796  4737 slave.cpp:1130] Got assigned task 0 for framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.761184  4737 slave.cpp:1245] Launching task 0 for framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.763586  4741 hierarchical_allocator_process.hpp:653] Recovered cpus(*):1; mem(*):1008; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):1008; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185051-2272962752-57018-4720-S0 from framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.764034  4741 hierarchical_allocator_process.hpp:689] Framework 20150114-185051-2272962752-57018-4720-0000 filtered slave 20150114-185051-2272962752-57018-4720-S0 for 5secs I0114 18:50:51.764984  4737 slave.cpp:3921] Launching executor default of framework 20150114-185051-2272962752-57018-4720-0000 in work directory '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000/executors/default/runs/dd104b76-b838-431e-ada9-ff7a2b07e694' I0114 18:50:51.775048  4737 exec.cpp:147] Version: 0.22.0 I0114 18:50:51.778069  4736 exec.cpp:197] Executor started at: executor(29)@192.168.122.135:57018 with pid 4720 I0114 18:50:51.778722  4737 slave.cpp:1368] Queuing task '0' for executor default of framework '20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.779103  4737 slave.cpp:566] Successfully attached file '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000/executors/default/runs/dd104b76-b838-431e-ada9-ff7a2b07e694' I0114 18:50:51.779470  4737 slave.cpp:1912] Got registration for executor 'default' of framework 20150114-185051-2272962752-57018-4720-0000 from executor(29)@192.168.122.135:57018 I0114 18:50:51.780288  4740 exec.cpp:221] Executor registered on slave 20150114-185051-2272962752-57018-4720-S0 I0114 18:50:51.782098  4740 exec.cpp:233] Executor::registered took 61371ns I0114 18:50:51.782616  4737 slave.cpp:2031] Flushing queued task 0 for executor 'default' of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.783262  4741 exec.cpp:308] Executor asked to run task '0' I0114 18:50:51.783614  4741 exec.cpp:317] Executor::launchTask took 97020ns I0114 18:50:51.785373  4741 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.785995  4737 slave.cpp:2890] Monitoring executor 'default' of framework '20150114-185051-2272962752-57018-4720-0000' in container 'dd104b76-b838-431e-ada9-ff7a2b07e694' I0114 18:50:51.789064  4737 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 from executor(29)@192.168.122.135:57018 I0114 18:50:51.789553  4735 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.789827  4735 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.790329  4735 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to the slave I0114 18:50:51.790875  4737 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to master@192.168.122.135:57018 I0114 18:50:51.791442  4736 master.cpp:3653] Forwarding status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.791813  4736 master.cpp:3625] Status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 from slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) I0114 18:50:51.792140  4736 master.cpp:4935] Updating the latest state of task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to TASK_RUNNING I0114 18:50:51.792690  4736 sched.cpp:696] Scheduler::statusUpdate took 70266ns I0114 18:50:51.793184  4739 master.cpp:3126] Forwarding status update acknowledgement 3f6824a3-8a23-4029-8505-8eb5f72e472b for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 to slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) I0114 18:50:51.794311  4720 master.cpp:654] Master terminating W0114 18:50:51.794908  4720 master.cpp:4980] Removing task 0 with resources cpus(*):1; mem(*):16 of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_RUNNING I0114 18:50:51.795251  4739 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.795881  4739 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to executor(29)@192.168.122.135:57018 I0114 18:50:51.796308  4739 exec.cpp:354] Executor received status update acknowledgement 3f6824a3-8a23-4029-8505-8eb5f72e472b for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.795326  4741 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.797854  4741 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.797144  4720 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) I0114 18:50:51.796748  4734 hierarchical_allocator_process.hpp:653] Recovered cpus(*):1; mem(*):16 (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185051-2272962752-57018-4720-S0 from framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:51.802438  4739 slave.cpp:2673] master@192.168.122.135:57018 exited W0114 18:50:51.802707  4739 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected I0114 18:50:51.849522  4720 leveldb.cpp:176] Opened db in 1.773376ms I0114 18:50:51.860327  4720 leveldb.cpp:183] Compacted db in 1.475626ms I0114 18:50:51.860661  4720 leveldb.cpp:198] Created db iterator in 58499ns I0114 18:50:51.861027  4720 leveldb.cpp:204] Seeked to beginning of db in 53681ns I0114 18:50:51.861476  4720 leveldb.cpp:273] Iterated through 3 keys in the db in 195975ns I0114 18:50:51.861803  4720 replica.cpp:744] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned I0114 18:50:51.862931  4737 recover.cpp:449] Starting replica recovery I0114 18:50:51.863837  4737 recover.cpp:475] Replica is in VOTING status I0114 18:50:51.864320  4737 recover.cpp:464] Recover process terminated I0114 18:50:51.912767  4734 master.cpp:262] Master 20150114-185051-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018 I0114 18:50:51.913460  4734 master.cpp:308] Master only allowing authenticated frameworks to register I0114 18:50:51.913712  4734 master.cpp:313] Master only allowing authenticated slaves to register I0114 18:50:51.914023  4734 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_yNprKi/credentials' I0114 18:50:51.914626  4734 master.cpp:357] Authorization enabled I0114 18:50:51.915576  4739 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process I0114 18:50:51.916064  4735 whitelist_watcher.cpp:65] No whitelist given I0114 18:50:51.919319  4734 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185051-2272962752-57018-4720 I0114 18:50:51.921718  4734 master.cpp:1232] Elected as the leading master! I0114 18:50:51.921975  4734 master.cpp:1050] Recovering from registrar I0114 18:50:51.922523  4738 registrar.cpp:313] Recovering registrar I0114 18:50:51.924142  4738 log.cpp:660] Attempting to start the writer I0114 18:50:51.926363  4739 replica.cpp:477] Replica received implicit promise request with proposal 2 I0114 18:50:51.927110  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 147268ns I0114 18:50:51.927486  4739 replica.cpp:345] Persisted promised to 2 I0114 18:50:51.935008  4741 coordinator.cpp:230] Coordinator attemping to fill missing position I0114 18:50:51.935816  4741 log.cpp:676] Writer started with ending position 4 I0114 18:50:51.937769  4739 leveldb.cpp:438] Reading position from leveldb took 108522ns I0114 18:50:51.938480  4739 leveldb.cpp:438] Reading position from leveldb took 171418ns I0114 18:50:51.942811  4740 registrar.cpp:346] Successfully fetched the registry (261B) in 19.91296ms I0114 18:50:51.943493  4740 registrar.cpp:445] Applied 1 operations in 96988ns; attempting to update the 'registry' I0114 18:50:51.946138  4737 log.cpp:684] Attempting to append 300 bytes to the log I0114 18:50:51.950773  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5 I0114 18:50:51.954259  4739 replica.cpp:511] Replica received write request for position 5 I0114 18:50:51.958901  4739 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 351525ns I0114 18:50:51.959277  4739 replica.cpp:679] Persisted action at 5 I0114 18:50:51.966125  4736 replica.cpp:658] Replica received learned notice for position 5 I0114 18:50:51.966882  4736 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 114790ns I0114 18:50:51.967159  4736 replica.cpp:679] Persisted action at 5 I0114 18:50:51.967515  4736 replica.cpp:664] Replica learned APPEND action at position 5 I0114 18:50:51.971989  4739 registrar.cpp:490] Successfully updated the 'registry' in 28.18304ms I0114 18:50:51.972854  4739 registrar.cpp:376] Successfully recovered registrar I0114 18:50:51.973675  4737 master.cpp:1077] Recovered 1 slaves from the Registry (261B) ; allowing 10mins for slaves to re-register I0114 18:50:51.974957  4737 log.cpp:703] Attempting to truncate the log to 5 I0114 18:50:51.975620  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6 I0114 18:50:51.977298  4740 replica.cpp:511] Replica received write request for position 6 I0114 18:50:51.978060  4740 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 108071ns I0114 18:50:51.978374  4740 replica.cpp:679] Persisted action at 6 I0114 18:50:51.982532  4737 replica.cpp:658] Replica received learned notice for position 6 I0114 18:50:51.983160  4737 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 89982ns I0114 18:50:51.983505  4737 leveldb.cpp:401] Deleting ~2 keys from leveldb took 64662ns I0114 18:50:51.983806  4737 replica.cpp:679] Persisted action at 6 I0114 18:50:51.984136  4737 replica.cpp:664] Replica learned TRUNCATE action at position 6 I0114 18:50:51.997160  4740 slave.cpp:613] New master detected at master@192.168.122.135:57018 I0114 18:50:51.998111  4740 slave.cpp:676] Authenticating with master master@192.168.122.135:57018 I0114 18:50:51.998437  4740 slave.cpp:681] Using default CRAM-MD5 authenticatee I0114 18:50:51.999161  4734 authenticatee.hpp:138] Creating new client SASL connection I0114 18:50:51.997766  4735 status_update_manager.cpp:171] Pausing sending status updates I0114 18:50:52.000628  4740 slave.cpp:649] Detecting new master I0114 18:50:52.001258  4734 master.cpp:4130] Authenticating slave(115)@192.168.122.135:57018 I0114 18:50:52.002085  4734 master.cpp:4141] Using default CRAM-MD5 authenticator I0114 18:50:52.003057  4734 authenticator.hpp:170] Creating new server SASL connection I0114 18:50:52.004458  4735 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0114 18:50:52.004762  4735 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0114 18:50:52.005210  4734 authenticator.hpp:276] Received SASL authentication start I0114 18:50:52.005544  4734 authenticator.hpp:398] Authentication requires more steps I0114 18:50:52.006116  4736 authenticatee.hpp:275] Received SASL authentication step I0114 18:50:52.006676  4734 authenticator.hpp:304] Received SASL authentication step I0114 18:50:52.007045  4734 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0114 18:50:52.007340  4734 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0114 18:50:52.007733  4734 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0114 18:50:52.008149  4734 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0114 18:50:52.008437  4734 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0114 18:50:52.008714  4734 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0114 18:50:52.009009  4734 authenticator.hpp:390] Authentication success I0114 18:50:52.009459  4741 authenticatee.hpp:315] Authentication success I0114 18:50:52.018327  4738 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(115)@192.168.122.135:57018 I0114 18:50:52.018959  4741 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018 I0114 18:50:52.020071  4739 master.cpp:3453] Re-registering slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) I0114 18:50:52.021256  4739 registrar.cpp:445] Applied 1 operations in 109203ns; attempting to update the 'registry' I0114 18:50:52.023926  4737 log.cpp:684] Attempting to append 300 bytes to the log I0114 18:50:52.024710  4735 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7 I0114 18:50:52.026480  4734 replica.cpp:511] Replica received write request for position 7 I0114 18:50:52.027065  4734 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 109150ns I0114 18:50:52.027524  4734 replica.cpp:679] Persisted action at 7 I0114 18:50:52.028818  4738 replica.cpp:658] Replica received learned notice for position 7 I0114 18:50:52.029525  4738 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 185197ns I0114 18:50:52.029930  4738 replica.cpp:679] Persisted action at 7 I0114 18:50:52.030205  4738 replica.cpp:664] Replica learned APPEND action at position 7 I0114 18:50:52.031692  4735 log.cpp:703] Attempting to truncate the log to 7 I0114 18:50:52.032083  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8 I0114 18:50:52.033411  4740 replica.cpp:511] Replica received write request for position 8 I0114 18:50:52.033768  4740 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 78202ns I0114 18:50:52.034054  4740 replica.cpp:679] Persisted action at 8 I0114 18:50:52.035274  4740 replica.cpp:658] Replica received learned notice for position 8 I0114 18:50:52.035912  4740 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 80144ns I0114 18:50:52.036262  4740 leveldb.cpp:401] Deleting ~2 keys from leveldb took 93273ns I0114 18:50:52.036558  4740 replica.cpp:679] Persisted action at 8 I0114 18:50:52.036883  4740 replica.cpp:664] Replica learned TRUNCATE action at position 8 I0114 18:50:52.038254  4741 slave.cpp:1075] Will retry registration in 5.240065ms if necessary I0114 18:50:52.044471  4739 registrar.cpp:490] Successfully updated the 'registry' in 22.825984ms I0114 18:50:52.045918  4740 master.hpp:766] Adding task 0 with resources cpus(*):1; mem(*):16 on slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19) W0114 18:50:52.052153  4740 master.cpp:4697] Possibly orphaned task 0 of framework 20150114-185051-2272962752-57018-4720-0000 running on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) I0114 18:50:52.053467  4738 hierarchical_allocator_process.hpp:453] Added slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):1; mem(*):1008; disk(*):1024; ports(*):[31000-32000] available) I0114 18:50:52.054124  4738 hierarchical_allocator_process.hpp:839] No resources available to allocate! I0114 18:50:52.054733  4738 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185051-2272962752-57018-4720-S0 in 795150ns I0114 18:50:52.055675  4736 slave.cpp:1075] Will retry registration in 4.9981ms if necessary I0114 18:50:52.056367  4736 slave.cpp:2588] Received ping from slave-observer(96)@192.168.122.135:57018 I0114 18:50:52.056958  4740 master.cpp:3521] Re-registered slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0114 18:50:52.057782  4740 master.cpp:3402] Re-registering slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) I0114 18:50:52.058290  4734 slave.cpp:849] Re-registered with master master@192.168.122.135:57018 I0114 18:50:52.061352  4734 slave.cpp:2948] Executor 'default' of framework 20150114-185051-2272962752-57018-4720-0000 exited with status 0 I0114 18:50:52.061640  4737 status_update_manager.cpp:178] Resuming sending status updates I0114 18:50:52.064230  4734 slave.cpp:2265] Handling status update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 from @0.0.0.0:0 I0114 18:50:52.064846  4734 slave.cpp:4229] Terminating task 0 W0114 18:50:52.065830  4734 slave.cpp:856] Already re-registered with master master@192.168.122.135:57018 I0114 18:50:52.067150  4739 master.cpp:3705] Executor default of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) exited with status 0 I0114 18:50:52.070163  4737 status_update_manager.cpp:317] Received status update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.070940  4737 status_update_manager.cpp:371] Forwarding update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to the slave I0114 18:50:52.071951  4736 sched.cpp:242] Scheduler::disconnected took 43823ns I0114 18:50:52.072419  4736 sched.cpp:248] New master detected at master@192.168.122.135:57018 I0114 18:50:52.072935  4736 sched.cpp:304] Authenticating with master master@192.168.122.135:57018 I0114 18:50:52.073321  4736 sched.cpp:311] Using default CRAM-MD5 authenticatee I0114 18:50:52.074064  4736 authenticatee.hpp:138] Creating new client SASL connection I0114 18:50:52.076202  4734 slave.cpp:2508] Forwarding the update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to master@192.168.122.135:57018 I0114 18:50:52.077155  4734 slave.cpp:2435] Status update manager successfully handled status update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.076659  4739 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) I0114 18:50:52.080638  4739 master.cpp:4130] Authenticating scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:52.081056  4739 master.cpp:4141] Using default CRAM-MD5 authenticator I0114 18:50:52.081892  4741 authenticator.hpp:170] Creating new server SASL connection I0114 18:50:52.083005  4741 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0114 18:50:52.083470  4741 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0114 18:50:52.083953  4741 authenticator.hpp:276] Received SASL authentication start I0114 18:50:52.084355  4741 authenticator.hpp:398] Authentication requires more steps I0114 18:50:52.084794  4741 authenticatee.hpp:275] Received SASL authentication step I0114 18:50:52.085310  4737 authenticator.hpp:304] Received SASL authentication step I0114 18:50:52.085654  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0114 18:50:52.085969  4737 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0114 18:50:52.086297  4737 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0114 18:50:52.086642  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0114 18:50:52.086942  4737 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0114 18:50:52.087226  4737 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0114 18:50:52.087550  4737 authenticator.hpp:390] Authentication success I0114 18:50:52.087934  4741 authenticatee.hpp:315] Authentication success I0114 18:50:52.088513  4741 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018 I0114 18:50:52.088899  4741 sched.cpp:515] Sending registration request to master@192.168.122.135:57018 I0114 18:50:52.089360  4741 sched.cpp:548] Will retry registration in 1.858884079secs if necessary I0114 18:50:52.090150  4739 master.cpp:1522] Queuing up re-registration request for framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 because authentication is still in progress I0114 18:50:52.095142  4739 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:52.108275  4739 master.cpp:1554] Received re-registration request from framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:52.108742  4739 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0114 18:50:52.109735  4739 master.cpp:1607] Re-registering framework 20150114-185051-2272962752-57018-4720-0000 (default)  at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:52.110985  4735 hierarchical_allocator_process.hpp:319] Added framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.120640  4735 hierarchical_allocator_process.hpp:746] Performed allocation for 1 slaves in 9.254989ms I0114 18:50:52.121709  4734 slave.cpp:1762] Updating framework 20150114-185051-2272962752-57018-4720-0000 pid to scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:52.122190  4734 status_update_manager.cpp:178] Resuming sending status updates W0114 18:50:52.122694  4734 status_update_manager.cpp:185] Resending status update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.123072  4734 status_update_manager.cpp:371] Forwarding update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to the slave I0114 18:50:52.123733  4734 slave.cpp:2508] Forwarding the update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to master@192.168.122.135:57018 I0114 18:50:52.124590  4720 sched.cpp:1471] Asked to stop the driver I0114 18:50:52.125461  4739 master.cpp:4072] Sending 1 offers to framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 I0114 18:50:52.126096  4739 master.cpp:654] Master terminating W0114 18:50:52.126626  4739 master.cpp:4980] Removing task 0 with resources cpus(*):1; mem(*):16 of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_RUNNING I0114 18:50:52.125669  4735 sched.cpp:423] Ignoring framework registered message because the driver is not running! I0114 18:50:52.127410  4735 sched.cpp:808] Stopping framework '20150114-185051-2272962752-57018-4720-0000' I0114 18:50:52.128592  4735 hierarchical_allocator_process.hpp:653] Recovered cpus(*):1; mem(*):16 (total allocatable: cpus(*):1; mem(*):16) on slave 20150114-185051-2272962752-57018-4720-S0 from framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.132880  4740 slave.cpp:2673] master@192.168.122.135:57018 exited W0114 18:50:52.133318  4740 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected I0114 18:50:52.173943  4720 slave.cpp:495] Slave terminating I0114 18:50:52.174928  4720 slave.cpp:1585] Asked to shut down framework 20150114-185051-2272962752-57018-4720-0000 by @0.0.0.0:0 I0114 18:50:52.175448  4720 slave.cpp:1610] Shutting down framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.175858  4720 slave.cpp:3057] Cleaning up executor 'default' of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.176615  4740 gc.cpp:56] Scheduling '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000/executors/default/runs/dd104b76-b838-431e-ada9-ff7a2b07e694' for gc 6.99999795726815days in the future I0114 18:50:52.177549  4734 gc.cpp:56] Scheduling '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000/executors/default' for gc 6.99999794655111days in the future I0114 18:50:52.178169  4720 slave.cpp:3136] Cleaning up framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.178683  4741 status_update_manager.cpp:279] Closing status update streams for framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.179054  4741 status_update_manager.cpp:525] Cleaning up status update stream for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 I0114 18:50:52.179730  4737 gc.cpp:56] Scheduling '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000' for gc 6.9999979210637days in the future tests/fault_tolerance_tests.cpp:1213: Failure Actual function call count doesn't match EXPECT_CALL(sched, registered(&driver, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] FaultToleranceTest.ReregisterFrameworkExitedExecutor (776 ms) {code}",2
"MESOS-2226","HookTest.VerifySlaveLaunchExecutorHook is flaky","Observed this on internal CI  {code} [ RUN      ] HookTest.VerifySlaveLaunchExecutorHook Using temporary directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME' I0114 18:51:34.659353  4720 leveldb.cpp:176] Opened db in 1.255951ms I0114 18:51:34.662112  4720 leveldb.cpp:183] Compacted db in 596090ns I0114 18:51:34.662364  4720 leveldb.cpp:198] Created db iterator in 177877ns I0114 18:51:34.662719  4720 leveldb.cpp:204] Seeked to beginning of db in 19709ns I0114 18:51:34.663010  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 18208ns I0114 18:51:34.663312  4720 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0114 18:51:34.664266  4735 recover.cpp:449] Starting replica recovery I0114 18:51:34.664908  4735 recover.cpp:475] Replica is in EMPTY status I0114 18:51:34.667842  4734 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0114 18:51:34.669117  4735 recover.cpp:195] Received a recover response from a replica in EMPTY status I0114 18:51:34.677913  4735 recover.cpp:566] Updating replica status to STARTING I0114 18:51:34.683157  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 137939ns I0114 18:51:34.683507  4735 replica.cpp:323] Persisted replica status to STARTING I0114 18:51:34.684013  4735 recover.cpp:475] Replica is in STARTING status I0114 18:51:34.685554  4738 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0114 18:51:34.696512  4736 recover.cpp:195] Received a recover response from a replica in STARTING status I0114 18:51:34.700552  4735 recover.cpp:566] Updating replica status to VOTING I0114 18:51:34.701128  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 115624ns I0114 18:51:34.701478  4735 replica.cpp:323] Persisted replica status to VOTING I0114 18:51:34.701817  4735 recover.cpp:580] Successfully joined the Paxos group I0114 18:51:34.702569  4735 recover.cpp:464] Recover process terminated I0114 18:51:34.716439  4736 master.cpp:262] Master 20150114-185134-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018 I0114 18:51:34.716913  4736 master.cpp:308] Master only allowing authenticated frameworks to register I0114 18:51:34.717136  4736 master.cpp:313] Master only allowing authenticated slaves to register I0114 18:51:34.717488  4736 credentials.hpp:36] Loading credentials for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME/credentials' I0114 18:51:34.718077  4736 master.cpp:357] Authorization enabled I0114 18:51:34.719238  4738 whitelist_watcher.cpp:65] No whitelist given I0114 18:51:34.719755  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process I0114 18:51:34.722584  4736 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185134-2272962752-57018-4720 I0114 18:51:34.722865  4736 master.cpp:1232] Elected as the leading master! I0114 18:51:34.723310  4736 master.cpp:1050] Recovering from registrar I0114 18:51:34.723760  4734 registrar.cpp:313] Recovering registrar I0114 18:51:34.725229  4740 log.cpp:660] Attempting to start the writer I0114 18:51:34.727893  4739 replica.cpp:477] Replica received implicit promise request with proposal 1 I0114 18:51:34.728425  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 114781ns I0114 18:51:34.728662  4739 replica.cpp:345] Persisted promised to 1 I0114 18:51:34.731271  4741 coordinator.cpp:230] Coordinator attemping to fill missing position I0114 18:51:34.733223  4734 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0114 18:51:34.734076  4734 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 87441ns I0114 18:51:34.734441  4734 replica.cpp:679] Persisted action at 0 I0114 18:51:34.740272  4739 replica.cpp:511] Replica received write request for position 0 I0114 18:51:34.740910  4739 leveldb.cpp:438] Reading position from leveldb took 59846ns I0114 18:51:34.741672  4739 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 189259ns I0114 18:51:34.741919  4739 replica.cpp:679] Persisted action at 0 I0114 18:51:34.743000  4739 replica.cpp:658] Replica received learned notice for position 0 I0114 18:51:34.746844  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 328487ns I0114 18:51:34.747118  4739 replica.cpp:679] Persisted action at 0 I0114 18:51:34.747553  4739 replica.cpp:664] Replica learned NOP action at position 0 I0114 18:51:34.751344  4737 log.cpp:676] Writer started with ending position 0 I0114 18:51:34.753504  4734 leveldb.cpp:438] Reading position from leveldb took 61183ns I0114 18:51:34.762962  4737 registrar.cpp:346] Successfully fetched the registry (0B) in 38.907904ms I0114 18:51:34.763610  4737 registrar.cpp:445] Applied 1 operations in 67206ns; attempting to update the 'registry' I0114 18:51:34.766079  4736 log.cpp:684] Attempting to append 130 bytes to the log I0114 18:51:34.766769  4736 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0114 18:51:34.768215  4741 replica.cpp:511] Replica received write request for position 1 I0114 18:51:34.768759  4741 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 87970ns I0114 18:51:34.768995  4741 replica.cpp:679] Persisted action at 1 I0114 18:51:34.770691  4736 replica.cpp:658] Replica received learned notice for position 1 I0114 18:51:34.771273  4736 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 83590ns I0114 18:51:34.771579  4736 replica.cpp:679] Persisted action at 1 I0114 18:51:34.771917  4736 replica.cpp:664] Replica learned APPEND action at position 1 I0114 18:51:34.773252  4738 log.cpp:703] Attempting to truncate the log to 1 I0114 18:51:34.773756  4735 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0114 18:51:34.775552  4736 replica.cpp:511] Replica received write request for position 2 I0114 18:51:34.775846  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71503ns I0114 18:51:34.776695  4736 replica.cpp:679] Persisted action at 2 I0114 18:51:34.785259  4739 replica.cpp:658] Replica received learned notice for position 2 I0114 18:51:34.786252  4737 registrar.cpp:490] Successfully updated the 'registry' in 22.340864ms I0114 18:51:34.787094  4737 registrar.cpp:376] Successfully recovered registrar I0114 18:51:34.787749  4737 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register I0114 18:51:34.787282  4739 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 707150ns I0114 18:51:34.788692  4739 leveldb.cpp:401] Deleting ~1 keys from leveldb took 60262ns I0114 18:51:34.789048  4739 replica.cpp:679] Persisted action at 2 I0114 18:51:34.789329  4739 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0114 18:51:34.819548  4738 slave.cpp:173] Slave started on 171)@192.168.122.135:57018 I0114 18:51:34.820530  4738 credentials.hpp:84] Loading credential for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/credential' I0114 18:51:34.820952  4738 slave.cpp:282] Slave using credential for: test-principal I0114 18:51:34.821516  4738 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0114 18:51:34.822217  4738 slave.cpp:329] Slave hostname: fedora-19 I0114 18:51:34.822502  4738 slave.cpp:330] Slave checkpoint: false W0114 18:51:34.822857  4738 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0114 18:51:34.824998  4737 state.cpp:33] Recovering state from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/meta' I0114 18:51:34.834015  4738 status_update_manager.cpp:197] Recovering status update manager I0114 18:51:34.834810  4738 slave.cpp:3519] Finished recovery I0114 18:51:34.835906  4734 status_update_manager.cpp:171] Pausing sending status updates I0114 18:51:34.836423  4738 slave.cpp:613] New master detected at master@192.168.122.135:57018 I0114 18:51:34.836908  4738 slave.cpp:676] Authenticating with master master@192.168.122.135:57018 I0114 18:51:34.837190  4738 slave.cpp:681] Using default CRAM-MD5 authenticatee I0114 18:51:34.837820  4737 authenticatee.hpp:138] Creating new client SASL connection I0114 18:51:34.838784  4738 slave.cpp:649] Detecting new master I0114 18:51:34.839306  4740 master.cpp:4130] Authenticating slave(171)@192.168.122.135:57018 I0114 18:51:34.839957  4740 master.cpp:4141] Using default CRAM-MD5 authenticator I0114 18:51:34.841236  4740 authenticator.hpp:170] Creating new server SASL connection I0114 18:51:34.842681  4741 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0114 18:51:34.843118  4741 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0114 18:51:34.843581  4740 authenticator.hpp:276] Received SASL authentication start I0114 18:51:34.843962  4740 authenticator.hpp:398] Authentication requires more steps I0114 18:51:34.844357  4740 authenticatee.hpp:275] Received SASL authentication step I0114 18:51:34.844780  4740 authenticator.hpp:304] Received SASL authentication step I0114 18:51:34.845113  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0114 18:51:34.845507  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0114 18:51:34.845835  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0114 18:51:34.846238  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0114 18:51:34.846542  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0114 18:51:34.846806  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0114 18:51:34.847110  4740 authenticator.hpp:390] Authentication success I0114 18:51:34.847808  4734 authenticatee.hpp:315] Authentication success I0114 18:51:34.851029  4734 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018 I0114 18:51:34.851608  4737 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(171)@192.168.122.135:57018 I0114 18:51:34.854962  4720 sched.cpp:151] Version: 0.22.0 I0114 18:51:34.856674  4734 slave.cpp:1075] Will retry registration in 3.085482ms if necessary I0114 18:51:34.857434  4739 sched.cpp:248] New master detected at master@192.168.122.135:57018 I0114 18:51:34.861433  4739 sched.cpp:304] Authenticating with master master@192.168.122.135:57018 I0114 18:51:34.861693  4739 sched.cpp:311] Using default CRAM-MD5 authenticatee I0114 18:51:34.857795  4737 master.cpp:3276] Registering slave at slave(171)@192.168.122.135:57018 (fedora-19) with id 20150114-185134-2272962752-57018-4720-S0 I0114 18:51:34.862951  4737 authenticatee.hpp:138] Creating new client SASL connection I0114 18:51:34.863919  4735 registrar.cpp:445] Applied 1 operations in 120272ns; attempting to update the 'registry' I0114 18:51:34.864645  4738 master.cpp:4130] Authenticating scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 I0114 18:51:34.865033  4738 master.cpp:4141] Using default CRAM-MD5 authenticator I0114 18:51:34.866904  4738 authenticator.hpp:170] Creating new server SASL connection I0114 18:51:34.868840  4737 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0114 18:51:34.869125  4737 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0114 18:51:34.869523  4737 authenticator.hpp:276] Received SASL authentication start I0114 18:51:34.869835  4737 authenticator.hpp:398] Authentication requires more steps I0114 18:51:34.870213  4737 authenticatee.hpp:275] Received SASL authentication step I0114 18:51:34.870622  4737 authenticator.hpp:304] Received SASL authentication step I0114 18:51:34.870946  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0114 18:51:34.871219  4737 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0114 18:51:34.871554  4737 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0114 18:51:34.871968  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0114 18:51:34.872297  4737 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0114 18:51:34.872655  4737 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0114 18:51:34.873024  4737 authenticator.hpp:390] Authentication success I0114 18:51:34.873428  4737 authenticatee.hpp:315] Authentication success I0114 18:51:34.873632  4739 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 I0114 18:51:34.875006  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018 I0114 18:51:34.875319  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018 I0114 18:51:34.876200  4740 sched.cpp:548] Will retry registration in 1.952991346secs if necessary I0114 18:51:34.876729  4738 master.cpp:1417] Received registration request for framework 'default' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 I0114 18:51:34.877040  4738 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0114 18:51:34.878059  4738 master.cpp:1481] Registering framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 I0114 18:51:34.878473  4739 log.cpp:684] Attempting to append 300 bytes to the log I0114 18:51:34.879464  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0114 18:51:34.880116  4734 hierarchical_allocator_process.hpp:319] Added framework 20150114-185134-2272962752-57018-4720-0000 I0114 18:51:34.880470  4734 hierarchical_allocator_process.hpp:839] No resources available to allocate! I0114 18:51:34.882331  4734 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 1.901284ms I0114 18:51:34.884024  4741 sched.cpp:442] Framework registered with 20150114-185134-2272962752-57018-4720-0000 I0114 18:51:34.884454  4741 sched.cpp:456] Scheduler::registered took 44320ns I0114 18:51:34.881965  4737 replica.cpp:511] Replica received write request for position 3 I0114 18:51:34.885218  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 134480ns I0114 18:51:34.885716  4737 replica.cpp:679] Persisted action at 3 I0114 18:51:34.886034  4739 slave.cpp:1075] Will retry registration in 22.947772ms if necessary I0114 18:51:34.886291  4740 master.cpp:3264] Ignoring register slave message from slave(171)@192.168.122.135:57018 (fedora-19) as admission is already in progress I0114 18:51:34.894690  4736 replica.cpp:658] Replica received learned notice for position 3 I0114 18:51:34.898638  4736 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 215501ns I0114 18:51:34.899055  4736 replica.cpp:679] Persisted action at 3 I0114 18:51:34.899416  4736 replica.cpp:664] Replica learned APPEND action at position 3 I0114 18:51:34.911782  4736 registrar.cpp:490] Successfully updated the 'registry' in 46.176768ms I0114 18:51:34.912286  4740 log.cpp:703] Attempting to truncate the log to 3 I0114 18:51:34.913108  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0114 18:51:34.915027  4736 master.cpp:3330] Registered slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0114 18:51:34.915642  4735 hierarchical_allocator_process.hpp:453] Added slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0114 18:51:34.917809  4735 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185134-2272962752-57018-4720-S0 in 514027ns I0114 18:51:34.916689  4738 replica.cpp:511] Replica received write request for position 4 I0114 18:51:34.915784  4741 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185134-2272962752-57018-4720-S0 I0114 18:51:34.919293  4741 slave.cpp:2588] Received ping from slave-observer(156)@192.168.122.135:57018 I0114 18:51:34.919775  4740 status_update_manager.cpp:178] Resuming sending status updates I0114 18:51:34.920374  4736 master.cpp:4072] Sending 1 offers to framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 I0114 18:51:34.920569  4738 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 1.540136ms I0114 18:51:34.921092  4738 replica.cpp:679] Persisted action at 4 I0114 18:51:34.927111  4735 replica.cpp:658] Replica received learned notice for position 4 I0114 18:51:34.927299  4734 sched.cpp:605] Scheduler::resourceOffers took 1.335524ms I0114 18:51:34.930418  4735 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.596377ms I0114 18:51:34.930882  4735 leveldb.cpp:401] Deleting ~2 keys from leveldb took 67578ns I0114 18:51:34.931115  4735 replica.cpp:679] Persisted action at 4 I0114 18:51:34.931529  4735 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0114 18:51:34.930356  4734 master.cpp:2541] Processing reply for offers: [ 20150114-185134-2272962752-57018-4720-O0 ] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) for framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 I0114 18:51:34.932834  4734 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins' W0114 18:51:34.934442  4736 master.cpp:2124] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0114 18:51:34.934960  4736 master.cpp:2136] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0114 18:51:34.935878  4736 master.hpp:766] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19) I0114 18:51:34.939453  4738 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0114 18:51:34.939950  4736 master.cpp:2897] Launching task 1 of framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) I0114 18:51:34.940467  4736 test_hook_module.cpp:52] Executing 'masterLaunchTaskLabelDecorator' hook I0114 18:51:34.941490  4740 slave.cpp:1130] Got assigned task 1 for framework 20150114-185134-2272962752-57018-4720-0000 I0114 18:51:34.942873  4740 slave.cpp:1245] Launching task 1 for framework 20150114-185134-2272962752-57018-4720-0000 I0114 18:51:34.943469  4740 test_hook_module.cpp:71] Executing 'slaveLaunchExecutorEnvironmentDecorator' hook I0114 18:51:34.946705  4740 slave.cpp:3921] Launching executor default of framework 20150114-185134-2272962752-57018-4720-0000 in work directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d' I0114 18:51:34.956496  4740 exec.cpp:147] Version: 0.22.0 I0114 18:51:34.960752  4737 exec.cpp:197] Executor started at: executor(56)@192.168.122.135:57018 with pid 4720 I0114 18:51:34.964501  4740 slave.cpp:1368] Queuing task '1' for executor default of framework '20150114-185134-2272962752-57018-4720-0000 I0114 18:51:34.965133  4740 slave.cpp:566] Successfully attached file '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d' I0114 18:51:34.965605  4740 slave.cpp:1912] Got registration for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000 from executor(56)@192.168.122.135:57018 I0114 18:51:34.966933  4734 exec.cpp:221] Executor registered on slave 20150114-185134-2272962752-57018-4720-S0 I0114 18:51:34.968889  4740 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000 I0114 18:51:34.969743  4740 slave.cpp:2890] Monitoring executor 'default' of framework '20150114-185134-2272962752-57018-4720-0000' in container 'd73da0e7-3d52-4a0e-91d0-eaef735fd65d' I0114 18:51:34.973484  4734 exec.cpp:233] Executor::registered took 4.814445ms I0114 18:51:34.974081  4734 exec.cpp:308] Executor asked to run task '1' I0114 18:51:34.974431  4734 exec.cpp:317] Executor::launchTask took 184910ns I0114 18:51:34.975292  4720 sched.cpp:1471] Asked to stop the driver I0114 18:51:34.975817  4738 sched.cpp:808] Stopping framework '20150114-185134-2272962752-57018-4720-0000' I0114 18:51:34.975697  4720 master.cpp:654] Master terminating W0114 18:51:34.976610  4720 master.cpp:4980] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_STAGING I0114 18:51:34.977880  4720 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) I0114 18:51:34.978196  4741 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185134-2272962752-57018-4720-S0 from framework 20150114-185134-2272962752-57018-4720-0000 I0114 18:51:34.982658  4735 slave.cpp:2673] master@192.168.122.135:57018 exited W0114 18:51:34.983065  4735 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected I0114 18:51:35.029485  4720 slave.cpp:495] Slave terminating I0114 18:51:35.034024  4720 slave.cpp:1585] Asked to shut down framework 20150114-185134-2272962752-57018-4720-0000 by @0.0.0.0:0 I0114 18:51:35.034335  4720 slave.cpp:1610] Shutting down framework 20150114-185134-2272962752-57018-4720-0000 I0114 18:51:35.034857  4720 slave.cpp:3198] Shutting down executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000 tests/hook_tests.cpp:271: Failure Value of: os::isfile(path.get())   Actual: true Expected: false [  FAILED  ] HookTest.VerifySlaveLaunchExecutorHook (412 ms)  {code}",3
"MESOS-2230","Update RateLimiter to allow the acquired future to be discarded","Currently there is no way for the future returned by RateLimiter's acquire() to be discarded by the user of the limiter. This is useful in cases where the user is no longer interested in the permit. See MESOS-1148 for an example use case.",3
"MESOS-2232","Suppress MockAllocator::transformAllocation() warnings.","After transforming allocated resources feature was added to allocator, a number of warnings are popping out for allocator tests. Commits leading to this behaviour: {{dacc88292cc13d4b08fe8cda4df71110a96cb12a}} {{5a02d5bdc75d3b1149dcda519016374be06ec6bd}} corresponding reviews: https://reviews.apache.org/r/29083 https://reviews.apache.org/r/29084  Here is an example: {code} [ RUN ] MasterAllocatorTest/0.FrameworkReregistersFirst GMOCK WARNING: Uninteresting mock function call - taking default action specified at: ../../../src/tests/mesos.hpp:719: Function call: transformAllocation(@0x7fd3bb5274d8 20150115-185632-1677764800-59671-44186-0000, @0x7fd3bb5274f8 20150115-185632-1677764800-59671-44186-S0, @0x1119140e0 16-byte object <F0-5E 52-BB D3-7F 00-00 C0-5F 52-BB D3-7F 00-00>) Stack trace: [ OK ] MasterAllocatorTest/0.FrameworkReregistersFirst (204 ms) {code}",3
"MESOS-2233","Run ASF CI mesos builds inside docker","There are several limitations to mesos projects current state of CI, which is run on builds.a.o  --> Only runs on Ubuntu --> Doesn't run any tests that deal with cgroups --> Doesn't run any tests that need root permissions  Now that ASF CI supports docker (https://issues.apache.org/jira/browse/BUILDS-25), it would be great for the Mesos project to use it.",5
"MESOS-2241","DiskUsageCollectorTest.SymbolicLink test is flaky","Observed this on a local machine running linux w/ sudo.  {code} [ RUN      ] DiskUsageCollectorTest.SymbolicLink ../../src/tests/disk_quota_tests.cpp:138: Failure Expected: (usage1.get()) < (Kilobytes(16)), actual: 24KB vs 8-byte object <00-40 00-00 00-00 00-00> [  FAILED  ] DiskUsageCollectorTest.SymbolicLink (201 ms) {code}",1
"MESOS-2273","Add ""tests"" target to Makefile for building-but-not-running tests.","'make check' allows one to build and run the test suite. However, often we just want to build the tests.  Currently, this is done by setting GTEST_FILTER to an empty string.  It will be nice to have a dedicated target such as 'make tests' that allows one to build the test suite without running it.",1
"MESOS-2283","SlaveRecoveryTest.ReconcileKillTask is flaky.","Saw this on an internal CI:  {noformat} [ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_D5wSwg' I0126 19:10:52.005317 13291 leveldb.cpp:176] Opened db in 978670ns I0126 19:10:52.006155 13291 leveldb.cpp:183] Compacted db in 541346ns I0126 19:10:52.006494 13291 leveldb.cpp:198] Created db iterator in 24562ns I0126 19:10:52.006798 13291 leveldb.cpp:204] Seeked to beginning of db in 3254ns I0126 19:10:52.007036 13291 leveldb.cpp:273] Iterated through 0 keys in the db in 949ns I0126 19:10:52.007369 13291 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0126 19:10:52.008362 13308 recover.cpp:449] Starting replica recovery I0126 19:10:52.009141 13308 recover.cpp:475] Replica is in EMPTY status I0126 19:10:52.016494 13308 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0126 19:10:52.017333 13309 recover.cpp:195] Received a recover response from a replica in EMPTY status I0126 19:10:52.018244 13309 recover.cpp:566] Updating replica status to STARTING I0126 19:10:52.019064 13305 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 113577ns I0126 19:10:52.019487 13305 replica.cpp:323] Persisted replica status to STARTING I0126 19:10:52.019937 13309 recover.cpp:475] Replica is in STARTING status I0126 19:10:52.021492 13307 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0126 19:10:52.022665 13309 recover.cpp:195] Received a recover response from a replica in STARTING status I0126 19:10:52.027971 13312 recover.cpp:566] Updating replica status to VOTING I0126 19:10:52.028590 13312 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 78452ns I0126 19:10:52.028869 13312 replica.cpp:323] Persisted replica status to VOTING I0126 19:10:52.029252 13312 recover.cpp:580] Successfully joined the Paxos group I0126 19:10:52.030828 13307 recover.cpp:464] Recover process terminated I0126 19:10:52.049947 13306 master.cpp:262] Master 20150126-191052-2272962752-35545-13291 (fedora-19) started on 192.168.122.135:35545 I0126 19:10:52.050499 13306 master.cpp:308] Master only allowing authenticated frameworks to register I0126 19:10:52.050765 13306 master.cpp:313] Master only allowing authenticated slaves to register I0126 19:10:52.051048 13306 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_D5wSwg/credentials' I0126 19:10:52.051589 13306 master.cpp:357] Authorization enabled I0126 19:10:52.052531 13305 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process I0126 19:10:52.052881 13311 whitelist_watcher.cpp:65] No whitelist given I0126 19:10:52.055524 13306 master.cpp:1219] The newly elected leader is master@192.168.122.135:35545 with id 20150126-191052-2272962752-35545-13291 I0126 19:10:52.056226 13306 master.cpp:1232] Elected as the leading master! I0126 19:10:52.056639 13306 master.cpp:1050] Recovering from registrar I0126 19:10:52.057045 13307 registrar.cpp:313] Recovering registrar I0126 19:10:52.058554 13312 log.cpp:660] Attempting to start the writer I0126 19:10:52.060868 13309 replica.cpp:477] Replica received implicit promise request with proposal 1 I0126 19:10:52.061691 13309 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 91680ns I0126 19:10:52.062261 13309 replica.cpp:345] Persisted promised to 1 I0126 19:10:52.064559 13310 coordinator.cpp:230] Coordinator attemping to fill missing position I0126 19:10:52.069105 13311 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0126 19:10:52.069860 13311 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 94858ns I0126 19:10:52.070350 13311 replica.cpp:679] Persisted action at 0 I0126 19:10:52.080348 13305 replica.cpp:511] Replica received write request for position 0 I0126 19:10:52.081153 13305 leveldb.cpp:438] Reading position from leveldb took 62247ns I0126 19:10:52.081676 13305 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 81487ns I0126 19:10:52.082053 13305 replica.cpp:679] Persisted action at 0 I0126 19:10:52.083566 13309 replica.cpp:658] Replica received learned notice for position 0 I0126 19:10:52.085734 13309 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 283144ns I0126 19:10:52.086067 13309 replica.cpp:679] Persisted action at 0 I0126 19:10:52.086448 13309 replica.cpp:664] Replica learned NOP action at position 0 I0126 19:10:52.089784 13306 log.cpp:676] Writer started with ending position 0 I0126 19:10:52.093415 13309 leveldb.cpp:438] Reading position from leveldb took 66744ns I0126 19:10:52.104814 13306 registrar.cpp:346] Successfully fetched the registry (0B) in 47.451136ms I0126 19:10:52.105731 13306 registrar.cpp:445] Applied 1 operations in 42124ns; attempting to update the 'registry' I0126 19:10:52.111935 13305 log.cpp:684] Attempting to append 131 bytes to the log I0126 19:10:52.112754 13305 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0126 19:10:52.114297 13308 replica.cpp:511] Replica received write request for position 1 I0126 19:10:52.114908 13308 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 98332ns I0126 19:10:52.115387 13308 replica.cpp:679] Persisted action at 1 I0126 19:10:52.117277 13305 replica.cpp:658] Replica received learned notice for position 1 I0126 19:10:52.118142 13305 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 227799ns I0126 19:10:52.118621 13305 replica.cpp:679] Persisted action at 1 I0126 19:10:52.118979 13305 replica.cpp:664] Replica learned APPEND action at position 1 I0126 19:10:52.121311 13305 registrar.cpp:490] Successfully updated the 'registry' in 15.161088ms I0126 19:10:52.121548 13311 log.cpp:703] Attempting to truncate the log to 1 I0126 19:10:52.122697 13311 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0126 19:10:52.124316 13307 replica.cpp:511] Replica received write request for position 2 I0126 19:10:52.124913 13307 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 87281ns I0126 19:10:52.125334 13307 replica.cpp:679] Persisted action at 2 I0126 19:10:52.127018 13311 replica.cpp:658] Replica received learned notice for position 2 I0126 19:10:52.127835 13311 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 201050ns I0126 19:10:52.128232 13311 leveldb.cpp:401] Deleting ~1 keys from leveldb took 78012ns I0126 19:10:52.128835 13305 registrar.cpp:376] Successfully recovered registrar I0126 19:10:52.128551 13311 replica.cpp:679] Persisted action at 2 I0126 19:10:52.130105 13311 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0126 19:10:52.131479 13312 master.cpp:1077] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register I0126 19:10:52.143465 13291 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem I0126 19:10:52.170471 13309 slave.cpp:173] Slave started on 101)@192.168.122.135:35545 I0126 19:10:52.171723 13309 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/credential' I0126 19:10:52.172286 13309 slave.cpp:282] Slave using credential for: test-principal I0126 19:10:52.172821 13309 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0126 19:10:52.173982 13309 slave.cpp:329] Slave hostname: fedora-19 I0126 19:10:52.174505 13309 slave.cpp:330] Slave checkpoint: true I0126 19:10:52.179308 13309 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta' I0126 19:10:52.180075 13308 status_update_manager.cpp:197] Recovering status update manager I0126 19:10:52.180611 13308 containerizer.cpp:300] Recovering containerizer I0126 19:10:52.182473 13309 slave.cpp:3519] Finished recovery I0126 19:10:52.184403 13312 slave.cpp:613] New master detected at master@192.168.122.135:35545 I0126 19:10:52.184916 13312 slave.cpp:676] Authenticating with master master@192.168.122.135:35545 I0126 19:10:52.185230 13312 slave.cpp:681] Using default CRAM-MD5 authenticatee I0126 19:10:52.185715 13312 slave.cpp:649] Detecting new master I0126 19:10:52.186420 13312 authenticatee.hpp:138] Creating new client SASL connection I0126 19:10:52.186002 13311 status_update_manager.cpp:171] Pausing sending status updates I0126 19:10:52.188293 13312 master.cpp:4129] Authenticating slave(101)@192.168.122.135:35545 I0126 19:10:52.188748 13312 master.cpp:4140] Using default CRAM-MD5 authenticator I0126 19:10:52.189525 13312 authenticator.hpp:170] Creating new server SASL connection I0126 19:10:52.191082 13305 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0126 19:10:52.191550 13305 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0126 19:10:52.191990 13312 authenticator.hpp:276] Received SASL authentication start I0126 19:10:52.192365 13312 authenticator.hpp:398] Authentication requires more steps I0126 19:10:52.192800 13311 authenticatee.hpp:275] Received SASL authentication step I0126 19:10:52.193244 13312 authenticator.hpp:304] Received SASL authentication step I0126 19:10:52.193565 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0126 19:10:52.193902 13312 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0126 19:10:52.194301 13312 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0126 19:10:52.195669 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0126 19:10:52.196048 13312 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0126 19:10:52.196395 13312 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0126 19:10:52.196723 13312 authenticator.hpp:390] Authentication success I0126 19:10:52.197206 13305 authenticatee.hpp:315] Authentication success I0126 19:10:52.204121 13305 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:35545 I0126 19:10:52.204676 13310 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(101)@192.168.122.135:35545 I0126 19:10:52.205729 13305 slave.cpp:1075] Will retry registration in 5.608661ms if necessary I0126 19:10:52.206451 13310 master.cpp:3275] Registering slave at slave(101)@192.168.122.135:35545 (fedora-19) with id 20150126-191052-2272962752-35545-13291-S0 I0126 19:10:52.210019 13310 registrar.cpp:445] Applied 1 operations in 235087ns; attempting to update the 'registry' I0126 19:10:52.220736 13308 slave.cpp:1075] Will retry registration in 9.28397ms if necessary I0126 19:10:52.221309 13311 master.cpp:3263] Ignoring register slave message from slave(101)@192.168.122.135:35545 (fedora-19) as admission is already in progress I0126 19:10:52.224818 13307 log.cpp:684] Attempting to append 302 bytes to the log I0126 19:10:52.225554 13307 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0126 19:10:52.227422 13305 replica.cpp:511] Replica received write request for position 3 I0126 19:10:52.227969 13305 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 100350ns I0126 19:10:52.228276 13305 replica.cpp:679] Persisted action at 3 I0126 19:10:52.232475 13312 replica.cpp:658] Replica received learned notice for position 3 I0126 19:10:52.233280 13312 leveldb.cpp:343] Persisting action (323 bytes) to leveldb took 546567ns I0126 19:10:52.233726 13312 replica.cpp:679] Persisted action at 3 I0126 19:10:52.234035 13312 replica.cpp:664] Replica learned APPEND action at position 3 I0126 19:10:52.236556 13310 registrar.cpp:490] Successfully updated the 'registry' in 26.040064ms I0126 19:10:52.237330 13305 log.cpp:703] Attempting to truncate the log to 3 I0126 19:10:52.238056 13311 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0126 19:10:52.239594 13311 replica.cpp:511] Replica received write request for position 4 I0126 19:10:52.240129 13311 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 92868ns I0126 19:10:52.240458 13311 replica.cpp:679] Persisted action at 4 I0126 19:10:52.241976 13308 replica.cpp:658] Replica received learned notice for position 4 I0126 19:10:52.242645 13308 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 95635ns I0126 19:10:52.242990 13308 leveldb.cpp:401] Deleting ~2 keys from leveldb took 58066ns I0126 19:10:52.243337 13308 replica.cpp:679] Persisted action at 4 I0126 19:10:52.243695 13308 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0126 19:10:52.245657 13291 sched.cpp:151] Version: 0.22.0 I0126 19:10:52.247625 13305 master.cpp:3329] Registered slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0126 19:10:52.248942 13307 slave.cpp:781] Registered with master master@192.168.122.135:35545; given slave ID 20150126-191052-2272962752-35545-13291-S0 I0126 19:10:52.250396 13307 slave.cpp:797] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/slave.info' I0126 19:10:52.250731 13309 status_update_manager.cpp:178] Resuming sending status updates I0126 19:10:52.251765 13307 slave.cpp:2588] Received ping from slave-observer(99)@192.168.122.135:35545 I0126 19:10:52.247951 13310 hierarchical_allocator_process.hpp:453] Added slave 20150126-191052-2272962752-35545-13291-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0126 19:10:52.252810 13310 hierarchical_allocator_process.hpp:831] No resources available to allocate! I0126 19:10:52.254365 13310 hierarchical_allocator_process.hpp:756] Performed allocation for slave 20150126-191052-2272962752-35545-13291-S0 in 1.732701ms I0126 19:10:52.254137 13307 sched.cpp:248] New master detected at master@192.168.122.135:35545 I0126 19:10:52.257863 13307 sched.cpp:304] Authenticating with master master@192.168.122.135:35545 I0126 19:10:52.258249 13307 sched.cpp:311] Using default CRAM-MD5 authenticatee I0126 19:10:52.258908 13306 authenticatee.hpp:138] Creating new client SASL connection I0126 19:10:52.261397 13309 master.cpp:4129] Authenticating scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 I0126 19:10:52.261776 13309 master.cpp:4140] Using default CRAM-MD5 authenticator I0126 19:10:52.264528 13309 authenticator.hpp:170] Creating new server SASL connection I0126 19:10:52.266248 13312 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0126 19:10:52.266749 13312 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0126 19:10:52.267143 13312 authenticator.hpp:276] Received SASL authentication start I0126 19:10:52.267525 13312 authenticator.hpp:398] Authentication requires more steps I0126 19:10:52.267917 13312 authenticatee.hpp:275] Received SASL authentication step I0126 19:10:52.268404 13312 authenticator.hpp:304] Received SASL authentication step I0126 19:10:52.268725 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0126 19:10:52.269078 13312 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0126 19:10:52.269498 13312 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0126 19:10:52.269881 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0126 19:10:52.270385 13312 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0126 19:10:52.271015 13312 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0126 19:10:52.271599 13312 authenticator.hpp:390] Authentication success I0126 19:10:52.272126 13312 authenticatee.hpp:315] Authentication success I0126 19:10:52.272415 13305 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 I0126 19:10:52.273998 13307 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:35545 I0126 19:10:52.274415 13307 sched.cpp:515] Sending registration request to master@192.168.122.135:35545 I0126 19:10:52.274842 13307 sched.cpp:548] Will retry registration in 674.656506ms if necessary I0126 19:10:52.275235 13305 master.cpp:1420] Received registration request for framework 'default' at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 I0126 19:10:52.276017 13305 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0126 19:10:52.277027 13305 master.cpp:1484] Registering framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 I0126 19:10:52.278285 13308 hierarchical_allocator_process.hpp:319] Added framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.279575 13308 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 697902ns I0126 19:10:52.287966 13305 master.cpp:4071] Sending 1 offers to framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 I0126 19:10:52.288776 13307 sched.cpp:442] Framework registered with 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.289373 13307 sched.cpp:456] Scheduler::registered took 21674ns I0126 19:10:52.289932 13307 sched.cpp:605] Scheduler::resourceOffers took 76147ns I0126 19:10:52.293220 13311 master.cpp:2677] Processing ACCEPT call for offers: [ 20150126-191052-2272962752-35545-13291-O0 ] on slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) for framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 I0126 19:10:52.293586 13311 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e as user 'jenkins' I0126 19:10:52.295825 13311 master.hpp:782] Adding task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150126-191052-2272962752-35545-13291-S0 (fedora-19) I0126 19:10:52.296272 13311 master.cpp:2885] Launching task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) I0126 19:10:52.296886 13309 slave.cpp:1130] Got assigned task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.297324 13309 slave.cpp:3846] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/framework.info' I0126 19:10:52.297919 13309 slave.cpp:3853] Checkpointing framework pid 'scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/framework.pid' I0126 19:10:52.299072 13309 slave.cpp:1245] Launching task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.308050 13309 slave.cpp:4289] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/executor.info' I0126 19:10:52.310894 13309 slave.cpp:3921] Launching executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 in work directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0' I0126 19:10:52.311957 13308 containerizer.cpp:445] Starting container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework '20150126-191052-2272962752-35545-13291-0000' W0126 19:10:52.313951 13307 containerizer.cpp:296] CommandInfo.grace_period flag is not set, using default value: 3secs I0126 19:10:52.330166 13309 slave.cpp:4312] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/tasks/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/task.info' I0126 19:10:52.333307 13309 slave.cpp:1368] Queuing task '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' for executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework '20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.332506 13307 launcher.cpp:137] Forked child with pid '15795' for container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' I0126 19:10:52.334852 13307 containerizer.cpp:655] Checkpointing executor's forked pid 15795 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/pids/forked.pid' I0126 19:10:52.339607 13309 slave.cpp:566] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0' I0126 19:10:52.341423 13309 slave.cpp:2890] Monitoring executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework '20150126-191052-2272962752-35545-13291-0000' in container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' WARNING: Logging before InitGoogleLogging() is written to STDERR I0126 19:10:52.584766 15795 process.cpp:958] libprocess is initialized on 192.168.122.135:41245 for 8 cpus I0126 19:10:52.597306 15795 logging.cpp:177] Logging to STDERR I0126 19:10:52.606741 15795 exec.cpp:147] Version: 0.22.0 I0126 19:10:52.617653 15825 exec.cpp:197] Executor started at: executor(1)@192.168.122.135:41245 with pid 15795 I0126 19:10:52.643771 13309 slave.cpp:1912] Got registration for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 from executor(1)@192.168.122.135:41245 I0126 19:10:52.644484 13309 slave.cpp:1998] Checkpointing executor pid 'executor(1)@192.168.122.135:41245' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/pids/libprocess.pid' I0126 19:10:52.648509 13309 slave.cpp:2031] Flushing queued task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.701879 15830 exec.cpp:221] Executor registered on slave 20150126-191052-2272962752-35545-13291-S0 Shutdown timeout is set to 3secsRegistered executor on fedora-19 I0126 19:10:52.706497 15830 exec.cpp:233] Executor::registered took 2.369798ms I0126 19:10:52.710708 15830 exec.cpp:308] Executor asked to run task '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' Starting task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e I0126 19:10:52.713075 15830 exec.cpp:317] Executor::launchTask took 1.248631ms sh -c 'sleep 1000' Forked command at 15832 I0126 19:10:52.720675 15824 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.722925 13308 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from executor(1)@192.168.122.135:41245 I0126 19:10:52.723328 13308 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.723371 13308 status_update_manager.cpp:494] Creating StatusUpdate stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.723803 13308 status_update_manager.hpp:346] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.723963 13308 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to the slave I0126 19:10:52.724717 13312 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to master@192.168.122.135:35545 I0126 19:10:52.725385 13305 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.725857 13305 master.cpp:3624] Status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) I0126 19:10:52.726471 13305 master.cpp:4934] Updating the latest state of task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to TASK_RUNNING I0126 19:10:52.726269 13311 sched.cpp:696] Scheduler::statusUpdate took 22534ns I0126 19:10:52.727679 13311 master.cpp:3125] Forwarding status update acknowledgement 9577ef79-7e59-4be6-a892-5a20baa13647 for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 to slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) I0126 19:10:52.728380 13308 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.728579 13311 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.729403 13311 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to executor(1)@192.168.122.135:41245 I0126 19:10:52.728869 13308 status_update_manager.hpp:346] Checkpointing ACK for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.731828 13307 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.732923 13307 slave.cpp:495] Slave terminating I0126 19:10:52.739572 15827 exec.cpp:354] Executor received status update acknowledgement 9577ef79-7e59-4be6-a892-5a20baa13647 for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.743466 13306 master.cpp:795] Slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) disconnected I0126 19:10:52.743948 13306 master.cpp:1826] Disconnecting slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) I0126 19:10:52.744940 13306 master.cpp:1845] Deactivating slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) I0126 19:10:52.752821 13306 hierarchical_allocator_process.hpp:512] Slave 20150126-191052-2272962752-35545-13291-S0 deactivated I0126 19:10:52.765900 13291 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem I0126 19:10:52.766723 13309 master.cpp:2961] Asked to kill task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 W0126 19:10:52.767549 13309 master.cpp:3030] Cannot kill task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 because the slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) is disconnected. Kill will be retried if the slave re-registers I0126 19:10:52.789048 13307 slave.cpp:173] Slave started on 102)@192.168.122.135:35545 I0126 19:10:52.790671 13307 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/credential' I0126 19:10:52.791497 13307 slave.cpp:282] Slave using credential for: test-principal I0126 19:10:52.792064 13307 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0126 19:10:52.793090 13307 slave.cpp:329] Slave hostname: fedora-19 I0126 19:10:52.793556 13307 slave.cpp:330] Slave checkpoint: true I0126 19:10:52.795727 13311 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta' I0126 19:10:52.796282 13311 state.cpp:668] Failed to find resources file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/resources/resources.info' I0126 19:10:52.804524 13309 slave.cpp:3601] Recovering framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.805106 13309 slave.cpp:4040] Recovering executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.807494 13309 slave.cpp:566] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0' I0126 19:10:52.807888 13310 status_update_manager.cpp:197] Recovering status update manager I0126 19:10:52.808390 13310 status_update_manager.cpp:205] Recovering executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.808830 13310 status_update_manager.cpp:494] Creating StatusUpdate stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.809484 13310 status_update_manager.hpp:310] Replaying status update stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e I0126 19:10:52.810966 13308 containerizer.cpp:300] Recovering containerizer I0126 19:10:52.811550 13308 containerizer.cpp:342] Recovering container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:52.816074 13305 slave.cpp:3460] Sending reconnect request to executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 at executor(1)@192.168.122.135:41245 I0126 19:10:52.929554 15827 exec.cpp:267] Received reconnect request from slave 20150126-191052-2272962752-35545-13291-S0 I0126 19:10:52.946156 13305 slave.cpp:2089] Re-registering executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:53.010731 15829 exec.cpp:244] Executor re-registered on slave 20150126-191052-2272962752-35545-13291-S0 Re-registered executor on fedora-19 I0126 19:10:53.012980 15829 exec.cpp:256] Executor::reregistered took 313096ns I0126 19:10:53.054590 13309 hierarchical_allocator_process.hpp:831] No resources available to allocate! I0126 19:10:53.054930 13309 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 388184ns I0126 19:10:54.055598 13312 hierarchical_allocator_process.hpp:831] No resources available to allocate! I0126 19:10:54.058614 13312 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 3.086403ms I0126 19:10:54.818248 13310 slave.cpp:2214] Cleaning up un-reregistered executors I0126 19:10:54.821072 13310 slave.cpp:3519] Finished recovery I0126 19:10:54.823081 13312 status_update_manager.cpp:171] Pausing sending status updates I0126 19:10:54.823719 13310 slave.cpp:613] New master detected at master@192.168.122.135:35545 I0126 19:10:54.824260 13310 slave.cpp:676] Authenticating with master master@192.168.122.135:35545 I0126 19:10:54.824583 13310 slave.cpp:681] Using default CRAM-MD5 authenticatee I0126 19:10:54.825479 13307 authenticatee.hpp:138] Creating new client SASL connection I0126 19:10:54.826686 13310 slave.cpp:649] Detecting new master I0126 19:10:54.827214 13307 master.cpp:4129] Authenticating slave(102)@192.168.122.135:35545 I0126 19:10:54.827747 13307 master.cpp:4140] Using default CRAM-MD5 authenticator I0126 19:10:54.828635 13307 authenticator.hpp:170] Creating new server SASL connection I0126 19:10:54.830049 13306 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0126 19:10:54.830447 13306 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0126 19:10:54.830934 13307 authenticator.hpp:276] Received SASL authentication start I0126 19:10:54.831362 13307 authenticator.hpp:398] Authentication requires more steps I0126 19:10:54.831837 13309 authenticatee.hpp:275] Received SASL authentication step I0126 19:10:54.832283 13307 authenticator.hpp:304] Received SASL authentication step I0126 19:10:54.832615 13307 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0126 19:10:54.833143 13307 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0126 19:10:54.833549 13307 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0126 19:10:54.833904 13307 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0126 19:10:54.834241 13307 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0126 19:10:54.834539 13307 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0126 19:10:54.834869 13307 authenticator.hpp:390] Authentication success I0126 19:10:54.836004 13311 authenticatee.hpp:315] Authentication success I0126 19:10:54.842200 13311 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:35545 I0126 19:10:54.842851 13308 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(102)@192.168.122.135:35545 I0126 19:10:54.844679 13309 master.cpp:3401] Re-registering slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) W0126 19:10:54.845654 13309 master.cpp:4347]  Slave 20150126-191052-2272962752-35545-13291-S0 at slave(102)@192.168.122.135:35545 (fedora-19) has non-terminal task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e that is supposed to be killed. Killing it now! I0126 19:10:54.846365 13308 hierarchical_allocator_process.hpp:498] Slave 20150126-191052-2272962752-35545-13291-S0 reactivated I0126 19:10:54.846976 13311 slave.cpp:1075] Will retry registration in 10.72364ms if necessary I0126 19:10:54.847618 13311 slave.cpp:849] Re-registered with master master@192.168.122.135:35545 I0126 19:10:54.848054 13309 status_update_manager.cpp:178] Resuming sending status updates I0126 19:10:54.848565 13311 slave.cpp:1424] Asked to kill task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:54.850329 13311 slave.cpp:1762] Updating framework 20150126-191052-2272962752-35545-13291-0000 pid to scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 I0126 19:10:54.853868 13311 slave.cpp:1770] Checkpointing framework pid 'scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/framework.pid' I0126 19:10:54.854627 13312 status_update_manager.cpp:178] Resuming sending status updates I0126 19:10:54.920938 15824 exec.cpp:328] Executor asked to kill task '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' I0126 19:10:54.921044 15824 exec.cpp:337] Executor::killTask took 56676ns Shutting down Sending SIGTERM to process tree at pid 15832 Killing the following process trees: [  --- 15832 sleep 1000  ] Command terminated with signal Terminated (pid: 15832) I0126 19:10:55.045547 15825 exec.cpp:540] Executor sending status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.059789 13312 hierarchical_allocator_process.hpp:831] No resources available to allocate! I0126 19:10:55.060405 13312 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 918365ns I0126 19:10:55.115810 13309 slave.cpp:2265] Handling status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from executor(1)@192.168.122.135:41245 I0126 19:10:55.116387 13309 slave.cpp:4229] Terminating task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e I0126 19:10:55.119729 13305 status_update_manager.cpp:317] Received status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.120384 13305 status_update_manager.hpp:346] Checkpointing UPDATE for status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.120900 13305 status_update_manager.cpp:371] Forwarding update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to the slave I0126 19:10:55.121579 13305 slave.cpp:2508] Forwarding the update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to master@192.168.122.135:35545 I0126 19:10:55.122256 13310 master.cpp:3652] Forwarding status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.122685 13310 master.cpp:3624] Status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from slave 20150126-191052-2272962752-35545-13291-S0 at slave(102)@192.168.122.135:35545 (fedora-19) I0126 19:10:55.123086 13308 sched.cpp:696] Scheduler::statusUpdate took 79719ns I0126 19:10:55.124562 13310 master.cpp:4934] Updating the latest state of task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to TASK_KILLED I0126 19:10:55.125345 13310 master.cpp:4993] Removing task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150126-191052-2272962752-35545-13291-0000 on slave 20150126-191052-2272962752-35545-13291-S0 at slave(102)@192.168.122.135:35545 (fedora-19) I0126 19:10:55.125810 13306 hierarchical_allocator_process.hpp:645] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150126-191052-2272962752-35545-13291-S0 from framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.126552 13310 master.cpp:3125] Forwarding status update acknowledgement 2c2ef52e-8c0d-4a83-be36-e6433316989e for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 to slave 20150126-191052-2272962752-35545-13291-S0 at slave(102)@192.168.122.135:35545 (fedora-19) I0126 19:10:55.126843 13305 slave.cpp:2435] Status update manager successfully handled status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.127396 13305 slave.cpp:2441] Sending acknowledgement for status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to executor(1)@192.168.122.135:41245 I0126 19:10:55.129451 13306 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.129976 13306 status_update_manager.hpp:346] Checkpointing ACK for status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.130367 13306 status_update_manager.cpp:525] Cleaning up status update stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.130980 13305 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:55.131376 13305 slave.cpp:4268] Completing task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e I0126 19:10:55.148888 15823 exec.cpp:354] Executor received status update acknowledgement 2c2ef52e-8c0d-4a83-be36-e6433316989e for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:56.061642 13305 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 612945ns I0126 19:10:56.065135 13310 master.cpp:4071] Sending 1 offers to framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 I0126 19:10:56.068106 13310 sched.cpp:605] Scheduler::resourceOffers took 98788ns I0126 19:10:56.068989 13291 sched.cpp:1471] Asked to stop the driver I0126 19:10:56.069831 13291 master.cpp:654] Master terminating I0126 19:10:56.070969 13310 sched.cpp:808] Stopping framework '20150126-191052-2272962752-35545-13291-0000' I0126 19:10:56.072089 13312 slave.cpp:2673] master@192.168.122.135:35545 exited W0126 19:10:56.072654 13312 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected I0126 19:10:56.110337 13310 containerizer.cpp:1084] Executor for container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' has exited I0126 19:10:56.110785 13310 containerizer.cpp:879] Destroying container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' I./tests/cluster.hpp:451: Failure (wait).failure(): Unknown container: 960eca2c-9e2c-415a-b6a5-159efca1f1b0 0126 19:10:56.146338 13307 slave.cpp:2948] Executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 exited with status 0 W0126 19:10:56.147302 13309 containerizer.cpp:868] Ignoring destroy of unknown container: 960eca2c-9e2c-415a-b6a5-159efca1f1b0 *** Aborted at 1422328256 (unix time) try ""date -d @1422328256"" if you are using GNU date *** I0126 19:10:56.151959 13307 slave.cpp:3057] Cleaning up executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:56.153216 13309 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0' for gc 6.99999822829926days in the future I0126 19:10:56.154017 13305 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' for gc 6.99999821866963days in the future I0126 19:10:56.154710 13312 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0' for gc 6.99999821037037days in the future IPC: @           0x8f9d48 mesos::internal::tests::Cluster::Slaves::shutdown() 0126 19:10:56.155350 13307 slave.cpp:3136] Cleaning up framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:56.155609 13310 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' for gc 6.99999820308148days in the future I0126 19:10:56.158103 13310 status_update_manager.cpp:279] Closing status update streams for framework 20150126-191052-2272962752-35545-13291-0000 I0126 19:10:56.163135 13310 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000' for gc 6.99999817088days in the future I0126 19:10:56.168100 13307 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000' for gc 6.99999805755852days in the future *** SIGSEGV (@0x0) received by PID 13291 (TID 0x7fc0fdb22880) from PID 0; stack trace: ***     @     0x7fc0da188cbb (unknown)     @     0x7fc0da18d1a1 (unknown)     @       0x3aa2a0efa0 (unknown)     @           0x8f9d48 mesos::internal::tests::Cluster::Slaves::shutdown()     @           0xe0bfba mesos::internal::tests::MesosTest::ShutdownSlaves()     @           0xe0bf7e mesos::internal::tests::MesosTest::Shutdown()     @           0xe0981f mesos::internal::tests::MesosTest::TearDown()     @           0xe0f2b6 mesos::internal::tests::ContainerizerTest<>::TearDown()     @          0x10d8180 testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x10d3356 testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x10bb718 testing::Test::Run()     @          0x10bbdf2 testing::TestInfo::Run()     @          0x10bc37a testing::TestCase::Run()     @          0x10c10f6 testing::internal::UnitTestImpl::RunAllTests()     @          0x10d8ff1 testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x10d4047 testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x10bffa6 testing::UnitTest::Run()     @           0xce850d main     @       0x3aa2221b45 (unknown)     @           0x8d59f9 (unknown) make[3]: *** [check-local] Segmentation fault (core dumped) {noformat}",1
"MESOS-2302","FaultToleranceTest.SchedulerFailoverFrameworkMessage is flaky.","Bad Run: {noformat} [ RUN      ] FaultToleranceTest.SchedulerFailoverFrameworkMessage Using temporary directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_f3jYkr' I0123 18:50:11.669674 15688 leveldb.cpp:176] Opened db in 31.920683ms I0123 18:50:11.678328 15688 leveldb.cpp:183] Compacted db in 8.580569ms I0123 18:50:11.678455 15688 leveldb.cpp:198] Created db iterator in 38478ns I0123 18:50:11.678478 15688 leveldb.cpp:204] Seeked to beginning of db in 3057ns I0123 18:50:11.678489 15688 leveldb.cpp:273] Iterated through 0 keys in the db in 427ns I0123 18:50:11.678539 15688 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0123 18:50:11.682271 15705 recover.cpp:449] Starting replica recovery I0123 18:50:11.682634 15705 recover.cpp:475] Replica is in EMPTY status I0123 18:50:11.684389 15708 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0123 18:50:11.685132 15708 recover.cpp:195] Received a recover response from a replica in EMPTY status I0123 18:50:11.689842 15708 recover.cpp:566] Updating replica status to STARTING I0123 18:50:11.702548 15708 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 12.484558ms I0123 18:50:11.702615 15708 replica.cpp:323] Persisted replica status to STARTING I0123 18:50:11.703531 15708 recover.cpp:475] Replica is in STARTING status I0123 18:50:11.705080 15704 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0123 18:50:11.712587 15708 recover.cpp:195] Received a recover response from a replica in STARTING status I0123 18:50:11.722898 15708 recover.cpp:566] Updating replica status to VOTING I0123 18:50:11.725427 15703 master.cpp:262] Master 20150123-185011-16777343-37526-15688 (localhost.localdomain) started on 127.0.0.1:37526 W0123 18:50:11.725464 15703 master.cpp:266]  ************************************************** Master bound to loopback interface! Cannot communicate with remote schedulers or slaves. You might want to set '--ip' flag to a routable IP address. ************************************************** I0123 18:50:11.725502 15703 master.cpp:308] Master only allowing authenticated frameworks to register I0123 18:50:11.725513 15703 master.cpp:313] Master only allowing authenticated slaves to register I0123 18:50:11.725543 15703 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_f3jYkr/credentials' I0123 18:50:11.725774 15703 master.cpp:357] Authorization enabled I0123 18:50:11.728428 15707 whitelist_watcher.cpp:65] No whitelist given I0123 18:50:11.729169 15707 master.cpp:1219] The newly elected leader is master@127.0.0.1:37526 with id 20150123-185011-16777343-37526-15688 I0123 18:50:11.729200 15707 master.cpp:1232] Elected as the leading master! I0123 18:50:11.729223 15707 master.cpp:1050] Recovering from registrar I0123 18:50:11.729595 15706 registrar.cpp:313] Recovering registrar I0123 18:50:11.730715 15703 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process I0123 18:50:11.737431 15708 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.259597ms I0123 18:50:11.737511 15708 replica.cpp:323] Persisted replica status to VOTING I0123 18:50:11.737768 15708 recover.cpp:580] Successfully joined the Paxos group I0123 18:50:11.737977 15708 recover.cpp:464] Recover process terminated I0123 18:50:11.739083 15706 log.cpp:660] Attempting to start the writer I0123 18:50:11.741236 15706 replica.cpp:477] Replica received implicit promise request with proposal 1 I0123 18:50:11.750435 15706 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 8.813783ms I0123 18:50:11.750514 15706 replica.cpp:345] Persisted promised to 1 I0123 18:50:11.752239 15708 coordinator.cpp:230] Coordinator attemping to fill missing position I0123 18:50:11.754176 15706 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0123 18:50:11.763464 15706 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 8.799822ms I0123 18:50:11.763535 15706 replica.cpp:679] Persisted action at 0 I0123 18:50:11.765697 15709 replica.cpp:511] Replica received write request for position 0 I0123 18:50:11.766293 15709 leveldb.cpp:438] Reading position from leveldb took 54028ns I0123 18:50:11.776468 15709 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 9.789169ms I0123 18:50:11.776561 15709 replica.cpp:679] Persisted action at 0 I0123 18:50:11.777515 15709 replica.cpp:658] Replica received learned notice for position 0 I0123 18:50:11.785459 15709 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.897242ms I0123 18:50:11.785531 15709 replica.cpp:679] Persisted action at 0 I0123 18:50:11.785565 15709 replica.cpp:664] Replica learned NOP action at position 0 I0123 18:50:11.786633 15709 log.cpp:676] Writer started with ending position 0 I0123 18:50:11.788460 15709 leveldb.cpp:438] Reading position from leveldb took 266087ns I0123 18:50:11.801141 15709 registrar.cpp:346] Successfully fetched the registry (0B) in 71.491072ms I0123 18:50:11.801300 15709 registrar.cpp:445] Applied 1 operations in 41795ns; attempting to update the 'registry' I0123 18:50:11.805186 15707 log.cpp:684] Attempting to append 136 bytes to the log I0123 18:50:11.805454 15707 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0123 18:50:11.806677 15703 replica.cpp:511] Replica received write request for position 1 I0123 18:50:11.815621 15703 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 8.89177ms I0123 18:50:11.815692 15703 replica.cpp:679] Persisted action at 1 I0123 18:50:11.817358 15704 replica.cpp:658] Replica received learned notice for position 1 I0123 18:50:11.825014 15704 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 7.578558ms I0123 18:50:11.825088 15704 replica.cpp:679] Persisted action at 1 I0123 18:50:11.825124 15704 replica.cpp:664] Replica learned APPEND action at position 1 I0123 18:50:11.827008 15705 registrar.cpp:490] Successfully updated the 'registry' in 25.629952ms I0123 18:50:11.827143 15705 registrar.cpp:376] Successfully recovered registrar I0123 18:50:11.827517 15705 master.cpp:1077] Recovered 0 slaves from the Registry (98B) ; allowing 10mins for slaves to re-register I0123 18:50:11.828515 15704 log.cpp:703] Attempting to truncate the log to 1 I0123 18:50:11.829074 15704 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0123 18:50:11.830546 15709 replica.cpp:511] Replica received write request for position 2 I0123 18:50:11.837752 15709 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.142431ms I0123 18:50:11.837826 15709 replica.cpp:679] Persisted action at 2 I0123 18:50:11.839334 15709 replica.cpp:658] Replica received learned notice for position 2 I0123 18:50:11.847069 15709 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 7.116607ms I0123 18:50:11.847214 15709 leveldb.cpp:401] Deleting ~1 keys from leveldb took 74008ns I0123 18:50:11.847241 15709 replica.cpp:679] Persisted action at 2 I0123 18:50:11.847295 15709 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0123 18:50:11.870337 15710 slave.cpp:173] Slave started on 94)@127.0.0.1:37526 W0123 18:50:11.870980 15710 slave.cpp:176]  ************************************************** Slave bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address. ************************************************** I0123 18:50:11.871412 15710 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/credential' I0123 18:50:11.871819 15710 slave.cpp:282] Slave using credential for: test-principal I0123 18:50:11.873178 15710 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0123 18:50:11.873620 15710 slave.cpp:329] Slave hostname: localhost.localdomain I0123 18:50:11.873837 15710 slave.cpp:330] Slave checkpoint: false W0123 18:50:11.874068 15710 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0123 18:50:11.879103 15705 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/meta' W0123 18:50:11.882972 15688 sched.cpp:1246]  ************************************************** Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address. ************************************************** I0123 18:50:11.884106 15709 status_update_manager.cpp:197] Recovering status update manager I0123 18:50:11.884703 15710 slave.cpp:3519] Finished recovery I0123 18:50:11.892076 15704 status_update_manager.cpp:171] Pausing sending status updates I0123 18:50:11.892590 15710 slave.cpp:613] New master detected at master@127.0.0.1:37526 I0123 18:50:11.892937 15710 slave.cpp:676] Authenticating with master master@127.0.0.1:37526 I0123 18:50:11.893165 15710 slave.cpp:681] Using default CRAM-MD5 authenticatee I0123 18:50:11.893754 15708 authenticatee.hpp:138] Creating new client SASL connection I0123 18:50:11.894120 15708 master.cpp:4129] Authenticating slave(94)@127.0.0.1:37526 I0123 18:50:11.894153 15708 master.cpp:4140] Using default CRAM-MD5 authenticator I0123 18:50:11.894628 15708 authenticator.hpp:170] Creating new server SASL connection I0123 18:50:11.894913 15708 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0123 18:50:11.894942 15708 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0123 18:50:11.895043 15708 authenticator.hpp:276] Received SASL authentication start I0123 18:50:11.895095 15708 authenticator.hpp:398] Authentication requires more steps I0123 18:50:11.895165 15708 authenticatee.hpp:275] Received SASL authentication step I0123 18:50:11.895261 15708 authenticator.hpp:304] Received SASL authentication step I0123 18:50:11.895292 15708 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0123 18:50:11.895305 15708 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0123 18:50:11.895354 15708 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0123 18:50:11.895881 15710 slave.cpp:649] Detecting new master I0123 18:50:11.898449 15708 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0123 18:50:11.899024 15708 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0123 18:50:11.899106 15708 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0123 18:50:11.899190 15708 authenticator.hpp:390] Authentication success I0123 18:50:11.899569 15706 authenticatee.hpp:315] Authentication success I0123 18:50:11.902299 15706 slave.cpp:747] Successfully authenticated with master master@127.0.0.1:37526 I0123 18:50:11.902847 15706 slave.cpp:1075] Will retry registration in 19.809649ms if necessary I0123 18:50:11.903264 15705 master.cpp:3214] Queuing up registration request from slave(94)@127.0.0.1:37526 because authentication is still in progress I0123 18:50:11.903497 15705 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(94)@127.0.0.1:37526 I0123 18:50:11.903940 15705 master.cpp:3275] Registering slave at slave(94)@127.0.0.1:37526 (localhost.localdomain) with id 20150123-185011-16777343-37526-15688-S0 I0123 18:50:11.904398 15705 registrar.cpp:445] Applied 1 operations in 63679ns; attempting to update the 'registry' I0123 18:50:11.917883 15688 sched.cpp:151] Version: 0.22.0 I0123 18:50:11.919347 15703 log.cpp:684] Attempting to append 315 bytes to the log I0123 18:50:11.921039 15703 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0123 18:50:11.919992 15706 sched.cpp:248] New master detected at master@127.0.0.1:37526 I0123 18:50:11.921352 15706 sched.cpp:304] Authenticating with master master@127.0.0.1:37526 I0123 18:50:11.921408 15706 sched.cpp:311] Using default CRAM-MD5 authenticatee I0123 18:50:11.921773 15706 authenticatee.hpp:138] Creating new client SASL connection I0123 18:50:11.922266 15706 master.cpp:4129] Authenticating scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 I0123 18:50:11.922301 15706 master.cpp:4140] Using default CRAM-MD5 authenticator I0123 18:50:11.923928 15703 replica.cpp:511] Replica received write request for position 3 I0123 18:50:11.924285 15707 authenticator.hpp:170] Creating new server SASL connection I0123 18:50:11.925091 15707 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0123 18:50:11.925122 15707 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0123 18:50:11.925194 15707 authenticator.hpp:276] Received SASL authentication start I0123 18:50:11.925257 15707 authenticator.hpp:398] Authentication requires more steps I0123 18:50:11.925325 15707 authenticatee.hpp:275] Received SASL authentication step I0123 18:50:11.925442 15707 authenticator.hpp:304] Received SASL authentication step I0123 18:50:11.925473 15707 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0123 18:50:11.925487 15707 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0123 18:50:11.925532 15707 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0123 18:50:11.925559 15707 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0123 18:50:11.925571 15707 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0123 18:50:11.925580 15707 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0123 18:50:11.925595 15707 authenticator.hpp:390] Authentication success I0123 18:50:11.925695 15707 authenticatee.hpp:315] Authentication success I0123 18:50:11.925792 15707 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 I0123 18:50:11.926127 15707 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:37526 I0123 18:50:11.926154 15707 sched.cpp:515] Sending registration request to master@127.0.0.1:37526 I0123 18:50:11.926215 15707 sched.cpp:548] Will retry registration in 866.81063ms if necessary I0123 18:50:11.926640 15707 master.cpp:1420] Received registration request for framework 'default' at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 I0123 18:50:11.926960 15707 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0123 18:50:11.927691 15707 master.cpp:1484] Registering framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 I0123 18:50:11.928292 15708 hierarchical_allocator_process.hpp:319] Added framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.928326 15708 hierarchical_allocator_process.hpp:839] No resources available to allocate! I0123 18:50:11.928340 15708 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 21080ns I0123 18:50:11.934458 15707 sched.cpp:442] Framework registered with 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.934927 15707 sched.cpp:456] Scheduler::registered took 112885ns I0123 18:50:11.935747 15709 slave.cpp:1075] Will retry registration in 19.609252ms if necessary I0123 18:50:11.935981 15709 master.cpp:3263] Ignoring register slave message from slave(94)@127.0.0.1:37526 (localhost.localdomain) as admission is already in progress I0123 18:50:11.938997 15703 leveldb.cpp:343] Persisting action (334 bytes) to leveldb took 10.171709ms I0123 18:50:11.939049 15703 replica.cpp:679] Persisted action at 3 I0123 18:50:11.940630 15709 replica.cpp:658] Replica received learned notice for position 3 I0123 18:50:11.945473 15709 leveldb.cpp:343] Persisting action (336 bytes) to leveldb took 4.804742ms I0123 18:50:11.945521 15709 replica.cpp:679] Persisted action at 3 I0123 18:50:11.945550 15709 replica.cpp:664] Replica learned APPEND action at position 3 I0123 18:50:11.947105 15709 registrar.cpp:490] Successfully updated the 'registry' in 42.637056ms I0123 18:50:11.948020 15703 master.cpp:3329] Registered slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0123 18:50:11.948318 15703 hierarchical_allocator_process.hpp:453] Added slave 20150123-185011-16777343-37526-15688-S0 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0123 18:50:11.948719 15703 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150123-185011-16777343-37526-15688-S0 in 355831ns I0123 18:50:11.948813 15703 slave.cpp:781] Registered with master master@127.0.0.1:37526; given slave ID 20150123-185011-16777343-37526-15688-S0 I0123 18:50:11.948969 15703 slave.cpp:2588] Received ping from slave-observer(92)@127.0.0.1:37526 I0123 18:50:11.949324 15703 master.cpp:4071] Sending 1 offers to framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 I0123 18:50:11.949571 15706 status_update_manager.cpp:178] Resuming sending status updates I0123 18:50:11.950023 15709 log.cpp:703] Attempting to truncate the log to 3 I0123 18:50:11.950810 15705 sched.cpp:605] Scheduler::resourceOffers took 135580ns I0123 18:50:11.952793 15708 master.cpp:2677] Processing ACCEPT call for offers: [ 20150123-185011-16777343-37526-15688-O0 ] on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) for framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 I0123 18:50:11.952852 15708 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins' W0123 18:50:11.954649 15708 master.cpp:2130] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0123 18:50:11.954988 15708 master.cpp:2142] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0123 18:50:11.955579 15708 master.hpp:782] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150123-185011-16777343-37526-15688-S0 (localhost.localdomain) I0123 18:50:11.956035 15703 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0123 18:50:11.957592 15704 replica.cpp:511] Replica received write request for position 4 I0123 18:50:11.958485 15708 master.cpp:2885] Launching task 1 of framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) I0123 18:50:11.960578 15706 slave.cpp:1130] Got assigned task 1 for framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.961293 15706 slave.cpp:1245] Launching task 1 for framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.964450 15704 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 6.81421ms I0123 18:50:11.964496 15704 replica.cpp:679] Persisted action at 4 I0123 18:50:11.966328 15705 replica.cpp:658] Replica received learned notice for position 4 I0123 18:50:11.969648 15706 slave.cpp:3921] Launching executor default of framework 20150123-185011-16777343-37526-15688-0000 in work directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/slaves/20150123-185011-16777343-37526-15688-S0/frameworks/20150123-185011-16777343-37526-15688-0000/executors/default/runs/02536e4f-fb59-4b75-99aa-611fd7fffcb1' I0123 18:50:11.976954 15705 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 10.584003ms I0123 18:50:11.977078 15705 leveldb.cpp:401] Deleting ~2 keys from leveldb took 72466ns I0123 18:50:11.977104 15705 replica.cpp:679] Persisted action at 4 I0123 18:50:11.977138 15705 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0123 18:50:11.978016 15706 exec.cpp:147] Version: 0.22.0 I0123 18:50:11.978646 15710 exec.cpp:197] Executor started at: executor(50)@127.0.0.1:37526 with pid 15688 I0123 18:50:11.982480 15706 slave.cpp:1368] Queuing task '1' for executor default of framework '20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.982676 15706 slave.cpp:566] Successfully attached file '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/slaves/20150123-185011-16777343-37526-15688-S0/frameworks/20150123-185011-16777343-37526-15688-0000/executors/default/runs/02536e4f-fb59-4b75-99aa-611fd7fffcb1' I0123 18:50:11.982770 15706 slave.cpp:1912] Got registration for executor 'default' of framework 20150123-185011-16777343-37526-15688-0000 from executor(50)@127.0.0.1:37526 I0123 18:50:11.983203 15706 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.983505 15706 slave.cpp:2890] Monitoring executor 'default' of framework '20150123-185011-16777343-37526-15688-0000' in container '02536e4f-fb59-4b75-99aa-611fd7fffcb1' I0123 18:50:11.983749 15706 exec.cpp:221] Executor registered on slave 20150123-185011-16777343-37526-15688-S0 I0123 18:50:11.986131 15706 exec.cpp:233] Executor::registered took 30292ns I0123 18:50:11.989857 15706 exec.cpp:308] Executor asked to run task '1' I0123 18:50:11.990216 15706 exec.cpp:317] Executor::launchTask took 83992ns I0123 18:50:11.992413 15706 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.996598 15703 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 from executor(50)@127.0.0.1:37526 I0123 18:50:11.996922 15703 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.996960 15703 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.997187 15703 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to the slave I0123 18:50:11.997541 15703 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to master@127.0.0.1:37526 I0123 18:50:11.997678 15703 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.997707 15703 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to executor(50)@127.0.0.1:37526 I0123 18:50:11.997936 15703 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.998054 15703 master.cpp:3624] Status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 from slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) I0123 18:50:11.998106 15703 master.cpp:4934] Updating the latest state of task 1 of framework 20150123-185011-16777343-37526-15688-0000 to TASK_RUNNING I0123 18:50:11.998301 15703 sched.cpp:696] Scheduler::statusUpdate took 54363ns I0123 18:50:11.998615 15707 master.cpp:3125] Forwarding status update acknowledgement 3887f5e3-3349-4d1d-8e18-299be6c3c294 for task 1 of framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 to slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) I0123 18:50:11.998867 15707 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:11.999047 15707 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 W0123 18:50:12.001930 15688 sched.cpp:1246]  ************************************************** Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address. ************************************************** I0123 18:50:12.006674 15706 exec.cpp:354] Executor received status update acknowledgement 3887f5e3-3349-4d1d-8e18-299be6c3c294 for task 1 of framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:12.015889 15688 sched.cpp:151] Version: 0.22.0 I0123 18:50:12.017143 15706 sched.cpp:248] New master detected at master@127.0.0.1:37526 I0123 18:50:12.017241 15706 sched.cpp:304] Authenticating with master master@127.0.0.1:37526 I0123 18:50:12.017264 15706 sched.cpp:311] Using default CRAM-MD5 authenticatee I0123 18:50:12.017680 15710 authenticatee.hpp:138] Creating new client SASL connection I0123 18:50:12.018093 15710 master.cpp:4129] Authenticating scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 I0123 18:50:12.018129 15710 master.cpp:4140] Using default CRAM-MD5 authenticator I0123 18:50:12.018590 15710 authenticator.hpp:170] Creating new server SASL connection I0123 18:50:12.018904 15710 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0123 18:50:12.018934 15710 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0123 18:50:12.019039 15710 authenticator.hpp:276] Received SASL authentication start I0123 18:50:12.019101 15710 authenticator.hpp:398] Authentication requires more steps I0123 18:50:12.019172 15710 authenticatee.hpp:275] Received SASL authentication step I0123 18:50:12.019273 15710 authenticator.hpp:304] Received SASL authentication step I0123 18:50:12.019304 15710 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0123 18:50:12.019316 15710 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0123 18:50:12.019364 15710 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0123 18:50:12.020604 15710 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0123 18:50:12.020859 15710 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0123 18:50:12.021114 15710 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0123 18:50:12.021402 15710 authenticator.hpp:390] Authentication success I0123 18:50:12.021790 15705 authenticatee.hpp:315] Authentication success I0123 18:50:12.029628 15705 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:37526 I0123 18:50:12.029682 15705 sched.cpp:515] Sending registration request to master@127.0.0.1:37526 I0123 18:50:12.029784 15705 sched.cpp:548] Will retry registration in 371.903559ms if necessary I0123 18:50:12.030015 15705 master.cpp:1525] Queuing up re-registration request for framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 because authentication is still in progress I0123 18:50:12.030215 15710 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 I0123 18:50:12.030539 15710 master.cpp:1557] Received re-registration request from framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 I0123 18:50:12.030618 15710 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0123 18:50:12.031014 15710 master.cpp:1610] Re-registering framework 20150123-185011-16777343-37526-15688-0000 (default)  at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 I0123 18:50:12.031060 15710 master.cpp:1639] Framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 failed over I0123 18:50:12.031723 15703 sched.cpp:442] Framework registered with 20150123-185011-16777343-37526-15688-0000 I0123 18:50:12.031841 15703 sched.cpp:456] Scheduler::registered took 54566ns I0123 18:50:12.032662 15709 slave.cpp:1762] Updating framework 20150123-185011-16777343-37526-15688-0000 pid to scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 I0123 18:50:12.032924 15709 status_update_manager.cpp:178] Resuming sending status updates I0123 18:50:12.034113 15703 slave.cpp:2571] Sending message for framework 20150123-185011-16777343-37526-15688-0000 to scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 I0123 18:50:12.034302 15703 sched.cpp:782] Scheduler::frameworkMessage took 53684ns I0123 18:50:12.034771 15688 sched.cpp:1471] Asked to stop the driver I0123 18:50:12.034864 15688 sched.cpp:1471] Asked to stop the driver I0123 18:50:12.034942 15688 master.cpp:654] Master terminating W0123 18:50:12.035094 15688 master.cpp:4979] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150123-185011-16777343-37526-15688-0000 on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) in non-terminal state TASK_RUNNING I0123 18:50:12.035724 15688 master.cpp:5022] Removing executor 'default' with resources  of framework 20150123-185011-16777343-37526-15688-0000 on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) I0123 18:50:12.036705 15709 sched.cpp:808] Stopping framework '20150123-185011-16777343-37526-15688-0000' I0123 18:50:12.036960 15709 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150123-185011-16777343-37526-15688-S0 from framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:12.037048 15709 slave.cpp:2673] master@127.0.0.1:37526 exited W0123 18:50:12.037071 15709 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected I0123 18:50:12.037359 15710 sched.cpp:788] Ignoring error message because the driver is not running! I0123 18:50:12.037513 15710 sched.cpp:808] Stopping framework '20150123-185011-16777343-37526-15688-0000' I0123 18:50:12.076481 15688 slave.cpp:495] Slave terminating I0123 18:50:12.080759 15688 slave.cpp:1585] Asked to shut down framework 20150123-185011-16777343-37526-15688-0000 by @0.0.0.0:0 I0123 18:50:12.081023 15688 slave.cpp:1610] Shutting down framework 20150123-185011-16777343-37526-15688-0000 I0123 18:50:12.081351 15688 slave.cpp:3198] Shutting down executor 'default' of framework 20150123-185011-16777343-37526-15688-0000 tests/fault_tolerance_tests.cpp:1383: Failure Actual function call count doesn't match EXPECT_CALL(sched1, error(&driver1, ""Framework failed over""))...          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] FaultToleranceTest.SchedulerFailoverFrameworkMessage (481 ms) {noformat}  Good Run: {noformat} [ RUN      ] FaultToleranceTest.SchedulerFailoverFrameworkMessage Using temporary directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_hEc3n7' I0122 19:15:01.356081  3518 leveldb.cpp:176] Opened db in 19.797885ms I0122 19:15:01.362119  3518 leveldb.cpp:183] Compacted db in 5.953605ms I0122 19:15:01.362191  3518 leveldb.cpp:198] Created db iterator in 30691ns I0122 19:15:01.362210  3518 leveldb.cpp:204] Seeked to beginning of db in 2240ns I0122 19:15:01.362221  3518 leveldb.cpp:273] Iterated through 0 keys in the db in 517ns I0122 19:15:01.362295  3518 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0122 19:15:01.364575  3534 recover.cpp:449] Starting replica recovery I0122 19:15:01.365314  3534 recover.cpp:475] Replica is in EMPTY status I0122 19:15:01.389731  3534 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0122 19:15:01.390005  3534 recover.cpp:195] Received a recover response from a replica in EMPTY status I0122 19:15:01.391346  3538 recover.cpp:566] Updating replica status to STARTING I0122 19:15:01.403445  3538 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 11.806565ms I0122 19:15:01.403795  3538 replica.cpp:323] Persisted replica status to STARTING I0122 19:15:01.406898  3538 recover.cpp:475] Replica is in STARTING status I0122 19:15:01.408671  3537 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0122 19:15:01.413719  3537 recover.cpp:195] Received a recover response from a replica in STARTING status I0122 19:15:01.419553  3538 recover.cpp:566] Updating replica status to VOTING I0122 19:15:01.426426  3536 master.cpp:262] Master 20150122-191501-16777343-50172-3518 (localhost.localdomain) started on 127.0.0.1:50172 W0122 19:15:01.426473  3536 master.cpp:266]  ************************************************** Master bound to loopback interface! Cannot communicate with remote schedulers or slaves. You might want to set '--ip' flag to a routable IP address. ************************************************** I0122 19:15:01.426519  3536 master.cpp:308] Master only allowing authenticated frameworks to register I0122 19:15:01.426532  3536 master.cpp:313] Master only allowing authenticated slaves to register I0122 19:15:01.426564  3536 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_hEc3n7/credentials' I0122 19:15:01.426841  3536 master.cpp:357] Authorization enabled I0122 19:15:01.428205  3533 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process I0122 19:15:01.428627  3534 whitelist_watcher.cpp:65] No whitelist given I0122 19:15:01.429839  3538 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 6.340373ms I0122 19:15:01.429879  3538 replica.cpp:323] Persisted replica status to VOTING I0122 19:15:01.430024  3538 recover.cpp:580] Successfully joined the Paxos group I0122 19:15:01.430233  3538 recover.cpp:464] Recover process terminated I0122 19:15:01.432348  3536 master.cpp:1219] The newly elected leader is master@127.0.0.1:50172 with id 20150122-191501-16777343-50172-3518 I0122 19:15:01.436343  3536 master.cpp:1232] Elected as the leading master! I0122 19:15:01.436738  3536 master.cpp:1050] Recovering from registrar I0122 19:15:01.437191  3535 registrar.cpp:313] Recovering registrar I0122 19:15:01.438340  3535 log.cpp:660] Attempting to start the writer I0122 19:15:01.440163  3533 replica.cpp:477] Replica received implicit promise request with proposal 1 I0122 19:15:01.445287  3533 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 4.550707ms I0122 19:15:01.445327  3533 replica.cpp:345] Persisted promised to 1 I0122 19:15:01.446691  3537 coordinator.cpp:230] Coordinator attemping to fill missing position I0122 19:15:01.448724  3537 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0122 19:15:01.453824  3537 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 4.787009ms I0122 19:15:01.453860  3537 replica.cpp:679] Persisted action at 0 I0122 19:15:01.455684  3533 replica.cpp:511] Replica received write request for position 0 I0122 19:15:01.456087  3533 leveldb.cpp:438] Reading position from leveldb took 37133ns I0122 19:15:01.460862  3533 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 4.458448ms I0122 19:15:01.460897  3533 replica.cpp:679] Persisted action at 0 I0122 19:15:01.461601  3533 replica.cpp:658] Replica received learned notice for position 0 I0122 19:15:01.466660  3533 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.028194ms I0122 19:15:01.466696  3533 replica.cpp:679] Persisted action at 0 I0122 19:15:01.466718  3533 replica.cpp:664] Replica learned NOP action at position 0 I0122 19:15:01.467931  3537 log.cpp:676] Writer started with ending position 0 I0122 19:15:01.469182  3537 leveldb.cpp:438] Reading position from leveldb took 32199ns I0122 19:15:01.479857  3537 registrar.cpp:346] Successfully fetched the registry (0B) in 42.6048ms I0122 19:15:01.480340  3537 registrar.cpp:445] Applied 1 operations in 42179ns; attempting to update the 'registry' I0122 19:15:01.484465  3535 log.cpp:684] Attempting to append 134 bytes to the log I0122 19:15:01.484661  3535 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0122 19:15:01.486250  3535 replica.cpp:511] Replica received write request for position 1 I0122 19:15:01.491243  3535 leveldb.cpp:343] Persisting action (153 bytes) to leveldb took 4.582496ms I0122 19:15:01.491296  3535 replica.cpp:679] Persisted action at 1 I0122 19:15:01.492647  3539 replica.cpp:658] Replica received learned notice for position 1 I0122 19:15:01.497707  3539 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 5.027112ms I0122 19:15:01.497743  3539 replica.cpp:679] Persisted action at 1 I0122 19:15:01.497767  3539 replica.cpp:664] Replica learned APPEND action at position 1 I0122 19:15:01.499428  3539 registrar.cpp:490] Successfully updated the 'registry' in 18.743808ms I0122 19:15:01.499609  3535 log.cpp:703] Attempting to truncate the log to 1 I0122 19:15:01.500036  3535 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0122 19:15:01.501029  3535 replica.cpp:511] Replica received write request for position 2 I0122 19:15:01.501694  3539 registrar.cpp:376] Successfully recovered registrar I0122 19:15:01.502358  3536 master.cpp:1077] Recovered 0 slaves from the Registry (97B) ; allowing 10mins for slaves to re-register I0122 19:15:01.514142  3535 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 13.074759ms I0122 19:15:01.514189  3535 replica.cpp:679] Persisted action at 2 I0122 19:15:01.515473  3535 replica.cpp:658] Replica received learned notice for position 2 I0122 19:15:01.527171  3535 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 11.064363ms I0122 19:15:01.527456  3535 leveldb.cpp:401] Deleting ~1 keys from leveldb took 118227ns I0122 19:15:01.527495  3535 replica.cpp:679] Persisted action at 2 I0122 19:15:01.527537  3535 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0122 19:15:01.563684  3538 slave.cpp:173] Slave started on 76)@127.0.0.1:50172 W0122 19:15:01.563736  3538 slave.cpp:176]  ************************************************** Slave bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address. ************************************************** I0122 19:15:01.563751  3538 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_XZCq6R/credential' I0122 19:15:01.563921  3538 slave.cpp:282] Slave using credential for: test-principal I0122 19:15:01.564209  3538 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0122 19:15:01.564327  3538 slave.cpp:329] Slave hostname: localhost.localdomain I0122 19:15:01.564348  3538 slave.cpp:330] Slave checkpoint: false W0122 19:15:01.564357  3538 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0122 19:15:01.566193  3537 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_XZCq6R/meta' I0122 19:15:01.566629  3537 status_update_manager.cpp:197] Recovering status update manager I0122 19:15:01.566962  3537 slave.cpp:3519] Finished recovery I0122 19:15:01.571022  3533 status_update_manager.cpp:171] Pausing sending status updates I0122 19:15:01.571466  3537 slave.cpp:613] New master detected at master@127.0.0.1:50172 I0122 19:15:01.573503  3537 slave.cpp:676] Authenticating with master master@127.0.0.1:50172 I0122 19:15:01.573771  3537 slave.cpp:681] Using default CRAM-MD5 authenticatee I0122 19:15:01.574427  3540 authenticatee.hpp:138] Creating new client SASL connection I0122 19:15:01.574857  3535 master.cpp:4129] Authenticating slave(76)@127.0.0.1:50172 I0122 19:15:01.574893  3535 master.cpp:4140] Using default CRAM-MD5 authenticator W0122 19:15:01.575266  3518 sched.cpp:1246]  ************************************************** Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address. ************************************************** I0122 19:15:01.576125  3535 authenticator.hpp:170] Creating new server SASL connection I0122 19:15:01.576526  3535 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0122 19:15:01.576563  3535 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0122 19:15:01.576671  3535 authenticator.hpp:276] Received SASL authentication start I0122 19:15:01.576740  3535 authenticator.hpp:398] Authentication requires more steps I0122 19:15:01.576812  3535 authenticatee.hpp:275] Received SASL authentication step I0122 19:15:01.576915  3535 authenticator.hpp:304] Received SASL authentication step I0122 19:15:01.576943  3535 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0122 19:15:01.576967  3535 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0122 19:15:01.577026  3535 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0122 19:15:01.577061  3535 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0122 19:15:01.577076  3535 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0122 19:15:01.577085  3535 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0122 19:15:01.577101  3535 authenticator.hpp:390] Authentication success I0122 19:15:01.577209  3535 authenticatee.hpp:315] Authentication success I0122 19:15:01.577304  3535 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(76)@127.0.0.1:50172 I0122 19:15:01.577615  3537 slave.cpp:649] Detecting new master I0122 19:15:01.580585  3537 slave.cpp:747] Successfully authenticated with master master@127.0.0.1:50172 I0122 19:15:01.585831  3537 slave.cpp:1075] Will retry registration in 18.23486ms if necessary I0122 19:15:01.588697  3535 master.cpp:3275] Registering slave at slave(76)@127.0.0.1:50172 (localhost.localdomain) with id 20150122-191501-16777343-50172-3518-S0 I0122 19:15:01.589609  3539 registrar.cpp:445] Applied 1 operations in 117629ns; attempting to update the 'registry' I0122 19:15:01.592538  3536 log.cpp:684] Attempting to append 313 bytes to the log I0122 19:15:01.592766  3536 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0122 19:15:01.594276  3536 replica.cpp:511] Replica received write request for position 3 I0122 19:15:01.599052  3518 sched.cpp:151] Version: 0.22.0 I0122 19:15:01.600783  3533 sched.cpp:248] New master detected at master@127.0.0.1:50172 I0122 19:15:01.600873  3533 sched.cpp:304] Authenticating with master master@127.0.0.1:50172 I0122 19:15:01.600896  3533 sched.cpp:311] Using default CRAM-MD5 authenticatee I0122 19:15:01.601238  3533 authenticatee.hpp:138] Creating new client SASL connection I0122 19:15:01.601773  3534 master.cpp:4129] Authenticating scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 I0122 19:15:01.601809  3534 master.cpp:4140] Using default CRAM-MD5 authenticator I0122 19:15:01.602197  3534 authenticator.hpp:170] Creating new server SASL connection I0122 19:15:01.602519  3534 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0122 19:15:01.602548  3534 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0122 19:15:01.602651  3534 authenticator.hpp:276] Received SASL authentication start I0122 19:15:01.602705  3534 authenticator.hpp:398] Authentication requires more steps I0122 19:15:01.602774  3534 authenticatee.hpp:275] Received SASL authentication step I0122 19:15:01.602854  3534 authenticator.hpp:304] Received SASL authentication step I0122 19:15:01.602881  3534 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0122 19:15:01.602895  3534 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0122 19:15:01.602936  3534 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0122 19:15:01.602960  3534 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0122 19:15:01.602973  3534 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0122 19:15:01.602982  3534 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0122 19:15:01.602996  3534 authenticator.hpp:390] Authentication success I0122 19:15:01.603091  3534 authenticatee.hpp:315] Authentication success I0122 19:15:01.603174  3534 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 I0122 19:15:01.603533  3535 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:50172 I0122 19:15:01.603559  3535 sched.cpp:515] Sending registration request to master@127.0.0.1:50172 I0122 19:15:01.603626  3535 sched.cpp:548] Will retry registration in 64.334563ms if necessary I0122 19:15:01.604531  3534 master.cpp:1420] Received registration request for framework 'default' at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 I0122 19:15:01.604869  3536 leveldb.cpp:343] Persisting action (332 bytes) to leveldb took 10.363251ms I0122 19:15:01.605108  3536 replica.cpp:679] Persisted action at 3 I0122 19:15:01.606084  3536 replica.cpp:658] Replica received learned notice for position 3 I0122 19:15:01.606972  3534 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0122 19:15:01.607828  3534 master.cpp:1484] Registering framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 I0122 19:15:01.608331  3540 slave.cpp:1075] Will retry registration in 28.084371ms if necessary I0122 19:15:01.610283  3539 hierarchical_allocator_process.hpp:319] Added framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.610349  3539 hierarchical_allocator_process.hpp:839] No resources available to allocate! I0122 19:15:01.610391  3539 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 54938ns I0122 19:15:01.614012  3536 leveldb.cpp:343] Persisting action (334 bytes) to leveldb took 7.895962ms I0122 19:15:01.614048  3536 replica.cpp:679] Persisted action at 3 I0122 19:15:01.614073  3536 replica.cpp:664] Replica learned APPEND action at position 3 I0122 19:15:01.615972  3536 registrar.cpp:490] Successfully updated the 'registry' in 26.294016ms I0122 19:15:01.616164  3533 log.cpp:703] Attempting to truncate the log to 3 I0122 19:15:01.616703  3533 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0122 19:15:01.617676  3533 replica.cpp:511] Replica received write request for position 4 I0122 19:15:01.625704  3537 sched.cpp:442] Framework registered with 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.625782  3537 sched.cpp:456] Scheduler::registered took 45146ns I0122 19:15:01.626240  3534 master.cpp:3263] Ignoring register slave message from slave(76)@127.0.0.1:50172 (localhost.localdomain) as admission is already in progress I0122 19:15:01.627259  3534 master.cpp:3329] Registered slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0122 19:15:01.627607  3534 hierarchical_allocator_process.hpp:453] Added slave 20150122-191501-16777343-50172-3518-S0 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0122 19:15:01.628016  3534 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150122-191501-16777343-50172-3518-S0 in 361785ns I0122 19:15:01.628105  3534 slave.cpp:781] Registered with master master@127.0.0.1:50172; given slave ID 20150122-191501-16777343-50172-3518-S0 I0122 19:15:01.628268  3534 slave.cpp:2588] Received ping from slave-observer(62)@127.0.0.1:50172 I0122 19:15:01.628720  3535 master.cpp:4071] Sending 1 offers to framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 I0122 19:15:01.628861  3535 status_update_manager.cpp:178] Resuming sending status updates I0122 19:15:01.629256  3535 sched.cpp:605] Scheduler::resourceOffers took 76294ns I0122 19:15:01.629585  3533 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 11.873298ms I0122 19:15:01.629623  3533 replica.cpp:679] Persisted action at 4 I0122 19:15:01.631567  3533 replica.cpp:658] Replica received learned notice for position 4 I0122 19:15:01.633208  3540 master.cpp:2677] Processing ACCEPT call for offers: [ 20150122-191501-16777343-50172-3518-O0 ] on slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) for framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 I0122 19:15:01.633386  3540 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins' W0122 19:15:01.635479  3540 master.cpp:2130] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0122 19:15:01.636101  3540 master.cpp:2142] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0122 19:15:01.636804  3540 master.hpp:782] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150122-191501-16777343-50172-3518-S0 (localhost.localdomain) I0122 19:15:01.638121  3540 master.cpp:2885] Launching task 1 of framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) I0122 19:15:01.642609  3536 slave.cpp:1130] Got assigned task 1 for framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.643496  3536 slave.cpp:1245] Launching task 1 for framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.644604  3533 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 13.001968ms I0122 19:15:01.644726  3533 leveldb.cpp:401] Deleting ~2 keys from leveldb took 61434ns I0122 19:15:01.644752  3533 replica.cpp:679] Persisted action at 4 I0122 19:15:01.644778  3533 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0122 19:15:01.648011  3536 slave.cpp:3921] Launching executor default of framework 20150122-191501-16777343-50172-3518-0000 in work directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_XZCq6R/slaves/20150122-191501-16777343-50172-3518-S0/frameworks/20150122-191501-16777343-50172-3518-0000/executors/default/runs/c386f98d-3df4-4aee-b3ad-9e9c1ec7cc15' I0122 19:15:01.657420  3536 exec.cpp:147] Version: 0.22.0 I0122 19:15:01.661609  3534 exec.cpp:197] Executor started at: executor(14)@127.0.0.1:50172 with pid 3518 I0122 19:15:01.662360  3536 slave.cpp:1368] Queuing task '1' for executor default of framework '20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.663007  3536 slave.cpp:566] Successfully attached file '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_XZCq6R/slaves/20150122-191501-16777343-50172-3518-S0/frameworks/20150122-191501-16777343-50172-3518-0000/executors/default/runs/c386f98d-3df4-4aee-b3ad-9e9c1ec7cc15' I0122 19:15:01.665674  3536 slave.cpp:1912] Got registration for executor 'default' of framework 20150122-191501-16777343-50172-3518-0000 from executor(14)@127.0.0.1:50172 I0122 19:15:01.666738  3539 exec.cpp:221] Executor registered on slave 20150122-191501-16777343-50172-3518-S0 I0122 19:15:01.668758  3539 exec.cpp:233] Executor::registered took 76393ns I0122 19:15:01.669208  3536 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.670045  3533 exec.cpp:308] Executor asked to run task '1' I0122 19:15:01.670194  3533 exec.cpp:317] Executor::launchTask took 118431ns I0122 19:15:01.670605  3536 slave.cpp:2890] Monitoring executor 'default' of framework '20150122-191501-16777343-50172-3518-0000' in container 'c386f98d-3df4-4aee-b3ad-9e9c1ec7cc15' I0122 19:15:01.673183  3533 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.675230  3534 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 from executor(14)@127.0.0.1:50172 I0122 19:15:01.675647  3534 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.675689  3534 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.675981  3534 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 to the slave I0122 19:15:01.676338  3534 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 to master@127.0.0.1:50172 I0122 19:15:01.676910  3538 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.677034  3538 master.cpp:3624] Status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 from slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) I0122 19:15:01.677098  3538 master.cpp:4934] Updating the latest state of task 1 of framework 20150122-191501-16777343-50172-3518-0000 to TASK_RUNNING I0122 19:15:01.677338  3538 sched.cpp:696] Scheduler::statusUpdate took 71579ns I0122 19:15:01.677701  3538 master.cpp:3125] Forwarding status update acknowledgement 2150029d-e89c-40a6-998f-c0295d72d964 for task 1 of framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 to slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) W0122 19:15:01.680450  3518 sched.cpp:1246]  ************************************************** Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address. ************************************************** I0122 19:15:01.684923  3534 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.685042  3534 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 to executor(14)@127.0.0.1:50172 I0122 19:15:01.687777  3534 exec.cpp:354] Executor received status update acknowledgement 2150029d-e89c-40a6-998f-c0295d72d964 for task 1 of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.687896  3537 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.688174  3537 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.707738  3518 sched.cpp:151] Version: 0.22.0 I0122 19:15:01.708892  3540 sched.cpp:248] New master detected at master@127.0.0.1:50172 I0122 19:15:01.708973  3540 sched.cpp:304] Authenticating with master master@127.0.0.1:50172 I0122 19:15:01.708997  3540 sched.cpp:311] Using default CRAM-MD5 authenticatee I0122 19:15:01.709345  3540 authenticatee.hpp:138] Creating new client SASL connection I0122 19:15:01.710389  3534 master.cpp:4129] Authenticating scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172 I0122 19:15:01.710440  3534 master.cpp:4140] Using default CRAM-MD5 authenticator I0122 19:15:01.710844  3534 authenticator.hpp:170] Creating new server SASL connection I0122 19:15:01.711359  3540 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0122 19:15:01.711762  3540 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0122 19:15:01.712133  3540 authenticator.hpp:276] Received SASL authentication start I0122 19:15:01.712434  3540 authenticator.hpp:398] Authentication requires more steps I0122 19:15:01.712754  3540 authenticatee.hpp:275] Received SASL authentication step I0122 19:15:01.713156  3536 authenticator.hpp:304] Received SASL authentication step I0122 19:15:01.713191  3536 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0122 19:15:01.713204  3536 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0122 19:15:01.713263  3536 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0122 19:15:01.713290  3536 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0122 19:15:01.713304  3536 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0122 19:15:01.713311  3536 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0122 19:15:01.713326  3536 authenticator.hpp:390] Authentication success I0122 19:15:01.713470  3536 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172 I0122 19:15:01.716526  3540 authenticatee.hpp:315] Authentication success I0122 19:15:01.720747  3540 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:50172 I0122 19:15:01.720780  3540 sched.cpp:515] Sending registration request to master@127.0.0.1:50172 I0122 19:15:01.720852  3540 sched.cpp:548] Will retry registration in 1.20284193secs if necessary I0122 19:15:01.721050  3540 master.cpp:1557] Received re-registration request from framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172 I0122 19:15:01.721143  3540 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0122 19:15:01.721583  3540 master.cpp:1610] Re-registering framework 20150122-191501-16777343-50172-3518-0000 (default)  at scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172 I0122 19:15:01.721629  3540 master.cpp:1639] Framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 failed over I0122 19:15:01.721943  3540 sched.cpp:792] Got error 'Framework failed over' I0122 19:15:01.721972  3540 sched.cpp:1505] Asked to abort the driver I0122 19:15:01.722039  3540 sched.cpp:803] Scheduler::error took 26469ns I0122 19:15:01.722084  3540 sched.cpp:833] Aborting framework '20150122-191501-16777343-50172-3518-0000' I0122 19:15:01.722196  3540 sched.cpp:442] Framework registered with 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.722262  3540 sched.cpp:456] Scheduler::registered took 40517ns W0122 19:15:01.722734  3538 master.cpp:1775] Ignoring deactivate framework message for framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172 because it is not expected from scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 I0122 19:15:01.724819  3540 slave.cpp:1762] Updating framework 20150122-191501-16777343-50172-3518-0000 pid to scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172 I0122 19:15:01.725316  3533 status_update_manager.cpp:178] Resuming sending status updates I0122 19:15:01.726033  3540 slave.cpp:2571] Sending message for framework 20150122-191501-16777343-50172-3518-0000 to scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172 I0122 19:15:01.727016  3534 sched.cpp:782] Scheduler::frameworkMessage took 57233ns I0122 19:15:01.727601  3518 sched.cpp:1471] Asked to stop the driver I0122 19:15:01.727665  3518 sched.cpp:1471] Asked to stop the driver I0122 19:15:01.727743  3518 master.cpp:654] Master terminating W0122 19:15:01.727893  3518 master.cpp:4979] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150122-191501-16777343-50172-3518-0000 on slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) in non-terminal state TASK_RUNNING I0122 19:15:01.728521  3518 master.cpp:5022] Removing executor 'default' with resources  of framework 20150122-191501-16777343-50172-3518-0000 on slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) I0122 19:15:01.729651  3534 sched.cpp:808] Stopping framework '20150122-191501-16777343-50172-3518-0000' I0122 19:15:01.729786  3534 sched.cpp:808] Stopping framework '20150122-191501-16777343-50172-3518-0000' I0122 19:15:01.730036  3534 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150122-191501-16777343-50172-3518-S0 from framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.730134  3534 slave.cpp:2673] master@127.0.0.1:50172 exited W0122 19:15:01.730156  3534 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected I0122 19:15:01.782312  3518 slave.cpp:495] Slave terminating I0122 19:15:01.786846  3518 slave.cpp:1585] Asked to shut down framework 20150122-191501-16777343-50172-3518-0000 by @0.0.0.0:0 I0122 19:15:01.787127  3518 slave.cpp:1610] Shutting down framework 20150122-191501-16777343-50172-3518-0000 I0122 19:15:01.787394  3518 slave.cpp:3198] Shutting down executor 'default' of framework 20150122-191501-16777343-50172-3518-0000 [       OK ] FaultToleranceTest.SchedulerFailoverFrameworkMessage (495 ms) {noformat}",1
"MESOS-2306","MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky.","Good run:  {noformat} [ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD' I0122 19:23:06.481690 17483 leveldb.cpp:176] Opened db in 21.058723ms I0122 19:23:06.488590 17483 leveldb.cpp:183] Compacted db in 6.6715ms I0122 19:23:06.488816 17483 leveldb.cpp:198] Created db iterator in 30034ns I0122 19:23:06.489053 17483 leveldb.cpp:204] Seeked to beginning of db in 2908ns I0122 19:23:06.489073 17483 leveldb.cpp:273] Iterated through 0 keys in the db in 492ns I0122 19:23:06.489148 17483 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0122 19:23:06.490272 17504 recover.cpp:449] Starting replica recovery I0122 19:23:06.490900 17504 recover.cpp:475] Replica is in EMPTY status I0122 19:23:06.492422 17504 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0122 19:23:06.492694 17504 recover.cpp:195] Received a recover response from a replica in EMPTY status I0122 19:23:06.493185 17504 recover.cpp:566] Updating replica status to STARTING I0122 19:23:06.514881 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 21.459963ms I0122 19:23:06.514920 17504 replica.cpp:323] Persisted replica status to STARTING I0122 19:23:06.515861 17501 master.cpp:262] Master 20150122-192306-16842879-46283-17483 (lucid) started on 127.0.1.1:46283 I0122 19:23:06.515910 17501 master.cpp:308] Master only allowing authenticated frameworks to register I0122 19:23:06.515923 17501 master.cpp:313] Master only allowing authenticated slaves to register I0122 19:23:06.515946 17501 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD/credentials' I0122 19:23:06.516150 17501 master.cpp:357] Authorization enabled I0122 19:23:06.517511 17501 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process I0122 19:23:06.517607 17501 whitelist_watcher.cpp:65] No whitelist given I0122 19:23:06.518066 17498 master.cpp:1219] The newly elected leader is master@127.0.1.1:46283 with id 20150122-192306-16842879-46283-17483 I0122 19:23:06.518095 17498 master.cpp:1232] Elected as the leading master! I0122 19:23:06.518121 17498 master.cpp:1050] Recovering from registrar I0122 19:23:06.518333 17498 registrar.cpp:313] Recovering registrar I0122 19:23:06.523987 17504 recover.cpp:475] Replica is in STARTING status I0122 19:23:06.525090 17504 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0122 19:23:06.525337 17504 recover.cpp:195] Received a recover response from a replica in STARTING status I0122 19:23:06.525693 17504 recover.cpp:566] Updating replica status to VOTING I0122 19:23:06.532680 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 6.810884ms I0122 19:23:06.532714 17504 replica.cpp:323] Persisted replica status to VOTING I0122 19:23:06.532835 17504 recover.cpp:580] Successfully joined the Paxos group I0122 19:23:06.533004 17504 recover.cpp:464] Recover process terminated I0122 19:23:06.533833 17500 log.cpp:660] Attempting to start the writer I0122 19:23:06.535225 17500 replica.cpp:477] Replica received implicit promise request with proposal 1 I0122 19:23:06.540340 17500 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.086139ms I0122 19:23:06.540371 17500 replica.cpp:345] Persisted promised to 1 I0122 19:23:06.541502 17504 coordinator.cpp:230] Coordinator attemping to fill missing position I0122 19:23:06.543021 17504 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0122 19:23:06.548140 17504 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.083443ms I0122 19:23:06.548171 17504 replica.cpp:679] Persisted action at 0 I0122 19:23:06.549746 17500 replica.cpp:511] Replica received write request for position 0 I0122 19:23:06.549926 17500 leveldb.cpp:438] Reading position from leveldb took 31962ns I0122 19:23:06.555033 17500 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.065823ms I0122 19:23:06.555064 17500 replica.cpp:679] Persisted action at 0 I0122 19:23:06.556094 17504 replica.cpp:658] Replica received learned notice for position 0 I0122 19:23:06.558815 17504 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.688382ms I0122 19:23:06.558847 17504 replica.cpp:679] Persisted action at 0 I0122 19:23:06.558868 17504 replica.cpp:664] Replica learned NOP action at position 0 I0122 19:23:06.559917 17500 log.cpp:676] Writer started with ending position 0 I0122 19:23:06.560995 17500 leveldb.cpp:438] Reading position from leveldb took 27742ns I0122 19:23:06.563467 17500 registrar.cpp:346] Successfully fetched the registry (0B) in 45.095936ms I0122 19:23:06.563551 17500 registrar.cpp:445] Applied 1 operations in 19686ns; attempting to update the 'registry' I0122 19:23:06.566107 17500 log.cpp:684] Attempting to append 118 bytes to the log I0122 19:23:06.566267 17500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0122 19:23:06.567126 17500 replica.cpp:511] Replica received write request for position 1 I0122 19:23:06.582588 17500 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 15.425511ms I0122 19:23:06.582631 17500 replica.cpp:679] Persisted action at 1 I0122 19:23:06.583425 17500 replica.cpp:658] Replica received learned notice for position 1 I0122 19:23:06.589001 17500 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.549486ms I0122 19:23:06.589200 17500 replica.cpp:679] Persisted action at 1 I0122 19:23:06.589416 17500 replica.cpp:664] Replica learned APPEND action at position 1 I0122 19:23:06.596420 17500 registrar.cpp:490] Successfully updated the 'registry' in 32.815104ms I0122 19:23:06.596551 17500 registrar.cpp:376] Successfully recovered registrar I0122 19:23:06.596923 17500 master.cpp:1077] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register I0122 19:23:06.597007 17500 log.cpp:703] Attempting to truncate the log to 1 I0122 19:23:06.597239 17500 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0122 19:23:06.598464 17501 replica.cpp:511] Replica received write request for position 2 I0122 19:23:06.604038 17501 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.536264ms I0122 19:23:06.604084 17501 replica.cpp:679] Persisted action at 2 I0122 19:23:06.608747 17503 replica.cpp:658] Replica received learned notice for position 2 I0122 19:23:06.614094 17503 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.315347ms I0122 19:23:06.614171 17503 leveldb.cpp:401] Deleting ~1 keys from leveldb took 33021ns I0122 19:23:06.614188 17503 replica.cpp:679] Persisted action at 2 I0122 19:23:06.614208 17503 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0122 19:23:06.628820 17483 sched.cpp:151] Version: 0.22.0 I0122 19:23:06.629879 17505 sched.cpp:248] New master detected at master@127.0.1.1:46283 I0122 19:23:06.629973 17505 sched.cpp:304] Authenticating with master master@127.0.1.1:46283 I0122 19:23:06.629995 17505 sched.cpp:311] Using default CRAM-MD5 authenticatee I0122 19:23:06.630314 17505 authenticatee.hpp:138] Creating new client SASL connection I0122 19:23:06.630722 17505 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.630750 17505 master.cpp:4140] Using default CRAM-MD5 authenticator I0122 19:23:06.631115 17505 authenticator.hpp:170] Creating new server SASL connection I0122 19:23:06.631423 17505 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0122 19:23:06.631459 17505 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0122 19:23:06.631563 17505 authenticator.hpp:276] Received SASL authentication start I0122 19:23:06.631605 17505 authenticator.hpp:398] Authentication requires more steps I0122 19:23:06.631671 17505 authenticatee.hpp:275] Received SASL authentication step I0122 19:23:06.631748 17505 authenticator.hpp:304] Received SASL authentication step I0122 19:23:06.631774 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0122 19:23:06.631784 17505 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0122 19:23:06.631822 17505 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0122 19:23:06.631856 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0122 19:23:06.631870 17505 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0122 19:23:06.631877 17505 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0122 19:23:06.631892 17505 authenticator.hpp:390] Authentication success I0122 19:23:06.631988 17505 authenticatee.hpp:315] Authentication success I0122 19:23:06.632066 17505 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.632359 17505 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283 I0122 19:23:06.632382 17505 sched.cpp:515] Sending registration request to master@127.0.1.1:46283 I0122 19:23:06.632432 17505 sched.cpp:548] Will retry registration in 598.155756ms if necessary I0122 19:23:06.632575 17505 master.cpp:1420] Received registration request for framework 'default' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.632639 17505 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0122 19:23:06.632912 17505 master.cpp:1484] Registering framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.633421 17505 hierarchical_allocator_process.hpp:319] Added framework 20150122-192306-16842879-46283-17483-0000 I0122 19:23:06.633448 17505 hierarchical_allocator_process.hpp:839] No resources available to allocate! I0122 19:23:06.633458 17505 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 17704ns I0122 19:23:06.633919 17505 sched.cpp:442] Framework registered with 20150122-192306-16842879-46283-17483-0000 I0122 19:23:06.633980 17505 sched.cpp:456] Scheduler::registered took 37063ns I0122 19:23:06.636554 17500 sched.cpp:242] Scheduler::disconnected took 14843ns I0122 19:23:06.636579 17500 sched.cpp:248] New master detected at master@127.0.1.1:46283 I0122 19:23:06.636625 17500 sched.cpp:304] Authenticating with master master@127.0.1.1:46283 I0122 19:23:06.636641 17500 sched.cpp:311] Using default CRAM-MD5 authenticatee I0122 19:23:06.636914 17500 authenticatee.hpp:138] Creating new client SASL connection I0122 19:23:06.637313 17500 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.637341 17500 master.cpp:4140] Using default CRAM-MD5 authenticator I0122 19:23:06.637675 17500 authenticator.hpp:170] Creating new server SASL connection I0122 19:23:06.638056 17501 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0122 19:23:06.638083 17501 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0122 19:23:06.638182 17501 authenticator.hpp:276] Received SASL authentication start I0122 19:23:06.638221 17501 authenticator.hpp:398] Authentication requires more steps I0122 19:23:06.638286 17501 authenticatee.hpp:275] Received SASL authentication step I0122 19:23:06.638360 17501 authenticator.hpp:304] Received SASL authentication step I0122 19:23:06.638383 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0122 19:23:06.638393 17501 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0122 19:23:06.638422 17501 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0122 19:23:06.638447 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0122 19:23:06.638458 17501 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0122 19:23:06.638464 17501 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0122 19:23:06.638478 17501 authenticator.hpp:390] Authentication success I0122 19:23:06.638566 17501 authenticatee.hpp:315] Authentication success I0122 19:23:06.638643 17501 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.638919 17501 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283 I0122 19:23:06.638942 17501 sched.cpp:515] Sending registration request to master@127.0.1.1:46283 I0122 19:23:06.638994 17501 sched.cpp:548] Will retry registration in 489.304713ms if necessary I0122 19:23:06.639169 17501 master.cpp:1557] Received re-registration request from framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.639242 17501 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0122 19:23:06.639839 17483 sched.cpp:1471] Asked to stop the driver I0122 19:23:06.640379 17499 sched.cpp:808] Stopping framework '20150122-192306-16842879-46283-17483-0000' I0122 19:23:06.640697 17499 master.cpp:745] Framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 disconnected I0122 19:23:06.640723 17499 master.cpp:1789] Disconnecting framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.640744 17499 master.cpp:1805] Deactivating framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.640806 17499 master.cpp:767] Giving framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 0ns to failover I0122 19:23:06.640951 17499 hierarchical_allocator_process.hpp:398] Deactivated framework 20150122-192306-16842879-46283-17483-0000 I0122 19:23:06.646342 17498 master.cpp:1604] Dropping re-registration request of framework 20150122-192306-16842879-46283-17483-0000 (default)  at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 because it is not authenticated I0122 19:23:06.648844 17498 master.cpp:3941] Framework failover timeout, removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.648871 17498 master.cpp:4499] Removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 I0122 19:23:06.649624 17498 hierarchical_allocator_process.hpp:352] Removed framework 20150122-192306-16842879-46283-17483-0000 I0122 19:23:06.656532 17483 master.cpp:654] Master terminating [       OK ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration (216 ms) {noformat}  Bad run:  {noformat} [ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm' I0126 19:19:55.517570  2381 leveldb.cpp:176] Opened db in 34.341401ms I0126 19:19:55.529630  2381 leveldb.cpp:183] Compacted db in 11.824435ms I0126 19:19:55.529878  2381 leveldb.cpp:198] Created db iterator in 26176ns I0126 19:19:55.530200  2381 leveldb.cpp:204] Seeked to beginning of db in 3457ns I0126 19:19:55.530455  2381 leveldb.cpp:273] Iterated through 0 keys in the db in 902ns I0126 19:19:55.530658  2381 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0126 19:19:55.531492  2397 recover.cpp:449] Starting replica recovery I0126 19:19:55.531793  2397 recover.cpp:475] Replica is in EMPTY status I0126 19:19:55.533327  2397 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0126 19:19:55.533608  2397 recover.cpp:195] Received a recover response from a replica in EMPTY status I0126 19:19:55.534101  2397 recover.cpp:566] Updating replica status to STARTING I0126 19:19:55.550417  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.106821ms I0126 19:19:55.550472  2397 replica.cpp:323] Persisted replica status to STARTING I0126 19:19:55.551434  2397 recover.cpp:475] Replica is in STARTING status I0126 19:19:55.552846  2397 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0126 19:19:55.553099  2397 recover.cpp:195] Received a recover response from a replica in STARTING status I0126 19:19:55.553565  2397 recover.cpp:566] Updating replica status to VOTING I0126 19:19:55.564590  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 10.719218ms I0126 19:19:55.564919  2397 replica.cpp:323] Persisted replica status to VOTING I0126 19:19:55.565982  2397 recover.cpp:580] Successfully joined the Paxos group I0126 19:19:55.566231  2397 recover.cpp:464] Recover process terminated I0126 19:19:55.567878  2401 master.cpp:262] Master 20150126-191955-16842879-51862-2381 (lucid) started on 127.0.1.1:51862 I0126 19:19:55.567927  2401 master.cpp:308] Master only allowing authenticated frameworks to register I0126 19:19:55.567950  2401 master.cpp:313] Master only allowing authenticated slaves to register I0126 19:19:55.567978  2401 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm/credentials' I0126 19:19:55.568220  2401 master.cpp:357] Authorization enabled I0126 19:19:55.569890  2401 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process I0126 19:19:55.569999  2401 whitelist_watcher.cpp:65] No whitelist given I0126 19:19:55.570694  2401 master.cpp:1219] The newly elected leader is master@127.0.1.1:51862 with id 20150126-191955-16842879-51862-2381 I0126 19:19:55.570721  2401 master.cpp:1232] Elected as the leading master! I0126 19:19:55.570742  2401 master.cpp:1050] Recovering from registrar I0126 19:19:55.570977  2401 registrar.cpp:313] Recovering registrar I0126 19:19:55.571959  2401 log.cpp:660] Attempting to start the writer I0126 19:19:55.573441  2401 replica.cpp:477] Replica received implicit promise request with proposal 1 I0126 19:19:55.590724  2401 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.243964ms I0126 19:19:55.590785  2401 replica.cpp:345] Persisted promised to 1 I0126 19:19:55.592140  2396 coordinator.cpp:230] Coordinator attemping to fill missing position I0126 19:19:55.593834  2396 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0126 19:19:55.603837  2396 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 9.955824ms I0126 19:19:55.603902  2396 replica.cpp:679] Persisted action at 0 I0126 19:19:55.606082  2401 replica.cpp:511] Replica received write request for position 0 I0126 19:19:55.606331  2401 leveldb.cpp:438] Reading position from leveldb took 44524ns I0126 19:19:55.612546  2401 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.870411ms I0126 19:19:55.612597  2401 replica.cpp:679] Persisted action at 0 I0126 19:19:55.613416  2401 replica.cpp:658] Replica received learned notice for position 0 I0126 19:19:55.616269  2401 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.82145ms I0126 19:19:55.616305  2401 replica.cpp:679] Persisted action at 0 I0126 19:19:55.616328  2401 replica.cpp:664] Replica learned NOP action at position 0 I0126 19:19:55.628062  2399 log.cpp:676] Writer started with ending position 0 I0126 19:19:55.629328  2399 leveldb.cpp:438] Reading position from leveldb took 57003ns I0126 19:19:55.631995  2399 registrar.cpp:346] Successfully fetched the registry (0B) in 60.973824ms I0126 19:19:55.632109  2399 registrar.cpp:445] Applied 1 operations in 35531ns; attempting to update the 'registry' I0126 19:19:55.634799  2399 log.cpp:684] Attempting to append 117 bytes to the log I0126 19:19:55.634996  2399 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0126 19:19:55.636651  2397 replica.cpp:511] Replica received write request for position 1 I0126 19:19:55.642165  2397 leveldb.cpp:343] Persisting action (134 bytes) to leveldb took 5.474306ms I0126 19:19:55.642215  2397 replica.cpp:679] Persisted action at 1 I0126 19:19:55.643226  2397 replica.cpp:658] Replica received learned notice for position 1 I0126 19:19:55.648574  2397 leveldb.cpp:343] Persisting action (136 bytes) to leveldb took 5.317891ms I0126 19:19:55.648808  2397 replica.cpp:679] Persisted action at 1 I0126 19:19:55.649158  2397 replica.cpp:664] Replica learned APPEND action at position 1 I0126 19:19:55.663101  2397 registrar.cpp:490] Successfully updated the 'registry' in 30.918144ms I0126 19:19:55.663267  2397 registrar.cpp:376] Successfully recovered registrar I0126 19:19:55.663699  2397 master.cpp:1077] Recovered 0 slaves from the Registry (81B) ; allowing 10mins for slaves to re-register I0126 19:19:55.663795  2397 log.cpp:703] Attempting to truncate the log to 1 I0126 19:19:55.664083  2397 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0126 19:19:55.665573  2403 replica.cpp:511] Replica received write request for position 2 I0126 19:19:55.671500  2403 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.883759ms I0126 19:19:55.671547  2403 replica.cpp:679] Persisted action at 2 I0126 19:19:55.672780  2403 replica.cpp:658] Replica received learned notice for position 2 I0126 19:19:55.685999  2403 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 12.808643ms I0126 19:19:55.686099  2403 leveldb.cpp:401] Deleting ~1 keys from leveldb took 49867ns I0126 19:19:55.686121  2403 replica.cpp:679] Persisted action at 2 I0126 19:19:55.686149  2403 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0126 19:19:55.722545  2381 sched.cpp:151] Version: 0.22.0 I0126 19:19:55.723795  2401 sched.cpp:248] New master detected at master@127.0.1.1:51862 I0126 19:19:55.723891  2401 sched.cpp:304] Authenticating with master master@127.0.1.1:51862 I0126 19:19:55.723914  2401 sched.cpp:311] Using default CRAM-MD5 authenticatee I0126 19:19:55.724244  2401 authenticatee.hpp:138] Creating new client SASL connection I0126 19:19:55.724694  2401 master.cpp:4129] Authenticating scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.724725  2401 master.cpp:4140] Using default CRAM-MD5 authenticator I0126 19:19:55.725108  2401 authenticator.hpp:170] Creating new server SASL connection I0126 19:19:55.725390  2401 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0126 19:19:55.725415  2401 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0126 19:19:55.725515  2401 authenticator.hpp:276] Received SASL authentication start I0126 19:19:55.725566  2401 authenticator.hpp:398] Authentication requires more steps I0126 19:19:55.725632  2401 authenticatee.hpp:275] Received SASL authentication step I0126 19:19:55.725710  2401 authenticator.hpp:304] Received SASL authentication step I0126 19:19:55.725744  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0126 19:19:55.725757  2401 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0126 19:19:55.725808  2401 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0126 19:19:55.725834  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0126 19:19:55.725847  2401 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0126 19:19:55.725853  2401 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0126 19:19:55.725867  2401 authenticator.hpp:390] Authentication success I0126 19:19:55.728629  2399 authenticatee.hpp:315] Authentication success I0126 19:19:55.729228  2399 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:51862 I0126 19:19:55.729277  2399 sched.cpp:515] Sending registration request to master@127.0.1.1:51862 I0126 19:19:55.729365  2399 sched.cpp:548] Will retry registration in 3.855403ms if necessary I0126 19:19:55.729671  2399 master.cpp:1411] Queuing up registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 because authentication is still in progress I0126 19:19:55.733487  2400 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.734094  2400 master.cpp:1420] Received registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.734177  2400 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0126 19:19:55.734724  2400 master.cpp:1484] Registering framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.735335  2402 hierarchical_allocator_process.hpp:319] Added framework 20150126-191955-16842879-51862-2381-0000 I0126 19:19:55.735376  2402 hierarchical_allocator_process.hpp:831] No resources available to allocate! I0126 19:19:55.735389  2402 hierarchical_allocator_process.hpp:738] Performed allocation for 0 slaves in 22978ns I0126 19:19:55.741891  2398 sched.cpp:515] Sending registration request to master@127.0.1.1:51862 I0126 19:19:55.744575  2398 sched.cpp:548] Will retry registration in 3.86742709secs if necessary I0126 19:19:55.744742  2398 sched.cpp:442] Framework registered with 20150126-191955-16842879-51862-2381-0000 I0126 19:19:55.744827  2398 sched.cpp:456] Scheduler::registered took 60111ns I0126 19:19:55.744956  2398 master.cpp:1420] Received registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.745020  2398 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0126 19:19:55.749315  2401 sched.cpp:242] Scheduler::disconnected took 19450ns I0126 19:19:55.749343  2401 sched.cpp:248] New master detected at master@127.0.1.1:51862 I0126 19:19:55.749394  2401 sched.cpp:304] Authenticating with master master@127.0.1.1:51862 I0126 19:19:55.749411  2401 sched.cpp:311] Using default CRAM-MD5 authenticatee I0126 19:19:55.749743  2401 authenticatee.hpp:138] Creating new client SASL connection I0126 19:19:55.750208  2401 master.cpp:4129] Authenticating scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.750238  2401 master.cpp:4140] Using default CRAM-MD5 authenticator I0126 19:19:55.750629  2401 authenticator.hpp:170] Creating new server SASL connection I0126 19:19:55.750938  2401 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0126 19:19:55.750963  2401 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0126 19:19:55.751063  2401 authenticator.hpp:276] Received SASL authentication start I0126 19:19:55.751109  2401 authenticator.hpp:398] Authentication requires more steps I0126 19:19:55.751175  2401 authenticatee.hpp:275] Received SASL authentication step I0126 19:19:55.751269  2401 authenticator.hpp:304] Received SASL authentication step I0126 19:19:55.751296  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0126 19:19:55.751307  2401 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0126 19:19:55.751358  2401 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0126 19:19:55.751392  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0126 19:19:55.751405  2401 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0126 19:19:55.751413  2401 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0126 19:19:55.751427  2401 authenticator.hpp:390] Authentication success I0126 19:19:55.751524  2401 authenticatee.hpp:315] Authentication success I0126 19:19:55.751605  2401 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.751898  2401 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:51862 I0126 19:19:55.751922  2401 sched.cpp:515] Sending registration request to master@127.0.1.1:51862 I0126 19:19:55.751996  2401 sched.cpp:548] Will retry registration in 1.511226315secs if necessary I0126 19:19:55.752174  2401 master.cpp:1557] Received re-registration request from framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.752256  2401 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*' I0126 19:19:55.752485  2401 master.cpp:1610] Re-registering framework 20150126-191955-16842879-51862-2381-0000 (default)  at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.752527  2401 master.cpp:1650] Allowing framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 to re-register with an already used id I0126 19:19:55.752689  2401 sched.cpp:484] Framework re-registered with 20150126-191955-16842879-51862-2381-0000 tests/master_authorization_tests.cpp:980: Failure Mock function called more times than expected - returning directly.     Function call: reregistered(0x7fff5cef57e0, @0x56077d0 id: ""20150126-191955-16842879-51862-2381"" ip: 16842879 port: 51862 pid: ""master@127.0.1.1:51862"" hostname: ""lucid"" )          Expected: to be never called            Actual: called once - over-saturated and active I0126 19:19:55.753191  2401 sched.cpp:498] Scheduler::reregistered took 478798ns I0126 19:19:55.753600  2381 sched.cpp:1471] Asked to stop the driver I0126 19:19:55.754518  2402 sched.cpp:808] Stopping framework '20150126-191955-16842879-51862-2381-0000' I0126 19:19:55.755089  2402 master.cpp:1744] Asked to unregister framework 20150126-191955-16842879-51862-2381-0000 I0126 19:19:55.755302  2402 master.cpp:4499] Removing framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 I0126 19:19:55.759419  2402 hierarchical_allocator_process.hpp:398] Deactivated framework 20150126-191955-16842879-51862-2381-0000 I0126 19:19:55.759850  2402 hierarchical_allocator_process.hpp:352] Removed framework 20150126-191955-16842879-51862-2381-0000 I0126 19:19:55.761160  2400 master.cpp:1462] Dropping registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 because it is not authenticated I0126 19:19:55.771309  2381 master.cpp:654] Master terminating [  FAILED  ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration  (312 ms) {noformat}",1
"MESOS-2314","remove unnecessary constants","In {{src/slave/paths.cpp}} a number of string constants are defined to describe the formats of various paths. However, given there is a 1:1 mapping between the string constant and the functions that build the paths, the code would be more readable if the format strings were inline in the functions.  In the cases where one constant depends on another (see the {{EXECUTOR_INFO_PATH, EXECUTOR_PATH, FRAMEWORK_PATH, SLAVE_PATH, ROOT_PATH}} chain, for example) the function calls can just be chained together.  This will have the added benefit of removing some statically constructed string constants, which are dangerous.",2
"MESOS-2319","Unable to set --work_dir to a non /tmp device","When starting mesos-slave with --work_dir set to a directory which is not the same device as /tmp results in mesos-slave throwing a core dump: {code} mesos # GLOG_v=1 sbin/mesos-slave --master=zk://10.171.59.83:2181/mesos --work_dir=/var/lib/mesos/ WARNING: Logging before InitGoogleLogging() is written to STDERR I0204 18:24:49.274619 22922 process.cpp:958] libprocess is initialized on 10.169.146.67:5051 for 8 cpus I0204 18:24:49.274978 22922 logging.cpp:177] Logging to STDERR I0204 18:24:49.275111 22922 main.cpp:152] Build: 2015-02-03 22:59:30 by  I0204 18:24:49.275233 22922 main.cpp:154] Version: 0.22.0 I0204 18:24:49.275485 22922 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem 2015-02-04 18:24:49,275:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2015-02-04 18:24:49,275:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@716: Client environment:host.name=ip-10-169-146-67.ec2.internal 2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@724: Client environment:os.arch=3.18.2 2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@725: Client environment:os.version=#2 SMP Tue Jan 27 23:34:36 UTC 2015 2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@733: Client environment:user.name=core 2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@741: Client environment:user.home=/root 2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@753: Client environment:user.dir=/opt/mesosphere/dcos/0.0.1-0.1.20150203225612/mesos 2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=10.171.59.83:2181 sessionTimeout=10000 watcher=0x7ffdd97bccf0 sessionId=0 sessionPasswd=<null> context=0x7ffdc8000ba0 flags=0 I0204 18:24:49.276793 22922 main.cpp:180] Starting Mesos slave 2015-02-04 18:24:49,307:22922(0x7ffdd151f700):ZOO_INFO@check_events@1703: initiated connection to server [10.171.59.83:2181] I0204 18:24:49.307548 22922 slave.cpp:173] Slave started on 1)@10.169.146.67:5051 I0204 18:24:49.307955 22922 slave.cpp:300] Slave resources: cpus(*):1; mem(*):2728; disk(*):24736; ports(*):[31000-32000] I0204 18:24:49.308404 22922 slave.cpp:329] Slave hostname: ip-10-169-146-67.ec2.internal I0204 18:24:49.308459 22922 slave.cpp:330] Slave checkpoint: true I0204 18:24:49.310431 22924 state.cpp:33] Recovering state from '/var/lib/mesos/meta' I0204 18:24:49.310583 22924 state.cpp:668] Failed to find resources file '/var/lib/mesos/meta/resources/resources.info' I0204 18:24:49.310670 22924 state.cpp:74] Failed to find the latest slave from '/var/lib/mesos/meta' I0204 18:24:49.310803 22924 status_update_manager.cpp:197] Recovering status update manager I0204 18:24:49.310916 22924 containerizer.cpp:300] Recovering containerizer I0204 18:24:49.311110 22924 slave.cpp:3527] Finished recovery F0204 18:24:49.311312 22924 slave.cpp:3537] CHECK_SOME(state::checkpoint(path, bootId.get())): Failed to rename '/tmp/PSHLqV' to '/var/lib/mesos/meta/boot_id': Invalid cross-device link  2015-02-04 18:24:49,310:22922(0x7ffdd151f700):ZOO_INFO@check_events@1750: session establishment complete on server [10.171.59.83:2181], sessionId=0x14b51bc8506039a, negotiated timeout=10000 *** Check failure stack trace: ***     @     0x7ffdd9a6596d  google::LogMessage::Fail() I0204 18:24:49.313356 22930 group.cpp:313] Group process (group(1)@10.169.146.67:5051) connected to ZooKeeper     @     0x7ffdd9a677ad  google::LogMessage::SendToLog() I0204 18:24:49.313786 22930 group.cpp:790] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0204 18:24:49.314487 22930 group.cpp:385] Trying to create path '/mesos' in ZooKeeper I0204 18:24:49.323668 22930 group.cpp:717] Found non-sequence node 'log_replicas' at '/mesos' in ZooKeeper I0204 18:24:49.323806 22930 detector.cpp:138] Detected a new leader: (id='1') I0204 18:24:49.323958 22930 group.cpp:659] Trying to get '/mesos/info_0000000001' in ZooKeeper I0204 18:24:49.324595 22930 detector.cpp:433] A new leading master (UPID=master@10.171.59.83:5050) is detected     @     0x7ffdd9a6555c  google::LogMessage::Flush()     @     0x7ffdd9a680a9  google::LogMessageFatal::~LogMessageFatal()     @     0x7ffdd94b7179  _CheckFatal::~_CheckFatal()     @     0x7ffdd96718e2  mesos::internal::slave::Slave::__recover()     @     0x7ffdd9a1524a  process::ProcessManager::resume()     @     0x7ffdd9a1550c  process::schedule()     @     0x7ffdd83832ad  (unknown)     @     0x7ffdd80b834d  (unknown) Aborted (core dumped) {code}  Removing the --work_dir option results in the slave starting successfully.",2
"MESOS-2332","Report per-container metrics for network bandwidth throttling","Export metrics from the network isolation to identify scope and duration of container throttling.    Packet loss can be identified from the overlimits and requeues fields of the htb qdisc report for the virtual interface, e.g.  {noformat} $ tc -s -d qdisc show dev mesos19223 qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1  Sent 158213287452 bytes 1030876393 pkt (dropped 0, overlimits 0 requeues 0)  backlog 0b 0p requeues 0 qdisc ingress ffff: parent ffff:fff1 ----------------  Sent 119381747824 bytes 1144549901 pkt (dropped 2044879, overlimits 0 requeues 0)  backlog 0b 0p requeues 0 {noformat}  Note that since a packet can be examined multiple times before transmission, overlimits can exceed total packets sent.    Add to the port_mapping isolator usage() and the container statistics protobuf. Carefully consider the naming (esp tx/rx) + commenting of the protobuf fields so it's clear what these represent and how they are different to the existing dropped packet counts from the network stack.",5
"MESOS-2347","Add ability for schedulers to explicitly acknowledge status updates on the driver.","In order for schedulers to be able to handle status updates in a scalable manner, they need the ability to send acknowledgements through the driver. This enables optimizations in schedulers (e.g. process status updates asynchronously w/o backing up the driver, processing/acking updates in batch).  Without this, an implicit reconciliation can overload a scheduler (hence the motivation for MESOS-2308).",8
"MESOS-2350","Add support for MesosContainerizerLaunch to chroot to a specified path","In preparation for the MesosContainerizer to support a filesystem isolator the MesosContainerizerLauncher must support chrooting. Optionally, it should also configure the chroot environment by (re-)mounting special filesystems such as /proc and /sys and making device nodes such as /dev/zero, etc., such that the chroot environment is functional.",5
"MESOS-2367","Improve slave resiliency in the face of orphan containers ","Right now there's a case where a misbehaving executor can cause a slave process to flap:  {panel:title=Quote From [~jieyu]} {quote} 1) User tries to kill an instance 2) Slave sends {{KillTaskMessage}} to executor 3) Executor sends kill signals to task processes 4) Executor sends {{TASK_KILLED}} to slave 5) Slave updates container cpu limit to be 0.01 cpus 6) A user-process is still processing the kill signal 7) the task process cannot exit since it has too little cpu share and is throttled 8) Executor itself terminates 9) Slave tries to destroy the container, but cannot because the user-process is stuck in the exit path. 10) Slave restarts, and is constantly flapping because it cannot kill orphan containers {quote} {panel}  The slave's orphan container handling should be improved to deal with this case despite ill-behaved users (framework writers).",5
"MESOS-2387","SlaveTest.TaskLaunchContainerizerUpdateFails is flaky","Observed on internal CI  {code} [ RUN      ] SlaveTest.TaskLaunchContainerizerUpdateFails Using temporary directory '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_tUjtcI' I0222 04:59:56.568491 21813 process.cpp:2117] Dropped / Lost event for PID: slave(52)@192.168.122.68:39461 I0222 04:59:56.595433 21791 leveldb.cpp:175] Opened db in 27.59732ms I0222 04:59:56.603965 21791 leveldb.cpp:182] Compacted db in 8.49192ms I0222 04:59:56.604019 21791 leveldb.cpp:197] Created db iterator in 19206ns I0222 04:59:56.604037 21791 leveldb.cpp:203] Seeked to beginning of db in 1802ns I0222 04:59:56.604046 21791 leveldb.cpp:272] Iterated through 0 keys in the db in 467ns I0222 04:59:56.604081 21791 replica.cpp:743] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0222 04:59:56.607413 21809 recover.cpp:448] Starting replica recovery I0222 04:59:56.607687 21809 recover.cpp:474] Replica is in 4 status I0222 04:59:56.609011 21809 replica.cpp:640] Replica in 4 status received a broadcasted recover request I0222 04:59:56.609262 21809 recover.cpp:194] Received a recover response from a replica in 4 status I0222 04:59:56.609709 21809 recover.cpp:565] Updating replica status to 3 I0222 04:59:56.610749 21811 master.cpp:347] Master 20150222-045956-1148889280-39461-21791 (centos-7) started on 192.168.122.68:39461 I0222 04:59:56.610791 21811 master.cpp:393] Master only allowing authenticated frameworks to register I0222 04:59:56.610802 21811 master.cpp:398] Master only allowing authenticated slaves to register I0222 04:59:56.610821 21811 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_tUjtcI/credentials' I0222 04:59:56.611042 21811 master.cpp:440] Authorization enabled I0222 04:59:56.612329 21811 hierarchical.hpp:286] Initialized hierarchical allocator process I0222 04:59:56.612416 21811 whitelist_watcher.cpp:78] No whitelist given I0222 04:59:56.613005 21811 master.cpp:1354] The newly elected leader is master@192.168.122.68:39461 with id 20150222-045956-1148889280-39461-21791 I0222 04:59:56.613034 21811 master.cpp:1367] Elected as the leading master! I0222 04:59:56.613050 21811 master.cpp:1185] Recovering from registrar I0222 04:59:56.613229 21811 registrar.cpp:312] Recovering registrar I0222 04:59:56.622866 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 12.988429ms I0222 04:59:56.622913 21809 replica.cpp:322] Persisted replica status to 3 I0222 04:59:56.623118 21809 recover.cpp:474] Replica is in 3 status I0222 04:59:56.624419 21809 replica.cpp:640] Replica in 3 status received a broadcasted recover request I0222 04:59:56.624685 21809 recover.cpp:194] Received a recover response from a replica in 3 status I0222 04:59:56.625200 21809 recover.cpp:565] Updating replica status to 1 I0222 04:59:56.635154 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 9.799671ms I0222 04:59:56.635197 21809 replica.cpp:322] Persisted replica status to 1 I0222 04:59:56.635296 21809 recover.cpp:579] Successfully joined the Paxos group I0222 04:59:56.635426 21809 recover.cpp:463] Recover process terminated I0222 04:59:56.635812 21809 log.cpp:659] Attempting to start the writer I0222 04:59:56.637075 21809 replica.cpp:476] Replica received implicit promise request with proposal 1 I0222 04:59:56.648674 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 11.566146ms I0222 04:59:56.648717 21809 replica.cpp:344] Persisted promised to 1 I0222 04:59:56.649456 21809 coordinator.cpp:229] Coordinator attemping to fill missing position I0222 04:59:56.650800 21809 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2 I0222 04:59:56.659916 21809 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 9.078258ms I0222 04:59:56.659981 21809 replica.cpp:678] Persisted action at 0 I0222 04:59:56.661075 21809 replica.cpp:510] Replica received write request for position 0 I0222 04:59:56.661129 21809 leveldb.cpp:437] Reading position from leveldb took 26387ns I0222 04:59:56.671227 21809 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 10.064302ms I0222 04:59:56.671262 21809 replica.cpp:678] Persisted action at 0 I0222 04:59:56.671821 21809 replica.cpp:657] Replica received learned notice for position 0 I0222 04:59:56.684200 21809 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 12.346897ms I0222 04:59:56.684242 21809 replica.cpp:678] Persisted action at 0 I0222 04:59:56.684262 21809 replica.cpp:663] Replica learned 1 action at position 0 I0222 04:59:56.684875 21809 log.cpp:675] Writer started with ending position 0 I0222 04:59:56.685932 21809 leveldb.cpp:437] Reading position from leveldb took 27308ns I0222 04:59:56.688256 21809 registrar.cpp:345] Successfully fetched the registry (0B) in 74.992128ms I0222 04:59:56.688344 21809 registrar.cpp:444] Applied 1 operations in 19566ns; attempting to update the 'registry' I0222 04:59:56.690690 21809 log.cpp:683] Attempting to append 129 bytes to the log I0222 04:59:56.690848 21809 coordinator.cpp:339] Coordinator attempting to write 2 action at position 1 I0222 04:59:56.691661 21809 replica.cpp:510] Replica received write request for position 1 I0222 04:59:56.701247 21809 leveldb.cpp:342] Persisting action (148 bytes) to leveldb took 9.550768ms I0222 04:59:56.701292 21809 replica.cpp:678] Persisted action at 1 I0222 04:59:56.702066 21809 replica.cpp:657] Replica received learned notice for position 1 I0222 04:59:56.712136 21809 leveldb.cpp:342] Persisting action (150 bytes) to leveldb took 10.041696ms I0222 04:59:56.712175 21809 replica.cpp:678] Persisted action at 1 I0222 04:59:56.712198 21809 replica.cpp:663] Replica learned 2 action at position 1 I0222 04:59:56.713289 21809 registrar.cpp:489] Successfully updated the 'registry' in 24.890112ms I0222 04:59:56.713397 21809 registrar.cpp:375] Successfully recovered registrar I0222 04:59:56.713537 21809 log.cpp:702] Attempting to truncate the log to 1 I0222 04:59:56.713795 21809 master.cpp:1212] Recovered 0 slaves from the Registry (93B) ; allowing 10mins for slaves to re-register I0222 04:59:56.713871 21809 coordinator.cpp:339] Coordinator attempting to write 3 action at position 2 I0222 04:59:56.714879 21809 replica.cpp:510] Replica received write request for position 2 I0222 04:59:56.725225 21809 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 10.311704ms I0222 04:59:56.725270 21809 replica.cpp:678] Persisted action at 2 I0222 04:59:56.726066 21809 replica.cpp:657] Replica received learned notice for position 2 I0222 04:59:56.734110 21809 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 8.012327ms I0222 04:59:56.734180 21809 leveldb.cpp:400] Deleting ~1 keys from leveldb took 36578ns I0222 04:59:56.734201 21809 replica.cpp:678] Persisted action at 2 I0222 04:59:56.734221 21809 replica.cpp:663] Replica learned 3 action at position 2 I0222 04:59:56.747556 21809 slave.cpp:173] Slave started on 53)@192.168.122.68:39461 I0222 04:59:56.747601 21809 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/credential' I0222 04:59:56.747774 21809 slave.cpp:280] Slave using credential for: test-principal I0222 04:59:56.748021 21809 slave.cpp:298] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0222 04:59:56.748682 21809 slave.cpp:327] Slave hostname: centos-7 I0222 04:59:56.748705 21809 slave.cpp:328] Slave checkpoint: false W0222 04:59:56.748714 21809 slave.cpp:330] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0222 04:59:56.749826 21809 state.cpp:34] Recovering state from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/meta' I0222 04:59:56.750191 21809 status_update_manager.cpp:196] Recovering status update manager I0222 04:59:56.750465 21809 slave.cpp:3775] Finished recovery I0222 04:59:56.751260 21809 slave.cpp:623] New master detected at master@192.168.122.68:39461 I0222 04:59:56.751349 21809 slave.cpp:686] Authenticating with master master@192.168.122.68:39461 I0222 04:59:56.751369 21809 slave.cpp:691] Using default CRAM-MD5 authenticatee I0222 04:59:56.751502 21809 slave.cpp:659] Detecting new master I0222 04:59:56.751596 21809 status_update_manager.cpp:170] Pausing sending status updates I0222 04:59:56.751668 21809 authenticatee.hpp:138] Creating new client SASL connection I0222 04:59:56.752781 21809 master.cpp:3811] Authenticating slave(53)@192.168.122.68:39461 I0222 04:59:56.752820 21809 master.cpp:3822] Using default CRAM-MD5 authenticator I0222 04:59:56.753124 21809 authenticator.hpp:169] Creating new server SASL connection I0222 04:59:56.755609 21809 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0222 04:59:56.755641 21809 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0222 04:59:56.755708 21809 authenticator.hpp:275] Received SASL authentication start I0222 04:59:56.755751 21809 authenticator.hpp:397] Authentication requires more steps I0222 04:59:56.755813 21809 authenticatee.hpp:275] Received SASL authentication step I0222 04:59:56.755887 21809 authenticator.hpp:303] Received SASL authentication step I0222 04:59:56.755920 21809 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0222 04:59:56.755934 21809 auxprop.cpp:170] Looking up auxiliary property '*userPassword' I0222 04:59:56.756005 21809 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0222 04:59:56.756036 21809 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0222 04:59:56.756047 21809 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0222 04:59:56.756054 21809 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0222 04:59:56.756068 21809 authenticator.hpp:389] Authentication success I0222 04:59:56.756155 21809 authenticatee.hpp:315] Authentication success I0222 04:59:56.756219 21809 master.cpp:3869] Successfully authenticated principal 'test-principal' at slave(53)@192.168.122.68:39461 I0222 04:59:56.756503 21809 slave.cpp:757] Successfully authenticated with master master@192.168.122.68:39461 I0222 04:59:56.756611 21809 slave.cpp:1089] Will retry registration in 11.221976ms if necessary I0222 04:59:56.756876 21809 master.cpp:2936] Registering slave at slave(53)@192.168.122.68:39461 (centos-7) with id 20150222-045956-1148889280-39461-21791-S0 I0222 04:59:56.757323 21809 registrar.cpp:444] Applied 1 operations in 70787ns; attempting to update the 'registry' I0222 04:59:56.759790 21809 log.cpp:683] Attempting to append 299 bytes to the log I0222 04:59:56.760000 21809 coordinator.cpp:339] Coordinator attempting to write 2 action at position 3 I0222 04:59:56.760920 21809 replica.cpp:510] Replica received write request for position 3 I0222 04:59:56.762037 21791 sched.cpp:154] Version: 0.22.0 I0222 04:59:56.762763 21806 sched.cpp:251] New master detected at master@192.168.122.68:39461 I0222 04:59:56.762835 21806 sched.cpp:307] Authenticating with master master@192.168.122.68:39461 I0222 04:59:56.762856 21806 sched.cpp:314] Using default CRAM-MD5 authenticatee I0222 04:59:56.763082 21806 authenticatee.hpp:138] Creating new client SASL connection I0222 04:59:56.763753 21806 master.cpp:3811] Authenticating scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 I0222 04:59:56.763784 21806 master.cpp:3822] Using default CRAM-MD5 authenticator I0222 04:59:56.764040 21806 authenticator.hpp:169] Creating new server SASL connection I0222 04:59:56.764624 21806 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5 I0222 04:59:56.764653 21806 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5' I0222 04:59:56.764719 21806 authenticator.hpp:275] Received SASL authentication start I0222 04:59:56.764758 21806 authenticator.hpp:397] Authentication requires more steps I0222 04:59:56.764819 21806 authenticatee.hpp:275] Received SASL authentication step I0222 04:59:56.764889 21806 authenticator.hpp:303] Received SASL authentication step I0222 04:59:56.764911 21806 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0222 04:59:56.764922 21806 auxprop.cpp:170] Looking up auxiliary property '*userPassword' I0222 04:59:56.764974 21806 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0222 04:59:56.765005 21806 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0222 04:59:56.765017 21806 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0222 04:59:56.765023 21806 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0222 04:59:56.765036 21806 authenticator.hpp:389] Authentication success I0222 04:59:56.765120 21806 authenticatee.hpp:315] Authentication success I0222 04:59:56.765182 21806 master.cpp:3869] Successfully authenticated principal 'test-principal' at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 I0222 04:59:56.765442 21806 sched.cpp:395] Successfully authenticated with master master@192.168.122.68:39461 I0222 04:59:56.765465 21806 sched.cpp:518] Sending registration request to master@192.168.122.68:39461 I0222 04:59:56.765522 21806 sched.cpp:551] Will retry registration in 1.283564292secs if necessary I0222 04:59:56.765637 21806 master.cpp:1572] Received registration request for framework 'default' at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 I0222 04:59:56.765699 21806 master.cpp:1433] Authorizing framework principal 'test-principal' to receive offers for role '*' I0222 04:59:56.766120 21806 master.cpp:1636] Registering framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 I0222 04:59:56.766572 21806 hierarchical.hpp:320] Added framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.766598 21806 hierarchical.hpp:831] No resources available to allocate! I0222 04:59:56.766609 21806 hierarchical.hpp:738] Performed allocation for 0 slaves in 15902ns I0222 04:59:56.766753 21806 sched.cpp:445] Framework registered with 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.766790 21806 sched.cpp:459] Scheduler::registered took 15076ns I0222 04:59:56.773710 21806 slave.cpp:1089] Will retry registration in 3.454005ms if necessary I0222 04:59:56.773900 21806 master.cpp:2924] Ignoring register slave message from slave(53)@192.168.122.68:39461 (centos-7) as admission is already in progress I0222 04:59:56.775297 21809 leveldb.cpp:342] Persisting action (318 bytes) to leveldb took 14.319807ms I0222 04:59:56.775344 21809 replica.cpp:678] Persisted action at 3 I0222 04:59:56.776139 21809 replica.cpp:657] Replica received learned notice for position 3 I0222 04:59:56.778630 21806 slave.cpp:1089] Will retry registration in 32.764468ms if necessary I0222 04:59:56.778779 21806 master.cpp:2924] Ignoring register slave message from slave(53)@192.168.122.68:39461 (centos-7) as admission is already in progress I0222 04:59:56.783778 21809 leveldb.cpp:342] Persisting action (320 bytes) to leveldb took 7.609533ms I0222 04:59:56.783828 21809 replica.cpp:678] Persisted action at 3 I0222 04:59:56.783849 21809 replica.cpp:663] Replica learned 2 action at position 3 I0222 04:59:56.785058 21809 registrar.cpp:489] Successfully updated the 'registry' in 27.669248ms I0222 04:59:56.785274 21809 log.cpp:702] Attempting to truncate the log to 3 I0222 04:59:56.785815 21809 master.cpp:2993] Registered slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0222 04:59:56.785913 21809 coordinator.cpp:339] Coordinator attempting to write 3 action at position 4 I0222 04:59:56.786267 21809 hierarchical.hpp:452] Added slave 20150222-045956-1148889280-39461-21791-S0 (centos-7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0222 04:59:56.786600 21809 hierarchical.hpp:756] Performed allocation for slave 20150222-045956-1148889280-39461-21791-S0 in 292298ns I0222 04:59:56.786684 21809 slave.cpp:791] Registered with master master@192.168.122.68:39461; given slave ID 20150222-045956-1148889280-39461-21791-S0 I0222 04:59:56.786792 21809 slave.cpp:2830] Received ping from slave-observer(52)@192.168.122.68:39461 I0222 04:59:56.787230 21809 master.cpp:3753] Sending 1 offers to framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 I0222 04:59:56.787334 21809 status_update_manager.cpp:177] Resuming sending status updates I0222 04:59:56.788156 21809 sched.cpp:608] Scheduler::resourceOffers took 557128ns I0222 04:59:56.788936 21809 master.cpp:2266] Processing ACCEPT call for offers: [ 20150222-045956-1148889280-39461-21791-O0 ] on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) for framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 I0222 04:59:56.789000 21809 master.cpp:2110] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' W0222 04:59:56.790506 21809 validation.cpp:327] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0222 04:59:56.790546 21809 validation.cpp:339] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0222 04:59:56.790808 21809 master.hpp:821] Adding task 0 with resources cpus(*):1; mem(*):128 on slave 20150222-045956-1148889280-39461-21791-S0 (centos-7) I0222 04:59:56.790885 21809 master.cpp:2543] Launching task 0 of framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 with resources cpus(*):1; mem(*):128 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) I0222 04:59:56.791201 21809 replica.cpp:510] Replica received write request for position 4 I0222 04:59:56.791610 21806 slave.cpp:1120] Got assigned task 0 for framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.792140 21806 slave.cpp:1230] Launching task 0 for framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.794872 21806 slave.cpp:4177] Launching executor default of framework 20150222-045956-1148889280-39461-21791-0000 in work directory '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default/runs/753232b5-43ff-4fbf-b29a-0f76161132ab' I0222 04:59:56.796846 21806 exec.cpp:130] Version: 0.22.0 I0222 04:59:56.797173 21806 slave.cpp:1377] Queuing task '0' for executor default of framework '20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.797355 21806 slave.cpp:3132] Monitoring executor 'default' of framework '20150222-045956-1148889280-39461-21791-0000' in container '753232b5-43ff-4fbf-b29a-0f76161132ab' I0222 04:59:56.797570 21806 hierarchical.hpp:645] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000]) on slave 20150222-045956-1148889280-39461-21791-S0 from framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.797613 21806 hierarchical.hpp:681] Framework 20150222-045956-1148889280-39461-21791-0000 filtered slave 20150222-045956-1148889280-39461-21791-S0 for 5secs I0222 04:59:56.797796 21806 exec.cpp:180] Executor started at: executor(24)@192.168.122.68:39461 with pid 21791 I0222 04:59:56.798068 21806 slave.cpp:576] Successfully attached file '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default/runs/753232b5-43ff-4fbf-b29a-0f76161132ab' I0222 04:59:56.798136 21806 slave.cpp:2140] Got registration for executor 'default' of framework 20150222-045956-1148889280-39461-21791-0000 from executor(24)@192.168.122.68:39461 E0222 04:59:56.798573 21806 slave.cpp:1445] Failed to update resources for container 753232b5-43ff-4fbf-b29a-0f76161132ab of executor 'default' of framework 20150222-045956-1148889280-39461-21791-0000, destroying container: update() failed I0222 04:59:56.798811 21806 slave.cpp:3190] Executor 'default' of framework 20150222-045956-1148889280-39461-21791-0000 exited with status 0 I0222 04:59:56.800436 21806 slave.cpp:2507] Handling status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 from @0.0.0.0:0 I0222 04:59:56.800520 21806 slave.cpp:4485] Terminating task 0 I0222 04:59:56.801142 21806 master.cpp:3386] Executor default of framework 20150222-045956-1148889280-39461-21791-0000 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) exited with status 0 I0222 04:59:56.801211 21806 master.cpp:4712] Removing executor 'default' with resources  of framework 20150222-045956-1148889280-39461-21791-0000 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) I0222 04:59:56.801378 21806 status_update_manager.cpp:316] Received status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.801412 21806 status_update_manager.cpp:493] Creating StatusUpdate stream for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.801574 21806 status_update_manager.cpp:370] Forwarding update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 to the slave I0222 04:59:56.801831 21806 slave.cpp:2750] Forwarding the update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 to master@192.168.122.68:39461 I0222 04:59:56.802109 21805 master.cpp:3293] Status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 from slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) I0222 04:59:56.802145 21805 master.cpp:3334] Forwarding status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.802266 21805 master.cpp:4616] Updating the latest state of task 0 of framework 20150222-045956-1148889280-39461-21791-0000 to TASK_LOST I0222 04:59:56.802685 21805 sched.cpp:714] Scheduler::statusUpdate took 40465ns I0222 04:59:56.802821 21805 hierarchical.hpp:645] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150222-045956-1148889280-39461-21791-S0 from framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.803130 21805 master.cpp:4683] Removing task 0 with resources cpus(*):1; mem(*):128 of framework 20150222-045956-1148889280-39461-21791-0000 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) I0222 04:59:56.803268 21805 master.cpp:2780] Forwarding status update acknowledgement 45243922-bcad-4e11-9a9f-db9213111a2a for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 to slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) I0222 04:59:56.803473 21791 sched.cpp:1585] Asked to stop the driver I0222 04:59:56.803547 21791 master.cpp:785] Master terminating I0222 04:59:56.804844 21791 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.68:39461 I0222 04:59:56.804921 21791 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.68:39461 I0222 04:59:56.805624 21812 sched.cpp:828] Stopping framework '20150222-045956-1148889280-39461-21791-0000' I0222 04:59:56.805675 21812 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.68:39461 I0222 04:59:56.807793 21812 process.cpp:2117] Dropped / Lost event for PID: log-coordinator(89)@192.168.122.68:39461 I0222 04:59:56.809552 21806 slave.cpp:2677] Status update manager successfully handled status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.809736 21806 slave.cpp:2915] master@192.168.122.68:39461 exited W0222 04:59:56.809759 21806 slave.cpp:2918] Master disconnected! Waiting for a new master to be elected I0222 04:59:56.809788 21806 status_update_manager.cpp:388] Received status update acknowledgement (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.809855 21806 status_update_manager.cpp:524] Cleaning up status update stream for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.810042 21806 slave.cpp:2080] Status update manager successfully handled status update acknowledgement (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.810088 21806 slave.cpp:4526] Completing task 0 I0222 04:59:56.810117 21806 slave.cpp:3299] Cleaning up executor 'default' of framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.810361 21806 slave.cpp:3378] Cleaning up framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.810509 21806 gc.cpp:55] Scheduling '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default/runs/753232b5-43ff-4fbf-b29a-0f76161132ab' for gc 6.99999062248889days in the future I0222 04:59:56.810673 21806 gc.cpp:55] Scheduling '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default' for gc 6.99999062130963days in the future I0222 04:59:56.810768 21806 gc.cpp:55] Scheduling '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000' for gc 6.9999906199763days in the future I0222 04:59:56.810861 21806 status_update_manager.cpp:278] Closing status update streams for framework 20150222-045956-1148889280-39461-21791-0000 I0222 04:59:56.817010 21809 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 25.747365ms I0222 04:59:56.817047 21809 replica.cpp:678] Persisted action at 4 I0222 04:59:56.817087 21809 process.cpp:2117] Dropped / Lost event for PID: (1371)@192.168.122.68:39461 I0222 04:59:56.817679 21791 slave.cpp:505] Slave terminating I0222 04:59:56.818411 21791 process.cpp:2117] Dropped / Lost event for PID: slave(53)@192.168.122.68:39461 I0222 04:59:56.818869 21791 process.cpp:2117] Dropped / Lost event for PID: scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 tests/slave_tests.cpp:1183: Failure Actual function call count doesn't match EXPECT_CALL(exec, registered(_, _, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] SlaveTest.TaskLaunchContainerizerUpdateFails (253 ms)  {code}",1
"MESOS-2401","MasterTest.ShutdownFrameworkWhileTaskRunning is flaky","Looks like the executorShutdownTimeout() was called immediately after executorShutdown() was called!  {code} [ RUN      ] MasterTest.ShutdownFrameworkWhileTaskRunning Using temporary directory '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_sBd6vK' I0224 18:51:17.385068 30213 leveldb.cpp:176] Opened db in 1.262442ms I0224 18:51:17.386360 30213 leveldb.cpp:183] Compacted db in 985102ns I0224 18:51:17.387025 30213 leveldb.cpp:198] Created db iterator in 78043ns I0224 18:51:17.387420 30213 leveldb.cpp:204] Seeked to beginning of db in 25814ns I0224 18:51:17.387804 30213 leveldb.cpp:273] Iterated through 0 keys in the db in 25025ns I0224 18:51:17.388270 30213 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0224 18:51:17.389760 30227 recover.cpp:449] Starting replica recovery I0224 18:51:17.395699 30227 recover.cpp:475] Replica is in 4 status I0224 18:51:17.398294 30227 replica.cpp:641] Replica in 4 status received a broadcasted recover request I0224 18:51:17.398816 30227 recover.cpp:195] Received a recover response from a replica in 4 status I0224 18:51:17.402415 30230 recover.cpp:566] Updating replica status to 3 I0224 18:51:17.403473 30229 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 273857ns I0224 18:51:17.404093 30229 replica.cpp:323] Persisted replica status to 3 I0224 18:51:17.404930 30229 recover.cpp:475] Replica is in 3 status I0224 18:51:17.407995 30233 replica.cpp:641] Replica in 3 status received a broadcasted recover request I0224 18:51:17.410697 30231 recover.cpp:195] Received a recover response from a replica in 3 status I0224 18:51:17.415710 30230 recover.cpp:566] Updating replica status to 1 I0224 18:51:17.416987 30227 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 221966ns I0224 18:51:17.417579 30227 replica.cpp:323] Persisted replica status to 1 I0224 18:51:17.418803 30234 recover.cpp:580] Successfully joined the Paxos group I0224 18:51:17.419699 30227 recover.cpp:464] Recover process terminated I0224 18:51:17.430594 30234 master.cpp:349] Master 20150224-185117-2272962752-44950-30213 (fedora-19) started on 192.168.122.135:44950 I0224 18:51:17.431082 30234 master.cpp:395] Master only allowing authenticated frameworks to register I0224 18:51:17.431453 30234 master.cpp:400] Master only allowing authenticated slaves to register I0224 18:51:17.431828 30234 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_sBd6vK/credentials' I0224 18:51:17.432740 30234 master.cpp:442] Authorization enabled I0224 18:51:17.434224 30229 hierarchical.hpp:287] Initialized hierarchical allocator process I0224 18:51:17.434994 30233 whitelist_watcher.cpp:79] No whitelist given I0224 18:51:17.440687 30234 master.cpp:1356] The newly elected leader is master@192.168.122.135:44950 with id 20150224-185117-2272962752-44950-30213 I0224 18:51:17.441764 30234 master.cpp:1369] Elected as the leading master! I0224 18:51:17.442430 30234 master.cpp:1187] Recovering from registrar I0224 18:51:17.443053 30229 registrar.cpp:313] Recovering registrar I0224 18:51:17.445468 30228 log.cpp:660] Attempting to start the writer I0224 18:51:17.449970 30233 replica.cpp:477] Replica received implicit promise request with proposal 1 I0224 18:51:17.451359 30233 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 339488ns I0224 18:51:17.451949 30233 replica.cpp:345] Persisted promised to 1 I0224 18:51:17.456845 30235 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(154)@192.168.122.135:44950 I0224 18:51:17.461741 30231 coordinator.cpp:230] Coordinator attemping to fill missing position I0224 18:51:17.464686 30228 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0224 18:51:17.465515 30228 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 170261ns I0224 18:51:17.465991 30228 replica.cpp:679] Persisted action at 0 I0224 18:51:17.470512 30229 replica.cpp:511] Replica received write request for position 0 I0224 18:51:17.471437 30229 leveldb.cpp:438] Reading position from leveldb took 139178ns I0224 18:51:17.472129 30229 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 141560ns I0224 18:51:17.472705 30229 replica.cpp:679] Persisted action at 0 I0224 18:51:17.476305 30228 replica.cpp:658] Replica received learned notice for position 0 I0224 18:51:17.477991 30228 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 208112ns I0224 18:51:17.478574 30228 replica.cpp:679] Persisted action at 0 I0224 18:51:17.479044 30228 replica.cpp:664] Replica learned 1 action at position 0 I0224 18:51:17.484371 30233 log.cpp:676] Writer started with ending position 0 I0224 18:51:17.487396 30233 leveldb.cpp:438] Reading position from leveldb took 96498ns I0224 18:51:17.498906 30233 registrar.cpp:346] Successfully fetched the registry (0B) in 55.234048ms I0224 18:51:17.499781 30233 registrar.cpp:445] Applied 1 operations in 97308ns; attempting to update the 'registry' I0224 18:51:17.503955 30231 log.cpp:684] Attempting to append 131 bytes to the log I0224 18:51:17.505009 30231 coordinator.cpp:340] Coordinator attempting to write 2 action at position 1 I0224 18:51:17.507428 30228 replica.cpp:511] Replica received write request for position 1 I0224 18:51:17.508517 30228 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 316570ns I0224 18:51:17.508985 30228 replica.cpp:679] Persisted action at 1 I0224 18:51:17.512902 30229 replica.cpp:658] Replica received learned notice for position 1 I0224 18:51:17.517261 30229 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 427860ns I0224 18:51:17.517470 30229 replica.cpp:679] Persisted action at 1 I0224 18:51:17.517796 30229 replica.cpp:664] Replica learned 2 action at position 1 I0224 18:51:17.532624 30232 registrar.cpp:490] Successfully updated the 'registry' in 32.31104ms I0224 18:51:17.533957 30228 log.cpp:703] Attempting to truncate the log to 1 I0224 18:51:17.534366 30228 coordinator.cpp:340] Coordinator attempting to write 3 action at position 2 I0224 18:51:17.536684 30227 replica.cpp:511] Replica received write request for position 2 I0224 18:51:17.537406 30227 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 196455ns I0224 18:51:17.537946 30227 replica.cpp:679] Persisted action at 2 I0224 18:51:17.537695 30232 registrar.cpp:376] Successfully recovered registrar I0224 18:51:17.544136 30231 master.cpp:1214] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register I0224 18:51:17.546041 30227 replica.cpp:658] Replica received learned notice for position 2 I0224 18:51:17.546728 30227 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 192442ns I0224 18:51:17.547058 30227 leveldb.cpp:401] Deleting ~1 keys from leveldb took 61064ns I0224 18:51:17.547363 30227 replica.cpp:679] Persisted action at 2 I0224 18:51:17.547669 30227 replica.cpp:664] Replica learned 3 action at position 2 I0224 18:51:17.565460 30234 slave.cpp:174] Slave started on 138)@192.168.122.135:44950 I0224 18:51:17.566038 30234 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/credential' I0224 18:51:17.566584 30234 slave.cpp:281] Slave using credential for: test-principal I0224 18:51:17.567198 30234 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0224 18:51:17.567930 30234 slave.cpp:328] Slave hostname: fedora-19 I0224 18:51:17.568172 30234 slave.cpp:329] Slave checkpoint: false W0224 18:51:17.568435 30234 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0224 18:51:17.570539 30227 state.cpp:35] Recovering state from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/meta' I0224 18:51:17.573499 30232 status_update_manager.cpp:197] Recovering status update manager I0224 18:51:17.574209 30234 slave.cpp:3775] Finished recovery I0224 18:51:17.576277 30229 status_update_manager.cpp:171] Pausing sending status updates I0224 18:51:17.576680 30234 slave.cpp:624] New master detected at master@192.168.122.135:44950 I0224 18:51:17.577131 30234 slave.cpp:687] Authenticating with master master@192.168.122.135:44950 I0224 18:51:17.577385 30234 slave.cpp:692] Using default CRAM-MD5 authenticatee I0224 18:51:17.577945 30228 authenticatee.hpp:139] Creating new client SASL connection I0224 18:51:17.578837 30234 slave.cpp:660] Detecting new master I0224 18:51:17.579270 30228 master.cpp:3813] Authenticating slave(138)@192.168.122.135:44950 I0224 18:51:17.579900 30228 master.cpp:3824] Using default CRAM-MD5 authenticator I0224 18:51:17.580572 30228 authenticator.hpp:170] Creating new server SASL connection I0224 18:51:17.581501 30231 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5 I0224 18:51:17.581805 30231 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5' I0224 18:51:17.582222 30228 authenticator.hpp:276] Received SASL authentication start I0224 18:51:17.582531 30228 authenticator.hpp:398] Authentication requires more steps I0224 18:51:17.582945 30230 authenticatee.hpp:276] Received SASL authentication step I0224 18:51:17.583351 30228 authenticator.hpp:304] Received SASL authentication step I0224 18:51:17.583643 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0224 18:51:17.583911 30228 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0224 18:51:17.584241 30228 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0224 18:51:17.584517 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0224 18:51:17.584787 30228 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0224 18:51:17.585075 30228 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0224 18:51:17.585358 30228 authenticator.hpp:390] Authentication success I0224 18:51:17.585750 30233 authenticatee.hpp:316] Authentication success I0224 18:51:17.586354 30232 master.cpp:3871] Successfully authenticated principal 'test-principal' at slave(138)@192.168.122.135:44950 I0224 18:51:17.590953 30234 slave.cpp:758] Successfully authenticated with master master@192.168.122.135:44950 I0224 18:51:17.591686 30233 master.cpp:2938] Registering slave at slave(138)@192.168.122.135:44950 (fedora-19) with id 20150224-185117-2272962752-44950-30213-S0 I0224 18:51:17.592718 30233 registrar.cpp:445] Applied 1 operations in 100358ns; attempting to update the 'registry' I0224 18:51:17.595989 30227 log.cpp:684] Attempting to append 302 bytes to the log I0224 18:51:17.596757 30227 coordinator.cpp:340] Coordinator attempting to write 2 action at position 3 I0224 18:51:17.599280 30227 replica.cpp:511] Replica received write request for position 3 I0224 18:51:17.599481 30234 slave.cpp:1090] Will retry registration in 12.331173ms if necessary I0224 18:51:17.601940 30227 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 999045ns I0224 18:51:17.602339 30227 replica.cpp:679] Persisted action at 3 I0224 18:51:17.612349 30229 replica.cpp:658] Replica received learned notice for position 3 I0224 18:51:17.612934 30229 leveldb.cpp:343] Persisting action (323 bytes) to leveldb took 152139ns I0224 18:51:17.613471 30229 replica.cpp:679] Persisted action at 3 I0224 18:51:17.613796 30229 replica.cpp:664] Replica learned 2 action at position 3 I0224 18:51:17.615980 30229 master.cpp:2926] Ignoring register slave message from slave(138)@192.168.122.135:44950 (fedora-19) as admission is already in progress I0224 18:51:17.614302 30233 slave.cpp:1090] Will retry registration in 11.014835ms if necessary I0224 18:51:17.617490 30234 registrar.cpp:490] Successfully updated the 'registry' in 24.179968ms I0224 18:51:17.618989 30234 master.cpp:2995] Registered slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0224 18:51:17.619567 30233 hierarchical.hpp:455] Added slave 20150224-185117-2272962752-44950-30213-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0224 18:51:17.621080 30233 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:17.621441 30233 hierarchical.hpp:759] Performed allocation for slave 20150224-185117-2272962752-44950-30213-S0 in 544608ns I0224 18:51:17.619704 30229 slave.cpp:792] Registered with master master@192.168.122.135:44950; given slave ID 20150224-185117-2272962752-44950-30213-S0 I0224 18:51:17.622195 30229 slave.cpp:2830] Received ping from slave-observer(125)@192.168.122.135:44950 I0224 18:51:17.622385 30227 status_update_manager.cpp:178] Resuming sending status updates I0224 18:51:17.620266 30232 log.cpp:703] Attempting to truncate the log to 3 I0224 18:51:17.623522 30232 coordinator.cpp:340] Coordinator attempting to write 3 action at position 4 I0224 18:51:17.624835 30229 replica.cpp:511] Replica received write request for position 4 I0224 18:51:17.625727 30229 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 259831ns I0224 18:51:17.626122 30229 replica.cpp:679] Persisted action at 4 I0224 18:51:17.627686 30227 replica.cpp:658] Replica received learned notice for position 4 I0224 18:51:17.628228 30227 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 93777ns I0224 18:51:17.628785 30227 leveldb.cpp:401] Deleting ~2 keys from leveldb took 57660ns I0224 18:51:17.629176 30227 replica.cpp:679] Persisted action at 4 I0224 18:51:17.629443 30227 replica.cpp:664] Replica learned 3 action at position 4 I0224 18:51:17.636715 30213 sched.cpp:157] Version: 0.23.0 I0224 18:51:17.638003 30229 sched.cpp:254] New master detected at master@192.168.122.135:44950 I0224 18:51:17.638602 30229 sched.cpp:310] Authenticating with master master@192.168.122.135:44950 I0224 18:51:17.639024 30229 sched.cpp:317] Using default CRAM-MD5 authenticatee I0224 18:51:17.639580 30228 authenticatee.hpp:139] Creating new client SASL connection I0224 18:51:17.640455 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950 I0224 18:51:17.641150 30228 master.cpp:3813] Authenticating scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:17.641597 30228 master.cpp:3824] Using default CRAM-MD5 authenticator I0224 18:51:17.642643 30228 authenticator.hpp:170] Creating new server SASL connection I0224 18:51:17.643698 30234 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5 I0224 18:51:17.644296 30234 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5' I0224 18:51:17.644739 30228 authenticator.hpp:276] Received SASL authentication start I0224 18:51:17.645143 30228 authenticator.hpp:398] Authentication requires more steps I0224 18:51:17.645654 30230 authenticatee.hpp:276] Received SASL authentication step I0224 18:51:17.646122 30228 authenticator.hpp:304] Received SASL authentication step I0224 18:51:17.646421 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0224 18:51:17.646746 30228 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0224 18:51:17.647203 30228 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0224 18:51:17.647644 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0224 18:51:17.648454 30228 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0224 18:51:17.648788 30228 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0224 18:51:17.649210 30228 authenticator.hpp:390] Authentication success I0224 18:51:17.649705 30231 authenticatee.hpp:316] Authentication success I0224 18:51:17.653314 30231 sched.cpp:398] Successfully authenticated with master master@192.168.122.135:44950 I0224 18:51:17.653766 30232 master.cpp:3871] Successfully authenticated principal 'test-principal' at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:17.654683 30231 sched.cpp:521] Sending registration request to master@192.168.122.135:44950 I0224 18:51:17.655138 30231 sched.cpp:554] Will retry registration in 1.028970132secs if necessary I0224 18:51:17.657112 30232 master.cpp:1574] Received registration request for framework 'default' at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:17.658509 30232 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*' I0224 18:51:17.659765 30232 master.cpp:1638] Registering framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:17.660727 30233 hierarchical.hpp:321] Added framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.661730 30233 hierarchical.hpp:741] Performed allocation for 1 slaves in 529369ns I0224 18:51:17.662911 30229 sched.cpp:448] Framework registered with 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.663374 30229 sched.cpp:462] Scheduler::registered took 35637ns I0224 18:51:17.664552 30232 master.cpp:3755] Sending 1 offers to framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:17.668009 30234 sched.cpp:611] Scheduler::resourceOffers took 2.574292ms I0224 18:51:17.671038 30232 master.cpp:2268] Processing ACCEPT call for offers: [ 20150224-185117-2272962752-44950-30213-O0 ] on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) for framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:17.672071 30232 master.cpp:2112] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins' W0224 18:51:17.674675 30232 validation.cpp:326] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0224 18:51:17.675395 30232 validation.cpp:338] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0224 18:51:17.676460 30232 master.hpp:822] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150224-185117-2272962752-44950-30213-S0 (fedora-19) I0224 18:51:17.677078 30232 master.cpp:2545] Launching task 1 of framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) I0224 18:51:17.678084 30230 slave.cpp:1121] Got assigned task 1 for framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.680057 30230 slave.cpp:1231] Launching task 1 for framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.684798 30230 slave.cpp:4177] Launching executor default of framework 20150224-185117-2272962752-44950-30213-0000 in work directory '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000/executors/default/runs/675638b4-5214-449d-96d8-c50551496152' I0224 18:51:17.688701 30230 exec.cpp:132] Version: 0.23.0 I0224 18:51:17.689615 30234 exec.cpp:182] Executor started at: executor(41)@192.168.122.135:44950 with pid 30213 I0224 18:51:17.690659 30230 slave.cpp:1379] Queuing task '1' for executor default of framework '20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.691382 30230 slave.cpp:577] Successfully attached file '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000/executors/default/runs/675638b4-5214-449d-96d8-c50551496152' I0224 18:51:17.691813 30230 slave.cpp:2140] Got registration for executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 from executor(41)@192.168.122.135:44950 I0224 18:51:17.692772 30231 exec.cpp:206] Executor registered on slave 20150224-185117-2272962752-44950-30213-S0 I0224 18:51:17.695121 30231 exec.cpp:218] Executor::registered took 80811ns I0224 18:51:17.697582 30230 slave.cpp:3132] Monitoring executor 'default' of framework '20150224-185117-2272962752-44950-30213-0000' in container '675638b4-5214-449d-96d8-c50551496152' I0224 18:51:17.699354 30230 slave.cpp:1533] Sending queued task '1' to executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.700932 30227 exec.cpp:293] Executor asked to run task '1' I0224 18:51:17.701679 30227 exec.cpp:302] Executor::launchTask took 140355ns I0224 18:51:17.705504 30227 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.707149 30228 slave.cpp:2507] Handling status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 from executor(41)@192.168.122.135:44950 I0224 18:51:17.708539 30228 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.709377 30228 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.710360 30228 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to the slave I0224 18:51:17.711405 30233 slave.cpp:2750] Forwarding the update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to master@192.168.122.135:44950 I0224 18:51:17.712425 30233 master.cpp:3295] Status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 from slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) I0224 18:51:17.713047 30233 master.cpp:3336] Forwarding status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.713930 30233 master.cpp:4615] Updating the latest state of task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to TASK_RUNNING I0224 18:51:17.714588 30232 sched.cpp:717] Scheduler::statusUpdate took 118286ns I0224 18:51:17.715512 30213 sched.cpp:1589] Asked to stop the driver I0224 18:51:17.718159 30232 master.cpp:2782] Forwarding status update acknowledgement 57877913-d602-445c-a0d9-87fc71e8eca5 for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 to slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) I0224 18:51:17.718691 30234 sched.cpp:831] Stopping framework '20150224-185117-2272962752-44950-30213-0000' I0224 18:51:17.721380 30232 master.cpp:1898] Asked to unregister framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.722920 30232 master.cpp:4183] Removing framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:17.725231 30231 hierarchical.hpp:400] Deactivated framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.725734 30229 slave.cpp:1746] Asked to shut down framework 20150224-185117-2272962752-44950-30213-0000 by master@192.168.122.135:44950 I0224 18:51:17.726658 30229 slave.cpp:1771] Shutting down framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.727322 30229 slave.cpp:3440] Shutting down executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.728742 30228 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.729042 30231 slave.cpp:2677] Status update manager successfully handled status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.730578 30231 slave.cpp:2683] Sending acknowledgement for status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to executor(41)@192.168.122.135:44950 I0224 18:51:17.731300 30231 slave.cpp:3510] Killing executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.733461 30232 master.cpp:4615] Updating the latest state of task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to TASK_KILLED I0224 18:51:17.734503 30231 slave.cpp:3190] Executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 exited with status 0 I0224 18:51:17.736177 30231 slave.cpp:3299] Cleaning up executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.737277 30233 gc.cpp:56] Scheduling '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000/executors/default/runs/675638b4-5214-449d-96d8-c50551496152' for gc 6.99999146853037days in the future I0224 18:51:17.738636 30231 slave.cpp:3378] Cleaning up framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.739148 30228 gc.cpp:56] Scheduling '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000/executors/default' for gc 6.99999145243259days in the future I0224 18:51:17.740373 30228 status_update_manager.cpp:279] Closing status update streams for framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.741170 30228 status_update_manager.cpp:525] Cleaning up status update stream for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.742255 30229 gc.cpp:56] Scheduling '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000' for gc 6.99999141055704days in the future I0224 18:51:17.743207 30231 slave.cpp:2080] Status update manager successfully handled status update acknowledgement (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 E0224 18:51:17.743799 30231 slave.cpp:2091] Status update acknowledgement (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of unknown framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.744699 30233 hierarchical.hpp:648] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150224-185117-2272962752-44950-30213-S0 from framework 20150224-185117-2272962752-44950-30213-0000 I0224 18:51:17.746369 30232 master.cpp:4682] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150224-185117-2272962752-44950-30213-0000 on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) I0224 18:51:17.747835 30232 master.cpp:4711] Removing executor 'default' with resources  of framework 20150224-185117-2272962752-44950-30213-0000 on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) I0224 18:51:17.749958 30230 hierarchical.hpp:354] Removed framework 20150224-185117-2272962752-44950-30213-0000 W0224 18:51:17.754004 30232 master.cpp:3382] Ignoring unknown exited executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) I0224 18:51:17.806908 30235 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(155)@192.168.122.135:44950 I0224 18:51:18.169684 30235 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(156)@192.168.122.135:44950 I0224 18:51:18.277674 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-279c11f0-4f87-4922-b09c-e75af333d93e@192.168.122.135:44950 I0224 18:51:18.435956 30229 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:18.438434 30229 hierarchical.hpp:741] Performed allocation for 1 slaves in 2.913091ms I0224 18:51:18.687819 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:18.840509 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950 I0224 18:51:19.440609 30233 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:19.444022 30233 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.667907ms I0224 18:51:20.445341 30229 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:20.448892 30229 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.787334ms I0224 18:51:20.840729 30235 process.cpp:2117] Dropped / Lost event for PID: slave(133)@192.168.122.135:44950 I0224 18:51:20.895016 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-deb787a0-9e87-42c9-813e-fc35f354582f@192.168.122.135:44950 I0224 18:51:21.016639 30235 process.cpp:2117] Dropped / Lost event for PID: slave(133)@192.168.122.135:44950 I0224 18:51:21.258066 30235 process.cpp:2117] Dropped / Lost event for PID: slave(134)@192.168.122.135:44950 I0224 18:51:21.312721 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-7a62b2f4-6959-49de-9fd8-72ffd048f4e3@192.168.122.135:44950 I0224 18:51:21.450574 30230 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:21.451004 30230 hierarchical.hpp:741] Performed allocation for 1 slaves in 761280ns I0224 18:51:21.557883 30235 process.cpp:2117] Dropped / Lost event for PID: slave(135)@192.168.122.135:44950 I0224 18:51:21.611552 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-279c11f0-4f87-4922-b09c-e75af333d93e@192.168.122.135:44950 I0224 18:51:21.709940 30235 process.cpp:2117] Dropped / Lost event for PID: slave(135)@192.168.122.135:44950 I0224 18:51:21.915220 30235 process.cpp:2117] Dropped / Lost event for PID: slave(136)@192.168.122.135:44950 I0224 18:51:21.997714 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950 I0224 18:51:22.107311 30235 process.cpp:2117] Dropped / Lost event for PID: slave(136)@192.168.122.135:44950 I0224 18:51:22.219341 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950 I0224 18:51:22.269714 30235 process.cpp:2117] Dropped / Lost event for PID: slave(137)@192.168.122.135:44950 I0224 18:51:22.453269 30229 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:22.457568 30229 hierarchical.hpp:741] Performed allocation for 1 slaves in 4.67818ms I0224 18:51:22.644316 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 I0224 18:51:23.459383 30231 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:23.462417 30231 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.417923ms I0224 18:51:24.464651 30228 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:24.468094 30228 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.698248ms I0224 18:51:25.469254 30232 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:25.472430 30232 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.477698ms I0224 18:51:25.971513 30235 process.cpp:2117] Dropped / Lost event for PID: (2965)@192.168.122.135:44950 I0224 18:51:26.474663 30234 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:26.475232 30234 hierarchical.hpp:741] Performed allocation for 1 slaves in 942399ns I0224 18:51:26.672420 30235 process.cpp:2117] Dropped / Lost event for PID: (2996)@192.168.122.135:44950 I0224 18:51:27.069792 30235 process.cpp:2117] Dropped / Lost event for PID: (3010)@192.168.122.135:44950 I0224 18:51:27.476572 30228 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:27.479708 30228 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.419391ms I0224 18:51:28.481403 30228 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:28.484798 30228 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.709639ms I0224 18:51:29.487264 30228 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:29.491909 30228 hierarchical.hpp:741] Performed allocation for 1 slaves in 5.065187ms I0224 18:51:29.623121 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(739)@192.168.122.135:44950 I0224 18:51:29.641655 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(120)@192.168.122.135:44950 I0224 18:51:29.647222 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(740)@192.168.122.135:44950 I0224 18:51:30.493526 30232 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:30.496922 30232 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.714894ms I0224 18:51:30.670241 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(741)@192.168.122.135:44950 I0224 18:51:30.670737 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(742)@192.168.122.135:44950 I0224 18:51:30.882052 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(121)@192.168.122.135:44950 I0224 18:51:30.922494 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(744)@192.168.122.135:44950 I0224 18:51:30.985663 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(745)@192.168.122.135:44950 I0224 18:51:31.293728 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(122)@192.168.122.135:44950 I0224 18:51:31.328766 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(747)@192.168.122.135:44950 I0224 18:51:31.497968 30234 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:31.501803 30234 hierarchical.hpp:741] Performed allocation for 1 slaves in 4.13526ms I0224 18:51:31.604324 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(123)@192.168.122.135:44950 I0224 18:51:31.645259 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(749)@192.168.122.135:44950 I0224 18:51:31.676254 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(750)@192.168.122.135:44950 I0224 18:51:31.913120 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(752)@192.168.122.135:44950 I0224 18:51:31.957001 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(124)@192.168.122.135:44950 I0224 18:51:31.996151 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(753)@192.168.122.135:44950 I0224 18:51:32.033216 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(754)@192.168.122.135:44950 I0224 18:51:32.231158 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(756)@192.168.122.135:44950 I0224 18:51:32.267324 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(757)@192.168.122.135:44950 I0224 18:51:32.503401 30227 hierarchical.hpp:834] No resources available to allocate! I0224 18:51:32.503978 30227 hierarchical.hpp:741] Performed allocation for 1 slaves in 1.062132ms I0224 18:51:32.621012 30232 slave.cpp:2830] Received ping from slave-observer(125)@192.168.122.135:44950 I0224 18:51:32.658608 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(759)@192.168.122.135:44950 I0224 18:51:32.671058 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(760)@192.168.122.135:44950 I0224 18:51:32.719501 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(761)@192.168.122.135:44950 tests/master_tests.cpp:259: Failure Failed to wait 15secs for shutdown I0224 18:51:32.737337 30213 process.cpp:2117] Dropped / Lost event for PID: scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 tests/master_tests.cpp:249: Failure Actual function call count doesn't match EXPECT_CALL(exec, shutdown(_))...          Expected: to be called once            Actual: never called - unsatisfied and active I0224 18:51:32.741822 30229 master.cpp:787] Master terminating I0224 18:51:32.750730 30234 slave.cpp:2915] master@192.168.122.135:44950 exited W0224 18:51:32.751667 30234 slave.cpp:2918] Master disconnected! Waiting for a new master to be elected I0224 18:51:32.769243 30213 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.135:44950 I0224 18:51:32.770519 30213 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.135:44950 *** Aborted at 1424832692 (unix time) try ""date -d @1424832692"" if you are using GNU date *** PC: @          0x4612540 (unknown) *** SIGSEGV (@0x4612540) received by PID 30213 (TID 0x7fc7f2fa6880) from PID 73475392; stack trace: ***     @       0x3aa2a0efa0 (unknown)     @          0x4612540 (unknown) make[3]: *** [check-local] Segmentation fault (core dumped)  {code}",1
"MESOS-2403","MasterAllocatorTest/0.FrameworkReregistersFirst is flaky","{code} [ RUN      ] MasterAllocatorTest/0.FrameworkReregistersFirst Using temporary directory '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_Vy5Nml' I0224 23:22:31.681670 30589 leveldb.cpp:176] Opened db in 2.943518ms I0224 23:22:31.682152 30619 process.cpp:2117] Dropped / Lost event for PID: slave(65)@67.195.81.187:38391 I0224 23:22:31.682732 30589 leveldb.cpp:183] Compacted db in 1.029469ms I0224 23:22:31.682777 30589 leveldb.cpp:198] Created db iterator in 15460ns I0224 23:22:31.682792 30589 leveldb.cpp:204] Seeked to beginning of db in 1832ns I0224 23:22:31.682802 30589 leveldb.cpp:273] Iterated through 0 keys in the db in 319ns I0224 23:22:31.682833 30589 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0224 23:22:31.683228 30605 recover.cpp:449] Starting replica recovery I0224 23:22:31.683537 30605 recover.cpp:475] Replica is in 4 status I0224 23:22:31.684624 30615 replica.cpp:641] Replica in 4 status received a broadcasted recover request I0224 23:22:31.684978 30616 recover.cpp:195] Received a recover response from a replica in 4 status I0224 23:22:31.685405 30610 recover.cpp:566] Updating replica status to 3 I0224 23:22:31.686249 30609 master.cpp:349] Master 20150224-232231-3142697795-38391-30589 (pomona.apache.org) started on 67.195.81.187:38391 I0224 23:22:31.686265 30617 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 717897ns I0224 23:22:31.686319 30617 replica.cpp:323] Persisted replica status to 3 I0224 23:22:31.686336 30609 master.cpp:395] Master only allowing authenticated frameworks to register I0224 23:22:31.686357 30609 master.cpp:400] Master only allowing authenticated slaves to register I0224 23:22:31.686390 30609 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_Vy5Nml/credentials' I0224 23:22:31.686511 30606 recover.cpp:475] Replica is in 3 status I0224 23:22:31.686563 30609 master.cpp:442] Authorization enabled I0224 23:22:31.686929 30607 whitelist_watcher.cpp:79] No whitelist given I0224 23:22:31.686954 30603 hierarchical.hpp:287] Initialized hierarchical allocator process I0224 23:22:31.687134 30605 replica.cpp:641] Replica in 3 status received a broadcasted recover request I0224 23:22:31.687731 30609 master.cpp:1356] The newly elected leader is master@67.195.81.187:38391 with id 20150224-232231-3142697795-38391-30589 I0224 23:22:31.839818 30609 master.cpp:1369] Elected as the leading master! I0224 23:22:31.839834 30609 master.cpp:1187] Recovering from registrar I0224 23:22:31.839926 30605 registrar.cpp:313] Recovering registrar I0224 23:22:31.840000 30613 recover.cpp:195] Received a recover response from a replica in 3 status I0224 23:22:31.840504 30606 recover.cpp:566] Updating replica status to 1 I0224 23:22:31.841599 30611 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 990330ns I0224 23:22:31.841627 30611 replica.cpp:323] Persisted replica status to 1 I0224 23:22:31.841743 30611 recover.cpp:580] Successfully joined the Paxos group I0224 23:22:31.841904 30611 recover.cpp:464] Recover process terminated I0224 23:22:31.842366 30608 log.cpp:660] Attempting to start the writer I0224 23:22:31.843557 30607 replica.cpp:477] Replica received implicit promise request with proposal 1 I0224 23:22:31.844312 30607 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 722368ns I0224 23:22:31.844337 30607 replica.cpp:345] Persisted promised to 1 I0224 23:22:31.844889 30615 coordinator.cpp:230] Coordinator attemping to fill missing position I0224 23:22:31.846043 30614 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0224 23:22:31.846729 30614 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 660024ns I0224 23:22:31.846746 30614 replica.cpp:679] Persisted action at 0 I0224 23:22:31.847671 30611 replica.cpp:511] Replica received write request for position 0 I0224 23:22:31.847723 30611 leveldb.cpp:438] Reading position from leveldb took 27349ns I0224 23:22:31.848429 30611 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 671461ns I0224 23:22:31.848454 30611 replica.cpp:679] Persisted action at 0 I0224 23:22:31.849041 30615 replica.cpp:658] Replica received learned notice for position 0 I0224 23:22:31.849762 30615 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 690386ns I0224 23:22:31.849787 30615 replica.cpp:679] Persisted action at 0 I0224 23:22:31.849808 30615 replica.cpp:664] Replica learned 1 action at position 0 I0224 23:22:31.850416 30612 log.cpp:676] Writer started with ending position 0 I0224 23:22:31.851490 30615 leveldb.cpp:438] Reading position from leveldb took 30659ns I0224 23:22:31.854452 30610 registrar.cpp:346] Successfully fetched the registry (0B) in 14.491136ms I0224 23:22:31.854543 30610 registrar.cpp:445] Applied 1 operations in 18024ns; attempting to update the 'registry' I0224 23:22:31.857095 30604 log.cpp:684] Attempting to append 139 bytes to the log I0224 23:22:31.857208 30608 coordinator.cpp:340] Coordinator attempting to write 2 action at position 1 I0224 23:22:31.858073 30609 replica.cpp:511] Replica received write request for position 1 I0224 23:22:31.858808 30609 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 701708ns I0224 23:22:31.858835 30609 replica.cpp:679] Persisted action at 1 I0224 23:22:31.859508 30618 replica.cpp:658] Replica received learned notice for position 1 I0224 23:22:31.860267 30618 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 731035ns I0224 23:22:31.860309 30618 replica.cpp:679] Persisted action at 1 I0224 23:22:31.860332 30618 replica.cpp:664] Replica learned 2 action at position 1 I0224 23:22:31.860983 30609 registrar.cpp:490] Successfully updated the 'registry' in 6.39616ms I0224 23:22:31.861071 30609 registrar.cpp:376] Successfully recovered registrar I0224 23:22:31.861126 30608 log.cpp:703] Attempting to truncate the log to 1 I0224 23:22:31.861249 30603 coordinator.cpp:340] Coordinator attempting to write 3 action at position 2 I0224 23:22:31.861248 30617 master.cpp:1214] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I0224 23:22:31.861831 30613 replica.cpp:511] Replica received write request for position 2 I0224 23:22:31.862504 30613 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 648125ns I0224 23:22:31.862531 30613 replica.cpp:679] Persisted action at 2 I0224 23:22:31.863067 30603 replica.cpp:658] Replica received learned notice for position 2 I0224 23:22:31.863689 30603 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 602784ns I0224 23:22:31.863737 30603 leveldb.cpp:401] Deleting ~1 keys from leveldb took 28697ns I0224 23:22:31.863751 30603 replica.cpp:679] Persisted action at 2 I0224 23:22:31.863767 30603 replica.cpp:664] Replica learned 3 action at position 2 I0224 23:22:31.875962 30610 slave.cpp:174] Slave started on 66)@67.195.81.187:38391 I0224 23:22:31.876008 30610 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/credential' I0224 23:22:31.876144 30610 slave.cpp:281] Slave using credential for: test-principal I0224 23:22:31.876404 30610 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I0224 23:22:31.876489 30610 slave.cpp:328] Slave hostname: pomona.apache.org I0224 23:22:31.876502 30610 slave.cpp:329] Slave checkpoint: false W0224 23:22:31.876507 30610 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0224 23:22:31.877014 30603 state.cpp:35] Recovering state from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/meta' I0224 23:22:31.877230 30610 status_update_manager.cpp:197] Recovering status update manager I0224 23:22:31.877495 30609 slave.cpp:3776] Finished recovery I0224 23:22:31.877879 30607 status_update_manager.cpp:171] Pausing sending status updates I0224 23:22:31.877879 30604 slave.cpp:624] New master detected at master@67.195.81.187:38391 I0224 23:22:31.877959 30604 slave.cpp:687] Authenticating with master master@67.195.81.187:38391 I0224 23:22:31.877975 30604 slave.cpp:692] Using default CRAM-MD5 authenticatee I0224 23:22:31.878069 30604 slave.cpp:660] Detecting new master I0224 23:22:31.878093 30608 authenticatee.hpp:139] Creating new client SASL connection I0224 23:22:31.878223 30604 master.cpp:3813] Authenticating slave(66)@67.195.81.187:38391 I0224 23:22:31.878244 30604 master.cpp:3824] Using default CRAM-MD5 authenticator I0224 23:22:31.878412 30613 authenticator.hpp:170] Creating new server SASL connection I0224 23:22:31.878525 30603 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5 I0224 23:22:31.878551 30603 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5' I0224 23:22:31.878625 30617 authenticator.hpp:276] Received SASL authentication start I0224 23:22:31.878662 30617 authenticator.hpp:398] Authentication requires more steps I0224 23:22:31.878727 30603 authenticatee.hpp:276] Received SASL authentication step I0224 23:22:31.878815 30617 authenticator.hpp:304] Received SASL authentication step I0224 23:22:31.878839 30617 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0224 23:22:31.878847 30617 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0224 23:22:31.878875 30617 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0224 23:22:31.878891 30617 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0224 23:22:31.878900 30617 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0224 23:22:31.878906 30617 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0224 23:22:31.878916 30617 authenticator.hpp:390] Authentication success I0224 23:22:31.880717 30589 sched.cpp:157] Version: 0.23.0 I0224 23:22:32.017823 30611 authenticatee.hpp:316] Authentication success I0224 23:22:32.017901 30618 master.cpp:3871] Successfully authenticated principal 'test-principal' at slave(66)@67.195.81.187:38391 I0224 23:22:32.018156 30615 sched.cpp:254] New master detected at master@67.195.81.187:38391 I0224 23:22:32.018240 30615 sched.cpp:310] Authenticating with master master@67.195.81.187:38391 I0224 23:22:32.018263 30615 sched.cpp:317] Using default CRAM-MD5 authenticatee I0224 23:22:32.018496 30613 slave.cpp:758] Successfully authenticated with master master@67.195.81.187:38391 I0224 23:22:32.018579 30611 authenticatee.hpp:139] Creating new client SASL connection I0224 23:22:32.018620 30613 slave.cpp:1090] Will retry registration in 363167ns if necessary I0224 23:22:32.018811 30615 master.cpp:2938] Registering slave at slave(66)@67.195.81.187:38391 (pomona.apache.org) with id 20150224-232231-3142697795-38391-30589-S0 I0224 23:22:32.019122 30615 master.cpp:3813] Authenticating scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.019156 30615 master.cpp:3824] Using default CRAM-MD5 authenticator I0224 23:22:32.019232 30612 registrar.cpp:445] Applied 1 operations in 57599ns; attempting to update the 'registry' I0224 23:22:32.019394 30603 authenticator.hpp:170] Creating new server SASL connection I0224 23:22:32.019541 30611 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5 I0224 23:22:32.019568 30611 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5' I0224 23:22:32.019666 30605 authenticator.hpp:276] Received SASL authentication start I0224 23:22:32.019717 30605 authenticator.hpp:398] Authentication requires more steps I0224 23:22:32.019805 30615 authenticatee.hpp:276] Received SASL authentication step I0224 23:22:32.019942 30605 authenticator.hpp:304] Received SASL authentication step I0224 23:22:32.019979 30605 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0224 23:22:32.019994 30605 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0224 23:22:32.020025 30605 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0224 23:22:32.020036 30610 slave.cpp:1090] Will retry registration in 10.850555ms if necessary I0224 23:22:32.020053 30605 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0224 23:22:32.020102 30605 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0224 23:22:32.020117 30605 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0224 23:22:32.020133 30605 authenticator.hpp:390] Authentication success I0224 23:22:32.020151 30611 master.cpp:2926] Ignoring register slave message from slave(66)@67.195.81.187:38391 (pomona.apache.org) as admission is already in progress I0224 23:22:32.020226 30603 authenticatee.hpp:316] Authentication success I0224 23:22:32.020256 30611 master.cpp:3871] Successfully authenticated principal 'test-principal' at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.020534 30615 sched.cpp:398] Successfully authenticated with master master@67.195.81.187:38391 I0224 23:22:32.020561 30615 sched.cpp:521] Sending registration request to master@67.195.81.187:38391 I0224 23:22:32.020635 30615 sched.cpp:554] Will retry registration in 490.035142ms if necessary I0224 23:22:32.020720 30613 master.cpp:1574] Received registration request for framework 'default' at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.020787 30613 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*' I0224 23:22:32.021122 30607 master.cpp:1638] Registering framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.021502 30611 hierarchical.hpp:321] Added framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.021531 30611 hierarchical.hpp:834] No resources available to allocate! I0224 23:22:32.021543 30611 hierarchical.hpp:741] Performed allocation for 0 slaves in 18915ns I0224 23:22:32.021618 30609 sched.cpp:448] Framework registered with 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.021673 30609 sched.cpp:462] Scheduler::registered took 26310ns I0224 23:22:32.022400 30613 log.cpp:684] Attempting to append 316 bytes to the log I0224 23:22:32.022523 30608 coordinator.cpp:340] Coordinator attempting to write 2 action at position 3 I0224 23:22:32.023232 30607 replica.cpp:511] Replica received write request for position 3 I0224 23:22:32.024055 30607 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 798548ns I0224 23:22:32.024073 30607 replica.cpp:679] Persisted action at 3 I0224 23:22:32.024651 30610 replica.cpp:658] Replica received learned notice for position 3 I0224 23:22:32.025252 30610 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 580525ns I0224 23:22:32.025271 30610 replica.cpp:679] Persisted action at 3 I0224 23:22:32.025297 30610 replica.cpp:664] Replica learned 2 action at position 3 I0224 23:22:32.025995 30618 registrar.cpp:490] Successfully updated the 'registry' in 6.586112ms I0224 23:22:32.026228 30604 log.cpp:703] Attempting to truncate the log to 3 I0224 23:22:32.026360 30609 coordinator.cpp:340] Coordinator attempting to write 3 action at position 4 I0224 23:22:32.026669 30609 slave.cpp:2831] Received ping from slave-observer(66)@67.195.81.187:38391 I0224 23:22:32.026772 30609 slave.cpp:792] Registered with master master@67.195.81.187:38391; given slave ID 20150224-232231-3142697795-38391-30589-S0 I0224 23:22:32.026737 30603 master.cpp:2995] Registered slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I0224 23:22:32.026867 30603 status_update_manager.cpp:178] Resuming sending status updates I0224 23:22:32.026868 30617 hierarchical.hpp:455] Added slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available) I0224 23:22:32.026921 30615 replica.cpp:511] Replica received write request for position 4 I0224 23:22:32.027276 30617 hierarchical.hpp:759] Performed allocation for slave 20150224-232231-3142697795-38391-30589-S0 in 351257ns I0224 23:22:32.027580 30615 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 624249ns I0224 23:22:32.027604 30615 replica.cpp:679] Persisted action at 4 I0224 23:22:32.027642 30618 master.cpp:3755] Sending 1 offers to framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.028223 30617 replica.cpp:658] Replica received learned notice for position 4 I0224 23:22:32.028621 30607 sched.cpp:611] Scheduler::resourceOffers took 648326ns I0224 23:22:32.028916 30617 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 662416ns I0224 23:22:32.028991 30617 leveldb.cpp:401] Deleting ~2 keys from leveldb took 47386ns I0224 23:22:32.029021 30617 replica.cpp:679] Persisted action at 4 I0224 23:22:32.029044 30617 replica.cpp:664] Replica learned 3 action at position 4 I0224 23:22:32.029534 30613 master.cpp:2268] Processing ACCEPT call for offers: [ 20150224-232231-3142697795-38391-30589-O0 ] on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) for framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.190521 30613 master.cpp:2112] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' W0224 23:22:32.191864 30604 validation.cpp:328] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0224 23:22:32.191905 30604 validation.cpp:340] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0224 23:22:32.192206 30604 master.hpp:822] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org) I0224 23:22:32.192318 30604 master.cpp:2545] Launching task 0 of framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 with resources cpus(*):1; mem(*):500 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) I0224 23:22:32.192659 30611 slave.cpp:1121] Got assigned task 0 for framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.192847 30609 hierarchical.hpp:648] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20150224-232231-3142697795-38391-30589-S0 from framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.192916 30609 hierarchical.hpp:684] Framework 20150224-232231-3142697795-38391-30589-0000 filtered slave 20150224-232231-3142697795-38391-30589-S0 for 5secs I0224 23:22:32.193327 30611 slave.cpp:1231] Launching task 0 for framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.196038 30611 slave.cpp:4178] Launching executor default of framework 20150224-232231-3142697795-38391-30589-0000 in work directory '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000/executors/default/runs/7c203619-b40f-4d0a-9b75-9ddeecb63472' I0224 23:22:32.197996 30611 exec.cpp:132] Version: 0.23.0 I0224 23:22:32.198206 30605 exec.cpp:182] Executor started at: executor(32)@67.195.81.187:38391 with pid 30589 I0224 23:22:32.198314 30611 slave.cpp:1378] Queuing task '0' for executor default of framework '20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.198407 30611 slave.cpp:577] Successfully attached file '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000/executors/default/runs/7c203619-b40f-4d0a-9b75-9ddeecb63472' I0224 23:22:32.198508 30611 slave.cpp:3133] Monitoring executor 'default' of framework '20150224-232231-3142697795-38391-30589-0000' in container '7c203619-b40f-4d0a-9b75-9ddeecb63472' I0224 23:22:32.198637 30611 slave.cpp:2141] Got registration for executor 'default' of framework 20150224-232231-3142697795-38391-30589-0000 from executor(32)@67.195.81.187:38391 I0224 23:22:32.198839 30604 exec.cpp:206] Executor registered on slave 20150224-232231-3142697795-38391-30589-S0 I0224 23:22:32.199090 30611 slave.cpp:1532] Sending queued task '0' to executor 'default' of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.200916 30604 exec.cpp:218] Executor::registered took 24976ns I0224 23:22:32.201087 30604 exec.cpp:293] Executor asked to run task '0' I0224 23:22:32.201164 30604 exec.cpp:302] Executor::launchTask took 54201ns I0224 23:22:32.203301 30604 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.203533 30618 slave.cpp:2508] Handling status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 from executor(32)@67.195.81.187:38391 I0224 23:22:32.203799 30604 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.203840 30604 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.204038 30604 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 to the slave I0224 23:22:32.204262 30607 slave.cpp:2751] Forwarding the update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 to master@67.195.81.187:38391 I0224 23:22:32.204473 30607 slave.cpp:2678] Status update manager successfully handled status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.204511 30607 slave.cpp:2684] Sending acknowledgement for status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 to executor(32)@67.195.81.187:38391 I0224 23:22:32.204558 30616 master.cpp:3295] Status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 from slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) I0224 23:22:32.204602 30616 master.cpp:3336] Forwarding status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.204676 30610 exec.cpp:339] Executor received status update acknowledgement 49340611-fb39-423b-96f6-6c1e724c1a53 for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.204753 30616 master.cpp:4615] Updating the latest state of task 0 of framework 20150224-232231-3142697795-38391-30589-0000 to TASK_RUNNING I0224 23:22:32.204874 30603 sched.cpp:717] Scheduler::statusUpdate took 52057ns I0224 23:22:32.205277 30614 master.cpp:2782] Forwarding status update acknowledgement 49340611-fb39-423b-96f6-6c1e724c1a53 for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 to slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) I0224 23:22:32.205579 30618 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.205798 30604 slave.cpp:2081] Status update manager successfully handled status update acknowledgement (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.206073 30607 master.cpp:787] Master terminating W0224 23:22:32.206197 30607 master.cpp:4668] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20150224-232231-3142697795-38391-30589-0000 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) in non-terminal state TASK_RUNNING I0224 23:22:32.206420 30614 hierarchical.hpp:648] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20150224-232231-3142697795-38391-30589-S0 from framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.206552 30607 master.cpp:4711] Removing executor 'default' with resources  of framework 20150224-232231-3142697795-38391-30589-0000 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) I0224 23:22:32.321058 30619 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(81)@67.195.81.187:38391 I0224 23:22:32.367074 30612 slave.cpp:2916] master@67.195.81.187:38391 exited W0224 23:22:32.367101 30612 slave.cpp:2919] Master disconnected! Waiting for a new master to be elected I0224 23:22:32.368388 30589 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391 I0224 23:22:32.368482 30589 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391 I0224 23:22:32.375794 30589 leveldb.cpp:176] Opened db in 3.725405ms I0224 23:22:32.379137 30589 leveldb.cpp:183] Compacted db in 3.309337ms I0224 23:22:32.379196 30589 leveldb.cpp:198] Created db iterator in 21994ns I0224 23:22:32.379236 30589 leveldb.cpp:204] Seeked to beginning of db in 20370ns I0224 23:22:32.379356 30589 leveldb.cpp:273] Iterated through 3 keys in the db in 102640ns I0224 23:22:32.379411 30589 replica.cpp:744] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned I0224 23:22:32.379884 30610 recover.cpp:449] Starting replica recovery I0224 23:22:32.380156 30610 recover.cpp:475] Replica is in 1 status I0224 23:22:32.380460 30610 recover.cpp:464] Recover process terminated I0224 23:22:32.381878 30616 master.cpp:349] Master 20150224-232232-3142697795-38391-30589 (pomona.apache.org) started on 67.195.81.187:38391 I0224 23:22:32.381927 30616 master.cpp:395] Master only allowing authenticated frameworks to register I0224 23:22:32.381945 30616 master.cpp:400] Master only allowing authenticated slaves to register I0224 23:22:32.381974 30616 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_Vy5Nml/credentials' I0224 23:22:32.382220 30616 master.cpp:442] Authorization enabled I0224 23:22:32.382783 30604 whitelist_watcher.cpp:79] No whitelist given I0224 23:22:32.382861 30607 hierarchical.hpp:287] Initialized hierarchical allocator process I0224 23:22:32.384114 30616 master.cpp:1356] The newly elected leader is master@67.195.81.187:38391 with id 20150224-232232-3142697795-38391-30589 I0224 23:22:32.384150 30616 master.cpp:1369] Elected as the leading master! I0224 23:22:32.384178 30616 master.cpp:1187] Recovering from registrar I0224 23:22:32.384330 30617 registrar.cpp:313] Recovering registrar I0224 23:22:32.384851 30606 log.cpp:660] Attempting to start the writer I0224 23:22:32.386044 30615 replica.cpp:477] Replica received implicit promise request with proposal 2 I0224 23:22:32.386862 30615 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 790008ns I0224 23:22:32.386888 30615 replica.cpp:345] Persisted promised to 2 I0224 23:22:32.387544 30612 coordinator.cpp:230] Coordinator attemping to fill missing position I0224 23:22:32.387799 30615 log.cpp:676] Writer started with ending position 4 I0224 23:22:32.388854 30606 leveldb.cpp:438] Reading position from leveldb took 51391ns I0224 23:22:32.388949 30606 leveldb.cpp:438] Reading position from leveldb took 40544ns I0224 23:22:32.390015 30611 registrar.cpp:346] Successfully fetched the registry (277B) in 5.604096ms I0224 23:22:32.390182 30611 registrar.cpp:445] Applied 1 operations in 46501ns; attempting to update the 'registry' I0224 23:22:32.392963 30608 log.cpp:684] Attempting to append 316 bytes to the log I0224 23:22:32.393081 30606 coordinator.cpp:340] Coordinator attempting to write 2 action at position 5 I0224 23:22:32.393875 30607 replica.cpp:511] Replica received write request for position 5 I0224 23:22:32.394582 30607 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 675133ns I0224 23:22:32.394609 30607 replica.cpp:679] Persisted action at 5 I0224 23:22:32.395190 30614 replica.cpp:658] Replica received learned notice for position 5 I0224 23:22:32.395917 30614 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 701360ns I0224 23:22:32.395944 30614 replica.cpp:679] Persisted action at 5 I0224 23:22:32.395966 30614 replica.cpp:664] Replica learned 2 action at position 5 I0224 23:22:32.396880 30614 registrar.cpp:490] Successfully updated the 'registry' in 6.644224ms I0224 23:22:32.397056 30614 registrar.cpp:376] Successfully recovered registrar I0224 23:22:32.397111 30604 log.cpp:703] Attempting to truncate the log to 5 I0224 23:22:32.397274 30616 coordinator.cpp:340] Coordinator attempting to write 3 action at position 6 I0224 23:22:32.397708 30615 master.cpp:1214] Recovered 1 slaves from the Registry (277B) ; allowing 10mins for slaves to re-register I0224 23:22:32.398107 30612 replica.cpp:511] Replica received write request for position 6 I0224 23:22:32.398763 30612 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 625992ns I0224 23:22:32.398789 30612 replica.cpp:679] Persisted action at 6 I0224 23:22:32.399373 30615 replica.cpp:658] Replica received learned notice for position 6 I0224 23:22:32.400152 30615 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 754us I0224 23:22:32.400228 30615 leveldb.cpp:401] Deleting ~2 keys from leveldb took 39811ns I0224 23:22:32.400250 30615 replica.cpp:679] Persisted action at 6 I0224 23:22:32.400274 30615 replica.cpp:664] Replica learned 3 action at position 6 I0224 23:22:32.410408 30604 sched.cpp:248] Scheduler::disconnected took 15348ns I0224 23:22:32.410430 30604 sched.cpp:254] New master detected at master@67.195.81.187:38391 I0224 23:22:32.410508 30604 sched.cpp:310] Authenticating with master master@67.195.81.187:38391 I0224 23:22:32.410531 30604 sched.cpp:317] Using default CRAM-MD5 authenticatee I0224 23:22:32.410781 30605 authenticatee.hpp:139] Creating new client SASL connection I0224 23:22:32.410964 30608 master.cpp:3813] Authenticating scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.410991 30608 master.cpp:3824] Using default CRAM-MD5 authenticator I0224 23:22:32.411188 30617 authenticator.hpp:170] Creating new server SASL connection I0224 23:22:32.411362 30612 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5 I0224 23:22:32.411391 30612 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5' I0224 23:22:32.411499 30618 authenticator.hpp:276] Received SASL authentication start I0224 23:22:32.411600 30618 authenticator.hpp:398] Authentication requires more steps I0224 23:22:32.411710 30614 authenticatee.hpp:276] Received SASL authentication step I0224 23:22:32.411818 30618 authenticator.hpp:304] Received SASL authentication step I0224 23:22:32.411849 30618 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0224 23:22:32.411861 30618 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0224 23:22:32.411897 30618 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0224 23:22:32.411922 30618 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0224 23:22:32.411934 30618 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0224 23:22:32.411942 30618 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0224 23:22:32.411958 30618 authenticator.hpp:390] Authentication success I0224 23:22:32.412032 30614 authenticatee.hpp:316] Authentication success I0224 23:22:32.412061 30612 master.cpp:3871] Successfully authenticated principal 'test-principal' at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.412345 30605 sched.cpp:398] Successfully authenticated with master master@67.195.81.187:38391 I0224 23:22:32.412375 30605 sched.cpp:521] Sending registration request to master@67.195.81.187:38391 I0224 23:22:32.528939 30605 sched.cpp:554] Will retry registration in 718.250035ms if necessary I0224 23:22:32.529072 30605 sched.cpp:521] Sending registration request to master@67.195.81.187:38391 I0224 23:22:32.529136 30605 sched.cpp:554] Will retry registration in 2.186117614secs if necessary I0224 23:22:32.529161 30618 master.cpp:1711] Received re-registration request from framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.529274 30618 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*' I0224 23:22:32.529597 30618 master.cpp:1711] Received re-registration request from framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.529662 30618 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*' I0224 23:22:32.529856 30618 master.cpp:1764] Re-registering framework 20150224-232231-3142697795-38391-30589-0000 (default)  at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.530449 30614 hierarchical.hpp:321] Added framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.530490 30614 hierarchical.hpp:834] No resources available to allocate! I0224 23:22:32.530510 30614 hierarchical.hpp:741] Performed allocation for 0 slaves in 32479ns I0224 23:22:32.530743 30618 master.cpp:1764] Re-registering framework 20150224-232231-3142697795-38391-30589-0000 (default)  at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.530777 30611 sched.cpp:448] Framework registered with 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.530787 30618 master.cpp:1804] Allowing framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 to re-register with an already used id I0224 23:22:32.530824 30611 sched.cpp:462] Scheduler::registered took 20697ns I0224 23:22:32.530948 30605 sched.cpp:477] Ignoring framework re-registered message because the driver is already connected! I0224 23:22:32.533220 30606 status_update_manager.cpp:171] Pausing sending status updates I0224 23:22:32.533226 30605 slave.cpp:624] New master detected at master@67.195.81.187:38391 I0224 23:22:32.533362 30605 slave.cpp:687] Authenticating with master master@67.195.81.187:38391 I0224 23:22:32.533385 30605 slave.cpp:692] Using default CRAM-MD5 authenticatee I0224 23:22:32.533529 30605 slave.cpp:660] Detecting new master I0224 23:22:32.533576 30612 authenticatee.hpp:139] Creating new client SASL connection I0224 23:22:32.533759 30603 master.cpp:3813] Authenticating slave(66)@67.195.81.187:38391 I0224 23:22:32.533790 30603 master.cpp:3824] Using default CRAM-MD5 authenticator I0224 23:22:32.534018 30605 authenticator.hpp:170] Creating new server SASL connection I0224 23:22:32.534195 30609 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5 I0224 23:22:32.534227 30609 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5' I0224 23:22:32.534339 30609 authenticator.hpp:276] Received SASL authentication start I0224 23:22:32.534394 30609 authenticator.hpp:398] Authentication requires more steps I0224 23:22:32.534494 30609 authenticatee.hpp:276] Received SASL authentication step I0224 23:22:32.534600 30609 authenticator.hpp:304] Received SASL authentication step I0224 23:22:32.534629 30609 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0224 23:22:32.534642 30609 auxprop.cpp:171] Looking up auxiliary property '*userPassword' I0224 23:22:32.534692 30609 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0224 23:22:32.534725 30609 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0224 23:22:32.534740 30609 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0224 23:22:32.534749 30609 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0224 23:22:32.534765 30609 authenticator.hpp:390] Authentication success I0224 23:22:32.534891 30604 authenticatee.hpp:316] Authentication success I0224 23:22:32.534906 30609 master.cpp:3871] Successfully authenticated principal 'test-principal' at slave(66)@67.195.81.187:38391 I0224 23:22:32.535146 30608 slave.cpp:758] Successfully authenticated with master master@67.195.81.187:38391 I0224 23:22:32.535567 30608 slave.cpp:1090] Will retry registration in 10.96399ms if necessary I0224 23:22:32.535843 30610 master.cpp:3120] Re-registering slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) I0224 23:22:32.536542 30606 registrar.cpp:445] Applied 1 operations in 71981ns; attempting to update the 'registry' I0224 23:22:32.539535 30612 log.cpp:684] Attempting to append 316 bytes to the log I0224 23:22:32.539682 30617 coordinator.cpp:340] Coordinator attempting to write 2 action at position 7 I0224 23:22:32.540549 30608 replica.cpp:511] Replica received write request for position 7 I0224 23:22:32.540832 30608 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 249089ns I0224 23:22:32.540858 30608 replica.cpp:679] Persisted action at 7 I0224 23:22:32.541555 30604 replica.cpp:658] Replica received learned notice for position 7 I0224 23:22:32.542254 30604 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 671501ns I0224 23:22:32.542294 30604 replica.cpp:679] Persisted action at 7 I0224 23:22:32.542320 30604 replica.cpp:664] Replica learned 2 action at position 7 I0224 23:22:32.543231 30603 registrar.cpp:490] Successfully updated the 'registry' in 6.617088ms I0224 23:22:32.543637 30613 log.cpp:703] Attempting to truncate the log to 7 I0224 23:22:32.543784 30611 coordinator.cpp:340] Coordinator attempting to write 3 action at position 8 I0224 23:22:32.544088 30610 master.hpp:822] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org) I0224 23:22:32.544503 30612 slave.cpp:2831] Received ping from slave-observer(67)@67.195.81.187:38391 I0224 23:22:32.544530 30618 replica.cpp:511] Replica received write request for position 8 I0224 23:22:32.544800 30610 master.cpp:3191] Re-registered slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I0224 23:22:32.544862 30607 slave.cpp:860] Re-registered with master master@67.195.81.187:38391 I0224 23:22:32.544961 30606 status_update_manager.cpp:178] Resuming sending status updates I0224 23:22:32.544975 30610 master.cpp:3219] Sending updated checkpointed resources  to slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) I0224 23:22:32.545047 30612 hierarchical.hpp:455] Added slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] available) I0224 23:22:32.545090 30603 slave.cpp:1922] Updating framework 20150224-232231-3142697795-38391-30589-0000 pid to scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.545128 30618 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 567893ns I0224 23:22:32.545155 30618 replica.cpp:679] Persisted action at 8 I0224 23:22:32.545240 30613 status_update_manager.cpp:178] Resuming sending status updates I0224 23:22:32.545424 30603 slave.cpp:2010] Updated checkpointed resources from  to  I0224 23:22:32.545569 30612 hierarchical.hpp:759] Performed allocation for slave 20150224-232231-3142697795-38391-30589-S0 in 472208ns I0224 23:22:32.545915 30610 replica.cpp:658] Replica received learned notice for position 8 I0224 23:22:32.545930 30607 master.cpp:3755] Sending 1 offers to framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.688380 30615 hierarchical.hpp:741] Performed allocation for 1 slaves in 418073ns I0224 23:22:32.691712 30607 master.cpp:3755] Sending 1 offers to framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.691795 30605 sched.cpp:611] Scheduler::resourceOffers took 95977ns I0224 23:22:32.691961 30610 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 649479ns I0224 23:22:32.692033 30610 leveldb.cpp:401] Deleting ~2 keys from leveldb took 43354ns I0224 23:22:32.692061 30610 replica.cpp:679] Persisted action at 8 I0224 23:22:32.692093 30610 replica.cpp:664] Replica learned 3 action at position 8 I0224 23:22:32.692191 30589 sched.cpp:1589] Asked to stop the driver ../../src/tests/master_allocator_tests.cpp:1308: Failure Mock function called more times than expected - returning directly.     Function call: resourceOffers(0x7fff88bc3d10, @0x2b8726c54b30 { 128-byte object <D0-C4 A2-22 87-2B 00-00 00-00 00-00 00-00 00-00 E0-C2 00-3C 87-2B 00-00 60-C3 00-3C 87-2B 00-00 E0-C3 00-3C 87-2B 00-00 D0-C0 00-3C 87-2B 00-00 50-7B 00-3C 87-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 20-69 73-20 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 20-6E 65-76 50-C2 00-3C 87-2B 00-00 01-00 00-00 01-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 0F-00 00-00> })          Expected: to be called once            Actual: called twice - over-saturated and active I0224 23:22:32.692252 30605 sched.cpp:611] Scheduler::resourceOffers took 213312ns I0224 23:22:32.692296 30610 master.cpp:787] Master terminating I0224 23:22:32.692387 30605 sched.cpp:831] Stopping framework '20150224-232231-3142697795-38391-30589-0000' W0224 23:22:32.692448 30610 master.cpp:4668] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20150224-232231-3142697795-38391-30589-0000 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) in non-terminal state TASK_RUNNING I0224 23:22:32.692739 30613 hierarchical.hpp:648] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):1; mem(*):500) on slave 20150224-232231-3142697795-38391-30589-S0 from framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.692939 30610 master.cpp:4711] Removing executor 'default' with resources  of framework 20150224-232231-3142697795-38391-30589-0000 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) I0224 23:22:32.693953 30610 slave.cpp:2916] master@67.195.81.187:38391 exited W0224 23:22:32.693982 30610 slave.cpp:2919] Master disconnected! Waiting for a new master to be elected I0224 23:22:32.695185 30589 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391 I0224 23:22:32.695327 30589 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391 I0224 23:22:32.697404 30613 slave.cpp:3191] Executor 'default' of framework 20150224-232231-3142697795-38391-30589-0000 exited with status 0 I0224 23:22:32.699574 30613 slave.cpp:2508] Handling status update TASK_LOST (UUID: d33cdd1e-5586-4ae5-94b0-870a1443844b) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 from @0.0.0.0:0 I0224 23:22:32.699658 30613 slave.cpp:4486] Terminating task 0 I0224 23:22:32.699964 30613 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391 I0224 23:22:32.700003 30613 slave.cpp:506] Slave terminating I0224 23:22:32.700073 30613 slave.cpp:1745] Asked to shut down framework 20150224-232231-3142697795-38391-30589-0000 by @0.0.0.0:0 I0224 23:22:32.700098 30613 slave.cpp:1770] Shutting down framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.700160 30613 slave.cpp:3300] Cleaning up executor 'default' of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.700355 30610 gc.cpp:56] Scheduling '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000/executors/default/runs/7c203619-b40f-4d0a-9b75-9ddeecb63472' for gc 6.99999189524148days in the future I0224 23:22:32.700449 30613 slave.cpp:3379] Cleaning up framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.700522 30610 gc.cpp:56] Scheduling '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000/executors/default' for gc 6.99999189365926days in the future I0224 23:22:32.700580 30608 status_update_manager.cpp:279] Closing status update streams for framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.700630 30610 gc.cpp:56] Scheduling '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000' for gc 6.99999189199111days in the future I0224 23:22:32.700666 30608 status_update_manager.cpp:525] Cleaning up status update stream for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 I0224 23:22:32.702638 30589 process.cpp:2117] Dropped / Lost event for PID: scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 I0224 23:22:32.702955 30589 process.cpp:2117] Dropped / Lost event for PID: slave(66)@67.195.81.187:38391 [  FAILED  ] MasterAllocatorTest/0.FrameworkReregistersFirst, where TypeParam = mesos::internal::master::allocator::MesosAllocator<mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> > (1027 ms)  {code}",2
"MESOS-2427","Add Java binding for the acceptOffers API.","We introduced the new acceptOffers API in C++ driver. We need to provide Java binding for this API as well.",2
"MESOS-2428","Add Python bindings for the acceptOffers API.","We introduced the new acceptOffers API in C++ driver. We need to provide Python binding for this API as well.",2
"MESOS-2438","Improve support for streaming HTTP Responses in libprocess.","Currently libprocess' HTTP::Response supports a PIPE construct for doing streaming responses:  {code} struct Response {   ...    // Either provide a ""body"", an absolute ""path"" to a file, or a   // ""pipe"" for streaming a response. Distinguish between the cases   // using 'type' below.   //   // BODY: Uses 'body' as the body of the response. These may be   // encoded using gzip for efficiency, if 'Content-Encoding' is not   // already specified.   //   // PATH: Attempts to perform a 'sendfile' operation on the file   // found at 'path'.   //   // PIPE: Splices data from 'pipe' using 'Transfer-Encoding=chunked'.   // Note that the read end of the pipe will be closed by libprocess   // either after the write end has been closed or if the socket the   // data is being spliced to has been closed (i.e., nobody is   // listening any longer). This can cause writes to the pipe to   // generate a SIGPIPE (which will terminate your program unless you   // explicitly ignore them or handle them).   //   // In all cases (BODY, PATH, PIPE), you are expected to properly   // specify the 'Content-Type' header, but the 'Content-Length' and   // or 'Transfer-Encoding' headers will be filled in for you.   enum {     NONE,     BODY,     PATH,     PIPE   } type;    ... }; {code}  This interface is too low level and difficult to program against:  * Connection closure is signaled with SIGPIPE, which is difficult for callers to deal with (must suppress SIGPIPE locally or globally in order to get EPIPE instead). * Pipes are generally for inter-process communication, and the pipe has finite size. With a blocking pipe the caller must deal with blocking when the pipe's buffer limit is exceeded. With a non-blocking pipe, the caller must deal with retrying the write.  We'll want to consider a few use cases: # Sending an HTTP::Response with streaming data. # Making a request with http::get and http::post in which the data is returned in a streaming manner. # Making a request in which the request content is streaming.  This ticket will focus on 1 as it is required for the HTTP API.",8
"MESOS-2461","Slave should provide details on processes running in its cgroups","The slave can optionally be put into its own cgroups for a list of subsystems, e.g., for monitoring of memory and cpu. See the slave flag: --slave_subsystems  It currently refuses to start if there are any processes in its cgroups - this could be another slave or some subprocess started by a previous slave - and only logs the pids of those processes.  Improve this to log details about the processes: suggest at least the process command, uid running it, and perhaps its start time.",1
"MESOS-2485","Add ability to distinguish slave removals metrics by reason.","Currently we only expose a single removal metric ({{""master/slave_removals""}}) which makes it difficult to distinguish between removal reasons in the alerting.  Currently, a slave can be removed for the following reasons:  # Health checks failed. # Slave unregistered. # Slave was replaced by a new slave (on the same endpoint).  In the case of (2), we expect this to be due to maintenance and don't want to be notified as strongly as with health check failures.",3
"MESOS-2491","Persist the reservation state on the slave","h3. Goal  The goal for this task is to persist the reservation state stored on the master on the corresponding slave. The {{needCheckpointing}} predicate is used to capture the condition for which a resource needs to be checkpointed. Currently the only condition is {{isPersistentVolume}}. We'll update this to include dynamically reserved resources.  h3. Expected Outcome  * The dynamically reserved resources will be persisted on the slave.",5
"MESOS-2500","Doxygen setup for libprocess","Goals:  - Initial doxygen setup.  - Enable interested developers to generate already available doxygen content locally in their workspace and view it. - Form the basis for future contributions of more doxygen content.  1. Devise a way to use Doxygen with Mesos source code. (For example, solve this by adding optional brew/apt-get installation to the ""Getting Started"" doc.) 2. Create a make target for libprocess documentation that can be manually triggered. 3. Create initial library top level documentation. 4. Enhance one header file with Doxygen. Make sure the generated output has all necessary links to navigate from the lib to the file and back, etc. ",2
"MESOS-2501","Doxygen style for libprocess","Create a description of the Doxygen style to use for libprocess documentation.   It is expected that this will later also become the Doxygen style for stout and Mesos, but we are working on libprocess only for now.  Possible outcome: a file named docs/doxygen-style.md  We hope for much input and expect a lot of discussion! ",1
"MESOS-2507","Performance issue in the master when a large number of slaves are registering.","For large clusters, when a lot of slaves are registering, the master gets backlogged processing registration requests. {{perf}} revealed the following:  {code} Events: 14K cycles  25.44%  libmesos-0.22.0-x.so  [.] mesos::internal::master::Master::registerSlave(process::UPID const&, mesos::SlaveInfo const&, std::vector<mesos::Resource, std::allocator<mesos::Resource> > cons  11.18%  libmesos-0.22.0-x.so  [.] pipecb   5.88%  libc-2.5.so             [.] malloc_consolidate   5.33%  libc-2.5.so             [.] _int_free   5.25%  libc-2.5.so             [.] malloc   5.23%  libc-2.5.so             [.] _int_malloc   4.11%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&)   3.22%  libmesos-0.22.0-x.so  [.] mesos::Resource::SharedDtor()   3.10%  [kernel]                [k] _raw_spin_lock   1.97%  libmesos-0.22.0-x.so  [.] mesos::Attribute::SharedDtor()   1.28%  libc-2.5.so             [.] memcmp   1.08%  libc-2.5.so             [.] free {code}  This is likely because we loop over all the slaves for each registration:  {code} void Master::registerSlave(     const UPID& from,     const SlaveInfo& slaveInfo,     const vector<Resource>& checkpointedResources,     const string& version) {   // ...    // Check if this slave is already registered (because it retries).   foreachvalue (Slave* slave, slaves.registered) {     if (slave->pid == from) {       // ...     }   }   // ... } {code}",5
"MESOS-2534","PerfTest.ROOT_SampleInit test fails.","From MESOS-2300 as well, it looks like this test is not reliable:  {code} [ RUN      ] PerfTest.ROOT_SampleInit ../../src/tests/perf_tests.cpp:147: Failure Expected: (0u) < (statistics.get().cycles()), actual: 0 vs 0 ../../src/tests/perf_tests.cpp:150: Failure Expected: (0.0) < (statistics.get().task_clock()), {code}  It looks like this test samples PID 1, which is either {{init}} or {{systemd}}. Per a chat with [~idownes] this should probably sample something that is guaranteed to be consuming cycles.",2
"MESOS-2581","Document tips, best practices, guidelines for doing code reviews.","We currently have a [""Committers Guide""|https://github.com/apache/mesos/blob/0.22.0/docs/committers-guide.md], however most of this information is relevant to all contributors looking to be participating in the code review process.  I'm proposing we extract much of this information into a more general ""Code Reviewing"" document, and include additional tips, best practices, lessons learned from members of the community.  This would be a great pre-requisite for on-boarding more committers and adding [MAINTAINERS|http://mail-archives.apache.org/mod_mbox/mesos-dev/201502.mbox/%3CCA+8RcoReugMVqoOpsnB8WGYBELa5fHwPA=J=YHJE22iwZvsbeQ@mail.gmail.com%3E].  The committers guide can be more specific to our expectations of committers, so we may want to make this into a ""committership"" document to help set expectations for contributors looking to become committers.",3
"MESOS-2591","Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse","Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse",2
"MESOS-2595","Create docker executor","Currently we're reusing the command executor to wait on the progress of the docker executor, but has the following drawback:  - We need to launch a seperate docker log process just to forward logs, where we can just simply reattach stdout/stderr if we create a specific executor for docker - In general, Mesos slave is assuming that the executor is the one starting the actual task. But the current docker containerizer, the containerizer is actually starting the docker container first then launches the command executor to wait on it. This can cause problems if the container failed before the command executor was able to launch, as slave will try to update the limits of the containerizer on executor registration but then the docker containerizer will fail to do so since the container failed.   Overall it's much simpler to tie the container lifecycle with the executor and simplfies logic and log management.",8
"MESOS-2615","Pipe 'updateFramework' path from master to Allocator to support framework re-registration","Pipe the 'updateFramework' call from the master through the allocator, as described in the design doc in the epic: MESOS-703",1
"MESOS-2622","Document the semantic change in decorator return values","In order to enable decorator modules to _remove_ metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks.  The Result<T> return values means:  ||State||Before||After|| |Error|Error is propagated to the call-site|No change| |None|The result of the decorator is not applied|No change| |Some|The result of the decorator is *appended*|The result of the decorator *overwrites* the final labels/environment object|",1
"MESOS-2680","Update modules doc with hook usage example","Modules doc states the possibility of using hooks, but doesn't refer to necessary flags and usage example.",1
"MESOS-2696","Explore exposing stats from kernel","Exploratory work.  Additional tickets to follow.",5
"MESOS-2700","Determine CFS behavior with biased cpu.shares subtrees","See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.  * Understand the relationship between cpu.shares and CFS quota. * Determine range of possible bias splits * Determine how to achieve bias, e.g., should 20:1 be 20480:1024 or ~1024:50 * Rigorous testing of behavior with varying loads, particularly the combination of latency sensitive loads for high biased tasks (non-revokable), and cpu intensive loads for the low biased tasks (revokable). * Discover any performance edge cases?",13
"MESOS-2701","Implement bi-level cpu.shares subtrees in cgroups/cpu isolator.","See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.  # Configurable bias # Change cgroup layout ** Implement roll-forward migration path in isolator recover ** Document roll-back migration path",8
"MESOS-2702","Compare split/flattened cgroup hierarchy for CPU oversubscription","Investigate if a flat hierarchy is sufficient for oversubscription of CPU or if a two-way split is necessary/preferred.",3
"MESOS-2709","Design Master discovery functionality for HTTP-only clients","When building clients that do not bind to {{libmesos}} and only use the HTTP API (via ""pure"" language bindings - eg, Java-only) there is no simple way to discover the Master's IP address to connect to.  Rather than relying on 'out-of-band' configuration mechanisms, we would like to enable the ability of interrogating the ZooKeeper ensemble to discover the Master's IP address (and, possibly, other information) to which the HTTP API requests can be addressed to.",3
"MESOS-2722","Create access to the Mesos ""state abstraction"" that does not require linking with libmesos","See ""src/state/state.hpp"" and ""src/java/src/org/apache/mesos/state/*.java"" for what the ""state abstraction"" is.  With the new HTTP API (see MESOS-2288, MESOS-2289), there will be no need to link to libmesos to a framework for it to communicate with a Mesos master. However, if a framework uses the Mesos ""state abstraction"", either directly in C++ or through other language bindings (e.g., Java), it still needs to link with libmesos. So, in order to achieve libmesos-free frameworks that can leverage all APIs Mesos has to offer, we need a different way to access the ""state abstraction"".   ---  One approach is to provide an HTTP API for state queries that get routed through the Mesos master, which relays them by making calls into libmesos. Details TBD, including how separate this will be from the general HTTP API. ",13
"MESOS-2726","Add support for enabling network namespace without enabling the network isolator","Following the discussion Kapil started, it is currently not possible to enable the linux network namespace for a container without enabling the network isolator (which requires certain kernel capabilities and dependencies). Following the pattern of enabling pid namespaces (--isolation=""namespaces/pid""). One possible solution could be to add another one for network i.e. ""namespaces/network"".  ",13
"MESOS-2737","Add documentation for maintainers.","In order to scale the number of committers in the project, we proposed the concept of maintainers here:  http://markmail.org/thread/cjmdn3d7qfzbxhpm  To follow up on that proposal, we'll need some documentation to capture the concept of maintainers. Both how contributors can benefit from maintainer feedback and the expectations of ""maintainer-ship"".  In order to not enforce an excessive amount of process, maintainers will initially only serve as an encouraged means to help contributors find reviewers and get meaningful feedback.",3
"MESOS-2783","document the fetcher","For framework developers specifically, Mesos provides a fetcher to move binaries. This needs MVP documentation.  - What is it - How does it help - What protocols or schemas are supported - Can it be extended  This is important to get framework developers over the hump of learning to code against Mesos and grow the ecosystem.",5
"MESOS-2793","Add support for container rootfs to Mesos isolators","Mesos containers can have a different rootfs to the host. Update Isolator interface to pass rootfs during Isolator::prepare(). Update Isolators where  necessary.",1
"MESOS-2794","Implement filesystem isolators","Move persistent volume support from Mesos containerizer to separate filesystem isolators, including support for container rootfs, where possible.  Use symlinks for posix systems without container rootfs. Use bind mounts for Linux with/without container rootfs.",13
"MESOS-2800","Rename Option<T>::get(const T& _t) to getOrElse() and refactor the original function","As suggested, if we want to change the name then we should refactor the original function as opposed to having 2 copies.  If we did have 2 versions of the same function, would it make more sense to delegate one of them to the other.  As of today, there is only one file need to be refactor: 3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp at line 151, 161",3
"MESOS-2801","Remove dynamic allocation from Future<T>","Remove the dynamic allocation of `T*` inside `Future::Data`",3
"MESOS-2804","Log framework capabilities in the master.","Now that {{Capabilities}} has been added to FrameworkInfo, we should log these in the master when a framework (re-)registers (i.e. which capabilities are enabled and disabled). This would make debugging easier for framework developers.  Ideally, folding in the old {{checkpoint}} capability and logging that as well. In the past, the fact that {{checkpoint}} defaults to false has tripped up a lot of developers.",1
"MESOS-2805","Make synchronized as primary form of synchronization.","Re-organize Synchronized to allow {{synchronized(m)}} to work on:   1. {{std::mutex}}   2. {{std::recursive_mutex}}   3. {{std::atomic_flag}}  Move synchronized.hpp into stout, so that developers don't think it's part of the utility suite for actors in libprocess.  Remove references to internal.hpp and replace them with {{std::atomic_flag}} synchronization.",8
"MESOS-2807","As a developer I need an easy way to convert MasterInfo protobuf to/from JSON","As a preliminary to MESOS-2340, this requires the implementation of a simple (de)serialization mechanism to JSON from/to {{MasterInfo}} protobuf.",3
"MESOS-2815","Flaky test: FetcherCacheHttpTest.HttpCachedSerialized","FetcherCacheHttpTest.HttpCachedSerialized has been observed to fail (once  so far), but normally works fine. Here is the failure output:  [ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: resourceOffers(0x3cca8e0, @0x2b1053422b20 { 128-byte object <D0-E1 59-49 10-2B 00-00 00-00 00-00 00-00 00-00 10-A1 00-68 10-2B 00-00 A0-DE 00-68 10-2B 00-00 20-DF 00-68 10-2B 00-00 40-9C 00-68 10-2B 00-00 90-2D 00-68 10-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 10-2B 00-00 00-00 00-00 0F-00 00-00> }) Stack trace: F0604 13:08:16.377907  6813 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers *** Check failure stack trace: ***     @     0x2b10488ff6c0  google::LogMessage::Fail()     @     0x2b10488ff60c  google::LogMessage::SendToLog()     @     0x2b10488ff00e  google::LogMessage::Flush()     @     0x2b1048901f22  google::LogMessageFatal::~LogMessageFatal()     @           0x9721e4  _CheckFatal::~_CheckFatal()     @           0xb4da86  mesos::internal::tests::FetcherCacheTest::launchTask()     @           0xb53f8d  mesos::internal::tests::FetcherCacheHttpTest_HttpCachedSerialized_Test::TestBody()     @          0x116ac21  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x1165e1e  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x114e1df  testing::Test::Run()     @          0x114e902  testing::TestInfo::Run()     @          0x114ee8a  testing::TestCase::Run()     @          0x1153b54  testing::internal::UnitTestImpl::RunAllTests()     @          0x116ba93  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x1166b0f  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x1152a60  testing::UnitTest::Run()     @           0xcbc50f  main     @     0x2b104af78ec5  (unknown)     @           0x867559  (unknown) make[4]: *** [check-local] Aborted make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[3]: *** [check-am] Error 2 make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[2]: *** [check] Error 2 make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[1]: *** [check-recursive] Error 1 make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build' make: *** [distcheck] Error 1 ",2
"MESOS-2817","Support revocable/non-revocable CPU updates in Mesos containerizer","MESOS-2652 provided preliminary support for revocable cpu resources only when specified in the initial resources for a container. Improve this to support updates to/from revocable cpu. Note, *any* revocable cpu will result in the entire container's cpu being treated as revocable at the cpu isolator level. Higher level logic is responsible for adding/removing based on some policy.",3
"MESOS-2830","Add an endpoint to slaves to allow launching system administration tasks","As a System Administrator often times I need to run a organization-mandated task on every machine in the cluster. Ideally I could do this within the framework of mesos resources if it is a ""cleanup"" or auditing task, but sometimes I just have to run something, and run it now, regardless if a machine has un-accounted resources  (Ex: Adding/removing a user).  Currently to do this I have to completely bypass Mesos and SSH to the box. Ideally I could tell a mesos slave (With proper authentication) to run a container with the limited special permissions needed to get the task done.",8
"MESOS-2831","FetcherCacheTest.SimpleEviction is flaky","Saw this when reviewbot was testing an unrelated review https://reviews.apache.org/r/35119/  {code} [ RUN      ] FetcherCacheTest.SimpleEviction  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: resourceOffers(0x5365320, @0x2b7bef9f1b20 { 128-byte object <B0-C0 36-E6 7B-2B 00-00 00-00 00-00 00-00 00-00 20-75 00-18 7C-2B 00-00 C0-75 00-18 7C-2B 00-00 60-76 00-18 7C-2B 00-00 00-77 00-18 7C-2B 00-00 40-3A 00-18 7C-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 7C-2B 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 0F-00 00-00> }) Stack trace: F0607 21:19:23.181392  4246 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers *** Check failure stack trace: ***     @     0x2b7be56c5972  google::LogMessage::Fail()     @     0x2b7be56c58be  google::LogMessage::SendToLog()     @     0x2b7be56c52c0  google::LogMessage::Flush()     @     0x2b7be56c81d4  google::LogMessageFatal::~LogMessageFatal()     @           0x97d182  _CheckFatal::~_CheckFatal()     @           0xb58a28  mesos::internal::tests::FetcherCacheTest::launchTask()     @           0xb65b50  mesos::internal::tests::FetcherCacheTest_SimpleEviction_Test::TestBody()     @          0x11923b7  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x118d5b4  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x1175975  testing::Test::Run()     @          0x1176098  testing::TestInfo::Run()     @          0x1176620  testing::TestCase::Run()     @          0x117b2ea  testing::internal::UnitTestImpl::RunAllTests()     @          0x1193229  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x118e2a5  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x117a1f6  testing::UnitTest::Run()     @           0xcc832b  main     @     0x2b7be7d46ec5  (unknown)     @           0x872379  (unknown) {code}",0
"MESOS-2857","FetcherCacheTest.LocalCachedExtract is flaky.","From jenkins:  {noformat} [ RUN      ] FetcherCacheTest.LocalCachedExtract Using temporary directory '/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj' I0610 20:04:48.591573 24561 leveldb.cpp:176] Opened db in 3.512525ms I0610 20:04:48.592456 24561 leveldb.cpp:183] Compacted db in 828630ns I0610 20:04:48.592512 24561 leveldb.cpp:198] Created db iterator in 32992ns I0610 20:04:48.592531 24561 leveldb.cpp:204] Seeked to beginning of db in 8967ns I0610 20:04:48.592545 24561 leveldb.cpp:273] Iterated through 0 keys in the db in 7762ns I0610 20:04:48.592604 24561 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0610 20:04:48.593438 24587 recover.cpp:449] Starting replica recovery I0610 20:04:48.593698 24587 recover.cpp:475] Replica is in EMPTY status I0610 20:04:48.595641 24580 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0610 20:04:48.596086 24590 recover.cpp:195] Received a recover response from a replica in EMPTY status I0610 20:04:48.596607 24590 recover.cpp:566] Updating replica status to STARTING I0610 20:04:48.597507 24590 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 717888ns I0610 20:04:48.597535 24590 replica.cpp:323] Persisted replica status to STARTING I0610 20:04:48.597697 24590 recover.cpp:475] Replica is in STARTING status I0610 20:04:48.599165 24584 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0610 20:04:48.599434 24584 recover.cpp:195] Received a recover response from a replica in STARTING status I0610 20:04:48.599915 24590 recover.cpp:566] Updating replica status to VOTING I0610 20:04:48.600545 24590 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 432335ns I0610 20:04:48.600574 24590 replica.cpp:323] Persisted replica status to VOTING I0610 20:04:48.600659 24590 recover.cpp:580] Successfully joined the Paxos group I0610 20:04:48.600797 24590 recover.cpp:464] Recover process terminated I0610 20:04:48.602905 24594 master.cpp:363] Master 20150610-200448-3875541420-32907-24561 (dbade881e927) started on 172.17.0.231:32907 I0610 20:04:48.602957 24594 master.cpp:365] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --credentials=""/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.23.0/_inst/share/mesos/webui"" --work_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/master"" --zk_session_timeout=""10secs"" I0610 20:04:48.603374 24594 master.cpp:410] Master only allowing authenticated frameworks to register I0610 20:04:48.603392 24594 master.cpp:415] Master only allowing authenticated slaves to register I0610 20:04:48.603404 24594 credentials.hpp:37] Loading credentials for authentication from '/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/credentials' I0610 20:04:48.603751 24594 master.cpp:454] Using default 'crammd5' authenticator I0610 20:04:48.604928 24594 master.cpp:491] Authorization enabled I0610 20:04:48.606034 24593 hierarchical.hpp:309] Initialized hierarchical allocator process I0610 20:04:48.606106 24593 whitelist_watcher.cpp:79] No whitelist given I0610 20:04:48.607430 24594 master.cpp:1476] The newly elected leader is master@172.17.0.231:32907 with id 20150610-200448-3875541420-32907-24561 I0610 20:04:48.607466 24594 master.cpp:1489] Elected as the leading master! I0610 20:04:48.607481 24594 master.cpp:1259] Recovering from registrar I0610 20:04:48.607712 24594 registrar.cpp:313] Recovering registrar I0610 20:04:48.608543 24588 log.cpp:661] Attempting to start the writer I0610 20:04:48.610231 24588 replica.cpp:477] Replica received implicit promise request with proposal 1 I0610 20:04:48.611335 24588 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.086439ms I0610 20:04:48.611382 24588 replica.cpp:345] Persisted promised to 1 I0610 20:04:48.612303 24588 coordinator.cpp:230] Coordinator attemping to fill missing position I0610 20:04:48.613883 24593 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0610 20:04:48.619205 24593 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.228235ms I0610 20:04:48.619257 24593 replica.cpp:679] Persisted action at 0 I0610 20:04:48.621919 24593 replica.cpp:511] Replica received write request for position 0 I0610 20:04:48.621987 24593 leveldb.cpp:438] Reading position from leveldb took 49394ns I0610 20:04:48.622689 24593 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 668412ns I0610 20:04:48.622716 24593 replica.cpp:679] Persisted action at 0 I0610 20:04:48.623507 24584 replica.cpp:658] Replica received learned notice for position 0 I0610 20:04:48.624155 24584 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 612283ns I0610 20:04:48.624186 24584 replica.cpp:679] Persisted action at 0 I0610 20:04:48.624215 24584 replica.cpp:664] Replica learned NOP action at position 0 I0610 20:04:48.625144 24593 log.cpp:677] Writer started with ending position 0 I0610 20:04:48.626724 24589 leveldb.cpp:438] Reading position from leveldb took 72013ns I0610 20:04:48.629276 24591 registrar.cpp:346] Successfully fetched the registry (0B) in 21.520128ms I0610 20:04:48.629663 24591 registrar.cpp:445] Applied 1 operations in 129587ns; attempting to update the 'registry' I0610 20:04:48.632237 24579 log.cpp:685] Attempting to append 131 bytes to the log I0610 20:04:48.632624 24579 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0610 20:04:48.633739 24579 replica.cpp:511] Replica received write request for position 1 I0610 20:04:48.634351 24579 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 583937ns I0610 20:04:48.634382 24579 replica.cpp:679] Persisted action at 1 I0610 20:04:48.635073 24583 replica.cpp:658] Replica received learned notice for position 1 I0610 20:04:48.635442 24583 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 357122ns I0610 20:04:48.635469 24583 replica.cpp:679] Persisted action at 1 I0610 20:04:48.635494 24583 replica.cpp:664] Replica learned APPEND action at position 1 I0610 20:04:48.636337 24583 registrar.cpp:490] Successfully updated the 'registry' in 6.534144ms I0610 20:04:48.636725 24594 log.cpp:704] Attempting to truncate the log to 1 I0610 20:04:48.636858 24583 registrar.cpp:376] Successfully recovered registrar I0610 20:04:48.637073 24594 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0610 20:04:48.637789 24594 master.cpp:1286] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register I0610 20:04:48.638630 24583 replica.cpp:511] Replica received write request for position 2 I0610 20:04:48.639127 24583 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 396272ns I0610 20:04:48.639153 24583 replica.cpp:679] Persisted action at 2 I0610 20:04:48.639804 24583 replica.cpp:658] Replica received learned notice for position 2 I0610 20:04:48.640965 24583 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.147322ms I0610 20:04:48.641054 24583 leveldb.cpp:401] Deleting ~1 keys from leveldb took 72395ns I0610 20:04:48.641197 24583 replica.cpp:679] Persisted action at 2 I0610 20:04:48.641345 24583 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0610 20:04:48.652274 24561 containerizer.cpp:111] Using isolation: posix/cpu,posix/mem I0610 20:04:48.658994 24590 slave.cpp:188] Slave started on 42)@172.17.0.231:32907 I0610 20:04:48.659049 24590 slave.cpp:189] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_sandbox_directory=""/mnt/mesos/sandbox"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.23.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus(*):1000; mem(*):1000"" --revocable_cpu_low_priority=""true"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM"" I0610 20:04:48.659570 24590 credentials.hpp:85] Loading credential for authentication from '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/credential' I0610 20:04:48.659803 24590 slave.cpp:319] Slave using credential for: test-principal I0610 20:04:48.660441 24590 slave.cpp:352] Slave resources: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] I0610 20:04:48.660555 24590 slave.cpp:382] Slave hostname: dbade881e927 I0610 20:04:48.660578 24590 slave.cpp:387] Slave checkpoint: true I0610 20:04:48.661550 24588 state.cpp:35] Recovering state from '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/meta' I0610 20:04:48.661913 24590 status_update_manager.cpp:201] Recovering status update manager I0610 20:04:48.662253 24590 containerizer.cpp:312] Recovering containerizer I0610 20:04:48.663207 24581 slave.cpp:3950] Finished recovery I0610 20:04:48.663761 24581 slave.cpp:4104] Querying resource estimator for oversubscribable resources I0610 20:04:48.664077 24581 slave.cpp:678] New master detected at master@172.17.0.231:32907 I0610 20:04:48.664088 24586 status_update_manager.cpp:175] Pausing sending status updates I0610 20:04:48.664245 24581 slave.cpp:741] Authenticating with master master@172.17.0.231:32907 I0610 20:04:48.664388 24581 slave.cpp:746] Using default CRAM-MD5 authenticatee I0610 20:04:48.664611 24581 slave.cpp:714] Detecting new master I0610 20:04:48.664647 24594 authenticatee.hpp:139] Creating new client SASL connection I0610 20:04:48.664813 24581 slave.cpp:4125] Received oversubscribable resources  from the resource estimator I0610 20:04:48.665060 24581 slave.cpp:4129] No master detected. Re-querying resource estimator after 15secs I0610 20:04:48.665096 24594 master.cpp:4181] Authenticating slave(42)@172.17.0.231:32907 I0610 20:04:48.665247 24581 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(130)@172.17.0.231:32907 I0610 20:04:48.665657 24581 authenticator.cpp:92] Creating new server SASL connection I0610 20:04:48.666013 24581 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5 I0610 20:04:48.666159 24581 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5' I0610 20:04:48.666443 24592 authenticator.cpp:197] Received SASL authentication start I0610 20:04:48.666591 24592 authenticator.cpp:319] Authentication requires more steps I0610 20:04:48.666779 24592 authenticatee.hpp:276] Received SASL authentication step I0610 20:04:48.667007 24585 authenticator.cpp:225] Received SASL authentication step I0610 20:04:48.667043 24585 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0610 20:04:48.667058 24585 auxprop.cpp:173] Looking up auxiliary property '*userPassword' I0610 20:04:48.667110 24585 auxprop.cpp:173] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0610 20:04:48.667142 24585 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0610 20:04:48.667155 24585 auxprop.cpp:123] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0610 20:04:48.667163 24585 auxprop.cpp:123] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0610 20:04:48.667181 24585 authenticator.cpp:311] Authentication success I0610 20:04:48.667331 24585 authenticatee.hpp:316] Authentication success I0610 20:04:48.667414 24585 master.cpp:4211] Successfully authenticated principal 'test-principal' at slave(42)@172.17.0.231:32907 I0610 20:04:48.667505 24585 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(130)@172.17.0.231:32907 I0610 20:04:48.667809 24585 slave.cpp:812] Successfully authenticated with master master@172.17.0.231:32907 I0610 20:04:48.667982 24585 slave.cpp:1146] Will retry registration in 7.257154ms if necessary I0610 20:04:48.668226 24585 master.cpp:3157] Registering slave at slave(42)@172.17.0.231:32907 (dbade881e927) with id 20150610-200448-3875541420-32907-24561-S0 I0610 20:04:48.668737 24585 registrar.cpp:445] Applied 1 operations in 90255ns; attempting to update the 'registry' I0610 20:04:48.672297 24585 log.cpp:685] Attempting to append 305 bytes to the log I0610 20:04:48.672541 24585 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0610 20:04:48.673528 24593 replica.cpp:511] Replica received write request for position 3 I0610 20:04:48.674321 24593 leveldb.cpp:343] Persisting action (324 bytes) to leveldb took 766804ns I0610 20:04:48.674355 24593 replica.cpp:679] Persisted action at 3 I0610 20:04:48.675138 24587 replica.cpp:658] Replica received learned notice for position 3 I0610 20:04:48.675866 24587 leveldb.cpp:343] Persisting action (326 bytes) to leveldb took 714643ns I0610 20:04:48.675897 24587 replica.cpp:679] Persisted action at 3 I0610 20:04:48.675922 24587 replica.cpp:664] Replica learned APPEND action at position 3 I0610 20:04:48.677471 24587 registrar.cpp:490] Successfully updated the 'registry' in 8.656128ms I0610 20:04:48.677759 24587 log.cpp:704] Attempting to truncate the log to 3 I0610 20:04:48.678423 24593 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0610 20:04:48.678621 24587 master.cpp:3214] Registered slave 20150610-200448-3875541420-32907-24561-S0 at slave(42)@172.17.0.231:32907 (dbade881e927) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] I0610 20:04:48.678959 24593 hierarchical.hpp:496] Added slave 20150610-200448-3875541420-32907-24561-S0 (dbade881e927) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] available) I0610 20:04:48.679157 24593 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:48.679183 24593 hierarchical.hpp:852] Performed allocation for slave 20150610-200448-3875541420-32907-24561-S0 in 175519ns I0610 20:04:48.679805 24593 replica.cpp:511] Replica received write request for position 4 I0610 20:04:48.684160 24587 slave.cpp:846] Registered with master master@172.17.0.231:32907; given slave ID 20150610-200448-3875541420-32907-24561-S0 I0610 20:04:48.684229 24587 fetcher.cpp:77] Clearing fetcher cache I0610 20:04:48.684666 24587 slave.cpp:869] Checkpointing SlaveInfo to '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/meta/slaves/20150610-200448-3875541420-32907-24561-S0/slave.info' I0610 20:04:48.687366 24587 slave.cpp:2895] Received ping from slave-observer(42)@172.17.0.231:32907 I0610 20:04:48.687453 24584 status_update_manager.cpp:182] Resuming sending status updates I0610 20:04:48.690901 24593 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 3.385583ms I0610 20:04:48.690975 24593 replica.cpp:679] Persisted action at 4 I0610 20:04:48.692137 24593 replica.cpp:658] Replica received learned notice for position 4 I0610 20:04:48.692603 24593 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 449838ns I0610 20:04:48.692674 24593 leveldb.cpp:401] Deleting ~2 keys from leveldb took 52471ns I0610 20:04:48.692699 24593 replica.cpp:679] Persisted action at 4 I0610 20:04:48.692726 24593 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0610 20:04:48.693544 24561 sched.cpp:157] Version: 0.23.0 I0610 20:04:48.695550 24590 sched.cpp:254] New master detected at master@172.17.0.231:32907 I0610 20:04:48.697090 24590 sched.cpp:310] Authenticating with master master@172.17.0.231:32907 I0610 20:04:48.697136 24590 sched.cpp:317] Using default CRAM-MD5 authenticatee I0610 20:04:48.697511 24586 authenticatee.hpp:139] Creating new client SASL connection I0610 20:04:48.697937 24586 master.cpp:4181] Authenticating scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907 I0610 20:04:48.698185 24584 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(131)@172.17.0.231:32907 I0610 20:04:48.698575 24584 authenticator.cpp:92] Creating new server SASL connection I0610 20:04:48.698807 24584 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5 I0610 20:04:48.699898 24584 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5' I0610 20:04:48.700040 24584 authenticator.cpp:197] Received SASL authentication start I0610 20:04:48.700119 24584 authenticator.cpp:319] Authentication requires more steps I0610 20:04:48.700193 24584 authenticatee.hpp:276] Received SASL authentication step I0610 20:04:48.700287 24584 authenticator.cpp:225] Received SASL authentication step I0610 20:04:48.700320 24584 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0610 20:04:48.700333 24584 auxprop.cpp:173] Looking up auxiliary property '*userPassword' I0610 20:04:48.700392 24584 auxprop.cpp:173] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0610 20:04:48.700425 24584 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0610 20:04:48.700439 24584 auxprop.cpp:123] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0610 20:04:48.700448 24584 auxprop.cpp:123] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0610 20:04:48.700467 24584 authenticator.cpp:311] Authentication success I0610 20:04:48.700640 24584 authenticatee.hpp:316] Authentication success I0610 20:04:48.700742 24584 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(131)@172.17.0.231:32907 I0610 20:04:48.701282 24590 sched.cpp:398] Successfully authenticated with master master@172.17.0.231:32907 I0610 20:04:48.701315 24590 sched.cpp:521] Sending registration request to master@172.17.0.231:32907 I0610 20:04:48.701386 24590 sched.cpp:554] Will retry registration in 1.128089605secs if necessary I0610 20:04:48.701676 24586 master.cpp:4211] Successfully authenticated principal 'test-principal' at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907 I0610 20:04:48.702863 24586 master.cpp:1716] Received registration request for framework 'default' at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907 W0610 20:04:48.702924 24586 master.cpp:1539] Framework at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907 (authenticated as 'test-principal') does not specify principal in its FrameworkInfo I0610 20:04:48.702957 24586 master.cpp:1555] Authorizing framework principal '' to receive offers for role '*' I0610 20:04:48.703580 24586 master.cpp:1783] Registering framework 20150610-200448-3875541420-32907-24561-0000 (default) at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907 with checkpointing enabled and capabilities [  ] I0610 20:04:48.705044 24590 hierarchical.hpp:354] Added framework 20150610-200448-3875541420-32907-24561-0000 I0610 20:04:48.705657 24590 hierarchical.hpp:834] Performed allocation for 1 slaves in 583520ns I0610 20:04:48.707613 24586 master.cpp:4100] Sending 1 offers to framework 20150610-200448-3875541420-32907-24561-0000 (default) at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907 I0610 20:04:48.709035 24590 sched.cpp:448] Framework registered with 20150610-200448-3875541420-32907-24561-0000 I0610 20:04:48.709113 24590 sched.cpp:462] Scheduler::registered took 33214ns  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: resourceOffers(0x3bd7fb0, @0x7fe5bb7af898 { 128-byte object <90-8D 30-CD E5-7F 00-00 00-00 00-00 00-00 00-00 30-E8 00-80 E5-7F 00-00 D0-E8 00-80 E5-7F 00-00 70-E9 00-80 E5-7F 00-00 10-EA 00-80 E5-7F 00-00 F0-58 00-80 E5-7F 00-00 04-00 00-00 04-00 00-00 04-00 00-00 65-45 76-65 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 E5-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 0F-00 00-00> }) Stack trace: I0610 20:04:48.709631 24590 sched.cpp:611] Scheduler::resourceOffers took 189034ns I0610 20:04:49.607378 24589 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:49.607435 24589 hierarchical.hpp:834] Performed allocation for 1 slaves in 474600ns I0610 20:04:50.608489 24582 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:50.608551 24582 hierarchical.hpp:834] Performed allocation for 1 slaves in 517133ns I0610 20:04:51.609849 24589 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:51.609908 24589 hierarchical.hpp:834] Performed allocation for 1 slaves in 440523ns I0610 20:04:52.611188 24584 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:52.611250 24584 hierarchical.hpp:834] Performed allocation for 1 slaves in 471882ns I0610 20:04:53.612911 24581 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:53.612962 24581 hierarchical.hpp:834] Performed allocation for 1 slaves in 411941ns I0610 20:04:54.614280 24582 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:54.614336 24582 hierarchical.hpp:834] Performed allocation for 1 slaves in 448103ns I0610 20:04:55.615985 24583 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:55.616046 24583 hierarchical.hpp:834] Performed allocation for 1 slaves in 494677ns I0610 20:04:56.616896 24580 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:56.616952 24580 hierarchical.hpp:834] Performed allocation for 1 slaves in 461555ns I0610 20:04:57.618814 24587 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:57.618885 24587 hierarchical.hpp:834] Performed allocation for 1 slaves in 491478ns I0610 20:04:58.620564 24589 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:58.620621 24589 hierarchical.hpp:834] Performed allocation for 1 slaves in 434384ns I0610 20:04:59.621649 24584 hierarchical.hpp:933] No resources available to allocate! I0610 20:04:59.621706 24584 hierarchical.hpp:834] Performed allocation for 1 slaves in 453279ns I0610 20:05:00.623241 24593 hierarchical.hpp:933] No resources available to allocate! I0610 20:05:00.623299 24593 hierarchical.hpp:834] Performed allocation for 1 slaves in 423551ns I0610 20:05:01.624984 24590 hierarchical.hpp:933] No resources available to allocate! I0610 20:05:01.625041 24590 hierarchical.hpp:834] Performed allocation for 1 slaves in 458545ns I0610 20:05:02.626266 24591 hierarchical.hpp:933] No resources available to allocate! I0610 20:05:02.626327 24591 hierarchical.hpp:834] Performed allocation for 1 slaves in 490068ns I0610 20:05:03.627702 24593 hierarchical.hpp:933] No resources available to allocate! I0610 20:05:03.627766 24593 hierarchical.hpp:834] Performed allocation for 1 slaves in 473279ns I0610 20:05:03.666060 24581 slave.cpp:4104] Querying resource estimator for oversubscribable resources I0610 20:05:03.666353 24581 slave.cpp:4125] Received oversubscribable resources  from the resource estimator I0610 20:05:03.680258 24588 slave.cpp:2895] Received ping from slave-observer(42)@172.17.0.231:32907 F0610 20:05:03.725155 24561 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers *** Check failure stack trace: ***     @     0x7fe5cc5a2a0d  google::LogMessage::Fail()     @     0x7fe5cc5a1dee  google::LogMessage::SendToLog()     @     0x7fe5cc5a26cd  google::LogMessage::Flush()     @     0x7fe5cc5a5b38  google::LogMessageFatal::~LogMessageFatal()     @           0x8947c7  _CheckFatal::~_CheckFatal()     @           0xadf458  mesos::internal::tests::FetcherCacheTest::launchTask()     @           0xae5ea4  mesos::internal::tests::FetcherCacheTest_LocalCachedExtract_Test::TestBody()     @          0x128fb83  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x127a7e7  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x1261c45  testing::Test::Run()     @          0x126287b  testing::TestInfo::Run()     @          0x1262ec7  testing::TestCase::Run()     @          0x126854a  testing::internal::UnitTestImpl::RunAllTests()     @          0x128c163  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x127c817  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x1268247  testing::UnitTest::Run()     @           0xca398e  main     @     0x7fe5c8436ec5  (unknown)     @           0x749e8c  (unknown) {noformat}  [~bernd-mesos] not sure if there's a ticket capturing this already, sorry if this is a duplicate.",1
"MESOS-2862","mesos-fetcher won't fetch uris which begin with a "" ""","Discovered while running mesos with marathon on top. If I launch a marathon task with a URI which is "" http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz"" mesos will log to stderr:  {code} I0611 22:39:22.815636 35673 logging.cpp:177] Logging to STDERR I0611 22:39:25.643889 35673 fetcher.cpp:214] Fetching URI ' http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz' I0611 22:39:25.648111 35673 fetcher.cpp:94] Hadoop Client not available, skipping fetch with Hadoop Client Failed to fetch:  http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz Failed to synchronize with slave (it's probably exited) {code}  It would be nice if mesos trimmed leading whitespace before doing protocol detection so that simple mistakes are just fixed. ",2
"MESOS-2866","Slave should send oversubscribed resource information after master failover.","After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this.",3
"MESOS-2874","Convert PortMappingStatistics to use automatic JSON encoding/decoding","Simplify PortMappingStatistics by using JSON::Protocol and protobuf::parse to convert ResourceStatistics to/from line format.  This change will simplify the implementation of MESOS-2332.",2
"MESOS-2879","Random recursive_mutex errors in when running make check","While running make check on OS X, from time to time {{recursive_mutex}} errors appear after running all the test successfully. Just one of the experience messages actually stops {{make check}} reporting an error.  The following error messages have been experienced:  {code} libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument   *** Aborted at 1434553937 (unix time) try ""date -d @1434553937"" if you are using GNU date *** {code}  {code} libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument *** Aborted at 1434557001 (unix time) try ""date -d @1434557001"" if you are using GNU date *** libc++abi.dylib: PC: @     0x7fff93855286 __pthread_kill libc++abi.dylib: *** SIGABRT (@0x7fff93855286) received by PID 88060 (TID 0x10fc40000) stack trace: ***     @     0x7fff8e1d6f1a _sigtramp libc++abi.dylib:     @        0x10fc3f1a8 (unknown) libc++abi.dylib:     @     0x7fff979deb53 abort libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentMaking check in include {code}  {code} Assertion failed: (e == 0), function ~recursive_mutex, file /SourceCache/libcxx/libcxx-120/src/mutex.cpp, line 82. *** Aborted at 1434555685 (unix time) try ""date -d @1434555685"" if you are using GNU date *** PC: @     0x7fff93855286 __pthread_kill *** SIGABRT (@0x7fff93855286) received by PID 60235 (TID 0x7fff7ebdc300) stack trace: ***     @     0x7fff8e1d6f1a _sigtramp     @        0x10b512350 google::CheckNotNull<>()     @     0x7fff979deb53 abort     @     0x7fff979a6c39 __assert_rtn     @     0x7fff9bffdcc9 std::__1::recursive_mutex::~recursive_mutex()     @        0x10b881928 process::ProcessManager::~ProcessManager()     @        0x10b874445 process::ProcessManager::~ProcessManager()     @        0x10b874418 process::finalize()     @        0x10b2f7aec main     @     0x7fff98edc5c9 start make[5]: *** [check-local] Abort trap: 6 make[4]: *** [check-am] Error 2 make[3]: *** [check-recursive] Error 1 make[2]: *** [check-recursive] Error 1 make[1]: *** [check] Error 2 make: *** [check-recursive] Error 1 {code}",1
"MESOS-2883","Do not call hook manager if no hooks installed","Hooks modules allow us to provide decorators during various aspects of a task lifecycle such as label decorator, environment decorator, etc.  Often the call into such a decorator hooks results in a new copy of labels, environment, etc., being returned to the call site. This is an unnecessary overhead if there are no hooks installed.  The proper way would be to call decorators via the hook manager only if there are some hooks installed. This would prevent unnecessary copying overhead if no hooks are available.",2
"MESOS-2884","Allow isolators to specify required namespaces","Currently, the LinuxLauncher looks into SlaveFlags to compute the namespaces that should be enabled when launching the executor. This means that a custom Isolator module doesn't have any way to specify dependency on a set of namespaces.  The proposed solution is to extend the Isolator interface to also export the namespaces dependency. This way the MesosContainerizer can directly query all loaded Isolators (inbuilt and custom modules) to compute the set of namespaces required by the executor. This set of namespaces is then passed on to the LinuxLauncher. ",5
"MESOS-2891","Performance regression in hierarchical allocator.","For large clusters, the 0.23.0 allocator cannot keep up with the volume of slaves. After the following slave was re-registered, it took the allocator a long time to work through the backlog of slaves to add:  {noformat:title=45 minute delay} I0618 18:55:40.738399 10172 master.cpp:3419] Re-registered slave 20150422-211121-2148346890-5050-3253-S4695 I0618 19:40:14.960636 10164 hierarchical.hpp:496] Added slave 20150422-211121-2148346890-5050-3253-S4695 {noformat}  Empirically, [addSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L462] and [updateSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L533] have become expensive.  Some timings from a production cluster reveal that the allocator spending in the low tens of milliseconds for each call to {{addSlave}} and {{updateSlave}}, when there are tens of thousands of slaves this amounts to the large delay seen above.  We also saw a slow steady increase in memory consumption, hinting further at a queue backup in the allocator.  A synthetic benchmark like we did for the registrar would be prudent here, along with visibility into the allocator's queue size.",3
"MESOS-2892","Add benchmark for hierarchical allocator.","In light of the performance regression in MESOS-2891, we'd like to have a synthetic benchmark of the allocator code, in order to analyze and direct improvements.",3
"MESOS-2893","Add queue size metrics for the allocator.","In light of the performance regression in MESOS-2891, we'd like to have visibility into the queue size of the allocator. This will enable alerting on performance problems.  We currently have no metrics in the allocator.  I will also look into MESOS-1286 now that we have gcc 4.8, current queue size gauges require a trip through the Process' queue.",1
"MESOS-2902","Enable Mesos to use arbitrary script / module to figure out IP, HOSTNAME","Currently Mesos tries to guess the IP, HOSTNAME by doing a reverse DNS lookup. This doesn't work on a lot of clouds as we want things like public IPs (which aren't the default DNS), there aren't FQDN names (Azure), or the correct way to figure it out is to call some cloud-specific endpoint.  If Mesos / Libprocess could load a mesos-module (Or run a script) which is provided per-cloud, we can figure out perfectly the IP / Hostname for the given environment. It also means we can ship one identical set of files to all hosts in a given provider which doesn't happen to have the DNS scheme + hostnames that libprocess/Mesos expects. Currently we have to generate host-specific config files which Mesos uses to guess.  The host-specific files break / fall apart if machines change IP / hostname without being reinstalled.",5
"MESOS-2903","Network isolator should not fail when target state already exists","Network isolator has multiple instances of the following pattern:  {noformat}   Try<bool> something = ....::create();                                     if (something.isError()) {                                                        ++metrics.something_errors;                                           return Failure(""Failed to create something ..."")   } else if (!icmpVethToEth0.get()) {                                                    ++metrics.adding_veth_icmp_filters_already_exist;                                    return Failure(""Something already exists"");   }                                                                                  {noformat}  These failures have occurred in operation due to the failure to recover or delete an orphan, causing the slave to remain on line but unable to create new resources.    We should convert the second failure message in this pattern to an information message since the final state of the system is the state that we requested.",3
"MESOS-2904","Add slave metric to count container launch failures","We have seen circumstances where a machine has been consistently unable to launch containers due to an inconsistent state (for example, unexpected network configuration).   Adding a metric to track container launch failures will allow us to detect and alert on slaves in such a state.",1
"MESOS-2917","Specify correct libnl version for configure check","Currently configure.ac lists 3.2.24 as the required libnl version. However, https://reviews.apache.org/r/31503 caused the minimum required version to be bumped to 3.2.26. The configure check thus fails to error out during execution and the dependency is captured only during the build step.",1
"MESOS-2925","Invalid usage of ATOMIC_FLAG_INIT in member initialization","The C++ specification states:  The macro ATOMIC_FLAG_INIT shall be defined in such a way that it can be used to initialize an object of type atomic_flag to the clear state. The macro can be used in the form: ""atomic_flag guard = ATOMIC_FLAG_INIT; ""It is unspecified whether the macro can be used in other initialization contexts.""   Clang catches this (although reports it erroneously as a braced scaled init issue) and refuses to compile libprocess.",1
"MESOS-2938","Linux docker inspect crashes","On linux,  when a simple task is being executed on docker container executor, the sandbox stderr shows a backtrace:  *** Aborted at 1435254156 (unix time) try ""date -d @1435254156"" if you are using GNU date *** PC: @     0x7ffff2b1364d (unknown) *** SIGSEGV (@0xfffffffffffffff8) received by PID 88424 (TID 0x7fffe88fb700) from PID 18446744073709551608; stack trace: ***     @     0x7ffff25a4340 (unknown)     @     0x7ffff2b1364d (unknown)     @     0x7ffff2b724df (unknown)     @           0x4a6466 Docker::Container::~Container()     @     0x7ffff5bfa49a Option<>::~Option()     @     0x7ffff5c15989 Option<>::operator=()     @     0x7ffff5c09e9f Try<>::operator=()     @     0x7ffff5c09ee3 Result<>::operator=()     @     0x7ffff5c0a938 process::Future<>::set()     @     0x7ffff5bff412 process::Promise<>::set()     @     0x7ffff5be53e3 Docker::___inspect()     @     0x7ffff5be3cf8 _ZZN6Docker9__inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationENS2_6FutureISsEERKNS2_10SubprocessEENKUlRKSG_E1_clESL_     @     0x7ffff5be91e9 _ZZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferEENUlSM_E_clESM_     @     0x7ffff5be9d9d _ZNSt17_Function_handlerIFvRKN7process6FutureISsEEEZNKS2_5onAnyIZN6Docker9__inspectERKSsRKNS0_5OwnedINS0_7PromiseINS7_9ContainerEEEEERK6OptionI8DurationES2_RKNS0_10SubprocessEEUlS4_E1_vEES4_OT_NS2_6PreferEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_     @     0x7ffff5c1eadd std::function<>::operator()()     @     0x7ffff5c15e07 process::Future<>::onAny()     @     0x7ffff5be93a1 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferE     @     0x7ffff5be87f6 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_EESM_OT_     @     0x7ffff5be459c Docker::__inspect()     @     0x7ffff5be337c _ZZN6Docker8_inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationEENKUlvE_clEv     @     0x7ffff5be8c5a _ZZNK7process6FutureI6OptionIiEE5onAnyIZN6Docker8_inspectERKSsRKNS_5OwnedINS_7PromiseINS5_9ContainerEEEEERKS1_I8DurationEEUlvE_vEERKS3_OT_NS3_10LessPreferEENUlSL_E_clESL_     @     0x7ffff5be9b36 _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyIZN6Docker8_inspectERKSsRKNS0_5OwnedINS0_7PromiseINS9_9ContainerEEEEERKS2_I8DurationEEUlvE_vEES6_OT_NS4_10LessPreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_     @     0x7ffff5c1e9b3 std::function<>::operator()()     @     0x7ffff6184a1a _ZN7process8internal3runISt8functionIFvRKNS_6FutureI6OptionIiEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_     @     0x7ffff617e64d process::Future<>::set()     @     0x7ffff6752e46 process::Promise<>::set()     @     0x7ffff675faec process::internal::cleanup()     @     0x7ffff6765293 _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EE6__callIvIS6_EILm0ELm1ELm2EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE     @     0x7ffff6764bcd _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EEclIJS6_EvEET0_DpOT_     @     0x7ffff67642a5 _ZZNK7process6FutureI6OptionIiEE5onAnyISt5_BindIFPFvRKS3_PNS_7PromiseIS2_EERKNS_10SubprocessEESt12_PlaceholderILi1EESA_SB_EEvEES7_OT_NS3_6PreferEENUlS7_E_clES7_     @     0x7ffff676531d _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyISt5_BindIFPFvS6_PNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EESC_SD_EEvEES6_OT_NS4_6PreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_     @     0x7ffff5c1e9b3 std::function<>::operator()() (END)  ",1
"MESOS-2940","Reconciliation is expensive for large numbers of tasks.","We've observed that both implicit and explicit reconciliation are expensive for large numbers of tasks:  {noformat: title=Explicit O(100,000) tasks: 70secs} I0625 20:55:23.716320 21937 master.cpp:3863] Performing explicit task state reconciliation for N tasks of framework F (NAME) at S@IP:PORT I0625 20:56:34.812464 21937 master.cpp:5041] Removing task T with resources R of framework F on slave S at slave(1)@IP:PORT (HOST) {noformat}  {noformat: title=Implicit with O(100,000) tasks: 60secs} I0625 20:25:22.310601 21936 master.cpp:3802] Performing implicit task state reconciliation for framework F (NAME) at S@IP:PORT I0625 20:26:23.874528 21921 master.cpp:218] Scheduling shutdown of slave S due to health check timeout {noformat}  Let's add a benchmark to see if there are any bottlenecks here, and to guide improvements.",3
"MESOS-2950","Implement current mesos Authorizer in terms of generalized Authorizer interface","In order to maintain compatibility with existent versions of Mesos, as well as to prove the flexibility of the generalized {{mesos::Authorizer}} design, the current authorization mechanism through ACL definitions needs to run under the updated interface without any changes being noticeable by the current authorization users.",8
"MESOS-2951","Inefficient container usage collection","docker containerizer currently collects usage statistics by calling os's process statistics (eg ps ). There is scope for making this efficient, say by querying cgroups file system.  ",3
"MESOS-2957","Add version to MasterInfo","This will help schedulers figure out the version of the master that they are interacting with. See MESOS-2736 for additional context.",1
"MESOS-2962","Slave fails with Abort stacktrace when DNS cannot resolve hostname","If the DNS cannot resolve the hostname-to-IP for a slave node, we correctly return an {{Error}} object, but we then fail with a segfault.  This code adds a more user-friendly message and exits normally (with an {{EXIT_FAILURE}} code).  For example, forcing {{net::getIp()}} to always return an {{Error}}, now causes the slave to exit like this:  {noformat} $ ./bin/mesos-slave.sh --master=10.10.1.121:5405 WARNING: Logging before InitGoogleLogging() is written to STDERR E0630 11:31:45.777465 1944417024 process.cpp:899] Could not obtain the IP address for stratos.local; the DNS service may not be able to resolve it: >>> Marco was here!!!  $ echo $? 1 {noformat}",1
"MESOS-2966","socket::peer() and socket::address() might fail with SSL enabled","libevent SSL currently uses a secondary FD so we need to virtualize the get() function on socket interface. ",5
"MESOS-2967","Missing doxygen documentation for libprocess socket interface ","Convert existing comments to doxygen format.  ",5
"MESOS-2986","Docker version output is not compatible with Mesos","We currently use docker version to get Docker version, in Docker master branch and soon in Docker 1.8 [1] the output for this command changes. The solution for now will be to use the unchanged docker --version output, in the long term we should consider stop using the cli and use the API instead.   [1] https://github.com/docker/docker/pull/14047",1
"MESOS-2993","Document  per container unique egress flow and network queueing statistics","Document new network isolation capabilities in 0.23",3
"MESOS-2997","SSL connection failure causes failed CHECK.","{code} [ RUN      ] SSLTest.BasicSameProcess F0706 18:32:28.465451 238583808 libevent_ssl_socket.cpp:507] Check failed: 'self->bev' Must be non NULL {code}",3
"MESOS-3002","Rename Option<T>::get(const T& _t) to getOrElse() broke network isolator","Change to Option from get() to getOrElse() breaks network isolator.  Building with '../configure --with-network-isolator' generates the following error:  ../../src/slave/containerizer/isolators/network/port_mapping.cpp: In static member function 'static Try<mesos::slave::Isolator*> mesos::internal::slave::PortMappingIsolatorProcess::create(const mesos::internal::slave::Flags&)': ../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: error: no matching function for call to 'Option<std::basic_string<char> >::get(const char [1]) const'        flags.resources.get(""""),                              ^ ../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: note: candidates are: In file included from ../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp:26:0,                  from ../../3rdparty/libprocess/include/process/check.hpp:19,                  from ../../3rdparty/libprocess/include/process/collect.hpp:7,                  from ../../src/slave/containerizer/isolators/network/port_mapping.cpp:30: ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note: const T& Option<T>::get() const [with T = std::basic_string<char>]    const T& get() const { assert(isSome()); return t; }             ^ ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note:   candidate expects 0 arguments, 1 provided ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note: T& Option<T>::get() [with T = std::basic_string<char>]    T& get() { assert(isSome()); return t; }       ^ ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note:   candidate expects 0 arguments, 1 provided make[2]: *** [slave/containerizer/isolators/network/libmesos_no_3rdparty_la-port_mapping.lo] Error 1 make[2]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src' make[1]: *** [check] Error 2 make[1]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src' make: *** [check-recursive] Error 1 ",1
"MESOS-3005","SSL tests can fail depending on hostname configuration","Depending on how /etc/hosts is configured, the SSL tests can fail with a bad hostname match for the certificate. We can avoid this by explicitly matching the hostname for the certificate to the IP that will be used during the test.",3
"MESOS-3032","Document containerizer launch ","We currently dont have enough documentation for the containerizer component. This task adds documentation for containerizer launch sequence. The mail goals are: - Have diagrams (state, sequence, class etc) depicting the containerizer launch process. - Make the documentation newbie friendly. - Usable for future design discussions.",3
"MESOS-3046","Stout's UUID re-seeds a new random generator during each call to UUID::random.","Per [~StephanErb] and [~kevints]'s observations on MESOS-2940, stout's UUID abstraction is re-seeding the random generator during each call to {{UUID::random()}}, which is really expensive.  This is confirmed in the perf graph from MESOS-2940.",3
"MESOS-3051","performance issues with port ranges comparison","Testing in an environment with lots of frameworks (>200), where the frameworks permanently decline resources they don't need. The allocator ends up spending a lot of time figuring out whether offers are refused (the code path through {{HierarchicalAllocatorProcess::isFiltered()}}.  In profiling a synthetic benchmark, it turns out that comparing port ranges is very expensive, involving many temporary allocations. 61% of Resources::contains() run time is in operator -= (Resource). 35% of Resources::contains() run time is in Resources::_contains().  The heaviest call chain through {{Resources::_contains}} is:  {code} Running Time          Self (ms)         Symbol Name 7237.0ms   35.5%          4.0            mesos::Resources::_contains(mesos::Resource const&) const 7200.0ms   35.3%          1.0             mesos::contains(mesos::Resource const&, mesos::Resource const&) 7133.0ms   35.0%        121.0              mesos::operator<=(mesos::Value_Ranges const&, mesos::Value_Ranges const&) 6319.0ms   31.0%          7.0               mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Ranges const&) 6240.0ms   30.6%        161.0                mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&) 1867.0ms    9.1%         25.0                 mesos::Value_Ranges::add_range() 1694.0ms    8.3%          4.0                 mesos::Value_Ranges::~Value_Ranges() 1495.0ms    7.3%         16.0                 mesos::Value_Ranges::operator=(mesos::Value_Ranges const&)  445.0ms    2.1%         94.0                 mesos::Value_Range::MergeFrom(mesos::Value_Range const&)  154.0ms    0.7%         24.0                 mesos::Value_Ranges::range(int) const  103.0ms    0.5%         24.0                 mesos::Value_Ranges::range_size() const   95.0ms    0.4%          2.0                 mesos::Value_Range::Value_Range(mesos::Value_Range const&)   59.0ms    0.2%          4.0                 mesos::Value_Ranges::Value_Ranges()   50.0ms    0.2%         50.0                 mesos::Value_Range::begin() const   28.0ms    0.1%         28.0                 mesos::Value_Range::end() const   26.0ms    0.1%          0.0                 mesos::Value_Range::~Value_Range() {code}  mesos::coalesce(Value_Ranges) gets done a lot and ends up being really expensive. The heaviest parts of the inverted call chain are:  {code} Running Time Self (ms)  Symbol Name 3209.0ms   15.7% 3209.0   mesos::Value_Range::~Value_Range() 3209.0ms   15.7% 0.0    google::protobuf::internal::GenericTypeHandler<mesos::Value_Range>::Delete(mesos::Value_Range*) 3209.0ms   15.7% 0.0     void google::protobuf::internal::RepeatedPtrFieldBase::Destroy<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>() 3209.0ms   15.7% 0.0      google::protobuf::RepeatedPtrField<mesos::Value_Range>::~RepeatedPtrField() 3209.0ms   15.7% 0.0       google::protobuf::RepeatedPtrField<mesos::Value_Range>::~RepeatedPtrField() 3209.0ms   15.7% 0.0        mesos::Value_Ranges::~Value_Ranges() 3209.0ms   15.7% 0.0         mesos::Value_Ranges::~Value_Ranges() 2441.0ms   11.9% 0.0          mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)  452.0ms    2.2% 0.0          mesos::remove(mesos::Value_Ranges*, mesos::Value_Range const&)  169.0ms    0.8% 0.0          mesos::operator<=(mesos::Value_Ranges const&, mesos::Value_Ranges const&)   82.0ms    0.4% 0.0          mesos::operator-=(mesos::Value_Ranges&, mesos::Value_Ranges const&)   65.0ms    0.3% 0.0          mesos::Value_Ranges::~Value_Ranges()  2541.0ms   12.4% 2541.0   google::protobuf::internal::GenericTypeHandler<mesos::Value_Range>::New() 2541.0ms   12.4% 0.0    google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>() 2305.0ms   11.3% 0.0     google::protobuf::RepeatedPtrField<mesos::Value_Range>::Add() 2305.0ms   11.3% 0.0      mesos::Value_Ranges::add_range() 1962.0ms    9.6% 0.0       mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)  343.0ms    1.6% 0.0       mesos::ranges::add(mesos::Value_Ranges*, long long, long long)  236.0ms    1.1% 0.0     void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&) 1471.0ms    7.2% 1471.0   google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int) 1333.0ms    6.5% 0.0    google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>() 1333.0ms    6.5% 0.0     google::protobuf::RepeatedPtrField<mesos::Value_Range>::Add() 1333.0ms    6.5% 0.0      mesos::Value_Ranges::add_range() 1086.0ms    5.3% 0.0       mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)  247.0ms    1.2% 0.0       mesos::ranges::add(mesos::Value_Ranges*, long long, long long)  107.0ms    0.5% 0.0    void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&) 107.0ms    0.5% 0.0     google::protobuf::RepeatedPtrField<mesos::Value_Range>::MergeFrom(google::protobuf::RepeatedPtrField<mesos::Value_Range> const&) 107.0ms    0.5% 0.0      mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&) 105.0ms    0.5% 0.0       mesos::Value_Ranges::CopyFrom(mesos::Value_Ranges const&) 105.0ms    0.5% 0.0        mesos::Value_Ranges::operator=(mesos::Value_Ranges const&) 104.0ms    0.5% 0.0         mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)   1.0ms    0.0% 0.0         mesos::remove(mesos::Value_Ranges*, mesos::Value_Range const&)   2.0ms    0.0% 0.0       mesos::Resource::MergeFrom(mesos::Resource const&)   2.0ms    0.0% 0.0        google::protobuf::internal::GenericTypeHandler<mesos::Resource>::Merge(mesos::Resource const&, mesos::Resource*)   2.0ms    0.0% 0.0         void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Resource>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&)  29.0ms    0.1% 0.0    void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Resource>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&)  898.0ms    4.4% 898.0   google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>() 517.0ms    2.5% 0.0    google::protobuf::RepeatedPtrField<mesos::Value_Range>::Add() 517.0ms    2.5% 0.0     mesos::Value_Ranges::add_range() 429.0ms    2.1% 0.0      mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)  88.0ms    0.4% 0.0      mesos::ranges::add(mesos::Value_Ranges*, long long, long long) 379.0ms    1.8% 0.0    void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&) {code} ",8
"MESOS-3060","FTP response code for success not recognized by fetcher.","The response code for successful HTTP requests is 200, the response code for successful FTP file transfers is 226. The fetcher currently only checks for a response code of 200 even for FTP URIs. This results in failed fetching even though the resource gets downloaded successfully. This has been found by a dedicated external test using an FTP server. ",1
"MESOS-3101","Standardize separation of Windows/Linux-specific OS code","There are 50+ files that must be touched to separate OS-specific code.  First, we will standardize the changes by using stout/abort.hpp as an example. The review/discussion can be found here: https://reviews.apache.org/r/36625/",3
"MESOS-3102","Separate OS-specific code in the stout library","This issue tracks changes for all files under {{3rdparty/libprocess/3rdparty/stout/}}  The changes will be based on this commit: https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c",5
"MESOS-3112","Fetcher should perform cache eviction based on cache file usage patterns.","Currently, the fetcher uses a trivial strategy to select eviction victims: it picks the first cache file it finds in linear iteration. This means that potentially a file that has just been used gets evicted the next moment. This performance loss can be avoided by even the simplest enhancement of the selection procedure.  Proposed approach: determine an effective yet relatively uncomplex and quick algorithm and implement it in `FetcherProcess::Cache::selectVictims(const Bytes& requiredSpace)`. Suggestion: approximate MRU-retention somehow.  Unit-test what actually happens!",8
"MESOS-3121","Always disable SSLV2","The SSL protocol mismatch tests are failing on Centos7 when matching SSLV2 with SSLV2. Since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise.",2
"MESOS-3122","Add configurable UNIMPLEMENTED macro to stout","During the transition to support for windows, it would be great if we had the ability to use a macro that marks functions as un-implemented. To support being able to find all the unimplemented functions easily at compile time, while also being able to run the tests at the same time, we can add a configuration flag that controls whether this macro aborts or expands to a static assertion.",2
"MESOS-3127","Improve task reconciliation documentation.","Include additional information about task reconciliation that explain why the master may not return the states of all tasks immediately and why an explicit task reconciliation algorithm is necessary.",1
"MESOS-3129","Move all MesosContainerizer related files under src/slave/containerizer/mesos","Currently, some MesosContainerizer specific files are not in the correct location. For example: {noformat}  src/slave/containerizer/isolators/* src/slave/containerizer/provisioner.hpp|cpp {noformat}  They should be put under src/slave/containerizer/mesos/",2
"MESOS-3132","Allow slave to forward messages through the master for HTTP schedulers.","The master currently has no install handler for {{ExecutorToFramework}} messages and the slave directly sends these messages to the scheduler driver, bypassing the master entirely.  We need to preserve this behavior for the driver, but HTTP schedulers will not have a libprocess 'pid'. We'll have to ensure that the {{RunTaskMessage}} and {{UpdateFrameworkMessage}} have an optional pid. For now the master will continue to set the pid, but 0.24.0 slaves will know to send messages through the master when the 'pid' is not available.",5
"MESOS-3134","Port bootstrap to CMake","Bootstrap does a lot of significant things, like setting up the git commit hooks. We will want something like bootstrap to run also on systems that don't have bash -- ideally this should just run in CMake itself.",5
"MESOS-3135","Publish MasterInfo to ZK using JSON","Following from MESOS-2340, which now allows Master to correctly decode JSON information ({{MasterInfo}}) published to Zookeeper, we can now enable the Master Leader Contender to serialize it too in JSON.",2
"MESOS-3142","As a Developer I want a better way to run shell commands","When reviewing the code in [r/36425|https://reviews.apache.org/r/36425/] [~benjaminhindman] noticed that there is a better abstraction that is possible to introduce for {{os::shell()}} that will simplify the caller's life.  Instead of having to handle all possible outcomes, we propose to refactor {{os::shell()}} as follows:  {code} /**  * Returns the output from running the specified command with the shell.  */ Try<std::string> shell(const string& command) {   // Actually handle the WIFEXITED, WIFSIGNALED here! } {code}  where the returned string is {{stdout}} and, should the program be signaled, or exit with a non-zero exit code, we will simply return a {{Failure}} with an error message that will encapsulate both the returned/signaled state, and, possibly {{stderr}}.  And some test driven development: {code} EXPECT_ERROR(os::shell(""false"")); EXPECT_SOME(os::shell(""true""));  EXPECT_SOME_EQ(""hello world"", os::shell(""echo hello world"")); {code}  Alternatively, the caller can ask to have {{stderr}} conflated with {{stdout}}: {code} Try<string> outAndErr = os::shell(""myCmd --foo 2>&1""); {code}  However, {{stderr}} will be ignored by default: {code} // We don't read standard error by default. EXPECT_SOME_EQ("""", os::shell(""echo hello world 1>&2""));  // We don't even read stderr if something fails (to return in Try::error). Try<string> output = os::shell(""echo hello world 1>&2 && false""); EXPECT_ERROR(output); EXPECT_FALSE(strings::contains(output.error(), ""hello world"")); {code}  An analysis of existing usage shows that in almost all cases, the caller only cares {{if not error}}; in fact, the actual exit code is read only once, and even then, in a test case.  We believe this will simplify the API to the caller, and will significantly reduce the length and complexity at the calling sites (<6 LOC against the current 20+).",2
"MESOS-3143","Disable endpoints rule fails to recognize HTTP path delegates","In mesos, one can use the flag {{--firewall_rules}} to disable endpoints. Disabled endpoints will return a _403 Forbidden_ response whenever someone tries to access endpoints.  Libprocess support adding one default delegate for endpoints, which is the default process id which handles endpoints if no process id was given. For example, the default id of the master libprocess process is {{master}} which is also set as the delegate for the master system process, so a request to the endpoint {{http://master-address:5050/state.json}} will effectively be resolved by {{http://master-address:5050/master/state.json}}. But if one disables  {{/state.json}} because of how delegates work, it can still access {{/master/state.json}}.  The only workaround is to disabled both enpoints.",2
"MESOS-3154","Enable Mesos Agent Node to use arbitrary script / module to figure out IP, HOSTNAME","Following from MESOS-2902 we want to enable the same functionality in the Mesos Agents too.  This is probably best done once we implement the new {{os::shell}} semantics, as described in MESOS-3142.",1
"MESOS-3158","Libprocess Process: Join runqueue workers during finalization","The lack of synchronization between ProcessManager destruction and the thread pool threads running the queued processes means that the shared state that is part of the ProcessManager gets destroyed prematurely. Synchronizing the ProcessManager destructor with draining the work queues and stopping the workers will allow us to not require leaking the shared state to avoid use beyond destruction.",3
"MESOS-3162","Provide a means to check http connection equality for streaming connections.","If one uses an http::Pipe::Writer to stream a response, one cannot compare the writer with another to see if the connection has changed.  This is useful for example, in the master's http api when there is asynchronous disconnection logic. When we handle the disconnection, it's possible for the scheduler to have re-subscribed, and so the master needs to tell if the disconnection event is relevant for the current connection before taking action.",3
"MESOS-3164","Introduce QuotaInfo message","A {{QuotaInfo}} protobuf message is internal representation for quota related information (e.g. for persisting quota). The protobuf message should be extendable for future needs and allows for easy aggregation across roles and operator principals. It may also be used to pass quota information to allocators.",3
"MESOS-3168","MesosZooKeeperTest fixture can have side effects across tests","MesosZooKeeperTest fixture doesn't restart the ZooKeeper server for each test. This means if a test shuts down the ZooKeeper server, the next test (using the same fixture) might fail.   For an example see https://reviews.apache.org/r/36807/",2
"MESOS-3169","FrameworkInfo should only be updated if the re-registration is valid","See Ben Mahler's comment in https://reviews.apache.org/r/32961/ FrameworkInfo should not be updated if the re-registration is invalid. This can happen in a few cases under the branching logic, so this requires some refactoring. Notice that a {code}FrameworkErrorMessage{code} can be generated  both inside {code}else if (from != framework->pid){code} as well as from inside {code}failoverFramework(framework, from);{code}",2
"MESOS-3173","Mark Path::basename, Path::dirname as const functions.","The functions Path::basename and Path::dirname in stout/path.hpp are not marked const, although they could. Marking them const would remove some ambiguities in the usage of these functions.",1
"MESOS-3174","Fetcher logs erroneous message when successfully extracting an archive","When fetching an asset while not using the cache, the fetcher may erroneously report this: ""Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: "".  This message appears in the stderr log in the sandbox no matter whether extraction succeeded or not. It should be absent after successful extraction. ",1
"MESOS-3185","Refactor Subprocess logic in linux/perf.cpp to use common subroutine","MESOS-2834 will enhance the perf isolator to support the different output formats provided by difference kernel versions.  In order to achieve this, it requires to execute the ""perf --version"" command.   We should decompose the existing Subcommand processing in perf so that we can share the implementation between the multiple uses of perf.",3
"MESOS-3189","TimeTest.Now fails with --enable-libevent","[ RUN      ] TimeTest.Now ../../../3rdparty/libprocess/src/tests/time_tests.cpp:50: Failure Expected: (Microseconds(10)) < (Clock::now() - t1), actual: 8-byte object <10-27 00-00 00-00 00-00> vs 0ns [  FAILED  ] TimeTest.Now (0 ms)",2
"MESOS-3200","Remove unused 'fatal' and 'fatalerror' macros","There exist {{fatal}} and {{fatalerror}} macros in both {{libprocess}} and {{stout}}. None of them are currently used as we favor {{glog}}'s {{LOG(FATAL)}}, and therefore should be removed.",1
"MESOS-3201","Libev handle_async can deadlock with run_in_event_loop","Due to the arbitrary nature of the functions that are executed in handle_async, invoking them under the (A) {{watchers_mutex}} can lead to deadlocks if (B) is acquired before calling {{run_in_event_loop}} and (B) is also acquired within the arbitrary function. {code} ==82679== Thread #10: lock order ""0x60774F8 before 0x60768C0"" violated ==82679==  ==82679== Observed (incorrect) order is: acquisition of lock at 0x60768C0 ==82679==    at 0x4C32145: pthread_mutex_lock (in /usr/lib/valgrind/vgpreload_helgrind-amd64-linux.so) ==82679==    by 0x692C9B: __gthread_mutex_lock(pthread_mutex_t*) (gthr-default.h:748) ==82679==    by 0x6950BF: std::mutex::lock() (mutex:134) ==82679==    by 0x696219: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*)::{lambda(std::mutex*)#1}::operator()(std::mutex*) const (synchronized.hpp:58) ==82679==    by 0x696238: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*)::{lambda(std::mutex*)#1}::_FUN(std::mutex*) (synchronized.hpp:58) ==82679==    by 0x6984CF: Synchronized<std::mutex>::Synchronized(std::mutex*, void (*)(std::mutex*), void (*)(std::mutex*)) (synchronized.hpp:35) ==82679==    by 0x6962DE: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*) (synchronized.hpp:60) ==82679==    by 0x728FE1: process::handle_async(ev_loop*, ev_async*, int) (libev.cpp:48) ==82679==    by 0x761384: ev_invoke_pending (ev.c:2994) ==82679==    by 0x7643C4: ev_run (ev.c:3394) ==82679==    by 0x728E37: ev_loop (ev.h:826) ==82679==    by 0x729469: process::EventLoop::run() (libev.cpp:135) ==82679==  ==82679==  followed by a later acquisition of lock at 0x60774F8 ==82679==    at 0x4C32145: pthread_mutex_lock (in /usr/lib/valgrind/vgpreload_helgrind-amd64-linux.so) ==82679==    by 0x4C6F9D: __gthread_mutex_lock(pthread_mutex_t*) (gthr-default.h:748) ==82679==    by 0x4C6FED: __gthread_recursive_mutex_lock(pthread_mutex_t*) (gthr-default.h:810) ==82679==    by 0x4F5D3D: std::recursive_mutex::lock() (mutex:175) ==82679==    by 0x516513: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*)::{lambda(std::recursive_mutex*)#1}::operator()(std::recursive_mutex*) const (synchronized.hpp:58) ==82679==    by 0x516532: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*)::{lambda(std::recursive_mutex*)#1}::_FUN(std::recursive_mutex*) (synchronized.hpp:58) ==82679==    by 0x52E619: Synchronized<std::recursive_mutex>::Synchronized(std::recursive_mutex*, void (*)(std::recursive_mutex*), void (*)(std::recursive_mutex*)) (synchronized.hpp:35) ==82679==    by 0x5165D4: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*) (synchronized.hpp:60) ==82679==    by 0x6BF4E1: process::ProcessManager::use(process::UPID const&) (process.cpp:2127) ==82679==    by 0x6C2B8C: process::ProcessManager::terminate(process::UPID const&, bool, process::ProcessBase*) (process.cpp:2604) ==82679==    by 0x6C6C3C: process::terminate(process::UPID const&, bool) (process.cpp:3107) ==82679==    by 0x692B65: process::Latch::trigger() (latch.cpp:53) {code}  This was introduced in https://github.com/apache/mesos/commit/849fc4d361e40062073324153ba97e98e294fdf2",3
"MESOS-3203","MasterAuthorizationTest.DuplicateRegistration test is flaky","[ RUN      ] MasterAuthorizationTest.DuplicateRegistration Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7' I0804 22:16:01.578500 26185 leveldb.cpp:176] Opened db in 2.188338ms I0804 22:16:01.579172 26185 leveldb.cpp:183] Compacted db in 645075ns I0804 22:16:01.579211 26185 leveldb.cpp:198] Created db iterator in 15766ns I0804 22:16:01.579227 26185 leveldb.cpp:204] Seeked to beginning of db in 1658ns I0804 22:16:01.579238 26185 leveldb.cpp:273] Iterated through 0 keys in the db in 313ns I0804 22:16:01.579282 26185 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0804 22:16:01.579787 26212 recover.cpp:449] Starting replica recovery I0804 22:16:01.580075 26212 recover.cpp:475] Replica is in EMPTY status I0804 22:16:01.581014 26205 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0804 22:16:01.581357 26211 recover.cpp:195] Received a recover response from a replica in EMPTY status I0804 22:16:01.581761 26207 recover.cpp:566] Updating replica status to STARTING I0804 22:16:01.582334 26218 master.cpp:377] Master 20150804-221601-2550141356-59302-26185 (d6d349cd895b) started on 172.17.0.152:59302 I0804 22:16:01.582355 26218 master.cpp:379] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --credentials=""/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.24.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/master"" --zk_session_timeout=""10secs"" I0804 22:16:01.582711 26218 master.cpp:424] Master only allowing authenticated frameworks to register I0804 22:16:01.582722 26218 master.cpp:429] Master only allowing authenticated slaves to register I0804 22:16:01.582728 26218 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials' I0804 22:16:01.582929 26204 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421543ns I0804 22:16:01.582950 26204 replica.cpp:323] Persisted replica status to STARTING I0804 22:16:01.583032 26218 master.cpp:468] Using default 'crammd5' authenticator I0804 22:16:01.583132 26211 recover.cpp:475] Replica is in STARTING status I0804 22:16:01.583154 26218 master.cpp:505] Authorization enabled I0804 22:16:01.583356 26214 whitelist_watcher.cpp:79] No whitelist given I0804 22:16:01.583411 26217 hierarchical.hpp:346] Initialized hierarchical allocator process I0804 22:16:01.583976 26213 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0804 22:16:01.584187 26209 recover.cpp:195] Received a recover response from a replica in STARTING status I0804 22:16:01.584581 26213 master.cpp:1495] The newly elected leader is master@172.17.0.152:59302 with id 20150804-221601-2550141356-59302-26185 I0804 22:16:01.584609 26213 master.cpp:1508] Elected as the leading master! I0804 22:16:01.584627 26213 master.cpp:1278] Recovering from registrar I0804 22:16:01.584656 26204 recover.cpp:566] Updating replica status to VOTING I0804 22:16:01.584770 26212 registrar.cpp:313] Recovering registrar I0804 22:16:01.585261 26218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 370526ns I0804 22:16:01.585285 26218 replica.cpp:323] Persisted replica status to VOTING I0804 22:16:01.585412 26216 recover.cpp:580] Successfully joined the Paxos group I0804 22:16:01.585667 26216 recover.cpp:464] Recover process terminated I0804 22:16:01.586047 26213 log.cpp:661] Attempting to start the writer I0804 22:16:01.587164 26211 replica.cpp:477] Replica received implicit promise request with proposal 1 I0804 22:16:01.587549 26211 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 358261ns I0804 22:16:01.587568 26211 replica.cpp:345] Persisted promised to 1 I0804 22:16:01.588173 26209 coordinator.cpp:230] Coordinator attemping to fill missing position I0804 22:16:01.589316 26208 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0804 22:16:01.589700 26208 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 351778ns I0804 22:16:01.589721 26208 replica.cpp:679] Persisted action at 0 I0804 22:16:01.590698 26213 replica.cpp:511] Replica received write request for position 0 I0804 22:16:01.590754 26213 leveldb.cpp:438] Reading position from leveldb took 31557ns I0804 22:16:01.591147 26213 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 321842ns I0804 22:16:01.591167 26213 replica.cpp:679] Persisted action at 0 I0804 22:16:01.591790 26217 replica.cpp:658] Replica received learned notice for position 0 I0804 22:16:01.592133 26217 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 315281ns I0804 22:16:01.592155 26217 replica.cpp:679] Persisted action at 0 I0804 22:16:01.592180 26217 replica.cpp:664] Replica learned NOP action at position 0 I0804 22:16:01.592686 26211 log.cpp:677] Writer started with ending position 0 I0804 22:16:01.593729 26205 leveldb.cpp:438] Reading position from leveldb took 26394ns I0804 22:16:01.596165 26209 registrar.cpp:346] Successfully fetched the registry (0B) in 11.343104ms I0804 22:16:01.596281 26209 registrar.cpp:445] Applied 1 operations in 26242ns; attempting to update the 'registry' I0804 22:16:01.598415 26212 log.cpp:685] Attempting to append 178 bytes to the log I0804 22:16:01.598563 26215 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0804 22:16:01.599324 26215 replica.cpp:511] Replica received write request for position 1 I0804 22:16:01.599778 26215 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 420523ns I0804 22:16:01.599800 26215 replica.cpp:679] Persisted action at 1 I0804 22:16:01.600349 26204 replica.cpp:658] Replica received learned notice for position 1 I0804 22:16:01.600684 26204 leveldb.cpp:343] Persisting action (199 bytes) to leveldb took 310315ns I0804 22:16:01.600706 26204 replica.cpp:679] Persisted action at 1 I0804 22:16:01.600723 26204 replica.cpp:664] Replica learned APPEND action at position 1 I0804 22:16:01.601632 26213 registrar.cpp:490] Successfully updated the 'registry' in 5.287936ms I0804 22:16:01.601747 26213 registrar.cpp:376] Successfully recovered registrar I0804 22:16:01.601826 26215 log.cpp:704] Attempting to truncate the log to 1 I0804 22:16:01.601948 26210 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0804 22:16:01.602145 26208 master.cpp:1305] Recovered 0 slaves from the Registry (139B) ; allowing 10mins for slaves to re-register I0804 22:16:01.602859 26219 replica.cpp:511] Replica received write request for position 2 I0804 22:16:01.603181 26219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284713ns I0804 22:16:01.603209 26219 replica.cpp:679] Persisted action at 2 I0804 22:16:01.603984 26211 replica.cpp:658] Replica received learned notice for position 2 I0804 22:16:01.604313 26211 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 302445ns I0804 22:16:01.604365 26211 leveldb.cpp:401] Deleting ~1 keys from leveldb took 29354ns I0804 22:16:01.604387 26211 replica.cpp:679] Persisted action at 2 I0804 22:16:01.604408 26211 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0804 22:16:01.616402 26185 sched.cpp:164] Version: 0.24.0 I0804 22:16:01.616902 26209 sched.cpp:262] New master detected at master@172.17.0.152:59302 I0804 22:16:01.617000 26209 sched.cpp:318] Authenticating with master master@172.17.0.152:59302 I0804 22:16:01.617019 26209 sched.cpp:325] Using default CRAM-MD5 authenticatee I0804 22:16:01.617324 26212 authenticatee.cpp:115] Creating new client SASL connection I0804 22:16:01.617550 26209 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.617641 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(259)@172.17.0.152:59302 I0804 22:16:01.617858 26208 authenticator.cpp:92] Creating new server SASL connection I0804 22:16:01.618140 26216 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0804 22:16:01.618191 26216 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0804 22:16:01.618324 26213 authenticator.cpp:197] Received SASL authentication start I0804 22:16:01.618413 26213 authenticator.cpp:319] Authentication requires more steps I0804 22:16:01.618557 26216 authenticatee.cpp:252] Received SASL authentication step I0804 22:16:01.618664 26216 authenticator.cpp:225] Received SASL authentication step I0804 22:16:01.618703 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0804 22:16:01.618719 26216 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0804 22:16:01.618778 26216 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0804 22:16:01.618820 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0804 22:16:01.618834 26216 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.618839 26216 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.618857 26216 authenticator.cpp:311] Authentication success I0804 22:16:01.618954 26219 authenticatee.cpp:292] Authentication success I0804 22:16:01.619035 26204 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.619083 26219 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(259)@172.17.0.152:59302 I0804 22:16:01.619309 26208 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302 I0804 22:16:01.619335 26208 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302 I0804 22:16:01.619494 26208 sched.cpp:746] Will retry registration in 439203ns if necessary I0804 22:16:01.619627 26217 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.619695 26217 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*' I0804 22:16:01.620848 26217 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302 I0804 22:16:01.620929 26217 sched.cpp:746] Will retry registration in 2.099193326secs if necessary I0804 22:16:01.621036 26210 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.621083 26210 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*' I0804 22:16:01.621727 26217 master.cpp:1876] Subscribing framework default with checkpointing disabled and capabilities [  ] I0804 22:16:01.621981 26208 sched.cpp:262] New master detected at master@172.17.0.152:59302 I0804 22:16:01.622131 26208 sched.cpp:318] Authenticating with master master@172.17.0.152:59302 I0804 22:16:01.622153 26208 sched.cpp:325] Using default CRAM-MD5 authenticatee I0804 22:16:01.622323 26212 authenticatee.cpp:115] Creating new client SASL connection I0804 22:16:01.622324 26210 hierarchical.hpp:391] Added framework 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:01.622369 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:01.622386 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 28592ns I0804 22:16:01.622511 26210 sched.cpp:640] Framework registered with 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:01.622586 26210 sched.cpp:654] Scheduler::registered took 48005ns I0804 22:16:01.622592 26208 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.622673 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(260)@172.17.0.152:59302 I0804 22:16:01.622923 26205 authenticator.cpp:92] Creating new server SASL connection I0804 22:16:01.623112 26204 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0804 22:16:01.623133 26216 master.cpp:1870] Dropping SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302: Re-authentication in progress I0804 22:16:01.623144 26204 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0804 22:16:01.623258 26215 authenticator.cpp:197] Received SASL authentication start I0804 22:16:01.623313 26215 authenticator.cpp:319] Authentication requires more steps I0804 22:16:01.623394 26215 authenticatee.cpp:252] Received SASL authentication step I0804 22:16:01.623512 26212 authenticator.cpp:225] Received SASL authentication step I0804 22:16:01.623546 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0804 22:16:01.623564 26212 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0804 22:16:01.623603 26212 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0804 22:16:01.623622 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0804 22:16:01.623631 26212 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.623636 26212 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.623649 26212 authenticator.cpp:311] Authentication success I0804 22:16:01.623777 26212 authenticatee.cpp:292] Authentication success I0804 22:16:01.623846 26212 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.623913 26212 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(260)@172.17.0.152:59302 I0804 22:16:01.624130 26212 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302 I0804 22:16:02.583772 26218 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:02.583818 26218 hierarchical.hpp:908] Performed allocation for 0 slaves in 80538ns I0804 22:16:03.585110 26211 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:03.585156 26211 hierarchical.hpp:908] Performed allocation for 0 slaves in 69272ns I0804 22:16:04.586539 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:04.586586 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 79232ns I0804 22:16:05.587239 26209 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:05.587293 26209 hierarchical.hpp:908] Performed allocation for 0 slaves in 85128ns I0804 22:16:06.587935 26212 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:06.587985 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 78141ns I0804 22:16:07.588817 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:07.588865 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 81433ns I0804 22:16:08.589857 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:08.589906 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 71929ns I0804 22:16:09.591085 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:09.591133 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 78223ns I0804 22:16:10.591737 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:10.591785 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71894ns I0804 22:16:11.593166 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:11.593221 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 89782ns I0804 22:16:12.593647 26212 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:12.593689 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 69426ns I0804 22:16:13.594154 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:13.594202 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 70581ns I0804 22:16:14.594712 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:14.594758 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71201ns I0804 22:16:15.595412 26219 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:15.595464 26219 hierarchical.hpp:908] Performed allocation for 0 slaves in 85183ns I0804 22:16:16.596201 26217 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:16.596247 26217 hierarchical.hpp:908] Performed allocation for 0 slaves in 95132ns ../../src/tests/master_authorization_tests.cpp:794: Failure Failed to wait 15secs for frameworkRegisteredMessage I0804 22:16:16.624354 26212 master.cpp:966] Framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 disconnected I0804 22:16:16.624398 26212 master.cpp:2092] Disconnecting framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.624445 26212 master.cpp:2116] Deactivating framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.624686 26212 master.cpp:988] Giving framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 0ns to failover I0804 22:16:16.625641 26219 hierarchical.hpp:474] Deactivated framework 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:16.626688 26218 master.cpp:4180] Framework failover timeout, removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.626734 26218 master.cpp:4759] Removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.627074 26218 master.cpp:858] Master terminating I0804 22:16:16.627218 26215 hierarchical.hpp:428] Removed framework 20150804-221601-2550141356-59302-26185-0000 ../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <98-98 02-AC 54-2B 00-00>, 1-byte object <97>, 1-byte object <D2>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] MasterAuthorizationTest.DuplicateRegistration (15056 ms) ",1
"MESOS-3207","C++ style guide is not rendered correctly (code section syntax disregarded)","Some paragraphs at the bottom of docs/mesos-c++-style-guide.md containing code sections are not rendered correctly by the web site generator. It looks fine in a github gist and apparently the syntax used is correct.   ",1
"MESOS-3213","Design doc for docker registry token manager","Create design document for describing the component and interaction between Docker Registry Client and remote Docker Registry for token based authorization.",2
"MESOS-3222","Implement docker registry client","Implement the following functionality:  - fetch manifest from remote registry based on authorization method dictated by the registry. - fetch image layers from remote registry  based on authorization method dictated by the registry.. ",5
"MESOS-3252","Ignore no statistics condition for containers with no qdisc","In PortMappingStatistics::execute, we log the following errors to stderr if the egress rate limiting qdiscs are not configured inside the container.  {code} Failed to get the network statistics for the htb qdisc on eth0 Failed to get the network statistics for the fq_codel qdisc on eth0 {code}  This can occur because of an error reading the qdisc (statistics function return an error) or because the qdisc does not exist (function returns none).    We should not log an error when the qdisc does not exist since this is normal behaviour if the container is created without rate limiting.  We do not want to gate this function on the slave rate limiting flag since we would have to compare the behaviour against the flag value at the time the container was created.",2
"MESOS-3265","Starting maintenance needs to deactivate agents and kill tasks.","After using the {{/maintenance/start}} endpoint to begin maintenance on a machine, agents running on said machine should: * Be deactivated such that no offers are sent from that agent.  (Investigate if {{Master::deactivate(Slave*)}} can be used or modified for this purpose.) * Kill all tasks still running on the agent (See MESOS-1475). * Prevent other agents on that machine from registering or sending out offers.  This will likely involve some modifications to {{Master::register}} and {{Master::reregister}}.  ",8
"MESOS-3266","Stopping/Completing maintenance needs to reactivate agents.","After using the {{/maintenance/stop}} endpoint to end maintenance on a machine, any deactivated agents must be reactivated and allowed to register with the master.",5
"MESOS-3273","EventCall Test Framework is flaky","Observed this on ASF CI. h/t [~haosdent@gmail.com]  Looks like the HTTP scheduler never sent a SUBSCRIBE request to the master.  {code} [ RUN      ] ExamplesTest.EventCallFramework Using temporary directory '/tmp/ExamplesTest_EventCallFramework_k4vXkx' I0813 19:55:15.643579 26085 exec.cpp:443] Ignoring exited event because the driver is aborted! Shutting down Sending SIGTERM to process tree at pid 26061 Killing the following process trees: [   ] Shutting down Sending SIGTERM to process tree at pid 26062 Shutting down Killing the following process trees: [   ] Sending SIGTERM to process tree at pid 26063 Killing the following process trees: [   ] Shutting down Sending SIGTERM to process tree at pid 26098 Killing the following process trees: [   ] Shutting down Sending SIGTERM to process tree at pid 26099 Killing the following process trees: [   ] WARNING: Logging before InitGoogleLogging() is written to STDERR I0813 19:55:17.161726 26100 process.cpp:1012] libprocess is initialized on 172.17.2.10:60249 for 16 cpus I0813 19:55:17.161888 26100 logging.cpp:177] Logging to STDERR I0813 19:55:17.163625 26100 scheduler.cpp:157] Version: 0.24.0 I0813 19:55:17.175302 26100 leveldb.cpp:176] Opened db in 3.167446ms I0813 19:55:17.176393 26100 leveldb.cpp:183] Compacted db in 1.047996ms I0813 19:55:17.176496 26100 leveldb.cpp:198] Created db iterator in 77155ns I0813 19:55:17.176518 26100 leveldb.cpp:204] Seeked to beginning of db in 8429ns I0813 19:55:17.176527 26100 leveldb.cpp:273] Iterated through 0 keys in the db in 4219ns I0813 19:55:17.176708 26100 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0813 19:55:17.178951 26136 recover.cpp:449] Starting replica recovery I0813 19:55:17.179934 26136 recover.cpp:475] Replica is in EMPTY status I0813 19:55:17.181970 26126 master.cpp:378] Master 20150813-195517-167907756-60249-26100 (297daca2d01a) started on 172.17.2.10:60249 I0813 19:55:17.182317 26126 master.cpp:380] Flags at startup: --acls=""permissive: false register_frameworks {   principals {     type: SOME     values: ""test-principal""   }   roles {     type: SOME     values: ""*""   } } run_tasks {   principals {     type: SOME     values: ""test-principal""   }   users {     type: SOME     values: ""mesos""   } } "" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --credentials=""/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.24.0/src/webui"" --work_dir=""/tmp/mesos-II8Gua"" --zk_session_timeout=""10secs"" I0813 19:55:17.183475 26126 master.cpp:427] Master allowing unauthenticated frameworks to register I0813 19:55:17.183536 26126 master.cpp:432] Master allowing unauthenticated slaves to register I0813 19:55:17.183615 26126 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials' W0813 19:55:17.183859 26126 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials' are too open. It is recommended that your credentials file is NOT accessible by others. I0813 19:55:17.183969 26123 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0813 19:55:17.184306 26126 master.cpp:469] Using default 'crammd5' authenticator I0813 19:55:17.184661 26126 authenticator.cpp:512] Initializing server SASL I0813 19:55:17.185104 26138 recover.cpp:195] Received a recover response from a replica in EMPTY status I0813 19:55:17.185972 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix I0813 19:55:17.186058 26135 recover.cpp:566] Updating replica status to STARTING I0813 19:55:17.187001 26138 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 654586ns I0813 19:55:17.187037 26138 replica.cpp:323] Persisted replica status to STARTING I0813 19:55:17.187499 26134 recover.cpp:475] Replica is in STARTING status I0813 19:55:17.187605 26126 auxprop.cpp:66] Initialized in-memory auxiliary property plugin I0813 19:55:17.187710 26126 master.cpp:506] Authorization enabled I0813 19:55:17.188657 26138 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0813 19:55:17.188853 26131 hierarchical.hpp:346] Initialized hierarchical allocator process I0813 19:55:17.189252 26132 whitelist_watcher.cpp:79] No whitelist given I0813 19:55:17.189321 26134 recover.cpp:195] Received a recover response from a replica in STARTING status I0813 19:55:17.190001 26125 recover.cpp:566] Updating replica status to VOTING I0813 19:55:17.190696 26124 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 357331ns I0813 19:55:17.190775 26124 replica.cpp:323] Persisted replica status to VOTING I0813 19:55:17.190970 26133 recover.cpp:580] Successfully joined the Paxos group I0813 19:55:17.192183 26129 recover.cpp:464] Recover process terminated I0813 19:55:17.192699 26123 slave.cpp:190] Slave started on 1)@172.17.2.10:60249 I0813 19:55:17.192741 26123 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/0"" I0813 19:55:17.194514 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix I0813 19:55:17.194658 26123 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] I0813 19:55:17.194854 26123 slave.cpp:384] Slave hostname: 297daca2d01a I0813 19:55:17.194877 26123 slave.cpp:389] Slave checkpoint: true I0813 19:55:17.196751 26132 master.cpp:1524] The newly elected leader is master@172.17.2.10:60249 with id 20150813-195517-167907756-60249-26100 I0813 19:55:17.196797 26132 master.cpp:1537] Elected as the leading master! I0813 19:55:17.196815 26132 master.cpp:1307] Recovering from registrar I0813 19:55:17.197032 26138 registrar.cpp:311] Recovering registrar I0813 19:55:17.197845 26132 slave.cpp:190] Slave started on 2)@172.17.2.10:60249 I0813 19:55:17.198420 26125 log.cpp:661] Attempting to start the writer I0813 19:55:17.197948 26132 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/1"" I0813 19:55:17.199121 26132 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] I0813 19:55:17.199235 26138 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/0/meta' I0813 19:55:17.199322 26132 slave.cpp:384] Slave hostname: 297daca2d01a I0813 19:55:17.199345 26132 slave.cpp:389] Slave checkpoint: true I0813 19:55:17.199676 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix I0813 19:55:17.200085 26135 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/1/meta' I0813 19:55:17.200317 26132 status_update_manager.cpp:202] Recovering status update manager I0813 19:55:17.200371 26129 status_update_manager.cpp:202] Recovering status update manager I0813 19:55:17.202003 26129 replica.cpp:477] Replica received implicit promise request with proposal 1 I0813 19:55:17.202585 26131 slave.cpp:190] Slave started on 3)@172.17.2.10:60249 I0813 19:55:17.202596 26129 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 523191ns I0813 19:55:17.202756 26129 replica.cpp:345] Persisted promised to 1 I0813 19:55:17.202770 26132 containerizer.cpp:379] Recovering containerizer I0813 19:55:17.203061 26135 containerizer.cpp:379] Recovering containerizer I0813 19:55:17.202663 26131 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/2"" I0813 19:55:17.203819 26131 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] I0813 19:55:17.203930 26131 slave.cpp:384] Slave hostname: 297daca2d01a I0813 19:55:17.203948 26131 slave.cpp:389] Slave checkpoint: true I0813 19:55:17.204674 26137 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/2/meta' I0813 19:55:17.205178 26135 status_update_manager.cpp:202] Recovering status update manager I0813 19:55:17.205323 26135 containerizer.cpp:379] Recovering containerizer I0813 19:55:17.205521 26136 slave.cpp:4069] Finished recovery I0813 19:55:17.206074 26136 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0813 19:55:17.206424 26128 slave.cpp:4069] Finished recovery I0813 19:55:17.206722 26137 status_update_manager.cpp:176] Pausing sending status updates I0813 19:55:17.206858 26136 slave.cpp:684] New master detected at master@172.17.2.10:60249 I0813 19:55:17.206902 26138 slave.cpp:4069] Finished recovery I0813 19:55:17.206962 26128 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0813 19:55:17.208312 26134 scheduler.cpp:272] New master detected at master@172.17.2.10:60249 I0813 19:55:17.208364 26136 slave.cpp:709] No credentials provided. Attempting to register without authentication I0813 19:55:17.208608 26136 slave.cpp:720] Detecting new master I0813 19:55:17.208839 26138 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0813 19:55:17.209216 26123 coordinator.cpp:231] Coordinator attemping to fill missing position I0813 19:55:17.209247 26127 status_update_manager.cpp:176] Pausing sending status updates I0813 19:55:17.209259 26128 slave.cpp:684] New master detected at master@172.17.2.10:60249 I0813 19:55:17.209322 26127 status_update_manager.cpp:176] Pausing sending status updates I0813 19:55:17.209364 26128 slave.cpp:709] No credentials provided. Attempting to register without authentication I0813 19:55:17.209344 26138 slave.cpp:684] New master detected at master@172.17.2.10:60249 I0813 19:55:17.209455 26128 slave.cpp:720] Detecting new master I0813 19:55:17.209492 26138 slave.cpp:709] No credentials provided. Attempting to register without authentication I0813 19:55:17.209573 26128 slave.cpp:4240] Received oversubscribable resources  from the resource estimator I0813 19:55:17.209601 26138 slave.cpp:720] Detecting new master I0813 19:55:17.209730 26138 slave.cpp:4240] Received oversubscribable resources  from the resource estimator I0813 19:55:17.209883 26136 slave.cpp:4240] Received oversubscribable resources  from the resource estimator I0813 19:55:17.211266 26136 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0813 19:55:17.211771 26136 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 462128ns I0813 19:55:17.211797 26136 replica.cpp:679] Persisted action at 0 I0813 19:55:17.212980 26130 replica.cpp:511] Replica received write request for position 0 I0813 19:55:17.213124 26130 leveldb.cpp:438] Reading position from leveldb took 67075ns I0813 19:55:17.213580 26130 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 301649ns I0813 19:55:17.213603 26130 replica.cpp:679] Persisted action at 0 I0813 19:55:17.214284 26123 replica.cpp:658] Replica received learned notice for position 0 I0813 19:55:17.214622 26123 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284547ns I0813 19:55:17.214648 26123 replica.cpp:679] Persisted action at 0 I0813 19:55:17.214675 26123 replica.cpp:664] Replica learned NOP action at position 0 I0813 19:55:17.215420 26136 log.cpp:677] Writer started with ending position 0 I0813 19:55:17.217463 26133 leveldb.cpp:438] Reading position from leveldb took 47943ns I0813 19:55:17.220762 26125 registrar.cpp:344] Successfully fetched the registry (0B) in 23.649024ms I0813 19:55:17.221081 26125 registrar.cpp:443] Applied 1 operations in 136902ns; attempting to update the 'registry' I0813 19:55:17.223667 26133 log.cpp:685] Attempting to append 174 bytes to the log I0813 19:55:17.223778 26125 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1 I0813 19:55:17.224516 26127 replica.cpp:511] Replica received write request for position 1 I0813 19:55:17.225009 26127 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 466230ns I0813 19:55:17.225042 26127 replica.cpp:679] Persisted action at 1 I0813 19:55:17.225653 26126 replica.cpp:658] Replica received learned notice for position 1 I0813 19:55:17.225953 26126 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 286966ns I0813 19:55:17.225975 26126 replica.cpp:679] Persisted action at 1 I0813 19:55:17.226013 26126 replica.cpp:664] Replica learned APPEND action at position 1 I0813 19:55:17.227545 26137 registrar.cpp:488] Successfully updated the 'registry' in 6.328064ms I0813 19:55:17.227722 26137 registrar.cpp:374] Successfully recovered registrar I0813 19:55:17.227918 26124 log.cpp:704] Attempting to truncate the log to 1 I0813 19:55:17.228024 26133 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2 I0813 19:55:17.228193 26131 master.cpp:1334] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register I0813 19:55:17.228659 26127 replica.cpp:511] Replica received write request for position 2 I0813 19:55:17.228972 26127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 297903ns I0813 19:55:17.229004 26127 replica.cpp:679] Persisted action at 2 I0813 19:55:17.229565 26127 replica.cpp:658] Replica received learned notice for position 2 I0813 19:55:17.229837 26127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260326ns I0813 19:55:17.229899 26127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 48697ns I0813 19:55:17.229923 26127 replica.cpp:679] Persisted action at 2 I0813 19:55:17.229956 26127 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0813 19:55:17.325634 26138 slave.cpp:1209] Will retry registration in 445.955946ms if necessary I0813 19:55:17.326088 26124 master.cpp:3635] Registering slave at slave(2)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S0 I0813 19:55:17.327446 26124 registrar.cpp:443] Applied 1 operations in 231072ns; attempting to update the 'registry' I0813 19:55:17.330252 26136 log.cpp:685] Attempting to append 344 bytes to the log I0813 19:55:17.330407 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3 I0813 19:55:17.331418 26128 replica.cpp:511] Replica received write request for position 3 I0813 19:55:17.331753 26128 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 264140ns I0813 19:55:17.331778 26128 replica.cpp:679] Persisted action at 3 I0813 19:55:17.332324 26133 replica.cpp:658] Replica received learned notice for position 3 I0813 19:55:17.332809 26133 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 313064ns I0813 19:55:17.332834 26133 replica.cpp:679] Persisted action at 3 I0813 19:55:17.332865 26133 replica.cpp:664] Replica learned APPEND action at position 3 I0813 19:55:17.334211 26132 registrar.cpp:488] Successfully updated the 'registry' in 6.668032ms I0813 19:55:17.334430 26127 log.cpp:704] Attempting to truncate the log to 3 I0813 19:55:17.334566 26132 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4 I0813 19:55:17.335283 26129 replica.cpp:511] Replica received write request for position 4 I0813 19:55:17.335615 26127 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249 I0813 19:55:17.335816 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 458268ns I0813 19:55:17.335908 26137 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] I0813 19:55:17.335983 26129 replica.cpp:679] Persisted action at 4 I0813 19:55:17.336019 26136 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S0 I0813 19:55:17.336073 26136 fetcher.cpp:77] Clearing fetcher cache I0813 19:55:17.336220 26127 status_update_manager.cpp:183] Resuming sending status updates I0813 19:55:17.336328 26128 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S0 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: ) I0813 19:55:17.336599 26138 replica.cpp:658] Replica received learned notice for position 4 I0813 19:55:17.336910 26128 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:17.336957 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S0 in 580663ns I0813 19:55:17.337016 26136 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/1/meta/slaves/20150813-195517-167907756-60249-26100-S0/slave.info' I0813 19:55:17.337035 26138 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 403607ns I0813 19:55:17.337138 26138 leveldb.cpp:401] Deleting ~2 keys from leveldb took 77040ns I0813 19:55:17.337167 26138 replica.cpp:679] Persisted action at 4 I0813 19:55:17.337208 26138 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0813 19:55:17.337514 26136 slave.cpp:918] Forwarding total oversubscribed resources  I0813 19:55:17.337745 26131 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources  I0813 19:55:17.338240 26131 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S0 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) I0813 19:55:17.338479 26131 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:17.338505 26131 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S0 in 216259ns I0813 19:55:17.504086 26124 slave.cpp:1209] Will retry registration in 1.92618421secs if necessary I0813 19:55:17.504408 26124 master.cpp:3635] Registering slave at slave(3)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S1 I0813 19:55:17.505203 26124 registrar.cpp:443] Applied 1 operations in 144314ns; attempting to update the 'registry' I0813 19:55:17.507616 26124 log.cpp:685] Attempting to append 511 bytes to the log I0813 19:55:17.507796 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 5 I0813 19:55:17.508735 26128 replica.cpp:511] Replica received write request for position 5 I0813 19:55:17.509291 26128 leveldb.cpp:343] Persisting action (530 bytes) to leveldb took 527776ns I0813 19:55:17.509328 26128 replica.cpp:679] Persisted action at 5 I0813 19:55:17.509945 26124 replica.cpp:658] Replica received learned notice for position 5 I0813 19:55:17.510393 26124 leveldb.cpp:343] Persisting action (532 bytes) to leveldb took 438543ns I0813 19:55:17.510416 26124 replica.cpp:679] Persisted action at 5 I0813 19:55:17.510437 26124 replica.cpp:664] Replica learned APPEND action at position 5 I0813 19:55:17.511907 26125 registrar.cpp:488] Successfully updated the 'registry' in 6624us I0813 19:55:17.512225 26138 log.cpp:704] Attempting to truncate the log to 5 I0813 19:55:17.512305 26136 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 6 I0813 19:55:17.513066 26133 slave.cpp:3058] Received ping from slave-observer(2)@172.17.2.10:60249 I0813 19:55:17.513242 26133 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S1 I0813 19:55:17.513221 26126 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S1 at slave(3)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] I0813 19:55:17.513089 26129 replica.cpp:511] Replica received write request for position 6 I0813 19:55:17.513393 26133 fetcher.cpp:77] Clearing fetcher cache I0813 19:55:17.513380 26138 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S1 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: ) I0813 19:55:17.513805 26132 status_update_manager.cpp:183] Resuming sending status updates I0813 19:55:17.513949 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 340511ns I0813 19:55:17.514046 26138 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:17.514050 26129 replica.cpp:679] Persisted action at 6 I0813 19:55:17.514195 26133 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/2/meta/slaves/20150813-195517-167907756-60249-26100-S1/slave.info' I0813 19:55:17.514140 26138 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S1 in 417609ns I0813 19:55:17.514704 26133 slave.cpp:918] Forwarding total oversubscribed resources  I0813 19:55:17.514708 26138 replica.cpp:658] Replica received learned notice for position 6 I0813 19:55:17.514880 26133 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S1 at slave(3)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources  I0813 19:55:17.515244 26127 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S1 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) I0813 19:55:17.515454 26138 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 640882ns I0813 19:55:17.515522 26138 leveldb.cpp:401] Deleting ~2 keys from leveldb took 56550ns I0813 19:55:17.515547 26138 replica.cpp:679] Persisted action at 6 I0813 19:55:17.515581 26138 replica.cpp:664] Replica learned TRUNCATE action at position 6 I0813 19:55:17.515802 26127 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:17.515866 26127 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S1 in 591007ns I0813 19:55:17.984196 26135 slave.cpp:1209] Will retry registration in 1.542495291secs if necessary I0813 19:55:17.984391 26138 master.cpp:3635] Registering slave at slave(1)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S2 I0813 19:55:17.985170 26133 registrar.cpp:443] Applied 1 operations in 202126ns; attempting to update the 'registry' I0813 19:55:17.987498 26133 log.cpp:685] Attempting to append 678 bytes to the log I0813 19:55:17.987656 26123 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 7 I0813 19:55:17.988704 26138 replica.cpp:511] Replica received write request for position 7 I0813 19:55:17.989223 26138 leveldb.cpp:343] Persisting action (697 bytes) to leveldb took 490422ns I0813 19:55:17.989248 26138 replica.cpp:679] Persisted action at 7 I0813 19:55:17.989972 26126 replica.cpp:658] Replica received learned notice for position 7 I0813 19:55:17.990401 26126 leveldb.cpp:343] Persisting action (699 bytes) to leveldb took 404333ns I0813 19:55:17.990420 26126 replica.cpp:679] Persisted action at 7 I0813 19:55:17.990440 26126 replica.cpp:664] Replica learned APPEND action at position 7 I0813 19:55:17.994066 26123 registrar.cpp:488] Successfully updated the 'registry' in 8.788224ms I0813 19:55:17.994436 26134 log.cpp:704] Attempting to truncate the log to 7 I0813 19:55:17.994575 26123 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 8 I0813 19:55:17.995070 26134 slave.cpp:3058] Received ping from slave-observer(3)@172.17.2.10:60249 I0813 19:55:17.995291 26134 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S2 I0813 19:55:17.995319 26134 fetcher.cpp:77] Clearing fetcher cache I0813 19:55:17.995246 26129 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S2 at slave(1)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] I0813 19:55:17.995565 26123 status_update_manager.cpp:183] Resuming sending status updates I0813 19:55:17.995579 26129 replica.cpp:511] Replica received write request for position 8 I0813 19:55:17.996016 26134 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/0/meta/slaves/20150813-195517-167907756-60249-26100-S2/slave.info' I0813 19:55:17.996039 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 440511ns I0813 19:55:17.996067 26129 replica.cpp:679] Persisted action at 8 I0813 19:55:17.996294 26128 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S2 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: ) I0813 19:55:17.996556 26134 slave.cpp:918] Forwarding total oversubscribed resources  I0813 19:55:17.996623 26133 replica.cpp:658] Replica received learned notice for position 8 I0813 19:55:17.997095 26134 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S2 at slave(1)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources  I0813 19:55:17.997263 26133 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 442619ns I0813 19:55:17.997385 26133 leveldb.cpp:401] Deleting ~2 keys from leveldb took 95741ns I0813 19:55:17.997413 26133 replica.cpp:679] Persisted action at 8 I0813 19:55:17.997465 26133 replica.cpp:664] Replica learned TRUNCATE action at position 8 I0813 19:55:17.997756 26128 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:17.997925 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S2 in 1.14489ms I0813 19:55:17.998159 26128 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S2 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) I0813 19:55:17.998445 26128 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:17.998471 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S2 in 218856ns I0813 19:55:18.190146 26133 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:18.190217 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 637042ns I0813 19:55:19.191346 26131 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:19.191915 26131 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.215355ms I0813 19:55:20.193631 26135 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:20.193709 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 834491ns I0813 19:55:21.194805 26134 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:21.194870 26134 hierarchical.hpp:908] Performed allocation for 3 slaves in 536547ns I0813 19:55:22.196143 26137 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:22.196216 26137 hierarchical.hpp:908] Performed allocation for 3 slaves in 755140ns I0813 19:55:23.197412 26132 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:23.197979 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.223984ms I0813 19:55:24.199429 26132 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:24.199735 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 904654ns I0813 19:55:25.200978 26127 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:25.201206 26127 hierarchical.hpp:908] Performed allocation for 3 slaves in 939979ns I0813 19:55:26.203023 26132 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:26.203101 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 721178ns I0813 19:55:27.204815 26126 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:27.204888 26126 hierarchical.hpp:908] Performed allocation for 3 slaves in 767983ns I0813 19:55:28.206374 26126 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:28.206444 26126 hierarchical.hpp:908] Performed allocation for 3 slaves in 745214ns I0813 19:55:29.207515 26124 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:29.207579 26124 hierarchical.hpp:908] Performed allocation for 3 slaves in 551217ns I0813 19:55:30.208966 26136 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:30.209053 26136 hierarchical.hpp:908] Performed allocation for 3 slaves in 649887ns I0813 19:55:31.210078 26123 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:31.210144 26123 hierarchical.hpp:908] Performed allocation for 3 slaves in 558919ns I0813 19:55:32.211027 26130 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0813 19:55:32.211045 26129 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0813 19:55:32.211084 26132 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0813 19:55:32.211386 26129 slave.cpp:4240] Received oversubscribable resources  from the resource estimator I0813 19:55:32.211688 26132 slave.cpp:4240] Received oversubscribable resources  from the resource estimator I0813 19:55:32.211853 26133 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:32.212035 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 898985ns I0813 19:55:32.212169 26133 slave.cpp:4240] Received oversubscribable resources  from the resource estimator I0813 19:55:32.336745 26135 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249 I0813 19:55:32.514333 26129 slave.cpp:3058] Received ping from slave-observer(2)@172.17.2.10:60249 I0813 19:55:32.996134 26128 slave.cpp:3058] Received ping from slave-observer(3)@172.17.2.10:60249 I0813 19:55:33.213248 26128 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:33.213326 26128 hierarchical.hpp:908] Performed allocation for 3 slaves in 827511ns I0813 19:55:34.214326 26125 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:34.214391 26125 hierarchical.hpp:908] Performed allocation for 3 slaves in 546422ns I0813 19:55:35.215909 26123 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:35.215973 26123 hierarchical.hpp:908] Performed allocation for 3 slaves in 627190ns I0813 19:55:36.217156 26134 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:36.217339 26134 hierarchical.hpp:908] Performed allocation for 3 slaves in 906249ns I0813 19:55:37.218739 26132 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:37.219169 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.102465ms I0813 19:55:38.220641 26133 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:38.220711 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 643146ns I0813 19:55:39.221976 26133 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:39.222118 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 845334ns I0813 19:55:40.223338 26129 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:40.223546 26129 hierarchical.hpp:908] Performed allocation for 3 slaves in 849995ns I0813 19:55:41.225558 26138 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:41.225752 26138 hierarchical.hpp:908] Performed allocation for 3 slaves in 958480ns I0813 19:55:42.227176 26131 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:42.227378 26131 hierarchical.hpp:908] Performed allocation for 3 slaves in 927048ns I0813 19:55:43.228813 26137 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:43.229441 26137 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.310118ms I0813 19:55:44.230828 26135 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:44.231142 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 896369ns I0813 19:55:45.232656 26135 hierarchical.hpp:1008] No resources available to allocate! I0813 19:55:45.232903 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.357693ms I0813 19:55:46.234973 26137 hierarchical.hpp:1008 {code}",5
"MESOS-3280","Master fails to access replicated log after network partition","In a 5 node cluster with 3 masters and 2 slaves, and ZK on each node, when a network partition is forced, all the masters apparently lose access to their replicated log. The leading master halts. Unknown reasons, but presumably related to replicated log access. The others fail to recover from the replicated log. Unknown reasons. This could have to do with ZK setup, but it might also be a Mesos bug.   This was observed in a Chronos test drive scenario described in detail here: https://github.com/mesos/chronos/issues/511  With setup instructions here: https://github.com/mesos/chronos/issues/508  ",8
"MESOS-3284","JSON representation of Protobuf should use base64 encoding for 'bytes' fields.","Currently we encode 'bytes' fields as UTF-8 strings, which is lossy for binary data due to invalid byte sequences! In order to encode binary data in a lossless fashion, we can encode 'bytes' fields in base64.  Note that this is also how proto3 does its encoding (see [here|https://developers.google.com/protocol-buffers/docs/proto3?hl=en#json]), so this would make migration easier as well.",3
"MESOS-3288","Implement docker registry client","Implement the docker registry client as per design document:  https://docs.google.com/document/d/1kE-HXPQl4lQgamPIiaD4Ytdr-N4HeQc4fnE93WHR4X4/edit",5
"MESOS-3289","Add DockerRegistry unit tests","Add unit tests suite for docker registry implementation.  This could include:  - Creating mock docker registry server - Using openssl library for digest functions.",5
"MESOS-3311","SlaveTest.HTTPSchedulerSlaveRestart","Observed on ASF CI  {code} [ RUN      ] SlaveTest.HTTPSchedulerSlaveRestart Using temporary directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA' I0825 22:07:36.809872 27610 leveldb.cpp:176] Opened db in 3.751801ms I0825 22:07:36.811115 27610 leveldb.cpp:183] Compacted db in 1.2194ms I0825 22:07:36.811175 27610 leveldb.cpp:198] Created db iterator in 30669ns I0825 22:07:36.811197 27610 leveldb.cpp:204] Seeked to beginning of db in 7829ns I0825 22:07:36.811208 27610 leveldb.cpp:273] Iterated through 0 keys in the db in 6017ns I0825 22:07:36.811245 27610 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0825 22:07:36.811722 27638 recover.cpp:449] Starting replica recovery I0825 22:07:36.811980 27638 recover.cpp:475] Replica is in EMPTY status I0825 22:07:36.813033 27641 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0825 22:07:36.813355 27635 recover.cpp:195] Received a recover response from a replica in EMPTY status I0825 22:07:36.813756 27628 recover.cpp:566] Updating replica status to STARTING I0825 22:07:36.814434 27636 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 570160ns I0825 22:07:36.814471 27636 replica.cpp:323] Persisted replica status to STARTING I0825 22:07:36.814743 27642 recover.cpp:475] Replica is in STARTING status I0825 22:07:36.814965 27638 master.cpp:378] Master 20150825-220736-234885548-51219-27610 (09c6504e3a31) started on 172.17.0.14:51219 I0825 22:07:36.814999 27638 master.cpp:380] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.25.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/master"" --zk_session_timeout=""10secs"" I0825 22:07:36.815347 27638 master.cpp:425] Master only allowing authenticated frameworks to register I0825 22:07:36.815371 27638 master.cpp:430] Master only allowing authenticated slaves to register I0825 22:07:36.815402 27638 credentials.hpp:37] Loading credentials for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/credentials' I0825 22:07:36.815634 27632 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0825 22:07:36.815752 27638 master.cpp:469] Using default 'crammd5' authenticator I0825 22:07:36.815904 27638 master.cpp:506] Authorization enabled I0825 22:07:36.815979 27643 recover.cpp:195] Received a recover response from a replica in STARTING status I0825 22:07:36.816185 27637 whitelist_watcher.cpp:79] No whitelist given I0825 22:07:36.816186 27641 hierarchical.hpp:346] Initialized hierarchical allocator process I0825 22:07:36.816519 27630 recover.cpp:566] Updating replica status to VOTING I0825 22:07:36.817258 27639 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 475231ns I0825 22:07:36.817296 27639 replica.cpp:323] Persisted replica status to VOTING I0825 22:07:36.817420 27637 master.cpp:1525] The newly elected leader is master@172.17.0.14:51219 with id 20150825-220736-234885548-51219-27610 I0825 22:07:36.817467 27637 master.cpp:1538] Elected as the leading master! I0825 22:07:36.817483 27637 master.cpp:1308] Recovering from registrar I0825 22:07:36.817509 27635 recover.cpp:580] Successfully joined the Paxos group I0825 22:07:36.817708 27633 registrar.cpp:311] Recovering registrar I0825 22:07:36.817844 27635 recover.cpp:464] Recover process terminated I0825 22:07:36.818439 27631 log.cpp:661] Attempting to start the writer I0825 22:07:36.819694 27636 replica.cpp:477] Replica received implicit promise request with proposal 1 I0825 22:07:36.820133 27636 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421255ns I0825 22:07:36.820168 27636 replica.cpp:345] Persisted promised to 1 I0825 22:07:36.820804 27630 coordinator.cpp:231] Coordinator attemping to fill missing position I0825 22:07:36.822105 27638 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0825 22:07:36.822597 27638 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 468065ns I0825 22:07:36.822625 27638 replica.cpp:679] Persisted action at 0 I0825 22:07:36.823737 27637 replica.cpp:511] Replica received write request for position 0 I0825 22:07:36.823796 27637 leveldb.cpp:438] Reading position from leveldb took 39603ns I0825 22:07:36.824267 27637 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 446655ns I0825 22:07:36.824296 27637 replica.cpp:679] Persisted action at 0 I0825 22:07:36.824961 27634 replica.cpp:658] Replica received learned notice for position 0 I0825 22:07:36.825340 27634 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 362236ns I0825 22:07:36.825369 27634 replica.cpp:679] Persisted action at 0 I0825 22:07:36.825388 27634 replica.cpp:664] Replica learned NOP action at position 0 I0825 22:07:36.825975 27642 log.cpp:677] Writer started with ending position 0 I0825 22:07:36.826997 27628 leveldb.cpp:438] Reading position from leveldb took 56us I0825 22:07:36.829946 27639 registrar.cpp:344] Successfully fetched the registry (0B) in 12.187136ms I0825 22:07:36.830077 27639 registrar.cpp:443] Applied 1 operations in 40874ns; attempting to update the 'registry' I0825 22:07:36.832870 27635 log.cpp:685] Attempting to append 174 bytes to the log I0825 22:07:36.833088 27641 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1 I0825 22:07:36.833845 27636 replica.cpp:511] Replica received write request for position 1 I0825 22:07:36.834293 27636 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 425175ns I0825 22:07:36.834324 27636 replica.cpp:679] Persisted action at 1 I0825 22:07:36.835077 27643 replica.cpp:658] Replica received learned notice for position 1 I0825 22:07:36.835500 27643 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 404831ns I0825 22:07:36.835532 27643 replica.cpp:679] Persisted action at 1 I0825 22:07:36.835574 27643 replica.cpp:664] Replica learned APPEND action at position 1 I0825 22:07:36.836545 27643 registrar.cpp:488] Successfully updated the 'registry' in 6.393088ms I0825 22:07:36.836707 27643 registrar.cpp:374] Successfully recovered registrar I0825 22:07:36.836874 27639 log.cpp:704] Attempting to truncate the log to 1 I0825 22:07:36.837174 27632 master.cpp:1335] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register I0825 22:07:36.837291 27634 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2 I0825 22:07:36.838249 27639 replica.cpp:511] Replica received write request for position 2 I0825 22:07:36.838685 27639 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 412214ns I0825 22:07:36.838716 27639 replica.cpp:679] Persisted action at 2 I0825 22:07:36.839735 27628 replica.cpp:658] Replica received learned notice for position 2 I0825 22:07:36.840304 27628 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 547841ns I0825 22:07:36.840375 27628 leveldb.cpp:401] Deleting ~1 keys from leveldb took 51256ns I0825 22:07:36.840401 27628 replica.cpp:679] Persisted action at 2 I0825 22:07:36.840428 27628 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0825 22:07:36.849371 27610 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix I0825 22:07:36.856500 27633 slave.cpp:190] Slave started on 286)@172.17.0.14:51219 I0825 22:07:36.856541 27633 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L"" I0825 22:07:36.857074 27633 credentials.hpp:85] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential' I0825 22:07:36.857275 27633 slave.cpp:321] Slave using credential for: test-principal I0825 22:07:36.857822 27633 slave.cpp:354] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0825 22:07:36.857936 27633 slave.cpp:384] Slave hostname: 09c6504e3a31 I0825 22:07:36.857959 27633 slave.cpp:389] Slave checkpoint: true I0825 22:07:36.858886 27637 state.cpp:54] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta' I0825 22:07:36.859130 27638 status_update_manager.cpp:202] Recovering status update manager I0825 22:07:36.859465 27636 containerizer.cpp:379] Recovering containerizer I0825 22:07:36.860631 27634 slave.cpp:4069] Finished recovery I0825 22:07:36.861034 27634 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0825 22:07:36.861239 27643 status_update_manager.cpp:176] Pausing sending status updates I0825 22:07:36.861240 27634 slave.cpp:684] New master detected at master@172.17.0.14:51219 I0825 22:07:36.861322 27634 slave.cpp:747] Authenticating with master master@172.17.0.14:51219 I0825 22:07:36.861343 27634 slave.cpp:752] Using default CRAM-MD5 authenticatee I0825 22:07:36.861450 27634 slave.cpp:720] Detecting new master I0825 22:07:36.861495 27628 authenticatee.cpp:115] Creating new client SASL connection I0825 22:07:36.861569 27634 slave.cpp:4240] Received oversubscribable resources  from the resource estimator I0825 22:07:36.861716 27632 master.cpp:4694] Authenticating slave(286)@172.17.0.14:51219 I0825 22:07:36.861799 27629 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(665)@172.17.0.14:51219 I0825 22:07:36.862045 27642 authenticator.cpp:92] Creating new server SASL connection I0825 22:07:36.862308 27635 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0825 22:07:36.862337 27635 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0825 22:07:36.862421 27629 authenticator.cpp:197] Received SASL authentication start I0825 22:07:36.862478 27629 authenticator.cpp:319] Authentication requires more steps I0825 22:07:36.862579 27633 authenticatee.cpp:252] Received SASL authentication step I0825 22:07:36.862679 27628 authenticator.cpp:225] Received SASL authentication step I0825 22:07:36.862707 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0825 22:07:36.862717 27628 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0825 22:07:36.862754 27628 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0825 22:07:36.862785 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0825 22:07:36.862797 27628 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0825 22:07:36.862802 27628 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0825 22:07:36.862817 27628 authenticator.cpp:311] Authentication success I0825 22:07:36.862884 27629 authenticatee.cpp:292] Authentication success I0825 22:07:36.862921 27630 master.cpp:4724] Successfully authenticated principal 'test-principal' at slave(286)@172.17.0.14:51219 I0825 22:07:36.862969 27642 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(665)@172.17.0.14:51219 I0825 22:07:36.863139 27639 slave.cpp:815] Successfully authenticated with master master@172.17.0.14:51219 I0825 22:07:36.863256 27639 slave.cpp:1209] Will retry registration in 15.028678ms if necessary I0825 22:07:36.863382 27643 master.cpp:3636] Registering slave at slave(286)@172.17.0.14:51219 (09c6504e3a31) with id 20150825-220736-234885548-51219-27610-S0 I0825 22:07:36.863899 27610 sched.cpp:164] Version: 0.25.0 I0825 22:07:36.863940 27636 registrar.cpp:443] Applied 1 operations in 94492ns; attempting to update the 'registry' I0825 22:07:36.864670 27632 sched.cpp:262] New master detected at master@172.17.0.14:51219 I0825 22:07:36.864790 27632 sched.cpp:318] Authenticating with master master@172.17.0.14:51219 I0825 22:07:36.864821 27632 sched.cpp:325] Using default CRAM-MD5 authenticatee I0825 22:07:36.865095 27637 authenticatee.cpp:115] Creating new client SASL connection I0825 22:07:36.865453 27643 master.cpp:4694] Authenticating scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:36.865603 27629 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(666)@172.17.0.14:51219 I0825 22:07:36.865840 27638 authenticator.cpp:92] Creating new server SASL connection I0825 22:07:36.866217 27630 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0825 22:07:36.866260 27630 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0825 22:07:36.866433 27639 authenticator.cpp:197] Received SASL authentication start I0825 22:07:36.866513 27639 authenticator.cpp:319] Authentication requires more steps I0825 22:07:36.866710 27630 authenticatee.cpp:252] Received SASL authentication step I0825 22:07:36.866999 27638 authenticator.cpp:225] Received SASL authentication step I0825 22:07:36.867051 27638 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0825 22:07:36.867077 27638 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0825 22:07:36.867130 27638 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0825 22:07:36.867162 27638 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0825 22:07:36.867175 27638 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0825 22:07:36.867183 27638 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0825 22:07:36.867202 27638 authenticator.cpp:311] Authentication success I0825 22:07:36.867426 27636 authenticatee.cpp:292] Authentication success I0825 22:07:36.867434 27633 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(666)@172.17.0.14:51219 I0825 22:07:36.867627 27630 master.cpp:4724] Successfully authenticated principal 'test-principal' at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:36.867951 27641 sched.cpp:407] Successfully authenticated with master master@172.17.0.14:51219 I0825 22:07:36.867986 27641 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.14:51219 I0825 22:07:36.868114 27641 sched.cpp:746] Will retry registration in 1.352726078secs if necessary I0825 22:07:36.868233 27634 log.cpp:685] Attempting to append 344 bytes to the log I0825 22:07:36.868268 27638 master.cpp:2094] Received SUBSCRIBE call for framework 'default' at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:36.868305 27638 master.cpp:1564] Authorizing framework principal 'test-principal' to receive offers for role '*' I0825 22:07:36.868373 27631 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3 I0825 22:07:36.868614 27642 master.cpp:2164] Subscribing framework default with checkpointing enabled and capabilities [  ] I0825 22:07:36.868999 27643 hierarchical.hpp:391] Added framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:36.869030 27643 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:36.869046 27643 hierarchical.hpp:910] Performed allocation for 0 slaves in 34654ns I0825 22:07:36.869215 27631 sched.cpp:640] Framework registered with 20150825-220736-234885548-51219-27610-0000 I0825 22:07:36.869215 27643 replica.cpp:511] Replica received write request for position 3 I0825 22:07:36.869268 27631 sched.cpp:654] Scheduler::registered took 29976ns I0825 22:07:36.869453 27643 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 181689ns I0825 22:07:36.869477 27643 replica.cpp:679] Persisted action at 3 I0825 22:07:36.870075 27629 replica.cpp:658] Replica received learned notice for position 3 I0825 22:07:36.870542 27629 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 469081ns I0825 22:07:36.870589 27629 replica.cpp:679] Persisted action at 3 I0825 22:07:36.870622 27629 replica.cpp:664] Replica learned APPEND action at position 3 I0825 22:07:36.872133 27632 registrar.cpp:488] Successfully updated the 'registry' in 8.113152ms I0825 22:07:36.872354 27639 log.cpp:704] Attempting to truncate the log to 3 I0825 22:07:36.872470 27632 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4 I0825 22:07:36.872879 27637 slave.cpp:3058] Received ping from slave-observer(274)@172.17.0.14:51219 I0825 22:07:36.873015 27636 master.cpp:3699] Registered slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0825 22:07:36.873180 27637 slave.cpp:859] Registered with master master@172.17.0.14:51219; given slave ID 20150825-220736-234885548-51219-27610-S0 I0825 22:07:36.873219 27637 fetcher.cpp:77] Clearing fetcher cache I0825 22:07:36.873410 27634 status_update_manager.cpp:183] Resuming sending status updates I0825 22:07:36.873379 27628 hierarchical.hpp:542] Added slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: ) I0825 22:07:36.873482 27642 replica.cpp:511] Replica received write request for position 4 I0825 22:07:36.873661 27637 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/slave.info' I0825 22:07:36.874042 27642 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 538208ns I0825 22:07:36.874078 27642 replica.cpp:679] Persisted action at 4 I0825 22:07:36.874196 27628 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 739900ns I0825 22:07:36.874204 27637 slave.cpp:918] Forwarding total oversubscribed resources  I0825 22:07:36.874824 27635 master.cpp:4613] Sending 1 offers to framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:36.874958 27639 replica.cpp:658] Replica received learned notice for position 4 I0825 22:07:36.875074 27635 master.cpp:3998] Received update of slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) with total oversubscribed resources  I0825 22:07:36.875485 27636 sched.cpp:803] Scheduler::resourceOffers took 243089ns I0825 22:07:36.875450 27638 hierarchical.hpp:602] Slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) I0825 22:07:36.875495 27639 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 462264ns I0825 22:07:36.875643 27639 leveldb.cpp:401] Deleting ~2 keys from leveldb took 109856ns I0825 22:07:36.875682 27639 replica.cpp:679] Persisted action at 4 I0825 22:07:36.875717 27639 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0825 22:07:36.876045 27638 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:36.876072 27638 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 541099ns I0825 22:07:36.879416 27639 master.cpp:2739] Processing ACCEPT call for offers: [ 20150825-220736-234885548-51219-27610-O0 ] on slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) for framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:36.879475 27639 master.cpp:2570] Authorizing framework principal 'test-principal' to launch task b89d1df8-f2fb-44be-8f60-9352cf32a79d as user 'mesos' I0825 22:07:36.880975 27639 master.hpp:170] Adding task b89d1df8-f2fb-44be-8f60-9352cf32a79d with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) I0825 22:07:36.881124 27639 master.cpp:3069] Launching task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:36.882314 27636 slave.cpp:1249] Got assigned task b89d1df8-f2fb-44be-8f60-9352cf32a79d for framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:36.882470 27636 slave.cpp:4720] Checkpointing FrameworkInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.info' I0825 22:07:36.882984 27636 slave.cpp:4731] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.pid' I0825 22:07:36.884068 27636 slave.cpp:1365] Launching task b89d1df8-f2fb-44be-8f60-9352cf32a79d for framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:36.895586 27636 slave.cpp:5156] Checkpointing ExecutorInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/executor.info' I0825 22:07:36.896765 27636 slave.cpp:4799] Launching executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c' I0825 22:07:36.897374 27643 containerizer.cpp:633] Starting container '1499299a-93dd-4982-9249-ad0e19d1c06c' for executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework '20150825-220736-234885548-51219-27610-0000' I0825 22:07:36.897414 27636 slave.cpp:5179] Checkpointing TaskInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/tasks/b89d1df8-f2fb-44be-8f60-9352cf32a79d/task.info' I0825 22:07:36.897974 27636 slave.cpp:1583] Queuing task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' for executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework '20150825-220736-234885548-51219-27610-0000 I0825 22:07:36.898123 27636 slave.cpp:637] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c' I0825 22:07:36.902439 27641 launcher.cpp:131] Forked child with pid '2326' for container '1499299a-93dd-4982-9249-ad0e19d1c06c' I0825 22:07:36.902752 27641 containerizer.cpp:855] Checkpointing executor's forked pid 2326 to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/pids/forked.pid' WARNING: Logging before InitGoogleLogging() is written to STDERR I0825 22:07:37.029348  2340 process.cpp:1012] libprocess is initialized on 172.17.0.14:42774 for 16 cpus I0825 22:07:37.030342  2340 logging.cpp:177] Logging to STDERR I0825 22:07:37.032822  2340 exec.cpp:133] Version: 0.25.0 I0825 22:07:37.038837  2355 exec.cpp:183] Executor started at: executor(1)@172.17.0.14:42774 with pid 2340 I0825 22:07:37.041252 27638 slave.cpp:2358] Got registration for executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 from executor(1)@172.17.0.14:42774 I0825 22:07:37.041371 27638 slave.cpp:2444] Checkpointing executor pid 'executor(1)@172.17.0.14:42774' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/pids/libprocess.pid' I0825 22:07:37.044067 27634 slave.cpp:1739] Sending queued task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' to executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.044256  2358 exec.cpp:207] Executor registered on slave 20150825-220736-234885548-51219-27610-S0 I0825 22:07:37.046058  2358 exec.cpp:219] Executor::registered took 239083ns Registered executor on 09c6504e3a31 Starting task b89d1df8-f2fb-44be-8f60-9352cf32a79d I0825 22:07:37.046394  2358 exec.cpp:294] Executor asked to run task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' I0825 22:07:37.046493  2358 exec.cpp:303] Executor::launchTask took 84034ns sh -c 'sleep 1000' Forked command at 2371 I0825 22:07:37.049942  2366 exec.cpp:516] Executor sending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.050977 27635 slave.cpp:2696] Handling status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from executor(1)@172.17.0.14:42774 I0825 22:07:37.051316 27632 status_update_manager.cpp:322] Received status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.051379 27632 status_update_manager.cpp:499] Creating StatusUpdate stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.052251 27632 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.053840 27632 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave I0825 22:07:37.054127 27642 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219 I0825 22:07:37.054364 27642 slave.cpp:2899] Status update manager successfully handled status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.054407 27642 slave.cpp:2905] Sending acknowledgement for status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to executor(1)@172.17.0.14:42774 I0825 22:07:37.054469 27635 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:37.054519 27635 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.054743 27635 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING I0825 22:07:37.055011 27641 sched.cpp:910] Scheduler::statusUpdate took 169426ns I0825 22:07:37.055639 27634 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0 I0825 22:07:37.055665  2359 exec.cpp:340] Executor received status update acknowledgement 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.055886 27640 slave.cpp:564] Slave terminating I0825 22:07:37.056210 27634 master.cpp:1012] Slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) disconnected I0825 22:07:37.056257 27634 master.cpp:2415] Disconnecting slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:37.056339 27634 master.cpp:2434] Deactivating slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:37.056675 27643 hierarchical.hpp:635] Slave 20150825-220736-234885548-51219-27610-S0 deactivated I0825 22:07:37.059391 27610 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix I0825 22:07:37.066619 27641 slave.cpp:190] Slave started on 287)@172.17.0.14:51219 I0825 22:07:37.066668 27641 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L"" I0825 22:07:37.067343 27641 credentials.hpp:85] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential' I0825 22:07:37.067643 27641 slave.cpp:321] Slave using credential for: test-principal I0825 22:07:37.068413 27641 slave.cpp:354] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0825 22:07:37.068580 27641 slave.cpp:384] Slave hostname: 09c6504e3a31 I0825 22:07:37.068613 27641 slave.cpp:389] Slave checkpoint: true I0825 22:07:37.069970 27636 state.cpp:54] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta' I0825 22:07:37.070089 27636 state.cpp:690] Failed to find resources file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/resources/resources.info' I0825 22:07:37.075319 27628 fetcher.cpp:77] Clearing fetcher cache I0825 22:07:37.075393 27628 slave.cpp:4157] Recovering framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.075475 27628 slave.cpp:4908] Recovering executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.076370 27641 status_update_manager.cpp:202] Recovering status update manager I0825 22:07:37.076409 27641 status_update_manager.cpp:210] Recovering executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.076504 27641 status_update_manager.cpp:499] Creating StatusUpdate stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.077056 27641 status_update_manager.cpp:802] Replaying status update stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d I0825 22:07:37.077715 27628 slave.cpp:637] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c' I0825 22:07:37.078111 27634 containerizer.cpp:379] Recovering containerizer I0825 22:07:37.078229 27634 containerizer.cpp:434] Recovering container '1499299a-93dd-4982-9249-ad0e19d1c06c' for executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.079934 27640 slave.cpp:4010] Sending reconnect request to executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 at executor(1)@172.17.0.14:42774 I0825 22:07:37.081012  2354 exec.cpp:253] Received reconnect request from slave 20150825-220736-234885548-51219-27610-S0 I0825 22:07:37.081893 27631 slave.cpp:2508] Re-registering executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:37.082904  2362 exec.cpp:230] Executor re-registered on slave 20150825-220736-234885548-51219-27610-S0 Re-registered executor on 09c6504e3a31 I0825 22:07:37.084738  2362 exec.cpp:242] Executor::reregistered took 119419ns I0825 22:07:37.816828 27634 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:37.816884 27634 hierarchical.hpp:910] Performed allocation for 1 slaves in 129850ns I0825 22:07:38.817526 27629 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:38.817607 27629 hierarchical.hpp:910] Performed allocation for 1 slaves in 152923ns I0825 22:07:39.081434 27637 slave.cpp:2645] Cleaning up un-reregistered executors I0825 22:07:39.081596 27637 slave.cpp:4069] Finished recovery I0825 22:07:39.082165 27637 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0825 22:07:39.082417 27637 status_update_manager.cpp:176] Pausing sending status updates I0825 22:07:39.082442 27643 slave.cpp:684] New master detected at master@172.17.0.14:51219 I0825 22:07:39.082602 27643 slave.cpp:747] Authenticating with master master@172.17.0.14:51219 I0825 22:07:39.082628 27643 slave.cpp:752] Using default CRAM-MD5 authenticatee I0825 22:07:39.082830 27643 slave.cpp:720] Detecting new master I0825 22:07:39.082919 27638 authenticatee.cpp:115] Creating new client SASL connection I0825 22:07:39.082973 27643 slave.cpp:4240] Received oversubscribable resources  from the resource estimator I0825 22:07:39.083277 27631 master.cpp:4694] Authenticating slave(287)@172.17.0.14:51219 I0825 22:07:39.083427 27635 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(667)@172.17.0.14:51219 I0825 22:07:39.083731 27630 authenticator.cpp:92] Creating new server SASL connection I0825 22:07:39.083982 27634 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0825 22:07:39.084025 27634 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0825 22:07:39.084106 27634 authenticator.cpp:197] Received SASL authentication start I0825 22:07:39.084168 27634 authenticator.cpp:319] Authentication requires more steps I0825 22:07:39.084300 27639 authenticatee.cpp:252] Received SASL authentication step I0825 22:07:39.084527 27628 authenticator.cpp:225] Received SASL authentication step I0825 22:07:39.084625 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0825 22:07:39.084650 27628 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0825 22:07:39.084709 27628 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0825 22:07:39.084738 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0825 22:07:39.084750 27628 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0825 22:07:39.084763 27628 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0825 22:07:39.084780 27628 authenticator.cpp:311] Authentication success I0825 22:07:39.084905 27642 authenticatee.cpp:292] Authentication success I0825 22:07:39.085000 27637 master.cpp:4724] Successfully authenticated principal 'test-principal' at slave(287)@172.17.0.14:51219 I0825 22:07:39.085234 27642 slave.cpp:815] Successfully authenticated with master master@172.17.0.14:51219 I0825 22:07:39.085610 27642 slave.cpp:1209] Will retry registration in 6.014445ms if necessary I0825 22:07:39.085907 27643 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(667)@172.17.0.14:51219 I0825 22:07:39.092914 27640 master.cpp:3773] Re-registering slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:39.093181 27630 slave.cpp:1209] Will retry registration in 20.588077ms if necessary I0825 22:07:39.093858 27635 slave.cpp:959] Re-registered with master master@172.17.0.14:51219 I0825 22:07:39.093879 27638 hierarchical.hpp:621] Slave 20150825-220736-234885548-51219-27610-S0 reactivated I0825 22:07:39.093855 27640 master.cpp:3936] Sending updated checkpointed resources  to slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:39.094110 27631 status_update_manager.cpp:183] Resuming sending status updates I0825 22:07:39.094130 27635 slave.cpp:995] Forwarding total oversubscribed resources  W0825 22:07:39.094172 27631 status_update_manager.cpp:190] Resending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.094211 27631 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave I0825 22:07:39.094435 27640 master.cpp:3773] Re-registering slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:39.094602 27635 slave.cpp:2227] Updated checkpointed resources from  to  I0825 22:07:39.095346 27640 master.cpp:3936] Sending updated checkpointed resources  to slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:39.095775 27635 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219 I0825 22:07:39.095803 27640 master.cpp:3998] Received update of slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) with total oversubscribed resources  I0825 22:07:39.096372 27635 slave.cpp:2131] Updating framework 20150825-220736-234885548-51219-27610-0000 pid to @0.0.0.0:0 I0825 22:07:39.096467 27635 slave.cpp:2147] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.pid' I0825 22:07:39.096544 27640 hierarchical.hpp:602] Slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) I0825 22:07:39.096652 27639 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:39.096709 27639 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.096978 27639 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING I0825 22:07:39.097105 27639 status_update_manager.cpp:183] Resuming sending status updates W0825 22:07:39.097187 27635 slave.cpp:976] Already re-registered with master master@172.17.0.14:51219 I0825 22:07:39.097229 27635 slave.cpp:995] Forwarding total oversubscribed resources  W0825 22:07:39.097230 27639 status_update_manager.cpp:190] Resending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.097290 27639 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave I0825 22:07:39.097373 27643 sched.cpp:910] Scheduler::statusUpdate took 76470ns I0825 22:07:39.097450 27635 slave.cpp:2131] Updating framework 20150825-220736-234885548-51219-27610-0000 pid to scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:39.097473 27640 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:39.097497 27640 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 818746ns I0825 22:07:39.097525 27635 slave.cpp:2147] Checkpointing framework pid 'scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.pid' I0825 22:07:39.097991 27640 status_update_manager.cpp:183] Resuming sending status updates W0825 22:07:39.098043 27640 status_update_manager.cpp:190] Resending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.098093 27640 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave I0825 22:07:39.098242 27635 slave.cpp:2227] Updated checkpointed resources from  to  I0825 22:07:39.098433 27635 slave.cpp:3043] Sending message for framework 20150825-220736-234885548-51219-27610-0000 to scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:39.098480 27636 master.cpp:3998] Received update of slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) with total oversubscribed resources  I0825 22:07:39.098639 27635 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219 I0825 22:07:39.098755 27634 sched.cpp:1006] Scheduler::frameworkMessage took 68683ns I0825 22:07:39.098882 27636 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0 I0825 22:07:39.098906 27635 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219 I0825 22:07:39.099019 27641 hierarchical.hpp:602] Slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) I0825 22:07:39.099192 27636 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:39.099244 27636 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.099369 27641 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:39.099395 27641 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 332336ns I0825 22:07:39.099403 27636 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING I0825 22:07:39.099426 27635 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.099609 27641 sched.cpp:910] Scheduler::statusUpdate took 90272ns I0825 22:07:39.099617 27636 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:39.099669 27636 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.099607 27635 status_update_manager.cpp:826] Checkpointing ACK for status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.099834 27636 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING I0825 22:07:39.099992 27643 sched.cpp:910] Scheduler::statusUpdate took 29331ns I0825 22:07:39.100038 27636 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0 I0825 22:07:39.100381 27636 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0 I0825 22:07:39.102119 27635 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.102120 27637 slave.cpp:2298] Status update manager successfully handled status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.102375 27635 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 E0825 22:07:39.102407 27633 slave.cpp:2291] Failed to handle status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000: Unexpected status update acknowledgment (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 E0825 22:07:39.102546 27636 slave.cpp:2291] Failed to handle status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000: Unexpected status update acknowledgment (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:39.819394 27637 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:39.819452 27637 hierarchical.hpp:910] Performed allocation for 1 slaves in 536774ns 2015-08-25 22:07:40,051:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0825 22:07:40.820246 27633 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:40.820302 27633 hierarchical.hpp:910] Performed allocation for 1 slaves in 511814ns I0825 22:07:41.821671 27637 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:41.821719 27637 hierarchical.hpp:910] Performed allocation for 1 slaves in 518909ns I0825 22:07:42.822906 27628 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:42.822959 27628 hierarchical.hpp:910] Performed allocation for 1 slaves in 659816ns 2015-08-25 22:07:43,388:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0825 22:07:43.824976 27632 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:43.825032 27632 hierarchical.hpp:910] Performed allocation for 1 slaves in 727197ns I0825 22:07:44.825883 27641 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:44.825932 27641 hierarchical.hpp:910] Performed allocation for 1 slaves in 422745ns I0825 22:07:45.828217 27634 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:45.828445 27634 hierarchical.hpp:910] Performed allocation for 1 slaves in 1.288273ms 2015-08-25 22:07:46,724:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0825 22:07:46.829910 27632 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:46.829953 27632 hierarchical.hpp:910] Performed allocation for 1 slaves in 483478ns I0825 22:07:47.830860 27636 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:47.830922 27636 hierarchical.hpp:910] Performed allocation for 1 slaves in 551674ns I0825 22:07:48.832027 27628 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:48.832078 27628 hierarchical.hpp:910] Performed allocation for 1 slaves in 417868ns I0825 22:07:49.833906 27629 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:49.833962 27629 hierarchical.hpp:910] Performed allocation for 1 slaves in 472647ns 2015-08-25 22:07:50,060:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0825 22:07:50.835659 27630 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:50.835718 27630 hierarchical.hpp:910] Performed allocation for 1 slaves in 522864ns I0825 22:07:51.837473 27638 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:51.837537 27638 hierarchical.hpp:910] Performed allocation for 1 slaves in 575837ns I0825 22:07:52.839296 27641 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:52.839350 27641 hierarchical.hpp:910] Performed allocation for 1 slaves in 558642ns 2015-08-25 22:07:53,397:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0825 22:07:53.840854 27630 hierarchical.hpp:1010] No resources available to allocate! I0825 22:07:53.840904 27630 hierarchical.hpp:910] Performed allocation for 1 slaves in 557112ns I0825 22:07:54.083889 27631 slave.cpp:4226] Querying resource estimator for oversubscribable resources I0825 22:07:54.084323 27629 slave.cpp:4240] Received oversubscribable resources  from the resource estimator ../../src/tests/slave_tests.cpp:2651: Failure Failed to wait 15secs for executorToFrameworkMessage1 I0825 22:07:54.098143 27629 master.cpp:1051] Framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 disconnected I0825 22:07:54.098212 27629 master.cpp:2370] Disconnecting framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:54.098254 27629 master.cpp:2394] Deactivating framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:54.098363 27629 master.cpp:1075] Giving framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 0ns to failover I0825 22:07:54.098448 27631 hierarchical.hpp:474] Deactivated framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.098830 27641 master.cpp:4469] Framework failover timeout, removing framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:54.098867 27641 master.cpp:5112] Removing framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 I0825 22:07:54.099156 27629 slave.cpp:1959] Asked to shut down framework 20150825-220736-234885548-51219-27610-0000 by master@172.17.0.14:51219 I0825 22:07:54.099211 27629 slave.cpp:1984] Shutting down framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.099198 27641 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_KILLED I0825 22:07:54.099328 27629 slave.cpp:3710] Shutting down executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.099913 27641 master.cpp:5644] Removing task b89d1df8-f2fb-44be-8f60-9352cf32a79d with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150825-220736-234885548-51219-27610-0000 on slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) I0825 22:07:54.099987 27632 hierarchical.hpp:816] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 20150825-220736-234885548-51219-27610-S0 from framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.100440 27641 hierarchical.hpp:428] Removed framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.100608 27643 master.cpp:860] Master terminating I0825 22:07:54.100778  2360 exec.cpp:380] Executor asked to shutdown II0825 22:07:54.100929 27641 hierarchical.hpp:573] Removed slave 20150825-220736-234885548-51219-27610-S0 0825 22:07:54.100896  2364 exec.cpp:79] Scheduling shutdown of the executor I0825 22:07:54.100958  2360 exec.cpp:395] Executor::shutdown took 75333ns Shutting down Sending SIGTERM to process tree at pid 2371 I0825 22:07:54.101748 27640 slave.cpp:3143] master@172.17.0.14:51219 exited W0825 22:07:54.101866 27640 slave.cpp:3146] Master disconnected! Waiting for a new master to be elected I0825 22:07:54.106029 27632 containerizer.cpp:1079] Destroying container '1499299a-93dd-4982-9249-ad0e19d1c06c' Killing the following process trees: [  -+- 2371 sh -c sleep 1000   \--- 2372 sleep 1000  ] I0825 22:07:54.211082 27639 containerizer.cpp:1266] Executor for container '1499299a-93dd-4982-9249-ad0e19d1c06c' has exited I0825 22:07:54.211087 27630 containerizer.cpp:1266] Executor for container '1499299a-93dd-4982-9249-ad0e19d1c06c' has exited I0825 22:07:54.211143 27639 containerizer.cpp:1079] Destroying container '1499299a-93dd-4982-9249-ad0e19d1c06c' I0825 22:07:54.212609 27637 slave.cpp:3399] Executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 terminated with signal Killed I0825 22:07:54.212685 27637 slave.cpp:3503] Cleaning up executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.213062 27631 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c' for gc 6.99999753474667days in the future I0825 22:07:54.214745 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d' for gc 6.99999753268444days in the future I0825 22:07:54.214859 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c' for gc 6.99999751446815days in the future I0825 22:07:54.214921 27637 slave.cpp:3592] Cleaning up framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.215047 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d' for gc 6.99999751310222days in the future I0825 22:07:54.215140 27634 status_update_manager.cpp:284] Closing status update streams for framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.215338 27634 status_update_manager.cpp:530] Cleaning up status update stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 I0825 22:07:54.215358 27637 slave.cpp:564] Slave terminating I0825 22:07:54.215347 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000' for gc 6.99999751012741days in the future I0825 22:07:54.215608 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000' for gc 6.99999750907259days in the future ../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <58-B2 02-68 3A-2B 00-00>, 1, 1)          Expected: to be called once            Actual: never called - unsatisfied and active ../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <58-B2 02-68 3A-2B 00-00>, 1, 1-byte object <A8>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] SlaveTest.HTTPSchedulerSlaveRestart (17413 ms) {code}",2
"MESOS-3321","Spurious fetcher message about extracting an archive","The fetcher emits a spurious log message about not extracting an archive with "".tgz"" extension, even though the tarball is extracted correctly.  {code} I0826 19:02:08.304914  2109 logging.cpp:172] INFO level logging started! I0826 19:02:08.305253  2109 fetcher.cpp:413] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/20150826-185716-251662764-5050-1-S0\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""file:\/\/\/mesos\/sampleflaskapp.tgz""}}],""sandbox_directory"":""\/tmp\/mesos\/slaves\/20150826-185716-251662764-5050-1-S0\/frameworks\/20150826-185716-251662764-5050-1-0000\/executors\/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011\/runs\/e71f50b8-816d-46d5-bcc6-f9850a0402ed"",""user"":""root""} I0826 19:02:08.306834  2109 fetcher.cpp:368] Fetching URI 'file:///mesos/sampleflaskapp.tgz' I0826 19:02:08.306864  2109 fetcher.cpp:242] Fetching directly into the sandbox directory I0826 19:02:08.306884  2109 fetcher.cpp:179] Fetching URI 'file:///mesos/sampleflaskapp.tgz' I0826 19:02:08.306900  2109 fetcher.cpp:159] Copying resource with command:cp '/mesos/sampleflaskapp.tgz' '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz' I0826 19:02:08.309063  2109 fetcher.cpp:76] Extracting with command: tar -C '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed' -xf '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz' I0826 19:02:08.315313  2109 fetcher.cpp:84] Extracted '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz' into '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed' W0826 19:02:08.315381  2109 fetcher.cpp:264] Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: file:///mesos/sampleflaskapp.tgz I0826 19:02:08.315604  2109 fetcher.cpp:445] Fetched 'file:///mesos/sampleflaskapp.tgz' to '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz' {code}",1
"MESOS-3340","Command-line flags should take precedence over OS Env variables","Currently, it appears that re-defining a flag on the command-line that was already defined via a OS Env var ({{MESOS_*}}) causes the Master to fail with a not very helpful message.  For example, if one has {{MESOS_QUORUM}} defined, this happens: {noformat} $ ./mesos-master --zk=zk://192.168.1.4/mesos --quorum=1 --hostname=192.168.1.4 --ip=192.168.1.4 Duplicate flag 'quorum' on command line {noformat}  which is not very helpful.  Ideally, we would parse the flags with a ""well-known"" priority (command-line first, environment last) - but at the very least, the error message should be more helpful in explaining what the issue is.",2
"MESOS-3366","Allow resources/attributes discovery","In heterogeneous clusters, tasks sometimes have strong constraints on the type of hardware they need to execute on. The current solution is to use custom resources and attributes on the agents. Detecting non-standard resources/attributes requires wrapping the ""mesos-slave"" binary behind a script and use custom code to probe the agent. Unfortunately, this approach doesn't allow composition. The solution would be to provide a hook/module mechanism to allow users to use custom code performing resources/attributes discovery.  Please review the detailed document below: https://docs.google.com/document/d/15OkebDezFxzeyLsyQoU0upB0eoVECAlzEkeg0HQAX9w  Feel free to express comments/concerns by annotating the document or by replying to this issue. ",3
"MESOS-3375","Add executor protobuf to v1","A new protobuf for Executor was introduced in Mesos for the HTTP API, it needs to be added to /v1 so it reflects changes made on v1/mesos.proto. This protobuf is ought to be changed as the executor HTTP API design evolves.",1
"MESOS-3378","Document a test pattern for expediting event firing","We use {{Clock::advance()}} extensively in tests to expedite event firing and minimize overall {{make check}} time. Document this pattern for posterity.",3
"MESOS-3393","Remove unused executor protobuf","The executor protobuf definition living outside the v1/ directory is unused, it should  be removed to avoid confusion.",1
"MESOS-3413","Docker containerizer does not symlink persistent volumes into sandbox","For the ArangoDB framework I am trying to use the persistent primitives. nearly all is working, but I am missing a crucial piece at the end: I have successfully created a persistent disk resource and have set the persistence and volume information in the DiskInfo message. However, I do not see any way to find out what directory on the host the mesos slave has reserved for us. I know it is ${MESOS_SLAVE_WORKDIR}/volumes/roles/<myRole>/<NAME>_<UUID> but we have no way to query this information anywhere. The docker containerizer does not automatically mount this directory into our docker container, or symlinks it into our sandbox. Therefore, I have essentially no access to it. Note that the mesos containerizer (which I cannot use for other reasons) seems to create a symlink in the sandbox to the actual path for the persistent volume. With that, I could mount the volume into our docker container and all would be well.",5
"MESOS-3417","Log source address replicated log recieved broadcasts","Currently Mesos doesn't log what machine a replicated log status broadcast was recieved from: {code} Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.320164 15637 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request Sep 11 21:41:14 master-01 mesos-dns[15583]: I0911 21:41:14.321097   15583 detect.go:118] ignoring children-changed event, leader has not changed: /mesos Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.353914 15639 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.479132 15639 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request {code}  It would be really useful for debugging replicated log startup issues to have info about where the message came from (libprocess address, ip, or hostname) the message came from",2
"MESOS-3426","process::collect and process::await do not perform discard propagation.","When aggregating futures with collect, one may discard the outer future:  {code} Promise<int> p1; Promise<string> p2;  Future<int, string> collect = process::collect(p1.future(), p2.future());  collect.discard();  // collect will transition to DISCARDED  // However, p{1,2}.future().hasDiscard() remains false // as there is no discard propagation! {code}  Discard requests should propagate down into the inner futures being collected.",3
"MESOS-3458","Segfault when accepting or declining inverse offers","Discovered while writing a test for filters (in regards to inverse offers).  Fix here: https://reviews.apache.org/r/38470/",1
"MESOS-3485","Make hook execution order deterministic","Currently, when using multiple hooks of the same type, the execution order is implementation-defined.   This is because in src/hook/manager.cpp, the list of available hooks is stored in a {{hashmap<string, Hook*>}}. A hashmap is probably unnecessary for this task since the number of hooks should remain reasonable. A data structure preserving ordering should be used instead to allow the user to predict the execution order of the hooks. I suggest that the execution order should be the order in which hooks are specified with {{--hooks}} when starting an agent/master.  This will be useful when combining multiple hooks after MESOS-3366 is done.",3
"MESOS-3492","Expose maintenance user doc via the documentation home page","The committed docs can be found here: http://mesos.apache.org/documentation/latest/maintenance/  We need to add a link to {{docs/home.md}} Also, the doc needs some minor formatting tweaks.",1
"MESOS-3496","Create interface for digest verifier","Add interface for digest verifier so that we can add implementations for digest types like sha256, sha512 etc",2
"MESOS-3513","Cgroups Test Filters aborts tests on Centos 6.6 ","Running make check on centos 6.6 causes all tests to abort due to CHECK_SOME test in CgroupsFIlter:  {code} Build directory: /home/jenkins/workspace/mesos-config-centos6/build F0923 23:00:49.748896 27362 environment.cpp:132] CHECK_SOME(hierarchies_): Failed to determine canonical path of /sys/fs/cgroup/freezer: No such file or directory  *** Check failure stack trace: ***     @     0x7fb786ca0c4d  google::LogMessage::Fail()     @     0x7fb786ca298c  google::LogMessage::SendToLog()     @     0x7fb786ca083c  google::LogMessage::Flush()     @     0x7fb786ca3289  google::LogMessageFatal::~LogMessageFatal()     @           0x58e66c  mesos::internal::tests::CgroupsFilter::CgroupsFilter()     @           0x58712f  mesos::internal::tests::Environment::Environment()     @           0x4c882f  main     @     0x7fb782767d5d  __libc_start_main     @           0x4d6331  (unknown) make[3]: *** [check-local] Aborted {code}",1
"MESOS-3525","Figure out how to enforce 64-bit builds on Windows.","We need to make sure people don't try to compile Mesos on 32-bit architectures. We don't want a Windows repeat of something like this:  https://issues.apache.org/jira/browse/MESOS-267",3
"MESOS-3540","Libevent termination triggers Broken Pipe","When the libevent loop terminates and we unblock the {{SIGPIPE}} signal, the pending {{SIGPIPE}} instantly triggers and causes a broken pipe when the test binary stops running. {code} Program received signal SIGPIPE, Broken pipe. [Switching to Thread 0x7ffff18b4700 (LWP 16270)] pthread_sigmask (how=1, newmask=<optimized out>, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthread_sigmask.c:53 53 ../sysdeps/unix/sysv/linux/pthread_sigmask.c: No such file or directory. (gdb) bt #0  pthread_sigmask (how=1, newmask=<optimized out>, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthread_sigmask.c:53 #1  0x00000000006fd9a4 in unblock () at ../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/signals.hpp:90 #2  0x00000000007d7915 in run () at ../../../3rdparty/libprocess/src/libevent.cpp:125 #3  0x00000000007950cb in _M_invoke<>(void) () at /usr/include/c++/4.9/functional:1700 #4  0x0000000000795000 in operator() () at /usr/include/c++/4.9/functional:1688 #5  0x0000000000794f6e in _M_run () at /usr/include/c++/4.9/thread:115 #6  0x00007ffff668de30 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6 #7  0x00007ffff79a16aa in start_thread (arg=0x7ffff18b4700) at pthread_create.c:333 #8  0x00007ffff5df1eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109 {code}",2
"MESOS-3551","Replace use of strerror with thread-safe alternatives strerror_r / strerror_l.","{{strerror()}} is not required to be thread safe by POSIX and is listed as unsafe on Linux:  http://pubs.opengroup.org/onlinepubs/9699919799/ http://man7.org/linux/man-pages/man3/strerror.3.html  I don't believe we've seen any issues reported due to this. We should replace occurrences of strerror accordingly, possibly offering a wrapper in stout to simplify callsites.",3
"MESOS-3554","Allocator changes trigger large re-compiles","Due to the templatized nature of the allocator, even small changes trigger large recompiles of the code-base. This make iterating on changes expensive for developers.",3
"MESOS-3571","Refactor registry_client","Refactor registry client component to:  - Make methods shorter for readability - Pull out structs not related to registry client into common namespace.",5
"MESOS-3573","Mesos does not kill orphaned docker containers","After upgrade to 0.24.0 we noticed hanging containers appearing. Looks like there were changes between 0.23.0 and 0.24.0 that broke cleanup.  Here's how to trigger this bug:  1. Deploy app in docker container. 2. Kill corresponding mesos-docker-executor process 3. Observe hanging container  Here are the logs after kill:  {noformat} slave_1    | I1002 12:12:59.362002  7791 docker.cpp:1576] Executor for container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8' has exited slave_1    | I1002 12:12:59.362284  7791 docker.cpp:1374] Destroying container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8' slave_1    | I1002 12:12:59.363404  7791 docker.cpp:1478] Running docker stop on container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8' slave_1    | I1002 12:12:59.363876  7791 slave.cpp:3399] Executor 'sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c' of framework 20150923-122130-2153451692-5050-1-0000 terminated with signal Terminated slave_1    | I1002 12:12:59.367570  7791 slave.cpp:2696] Handling status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 from @0.0.0.0:0 slave_1    | I1002 12:12:59.367842  7791 slave.cpp:5094] Terminating task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c slave_1    | W1002 12:12:59.368484  7791 docker.cpp:986] Ignoring updating unknown container: f083aaa2-d5c3-43c1-b6ba-342de8829fa8 slave_1    | I1002 12:12:59.368671  7791 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 slave_1    | I1002 12:12:59.368741  7791 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 slave_1    | I1002 12:12:59.370636  7791 status_update_manager.cpp:376] Forwarding update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to the slave slave_1    | I1002 12:12:59.371335  7791 slave.cpp:2975] Forwarding the update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to master@172.16.91.128:5050 slave_1    | I1002 12:12:59.371908  7791 slave.cpp:2899] Status update manager successfully handled status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 master_1   | I1002 12:12:59.372047    11 master.cpp:4069] Status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 from slave 20151002-120829-2153451692-5050-1-S0 at slave(1)@172.16.91.128:5051 (172.16.91.128) master_1   | I1002 12:12:59.372534    11 master.cpp:4108] Forwarding status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 master_1   | I1002 12:12:59.373018    11 master.cpp:5576] Updating the latest state of task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to TASK_FAILED master_1   | I1002 12:12:59.373447    11 hierarchical.hpp:814] Recovered cpus(*):0.1; mem(*):16; ports(*):[31685-31685] (total: cpus(*):4; mem(*):1001; disk(*):52869; ports(*):[31000-32000], allocated: cpus(*):8.32667e-17) on slave 20151002-120829-2153451692-5050-1-S0 from framework 20150923-122130-2153451692-5050-1-0000 {noformat}  Another issue: if you restart mesos-slave on the host with orphaned docker containers, they are not getting killed. This was the case before and I hoped for this trick to kill hanging containers, but it doesn't work now.  Marking this as critical because it hoards cluster resources and blocks scheduling.",5
"MESOS-3579","FetcherCacheTest.LocalUncachedExtract is flaky","From ASF CI: https://builds.apache.org/job/Mesos/866/COMPILER=clang,CONFIGURATION=--verbose,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/console  {code} [ RUN      ] FetcherCacheTest.LocalUncachedExtract Using temporary directory '/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA' I0925 19:15:39.541198 27410 leveldb.cpp:176] Opened db in 3.43934ms I0925 19:15:39.542362 27410 leveldb.cpp:183] Compacted db in 1.136184ms I0925 19:15:39.542428 27410 leveldb.cpp:198] Created db iterator in 35866ns I0925 19:15:39.542448 27410 leveldb.cpp:204] Seeked to beginning of db in 8807ns I0925 19:15:39.542459 27410 leveldb.cpp:273] Iterated through 0 keys in the db in 6325ns I0925 19:15:39.542505 27410 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0925 19:15:39.543143 27438 recover.cpp:449] Starting replica recovery I0925 19:15:39.543393 27438 recover.cpp:475] Replica is in EMPTY status I0925 19:15:39.544373 27436 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0925 19:15:39.544791 27433 recover.cpp:195] Received a recover response from a replica in EMPTY status I0925 19:15:39.545284 27433 recover.cpp:566] Updating replica status to STARTING I0925 19:15:39.546155 27436 master.cpp:376] Master c8bf1c95-50f4-4832-a570-c560f0b466ae (f57fd4291168) started on 172.17.1.195:41781 I0925 19:15:39.546257 27433 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 747249ns I0925 19:15:39.546288 27433 replica.cpp:323] Persisted replica status to STARTING I0925 19:15:39.546483 27434 recover.cpp:475] Replica is in STARTING status I0925 19:15:39.546187 27436 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/master"" --zk_session_timeout=""10secs"" I0925 19:15:39.546567 27436 master.cpp:423] Master only allowing authenticated frameworks to register I0925 19:15:39.546617 27436 master.cpp:428] Master only allowing authenticated slaves to register I0925 19:15:39.546632 27436 credentials.hpp:37] Loading credentials for authentication from '/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/credentials' I0925 19:15:39.546931 27436 master.cpp:467] Using default 'crammd5' authenticator I0925 19:15:39.547044 27436 master.cpp:504] Authorization enabled I0925 19:15:39.547276 27441 whitelist_watcher.cpp:79] No whitelist given I0925 19:15:39.547320 27434 hierarchical.hpp:468] Initialized hierarchical allocator process I0925 19:15:39.547471 27438 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0925 19:15:39.548318 27443 recover.cpp:195] Received a recover response from a replica in STARTING status I0925 19:15:39.549067 27435 recover.cpp:566] Updating replica status to VOTING I0925 19:15:39.549115 27440 master.cpp:1603] The newly elected leader is master@172.17.1.195:41781 with id c8bf1c95-50f4-4832-a570-c560f0b466ae I0925 19:15:39.549162 27440 master.cpp:1616] Elected as the leading master! I0925 19:15:39.549190 27440 master.cpp:1376] Recovering from registrar I0925 19:15:39.549342 27434 registrar.cpp:309] Recovering registrar I0925 19:15:39.549666 27430 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418187ns I0925 19:15:39.549753 27430 replica.cpp:323] Persisted replica status to VOTING I0925 19:15:39.550089 27442 recover.cpp:580] Successfully joined the Paxos group I0925 19:15:39.550320 27442 recover.cpp:464] Recover process terminated I0925 19:15:39.550904 27430 log.cpp:661] Attempting to start the writer I0925 19:15:39.551955 27434 replica.cpp:477] Replica received implicit promise request with proposal 1 I0925 19:15:39.552351 27434 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 380746ns I0925 19:15:39.552372 27434 replica.cpp:345] Persisted promised to 1 I0925 19:15:39.552896 27436 coordinator.cpp:231] Coordinator attemping to fill missing position I0925 19:15:39.554003 27432 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0925 19:15:39.554534 27432 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 510572ns I0925 19:15:39.554558 27432 replica.cpp:679] Persisted action at 0 I0925 19:15:39.555516 27443 replica.cpp:511] Replica received write request for position 0 I0925 19:15:39.555595 27443 leveldb.cpp:438] Reading position from leveldb took 65355ns I0925 19:15:39.556177 27443 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 542757ns I0925 19:15:39.556200 27443 replica.cpp:679] Persisted action at 0 I0925 19:15:39.556813 27429 replica.cpp:658] Replica received learned notice for position 0 I0925 19:15:39.557251 27429 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 422272ns I0925 19:15:39.557281 27429 replica.cpp:679] Persisted action at 0 I0925 19:15:39.557315 27429 replica.cpp:664] Replica learned NOP action at position 0 I0925 19:15:39.558061 27442 log.cpp:677] Writer started with ending position 0 I0925 19:15:39.559294 27434 leveldb.cpp:438] Reading position from leveldb took 56536ns I0925 19:15:39.560333 27432 registrar.cpp:342] Successfully fetched the registry (0B) in 10.936064ms I0925 19:15:39.560469 27432 registrar.cpp:441] Applied 1 operations in 41340ns; attempting to update the 'registry' I0925 19:15:39.561244 27441 log.cpp:685] Attempting to append 176 bytes to the log I0925 19:15:39.561378 27436 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1 I0925 19:15:39.562126 27439 replica.cpp:511] Replica received write request for position 1 I0925 19:15:39.562515 27439 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 364968ns I0925 19:15:39.562539 27439 replica.cpp:679] Persisted action at 1 I0925 19:15:39.563160 27438 replica.cpp:658] Replica received learned notice for position 1 I0925 19:15:39.563699 27438 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 455933ns I0925 19:15:39.563730 27438 replica.cpp:679] Persisted action at 1 I0925 19:15:39.563753 27438 replica.cpp:664] Replica learned APPEND action at position 1 I0925 19:15:39.564749 27434 registrar.cpp:486] Successfully updated the 'registry' in 4.214016ms I0925 19:15:39.564893 27434 registrar.cpp:372] Successfully recovered registrar I0925 19:15:39.564950 27442 log.cpp:704] Attempting to truncate the log to 1 I0925 19:15:39.565039 27429 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2 I0925 19:15:39.565172 27430 master.cpp:1413] Recovered 0 slaves from the Registry (137B) ; allowing 10mins for slaves to re-register I0925 19:15:39.565946 27429 replica.cpp:511] Replica received write request for position 2 I0925 19:15:39.566349 27429 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 375473ns I0925 19:15:39.566371 27429 replica.cpp:679] Persisted action at 2 I0925 19:15:39.566994 27431 replica.cpp:658] Replica received learned notice for position 2 I0925 19:15:39.567440 27431 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 437095ns I0925 19:15:39.567483 27431 leveldb.cpp:401] Deleting ~1 keys from leveldb took 31954ns I0925 19:15:39.567498 27431 replica.cpp:679] Persisted action at 2 I0925 19:15:39.567514 27431 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0925 19:15:39.576660 27410 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix W0925 19:15:39.577055 27410 backend.cpp:50] Failed to create 'bind' backend: BindBackend requires root privileges I0925 19:15:39.583020 27443 slave.cpp:190] Slave started on 46)@172.17.1.195:41781 I0925 19:15:39.583062 27443 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus(*):1000; mem(*):1000"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4"" I0925 19:15:39.583472 27443 credentials.hpp:85] Loading credential for authentication from '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/credential' I0925 19:15:39.583752 27443 slave.cpp:321] Slave using credential for: test-principal I0925 19:15:39.584249 27443 slave.cpp:354] Slave resources: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] I0925 19:15:39.584344 27443 slave.cpp:390] Slave hostname: f57fd4291168 I0925 19:15:39.584362 27443 slave.cpp:395] Slave checkpoint: true I0925 19:15:39.585180 27428 state.cpp:54] Recovering state from '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta' I0925 19:15:39.585383 27440 status_update_manager.cpp:202] Recovering status update manager I0925 19:15:39.585636 27435 containerizer.cpp:386] Recovering containerizer I0925 19:15:39.586380 27438 slave.cpp:4110] Finished recovery I0925 19:15:39.586845 27438 slave.cpp:4267] Querying resource estimator for oversubscribable resources I0925 19:15:39.587059 27430 status_update_manager.cpp:176] Pausing sending status updates I0925 19:15:39.587064 27438 slave.cpp:705] New master detected at master@172.17.1.195:41781 I0925 19:15:39.587139 27438 slave.cpp:768] Authenticating with master master@172.17.1.195:41781 I0925 19:15:39.587163 27438 slave.cpp:773] Using default CRAM-MD5 authenticatee I0925 19:15:39.587321 27438 slave.cpp:741] Detecting new master I0925 19:15:39.587357 27434 authenticatee.cpp:115] Creating new client SASL connection I0925 19:15:39.587574 27438 slave.cpp:4281] Received oversubscribable resources  from the resource estimator I0925 19:15:39.587739 27442 master.cpp:5138] Authenticating slave(46)@172.17.1.195:41781 I0925 19:15:39.587853 27441 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(139)@172.17.1.195:41781 I0925 19:15:39.588052 27439 authenticator.cpp:92] Creating new server SASL connection I0925 19:15:39.588248 27431 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0925 19:15:39.588297 27431 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0925 19:15:39.588443 27437 authenticator.cpp:197] Received SASL authentication start I0925 19:15:39.588506 27437 authenticator.cpp:319] Authentication requires more steps I0925 19:15:39.588677 27443 authenticatee.cpp:252] Received SASL authentication step I0925 19:15:39.588814 27436 authenticator.cpp:225] Received SASL authentication step I0925 19:15:39.588855 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0925 19:15:39.588876 27436 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0925 19:15:39.588937 27436 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0925 19:15:39.588979 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0925 19:15:39.588997 27436 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0925 19:15:39.589011 27436 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0925 19:15:39.589036 27436 authenticator.cpp:311] Authentication success I0925 19:15:39.589126 27443 authenticatee.cpp:292] Authentication success I0925 19:15:39.589192 27437 master.cpp:5168] Successfully authenticated principal 'test-principal' at slave(46)@172.17.1.195:41781 I0925 19:15:39.589238 27433 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(139)@172.17.1.195:41781 I0925 19:15:39.589412 27440 slave.cpp:836] Successfully authenticated with master master@172.17.1.195:41781 I0925 19:15:39.589540 27440 slave.cpp:1230] Will retry registration in 13.562027ms if necessary I0925 19:15:39.589745 27436 master.cpp:3862] Registering slave at slave(46)@172.17.1.195:41781 (f57fd4291168) with id c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 I0925 19:15:39.590121 27438 registrar.cpp:441] Applied 1 operations in 70627ns; attempting to update the 'registry' I0925 19:15:39.590831 27430 log.cpp:685] Attempting to append 345 bytes to the log I0925 19:15:39.590927 27439 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3 I0925 19:15:39.591809 27430 replica.cpp:511] Replica received write request for position 3 I0925 19:15:39.592072 27430 leveldb.cpp:343] Persisting action (364 bytes) to leveldb took 221734ns I0925 19:15:39.592099 27430 replica.cpp:679] Persisted action at 3 I0925 19:15:39.592643 27442 replica.cpp:658] Replica received learned notice for position 3 I0925 19:15:39.593215 27442 leveldb.cpp:343] Persisting action (366 bytes) to leveldb took 560946ns I0925 19:15:39.593237 27442 replica.cpp:679] Persisted action at 3 I0925 19:15:39.593255 27442 replica.cpp:664] Replica learned APPEND action at position 3 I0925 19:15:39.594663 27433 registrar.cpp:486] Successfully updated the 'registry' in 4.472832ms I0925 19:15:39.594874 27431 log.cpp:704] Attempting to truncate the log to 3 I0925 19:15:39.595407 27429 slave.cpp:3138] Received ping from slave-observer(45)@172.17.1.195:41781 I0925 19:15:39.595450 27433 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4 I0925 19:15:39.596017 27442 replica.cpp:511] Replica received write request for position 4 I0925 19:15:39.596029 27429 hierarchical.hpp:675] Added slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: ) I0925 19:15:39.595952 27441 master.cpp:3930] Registered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] I0925 19:15:39.596240 27429 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:39.596263 27439 slave.cpp:880] Registered with master master@172.17.1.195:41781; given slave ID c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 I0925 19:15:39.596341 27439 fetcher.cpp:77] Clearing fetcher cache I0925 19:15:39.596345 27429 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:39.596367 27429 hierarchical.hpp:1239] Performed allocation for slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 in 299337ns I0925 19:15:39.596524 27434 status_update_manager.cpp:183] Resuming sending status updates I0925 19:15:39.596571 27442 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 575374ns I0925 19:15:39.596662 27442 replica.cpp:679] Persisted action at 4 I0925 19:15:39.596984 27439 slave.cpp:903] Checkpointing SlaveInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/slave.info' I0925 19:15:39.597522 27434 replica.cpp:658] Replica received learned notice for position 4 I0925 19:15:39.597553 27410 sched.cpp:164] Version: 0.26.0 I0925 19:15:39.597746 27439 slave.cpp:939] Forwarding total oversubscribed resources  I0925 19:15:39.598021 27429 master.cpp:4272] Received update of slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with total oversubscribed resources  I0925 19:15:39.598070 27434 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 531503ns I0925 19:15:39.598162 27434 leveldb.cpp:401] Deleting ~2 keys from leveldb took 79081ns I0925 19:15:39.598170 27428 sched.cpp:262] New master detected at master@172.17.1.195:41781 I0925 19:15:39.598206 27434 replica.cpp:679] Persisted action at 4 I0925 19:15:39.598238 27434 replica.cpp:664] Replica learned TRUNCATE action at position 4 I0925 19:15:39.598276 27428 sched.cpp:318] Authenticating with master master@172.17.1.195:41781 I0925 19:15:39.598296 27428 sched.cpp:325] Using default CRAM-MD5 authenticatee I0925 19:15:39.598950 27430 hierarchical.hpp:735] Slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) updated with oversubscribed resources  (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) I0925 19:15:39.599242 27430 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:39.599282 27430 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:39.599341 27430 hierarchical.hpp:1239] Performed allocation for slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 in 327742ns I0925 19:15:39.599632 27437 authenticatee.cpp:115] Creating new client SASL connection I0925 19:15:39.600005 27428 master.cpp:5138] Authenticating scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:39.600170 27435 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(140)@172.17.1.195:41781 I0925 19:15:39.600518 27433 authenticator.cpp:92] Creating new server SASL connection I0925 19:15:39.600788 27436 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0925 19:15:39.600831 27436 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0925 19:15:39.600944 27433 authenticator.cpp:197] Received SASL authentication start I0925 19:15:39.601019 27433 authenticator.cpp:319] Authentication requires more steps I0925 19:15:39.601150 27436 authenticatee.cpp:252] Received SASL authentication step I0925 19:15:39.601284 27436 authenticator.cpp:225] Received SASL authentication step I0925 19:15:39.601326 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0925 19:15:39.601341 27436 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0925 19:15:39.601387 27436 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0925 19:15:39.601413 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0925 19:15:39.601421 27436 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0925 19:15:39.601428 27436 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0925 19:15:39.601439 27436 authenticator.cpp:311] Authentication success I0925 19:15:39.601508 27433 authenticatee.cpp:292] Authentication success I0925 19:15:39.601644 27433 master.cpp:5168] Successfully authenticated principal 'test-principal' at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:39.601671 27436 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(140)@172.17.1.195:41781 I0925 19:15:39.601842 27434 sched.cpp:407] Successfully authenticated with master master@172.17.1.195:41781 I0925 19:15:39.601869 27434 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.1.195:41781 I0925 19:15:39.601955 27434 sched.cpp:747] Will retry registration in 749.975107ms if necessary I0925 19:15:39.602046 27443 master.cpp:2179] Received SUBSCRIBE call for framework 'default' at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 W0925 19:15:39.602128 27443 master.cpp:2186] Framework at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo I0925 19:15:39.602149 27443 master.cpp:1642] Authorizing framework principal '' to receive offers for role '*' I0925 19:15:39.602375 27437 master.cpp:2250] Subscribing framework default with checkpointing enabled and capabilities [  ] I0925 19:15:39.602712 27429 hierarchical.hpp:515] Added framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.602859 27437 sched.cpp:641] Framework registered with c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.602905 27437 sched.cpp:655] Scheduler::registered took 30086ns I0925 19:15:39.603204 27429 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:39.603234 27429 hierarchical.hpp:1221] Performed allocation for 1 slaves in 506104ns I0925 19:15:39.603520 27438 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:39.603962 27431 sched.cpp:811] Scheduler::resourceOffers took 123790ns I0925 19:15:39.605443 27432 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O0 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:39.605485 27432 master.cpp:2714] Authorizing framework principal '' to launch task 0 as user 'mesos' I0925 19:15:39.606487 27432 master.hpp:176] Adding task 0 with resources cpus(*):1; mem(*):1 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) I0925 19:15:39.606586 27432 master.cpp:3248] Launching task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 with resources cpus(*):1; mem(*):1 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) I0925 19:15:39.606875 27440 slave.cpp:1270] Got assigned task 0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.607050 27439 hierarchical.hpp:1103] Recovered cpus(*):999; mem(*):999; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):1) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.607087 27440 slave.cpp:4773] Checkpointing FrameworkInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/framework.info' I0925 19:15:39.607103 27439 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs I0925 19:15:39.607573 27440 slave.cpp:4784] Checkpointing framework pid 'scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781' to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/framework.pid' I0925 19:15:39.608544 27440 slave.cpp:1386] Launching task 0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.615109 27440 slave.cpp:5209] Checkpointing ExecutorInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/executor.info' I0925 19:15:39.616000 27440 slave.cpp:4852] Launching executor 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a' I0925 19:15:39.616510 27441 containerizer.cpp:640] Starting container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' for executor '0' of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' I0925 19:15:39.616612 27440 slave.cpp:5232] Checkpointing TaskInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a/tasks/0/task.info' I0925 19:15:39.617144 27440 slave.cpp:1604] Queuing task '0' for executor 0 of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.617277 27440 slave.cpp:658] Successfully attached file '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a' I0925 19:15:39.619359 27437 launcher.cpp:132] Forked child with pid '30069' for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' I0925 19:15:39.619583 27437 containerizer.cpp:873] Checkpointing executor's forked pid 30069 to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a/pids/forked.pid' I0925 19:15:39.622011 27441 fetcher.cpp:299] Starting to fetch URIs for container: 4bc31eb2-709b-4b09-a5a9-21a8387e355a, directory: /tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a I0925 19:15:39.633872 27441 fetcher.cpp:756] Fetching URIs using command '/mesos/mesos-0.26.0/_build/src/mesos-fetcher' E0925 19:15:39.724884 27430 fetcher.cpp:515] Failed to run mesos-fetcher: Failed to fetch all URIs for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' with exit status: 256 Failed to synchronize with slave (it's probably exited) E0925 19:15:39.725486 27443 slave.cpp:3342] Container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' for executor '0' of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' failed to start: Failed to fetch all URIs for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' with exit status: 256 I0925 19:15:39.725620 27430 containerizer.cpp:1097] Destroying container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' I0925 19:15:39.725651 27430 containerizer.cpp:1126] Waiting for the isolators to complete for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' I0925 19:15:39.825744 27443 containerizer.cpp:1284] Executor for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' has exited I0925 19:15:39.827075 27429 slave.cpp:3440] Executor '0' of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 exited with status 1 I0925 19:15:39.827324 27429 slave.cpp:2717] Handling status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 from @0.0.0.0:0 I0925 19:15:39.827514 27429 slave.cpp:5147] Terminating task 0 W0925 19:15:39.827745 27436 containerizer.cpp:988] Ignoring update for unknown container: 4bc31eb2-709b-4b09-a5a9-21a8387e355a I0925 19:15:39.828073 27440 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.828168 27440 status_update_manager.cpp:499] Creating StatusUpdate stream for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.828661 27440 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.830041 27440 status_update_manager.cpp:376] Forwarding update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to the slave I0925 19:15:39.830292 27434 slave.cpp:3016] Forwarding the update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to master@172.17.1.195:41781 I0925 19:15:39.830492 27434 slave.cpp:2940] Status update manager successfully handled status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.830641 27432 master.cpp:4415] Status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 from slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) I0925 19:15:39.830682 27432 master.cpp:4454] Forwarding status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.830842 27432 master.cpp:6081] Updating the latest state of task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to TASK_FAILED I0925 19:15:39.831075 27431 sched.cpp:918] Scheduler::statusUpdate took 176815ns I0925 19:15:39.831204 27439 hierarchical.hpp:1103] Recovered cpus(*):1; mem(*):1 (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.831357 27432 master.cpp:6149] Removing task 0 with resources cpus(*):1; mem(*):1 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) I0925 19:15:39.831491 27432 master.cpp:3606] Processing ACKNOWLEDGE call 6bb8651c-0668-4724-8fbd-76db8a91adb7 for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 I0925 19:15:39.831763 27437 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.831957 27437 status_update_manager.cpp:826] Checkpointing ACK for status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.833057 27437 status_update_manager.cpp:530] Cleaning up status update stream for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.833407 27432 slave.cpp:2319] Status update manager successfully handled status update acknowledgement (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.833447 27432 slave.cpp:5188] Completing task 0 I0925 19:15:39.833470 27432 slave.cpp:3544] Cleaning up executor '0' of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.833768 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a' for gc 6.99999035100741days in the future I0925 19:15:39.833933 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0' for gc 6.99999034949333days in the future I0925 19:15:39.834005 27432 slave.cpp:3633] Cleaning up framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.834031 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a' for gc 6.99999034847111days in the future I0925 19:15:39.834106 27430 status_update_manager.cpp:284] Closing status update streams for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:39.834121 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0' for gc 6.99999034757926days in the future I0925 19:15:39.834266 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' for gc 6.99999034594963days in the future I0925 19:15:39.834360 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' for gc 6.99999034517333days in the future I0925 19:15:40.549545 27428 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:40.549640 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 849712ns I0925 19:15:40.550092 27442 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:40.550679 27442 sched.cpp:811] Scheduler::resourceOffers took 157498ns I0925 19:15:40.551633 27432 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O1 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:40.552602 27432 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:40.552672 27432 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs I0925 19:15:41.551115 27428 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:41.551200 27428 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:41.551224 27428 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:41.551239 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 595589ns I0925 19:15:42.552183 27433 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:42.552254 27433 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:42.552271 27433 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:42.552281 27433 hierarchical.hpp:1221] Performed allocation for 1 slaves in 496429ns I0925 19:15:43.553062 27442 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:43.553134 27442 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:43.553151 27442 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:43.553163 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 482544ns I0925 19:15:44.554844 27443 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:44.554930 27443 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:44.554954 27443 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:44.554970 27443 hierarchical.hpp:1221] Performed allocation for 1 slaves in 699469ns I0925 19:15:45.556754 27442 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:45.556805 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 702577ns I0925 19:15:45.557119 27437 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:45.557569 27435 sched.cpp:811] Scheduler::resourceOffers took 122887ns I0925 19:15:45.558279 27433 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O2 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:45.559015 27441 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:45.559070 27441 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs I0925 19:15:46.558176 27439 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:46.558245 27439 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:46.558262 27439 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:46.558274 27439 hierarchical.hpp:1221] Performed allocation for 1 slaves in 509658ns I0925 19:15:47.559289 27429 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:47.559360 27429 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:47.559376 27429 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:47.559386 27429 hierarchical.hpp:1221] Performed allocation for 1 slaves in 495131ns I0925 19:15:48.560979 27442 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:48.561064 27442 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:48.561087 27442 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:48.561101 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 710782ns I0925 19:15:49.562594 27431 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:49.562666 27431 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:49.562683 27431 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:49.562695 27431 hierarchical.hpp:1221] Performed allocation for 1 slaves in 525867ns I0925 19:15:50.564564 27428 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:50.564620 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 621850ns I0925 19:15:50.565004 27432 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:50.565457 27428 sched.cpp:811] Scheduler::resourceOffers took 110220ns I0925 19:15:50.566159 27437 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O3 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:50.566815 27428 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:50.566869 27428 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs I0925 19:15:51.565913 27433 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:51.565981 27433 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:51.565999 27433 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:51.566009 27433 hierarchical.hpp:1221] Performed allocation for 1 slaves in 504883ns I0925 19:15:52.567260 27432 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:52.567333 27432 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:52.567350 27432 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:52.567361 27432 hierarchical.hpp:1221] Performed allocation for 1 slaves in 513500ns I0925 19:15:53.568176 27438 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:53.568248 27438 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:53.568266 27438 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:53.568281 27438 hierarchical.hpp:1221] Performed allocation for 1 slaves in 522293ns I0925 19:15:54.570142 27430 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:54.570226 27430 hierarchical.hpp:1326] No resources available to allocate! I0925 19:15:54.570250 27430 hierarchical.hpp:1421] No inverse offers to send out! I0925 19:15:54.570264 27430 hierarchical.hpp:1221] Performed allocation for 1 slaves in 626798ns I0925 19:15:54.588251 27442 slave.cpp:4267] Querying resource estimator for oversubscribable resources I0925 19:15:54.588673 27443 slave.cpp:4281] Received oversubscribable resources  from the resource estimator I0925 19:15:54.596678 27428 slave.cpp:3138] Received ping from slave-observer(45)@172.17.1.195:41781 ../../src/tests/fetcher_cache_tests.cpp:681: Failure Failed to wait 15secs for awaitFinished(task.get()) I0925 19:15:54.606274 27410 sched.cpp:1771] Asked to stop the driver I0925 19:15:54.606623 27439 master.cpp:1119] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 disconnected I0925 19:15:54.606679 27439 master.cpp:2475] Disconnecting framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:54.606855 27439 master.cpp:2499] Deactivating framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:54.607441 27439 master.cpp:1143] Giving framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 0ns to failover I0925 19:15:54.607770 27433 hierarchical.hpp:599] Deactivated framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:54.609256 27432 master.cpp:4815] Framework failover timeout, removing framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:54.609297 27432 master.cpp:5571] Removing framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 I0925 19:15:54.609501 27433 slave.cpp:1980] Asked to shut down framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 by master@172.17.1.195:41781 W0925 19:15:54.609549 27433 slave.cpp:1995] Cannot shut down unknown framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:54.609881 27432 master.cpp:919] Master terminating I0925 19:15:54.610255 27440 hierarchical.hpp:552] Removed framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 I0925 19:15:54.610627 27440 hierarchical.hpp:706] Removed slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 I0925 19:15:54.611197 27436 slave.cpp:3184] master@172.17.1.195:41781 exited W0925 19:15:54.611233 27436 slave.cpp:3187] Master disconnected! Waiting for a new master to be elected I0925 19:15:54.616207 27410 slave.cpp:585] Slave terminating [  FAILED  ] FetcherCacheTest.LocalUncachedExtract (15091 ms) {code} ",2
"MESOS-3584","rename libprocess tests to ""libprocess-tests""","Stout tests are in a binary named {{stout-tests}}, Mesos tests are in {{mesos-tests}}, but libprocess tests are just {{tests}}. It would be helpful to name them {{libprocess-tests}} ",1
"MESOS-3604","ExamplesTest.PersistentVolumeFramework does not work in OS X El Capitan","The example persistent volume framework test does not pass in OS X El Capitan. It seems to be executing the {{<build_dir>/src/.libs/mesos-executor}} directly while it should be executing the wrapper script at {{<build_dir>/src/mesos-executor}} instead. The no-executor framework passes however, which seem to have a very similar configuration with the persistent volume framework. The following is the output that shows the {{dyld}} load error:  {noformat} I1008 01:22:52.280140 4284416 launcher.cpp:132] Forked child with pid '1706' for contain er 'b6d3bd96-2ebd-47b1-a16a-a22ffba992aa' I1008 01:22:52.280300 4284416 containerizer.cpp:873] Checkpointing executor's forked pid  1706 to '/var/folders/p6/nfxknpz52dzfc6zqnz23tq180000gn/T/mesos-XXXXXX.5OZ3locB/0/meta/ slaves/34d6329e-69cb-4a72-aee4-fe892bf1c70b-S2/frameworks/34d6329e-69cb-4a72-aee4-fe892b f1c70b-0000/executors/dec188d4-d2dc-40c5-ac4d-881adc3d81c0/runs/b6d3bd96-2ebd-47b1-a16a- a22ffba992aa/pids/forked.pid' dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib   Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor   Reason: image not found dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib   Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor   Reason: image not found dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib   Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor   Reason: image not found I1008 01:22:52.365397 3211264 containerizer.cpp:1284] Executor for container '06b649be-88c8-4047-8fb5-e89bdd096b66' has exited I1008 01:22:52.365433 3211264 containerizer.cpp:1097] Destroying container '06b649be-88c8-4047-8fb5-e89bdd096b66' {noformat}",3
"MESOS-3615","Port slave/state.cpp","Important subset of changes this depends on:  slave/state.cpp: pid, os, path, protobuf, paths, state pid.hpp: address.hpp, ip.hpp address.hpp: ip.hpp, net.hpp net.hpp: ip, networking stuff state: type_utils, pid, os, path, protobuf, uuid type_utils.hpp: uuid.hpp",3
"MESOS-3692","Clarify error message 'could not chown work directory'","When deploying a framework I encountered the error message 'could not chown work directory'.  It took me a while to figure out that this happened because my framework was registered as a user on my host machine which did not exist on the Docker container and the agent was running as root.  I suggest to clarify this message by pointing out to either set {{--switch-user}}  to {{false}} or to run the framework as the same user as the agent.",1
"MESOS-3716","Update Allocator interface to support quota","An allocator should be notified when a quota is being set/updated or removed. Also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota.",3
"MESOS-3718","Implement Quota support in allocator","The built-in Hierarchical DRF allocator should support Quota. This includes (but not limited to): adding, updating, removing and satisfying quota; avoiding both overcomitting resources and handing them to non-quota'ed roles in presence of master failover.  A [design doc for Quota support in Allocator|https://issues.apache.org/jira/browse/MESOS-2937] provides an overview of a feature set required to be implemented.",5
"MESOS-3743","Provide diagnostic output in agent log when fetching fails","When fetching fails, the fetcher has written log output to stderr in the task sandbox, but it is not easy to get to. It may even be impossible to get to if one only has the agent log available and no more access to the sandbox. This is for instance the case when looking at output from a CI run.  The fetcher actor in the agent detects if the external fetcher program claims to have succeeded or not. When it exits with an error code, we could grab the fetcher log from the stderr file in the sandbox and append it to the agent log.  This is similar to this patch: https://reviews.apache.org/r/37813/  The difference is that the output of the latter is triggered by test failures outside the fetcher, whereas what is proposed here is triggering upon failures inside the fetcher.",2
"MESOS-3785","Use URI content modification time to trigger fetcher cache updates.","Instead of using checksums to trigger fetcher cache updates, we can for starters use the content modification time (mtime), which is available for a number of download protocols, e.g. HTTP and HDFS.  Proposal: Instead of just fetching the content size, we fetch both size  and mtime together. As before, if there is no size, then caching fails and we fall back on direct downloading to the sandbox.   Assuming a size is given, we compare the mtime from the fetch URI with the mtime known to the cache. If it differs, we update the cache. (As a defensive measure, a difference in size should also trigger an update.)   Not having an mtime available at the fetch URI is simply treated as a unique valid mtime value that differs from all others. This means that when initially there is no mtime, cache content remains valid until there is one. Thereafter,  anew lack of an mtime invalidates the cache once. In other words: any change from no mtime to having one or back is the same as encountering a new mtime.  Note that this scheme does not require any new protobuf fields. ",5
"MESOS-3833","/help endpoints do not work for nested paths","Mesos displays the list of all supported endpoints starting at a given path prefix using the {{/help}} suffix, e.g. {{master:5050/help}}.  It seems that the {{help}} functionality is broken for URL's having nested paths e.g. {{master:5050/help/master/machine/down}}. The response returned is: {quote} Malformed URL, expecting '/help/id/name/' {quote}",2
"MESOS-3849","Corrected style in Makefiles","Order of files in Makefiles is not strictly alphabetic",1
"MESOS-3864","Simplify and/or document the libprocess initialization synchronization logic","Tracks this [TODO|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L749].  The [synchronization logic of libprocess|https://github.com/apache/mesos/commit/cd757cf75637c92c438bf4cd22f21ba1b5be702f#diff-128d3b56fc8c9ec0176fdbadcfd11fc2] [predates abstractions|https://github.com/apache/mesos/commit/6c3b107e4e02d5ba0673eb3145d71ec9d256a639#diff-0eebc8689450916990abe080d86c2acb] like {{process::Once}}, which is used in almost all other one-time initialization blocks.    The logic should be documented.  It can also be simplified (see the [review description|https://reviews.apache.org/r/39949/]).  Or it can be replaced with {{process::Once}}.",1
"MESOS-3873","Enhance allocator interface with the recovery() method","There are some scenarios (e.g. quota is set for some roles) when it makes sense to notify an allocator about the recovery. Introduce a method into the allocator interface that allows for this.",3
"MESOS-3882","Libprocess: Implement process::Clock::finalize","Tracks this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L974-L975].  The {{Clock}} is initialized with a callback that, among other things, will dereference the global {{process_manager}} object.  When libprocess is shutting down, the {{process_manager}} is cleaned up.  Between cleanup and termination of libprocess, there is some chance that a {{Timer}} will time out and result in dereferencing {{process_manager}}.  *Proposal*  * Implement {{Clock::finalize}}.  This would clear: ** existing timers ** process-specific clocks ** ticks * Change {{process::finalize}}. *# Resume the clock.  (The clock is only paused during some tests.)  When the clock is not paused, the callback does not dereference {{process_manager}}. *# Clean up {{process_manager}}.  This terminates all the processes that would potentially interact with {{Clock}}. *# Call {{Clock::finalize}}.",3
"MESOS-3884","Corrected style in hierarchical allocator","The built-in allocator code has some style issues (namespaces in the .cpp file, unfortunate formatting) which should be corrected for readability.",1
"MESOS-3899","Wrong syntax and inconsistent formatting of JSON examples in flag documentation","The JSON examples in the documentation of the commandline flags ({{mesos-master.sh --help}} and {{mesos-slave.sh --help}}) don't have a consistent formatting. Furthermore, some examples aren't even compliant JSON because they have trailing commas were they shouldn't.",1
"MESOS-3900","Enable mesos-reviewbot project on jenkins to use docker","As a first step to adding capability for building multiple configurations on reviewbot, we need to change the build scripts to use docker. ",3
"MESOS-3909","isolator module headers depend on picojson headers","When trying to build an isolator module, stout headers end up depending on {{picojson.hpp}} which is not installed.  {code} In file included from /opt/mesos/include/mesos/module/isolator.hpp:25: In file included from /opt/mesos/include/mesos/slave/isolator.hpp:30: In file included from /opt/mesos/include/process/dispatch.hpp:22: In file included from /opt/mesos/include/process/process.hpp:26: In file included from /opt/mesos/include/process/event.hpp:21: In file included from /opt/mesos/include/process/http.hpp:39: /opt/mesos/include/stout/json.hpp:23:10: fatal error: 'picojson.h' file not found #include <picojson.h>          ^ 8 warnings and 1 error generated. {code}",3
"MESOS-3911","Add a `--force` flag to disable sanity check in quota","There are use cases when an operator may want to disable the sanity check for quota endpoints (MESOS-3074), even if this renders the cluster under quota. For example, an operator sets quota before adding more agents in order to make sure that no non-quota allocations from new agents are made. ",1
"MESOS-3912","Rescind offers in order to satisfy quota","When a quota request comes in, we may need to rescind a certain amount of outstanding offers in order to satisfy it. Because resources are allocated in the allocator, there can be a race between rescinding and allocating. This race makes it hard to determine the exact amount of offers that should be rescinded in the master.",3
"MESOS-3936","Document possible task state transitions for framework authors","We should document the possible ways in which the state of a task can evolve over time; what happens when an agent is partitioned from the master; and more generally, how we recommend that framework authors develop fault-tolerant schedulers and do task state reconciliation.",5
"MESOS-3939","ubsan error in net::IP::create(sockaddr const&): misaligned address","Running ubsan from GCC 5.2 on the current Mesos unit tests yields this, among other problems:  {noformat} /mesos/3rdparty/libprocess/3rdparty/stout/include/stout/ip.hpp:230:56: runtime error: reference binding to misaligned address 0x00000199629c for type 'const struct sockaddr_storage', which requires 8 byte alignment 0x00000199629c: note: pointer points here   00 00 00 00 02 00 00 00  ff ff ff 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00               ^     #0 0x5950cb in net::IP::create(sockaddr const&) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x5950cb)     #1 0x5970cd in net::IPNetwork::fromLinkDevice(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x5970cd)     #2 0x58e006 in NetTest_LinkDevice_Test::TestBody() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x58e006)     #3 0x85abd5 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x85abd5)     #4 0x848abc in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x848abc)     #5 0x7e2755 in testing::Test::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e2755)     #6 0x7e44a0 in testing::TestInfo::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e44a0)     #7 0x7e5ffa in testing::TestCase::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e5ffa)     #8 0x7ffe21 in testing::internal::UnitTestImpl::RunAllTests() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7ffe21)     #9 0x85d7a5 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x85d7a5)     #10 0x84b37a in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x84b37a)     #11 0x7f8a4a in testing::UnitTest::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7f8a4a)     #12 0x608a96 in RUN_ALL_TESTS() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x608a96)     #13 0x60896b in main (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x60896b)     #14 0x7fd0f0c7fa3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x20a3f)     #15 0x4145c8 in _start (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x4145c8) {noformat}",2
"MESOS-3949","User CGroup Isolation tests fail on Centos 6.","UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup and UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup fail on CentOS 6.6 with similar output when libevent and SSL are enabled.  {noformat} sudo ./bin/mesos-tests.sh --gtest_filter=""UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup"" --verbose {noformat} {noformat} [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from UserCgroupIsolatorTest/0, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess userdel: user 'mesos.test.unprivileged.user' does not exist [ RUN      ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup I1118 16:53:35.273717 30249 mem.cpp:605] Started listening for OOM events for container 867a829e-4a26-43f5-86e0-938bf1f47688 I1118 16:53:35.274538 30249 mem.cpp:725] Started listening on low memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688 I1118 16:53:35.275164 30249 mem.cpp:725] Started listening on medium memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688 I1118 16:53:35.275784 30249 mem.cpp:725] Started listening on critical memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688 I1118 16:53:35.276448 30249 mem.cpp:356] Updated 'memory.soft_limit_in_bytes' to 1GB for container 867a829e-4a26-43f5-86e0-938bf1f47688 I1118 16:53:35.277331 30249 mem.cpp:391] Updated 'memory.limit_in_bytes' to 1GB for container 867a829e-4a26-43f5-86e0-938bf1f47688 -bash: /sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/cgroup.procs: No such file or directory mkdir: cannot create directory `/sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/user': No such file or directory ../../src/tests/containerizer/isolator_tests.cpp:1307: Failure Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")   Actual: 256 Expected: 0 -bash: /sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/user/cgroup.procs: No such file or directory ../../src/tests/containerizer/isolator_tests.cpp:1316: Failure Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")   Actual: 256 Expected: 0 [  FAILED  ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess (149 ms) {noformat}  {noformat} sudo ./bin/mesos-tests.sh --gtest_filter=""UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup"" --verbose {noformat} {noformat} [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from UserCgroupIsolatorTest/1, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess userdel: user 'mesos.test.unprivileged.user' does not exist [ RUN      ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup I1118 17:01:00.550706 30357 cpushare.cpp:392] Updated 'cpu.shares' to 1024 (cpus 1) for container e57f4343-1a97-4b44-b347-803be47ace80 -bash: /sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/cgroup.procs: No such file or directory mkdir: cannot create directory `/sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user': No such file or directory ../../src/tests/containerizer/isolator_tests.cpp:1307: Failure Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")   Actual: 256 Expected: 0 -bash: /sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user/cgroup.procs: No such file or directory ../../src/tests/containerizer/isolator_tests.cpp:1316: Failure Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")   Actual: 256 Expected: 0 -bash: /sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/cgroup.procs: No such file or directory mkdir: cannot create directory `/sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user': No such file or directory ../../src/tests/containerizer/isolator_tests.cpp:1307: Failure Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")   Actual: 256 Expected: 0 -bash: /sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user/cgroup.procs: No such file or directory ../../src/tests/containerizer/isolator_tests.cpp:1316: Failure Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")   Actual: 256 Expected: 0 [  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess (116 ms) {noformat}",3
"MESOS-3964","LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs and LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs_Big_Quota fail on Debian 8.","sudo ./bin/mesos-test.sh --gtest_filter=""LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs""  {noformat} ... F1119 14:34:52.514742 30706 isolator_tests.cpp:455] CHECK_SOME(isolator): Failed to find 'cpu.cfs_quota_us'. Your kernel might be too old to use the CFS cgroups feature. {noformat} ",2
"MESOS-3965","Ensure resources in `QuotaInfo` protobuf do not contain `role`","{{QuotaInfo}} protobuf currently stores per-role quotas, including {{Resource}} objects. These resources are neither statically nor dynamically reserved, hence they may not contain {{role}} field. We should ensure this field is unset, as well as update validation routine for {{QuotaInfo}}",3
"MESOS-3981","Implement recovery in the Hierarchical allocator","The built-in Hierarchical allocator should implement the recovery (in the presence of quota).",3
"MESOS-3983","Tests for quota request validation","Tests should include: * JSON validation; * Absence of irrelevant fields; * Semantic validation.",3
"MESOS-4004","Support default entrypoint and command runtime config in Mesos containerizer","We need to use the entrypoint and command runtime configuration returned from image to be used in Mesos containerizer.",3
"MESOS-4005","Support workdir runtime configuration from image ","We need to support workdir runtime configuration returned from image such as Dockerfile.",2
"MESOS-4046","Enable `Env` specified in docker image can be returned from docker pull","Currently docker pull only return an image structure, which only contains entrypoint info. We have docker inspect as a subprocess inside docker pull, which contains many other useful information of a docker image. We should be able to support returning environment variables information from the image.",3
"MESOS-4047","MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky","{code:title=Output from passed test} [----------] 1 test from MemoryPressureMesosTest 1+0 records in 1+0 records out 1048576 bytes (1.0 MB) copied, 0.000430889 s, 2.4 GB/s [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery I1202 11:09:14.319327  5062 exec.cpp:134] Version: 0.27.0 I1202 11:09:14.333317  5079 exec.cpp:208] Executor registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 Registered executor on ubuntu Starting task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162 sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done' Forked command at 5085 I1202 11:09:14.391739  5077 exec.cpp:254] Received reconnect request from slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 I1202 11:09:14.398598  5082 exec.cpp:231] Executor re-registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 Re-registered executor on ubuntu Shutting down Sending SIGTERM to process tree at pid 5085 Killing the following process trees: [  -+- 5085 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done   \--- 5086 dd count=512 bs=1M if=/dev/zero of=./temp  ] [       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (1096 ms) {code}  {code:title=Output from failed test} [----------] 1 test from MemoryPressureMesosTest 1+0 records in 1+0 records out 1048576 bytes (1.0 MB) copied, 0.000404489 s, 2.6 GB/s [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery I1202 11:09:15.509950  5109 exec.cpp:134] Version: 0.27.0 I1202 11:09:15.568183  5123 exec.cpp:208] Executor registered on slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0 Registered executor on ubuntu Starting task 14b6bab9-9f60-4130-bdc4-44efba262bc6 Forked command at 5132 sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done' I1202 11:09:15.665498  5129 exec.cpp:254] Received reconnect request from slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0 I1202 11:09:15.670995  5123 exec.cpp:381] Executor asked to shutdown Shutting down Sending SIGTERM to process tree at pid 5132 ../../src/tests/containerizer/memory_pressure_tests.cpp:283: Failure (usage).failure(): Unknown container: ebe90e15-72fa-4519-837b-62f43052c913 *** Aborted at 1449083355 (unix time) try ""date -d @1449083355"" if you are using GNU date *** {code}  Notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent.",1
"MESOS-4087","Introduce a module for logging executor/task output","Existing executor/task logs are logged to files in their sandbox directory, with some nuances based on which containerizer is used (see background section in linked document).  A logger for executor/task logs has the following requirements: * The logger is given a command to run and must handle the stdout/stderr of the command. * The handling of stdout/stderr must be resilient across agent failover.  Logging should not stop if the agent fails. * Logs should be readable, presumably via the web UI, or via some other module-specific UI.",5
"MESOS-4107","`os::strerror_r` breaks the Windows build","`os::strerror_r` does not exist on Windows.",1
"MESOS-4108","Implement `os::mkdtemp` for Windows","Used basically exclusively for testing, this insecure and otherwise-not-quite-suitable-for-prod function needs to work to run what will eventually become the FS tests.",5
"MESOS-4109","HTTPConnectionTest.ClosingResponse is flaky","Output of the test: {code} [ RUN      ] HTTPConnectionTest.ClosingResponse I1210 01:20:27.048532 26671 process.cpp:3077] Handling HTTP event for process '(22)' with path: '/(22)/get' ../../../3rdparty/libprocess/src/tests/http_tests.cpp:919: Failure Actual function call count doesn't match EXPECT_CALL(*http.process, get(_))...          Expected: to be called twice            Actual: called once - unsatisfied and active [  FAILED  ] HTTPConnectionTest.ClosingResponse (43 ms) {code}",1
"MESOS-4110","Implement `WindowsError` to correspond with `ErrnoError`.","In the C standard library, `errno` records the last error on a thread. You can pretty-print it with `strerror`.  In Stout, we report these errors with `ErrnoError`.  The Windows API has something similar, called `GetLastError()`. The way to pretty-print this is hilariously unintuitive and terrible, so in this case it is actually very beneficial to wrap it with something similar to `ErrnoError`, maybe called `WindowsError`.",5
"MESOS-4130","Document how the fetcher can reach across a proxy connection.","The fetcher uses libcurl for downloading content from HTTP, HTTPS, etc. There is no source code in the pertinent parts of ""net.hpp"" that deals with proxy settings. However, libcurl automatically picks up certain environment variables and adjusts its settings accordingly. See ""man libcurl-tutorial"" for details. See section ""Proxies"", subsection ""Environment Variables"". If you follow this recipe in your Mesos agent startup script, you can use a proxy.   We should document this in the fetcher (cache) doc (http://mesos.apache.org/documentation/latest/fetcher/). ",1
"MESOS-4136","Add a ContainerLogger module that restrains log sizes","One of the major problems this logger module aims to solve is overflowing executor/task log files.  Log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).  We should add a {{ContainerLogger}} module that truncates logs as it reaches a configurable maximum size.  Additionally, we should determine if the web UI's {{pailer}} needs to be changed to deal with logs that are not append-only.  This will be a non-default module which will also serve as an example for how to implement the module.",3
"MESOS-4150","Implement container logger module metadata recovery","The {{ContainerLoggers}} are intended to be isolated from agent failover, in the same way that executors do not crash when the agent process crashes.  For default {{ContainerLogger}} s, like the {{SandboxContainerLogger}} and the (tentatively named) {{TruncatingSandboxContainerLogger}}, the log files are exposed during agent recovery regardless.  For non-default {{ContainerLogger}} s, the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs.  This can be implemented as part of {{Containerizer::recover}}.",3
"MESOS-4192","Add documentation for API Versioning","Currently, we don't have any documentation for:  - How Mesos implements API versioning ? - How are protobufs versioned and how does mesos handle them internally ? - What do contributors need to do when they make a change to a external user facing protobuf ?  The relevant design doc: https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b ",3
"MESOS-4200","Test case(s) for weights + allocation behavior","As far as I can see, we currently have NO test cases for behavior when weights are defined.",2
"MESOS-4207","Add an example bug due to a lack of defer() to the defer() documentation","In the past, some bugs have been introduced into the codebase due to a lack of {{defer()}} where it should have been used. It would be useful to add an example of this to the {{defer()}} documentation.",2
"MESOS-4209","Document ""how to program with dynamic reservations and persistent volumes""","Specifically, some of the gotchas around:  * Retrying reservation attempts after a timeout * Fuzzy-matching resources to determine whether a reservation/PV is successful * Represent client state as a state machine and repeatedly move ""toward"" successful terminate stats  Should also point to persistent volume example framework. We should also ask Gabriel and others (Arango?) who have built frameworks with PVs/DRs for feedback.",3
"MESOS-4222","Document containerizer from user perspective.","Add documentation that covers:  * Purpose of containerizers from a use case perspective. * What purpose does each containerizer (mesos. docker, compose) serve. * What criteria could be used to choose a containerizer.",3
"MESOS-4226","Enable passing docker image environment variables runtime config to provisioner","Collect environment variables runtime config information from a docker image, and save as a map. Pass it back to provisioner, and handling environment variables merge issue.",1
"MESOS-4241","Consolidate docker store slave flags","Currently there are too many slave flags for configuring the docker store/puller. We can remove the following flags:  docker_auth_server_port docker_local_archives_dir docker_registry_port docker_puller  And consolidate them into the existing flags.",3
"MESOS-4257","ExamplesTest.NoExecutorFramework runs forever.","{noformat: title=Good Run}  [ RUN      ] ExamplesTest.NoExecutorFramework  I1221 23:10:02.721617 32528 exec.cpp:444] Ignoring exited event because the driver is aborted!  Using temporary directory '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn'  I1221 23:10:02.721675 32539 exec.cpp:444] Ignoring exited event because the driver is aborted!  I1221 23:10:02.722024 32554 exec.cpp:444] Ignoring exited event because the driver is aborted!  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 23:10:05.179466 32569 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32;disk:32  Trying semicolon-delimited string format instead  I1221 23:10:05.180269 32569 logging.cpp:172] Logging to STDERR  I1221 23:10:05.185768 32569 process.cpp:998] libprocess is initialized on 172.17.0.2:40874 for 16 cpus  I1221 23:10:05.200728 32569 leveldb.cpp:174] Opened db in 4.184362ms  I1221 23:10:05.202234 32569 leveldb.cpp:181] Compacted db in 1.459268ms  I1221 23:10:05.202353 32569 leveldb.cpp:196] Created db iterator in 73761ns  I1221 23:10:05.202383 32569 leveldb.cpp:202] Seeked to beginning of db in 3382ns  I1221 23:10:05.202405 32569 leveldb.cpp:271] Iterated through 0 keys in the db in 633ns  I1221 23:10:05.202674 32569 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I1221 23:10:05.205301 32604 recover.cpp:447] Starting replica recovery  I1221 23:10:05.206414 32569 local.cpp:239] Using 'local' authorizer  I1221 23:10:05.206405 32604 recover.cpp:473] Replica is in EMPTY status  I1221 23:10:05.209595 32594 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.2:40874  I1221 23:10:05.210916 32596 recover.cpp:193] Received a recover response from a replica in EMPTY status  I1221 23:10:05.211515 32597 master.cpp:365] Master 3931c1a8-1cd6-49eb-94c8-d01b33bb008e (6ccf2ee56b13) started on 172.17.0.2:40874  I1221 23:10:05.211699 32605 recover.cpp:564] Updating replica status to STARTING  I1221 23:10:05.211539 32597 master.cpp:367] Flags at startup: --acls=""permissive: false  register_frameworks {    principals {      type: SOME      values: ""test-principal""    }    roles {      type: SOME      values: ""*""    }  }  run_tasks {    principals {      type: SOME      values: ""test-principal""    }    users {      type: SOME      values: ""mesos""    }  }  "" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-otpdch"" --zk_session_timeout=""10secs""  I1221 23:10:05.212323 32597 master.cpp:412] Master only allowing authenticated frameworks to register  I1221 23:10:05.212337 32597 master.cpp:419] Master allowing unauthenticated slaves to register  I1221 23:10:05.212347 32597 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials'  W1221 23:10:05.212442 32597 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.  I1221 23:10:05.212606 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 656857ns  I1221 23:10:05.212620 32597 master.cpp:456] Using default 'crammd5' authenticator  I1221 23:10:05.212631 32600 replica.cpp:320] Persisted replica status to STARTING  I1221 23:10:05.212893 32597 authenticator.cpp:518] Initializing server SASL  I1221 23:10:05.213091 32608 recover.cpp:473] Replica is in STARTING status  I1221 23:10:05.213958 32595 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.2:40874  I1221 23:10:05.214323 32594 recover.cpp:193] Received a recover response from a replica in STARTING status  I1221 23:10:05.214689 32595 recover.cpp:564] Updating replica status to VOTING  I1221 23:10:05.215353 32596 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 487419ns  I1221 23:10:05.215384 32596 replica.cpp:320] Persisted replica status to VOTING  I1221 23:10:05.215481 32605 recover.cpp:578] Successfully joined the Paxos group  I1221 23:10:05.215867 32605 recover.cpp:462] Recover process terminated  I1221 23:10:05.216111 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  W1221 23:10:05.217021 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.221482 32608 slave.cpp:191] Slave started on 1)@172.17.0.2:40874  I1221 23:10:05.221521 32608 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/0""  I1221 23:10:05.222578 32608 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.223465 32608 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.223621 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 23:10:05.223610 32608 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.223677 32608 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.223697 32608 slave.cpp:410] Slave checkpoint: true  W1221 23:10:05.224143 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.226668 32604 slave.cpp:191] Slave started on 2)@172.17.0.2:40874  I1221 23:10:05.226692 32604 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/1""  I1221 23:10:05.227520 32604 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.228037 32604 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.228148 32604 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.228169 32604 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.228184 32604 slave.cpp:410] Slave checkpoint: true  I1221 23:10:05.229123 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 23:10:05.229641 32605 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/0/meta'  W1221 23:10:05.229645 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 23:10:05.229636 32595 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/1/meta'  I1221 23:10:05.230242 32605 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.230254 32598 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.230515 32601 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.230562 32602 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.232681 32597 auxprop.cpp:71] Initialized in-memory auxiliary property plugin  I1221 23:10:05.232803 32597 master.cpp:493] Authorization enabled  I1221 23:10:05.232867 32600 slave.cpp:4427] Finished recovery  I1221 23:10:05.232980 32598 slave.cpp:191] Slave started on 3)@172.17.0.2:40874  I1221 23:10:05.233039 32594 slave.cpp:4427] Finished recovery  I1221 23:10:05.233376 32599 whitelist_watcher.cpp:77] No whitelist given  I1221 23:10:05.233428 32601 hierarchical.cpp:147] Initialized hierarchical allocator process  I1221 23:10:05.233003 32598 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/2""  I1221 23:10:05.233744 32600 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.233749 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 23:10:05.234222 32598 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.234284 32598 slave.cpp:400] Slave attributes: [  ]  I1221 23:10:05.234299 32598 slave.cpp:405] Slave hostname: 6ccf2ee56b13  I1221 23:10:05.234311 32598 slave.cpp:410] Slave checkpoint: true  I1221 23:10:05.234338 32600 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.234376 32604 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.234424 32600 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.234522 32600 slave.cpp:765] Detecting new master  I1221 23:10:05.234616 32569 sched.cpp:164] Version: 0.27.0  I1221 23:10:05.234658 32600 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.234671 32594 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.234884 32606 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.235038 32595 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.235043 32606 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.235111 32606 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.235147 32606 slave.cpp:765] Detecting new master  I1221 23:10:05.235240 32594 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/2/meta'  I1221 23:10:05.235443 32608 status_update_manager.cpp:200] Recovering status update manager  I1221 23:10:05.235625 32594 containerizer.cpp:383] Recovering containerizer  I1221 23:10:05.236549 32599 slave.cpp:4427] Finished recovery  I1221 23:10:05.236984 32593 sched.cpp:262] New master detected at master@172.17.0.2:40874  I1221 23:10:05.237004 32599 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 23:10:05.237221 32593 sched.cpp:318] Authenticating with master master@172.17.0.2:40874  I1221 23:10:05.237277 32593 sched.cpp:325] Using default CRAM-MD5 authenticatee  I1221 23:10:05.237285 32604 status_update_manager.cpp:174] Pausing sending status updates  I1221 23:10:05.237288 32599 slave.cpp:729] New master detected at master@172.17.0.2:40874  I1221 23:10:05.237361 32599 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 23:10:05.237433 32599 slave.cpp:765] Detecting new master  I1221 23:10:05.237565 32599 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 23:10:05.238154 32605 authenticatee.cpp:97] Initializing client SASL  I1221 23:10:05.238315 32605 authenticatee.cpp:121] Creating new client SASL connection  I1221 23:10:05.239640 32597 master.cpp:1200] Dropping 'mesos.internal.AuthenticateMessage' message since not elected yet  I1221 23:10:05.239765 32597 master.cpp:1629] The newly elected leader is master@172.17.0.2:40874 with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e  I1221 23:10:05.239794 32597 master.cpp:1642] Elected as the leading master!  I1221 23:10:05.239843 32597 master.cpp:1387] Recovering from registrar  I1221 23:10:05.240056 32600 registrar.cpp:307] Recovering registrar  I1221 23:10:05.241477 32608 log.cpp:659] Attempting to start the writer  I1221 23:10:05.244540 32600 replica.cpp:493] Replica received implicit promise request from (39)@172.17.0.2:40874 with proposal 1  I1221 23:10:05.245358 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 776937ns  I1221 23:10:05.245393 32600 replica.cpp:342] Persisted promised to 1  I1221 23:10:05.246625 32601 coordinator.cpp:238] Coordinator attempting to fill missing positions  I1221 23:10:05.248757 32605 replica.cpp:388] Replica received explicit promise request from (40)@172.17.0.2:40874 for position 0 with proposal 2  I1221 23:10:05.249214 32605 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 366567ns  I1221 23:10:05.249246 32605 replica.cpp:712] Persisted action at 0  I1221 23:10:05.250998 32599 replica.cpp:537] Replica received write request for position 0 from (41)@172.17.0.2:40874  I1221 23:10:05.251111 32599 leveldb.cpp:436] Reading position from leveldb took 66773ns  I1221 23:10:05.251734 32599 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 379612ns  I1221 23:10:05.251759 32599 replica.cpp:712] Persisted action at 0  I1221 23:10:05.252555 32601 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I1221 23:10:05.253010 32601 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 381858ns  I1221 23:10:05.253036 32601 replica.cpp:712] Persisted action at 0  I1221 23:10:05.253068 32601 replica.cpp:697] Replica learned NOP action at position 0  I1221 23:10:05.254043 32595 log.cpp:675] Writer started with ending position 0  I1221 23:10:05.256741 32595 leveldb.cpp:436] Reading position from leveldb took 48607ns  I1221 23:10:05.260617 32601 registrar.cpp:340] Successfully fetched the registry (0B) in 20.47616ms  I1221 23:10:05.260988 32601 registrar.cpp:439] Applied 1 operations in 103123ns; attempting to update the 'registry'  I1221 23:10:05.264700 32604 log.cpp:683] Attempting to append 170 bytes to the log  I1221 23:10:05.265138 32601 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I1221 23:10:05.266208 32603 replica.cpp:537] Replica received write request for position 1 from (42)@172.17.0.2:40874  I1221 23:10:05.266829 32603 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 551087ns  I1221 23:10:05.266861 32603 replica.cpp:712] Persisted action at 1  I1221 23:10:05.267918 32605 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0  I1221 23:10:05.268442 32605 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 453416ns  I1221 23:10:05.268470 32605 replica.cpp:712] Persisted action at 1  I1221 23:10:05.268506 32605 replica.cpp:697] Replica learned APPEND action at position 1  I1221 23:10:05.270512 32606 registrar.cpp:484] Successfully updated the 'registry' in 9.375232ms  I1221 23:10:05.270705 32606 registrar.cpp:370] Successfully recovered registrar  I1221 23:10:05.271045 32602 log.cpp:702] Attempting to truncate the log to 1  I1221 23:10:05.271178 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2  I1221 23:10:05.271695 32605 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register  I1221 23:10:05.271751 32603 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover  I1221 23:10:05.272274 32596 replica.cpp:537] Replica received write request for position 2 from (43)@172.17.0.2:40874  I1221 23:10:05.272838 32596 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 451122ns  I1221 23:10:05.272867 32596 replica.cpp:712] Persisted action at 2  I1221 23:10:05.273483 32607 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0  I1221 23:10:05.273919 32607 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 405444ns  I1221 23:10:05.273977 32607 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33953ns  I1221 23:10:05.273998 32607 replica.cpp:712] Persisted action at 2  I1221 23:10:05.274024 32607 replica.cpp:697] Replica learned TRUNCATE action at position 2  I1221 23:10:05.288020 32593 slave.cpp:1254] Will retry registration in 1.091568855secs if necessary  I1221 23:10:05.288321 32605 master.cpp:4132] Registering slave at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0  I1221 23:10:05.289105 32607 registrar.cpp:439] Applied 1 operations in 104776ns; attempting to update the 'registry'  I1221 23:10:05.291494 32605 log.cpp:683] Attempting to append 339 bytes to the log  I1221 23:10:05.291594 32595 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3  I1221 23:10:05.292273 32602 replica.cpp:537] Replica received write request for position 3 from (44)@172.17.0.2:40874  I1221 23:10:05.292811 32602 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 499449ns  I1221 23:10:05.292845 32602 replica.cpp:712] Persisted action at 3  I1221 23:10:05.293373 32594 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0  I1221 23:10:05.293776 32594 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 376184ns  I1221 23:10:05.293799 32594 replica.cpp:712] Persisted action at 3  I1221 23:10:05.293833 32594 replica.cpp:697] Replica learned APPEND action at position 3  I1221 23:10:05.295142 32603 registrar.cpp:484] Successfully updated the 'registry' in 5.945088ms  I1221 23:10:05.295403 32602 log.cpp:702] Attempting to truncate the log to 3  I1221 23:10:05.295513 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4  I1221 23:10:05.296146 32598 replica.cpp:537] Replica received write request for position 4 from (45)@172.17.0.2:40874  I1221 23:10:05.296443 32608 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:40874  I1221 23:10:05.296552 32598 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 368428ns  I1221 23:10:05.296604 32598 replica.cpp:712] Persisted action at 4  I1221 23:10:05.296744 32594 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.297025 32597 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0  I1221 23:10:05.297056 32597 fetcher.cpp:81] Clearing fetcher cache  I1221 23:10:05.297175 32599 status_update_manager.cpp:181] Resuming sending status updates  I1221 23:10:05.297184 32600 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )  I1221 23:10:05.297473 32597 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/2/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0/slave.info'  I1221 23:10:05.297618 32600 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.297688 32600 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 in 454887ns  I1221 23:10:05.298058 32597 slave.cpp:963] Forwarding total oversubscribed resources   I1221 23:10:05.298235 32597 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources   I1221 23:10:05.298396 32604 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0  I1221 23:10:05.298765 32597 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )  I1221 23:10:05.298907 32597 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.298933 32597 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 in 134328ns  I1221 23:10:05.298965 32604 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 505213ns  I1221 23:10:05.299031 32604 leveldb.cpp:399] Deleting ~2 keys from leveldb took 37007ns  I1221 23:10:05.299054 32604 replica.cpp:712] Persisted action at 4  I1221 23:10:05.299103 32604 replica.cpp:697] Replica learned TRUNCATE action at position 4  I1221 23:10:05.350281 32598 slave.cpp:1254] Will retry registration in 1.181298785secs if necessary  I1221 23:10:05.350510 32608 master.cpp:4132] Registering slave at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:05.351222 32607 registrar.cpp:439] Applied 1 operations in 118623ns; attempting to update the 'registry'  I1221 23:10:05.352174 32604 log.cpp:683] Attempting to append 505 bytes to the log  I1221 23:10:05.352375 32595 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5  I1221 23:10:05.353365 32601 replica.cpp:537] Replica received write request for position 5 from (46)@172.17.0.2:40874  I1221 23:10:05.353960 32601 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 552132ns  I1221 23:10:05.353986 32601 replica.cpp:712] Persisted action at 5  I1221 23:10:05.354867 32606 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0  I1221 23:10:05.355370 32606 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 456354ns  I1221 23:10:05.355399 32606 replica.cpp:712] Persisted action at 5  I1221 23:10:05.355432 32606 replica.cpp:697] Replica learned APPEND action at position 5  I1221 23:10:05.357318 32595 registrar.cpp:484] Successfully updated the 'registry' in 6.016768ms  I1221 23:10:05.357708 32595 log.cpp:702] Attempting to truncate the log to 5  I1221 23:10:05.357805 32606 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6  I1221 23:10:05.358273 32602 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:40874  I1221 23:10:05.358331 32599 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.358405 32602 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:05.358428 32602 fetcher.cpp:81] Clearing fetcher cache  I1221 23:10:05.358722 32599 status_update_manager.cpp:181] Resuming sending status updates  I1221 23:10:05.358736 32606 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )  I1221 23:10:05.358813 32599 replica.cpp:537] Replica received write request for position 6 from (47)@172.17.0.2:40874  I1221 23:10:05.358952 32602 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/slave.info'  I1221 23:10:05.358969 32606 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.358996 32606 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 in 217083ns  I1221 23:10:05.359350 32599 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 464454ns  I1221 23:10:05.359374 32599 replica.cpp:712] Persisted action at 6  I1221 23:10:05.359591 32602 slave.cpp:963] Forwarding total oversubscribed resources   I1221 23:10:05.359740 32606 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources   I1221 23:10:05.360227 32605 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0  I1221 23:10:05.360288 32606 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )  I1221 23:10:05.360539 32606 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.360591 32606 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 in 256841ns  I1221 23:10:05.360702 32605 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 444736ns  I1221 23:10:05.360759 32605 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30869ns  I1221 23:10:05.360777 32605 replica.cpp:712] Persisted action at 6  I1221 23:10:05.360800 32605 replica.cpp:697] Replica learned TRUNCATE action at position 6  I1221 23:10:05.957257 32601 slave.cpp:1254] Will retry registration in 627.120173ms if necessary  I1221 23:10:05.957504 32597 master.cpp:4132] Registering slave at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2  I1221 23:10:05.958284 32607 registrar.cpp:439] Applied 1 operations in 174321ns; attempting to update the 'registry'  I1221 23:10:05.959336 32594 log.cpp:683] Attempting to append 671 bytes to the log  I1221 23:10:05.959477 32606 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7  I1221 23:10:05.960484 32604 replica.cpp:537] Replica received write request for position 7 from (48)@172.17.0.2:40874  I1221 23:10:05.960891 32604 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 362101ns  I1221 23:10:05.960917 32604 replica.cpp:712] Persisted action at 7  I1221 23:10:05.961642 32597 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0  I1221 23:10:05.962502 32597 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 828414ns  I1221 23:10:05.962532 32597 replica.cpp:712] Persisted action at 7  I1221 23:10:05.962563 32597 replica.cpp:697] Replica learned APPEND action at position 7  I1221 23:10:05.964241 32598 registrar.cpp:484] Successfully updated the 'registry' in 5.87392ms  I1221 23:10:05.964552 32593 log.cpp:702] Attempting to truncate the log to 7  I1221 23:10:05.964643 32606 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8  I1221 23:10:05.964964 32593 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:40874  I1221 23:10:05.965152 32602 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2  I1221 23:10:05.965111 32600 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 23:10:05.965178 32602 fetcher.cpp:81] Clearing fetcher cache  I1221 23:10:05.965293 32607 status_update_manager.cpp:181] Resuming sending status updates  I1221 23:10:05.965293 32593 replica.cpp:537] Replica received write request for position 8 from (49)@172.17.0.2:40874  I1221 23:10:05.965637 32603 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )  I1221 23:10:05.965734 32602 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/1/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2/slave.info'  I1221 23:10:05.965751 32593 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 424617ns  I1221 23:10:05.965772 32593 replica.cpp:712] Persisted action at 8  I1221 23:10:05.965847 32603 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.965880 32603 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 in 209313ns  I1221 23:10:05.966145 32602 slave.cpp:963] Forwarding total oversubscribed resources   I1221 23:10:05.966325 32601 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0  I1221 23:10:05.966342 32596 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources   I1221 23:10:05.966703 32601 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 347667ns  I1221 23:10:05.966753 32601 leveldb.cpp:399] Deleting ~2 keys from leveldb took 29042ns  I1221 23:10:05.966770 32601 replica.cpp:712] Persisted action at 8  I1221 23:10:05.966791 32601 replica.cpp:697] Replica learned TRUNCATE action at position 8  I1221 23:10:05.966792 32602 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )  I1221 23:10:05.966943 32602 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:05.966969 32602 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 in 137680ns  I1221 23:10:06.235074 32595 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:06.235142 32595 hierarchical.cpp:1079] Performed allocation for 3 slaves in 594477ns  I1221 23:10:07.237238 32599 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:07.237309 32599 hierarchical.cpp:1079] Performed allocation for 3 slaves in 506418ns  I1221 23:10:07.702738 32564 exec.cpp:88] Committing suicide by killing the process group  II1221 23:10:07.703501 32546 exec.cpp:88] Committing suicide by killing the process group  1221 23:10:07.703501 32525 exec.cpp:88] Committing suicide by killing the process group  I1221 23:10:08.239282 32608 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:08.239356 32608 hierarchical.cpp:1079] Performed allocation for 3 slaves in 671976ns  I1221 23:10:09.241236 32600 hierarchical.cpp:1329] No resources available to allocate!  I1221 23:10:09.241303 32600 hierarchical.cpp:1079] Performed allocation for 3 slaves in 520741ns  W1221 23:10:10.239298 32601 sched.cpp:429] Authentication timed out  I1221 23:10:10.239653 32596 sched.cpp:387] Failed to authenticate with master master@172.17.0.2:40874: Authentication discarded  I1221 23:10:10.239714 32596 sched.cpp:318] Authenticating with master master@172.17.0.2:40874    ...  ...  ...  I1221 23:10:10.432864 32597 slave.cpp:1796] Sending queued task '0' to executor '0' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:60995  I1221 23:10:10.433215 32765 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.435856 32765 exec.cpp:220] Executor::registered took 343507ns  Registered executor on 6ccf2ee56b13  I1221 23:10:10.436198 32765 exec.cpp:295] Executor asked to run task '0'  I1221 23:10:10.436317 32765 exec.cpp:304] Executor::launchTask took 86293ns  Starting task 0  Forked command at 361  sh -c 'echo hello'  I1221 23:10:10.437671   335 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:33894 with pid 32696  I1221 23:10:10.438062   315 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:51530 with pid 32695  hello  I1221 23:10:10.440052 32599 slave.cpp:2578] Got registration for executor '2' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33894  I1221 23:10:10.440507 32599 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:33894' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/runs/63bf8626-189d-44f9-a961-b51b046d5057/pids/libprocess.pid'  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 23:10:10.440497 32733 process.cpp:998] libprocess is initialized on 172.17.0.2:33285 for 16 cpus  I1221 23:10:10.441426 32733 logging.cpp:172] Logging to STDERR  I1221 23:10:10.441916 32599 slave.cpp:2578] Got registration for executor '1' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:51530  I1221 23:10:10.442277 32599 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:51530' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/runs/468fb0f8-4dd0-448c-8e64-207cc4c81c63/pids/libprocess.pid'  I1221 23:10:10.442314   303 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.442684   339 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.443305 32599 slave.cpp:1796] Sending queued task '2' to executor '2' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:33894  I1221 23:10:10.443668 32733 exec.cpp:134] Version: 0.27.0  I1221 23:10:10.443768 32599 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:60995  I1221 23:10:10.444797   320 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.445159   339 exec.cpp:220] Executor::registered took 241955ns  Registered executor on 6ccf2ee56b13  I1221 23:10:10.445430   339 exec.cpp:295] Executor asked to run task '2'  Starting task 2  I1221 23:10:10.445519   339 exec.cpp:304] Executor::launchTask took 65600ns  I1221 23:10:10.445794 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.445888 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.446015 32599 slave.cpp:1796] Sending queued task '1' to executor '1' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:51530  Forked command at 362  I1221 23:10:10.446622 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.446990   320 exec.cpp:220] Executor::registered took 324561ns  sh -c 'echo hello'  Registered executor on 6ccf2ee56b13  I1221 23:10:10.447427   320 exec.cpp:295] Executor asked to run task '1'  Starting task 1  I1221 23:10:10.447540   320 exec.cpp:304] Executor::launchTask took 84828ns  Forked command at 363  sh -c 'echo hello'  hello  I1221 23:10:10.449244   334 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  hello  I1221 23:10:10.450145 32597 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33894  I1221 23:10:10.451092 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.451117   324 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.451370 32595 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.451637 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.451710 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.451652 32595 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.451838 32593 master.cpp:4687] Status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.451974 32593 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.452203 32593 master.cpp:6347] Updating the state of task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.451875 32595 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:60995  I1221 23:10:10.452308 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.452432 32603 no_executor_framework.cpp:160] Task '0' is in state TASK_RUNNING  I1221 23:10:10.452468 32603 sched.cpp:919] Scheduler::statusUpdate took 46157ns  I1221 23:10:10.452905 32606 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:51530  I1221 23:10:10.453151 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.453403   302 exec.cpp:341] Executor received status update acknowledgement 61620dd2-95c0-4570-b174-24ed26e0a289 for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.453454 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.453496 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.453488 32606 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.453840 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.453954 32603 master.cpp:3844] Processing ACKNOWLEDGE call 61620dd2-95c0-4570-b174-24ed26e0a289 for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.454144 32606 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.454195 32606 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:33894  I1221 23:10:10.454293 32600 master.cpp:4687] Status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.454344 32600 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.454502 32600 master.cpp:6347] Updating the state of task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.454665 32600 no_executor_framework.cpp:160] Task '2' is in state TASK_RUNNING  I1221 23:10:10.454691 32600 sched.cpp:919] Scheduler::statusUpdate took 31056ns  I1221 23:10:10.454856   335 exec.cpp:341] Executor received status update acknowledgement b774dc4c-940c-41d5-be2a-ed3a9e8379a7 for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.454898 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.455098 32600 master.cpp:3844] Processing ACKNOWLEDGE call b774dc4c-940c-41d5-be2a-ed3a9e8379a7 for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.455250 32605 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.455487 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.455533 32605 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.455538 32607 master.cpp:4687] Status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.455574 32607 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.455574 32605 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:51530  I1221 23:10:10.455685 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.455713 32607 master.cpp:6347] Updating the state of task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.455813 32606 no_executor_framework.cpp:160] Task '1' is in state TASK_RUNNING  I1221 23:10:10.455916 32606 sched.cpp:919] Scheduler::statusUpdate took 105226ns  I1221 23:10:10.455812   344 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:33285 with pid 32733  I1221 23:10:10.456140 32607 master.cpp:3844] Processing ACKNOWLEDGE call 7a5d5a7f-975d-4970-949e-cfa4d08a4a30 for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.456163   312 exec.cpp:341] Executor received status update acknowledgement 7a5d5a7f-975d-4970-949e-cfa4d08a4a30 for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.456812 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.456807 32606 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.457012 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.457720 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.457751 32607 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.457883 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.458319 32604 slave.cpp:2578] Got registration for executor '3' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33285  I1221 23:10:10.458766 32604 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:33285' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/runs/7f37bf6f-50f1-45a9-ae7a-20b1b829cc38/pids/libprocess.pid'  I1221 23:10:10.460311 32604 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.461024   349 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.461304 32603 slave.cpp:1796] Sending queued task '3' to executor '3' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:33285  I1221 23:10:10.462968   349 exec.cpp:220] Executor::registered took 273965ns  Registered executor on 6ccf2ee56b13  I1221 23:10:10.463317   349 exec.cpp:295] Executor asked to run task '3'  Starting task 3  I1221 23:10:10.463429   349 exec.cpp:304] Executor::launchTask took 87510ns  Forked command at 381  sh -c 'echo hello'  hello  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 23:10:10.466104 32747 process.cpp:998] libprocess is initialized on 172.17.0.2:39520 for 16 cpus  I1221 23:10:10.466836   351 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.467958 32604 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33285  I1221 23:10:10.468389 32595 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.468441 32595 status_update_manager.cpp:497] Creating StatusUpdate stream for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.469084 32595 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.470094 32595 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.470365 32597 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.470433 32747 logging.cpp:172] Logging to STDERR  I1221 23:10:10.470557 32597 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.470603 32597 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:33285  I1221 23:10:10.470706 32604 master.cpp:4687] Status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.470757 32604 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.470984 32604 master.cpp:6347] Updating the state of task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.471132 32595 no_executor_framework.cpp:160] Task '3' is in state TASK_RUNNING  I1221 23:10:10.471163 32595 sched.cpp:919] Scheduler::statusUpdate took 46297ns  I1221 23:10:10.471405 32595 master.cpp:3844] Processing ACKNOWLEDGE call f4c28f0e-5d99-46e2-8e97-be35f47a8447 for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.471519   346 exec.cpp:341] Executor received status update acknowledgement f4c28f0e-5d99-46e2-8e97-be35f47a8447 for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.471691 32595 status_update_manager.cpp:392] Received status update acknowledgement (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.471897 32595 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.472843 32608 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.476374 32747 exec.cpp:134] Version: 0.27.0  I1221 23:10:10.478024   373 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:39520 with pid 32747  I1221 23:10:10.480216 32607 slave.cpp:2578] Got registration for executor '4' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:39520  I1221 23:10:10.480746 32607 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:39520' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/runs/bc8306b3-d4ba-4bd6-b5f5-03dfb4759738/pids/libprocess.pid'  I1221 23:10:10.482753 32597 slave.cpp:1796] Sending queued task '4' to executor '4' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:39520  I1221 23:10:10.483325   379 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  Registered executor on 6ccf2ee56b13  I1221 23:10:10.485102   379 exec.cpp:220] Executor::registered took 269656ns  I1221 23:10:10.485415   379 exec.cpp:295] Executor asked to run task '4'  Starting task 4  I1221 23:10:10.485508   379 exec.cpp:304] Executor::launchTask took 63624ns  sh -c 'echo hello'  Forked command at 382  hello  I1221 23:10:10.488653   377 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.489480 32593 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:39520  I1221 23:10:10.489858 32595 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.489914 32595 status_update_manager.cpp:497] Creating StatusUpdate stream for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.490532 32595 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.491937 32595 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.492247 32597 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.492422 32597 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.492470 32597 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:39520  I1221 23:10:10.492741 32606 master.cpp:4687] Status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.492871 32606 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.493093   372 exec.cpp:341] Executor received status update acknowledgement c8c2c7b7-8040-48e9-84d6-75b9fbbb3419 for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.493377 32606 master.cpp:6347] Updating the state of task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)  I1221 23:10:10.493441 32595 no_executor_framework.cpp:160] Task '4' is in state TASK_RUNNING  I1221 23:10:10.493469 32595 sched.cpp:919] Scheduler::statusUpdate took 37984ns  I1221 23:10:10.493696 32595 master.cpp:3844] Processing ACKNOWLEDGE call c8c2c7b7-8040-48e9-84d6-75b9fbbb3419 for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.493988 32599 status_update_manager.cpp:392] Received status update acknowledgement (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.494120 32599 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.495306 32598 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  Command exited with status 0 (pid: 361)  I1221 23:10:10.543287   304 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.544509 32593 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:60995  I1221 23:10:10.544711 32593 slave.cpp:5520] Terminating task 0  I1221 23:10:10.546165 32608 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.546311 32608 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.547626 32608 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave  I1221 23:10:10.547925 32594 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874  I1221 23:10:10.548090 32594 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.548177 32594 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:60995  I1221 23:10:10.548307 32605 master.cpp:4687] Status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.548375 32605 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.548635 32605 master.cpp:6347] Updating the state of task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)  I1221 23:10:10.548717 32608 no_executor_framework.cpp:160] Task '0' is in state TASK_FINISHED  I1221 23:10:10.548748 32608 sched.cpp:919] Scheduler::statusUpdate took 39141ns  I1221 23:10:10.549018   301 exec.cpp:341] Executor received status update acknowledgement 133ecf74-5795-4890-a47d-02b397e9084c for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.549098 32605 master.cpp:3844] Processing ACKNOWLEDGE call 133ecf74-5795-4890-a47d-02b397e9084c for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1  I1221 23:10:10.549120 32607 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.4; mem(*):128; disk(*):128) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.549159 32605 master.cpp:6413] Removing task 0 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)  I1221 23:10:10.549618 32593 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  Command exited with status 0 (pid: 362)  Command exited with status 0 (pid: 363)  I1221 23:10:10.550003 32593 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.550715 32593 status_update_manager.cpp:528] Cleaning up status update stream for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.551157 32599 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000  I1221 23:10:10.551211 32599 slave.cpp:5561] Completing task 0    ...  ...  ...  I1221 23:10:10.598156   374 exec.cpp:396] Executor::shutdown took 14264ns  I1221 23:10:10.598224 32605 slave.cpp:3417] master@172.17.0.2:40874 exited  W1221 23:10:10.598253 32605 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected  I1221 23:10:10.598274   374 exec.cpp:80] Scheduling shutdown of the executor  I1221 23:10:10.600291 32593 slave.cpp:601] Slave terminating  I1221 23:10:10.603379 32569 slave.cpp:601] Slave terminating  I1221 23:10:10.605034 32604 slave.cpp:601] Slave terminating  [       OK ] ExamplesTest.NoExecutorFramework (7895 ms)  {noformat}    {noformat: title=Bad Run}  [ RUN      ] ExamplesTest.NoExecutorFramework  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 17:48:57.199091 32567 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32;disk:32  Trying semicolon-delimited string format instead  I1221 17:48:57.200100 32567 logging.cpp:172] Logging to STDERR  I1221 17:48:57.207792 32567 process.cpp:998] libprocess is initialized on 172.17.0.2:34597 for 16 cpus  I1221 17:48:57.235429 32567 leveldb.cpp:174] Opened db in 3.93451ms  I1221 17:48:57.236927 32567 leveldb.cpp:181] Compacted db in 1.417647ms  I1221 17:48:57.237071 32567 leveldb.cpp:196] Created db iterator in 92710ns  I1221 17:48:57.237102 32567 leveldb.cpp:202] Seeked to beginning of db in 6258ns  I1221 17:48:57.237119 32567 leveldb.cpp:271] Iterated through 0 keys in the db in 473ns  I1221 17:48:57.237413 32567 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I1221 17:48:57.240393 32594 recover.cpp:447] Starting replica recovery  I1221 17:48:57.241396 32594 recover.cpp:473] Replica is in EMPTY status  I1221 17:48:57.242455 32567 local.cpp:239] Using 'local' authorizer  I1221 17:48:57.245549 32600 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.2:34597  I1221 17:48:57.246796 32592 recover.cpp:193] Received a recover response from a replica in EMPTY status  I1221 17:48:57.247524 32595 master.cpp:365] Master f9d7159c-0e99-4adc-8090-abc4b60b98f5 (be9bd3a71092) started on 172.17.0.2:34597  I1221 17:48:57.247854 32601 recover.cpp:564] Updating replica status to STARTING  I1221 17:48:57.247812 32595 master.cpp:367] Flags at startup: --acls=""permissive: false  register_frameworks {    principals {      type: SOME      values: ""test-principal""    }    roles {      type: SOME      values: ""*""    }  }  run_tasks {    principals {      type: SOME      values: ""test-principal""    }    users {      type: SOME      values: ""mesos""    }  }  "" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_NoExecutorFramework_2wDEhF/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-Alzbfe"" --zk_session_timeout=""10secs""  I1221 17:48:57.248754 32595 master.cpp:412] Master only allowing authenticated frameworks to register  I1221 17:48:57.248770 32595 master.cpp:419] Master allowing unauthenticated slaves to register  I1221 17:48:57.248783 32595 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_NoExecutorFramework_2wDEhF/credentials'  W1221 17:48:57.248898 32595 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_NoExecutorFramework_2wDEhF/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.  I1221 17:48:57.249059 32592 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 890391ns  I1221 17:48:57.249110 32595 master.cpp:456] Using default 'crammd5' authenticator  I1221 17:48:57.249126 32592 replica.cpp:320] Persisted replica status to STARTING  I1221 17:48:57.249439 32595 authenticator.cpp:518] Initializing server SASL  I1221 17:48:57.249902 32592 recover.cpp:473] Replica is in STARTING status  I1221 17:48:57.251363 32600 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.2:34597  I1221 17:48:57.251474 32595 auxprop.cpp:71] Initialized in-memory auxiliary property plugin  I1221 17:48:57.251765 32595 master.cpp:493] Authorization enabled  I1221 17:48:57.251787 32592 recover.cpp:193] Received a recover response from a replica in STARTING status  I1221 17:48:57.252167 32567 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 17:48:57.252354 32592 recover.cpp:564] Updating replica status to VOTING  I1221 17:48:57.252828 32596 hierarchical.cpp:147] Initialized hierarchical allocator process  I1221 17:48:57.253104 32597 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 602231ns  I1221 17:48:57.253137 32597 replica.cpp:320] Persisted replica status to VOTING  W1221 17:48:57.253180 32567 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 17:48:57.253304 32597 whitelist_watcher.cpp:77] No whitelist given  I1221 17:48:57.253336 32597 recover.cpp:578] Successfully joined the Paxos group  I1221 17:48:57.253697 32597 recover.cpp:462] Recover process terminated  I1221 17:48:57.268092 32595 master.cpp:1629] The newly elected leader is master@172.17.0.2:34597 with id f9d7159c-0e99-4adc-8090-abc4b60b98f5  I1221 17:48:57.268255 32595 master.cpp:1642] Elected as the leading master!  I1221 17:48:57.268411 32595 master.cpp:1387] Recovering from registrar  I1221 17:48:57.268937 32603 registrar.cpp:307] Recovering registrar  I1221 17:48:57.271054 32600 log.cpp:659] Attempting to start the writer  I1221 17:48:57.271824 32595 slave.cpp:191] Slave started on 1)@172.17.0.2:34597  I1221 17:48:57.271908 32595 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-Alzbfe/0""  I1221 17:48:57.273372 32595 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 17:48:57.274093 32567 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  W1221 17:48:57.275342 32567 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 17:48:57.275346 32595 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 17:48:57.275511 32595 slave.cpp:400] Slave attributes: [  ]  I1221 17:48:57.275542 32595 slave.cpp:405] Slave hostname: be9bd3a71092  I1221 17:48:57.275553 32595 slave.cpp:410] Slave checkpoint: true  I1221 17:48:57.276523 32594 replica.cpp:493] Replica received implicit promise request from (19)@172.17.0.2:34597 with proposal 1  I1221 17:48:57.277181 32594 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 616640ns  I1221 17:48:57.277209 32594 replica.cpp:342] Persisted promised to 1  I1221 17:48:57.279067 32604 state.cpp:58] Recovering state from '/tmp/mesos-Alzbfe/0/meta'  I1221 17:48:57.279417 32604 slave.cpp:191] Slave started on 2)@172.17.0.2:34597  I1221 17:48:57.279439 32604 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-Alzbfe/1""  I1221 17:48:57.280268 32604 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 17:48:57.280439 32601 status_update_manager.cpp:200] Recovering status update manager  I1221 17:48:57.280815 32603 containerizer.cpp:383] Recovering containerizer  I1221 17:48:57.280939 32604 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 17:48:57.281018 32604 slave.cpp:400] Slave attributes: [  ]  I1221 17:48:57.281033 32604 slave.cpp:405] Slave hostname: be9bd3a71092  I1221 17:48:57.281041 32604 slave.cpp:410] Slave checkpoint: true  I1221 17:48:57.281885 32567 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 17:48:57.282161 32597 state.cpp:58] Recovering state from '/tmp/mesos-Alzbfe/1/meta'  W1221 17:48:57.282383 32567 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 17:48:57.282456 32604 status_update_manager.cpp:200] Recovering status update manager  I1221 17:48:57.282768 32604 containerizer.cpp:383] Recovering containerizer  I1221 17:48:57.284574 32596 slave.cpp:4427] Finished recovery  I1221 17:48:57.284680 32594 coordinator.cpp:238] Coordinator attempting to fill missing positions  I1221 17:48:57.284574 32602 slave.cpp:4427] Finished recovery  I1221 17:48:57.286054 32602 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 17:48:57.286500 32596 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 17:48:57.287051 32596 slave.cpp:729] New master detected at master@172.17.0.2:34597  I1221 17:48:57.287382 32596 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 17:48:57.288478 32596 slave.cpp:765] Detecting new master  I1221 17:48:57.288810 32596 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 17:48:57.289193 32604 replica.cpp:388] Replica received explicit promise request from (37)@172.17.0.2:34597 for position 0 with proposal 2  I1221 17:48:57.289732 32602 slave.cpp:729] New master detected at master@172.17.0.2:34597  I1221 17:48:57.289877 32602 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 17:48:57.290024 32602 slave.cpp:765] Detecting new master  I1221 17:48:57.287452 32601 status_update_manager.cpp:174] Pausing sending status updates  I1221 17:48:57.290153 32604 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 870482ns  I1221 17:48:57.290181 32604 replica.cpp:712] Persisted action at 0  I1221 17:48:57.290182 32601 status_update_manager.cpp:174] Pausing sending status updates  I1221 17:48:57.290633 32602 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 17:48:57.290832 32601 slave.cpp:191] Slave started on 3)@172.17.0.2:34597  I1221 17:48:57.290853 32601 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-Alzbfe/2""  I1221 17:48:57.291916 32601 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 17:48:57.293606 32592 replica.cpp:537] Replica received write request for position 0 from (40)@172.17.0.2:34597  I1221 17:48:57.293800 32592 leveldb.cpp:436] Reading position from leveldb took 85274ns  I1221 17:48:57.294116 32601 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 17:48:57.294201 32601 slave.cpp:400] Slave attributes: [  ]  I1221 17:48:57.294216 32601 slave.cpp:405] Slave hostname: be9bd3a71092  I1221 17:48:57.294226 32601 slave.cpp:410] Slave checkpoint: true  I1221 17:48:57.294476 32592 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 574132ns  ...  ...  ...  I1221 17:49:02.770455 32598 no_executor_framework.cpp:160] Task '0' is in state TASK_FINISHED  I1221 17:49:02.770493 32598 sched.cpp:919] Scheduler::statusUpdate took 62688ns  I1221 17:49:02.770823 32602 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.3; mem(*):96; disk(*):96) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.771128 32598 master.cpp:3844] Processing ACKNOWLEDGE call b6352200-525b-4124-a654-2ab82cdf2af2 for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0  I1221 17:49:02.771245 32598 master.cpp:6413] Removing task 0 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)  I1221 17:49:02.770465 32592 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.771577   301 exec.cpp:341] Executor received status update acknowledgement b6352200-525b-4124-a654-2ab82cdf2af2 for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.771955 32592 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.772097 32592 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.772390 32591 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:44314  I1221 17:49:02.772554 32591 slave.cpp:5520] Terminating task 2  I1221 17:49:02.772975 32591 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.773069 32591 slave.cpp:5561] Completing task 1  I1221 17:49:02.774068 32592 status_update_manager.cpp:528] Cleaning up status update stream for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.774581 32592 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.774747 32592 slave.cpp:5561] Completing task 0  I1221 17:49:02.775277 32595 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.775414 32595 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.778640 32595 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave  I1221 17:49:02.779016 32592 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597  I1221 17:49:02.779219 32592 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.779268 32592 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:44314  I1221 17:49:02.779527 32595 master.cpp:4687] Status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)  I1221 17:49:02.779698 32595 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.779952 32595 master.cpp:6347] Updating the state of task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)  I1221 17:49:02.780423 32595 no_executor_framework.cpp:160] Task '2' is in state TASK_FINISHED  I1221 17:49:02.780583   338 exec.cpp:341] Executor received status update acknowledgement 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.780850 32595 sched.cpp:919] Scheduler::statusUpdate took 432956ns  I1221 17:49:02.781097 32591 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.2; mem(*):64; disk(*):64) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.781183 32601 master.cpp:3844] Processing ACKNOWLEDGE call 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0  I1221 17:49:02.781383 32601 master.cpp:6413] Removing task 2 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)  I1221 17:49:02.782066 32601 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.782419 32601 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.783466 32601 status_update_manager.cpp:528] Cleaning up status update stream for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.784078 32602 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.784178 32602 slave.cpp:5561] Completing task 2  Command exited with status 0 (pid: 379)  I1221 17:49:02.801833   358 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.803642 32601 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:52442  I1221 17:49:02.803870 32601 slave.cpp:5520] Terminating task 4  I1221 17:49:02.806363 32601 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.806607 32601 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.808573 32601 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave  I1221 17:49:02.809147 32601 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597  I1221 17:49:02.809331 32601 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.809659 32593 master.cpp:4687] Status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)  I1221 17:49:02.809711 32593 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.809882 32593 master.cpp:6347] Updating the state of task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)  I1221 17:49:02.810335 32593 no_executor_framework.cpp:160] Task '4' is in state TASK_FINISHED  I1221 17:49:02.810369 32593 sched.cpp:919] Scheduler::statusUpdate took 40498ns  I1221 17:49:02.811712 32604 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.1; mem(*):32; disk(*):32) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.812052 32593 master.cpp:3844] Processing ACKNOWLEDGE call e54772f7-bf6f-477c-a76d-00c874928fa0 for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0  I1221 17:49:02.812126 32593 master.cpp:6413] Removing task 4 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)  I1221 17:49:02.809386 32601 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:52442  I1221 17:49:02.813179 32601 status_update_manager.cpp:392] Received status update acknowledgement (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.813343 32601 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.813365   345 exec.cpp:341] Executor received status update acknowledgement e54772f7-bf6f-477c-a76d-00c874928fa0 for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.815651 32601 status_update_manager.cpp:528] Cleaning up status update stream for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.816185 32606 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.816253 32606 slave.cpp:5561] Completing task 4  Command exited with status 0 (pid: 380)  I1221 17:49:02.821643   374 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.823426 32601 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:60087  I1221 17:49:02.823643 32601 slave.cpp:5520] Terminating task 3  I1221 17:49:02.825294 32602 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.825374 32602 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.827564 32602 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave  I1221 17:49:02.827869 32591 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597  I1221 17:49:02.828033 32591 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.828075 32591 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:60087  I1221 17:49:02.828188 32602 master.cpp:4687] Status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)  I1221 17:49:02.828230 32602 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.828552 32606 no_executor_framework.cpp:160] Task '3' is in state TASK_FINISHED  I1221 17:49:02.828582 32606 sched.cpp:1803] Asked to stop the driver  I1221 17:49:02.828631 32606 sched.cpp:919] Scheduler::statusUpdate took 96503ns  I1221 17:49:02.828649 32606 sched.cpp:926] Not sending status update acknowledgment message because the driver is not running!  I1221 17:49:02.828696 32606 sched.cpp:1041] Stopping framework 'f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000'  I1221 17:49:02.828698 32602 master.cpp:6347] Updating the state of task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)  I1221 17:49:02.829473 32567 sched.cpp:1803] Asked to stop the driver  I1221 17:49:02.829541 32567 sched.cpp:1806] Ignoring stop because the status of the driver is DRIVER_STOPPED  I1221 17:49:02.829524   372 exec.cpp:341] Executor received status update acknowledgement f7609151-4f05-4f8c-9fc1-98ee76dac298 for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.829692 32602 master.cpp:5823] Processing TEARDOWN call for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597  I1221 17:49:02.829725 32602 master.cpp:5835] Removing framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597  I1221 17:49:02.829743 32591 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):4.71845e-16) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.829884 32591 hierarchical.cpp:366] Deactivated framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.830040 32602 master.cpp:6347] Updating the state of task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_KILLED)  I1221 17:49:02.830144 32591 slave.cpp:2012] Asked to shut down framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 by master@172.17.0.2:34597  W1221 17:49:02.830174 32591 slave.cpp:2027] Cannot shut down unknown framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.830214 32591 slave.cpp:2012] Asked to shut down framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 by master@172.17.0.2:34597  W1221 17:49:02.830231 32591 slave.cpp:2027] Cannot shut down unknown framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.830263 32591 slave.cpp:2012] Asked to shut down framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 by master@172.17.0.2:34597  I1221 17:49:02.830288 32591 slave.cpp:2037] Shutting down framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.830116 32602 master.cpp:6413] Removing task 3 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)  I1221 17:49:02.830490 32591 slave.cpp:4060] Shutting down executor '0' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:41236  I1221 17:49:02.830888 32591 slave.cpp:4060] Shutting down executor '1' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:40805  I1221 17:49:02.831028 32602 master.cpp:930] Master terminating  I1221 17:49:02.831089 32591 slave.cpp:4060] Shutting down executor '2' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:44314  I1221 17:49:02.831279 32591 slave.cpp:4060] Shutting down executor '3' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:60087  I1221 17:49:02.831519   301 exec.cpp:381] Executor asked to shutdown  I1221 17:49:02.831696   301 exec.cpp:396] Executor::shutdown took 12788ns  I1221 17:49:02.831830   301 exec.cpp:80] Scheduling shutdown of the executor  I1221 17:49:02.831900 32595 hierarchical.cpp:321] Removed framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000  I1221 17:49:02.832252 32595 hierarchical.cpp:496] Removed slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2  I1221 17:49:02.832567 32591 slave.cpp:4060] Shutting down executor '4' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:52442  I1221 17:49:02.832967 32595 hierarchical.cpp:496] Removed slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1  II1221 17:49:02.833258 32595 hierarchical.cpp:496] Removed slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0  1221 17:49:02.833251   372 exec.cpp:381] Executor asked to shutdown  I1221 17:49:02.833380   372 exec.cpp:396] Executor::shutdown took 12276ns  I1221 17:49:02.833453   369 exec.cpp:80] Scheduling shutdown of the executor  I1221 17:49:02.833497   314 exec.cpp:381] Executor asked to shutdown  I1221 17:49:02.833633   314 exec.cpp:396] Executor::shutdown took 14634ns  I1221 17:49:02.833751   314 exec.cpp:80] Scheduling shutdown of the executor  I1221 17:49:02.834023   335 exec.cpp:381] Executor asked to shutdown  I1221 17:49:02.834144   335 exec.cpp:396] Executor::shutdown took 11303ns  I1221 17:49:02.834262   335 exec.cpp:80] Scheduling shutdown of the executor  I1221 17:49:02.834300   345 exec.cpp:381] Executor asked to shutdown  I1221 17:49:02.834462   345 exec.cpp:396] Executor::shutdown took 14263ns  I1221 17:49:02.834502   345 exec.cpp:80] Scheduling shutdown of the executor  I1221 17:49:02.834887 32602 slave.cpp:3417] master@172.17.0.2:34597 exited  W1221 17:49:02.834947 32602 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected  I1221 17:49:02.835018 32602 slave.cpp:3417] master@172.17.0.2:34597 exited  W1221 17:49:02.835083 32602 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected  I1221 17:49:02.835146 32602 slave.cpp:3417] master@172.17.0.2:34597 exited  W1221 17:49:02.835213 32602 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected  I1221 17:49:02.839617 32593 slave.cpp:601] Slave terminating  I1221 17:49:02.845129 32567 slave.cpp:601] Slave terminating  I1221 17:49:02.847450 32603 slave.cpp:601] Slave terminating  I1221 17:49:02.879467   314 exec.cpp:444] Ignoring exited event because the driver is aborted!  I1221 17:49:02.879524   301 exec.cpp:444] Ignoring exited event because the driver is aborted!  I[       OK ] ExamplesTest.NoExecutorFramework (8717 ms)  [ RUN      ] ExamplesTest.EventCallFramework  I1221 17:49:02.881482   352 exec.cpp:444] Ignoring exited event because the driver is aborted!  Using temporary directory '/tmp/ExamplesTest_EventCallFramework_9TX8jH'  1221 17:49:02.881122   335 exec.cpp:444] Ignoring exited event because the driver is aborted!  I1221 17:49:02.882071   372 exec.cpp:444] Ignoring exited event because the driver is aborted!  Shutting down  Shutting down  Sending SIGTERM to process tree at pid 343  Sending SIGTERM to process tree at pid 342  Killing the following process trees:  [     ]  Killing the following process trees:  [     ]  Shutting down  Sending SIGTERM to process tree at pid 361  Killing the following process trees:  [     ]  Shutting down  Sending SIGTERM to process tree at pid 379  Killing the following process trees:  [     ]  Shutting down  Sending SIGTERM to process tree at pid 380  Killing the following process trees:  [     ]  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1221 17:49:05.786998   381 process.cpp:998] libprocess is initialized on 172.17.0.2:60254 for 16 cpus  I1221 17:49:05.787230   381 logging.cpp:172] Logging to STDERR  I1221 17:49:05.788791   381 scheduler.cpp:154] Version: 0.27.0  I1221 17:49:05.809587   381 leveldb.cpp:174] Opened db in 3.241758ms  I1221 17:49:05.811599   381 leveldb.cpp:181] Compacted db in 1.820335ms  I1221 17:49:05.811724   381 leveldb.cpp:196] Created db iterator in 74530ns  I1221 17:49:05.811749   381 leveldb.cpp:202] Seeked to beginning of db in 5247ns  I1221 17:49:05.811760   381 leveldb.cpp:271] Iterated through 0 keys in the db in 324ns  I1221 17:49:05.811980   381 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned  I1221 17:49:05.816004   381 local.cpp:239] Using 'local' authorizer  I1221 17:49:05.818006   411 recover.cpp:447] Starting replica recovery  I1221 17:49:05.819463   406 recover.cpp:473] Replica is in EMPTY status  I1221 17:49:05.825531   413 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (5)@172.17.0.2:60254  I1221 17:49:05.831326   412 recover.cpp:193] Received a recover response from a replica in EMPTY status  I1221 17:49:05.832586   418 recover.cpp:564] Updating replica status to STARTING  I1221 17:49:05.834460   418 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.082224ms  I1221 17:49:05.834507   418 replica.cpp:320] Persisted replica status to STARTING  I1221 17:49:05.835194   417 recover.cpp:473] Replica is in STARTING status  I1221 17:49:05.838433   417 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (6)@172.17.0.2:60254  I1221 17:49:05.838778   405 master.cpp:365] Master 19baa4c9-78aa-4ffb-abb3-e4daf634ec63 (be9bd3a71092) started on 172.17.0.2:60254  I1221 17:49:05.838824   405 master.cpp:367] Flags at startup: --acls=""permissive: false  register_frameworks {    principals {      type: SOME      values: ""test-principal""    }    roles {      type: SOME      values: ""*""    }  }  run_tasks {    principals {      type: SOME      values: ""test-principal""    }    users {      type: SOME      values: ""mesos""    }  }  "" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_EventCallFramework_9TX8jH/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-WEDoSa"" --zk_session_timeout=""10secs""  I1221 17:49:05.839956   405 master.cpp:414] Master allowing unauthenticated frameworks to register  I1221 17:49:05.839974   405 master.cpp:419] Master allowing unauthenticated slaves to register  I1221 17:49:05.839990   405 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_9TX8jH/credentials'  W1221 17:49:05.840143   405 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_9TX8jH/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.  I1221 17:49:05.840260   418 recover.cpp:193] Received a recover response from a replica in STARTING status  I1221 17:49:05.840479   405 master.cpp:456] Using default 'crammd5' authenticator  I1221 17:49:05.840912   405 authenticator.cpp:518] Initializing server SASL  I1221 17:49:05.841112   412 recover.cpp:564] Updating replica status to VOTING  I1221 17:49:05.841882   413 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 641489ns  I1221 17:49:05.841920   413 replica.cpp:320] Persisted replica status to VOTING  I1221 17:49:05.842102   406 recover.cpp:578] Successfully joined the Paxos group  I1221 17:49:05.842471   406 recover.cpp:462] Recover process terminated  I1221 17:49:05.843750   405 auxprop.cpp:71] Initialized in-memory auxiliary property plugin  I1221 17:49:05.843941   405 master.cpp:493] Authorization enabled  I1221 17:49:05.844133   381 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 17:49:05.844813   410 hierarchical.cpp:147] Initialized hierarchical allocator process  W1221 17:49:05.845093   381 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 17:49:05.845433   416 whitelist_watcher.cpp:77] No whitelist given  I1221 17:49:05.850373   411 slave.cpp:191] Slave started on 1)@172.17.0.2:60254  I1221 17:49:05.850433   411 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-WEDoSa/0""  I1221 17:49:05.851650   411 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 17:49:05.854473   411 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 17:49:05.854701   411 slave.cpp:400] Slave attributes: [  ]  I1221 17:49:05.854821   411 slave.cpp:405] Slave hostname: be9bd3a71092  I1221 17:49:05.854907   411 slave.cpp:410] Slave checkpoint: true  I1221 17:49:05.855715   381 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  W1221 17:49:05.856274   381 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 17:49:05.861232   407 slave.cpp:191] Slave started on 2)@172.17.0.2:60254  I1221 17:49:05.861279   407 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-WEDoSa/1""  I1221 17:49:05.862290   407 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 17:49:05.862937   407 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 17:49:05.863024   407 slave.cpp:400] Slave attributes: [  ]  I1221 17:49:05.863040   407 slave.cpp:405] Slave hostname: be9bd3a71092  I1221 17:49:05.863049   407 slave.cpp:410] Slave checkpoint: true  I1221 17:49:05.863625   418 state.cpp:58] Recovering state from '/tmp/mesos-WEDoSa/0/meta'  I1221 17:49:05.863787   381 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem  I1221 17:49:05.864229   420 state.cpp:58] Recovering state from '/tmp/mesos-WEDoSa/1/meta'  W1221 17:49:05.864358   381 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges  I1221 17:49:05.864397   418 status_update_manager.cpp:200] Recovering status update manager  I1221 17:49:05.865824   411 status_update_manager.cpp:200] Recovering status update manager  I1221 17:49:05.866319   411 containerizer.cpp:383] Recovering containerizer  I1221 17:49:05.866439   410 containerizer.cpp:383] Recovering containerizer  I1221 17:49:05.867146   416 master.cpp:1629] The newly elected leader is master@172.17.0.2:60254 with id 19baa4c9-78aa-4ffb-abb3-e4daf634ec63  I1221 17:49:05.867189   416 master.cpp:1642] Elected as the leading master!  I1221 17:49:05.867229   416 master.cpp:1387] Recovering from registrar  I1221 17:49:05.867583   409 registrar.cpp:307] Recovering registrar  I1221 17:49:05.868595   420 slave.cpp:4427] Finished recovery  I1221 17:49:05.869122   406 slave.cpp:4427] Finished recovery  I1221 17:49:05.869529   420 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 17:49:05.869848   411 log.cpp:659] Attempting to start the writer  I1221 17:49:05.870846   413 status_update_manager.cpp:174] Pausing sending status updates  I1221 17:49:05.870885   420 slave.cpp:729] New master detected at master@172.17.0.2:60254  I1221 17:49:05.870996   420 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 17:49:05.871129   420 slave.cpp:765] Detecting new master  I1221 17:49:05.871261   420 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 17:49:05.872555   406 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 17:49:05.872836   406 status_update_manager.cpp:174] Pausing sending status updates  I1221 17:49:05.872849   420 slave.cpp:729] New master detected at master@172.17.0.2:60254  I1221 17:49:05.872915   420 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 17:49:05.872956   420 slave.cpp:765] Detecting new master  I1221 17:49:05.873045   420 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 17:49:05.873503   408 replica.cpp:493] Replica received implicit promise request from (37)@172.17.0.2:60254 with proposal 1  I1221 17:49:05.874140   408 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 594758ns  I1221 17:49:05.874171   408 replica.cpp:342] Persisted promised to 1  I1221 17:49:05.875378   408 slave.cpp:191] Slave started on 3)@172.17.0.2:60254  I1221 17:49:05.875962   412 coordinator.cpp:238] Coordinator attempting to fill missing positions  I1221 17:49:05.875965   414 scheduler.cpp:236] New master detected at master@172.17.0.2:60254  I1221 17:49:05.875458   408 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-WEDoSa/2""  I1221 17:49:05.877544   408 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240  Trying semicolon-delimited string format instead  I1221 17:49:05.878088   410 replica.cpp:388] Replica received explicit promise request from (40)@172.17.0.2:60254 for position 0 with proposal 2  I1221 17:49:05.878116   408 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]  I1221 17:49:05.878209   408 slave.cpp:400] Slave attributes: [  ]  I1221 17:49:05.878226   408 slave.cpp:405] Slave hostname: be9bd3a71092  I1221 17:49:05.878237   408 slave.cpp:410] Slave checkpoint: true  I1221 17:49:05.878959   410 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 769060ns  I1221 17:49:05.878995   410 replica.cpp:712] Persisted action at 0  I1221 17:49:05.880889   419 state.cpp:58] Recovering state from '/tmp/mesos-WEDoSa/2/meta'  I1221 17:49:05.881033   418 replica.cpp:537] Replica received write request for position 0 from (41)@172.17.0.2:60254  I1221 17:49:05.881433   418 leveldb.cpp:436] Reading position from leveldb took 163395ns  I1221 17:49:05.881222   416 status_update_manager.cpp:200] Recovering status update manager  I1221 17:49:05.881892   419 containerizer.cpp:383] Recovering containerizer  I1221 17:49:05.882032   418 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 486145ns  I1221 17:49:05.882069   418 replica.cpp:712] Persisted action at 0  I1221 17:49:05.882983   419 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0  I1221 17:49:05.883849   419 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 829258ns  I1221 17:49:05.883955   413 slave.cpp:4427] Finished recovery  I1221 17:49:05.883960   419 replica.cpp:712] Persisted action at 0  I1221 17:49:05.884121   419 replica.cpp:697] Replica learned NOP action at position 0  I1221 17:49:05.884806   413 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 17:49:05.884945   409 log.cpp:675] Writer started with ending position 0  I1221 17:49:05.885113   419 status_update_manager.cpp:174] Pausing sending status updates  I1221 17:49:05.885149   413 slave.cpp:729] New master detected at master@172.17.0.2:60254  I1221 17:49:05.885241   413 slave.cpp:754] No credentials provided. Attempting to register without authentication  I1221 17:49:05.885354   413 slave.cpp:765] Detecting new master  I1221 17:49:05.885524   413 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 17:49:05.888422   409 leveldb.cpp:436] Reading position from leveldb took 76121ns  I1221 17:49:05.893468   420 registrar.cpp:340] Successfully fetched the registry (0B) in 25.776896ms  I1221 17:49:05.893851   420 registrar.cpp:439] Applied 1 operations in 117027ns; attempting to update the 'registry'  I1221 17:49:05.896961   412 log.cpp:683] Attempting to append 170 bytes to the log  I1221 17:49:05.897126   418 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1  I1221 17:49:05.898660   412 replica.cpp:537] Replica received write request for position 1 from (43)@172.17.0.2:60254  I1221 17:49:05.899444   412 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 711265ns  I1221 17:49:05.899479   412 replica.cpp:712] Persisted action at 1  I1221 17:49:05.900216   410 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0    ...  ...  ...  I1221 17:49:29.890243   414 hierarchical.cpp:1329] No resources available to allocate!  I1221 17:49:29.890316   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 624179ns  I1221 17:49:30.892174   414 hierarchical.cpp:1329] No resources available to allocate!  I1221 17:49:30.892246   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 564425ns  I1221 17:49:31.894242   414 hierarchical.cpp:1329] No resources available to allocate!  I1221 17:49:31.894316   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 625358ns  I1221 17:49:32.896178   414 hierarchical.cpp:1329] No resources available to allocate!  I1221 17:49:32.896248   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 564486ns  I1221 17:49:33.898360   414 hierarchical.cpp:1329] No resources available to allocate!  I1221 17:49:33.898444   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 664324ns  I1221 17:49:34.900513   409 hierarchical.cpp:1329] No resources available to allocate!  I1221 17:49:34.900584   409 hierarchical.cpp:1079] Performed allocation for 3 slaves in 584647ns  I1221 17:49:35.875272   416 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 17:49:35.875267   418 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 17:49:35.875730   409 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 17:49:35.875766   412 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 17:49:35.887620   419 slave.cpp:4599] Querying resource estimator for oversubscribable resources  I1221 17:49:35.887943   413 slave.cpp:4613] Received oversubscribable resources  from the resource estimator  I1221 17:49:35.901360   412 hierarchical.cpp:1329] No resources available to allocate!  I1221 17:49:35.901435   412 hierarchical.cpp:1079] Performed allocation for 3 slaves in 552395ns  I1221 17:49:35.950346   416 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:60254  I1221 17:49:35.995688   416 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:60254  I1221 17:49:36.609561   418 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:60254  {noformat}",3
"MESOS-4261","Remove docker auth server flag","We currently use a configured docker auth server from a slave flag to get token auth for docker registry. However this doesn't work for private registries as docker registry supports sending down the correct auth server to contact.  We should remove docker auth server flag completely and ask the docker registry for auth server.",3
"MESOS-4262","Enable net_cls subsytem in cgroup infrastructure","Currently the control group infrastructure within mesos supports only the memory and CPU subsystems. We need to enhance this infrastructure to support the net_cls subsystem as well. Details of the net_cls subsystem and its use-cases can be found here: https://www.kernel.org/doc/Documentation/cgroups/net_cls.txt  Enabling the net_cls will allow us to provide operators to, potentially, regulate framework traffic on a per-container basis.  ",5
"MESOS-4282","Update isolator prepare function to use ContainerLaunchInfo","Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers.   By doing this (ContainerPrepareInfo -> ContainerLaunchInfo), we can select any necessary information and passing then to launcher.",2
"MESOS-4284","Draft design doc for multi-role frameworks","Create a document that describes the problems with having only single-role frameworks and proposes an MVP solution and implementation approach.",8
"MESOS-4285","Mesos command task doesn't support volumes with image","Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. ",3
"MESOS-4289","Design doc for simple appc image discovery","Create a design document describing the following:  - Model and abstraction of the Discoverer - Workflow of the discovery process ",5
"MESOS-4300","Add AuthN and AuthZ to maintenance endpoints.","Maintenance endpoints are currently only restricted by firewall settings.  They should also support authentication/authorization like other HTTP endpoints.",3
"MESOS-4304","hdfs operations fail due to prepended / on path for non-hdfs hadoop clients.","This bug was resolved for the hdfs protocol for MESOS-3602 but since the process checks for the ""hdfs"" protocol at the beginning of the URI, the fix does not extend itself to non-hdfs hadoop clients. {code} I0107 01:22:01.259490 17678 logging.cpp:172] INFO level logging started! I0107 01:22:01.259856 17678 fetcher.cpp:422] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""maprfs:\/\/\/mesos\/storm-mesos-0.9.3.tgz""}},{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""http:\/\/s0121.stag.urbanairship.com:36373\/conf\/storm.yaml""}}],""sandbox_directory"":""\/mnt\/data\/mesos\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/frameworks\/530dda5a-481a-4117-8154-3aee637d3b38-0000\/executors\/word-count-1-1452129714\/runs\/4443d5ac-d034-49b3-bf12-08fb9b0d92d0"",""user"":""root""} I0107 01:22:01.262171 17678 fetcher.cpp:377] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz' I0107 01:22:01.262212 17678 fetcher.cpp:248] Fetching directly into the sandbox directory I0107 01:22:01.262243 17678 fetcher.cpp:185] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz' I0107 01:22:01.671777 17678 fetcher.cpp:110] Downloading resource with Hadoop client from 'maprfs:///mesos/storm-mesos-0.9.3.tgz' to '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz' copyToLocal: java.net.URISyntaxException: Expected scheme-specific part at index 7: maprfs: Usage: java FsShell [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>] E0107 01:22:02.435556 17678 shell.hpp:90] Command 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'' failed; this is the output: Failed to fetch 'maprfs:///mesos/storm-mesos-0.9.3.tgz': HDFS copyToLocal failed: Failed to execute 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz''; the command was either not found or exited with a non-zero exit status: 255 Failed to synchronize with slave (it's probably exited) {code}  After a brief chat with [~jieyu], it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it.",1
"MESOS-4333","Refactor Appc provisioner tests  ","Current tests can be refactored so that we can reuse some common tasks like test image creation. This will benefit future tests like appc image puller tests.",2
"MESOS-4348","GMock warning in HookTest.VerifySlaveRunTaskHook, HookTest.VerifySlaveTaskStatusDecorator","{noformat} [ RUN      ] HookTest.VerifySlaveRunTaskHook  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: shutdown(0x7ff079cb2420) Stack trace: [       OK ] HookTest.VerifySlaveRunTaskHook (51 ms) [ RUN      ] HookTest.VerifySlaveTaskStatusDecorator  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: shutdown(0x7ff079cbb790) Stack trace: [       OK ] HookTest.VerifySlaveTaskStatusDecorator (54 ms) {noformat}  Occurs non-deterministically for me. OSX 10.10.",1
"MESOS-4349","GMock warning in SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor","{noformat} [ RUN      ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: shutdown(0x7fe189cae850) Stack trace: [       OK ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor (51 ms) {noformat}  Occurs non-deterministically for me on OSX 10.10, perhaps one run in ten.",1
"MESOS-4353","Limit the number of processes created by libprocess","Currently libprocess will create {{max(8, number of CPU cores)}} processes during the initialization, see https://github.com/apache/mesos/blob/0.26.0/3rdparty/libprocess/src/process.cpp#L2146 for details. This should be OK for a normal machine which has no much cores (e.g., 16, 32), but for a powerful machine which may have a large number of cores (e.g., an IBM Power machine may have 192 cores), this will cause too much worker threads which are not necessary.  And since libprocess is widely used in Mesos (master, agent, scheduler, executor), it may also cause some performance issue. For example, when user creates a Docker container via Mesos in a Mesos agent which is running on a powerful machine with 192 cores, the DockerContainerizer in Mesos agent will create a dedicated executor for the container, and there will be 192 worker threads in that executor. And if user creates 1000 Docker containers in that machine, then there will be 1000 executors, i.e., 1000 * 192 worker threads which is a large number and may thrash the OS.  ",1
"MESOS-4357","GMock warning in RoleTest.ImplicitRoleStaticReservation","{noformat} [ RUN      ] RoleTest.ImplicitRoleStaticReservation  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: shutdown(0x7fe37a4752f0) Stack trace: [       OK ] RoleTest.ImplicitRoleStaticReservation (52 ms) {noformat}",1
"MESOS-4358","Expose net_cls network handles in agent's state endpoint","We need to expose net_cls network handles, associated with containers, to operators and network utilities that would use these network handles to enforce network policy.   In order to achieve the above we need to add a new field in the `NetworkInfo` protobuf (say NetHandles) and update this field when a container gets assigned to a net_cls cgroup. The `ContainerStatus` protobuf already has the `NetworkInfo` protobuf as a nested message, and the `ContainerStatus` itself is exposed to operators as part of TaskInfo (for tasks associated with the container) in an agent's state.json. ",2
"MESOS-4377","Document units associated with resource types","We should document the units associated with memory and disk resources.",1
"MESOS-4410","Introduce protobuf for quota set request.","To document quota request JSON schema and simplify request processing, introduce a {{QuotaRequest}} protobuf wrapper.",3
"MESOS-4421","Document that /reserve, /create-volumes endpoints can return misleading ""success""","The docs for the {{/reserve}} endpoint say:  {noformat} 200 OK: Success (the requested resources have been reserved). {noformat}  This is not true: the master returns {{200}} when the request has been validated and a {{CheckpointResourcesMessage}} has been sent to the agent, but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed. Same behavior applies to {{/unreserve}}, {{/create-volumes}}, and {{/destroy-volumes}}.  We should _either_:  1. Accurately document what {{200}} return code means. 2. Change the implementation to wait for the agent's next checkpoint to succeed (and to include the effect of the operation) before returning success to the HTTP client.",3
"MESOS-4425","Introduce filtering test abstractions for HTTP events to libprocess","We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.  The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}.",3
"MESOS-4435","Update `Master::Http::stateSummary` to use `jsonify`.","Update {{state-summary}} to use {{jsonify}} to stay consistent with {{state}} HTTP endpoint.",3
"MESOS-4437","Disable the test RegistryClientTest.BadTokenServerAddress.","As we are retiring registry client, disable this test which looks flaky.",1
"MESOS-4438","Add 'dependency' message to 'AppcImageManifest' protobuf.","AppcImageManifest protobuf currently lacks 'dependencies' which is necessary for image discovery.",1
"MESOS-4454","Create common sha512 compute utility function.","Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. ",2
"MESOS-4488","Define a CgroupInfo protobuf to expose cgroup isolator configuration.","Within `MesosContainerizer` we have an isolator associated with each linux cgroup subsystem. The isolators apply subsystem specific configuration on the containers before launching the containers. For e.g cgroup/net_cls isolator applies net_cls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu-share isolator configures cpu shares.   Currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container. We therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container. This protobuf will be filled in by the cgroup isolator and will be stored as part of `ContainerConfig` in the containerizer. ",1
"MESOS-4505","Hierarchical allocator performance is slow due to Quota","Since we do not strip the non-scalar resources during the resource arithmetic for quota, the performance can degrade significantly, as currently resource arithmetic is expensive.  One approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars. This is valid as quota can currently only be set for scalar resource types.",3
"MESOS-4517","Introduce docker runtime isolator.","Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc.",3
"MESOS-4520","Introduce a status() interface for isolators","While launching a container mesos isolators end up configuring/modifying various properties of the container. For e.g., cgroup isolators (mem, cpu, net_cls) configure/change the properties associated with their respective subsystems before launching a container. Similary network isolator (net-modules, port mapping) configure the IP address and ports associated with a container.   Currently, there are not interface in the isolator to extract the run time state of these properties for a given container. Therefore a status() method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator. ",1
"MESOS-4529","Update the allocator to not offer unreserved resources beyond quota.","Eventually, we will want to offer unreserved resources as revocable beyond the role's quota. Rather than offering non-revocable resources beyond the role's quota's guarantee, in the short term, we choose to not offer resources beyond a role's quota.",2
"MESOS-4535","Logrotate ContainerLogger may not handle FD ownership correctly","One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.  The way the logrotate module uses this is slightly incorrect: # The module starts a subprocess with an output {{Subprocess::PIPE()}}. # That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}. # When the second subprocess starts, the pipe's FD is closed in the parent. # When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD.",1
"MESOS-4546","Mesos Agents needs to re-resolve hosts in zk string on leader change / failure to connect","Sample Mesos Agent log: https://gist.github.com/brndnmtthws/fb846fa988487250a809  Note, zookeeper has a function to change the list of servers at runtime: https://github.com/apache/zookeeper/blob/735ea78909e67c648a4978c8d31d63964986af73/src/c/src/zookeeper.c#L1207-L1232  This comes up when using an AWS AutoScalingGroup for managing the set of masters.   The agent when it comes up the first time, resolves the zk:// string. Once all the hosts that were in the original string fail (Each fails, is replaced by a new machine, which has the same DNS name), the agent just keeps spinning in an internal loop, never re-resolving the DNS names.  Two solutions I see are  1. Update the list of servers / re-resolve 2. Have the agent detect it hasn't connected recently, and kill itself (Which will force a re-resolution when the agent starts back up)",3
"MESOS-4562","Mesos UI shows wrong count for ""started"" tasks","The task started field shows the number of tasks in state ""TASKS_STARTING"" as opposed to those in ""TASK_RUNNING"" state.",2
"MESOS-4564","Separate Appc protobuf messages to its own file.","It would be cleaner to keep the Appc protobuf messages separate from other mesos messages.",2
"MESOS-4566","Avoid unnecessary temporary `std::string` constructions and copies in `jsonify`.","A few of the critical code paths in {{jsonify}} involve unnecessary temporary string construction and copies (inherited from the {{JSON::*}}). For example, {{strings::trim}} is used to remove trailing 0s from printing {{double}}. We print {{double}} a lot, and therefore constructing a temporary {{std::string}} on printing of every double is extremely costly. This ticket captures the work involved in avoiding them.",1
"MESOS-4576","Introduce a stout helper for ""which""","We may want to add a helper to {{stout/os.hpp}} that will natively emulate the functionality of the Linux utility {{which}}.  i.e. {code} Option<string> which(const string& command) {   Option<string> path = os::getenv(""PATH"");    // Loop through path and return the first one which os::exists(...).    return None(); } {code}  This helper may be useful: * for test filters in {{src/tests/environment.cpp}} * a few tests in {{src/tests/containerizer/port_mapping_tests.cpp}} * the {{sha512}} utility in {{src/common/command_utils.cpp}} * as runtime checks in the {{LogrotateContainerLogger}} * etc.",2
"MESOS-4584","Update Rakefile for mesos site generation","The stuff in site/ directory needs some updates to make it easier to generate updates for mesos.apache.org site.",2
"MESOS-4590","Add test case for reservations with same role, different principals","We don't have a test case that covers $SUBJECT; we probably should.",2
"MESOS-4614","SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky","Just saw this failure on the ASF CI:  {code} [ RUN      ] SlaveRecoveryTest/0.CleanupHTTPExecutor I0206 00:22:44.791671  2824 leveldb.cpp:174] Opened db in 2.539372ms I0206 00:22:44.792459  2824 leveldb.cpp:181] Compacted db in 740473ns I0206 00:22:44.792510  2824 leveldb.cpp:196] Created db iterator in 24164ns I0206 00:22:44.792532  2824 leveldb.cpp:202] Seeked to beginning of db in 1831ns I0206 00:22:44.792548  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 342ns I0206 00:22:44.792605  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0206 00:22:44.793256  2847 recover.cpp:447] Starting replica recovery I0206 00:22:44.793480  2847 recover.cpp:473] Replica is in EMPTY status I0206 00:22:44.794538  2847 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9472)@172.17.0.2:43484 I0206 00:22:44.795040  2848 recover.cpp:193] Received a recover response from a replica in EMPTY status I0206 00:22:44.795644  2848 recover.cpp:564] Updating replica status to STARTING I0206 00:22:44.796519  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 752810ns I0206 00:22:44.796545  2850 replica.cpp:320] Persisted replica status to STARTING I0206 00:22:44.796725  2848 recover.cpp:473] Replica is in STARTING status I0206 00:22:44.797828  2857 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9473)@172.17.0.2:43484 I0206 00:22:44.798355  2850 recover.cpp:193] Received a recover response from a replica in STARTING status I0206 00:22:44.799193  2850 recover.cpp:564] Updating replica status to VOTING I0206 00:22:44.799583  2855 master.cpp:376] Master 0b206a40-a9c3-4d44-a5bd-8032d60a32ca (6632562f1ade) started on 172.17.0.2:43484 I0206 00:22:44.799609  2855 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/n2FxQV/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/n2FxQV/master"" --zk_session_timeout=""10secs"" I0206 00:22:44.799991  2855 master.cpp:423] Master only allowing authenticated frameworks to register I0206 00:22:44.800009  2855 master.cpp:428] Master only allowing authenticated slaves to register I0206 00:22:44.800020  2855 credentials.hpp:35] Loading credentials for authentication from '/tmp/n2FxQV/credentials' I0206 00:22:44.800245  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 679345ns I0206 00:22:44.800370  2850 replica.cpp:320] Persisted replica status to VOTING I0206 00:22:44.800397  2855 master.cpp:468] Using default 'crammd5' authenticator I0206 00:22:44.800693  2855 master.cpp:537] Using default 'basic' HTTP authenticator I0206 00:22:44.800815  2855 master.cpp:571] Authorization enabled I0206 00:22:44.801216  2850 recover.cpp:578] Successfully joined the Paxos group I0206 00:22:44.801604  2850 recover.cpp:462] Recover process terminated I0206 00:22:44.801759  2856 whitelist_watcher.cpp:77] No whitelist given I0206 00:22:44.801725  2847 hierarchical.cpp:144] Initialized hierarchical allocator process I0206 00:22:44.803982  2855 master.cpp:1712] The newly elected leader is master@172.17.0.2:43484 with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca I0206 00:22:44.804026  2855 master.cpp:1725] Elected as the leading master! I0206 00:22:44.804059  2855 master.cpp:1470] Recovering from registrar I0206 00:22:44.804424  2855 registrar.cpp:307] Recovering registrar I0206 00:22:44.805202  2855 log.cpp:659] Attempting to start the writer I0206 00:22:44.806782  2856 replica.cpp:493] Replica received implicit promise request from (9475)@172.17.0.2:43484 with proposal 1 I0206 00:22:44.807368  2856 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547939ns I0206 00:22:44.807395  2856 replica.cpp:342] Persisted promised to 1 I0206 00:22:44.808375  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions I0206 00:22:44.809460  2848 replica.cpp:388] Replica received explicit promise request from (9476)@172.17.0.2:43484 for position 0 with proposal 2 I0206 00:22:44.809929  2848 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 427561ns I0206 00:22:44.809967  2848 replica.cpp:712] Persisted action at 0 I0206 00:22:44.811035  2850 replica.cpp:537] Replica received write request for position 0 from (9477)@172.17.0.2:43484 I0206 00:22:44.811149  2850 leveldb.cpp:436] Reading position from leveldb took 36452ns I0206 00:22:44.811532  2850 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 318924ns I0206 00:22:44.811615  2850 replica.cpp:712] Persisted action at 0 I0206 00:22:44.812532  2850 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0 I0206 00:22:44.813117  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 476530ns I0206 00:22:44.813143  2850 replica.cpp:712] Persisted action at 0 I0206 00:22:44.813166  2850 replica.cpp:697] Replica learned NOP action at position 0 I0206 00:22:44.813984  2848 log.cpp:675] Writer started with ending position 0 I0206 00:22:44.815549  2848 leveldb.cpp:436] Reading position from leveldb took 31800ns I0206 00:22:44.817061  2848 registrar.cpp:340] Successfully fetched the registry (0B) in 12.591104ms I0206 00:22:44.817319  2848 registrar.cpp:439] Applied 1 operations in 63480ns; attempting to update the 'registry' I0206 00:22:44.818780  2845 log.cpp:683] Attempting to append 170 bytes to the log I0206 00:22:44.818981  2845 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1 I0206 00:22:44.819941  2845 replica.cpp:537] Replica received write request for position 1 from (9478)@172.17.0.2:43484 I0206 00:22:44.820582  2845 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 600949ns I0206 00:22:44.820608  2845 replica.cpp:712] Persisted action at 1 I0206 00:22:44.821552  2845 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0 I0206 00:22:44.821934  2845 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 352813ns I0206 00:22:44.821960  2845 replica.cpp:712] Persisted action at 1 I0206 00:22:44.821979  2845 replica.cpp:697] Replica learned APPEND action at position 1 I0206 00:22:44.823447  2845 registrar.cpp:484] Successfully updated the 'registry' in 5.987072ms I0206 00:22:44.823580  2845 registrar.cpp:370] Successfully recovered registrar I0206 00:22:44.823833  2845 log.cpp:702] Attempting to truncate the log to 1 I0206 00:22:44.824203  2845 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register I0206 00:22:44.824291  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2 I0206 00:22:44.824645  2845 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover I0206 00:22:44.825222  2850 replica.cpp:537] Replica received write request for position 2 from (9479)@172.17.0.2:43484 I0206 00:22:44.825742  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 481617ns I0206 00:22:44.825772  2850 replica.cpp:712] Persisted action at 2 I0206 00:22:44.826748  2852 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0 I0206 00:22:44.827368  2852 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 588591ns I0206 00:22:44.827432  2852 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33059ns I0206 00:22:44.827450  2852 replica.cpp:712] Persisted action at 2 I0206 00:22:44.827468  2852 replica.cpp:697] Replica learned TRUNCATE action at position 2 I0206 00:22:44.838011  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix W0206 00:22:44.838873  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges I0206 00:22:44.843785  2857 slave.cpp:193] Slave started on 172.17.0.2:43484 I0206 00:22:44.843819  2857 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw"" I0206 00:22:44.844292  2857 credentials.hpp:83] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential' I0206 00:22:44.844518  2857 slave.cpp:324] Slave using credential for: test-principal I0206 00:22:44.844696  2857 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000] Trying semicolon-delimited string format instead I0206 00:22:44.845243  2857 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 00:22:44.845326  2857 slave.cpp:472] Slave attributes: [  ] I0206 00:22:44.845342  2857 slave.cpp:477] Slave hostname: 6632562f1ade I0206 00:22:44.845953  2824 sched.cpp:222] Version: 0.28.0 I0206 00:22:44.846853  2848 sched.cpp:326] New master detected at master@172.17.0.2:43484 I0206 00:22:44.846936  2848 sched.cpp:382] Authenticating with master master@172.17.0.2:43484 I0206 00:22:44.846958  2848 sched.cpp:389] Using default CRAM-MD5 authenticatee I0206 00:22:44.847692  2858 state.cpp:58] Recovering state from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta' I0206 00:22:44.848108  2850 status_update_manager.cpp:200] Recovering status update manager I0206 00:22:44.848325  2852 containerizer.cpp:397] Recovering containerizer I0206 00:22:44.848603  2845 authenticatee.cpp:121] Creating new client SASL connection I0206 00:22:44.849719  2845 master.cpp:5523] Authenticating scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:44.850052  2852 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(662)@172.17.0.2:43484 I0206 00:22:44.850227  2854 provisioner.cpp:245] Provisioner recovery complete I0206 00:22:44.850410  2852 authenticator.cpp:98] Creating new server SASL connection I0206 00:22:44.850692  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0206 00:22:44.850720  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 00:22:44.850805  2852 authenticator.cpp:203] Received SASL authentication start I0206 00:22:44.850862  2852 authenticator.cpp:325] Authentication requires more steps I0206 00:22:44.850939  2852 authenticatee.cpp:258] Received SASL authentication step I0206 00:22:44.851027  2852 authenticator.cpp:231] Received SASL authentication step I0206 00:22:44.851052  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0206 00:22:44.851063  2852 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0206 00:22:44.851102  2852 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0206 00:22:44.851121  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0206 00:22:44.851130  2852 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0206 00:22:44.851136  2852 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0206 00:22:44.851150  2852 authenticator.cpp:317] Authentication success I0206 00:22:44.851219  2850 authenticatee.cpp:298] Authentication success I0206 00:22:44.851310  2850 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:44.851485  2849 slave.cpp:4496] Finished recovery I0206 00:22:44.852154  2843 sched.cpp:471] Successfully authenticated with master master@172.17.0.2:43484 I0206 00:22:44.852175  2843 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.2:43484 I0206 00:22:44.852262  2843 sched.cpp:809] Will retry registration in 939.183679ms if necessary I0206 00:22:44.852375  2844 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:44.852448  2844 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*' I0206 00:22:44.852699  2852 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(662)@172.17.0.2:43484 I0206 00:22:44.852782  2844 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ] I0206 00:22:44.853056  2849 slave.cpp:4668] Querying resource estimator for oversubscribable resources I0206 00:22:44.853421  2856 hierarchical.cpp:265] Added framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:44.853513  2856 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:44.853582  2844 sched.cpp:703] Framework registered with 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:44.853613  2852 slave.cpp:4682] Received oversubscribable resources  from the resource estimator I0206 00:22:44.853663  2844 sched.cpp:717] Scheduler::registered took 53762ns I0206 00:22:44.853899  2843 slave.cpp:796] New master detected at master@172.17.0.2:43484 I0206 00:22:44.853955  2854 status_update_manager.cpp:174] Pausing sending status updates I0206 00:22:44.853997  2856 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:44.853960  2843 slave.cpp:859] Authenticating with master master@172.17.0.2:43484 I0206 00:22:44.854035  2843 slave.cpp:864] Using default CRAM-MD5 authenticatee I0206 00:22:44.854030  2856 hierarchical.cpp:1096] Performed allocation for 0 slaves in 581355ns I0206 00:22:44.854182  2843 slave.cpp:832] Detecting new master I0206 00:22:44.854277  2854 authenticatee.cpp:121] Creating new client SASL connection I0206 00:22:44.854517  2843 master.cpp:5523] Authenticating slave@172.17.0.2:43484 I0206 00:22:44.854603  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(663)@172.17.0.2:43484 I0206 00:22:44.854836  2855 authenticator.cpp:98] Creating new server SASL connection I0206 00:22:44.855013  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0206 00:22:44.855044  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 00:22:44.855139  2855 authenticator.cpp:203] Received SASL authentication start I0206 00:22:44.855186  2855 authenticator.cpp:325] Authentication requires more steps I0206 00:22:44.855263  2855 authenticatee.cpp:258] Received SASL authentication step I0206 00:22:44.855352  2855 authenticator.cpp:231] Received SASL authentication step I0206 00:22:44.855381  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0206 00:22:44.855389  2855 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0206 00:22:44.855419  2855 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0206 00:22:44.855438  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0206 00:22:44.855448  2855 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0206 00:22:44.855453  2855 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0206 00:22:44.855464  2855 authenticator.cpp:317] Authentication success I0206 00:22:44.855540  2851 authenticatee.cpp:298] Authentication success I0206 00:22:44.855721  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(663)@172.17.0.2:43484 I0206 00:22:44.855832  2852 slave.cpp:927] Successfully authenticated with master master@172.17.0.2:43484 I0206 00:22:44.855615  2855 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave@172.17.0.2:43484 I0206 00:22:44.855973  2852 slave.cpp:1321] Will retry registration in 9.327708ms if necessary I0206 00:22:44.856145  2854 master.cpp:4237] Registering slave at slave@172.17.0.2:43484 (6632562f1ade) with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 I0206 00:22:44.856598  2851 registrar.cpp:439] Applied 1 operations in 59112ns; attempting to update the 'registry' I0206 00:22:44.857403  2851 log.cpp:683] Attempting to append 339 bytes to the log I0206 00:22:44.857525  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3 I0206 00:22:44.858482  2844 replica.cpp:537] Replica received write request for position 3 from (9493)@172.17.0.2:43484 I0206 00:22:44.858755  2844 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 228484ns I0206 00:22:44.858855  2844 replica.cpp:712] Persisted action at 3 I0206 00:22:44.859751  2852 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0 I0206 00:22:44.860332  2852 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 549638ns I0206 00:22:44.860358  2852 replica.cpp:712] Persisted action at 3 I0206 00:22:44.860411  2852 replica.cpp:697] Replica learned APPEND action at position 3 I0206 00:22:44.862709  2856 registrar.cpp:484] Successfully updated the 'registry' in 6.020864ms I0206 00:22:44.863106  2850 log.cpp:702] Attempting to truncate the log to 3 I0206 00:22:44.863358  2850 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4 I0206 00:22:44.864321  2850 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484 I0206 00:22:44.864706  2849 hierarchical.cpp:473] Added slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: ) I0206 00:22:44.864716  2843 replica.cpp:537] Replica received write request for position 4 from (9494)@172.17.0.2:43484 I0206 00:22:44.865309  2843 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 410199ns I0206 00:22:44.865337  2843 replica.cpp:712] Persisted action at 4 I0206 00:22:44.866092  2849 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:44.866132  2848 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0 I0206 00:22:44.866137  2849 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 1.30657ms I0206 00:22:44.866497  2856 master.cpp:4305] Registered slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 00:22:44.866564  2843 slave.cpp:1321] Will retry registration in 32.803438ms if necessary I0206 00:22:44.866690  2843 slave.cpp:971] Registered with master master@172.17.0.2:43484; given slave ID 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 I0206 00:22:44.866716  2843 fetcher.cpp:81] Clearing fetcher cache I0206 00:22:44.867066  2856 master.cpp:5352] Sending 1 offers to framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:44.867105  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/slave.info' I0206 00:22:44.867347  2856 master.cpp:4207] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) already registered, resending acknowledgement I0206 00:22:44.867441  2856 status_update_manager.cpp:181] Resuming sending status updates I0206 00:22:44.867465  2843 slave.cpp:1030] Forwarding total oversubscribed resources  W0206 00:22:44.867547  2843 slave.cpp:1016] Already registered with master master@172.17.0.2:43484 I0206 00:22:44.867574  2843 slave.cpp:1030] Forwarding total oversubscribed resources  I0206 00:22:44.867710  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources  I0206 00:22:44.867951  2856 sched.cpp:873] Scheduler::resourceOffers took 133371ns I0206 00:22:44.867961  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources  I0206 00:22:44.868484  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) I0206 00:22:44.868599  2848 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.418545ms I0206 00:22:44.868700  2848 leveldb.cpp:399] Deleting ~2 keys from leveldb took 54053ns I0206 00:22:44.868751  2848 replica.cpp:712] Persisted action at 4 I0206 00:22:44.868811  2848 replica.cpp:697] Replica learned TRUNCATE action at position 4 I0206 00:22:44.869241  2856 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:44.869287  2856 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:44.869321  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 782848ns I0206 00:22:44.869840  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) I0206 00:22:44.869985  2856 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:44.870028  2856 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:44.870053  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 160104ns I0206 00:22:44.871824  2853 master.cpp:3138] Processing ACCEPT call for offers: [ 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-O0 ] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:44.871868  2853 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos' W0206 00:22:44.873613  2843 validation.cpp:404] Executor http for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0206 00:22:44.873667  2843 validation.cpp:416] Executor http for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0206 00:22:44.874035  2843 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) I0206 00:22:44.874223  2843 master.cpp:3623] Launching task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) I0206 00:22:44.874802  2843 slave.cpp:1361] Got assigned task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:44.874966  2843 slave.cpp:5202] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.info' I0206 00:22:44.875440  2843 slave.cpp:5213] Checkpointing framework pid 'scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484' to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.pid' I0206 00:22:44.876106  2843 slave.cpp:1480] Launching task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:44.876644  2843 paths.cpp:474] Trying to chown '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' to user 'mesos' I0206 00:22:44.884089  2843 slave.cpp:5654] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/executor.info' I0206 00:22:44.900928  2843 slave.cpp:5282] Launching executor http of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 with resources  in work directory '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' I0206 00:22:44.901449  2853 containerizer.cpp:656] Starting container 'fd4649a4-1c82-4eda-b663-b568b6110d17' for executor 'http' of framework '0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' I0206 00:22:44.901561  2843 slave.cpp:5677] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/tasks/1/task.info' I0206 00:22:44.902060  2843 slave.cpp:1698] Queuing task '1' for executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:44.902207  2843 slave.cpp:749] Successfully attached file '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' I0206 00:22:44.907027  2850 launcher.cpp:132] Forked child with pid '8875' for container 'fd4649a4-1c82-4eda-b663-b568b6110d17' I0206 00:22:44.907229  2850 containerizer.cpp:1094] Checkpointing executor's forked pid 8875 to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/pids/forked.pid' WARNING: Logging before InitGoogleLogging() is written to STDERR I0206 00:22:45.080060  8875 process.cpp:991] libprocess is initialized on 172.17.0.2:49724 for 16 cpus I0206 00:22:45.082499  8875 logging.cpp:193] Logging to STDERR I0206 00:22:45.082862  8875 executor.cpp:172] Version: 0.28.0 I0206 00:22:45.087201  8903 executor.cpp:316] Connected with the agent I0206 00:22:45.802878  2858 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:45.802969  2858 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:45.803014  2858 hierarchical.cpp:1096] Performed allocation for 1 slaves in 424120ns 2016-02-06 00:22:45,982:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0206 00:22:46.588022  2854 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration I0206 00:22:46.588969  2854 group.cpp:519] ZooKeeper session expired 2016-02-06 00:22:46,589:2824(0x7fd9fefd1700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade 2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency 2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos 2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV 2016-02-06 00:22:46,590:2824(0x7fda03fdb700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9d401bc10 flags=0 2016-02-06 00:22:46,590:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0206 00:22:46.804400  2844 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:46.804481  2844 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:46.804514  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 347954ns I0206 00:22:47.805842  2847 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:47.805934  2847 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:47.805980  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 415449ns I0206 00:22:48.807723  2851 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:48.807814  2851 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:48.807857  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 442104ns I0206 00:22:49.808733  2848 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:49.808816  2848 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:49.808856  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 384959ns 2016-02-06 00:22:49,926:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0206 00:22:50.810307  2847 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:50.810400  2847 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:50.810443  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 389572ns I0206 00:22:51.811586  2849 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:51.811681  2849 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:51.811722  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 404450ns I0206 00:22:52.812860  2851 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:52.812944  2851 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:52.812981  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 359671ns 2016-02-06 00:22:53,263:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0206 00:22:53.814512  2847 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:53.814599  2847 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:53.814651  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 386669ns I0206 00:22:54.815238  2852 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:54.815321  2852 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:54.815356  2852 hierarchical.cpp:1096] Performed allocation for 1 slaves in 376235ns I0206 00:22:55.816453  2846 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:55.816550  2846 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:55.816596  2846 hierarchical.cpp:1096] Performed allocation for 1 slaves in 416350ns W0206 00:22:56.592408  2849 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration I0206 00:22:56.593480  2849 group.cpp:519] ZooKeeper session expired 2016-02-06 00:22:56,593:2824(0x7fda017d6700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade 2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency 2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos 2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV 2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9e401f350 flags=0 2016-02-06 00:22:56,595:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0206 00:22:56.817683  2848 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:56.817766  2848 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:56.817803  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 374115ns I0206 00:22:57.818447  2844 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:57.818526  2844 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:57.818562  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 344545ns I0206 00:22:58.819828  2851 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:58.819914  2851 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:58.819957  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 376948ns I0206 00:22:59.820874  2848 hierarchical.cpp:1403] No resources available to allocate! I0206 00:22:59.820957  2848 hierarchical.cpp:1498] No inverse offers to send out! I0206 00:22:59.820991  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 344192ns I0206 00:22:59.854698  2845 slave.cpp:4668] Querying resource estimator for oversubscribable resources I0206 00:22:59.854991  2845 slave.cpp:4682] Received oversubscribable resources  from the resource estimator I0206 00:22:59.864612  2857 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484 ../../src/tests/slave_recovery_tests.cpp:1105: Failure Failed to wait 15secs for updateCall1 I0206 00:22:59.876358  2852 master.cpp:1213] Framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 disconnected I0206 00:22:59.876410  2852 master.cpp:2576] Disconnecting framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:59.876456  2852 master.cpp:2600] Deactivating framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:59.876569  2852 master.cpp:1237] Giving framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 0ns to failover I0206 00:22:59.876981  2844 hierarchical.cpp:375] Deactivated framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:59.877049  2844 master.cpp:5204] Framework failover timeout, removing framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:59.877075  2844 master.cpp:5935] Removing framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 I0206 00:22:59.877276  2844 master.cpp:6447] Updating the state of task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED) I0206 00:22:59.878051  2844 master.cpp:6513] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) I0206 00:22:59.878433  2844 master.cpp:6542] Removing executor 'http' with resources  of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) I0206 00:22:59.878667  2852 slave.cpp:2079] Asked to shut down framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 by master@172.17.0.2:43484 I0206 00:22:59.878733  2852 slave.cpp:2104] Shutting down framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:59.878806  2852 slave.cpp:4129] Shutting down executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 W0206 00:22:59.878834  2852 slave.hpp:655] Unable to send event to executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000: unknown connection type I0206 00:22:59.879550  2844 master.cpp:1027] Master terminating I0206 00:22:59.879703  2854 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 from framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:59.879947  2854 hierarchical.cpp:326] Removed framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:59.880306  2854 hierarchical.cpp:505] Removed slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 I0206 00:22:59.880666  2852 slave.cpp:3482] master@172.17.0.2:43484 exited W0206 00:22:59.880695  2852 slave.cpp:3485] Master disconnected! Waiting for a new master to be elected I0206 00:22:59.885498  2857 containerizer.cpp:1318] Destroying container 'fd4649a4-1c82-4eda-b663-b568b6110d17' I0206 00:22:59.904532  2858 containerizer.cpp:1534] Executor for container 'fd4649a4-1c82-4eda-b663-b568b6110d17' has exited I0206 00:22:59.907024  2858 provisioner.cpp:306] Ignoring destroy request for unknown container fd4649a4-1c82-4eda-b663-b568b6110d17 I0206 00:22:59.907428  2858 slave.cpp:3817] Executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 terminated with signal Killed I0206 00:22:59.907538  2858 slave.cpp:3921] Cleaning up executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:59.908213  2858 slave.cpp:4009] Cleaning up framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:59.908555  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' for gc 6.99998949252444days in the future I0206 00:22:59.908720  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http' for gc 6.99998949082074days in the future I0206 00:22:59.908807  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' for gc 6.99998948980444days in the future I0206 00:22:59.908927  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http' for gc 6.99998948890074days in the future I0206 00:22:59.909009  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' for gc 6.99998948710518days in the future I0206 00:22:59.909121  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' for gc 6.99998948630815days in the future I0206 00:22:59.909211  2858 status_update_manager.cpp:282] Closing status update streams for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 I0206 00:22:59.910423  2853 slave.cpp:668] Slave terminating ../../3rdparty/libprocess/include/process/gmock.hpp:425: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const HttpEvent&>()))...     Expected args: union http matcher (72-byte object <D0-11 44-12 DA-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>, UPDATE, 1-byte object <1B>, 16-byte object <1B-F4 34-01 00-00 00-00 00-00 00-00 DA-7F 00-00>)          Expected: to be called once            Actual: never called - unsatisfied and active ../../3rdparty/libprocess/include/process/gmock.hpp:425: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const HttpEvent&>()))...     Expected args: union http matcher (72-byte object <D0-11 44-12 DA-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>, UPDATE, 1-byte object <1B>, 16-byte object <1B-F4 34-01 00-00 00-00 00-00 00-00 DA-7F 00-00>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] SlaveRecoveryTest/0.CleanupHTTPExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (15126 ms) {code}",3
"MESOS-4615","ContainerLoggerTest.DefaultToSandbox is flaky","Just saw this failure on the ASF CI:  {code} [ RUN      ] ContainerLoggerTest.DefaultToSandbox I0206 01:25:03.766458  2824 leveldb.cpp:174] Opened db in 72.979786ms I0206 01:25:03.811712  2824 leveldb.cpp:181] Compacted db in 45.162067ms I0206 01:25:03.811810  2824 leveldb.cpp:196] Created db iterator in 26090ns I0206 01:25:03.811828  2824 leveldb.cpp:202] Seeked to beginning of db in 3173ns I0206 01:25:03.811839  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 497ns I0206 01:25:03.811900  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0206 01:25:03.812785  2849 recover.cpp:447] Starting replica recovery I0206 01:25:03.813043  2849 recover.cpp:473] Replica is in EMPTY status I0206 01:25:03.814668  2854 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (371)@172.17.0.8:37843 I0206 01:25:03.815210  2849 recover.cpp:193] Received a recover response from a replica in EMPTY status I0206 01:25:03.815732  2854 recover.cpp:564] Updating replica status to STARTING I0206 01:25:03.819664  2857 master.cpp:376] Master 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de (74ef606c4063) started on 172.17.0.8:37843 I0206 01:25:03.819703  2857 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/h5vu5I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/h5vu5I/master"" --zk_session_timeout=""10secs"" I0206 01:25:03.820241  2857 master.cpp:423] Master only allowing authenticated frameworks to register I0206 01:25:03.820257  2857 master.cpp:428] Master only allowing authenticated slaves to register I0206 01:25:03.820269  2857 credentials.hpp:35] Loading credentials for authentication from '/tmp/h5vu5I/credentials' I0206 01:25:03.821110  2857 master.cpp:468] Using default 'crammd5' authenticator I0206 01:25:03.821311  2857 master.cpp:537] Using default 'basic' HTTP authenticator I0206 01:25:03.821636  2857 master.cpp:571] Authorization enabled I0206 01:25:03.821979  2846 hierarchical.cpp:144] Initialized hierarchical allocator process I0206 01:25:03.822057  2846 whitelist_watcher.cpp:77] No whitelist given I0206 01:25:03.825460  2847 master.cpp:1712] The newly elected leader is master@172.17.0.8:37843 with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de I0206 01:25:03.825512  2847 master.cpp:1725] Elected as the leading master! I0206 01:25:03.825533  2847 master.cpp:1470] Recovering from registrar I0206 01:25:03.825835  2847 registrar.cpp:307] Recovering registrar I0206 01:25:03.848212  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 32.226093ms I0206 01:25:03.848299  2854 replica.cpp:320] Persisted replica status to STARTING I0206 01:25:03.848702  2854 recover.cpp:473] Replica is in STARTING status I0206 01:25:03.850728  2858 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (373)@172.17.0.8:37843 I0206 01:25:03.851230  2854 recover.cpp:193] Received a recover response from a replica in STARTING status I0206 01:25:03.852018  2854 recover.cpp:564] Updating replica status to VOTING I0206 01:25:03.881681  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.184163ms I0206 01:25:03.881772  2854 replica.cpp:320] Persisted replica status to VOTING I0206 01:25:03.882058  2854 recover.cpp:578] Successfully joined the Paxos group I0206 01:25:03.882258  2854 recover.cpp:462] Recover process terminated I0206 01:25:03.883076  2854 log.cpp:659] Attempting to start the writer I0206 01:25:03.885040  2854 replica.cpp:493] Replica received implicit promise request from (374)@172.17.0.8:37843 with proposal 1 I0206 01:25:03.915132  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.980589ms I0206 01:25:03.915215  2854 replica.cpp:342] Persisted promised to 1 I0206 01:25:03.916038  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions I0206 01:25:03.917659  2856 replica.cpp:388] Replica received explicit promise request from (375)@172.17.0.8:37843 for position 0 with proposal 2 I0206 01:25:03.948698  2856 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.974607ms I0206 01:25:03.948786  2856 replica.cpp:712] Persisted action at 0 I0206 01:25:03.950920  2849 replica.cpp:537] Replica received write request for position 0 from (376)@172.17.0.8:37843 I0206 01:25:03.951011  2849 leveldb.cpp:436] Reading position from leveldb took 44263ns I0206 01:25:03.982026  2849 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 30.947321ms I0206 01:25:03.982225  2849 replica.cpp:712] Persisted action at 0 I0206 01:25:03.983867  2849 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0 I0206 01:25:04.015499  2849 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 30.957888ms I0206 01:25:04.015591  2849 replica.cpp:712] Persisted action at 0 I0206 01:25:04.015682  2849 replica.cpp:697] Replica learned NOP action at position 0 I0206 01:25:04.016666  2849 log.cpp:675] Writer started with ending position 0 I0206 01:25:04.017881  2855 leveldb.cpp:436] Reading position from leveldb took 56779ns I0206 01:25:04.018934  2852 registrar.cpp:340] Successfully fetched the registry (0B) in 193.048064ms I0206 01:25:04.019076  2852 registrar.cpp:439] Applied 1 operations in 38180ns; attempting to update the 'registry' I0206 01:25:04.020100  2844 log.cpp:683] Attempting to append 170 bytes to the log I0206 01:25:04.020288  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1 I0206 01:25:04.021323  2844 replica.cpp:537] Replica received write request for position 1 from (377)@172.17.0.8:37843 I0206 01:25:04.054726  2844 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.309419ms I0206 01:25:04.054818  2844 replica.cpp:712] Persisted action at 1 I0206 01:25:04.055933  2844 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0 I0206 01:25:04.088142  2844 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 32.116643ms I0206 01:25:04.088230  2844 replica.cpp:712] Persisted action at 1 I0206 01:25:04.088265  2844 replica.cpp:697] Replica learned APPEND action at position 1 I0206 01:25:04.090070  2856 registrar.cpp:484] Successfully updated the 'registry' in 70.90816ms I0206 01:25:04.090338  2851 log.cpp:702] Attempting to truncate the log to 1 I0206 01:25:04.090358  2856 registrar.cpp:370] Successfully recovered registrar I0206 01:25:04.090507  2847 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2 I0206 01:25:04.090867  2858 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register I0206 01:25:04.091449  2858 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover I0206 01:25:04.092280  2857 replica.cpp:537] Replica received write request for position 2 from (378)@172.17.0.8:37843 I0206 01:25:04.125702  2857 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 33.192265ms I0206 01:25:04.125804  2857 replica.cpp:712] Persisted action at 2 I0206 01:25:04.127400  2857 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0 I0206 01:25:04.157727  2857 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 30.268594ms I0206 01:25:04.157905  2857 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88436ns I0206 01:25:04.157941  2857 replica.cpp:712] Persisted action at 2 I0206 01:25:04.157984  2857 replica.cpp:697] Replica learned TRUNCATE action at position 2 I0206 01:25:04.166174  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix W0206 01:25:04.166954  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges I0206 01:25:04.172008  2844 slave.cpp:193] Slave started on 9)@172.17.0.8:37843 I0206 01:25:04.172046  2844 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw"" I0206 01:25:04.172569  2844 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential' I0206 01:25:04.172886  2844 slave.cpp:324] Slave using credential for: test-principal I0206 01:25:04.173141  2844 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000] Trying semicolon-delimited string format instead I0206 01:25:04.173620  2844 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 01:25:04.173686  2844 slave.cpp:472] Slave attributes: [  ] I0206 01:25:04.173702  2844 slave.cpp:477] Slave hostname: 74ef606c4063 I0206 01:25:04.174816  2847 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta' I0206 01:25:04.175441  2847 status_update_manager.cpp:200] Recovering status update manager I0206 01:25:04.175678  2858 containerizer.cpp:397] Recovering containerizer I0206 01:25:04.177573  2858 provisioner.cpp:245] Provisioner recovery complete I0206 01:25:04.178231  2847 slave.cpp:4496] Finished recovery I0206 01:25:04.178834  2847 slave.cpp:4668] Querying resource estimator for oversubscribable resources I0206 01:25:04.179405  2847 slave.cpp:796] New master detected at master@172.17.0.8:37843 I0206 01:25:04.179500  2847 slave.cpp:859] Authenticating with master master@172.17.0.8:37843 I0206 01:25:04.179525  2847 slave.cpp:864] Using default CRAM-MD5 authenticatee I0206 01:25:04.179656  2858 status_update_manager.cpp:174] Pausing sending status updates I0206 01:25:04.179798  2847 slave.cpp:832] Detecting new master I0206 01:25:04.179891  2852 authenticatee.cpp:121] Creating new client SASL connection I0206 01:25:04.179916  2847 slave.cpp:4682] Received oversubscribable resources  from the resource estimator I0206 01:25:04.180286  2847 master.cpp:5523] Authenticating slave(9)@172.17.0.8:37843 I0206 01:25:04.180569  2847 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(32)@172.17.0.8:37843 I0206 01:25:04.181000  2847 authenticator.cpp:98] Creating new server SASL connection I0206 01:25:04.181315  2847 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0206 01:25:04.181387  2847 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 01:25:04.181562  2847 authenticator.cpp:203] Received SASL authentication start I0206 01:25:04.181648  2847 authenticator.cpp:325] Authentication requires more steps I0206 01:25:04.181843  2847 authenticatee.cpp:258] Received SASL authentication step I0206 01:25:04.182034  2853 authenticator.cpp:231] Received SASL authentication step I0206 01:25:04.182071  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0206 01:25:04.182093  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0206 01:25:04.182145  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0206 01:25:04.182173  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0206 01:25:04.182185  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0206 01:25:04.182193  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0206 01:25:04.182211  2853 authenticator.cpp:317] Authentication success I0206 01:25:04.182333  2849 authenticatee.cpp:298] Authentication success I0206 01:25:04.182422  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(9)@172.17.0.8:37843 I0206 01:25:04.182510  2853 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(32)@172.17.0.8:37843 I0206 01:25:04.182945  2849 slave.cpp:927] Successfully authenticated with master master@172.17.0.8:37843 I0206 01:25:04.183178  2849 slave.cpp:1321] Will retry registration in 9.87937ms if necessary I0206 01:25:04.183466  2852 master.cpp:4237] Registering slave at slave(9)@172.17.0.8:37843 (74ef606c4063) with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 I0206 01:25:04.184039  2845 registrar.cpp:439] Applied 1 operations in 89453ns; attempting to update the 'registry' I0206 01:25:04.185288  2856 log.cpp:683] Attempting to append 339 bytes to the log I0206 01:25:04.185672  2850 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3 I0206 01:25:04.186674  2846 replica.cpp:537] Replica received write request for position 3 from (392)@172.17.0.8:37843 I0206 01:25:04.195863  2856 slave.cpp:1321] Will retry registration in 11.038094ms if necessary I0206 01:25:04.196233  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress I0206 01:25:04.208094  2856 slave.cpp:1321] Will retry registration in 27.881223ms if necessary I0206 01:25:04.208472  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress I0206 01:25:04.216698  2846 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 29.961291ms I0206 01:25:04.216789  2846 replica.cpp:712] Persisted action at 3 I0206 01:25:04.218246  2845 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0 I0206 01:25:04.237861  2846 slave.cpp:1321] Will retry registration in 1.006941ms if necessary I0206 01:25:04.238221  2846 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress I0206 01:25:04.239858  2856 slave.cpp:1321] Will retry registration in 167.305686ms if necessary I0206 01:25:04.240044  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress I0206 01:25:04.241482  2845 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 23.193162ms I0206 01:25:04.241524  2845 replica.cpp:712] Persisted action at 3 I0206 01:25:04.241557  2845 replica.cpp:697] Replica learned APPEND action at position 3 I0206 01:25:04.243746  2844 registrar.cpp:484] Successfully updated the 'registry' in 59.587072ms I0206 01:25:04.244210  2857 log.cpp:702] Attempting to truncate the log to 3 I0206 01:25:04.244344  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4 I0206 01:25:04.244597  2856 master.cpp:4305] Registered slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 01:25:04.244746  2843 slave.cpp:3436] Received ping from slave-observer(8)@172.17.0.8:37843 I0206 01:25:04.244976  2845 hierarchical.cpp:473] Added slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: ) I0206 01:25:04.245072  2843 slave.cpp:971] Registered with master master@172.17.0.8:37843; given slave ID 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 I0206 01:25:04.245121  2843 fetcher.cpp:81] Clearing fetcher cache I0206 01:25:04.245146  2845 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:04.245178  2845 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 159744ns I0206 01:25:04.245465  2846 status_update_manager.cpp:181] Resuming sending status updates I0206 01:25:04.245776  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/slave.info' I0206 01:25:04.245745  2846 replica.cpp:537] Replica received write request for position 4 from (393)@172.17.0.8:37843 I0206 01:25:04.246273  2843 slave.cpp:1030] Forwarding total oversubscribed resources  I0206 01:25:04.246507  2850 master.cpp:4646] Received update of slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with total oversubscribed resources  I0206 01:25:04.247180  2824 sched.cpp:222] Version: 0.28.0 I0206 01:25:04.247155  2850 hierarchical.cpp:531] Slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) I0206 01:25:04.247357  2850 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:04.247406  2850 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 183250ns I0206 01:25:04.247938  2854 sched.cpp:326] New master detected at master@172.17.0.8:37843 I0206 01:25:04.248157  2854 sched.cpp:382] Authenticating with master master@172.17.0.8:37843 I0206 01:25:04.248265  2854 sched.cpp:389] Using default CRAM-MD5 authenticatee I0206 01:25:04.248769  2854 authenticatee.cpp:121] Creating new client SASL connection I0206 01:25:04.249311  2854 master.cpp:5523] Authenticating scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 I0206 01:25:04.249646  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(33)@172.17.0.8:37843 I0206 01:25:04.250114  2854 authenticator.cpp:98] Creating new server SASL connection I0206 01:25:04.250453  2854 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0206 01:25:04.250525  2854 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 01:25:04.250814  2853 authenticator.cpp:203] Received SASL authentication start I0206 01:25:04.250881  2853 authenticator.cpp:325] Authentication requires more steps I0206 01:25:04.250982  2853 authenticatee.cpp:258] Received SASL authentication step I0206 01:25:04.251092  2853 authenticator.cpp:231] Received SASL authentication step I0206 01:25:04.251128  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0206 01:25:04.251144  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0206 01:25:04.251200  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0206 01:25:04.251242  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0206 01:25:04.251260  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0206 01:25:04.251269  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0206 01:25:04.251288  2853 authenticator.cpp:317] Authentication success I0206 01:25:04.251471  2853 authenticatee.cpp:298] Authentication success I0206 01:25:04.251574  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 I0206 01:25:04.251669  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(33)@172.17.0.8:37843 I0206 01:25:04.252162  2854 sched.cpp:471] Successfully authenticated with master master@172.17.0.8:37843 I0206 01:25:04.252188  2854 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.8:37843 I0206 01:25:04.252286  2854 sched.cpp:809] Will retry registration in 1.575999657secs if necessary I0206 01:25:04.252583  2853 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 I0206 01:25:04.252694  2853 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*' I0206 01:25:04.253110  2853 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ] I0206 01:25:04.253703  2843 hierarchical.cpp:265] Added framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:04.255300  2843 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:04.255367  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 1.621522ms I0206 01:25:04.255820  2844 sched.cpp:703] Framework registered with 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:04.256006  2844 sched.cpp:717] Scheduler::registered took 105156ns I0206 01:25:04.256572  2853 master.cpp:5352] Sending 1 offers to framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 I0206 01:25:04.257524  2853 sched.cpp:873] Scheduler::resourceOffers took 173470ns I0206 01:25:04.260818  2855 master.cpp:3138] Processing ACCEPT call for offers: [ 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-O0 ] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 I0206 01:25:04.260968  2855 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 0e7267ed-c5ed-4914-9042-5970b2aaec1c as user 'mesos' I0206 01:25:04.264458  2844 master.hpp:176] Adding task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) I0206 01:25:04.264796  2844 master.cpp:3623] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) I0206 01:25:04.265341  2855 slave.cpp:1361] Got assigned task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:04.265941  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32 Trying semicolon-delimited string format instead I0206 01:25:04.267323  2855 slave.cpp:1480] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:04.267627  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32 Trying semicolon-delimited string format instead I0206 01:25:04.268705  2855 paths.cpp:474] Trying to chown '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' to user 'mesos' I0206 01:25:04.274116  2855 slave.cpp:5282] Launching executor 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' I0206 01:25:04.275185  2844 containerizer.cpp:656] Starting container '5c952202-44cf-427a-8452-0f501140a4b7' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000' I0206 01:25:04.275311  2846 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 29.403837ms I0206 01:25:04.275390  2846 replica.cpp:712] Persisted action at 4 I0206 01:25:04.275511  2855 slave.cpp:1698] Queuing task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:04.275832  2855 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' I0206 01:25:04.276707  2855 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0 I0206 01:25:04.284708  2844 launcher.cpp:132] Forked child with pid '2872' for container '5c952202-44cf-427a-8452-0f501140a4b7' I0206 01:25:04.301365  2855 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.497489ms I0206 01:25:04.301528  2855 leveldb.cpp:399] Deleting ~2 keys from leveldb took 92156ns I0206 01:25:04.301563  2855 replica.cpp:712] Persisted action at 4 I0206 01:25:04.301640  2855 replica.cpp:697] Replica learned TRUNCATE action at position 4 I0206 01:25:04.823314  2854 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:04.823387  2854 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:04.823420  2854 hierarchical.cpp:1096] Performed allocation for 1 slaves in 327509ns I0206 01:25:05.825943  2850 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:05.826027  2850 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:05.826066  2850 hierarchical.cpp:1096] Performed allocation for 1 slaves in 362856ns I0206 01:25:06.827154  2857 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:06.827235  2857 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:06.827275  2857 hierarchical.cpp:1096] Performed allocation for 1 slaves in 328221ns I0206 01:25:07.828547  2843 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:07.828753  2843 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:07.828907  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 624979ns I0206 01:25:08.829737  2855 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:08.829918  2855 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:08.830070  2855 hierarchical.cpp:1096] Performed allocation for 1 slaves in 596793ns I0206 01:25:09.831233  2856 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:09.831316  2856 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:09.831352  2856 hierarchical.cpp:1096] Performed allocation for 1 slaves in 353864ns I0206 01:25:10.832953  2849 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:10.833307  2849 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:10.833411  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 731864ns I0206 01:25:11.834967  2847 hierarchical.cpp:1403] No resources available to allocate! I0206 01:25:11.835149  2847 hierarchical.cpp:1498] No inverse offers to send out! I0206 01:25:11.835294  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 586988ns I0206 01:25:12.174247  2853 slave.cpp:2643] Got registration for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659 I0206 01:25:12.179061  2844 slave.cpp:1863] Sending queued task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' to executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659 I0206 01:25:12.194753  2858 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659 I0206 01:25:12.195852  2858 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.196094  2858 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.197000  2858 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to the slave I0206 01:25:12.197739  2855 slave.cpp:3354] Forwarding the update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to master@172.17.0.8:37843 I0206 01:25:12.198442  2855 master.cpp:4791] Status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) I0206 01:25:12.198673  2855 master.cpp:4839] Forwarding status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.199038  2855 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING) I0206 01:25:12.199581  2854 sched.cpp:981] Scheduler::statusUpdate took 159022ns I0206 01:25:12.200568  2854 master.cpp:3949] Processing ACKNOWLEDGE call 9d924a5b-76ab-4886-8091-7af3428ff179 for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 I0206 01:25:12.201513  2858 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 ../../src/tests/container_logger_tests.cpp:350: Failure Value of: strings::contains(stdout.get(), ""Hello World!"")   Actual: false Expected: true I0206 01:25:12.201702  2824 sched.cpp:1903] Asked to stop the driver I0206 01:25:12.202831  2848 sched.cpp:1143] Stopping framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000' I0206 01:25:12.203284  2848 master.cpp:5923] Processing TEARDOWN call for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 I0206 01:25:12.203321  2848 master.cpp:5935] Removing framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 I0206 01:25:12.201762  2854 slave.cpp:3248] Status update manager successfully handled status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.203384  2854 slave.cpp:3264] Sending acknowledgement for status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to executor(1)@172.17.0.8:43659 I0206 01:25:12.204712  2843 hierarchical.cpp:375] Deactivated framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.204953  2848 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED) I0206 01:25:12.205885  2854 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.206082  2854 slave.cpp:2079] Asked to shut down framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 by master@172.17.0.8:37843 I0206 01:25:12.206125  2854 slave.cpp:2104] Shutting down framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.206331  2854 slave.cpp:4129] Shutting down executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659 I0206 01:25:12.206408  2843 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 from framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.207352  2848 master.cpp:6513] Removing task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) I0206 01:25:12.208258  2848 master.cpp:1027] Master terminating I0206 01:25:12.208703  2857 hierarchical.cpp:326] Removed framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.209658  2857 hierarchical.cpp:505] Removed slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 I0206 01:25:12.212208  2848 slave.cpp:3482] master@172.17.0.8:37843 exited W0206 01:25:12.212261  2848 slave.cpp:3485] Master disconnected! Waiting for a new master to be elected I0206 01:25:12.224596  2854 containerizer.cpp:1318] Destroying container '5c952202-44cf-427a-8452-0f501140a4b7' I0206 01:25:12.241466  2852 slave.cpp:3482] executor(1)@172.17.0.8:43659 exited I0206 01:25:12.250931  2856 containerizer.cpp:1534] Executor for container '5c952202-44cf-427a-8452-0f501140a4b7' has exited I0206 01:25:12.253350  2850 provisioner.cpp:306] Ignoring destroy request for unknown container 5c952202-44cf-427a-8452-0f501140a4b7 I0206 01:25:12.253885  2850 slave.cpp:3817] Executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 terminated with signal Killed I0206 01:25:12.254125  2850 slave.cpp:3921] Cleaning up executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659 I0206 01:25:12.254545  2847 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' for gc 6.99999705530074days in the future I0206 01:25:12.254803  2850 slave.cpp:4009] Cleaning up framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.254822  2847 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c' for gc 6.99999705202667days in the future I0206 01:25:12.255084  2857 status_update_manager.cpp:282] Closing status update streams for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.255143  2856 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000' for gc 6.99999704808days in the future I0206 01:25:12.255190  2857 status_update_manager.cpp:528] Cleaning up status update stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 I0206 01:25:12.255192  2850 slave.cpp:668] Slave terminating [  FAILED  ] ContainerLoggerTest.DefaultToSandbox (8566 ms) {code}",1
"MESOS-4619","Remove markdown files from doxygen pages","The doxygen html pages corresponding to doc/* markdown files are redundant and have broken links. They don't serve any reasonable purpose in doxygen site.",1
"MESOS-4623","Add a stub Nvidia GPU isolator.","We'll first wire up a skeleton Nvidia GPU isolator, which needs to be guarded by a configure flag due to the dependency on NVML.",3
"MESOS-4624","Add allocation metrics for ""gpus"" resources.","Allocation metrics are currently hard-coded to include only {{\[""cpus"", ""mem"", ""disk""\]}} resources. We'll need to add ""gpus"" to the list to start, possibly following up on the TODO to remove the hard-coding.  See: https://github.com/apache/mesos/blob/0.27.0/src/master/metrics.cpp#L266-L269 https://github.com/apache/mesos/blob/0.27.0/src/slave/metrics.cpp#L123-L126 ",1
"MESOS-4625","Implement Nvidia GPU isolation w/o filesystem isolation enabled.","The Nvidia GPU isolator will need to use the device cgroup to restrict access to GPU resources, and will need to recover this information after agent failover. For now this will require that the operator specifies the GPU devices via a flag.  To handle filesystem isolation requires that we provide mechanisms for operators to inject volumes with the necessary libraries into all containers using GPU resources, we'll tackle this in a separate ticket.",5
"MESOS-4657","Add LOG(INFO) in `cgroups/net_cls` for debugging allocation of net_cls handles.","We need to add LOG(INFO) during the prepare phase of `cgroups/net_cls` for debugging management of `net_cls` handles within the isolator. ",1
"MESOS-4669","Add common compression utility","We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.",2
"MESOS-4683","Document docker runtime isolator.","Should include the following information:  *What features are currently supported in docker runtime isolator. *How to use the docker runtime isolator (user manual). *Compare the different semantics v.s. docker containerizer, and explain why.",2
"MESOS-4684","Create base docker image for test suite.","This should be widely used for unified containerizer testing. Should basically include:  *at least one layer. *repositories.  For each layer: *root file system as a layer tar ball. *docker image json (manifest). *docker version.",3
"MESOS-4702","Document default value of ""offer_timeout""","There isn't a default value (i.e., offers do not timeout by default), but we should clarify this in {{flags.cpp}} and {{configuration.md}}.",1
"MESOS-4703","Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)","Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.  We should make a StoutConfigure.cmake that can be included by any package downstream.",1
"MESOS-4713","ReviewBot should not fail hard if there are circular dependencies in a review chain","Instead of failing hard, ReviewBot should post an error to the review that a circular dependency is detected.",2
"MESOS-4714","""make DESTDIR=<path> install"" broken","There is a missing '$(DESTDIR)' prefix in the install-data-hook that causes DESTDIR builds to be broken.",2
"MESOS-4731","Update /frameworks to use jsonify","This should let us remove the duplicated code in {{http.cpp}} between {{model(Framework)}} and {{json(Full<Framework>)}}.",3
"MESOS-4747","ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation","Some cleanup of spawned processes is missing in {{ContainerLoggerTest.MesosContainerizerRecover}} so that when the test is run in isolation the global teardown might find lingering processes. {code} [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from ContainerLoggerTest [ RUN      ] ContainerLoggerTest.MesosContainerizerRecover [       OK ] ContainerLoggerTest.MesosContainerizerRecover (13 ms) [----------] 1 test from ContainerLoggerTest (13 ms total)  [----------] Global test environment tear-down ../../src/tests/environment.cpp:728: Failure Failed Tests completed with child processes remaining: -+- 7112 /SOME/PATH/src/mesos/build/src/.libs/mesos-tests --gtest_filter=ContainerLoggerTest.MesosContainerizerRecover  \--- 7130 (sh) [==========] 1 test from 1 test case ran. (23 ms total) [  PASSED  ] 1 test. [  FAILED  ] 0 tests, listed below:   0 FAILED TESTS {code}  Observered on OS X with clang-trunk and an unoptimized build. ",1
"MESOS-4748","Add Appc image fetcher tests.","Mesos now has support for fetching Appc images. Add tests that verifies the new component.",3
"MESOS-4754","The ""executors"" field is exposed under a backwards incompatible schema.","In 0.26.0, the master's {{/state}} endpoint generated the following:  {code} {   /* ... */   ""frameworks"": [     {       /* ... */       ""executors"": [         {           ""command"": {             ""argv"": [],             ""uris"": [],             ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""           },           ""executor_id"": ""default"",           ""framework_id"": ""0ea528a9-64ba-417f-98ea-9c4b8d418db6-0000"",           ""name"": ""Long Lived Executor (C++)"",           ""resources"": {             ""cpus"": 0,             ""disk"": 0,             ""mem"": 0           },           ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0""         }       ],       /* ... */     }   ]   /* ... */ } {code}  In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:  {code} {   /* ... */   ""frameworks"": [     {       /* ... */       ""executors"": [         {           ""command"": {             ""shell"": true,             ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""           },           ""executor_id"": {             ""value"": ""default""           },           ""framework_id"": {             ""value"": ""368a5a49-480b-41f6-a13b-24a69c92a72e-0000""           },           ""name"": ""Long Lived Executor (C++)"",           ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0"",           ""source"": ""cpp_long_lived_framework""         }       ],       /* ... */     }   ]   /* ... */ } {code}  This is a backwards incompatible API change.",2
"MESOS-4768","MasterMaintenanceTest.InverseOffers is flaky","[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].  Verbose logs from ASF Centos7 build: {code} [ RUN      ] MasterMaintenanceTest.InverseOffers I0224 22:35:53.714018  1948 leveldb.cpp:174] Opened db in 2.034387ms I0224 22:35:53.714663  1948 leveldb.cpp:181] Compacted db in 608839ns I0224 22:35:53.714709  1948 leveldb.cpp:196] Created db iterator in 19043ns I0224 22:35:53.714844  1948 leveldb.cpp:202] Seeked to beginning of db in 2330ns I0224 22:35:53.714956  1948 leveldb.cpp:271] Iterated through 0 keys in the db in 518ns I0224 22:35:53.715092  1948 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0224 22:35:53.715646  1968 recover.cpp:447] Starting replica recovery I0224 22:35:53.715915  1981 recover.cpp:473] Replica is in EMPTY status I0224 22:35:53.717067  1972 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4533)@172.17.0.1:36678 I0224 22:35:53.717445  1981 recover.cpp:193] Received a recover response from a replica in EMPTY status I0224 22:35:53.717888  1978 recover.cpp:564] Updating replica status to STARTING I0224 22:35:53.718585  1979 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 525061ns I0224 22:35:53.718618  1979 replica.cpp:320] Persisted replica status to STARTING I0224 22:35:53.718827  1982 recover.cpp:473] Replica is in STARTING status I0224 22:35:53.719728  1969 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4534)@172.17.0.1:36678 I0224 22:35:53.719974  1971 recover.cpp:193] Received a recover response from a replica in STARTING status I0224 22:35:53.720369  1970 recover.cpp:564] Updating replica status to VOTING I0224 22:35:53.720789  1982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 322308ns I0224 22:35:53.720823  1982 replica.cpp:320] Persisted replica status to VOTING I0224 22:35:53.720968  1982 recover.cpp:578] Successfully joined the Paxos group I0224 22:35:53.721101  1982 recover.cpp:462] Recover process terminated I0224 22:35:53.721698  1982 master.cpp:376] Master aab18b61-7811-4c43-a672-d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678 I0224 22:35:53.721719  1982 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MjbcWP/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MjbcWP/master"" --zk_session_timeout=""10secs"" I0224 22:35:53.722039  1982 master.cpp:425] Master allowing unauthenticated frameworks to register I0224 22:35:53.722053  1982 master.cpp:428] Master only allowing authenticated slaves to register I0224 22:35:53.722061  1982 credentials.hpp:35] Loading credentials for authentication from '/tmp/MjbcWP/credentials' I0224 22:35:53.722394  1982 master.cpp:468] Using default 'crammd5' authenticator I0224 22:35:53.722525  1982 master.cpp:537] Using default 'basic' HTTP authenticator I0224 22:35:53.722661  1982 master.cpp:571] Authorization enabled I0224 22:35:53.722813  1968 hierarchical.cpp:144] Initialized hierarchical allocator process I0224 22:35:53.722846  1980 whitelist_watcher.cpp:77] No whitelist given I0224 22:35:53.724957  1977 master.cpp:1712] The newly elected leader is master@172.17.0.1:36678 with id aab18b61-7811-4c43-a672-d1a63818c880 I0224 22:35:53.725000  1977 master.cpp:1725] Elected as the leading master! I0224 22:35:53.725023  1977 master.cpp:1470] Recovering from registrar I0224 22:35:53.725306  1967 registrar.cpp:307] Recovering registrar I0224 22:35:53.725808  1977 log.cpp:659] Attempting to start the writer I0224 22:35:53.727145  1973 replica.cpp:493] Replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1 I0224 22:35:53.727728  1973 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 424560ns I0224 22:35:53.727828  1973 replica.cpp:342] Persisted promised to 1 I0224 22:35:53.729080  1973 coordinator.cpp:238] Coordinator attempting to fill missing positions I0224 22:35:53.731009  1979 replica.cpp:388] Replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2 I0224 22:35:53.731580  1979 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 478479ns I0224 22:35:53.731613  1979 replica.cpp:712] Persisted action at 0 I0224 22:35:53.734354  1979 replica.cpp:537] Replica received write request for position 0 from (4538)@172.17.0.1:36678 I0224 22:35:53.734485  1979 leveldb.cpp:436] Reading position from leveldb took 60879ns I0224 22:35:53.735877  1979 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.324061ms I0224 22:35:53.735930  1979 replica.cpp:712] Persisted action at 0 I0224 22:35:53.737061  1970 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0 I0224 22:35:53.738881  1970 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.772814ms I0224 22:35:53.738939  1970 replica.cpp:712] Persisted action at 0 I0224 22:35:53.738975  1970 replica.cpp:697] Replica learned NOP action at position 0 I0224 22:35:53.740136  1976 log.cpp:675] Writer started with ending position 0 I0224 22:35:53.741750  1976 leveldb.cpp:436] Reading position from leveldb took 74863ns I0224 22:35:53.743479  1976 registrar.cpp:340] Successfully fetched the registry (0B) in 18.11968ms I0224 22:35:53.743755  1976 registrar.cpp:439] Applied 1 operations in 56670ns; attempting to update the 'registry' I0224 22:35:53.745604  1978 log.cpp:683] Attempting to append 170 bytes to the log I0224 22:35:53.745905  1977 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1 I0224 22:35:53.746968  1981 replica.cpp:537] Replica received write request for position 1 from (4539)@172.17.0.1:36678 I0224 22:35:53.747480  1981 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 456947ns I0224 22:35:53.747609  1981 replica.cpp:712] Persisted action at 1 I0224 22:35:53.750448  1981 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0 I0224 22:35:53.751158  1981 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 535163ns I0224 22:35:53.751258  1981 replica.cpp:712] Persisted action at 1 I0224 22:35:53.751389  1981 replica.cpp:697] Replica learned APPEND action at position 1 I0224 22:35:53.753149  1979 registrar.cpp:484] Successfully updated the 'registry' in 9.228032ms I0224 22:35:53.753324  1979 registrar.cpp:370] Successfully recovered registrar I0224 22:35:53.753593  1979 log.cpp:702] Attempting to truncate the log to 1 I0224 22:35:53.753805  1979 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2 I0224 22:35:53.754055  1981 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register I0224 22:35:53.754349  1979 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover I0224 22:35:53.755764  1977 replica.cpp:537] Replica received write request for position 2 from (4540)@172.17.0.1:36678 I0224 22:35:53.756459  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 488559ns I0224 22:35:53.756561  1977 replica.cpp:712] Persisted action at 2 I0224 22:35:53.757932  1972 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0 I0224 22:35:53.758400  1972 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 343827ns I0224 22:35:53.758539  1972 leveldb.cpp:399] Deleting ~1 keys from leveldb took 34231ns I0224 22:35:53.758658  1972 replica.cpp:712] Persisted action at 2 I0224 22:35:53.758782  1972 replica.cpp:697] Replica learned TRUNCATE action at position 2 I0224 22:35:53.778059  1978 slave.cpp:193] Slave started on 115)@172.17.0.1:36678 I0224 22:35:53.778105  1978 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF"" I0224 22:35:53.778609  1978 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential' I0224 22:35:53.779175  1978 slave.cpp:324] Slave using credential for: test-principal I0224 22:35:53.779520  1978 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000] Trying semicolon-delimited string format instead I0224 22:35:53.780192  1978 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0224 22:35:53.780362  1978 slave.cpp:472] Slave attributes: [  ] I0224 22:35:53.780483  1978 slave.cpp:477] Slave hostname: maintenance-host I0224 22:35:53.782126  1967 state.cpp:58] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta' I0224 22:35:53.782892  1969 status_update_manager.cpp:200] Recovering status update manager I0224 22:35:53.783242  1969 slave.cpp:4565] Finished recovery I0224 22:35:53.784001  1969 slave.cpp:4737] Querying resource estimator for oversubscribable resources I0224 22:35:53.784678  1969 slave.cpp:796] New master detected at master@172.17.0.1:36678 I0224 22:35:53.784874  1967 status_update_manager.cpp:174] Pausing sending status updates I0224 22:35:53.784808  1969 slave.cpp:859] Authenticating with master master@172.17.0.1:36678 I0224 22:35:53.784945  1969 slave.cpp:864] Using default CRAM-MD5 authenticatee I0224 22:35:53.785181  1969 slave.cpp:832] Detecting new master I0224 22:35:53.785326  1969 slave.cpp:4751] Received oversubscribable resources  from the resource estimator I0224 22:35:53.785557  1969 authenticatee.cpp:121] Creating new client SASL connection I0224 22:35:53.786227  1969 master.cpp:5526] Authenticating slave(115)@172.17.0.1:36678 I0224 22:35:53.786492  1969 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(298)@172.17.0.1:36678 I0224 22:35:53.786962  1969 authenticator.cpp:98] Creating new server SASL connection I0224 22:35:53.787274  1969 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0224 22:35:53.787308  1969 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0224 22:35:53.787400  1969 authenticator.cpp:203] Received SASL authentication start I0224 22:35:53.787470  1969 authenticator.cpp:325] Authentication requires more steps I0224 22:35:53.787884  1972 authenticatee.cpp:258] Received SASL authentication step I0224 22:35:53.787992  1972 authenticator.cpp:231] Received SASL authentication step I0224 22:35:53.788027  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0224 22:35:53.788040  1972 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0224 22:35:53.788090  1972 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0224 22:35:53.788122  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0224 22:35:53.788136  1972 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0224 22:35:53.788146  1972 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0224 22:35:53.788164  1972 authenticator.cpp:317] Authentication success I0224 22:35:53.788331  1972 authenticatee.cpp:298] Authentication success I0224 22:35:53.788439  1972 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(115)@172.17.0.1:36678 I0224 22:35:53.788529  1972 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(298)@172.17.0.1:36678 I0224 22:35:53.788988  1972 slave.cpp:927] Successfully authenticated with master master@172.17.0.1:36678 I0224 22:35:53.789139  1972 slave.cpp:1321] Will retry registration in 1.535786ms if necessary I0224 22:35:53.789515  1972 master.cpp:4240] Registering slave at slave(115)@172.17.0.1:36678 (maintenance-host) with id aab18b61-7811-4c43-a672-d1a63818c880-S0 I0224 22:35:53.790577  1972 registrar.cpp:439] Applied 1 operations in 78745ns; attempting to update the 'registry' I0224 22:35:53.791128  1971 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule' I0224 22:35:53.791877  1971 http.cpp:501] HTTP POST for /master/maintenance/schedule from 172.17.0.1:45095 I0224 22:35:53.793313  1972 log.cpp:683] Attempting to append 343 bytes to the log I0224 22:35:53.793586  1972 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3 I0224 22:35:53.794533  1971 replica.cpp:537] Replica received write request for position 3 from (4547)@172.17.0.1:36678 I0224 22:35:53.794862  1971 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 283614ns I0224 22:35:53.794893  1971 replica.cpp:712] Persisted action at 3 I0224 22:35:53.796646  1979 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0 I0224 22:35:53.797102  1972 slave.cpp:1321] Will retry registration in 17.198963ms if necessary I0224 22:35:53.797186  1979 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 498502ns I0224 22:35:53.797230  1979 replica.cpp:712] Persisted action at 3 I0224 22:35:53.797260  1979 replica.cpp:697] Replica learned APPEND action at position 3 I0224 22:35:53.797417  1972 master.cpp:4228] Ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenance-host) as admission is already in progress I0224 22:35:53.799119  1978 registrar.cpp:484] Successfully updated the 'registry' in 8.45824ms I0224 22:35:53.799613  1978 registrar.cpp:439] Applied 1 operations in 176193ns; attempting to update the 'registry' I0224 22:35:53.800472  1972 master.cpp:4308] Registered slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0224 22:35:53.800623  1978 log.cpp:702] Attempting to truncate the log to 3 I0224 22:35:53.801255  1969 hierarchical.cpp:473] Added slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: ) I0224 22:35:53.801301  1978 slave.cpp:971] Registered with master master@172.17.0.1:36678; given slave ID aab18b61-7811-4c43-a672-d1a63818c880-S0 I0224 22:35:53.801331  1978 fetcher.cpp:81] Clearing fetcher cache I0224 22:35:53.801431  1969 hierarchical.cpp:1434] No resources available to allocate! I0224 22:35:53.801466  1969 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 162751ns I0224 22:35:53.801532  1969 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4 I0224 22:35:53.801867  1978 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/slave.info' I0224 22:35:53.801877  1969 status_update_manager.cpp:181] Resuming sending status updates I0224 22:35:53.802898  1977 replica.cpp:537] Replica received write request for position 4 from (4548)@172.17.0.1:36678 I0224 22:35:53.803252  1978 slave.cpp:1030] Forwarding total oversubscribed resources  I0224 22:35:53.803640  1970 master.cpp:4649] Received update of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with total oversubscribed resources  I0224 22:35:53.803858  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 912626ns I0224 22:35:53.803889  1977 replica.cpp:712] Persisted action at 4 I0224 22:35:53.804144  1978 slave.cpp:3482] Received ping from slave-observer(117)@172.17.0.1:36678 I0224 22:35:53.804535  1971 hierarchical.cpp:531] Slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) I0224 22:35:53.804684  1971 hierarchical.cpp:1434] No resources available to allocate! I0224 22:35:53.804714  1971 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 131453ns I0224 22:35:53.805541  1967 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0 I0224 22:35:53.805941  1967 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 366444ns I0224 22:35:53.806015  1967 leveldb.cpp:399] Deleting ~2 keys from leveldb took 42808ns I0224 22:35:53.806041  1967 replica.cpp:712] Persisted action at 4 I0224 22:35:53.806066  1967 replica.cpp:697] Replica learned TRUNCATE action at position 4 I0224 22:35:53.807355  1978 log.cpp:683] Attempting to append 465 bytes to the log I0224 22:35:53.807551  1978 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5 I0224 22:35:53.809638  1979 replica.cpp:537] Replica received write request for position 5 from (4549)@172.17.0.1:36678 I0224 22:35:53.810858  1979 leveldb.cpp:341] Persisting action (484 bytes) to leveldb took 1.167663ms I0224 22:35:53.810904  1979 replica.cpp:712] Persisted action at 5 I0224 22:35:53.811997  1979 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0 I0224 22:35:53.812348  1979 leveldb.cpp:341] Persisting action (486 bytes) to leveldb took 318928ns I0224 22:35:53.812376  1979 replica.cpp:712] Persisted action at 5 I0224 22:35:53.812397  1979 replica.cpp:697] Replica learned APPEND action at position 5 I0224 22:35:53.815132  1973 registrar.cpp:484] Successfully updated the 'registry' in 15.437312ms I0224 22:35:53.815491  1976 log.cpp:702] Attempting to truncate the log to 5 I0224 22:35:53.815610  1973 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6 I0224 22:35:53.815661  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks I0224 22:35:53.815845  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks I0224 22:35:53.816069  1975 hierarchical.cpp:1434] No resources available to allocate! I0224 22:35:53.816103  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 175822ns I0224 22:35:53.816272  1975 hierarchical.cpp:1434] No resources available to allocate! I0224 22:35:53.816303  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 110913ns I0224 22:35:53.817291  1972 replica.cpp:537] Replica received write request for position 6 from (4550)@172.17.0.1:36678 I0224 22:35:53.817908  1972 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 576032ns I0224 22:35:53.817932  1972 replica.cpp:712] Persisted action at 6 I0224 22:35:53.818686  1980 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0 I0224 22:35:53.819021  1980 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 305298ns I0224 22:35:53.819095  1980 leveldb.cpp:399] Deleting ~2 keys from leveldb took 44332ns I0224 22:35:53.819120  1980 replica.cpp:712] Persisted action at 6 I0224 22:35:53.819162  1980 replica.cpp:697] Replica learned TRUNCATE action at position 6 I0224 22:35:53.820662  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/status' I0224 22:35:53.821190  1976 http.cpp:501] HTTP GET for /master/maintenance/status from 172.17.0.1:45096 I0224 22:35:53.823709  1948 scheduler.cpp:154] Version: 0.28.0 I0224 22:35:53.824424  1972 scheduler.cpp:236] New master detected at master@172.17.0.1:36678 I0224 22:35:53.825402  1982 scheduler.cpp:298] Sending SUBSCRIBE call to master@172.17.0.1:36678 I0224 22:35:53.827201  1978 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I0224 22:35:53.827636  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45097 I0224 22:35:53.827922  1978 master.cpp:1974] Received subscription request for HTTP framework 'default' I0224 22:35:53.827991  1978 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*' I0224 22:35:53.828418  1982 master.cpp:2065] Subscribing framework 'default' with checkpointing disabled and capabilities [  ] I0224 22:35:53.828943  1968 hierarchical.cpp:265] Added framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.829124  1982 master.hpp:1657] Sending heartbeat to aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.829987  1968 hierarchical.cpp:1127] Performed allocation for 1 slaves in 1.011356ms I0224 22:35:53.830204  1982 master.cpp:5355] Sending 1 offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) I0224 22:35:53.830801  1982 master.cpp:5445] Sending 1 inverse offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) I0224 22:35:53.831132  1969 scheduler.cpp:457] Enqueuing event SUBSCRIBED received from master@172.17.0.1:36678 I0224 22:35:53.832396  1968 scheduler.cpp:457] Enqueuing event HEARTBEAT received from master@172.17.0.1:36678 I0224 22:35:53.833050  1976 master_maintenance_tests.cpp:177] Ignoring HEARTBEAT event I0224 22:35:53.833256  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678 I0224 22:35:53.833775  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678 I0224 22:35:53.835662  1980 scheduler.cpp:298] Sending ACCEPT call to master@172.17.0.1:36678 I0224 22:35:53.837591  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I0224 22:35:53.838021  1967 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45098 I0224 22:35:53.838851  1967 master.cpp:3138] Processing ACCEPT call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O0 ] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) I0224 22:35:53.838946  1967 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 as user 'mesos' W0224 22:35:53.841048  1967 validation.cpp:404] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W0224 22:35:53.841101  1967 validation.cpp:416] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I0224 22:35:53.841624  1967 master.hpp:176] Adding task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) I0224 22:35:53.842157  1967 master.cpp:3623] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) I0224 22:35:53.842571  1980 slave.cpp:1361] Got assigned task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.843122  1980 slave.cpp:1480] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.843718  1980 paths.cpp:474] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' to user 'mesos' I0224 22:35:53.852052  1980 slave.cpp:5367] Launching executor default of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' I0224 22:35:53.854452  1980 exec.cpp:143] Version: 0.28.0 I0224 22:35:53.854812  1967 exec.cpp:193] Executor started at: executor(47)@172.17.0.1:36678 with pid 1948 I0224 22:35:53.855108  1980 slave.cpp:1698] Queuing task '90bcae0c-9d40-40b7-9537-dae7e83479f6' for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.855264  1980 slave.cpp:749] Successfully attached file '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' I0224 22:35:53.855362  1980 slave.cpp:2643] Got registration for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678 I0224 22:35:53.855785  1974 exec.cpp:217] Executor registered on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 I0224 22:35:53.855857  1974 exec.cpp:229] Executor::registered took 42512ns I0224 22:35:53.856391  1980 slave.cpp:1863] Sending queued task '90bcae0c-9d40-40b7-9537-dae7e83479f6' to executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678 I0224 22:35:53.856720  1974 exec.cpp:304] Executor asked to run task '90bcae0c-9d40-40b7-9537-dae7e83479f6' I0224 22:35:53.856812  1974 exec.cpp:313] Executor::launchTask took 65703ns I0224 22:35:53.856922  1974 exec.cpp:526] Executor sending status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.857378  1980 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678 I0224 22:35:53.858175  1980 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.858222  1980 status_update_manager.cpp:497] Creating StatusUpdate stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.858687  1980 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave I0224 22:35:53.859210  1980 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678 I0224 22:35:53.859390  1980 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.859436  1980 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to executor(47)@172.17.0.1:36678 I0224 22:35:53.859663  1980 exec.cpp:350] Executor received status update acknowledgement 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.859657  1967 master.cpp:4794] Status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) I0224 22:35:53.859851  1967 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.860587  1967 master.cpp:6450] Updating the state of task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING) I0224 22:35:53.862711  1967 scheduler.cpp:457] Enqueuing event UPDATE received from master@172.17.0.1:36678 I0224 22:35:53.866711  1976 scheduler.cpp:298] Sending ACKNOWLEDGE call to master@172.17.0.1:36678 I0224 22:35:53.870667  1972 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I0224 22:35:53.871269  1972 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45099 I0224 22:35:53.871459  1972 master.cpp:3952] Processing ACKNOWLEDGE call 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 I0224 22:35:53.872184  1972 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.872537  1972 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:35:53.874407  1975 scheduler.cpp:298] Sending DECLINE call to master@172.17.0.1:36678 I0224 22:35:53.877537  1979 hierarchical.cpp:1434] No resources available to allocate! I0224 22:35:53.877795  1979 hierarchical.cpp:1127] Performed allocation for 1 slaves in 482441ns I0224 22:35:53.878082  1981 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler' I0224 22:35:53.878675  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45100 I0224 22:35:53.878931  1978 master.cpp:3675] Processing DECLINE call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O1 ] for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) ../../src/tests/master_maintenance_tests.cpp:1222: Failure Failed to wait 15secs for event I0224 22:36:08.881649  1948 master.cpp:1027] Master terminating W0224 22:36:08.881925  1948 master.cpp:6502] Removing task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) in non-terminal state TASK_RUNNING I0224 22:36:08.882961  1948 master.cpp:6545] Removing executor 'default' with resources  of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) I0224 22:36:08.884789  1969 hierarchical.cpp:505] Removed slave aab18b61-7811-4c43-a672-d1a63818c880-S0 I0224 22:36:08.887261  1969 hierarchical.cpp:326] Removed framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:36:08.916983  1976 slave.cpp:3528] master@172.17.0.1:36678 exited W0224 22:36:08.917191  1976 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected I0224 22:36:08.934546  1975 slave.cpp:3528] executor(47)@172.17.0.1:36678 exited I0224 22:36:08.934806  1974 slave.cpp:3886] Executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 exited with status 0 I0224 22:36:08.935024  1974 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from @0.0.0.0:0 I0224 22:36:08.935505  1974 slave.cpp:5677] Terminating task 90bcae0c-9d40-40b7-9537-dae7e83479f6 I0224 22:36:08.936190  1967 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:36:08.936368  1967 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave I0224 22:36:08.936606  1974 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678 I0224 22:36:08.936779  1974 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:36:08.955370  1967 slave.cpp:668] Slave terminating I0224 22:36:08.955499  1967 slave.cpp:2079] Asked to shut down framework aab18b61-7811-4c43-a672-d1a63818c880-0000 by @0.0.0.0:0 I0224 22:36:08.955538  1967 slave.cpp:2104] Shutting down framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:36:08.955606  1967 slave.cpp:3990] Cleaning up executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678 I0224 22:36:08.956053  1967 slave.cpp:4078] Cleaning up framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:36:08.956327  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' for gc 1.00002336880296weeks in the future I0224 22:36:08.956495  1973 status_update_manager.cpp:282] Closing status update streams for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:36:08.956524  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default' for gc 1.00002336880296weeks in the future I0224 22:36:08.956549  1973 status_update_manager.cpp:528] Cleaning up status update stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 I0224 22:36:08.956619  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000' for gc 1.00002336880296weeks in the future [  FAILED  ] MasterMaintenanceTest.InverseOffers (15258 ms) {code}",1
"MESOS-4783","Disable rate limiting of the global metrics endpoint for mesos-tests execution","Once we can optionally disable rate limiting in the global metrics endpoint with MESOS-4776 we should disable the rate limiting during the execution of mesos-tests.  * rate limiting makes it cumbersome to repeatedly hit the endpoint since one would not want to interfere with the rate limiting * rate limiting might incur additional wait time which might slown down tests",3
"MESOS-4784","SlaveTest.MetricsSlaveLaunchErrors test relies on implicit blocking behavior hitting the global metrics endpoint","The test attempts to observe a change in the {{slave/container_launch_errors}} metric, but does not wait for the triggering action to take place. Currently the test passes since hitting the endpoint blocks for some rate limit-related time which provides under many circumstances enough wait time for the action to take place. ",1
"MESOS-4807","IOTest.BufferedRead writes to the current directory","libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,  * should the test fail data might be leaked to random locations, * the test cannot be executed from a write-only directory, or * executing the same test in parallel would race on the existence of the created file, and show bogus behavior.  The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",1
"MESOS-4820","Need to set `EXPOSED` ports from docker images into `ContainerConfig`","Most docker images have an `EXPOSE` command associated with them. This tells the container run-time the TCP ports that the micro-service ""wishes"" to expose to the outside world.   With the `Unified containerizer` project since `MesosContainerizer` is going to natively support docker images it is imperative that the Mesos container run time have a mechanism to expose ports listed in a Docker image. The first step to achieve this is to extract this information from the `Docker` image and set in the `ContainerConfig` . The `ContainerConfig` can then be used to pass this information to any isolator (for e.g. `network/cni` isolator) that will install port forwarding rules to expose the desired ports.",1
"MESOS-4822","Add support for local image fetching in Appc provisioner.","Currently Appc image provisioner supports http(s) fetching. It would be valuable to add support for local file path(URI) based  fetching.",2
"MESOS-4824","""filesystem/linux"" isolator does not unmount orphaned persistent volumes","A persistent volume can be orphaned when: # A framework registers with checkpointing enabled. # The framework starts a task + a persistent volume. # The agent exits.  The task continues to run. # Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent. # The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.  The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}})  {code} I0229 23:55:42.078940  5635 linux.cpp:711] Ignoring cleanup request for unknown container: a35189d3-85d5-4d02-b568-67f675b6dc97 {code}  Test implemented here: https://reviews.apache.org/r/44122/",2
"MESOS-4825","Master's slave reregister logic does not update version field","The master's logic for reregistering a slave does not update the version field if the slave re-registers with a new version.",1
"MESOS-4832","DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes exits when the /tmp directory is bind-mounted","If the {{/tmp}} directory (where Mesos tests create temporary directories) is a bind mount, the test suite will exit here: {code} [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes I0226 03:17:26.722806  1097 leveldb.cpp:174] Opened db in 12.587676ms I0226 03:17:26.723496  1097 leveldb.cpp:181] Compacted db in 636999ns I0226 03:17:26.723536  1097 leveldb.cpp:196] Created db iterator in 18271ns I0226 03:17:26.723547  1097 leveldb.cpp:202] Seeked to beginning of db in 1555ns I0226 03:17:26.723554  1097 leveldb.cpp:271] Iterated through 0 keys in the db in 363ns I0226 03:17:26.723593  1097 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0226 03:17:26.724128  1117 recover.cpp:447] Starting replica recovery I0226 03:17:26.724367  1117 recover.cpp:473] Replica is in EMPTY status I0226 03:17:26.725237  1117 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13810)@172.30.2.151:51934 I0226 03:17:26.725744  1114 recover.cpp:193] Received a recover response from a replica in EMPTY status I0226 03:17:26.726356  1111 master.cpp:376] Master 5cc57c0e-f1ad-4107-893f-420ed1a1db1a (ip-172-30-2-151.mesosphere.io) started on 172.30.2.151:51934 I0226 03:17:26.726369  1118 recover.cpp:564] Updating replica status to STARTING I0226 03:17:26.726378  1111 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/djHTVQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/djHTVQ/master"" --zk_session_timeout=""10secs"" I0226 03:17:26.726605  1111 master.cpp:423] Master only allowing authenticated frameworks to register I0226 03:17:26.726616  1111 master.cpp:428] Master only allowing authenticated slaves to register I0226 03:17:26.726632  1111 credentials.hpp:35] Loading credentials for authentication from '/tmp/djHTVQ/credentials' I0226 03:17:26.726860  1111 master.cpp:468] Using default 'crammd5' authenticator I0226 03:17:26.726977  1111 master.cpp:537] Using default 'basic' HTTP authenticator I0226 03:17:26.727092  1111 master.cpp:571] Authorization enabled I0226 03:17:26.727243  1118 hierarchical.cpp:144] Initialized hierarchical allocator process I0226 03:17:26.727285  1116 whitelist_watcher.cpp:77] No whitelist given I0226 03:17:26.728852  1114 master.cpp:1712] The newly elected leader is master@172.30.2.151:51934 with id 5cc57c0e-f1ad-4107-893f-420ed1a1db1a I0226 03:17:26.728876  1114 master.cpp:1725] Elected as the leading master! I0226 03:17:26.728891  1114 master.cpp:1470] Recovering from registrar I0226 03:17:26.728977  1117 registrar.cpp:307] Recovering registrar I0226 03:17:26.731503  1112 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 4.977811ms I0226 03:17:26.731539  1112 replica.cpp:320] Persisted replica status to STARTING I0226 03:17:26.731711  1111 recover.cpp:473] Replica is in STARTING status I0226 03:17:26.732501  1114 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13812)@172.30.2.151:51934 I0226 03:17:26.732862  1111 recover.cpp:193] Received a recover response from a replica in STARTING status I0226 03:17:26.733264  1117 recover.cpp:564] Updating replica status to VOTING I0226 03:17:26.733836  1118 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 388246ns I0226 03:17:26.733855  1118 replica.cpp:320] Persisted replica status to VOTING I0226 03:17:26.733979  1113 recover.cpp:578] Successfully joined the Paxos group I0226 03:17:26.734149  1113 recover.cpp:462] Recover process terminated I0226 03:17:26.734478  1111 log.cpp:659] Attempting to start the writer I0226 03:17:26.735523  1114 replica.cpp:493] Replica received implicit promise request from (13813)@172.30.2.151:51934 with proposal 1 I0226 03:17:26.736130  1114 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 576451ns I0226 03:17:26.736150  1114 replica.cpp:342] Persisted promised to 1 I0226 03:17:26.736709  1115 coordinator.cpp:238] Coordinator attempting to fill missing positions I0226 03:17:26.737771  1114 replica.cpp:388] Replica received explicit promise request from (13814)@172.30.2.151:51934 for position 0 with proposal 2 I0226 03:17:26.738386  1114 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 583184ns I0226 03:17:26.738404  1114 replica.cpp:712] Persisted action at 0 I0226 03:17:26.739312  1118 replica.cpp:537] Replica received write request for position 0 from (13815)@172.30.2.151:51934 I0226 03:17:26.739367  1118 leveldb.cpp:436] Reading position from leveldb took 26157ns I0226 03:17:26.740638  1118 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.238477ms I0226 03:17:26.740669  1118 replica.cpp:712] Persisted action at 0 I0226 03:17:26.741158  1118 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0 I0226 03:17:26.742878  1118 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.697254ms I0226 03:17:26.742902  1118 replica.cpp:712] Persisted action at 0 I0226 03:17:26.742916  1118 replica.cpp:697] Replica learned NOP action at position 0 I0226 03:17:26.743393  1117 log.cpp:675] Writer started with ending position 0 I0226 03:17:26.744370  1112 leveldb.cpp:436] Reading position from leveldb took 34329ns I0226 03:17:26.745240  1117 registrar.cpp:340] Successfully fetched the registry (0B) in 16.21888ms I0226 03:17:26.745350  1117 registrar.cpp:439] Applied 1 operations in 30460ns; attempting to update the 'registry' I0226 03:17:26.746016  1111 log.cpp:683] Attempting to append 210 bytes to the log I0226 03:17:26.746119  1116 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1 I0226 03:17:26.746798  1114 replica.cpp:537] Replica received write request for position 1 from (13816)@172.30.2.151:51934 I0226 03:17:26.747251  1114 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 411333ns I0226 03:17:26.747269  1114 replica.cpp:712] Persisted action at 1 I0226 03:17:26.747808  1113 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0 I0226 03:17:26.749511  1113 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.673488ms I0226 03:17:26.749534  1113 replica.cpp:712] Persisted action at 1 I0226 03:17:26.749550  1113 replica.cpp:697] Replica learned APPEND action at position 1 I0226 03:17:26.750422  1111 registrar.cpp:484] Successfully updated the 'registry' in 5.021952ms I0226 03:17:26.750560  1111 registrar.cpp:370] Successfully recovered registrar I0226 03:17:26.750635  1112 log.cpp:702] Attempting to truncate the log to 1 I0226 03:17:26.750751  1113 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2 I0226 03:17:26.751096  1116 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register I0226 03:17:26.751126  1111 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover I0226 03:17:26.751561  1118 replica.cpp:537] Replica received write request for position 2 from (13817)@172.30.2.151:51934 I0226 03:17:26.751999  1118 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406823ns I0226 03:17:26.752018  1118 replica.cpp:712] Persisted action at 2 I0226 03:17:26.752521  1113 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0 I0226 03:17:26.754161  1113 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.614888ms I0226 03:17:26.754210  1113 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26384ns I0226 03:17:26.754225  1113 replica.cpp:712] Persisted action at 2 I0226 03:17:26.754240  1113 replica.cpp:697] Replica learned TRUNCATE action at position 2 I0226 03:17:26.765103  1115 slave.cpp:193] Slave started on 399)@172.30.2.151:51934 I0226 03:17:26.765130  1115 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP"" I0226 03:17:26.765403  1115 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential' I0226 03:17:26.765573  1115 slave.cpp:324] Slave using credential for: test-principal I0226 03:17:26.765733  1115 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048 Trying semicolon-delimited string format instead I0226 03:17:26.766185  1115 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] I0226 03:17:26.766242  1115 slave.cpp:472] Slave attributes: [  ] I0226 03:17:26.766250  1115 slave.cpp:477] Slave hostname: ip-172-30-2-151.mesosphere.io I0226 03:17:26.767325  1097 sched.cpp:222] Version: 0.28.0 I0226 03:17:26.767390  1111 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta' I0226 03:17:26.767603  1115 status_update_manager.cpp:200] Recovering status update manager I0226 03:17:26.767865  1113 docker.cpp:726] Recovering Docker containers I0226 03:17:26.767971  1111 sched.cpp:326] New master detected at master@172.30.2.151:51934 I0226 03:17:26.768045  1111 sched.cpp:382] Authenticating with master master@172.30.2.151:51934 I0226 03:17:26.768059  1111 sched.cpp:389] Using default CRAM-MD5 authenticatee I0226 03:17:26.768070  1118 slave.cpp:4565] Finished recovery I0226 03:17:26.768273  1112 authenticatee.cpp:121] Creating new client SASL connection I0226 03:17:26.768435  1118 slave.cpp:4737] Querying resource estimator for oversubscribable resources I0226 03:17:26.768565  1111 master.cpp:5526] Authenticating scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 I0226 03:17:26.768661  1118 slave.cpp:796] New master detected at master@172.30.2.151:51934 I0226 03:17:26.768659  1115 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(839)@172.30.2.151:51934 I0226 03:17:26.768679  1113 status_update_manager.cpp:174] Pausing sending status updates I0226 03:17:26.768728  1118 slave.cpp:859] Authenticating with master master@172.30.2.151:51934 I0226 03:17:26.768743  1118 slave.cpp:864] Using default CRAM-MD5 authenticatee I0226 03:17:26.768865  1118 slave.cpp:832] Detecting new master I0226 03:17:26.768868  1112 authenticator.cpp:98] Creating new server SASL connection I0226 03:17:26.768908  1114 authenticatee.cpp:121] Creating new client SASL connection I0226 03:17:26.769003  1118 slave.cpp:4751] Received oversubscribable resources  from the resource estimator I0226 03:17:26.769103  1115 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0226 03:17:26.769131  1115 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0226 03:17:26.769209  1116 master.cpp:5526] Authenticating slave(399)@172.30.2.151:51934 I0226 03:17:26.769253  1114 authenticator.cpp:203] Received SASL authentication start I0226 03:17:26.769295  1115 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(840)@172.30.2.151:51934 I0226 03:17:26.769307  1114 authenticator.cpp:325] Authentication requires more steps I0226 03:17:26.769403  1117 authenticatee.cpp:258] Received SASL authentication step I0226 03:17:26.769495  1114 authenticator.cpp:98] Creating new server SASL connection I0226 03:17:26.769531  1115 authenticator.cpp:231] Received SASL authentication step I0226 03:17:26.769554  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0226 03:17:26.769562  1115 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0226 03:17:26.769608  1115 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0226 03:17:26.769629  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0226 03:17:26.769637  1115 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0226 03:17:26.769642  1115 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0226 03:17:26.769654  1115 authenticator.cpp:317] Authentication success I0226 03:17:26.769728  1117 authenticatee.cpp:298] Authentication success I0226 03:17:26.769769  1112 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0226 03:17:26.769767  1118 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 I0226 03:17:26.769803  1112 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0226 03:17:26.769798  1114 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(839)@172.30.2.151:51934 I0226 03:17:26.769881  1112 authenticator.cpp:203] Received SASL authentication start I0226 03:17:26.769932  1112 authenticator.cpp:325] Authentication requires more steps I0226 03:17:26.769981  1117 sched.cpp:471] Successfully authenticated with master master@172.30.2.151:51934 I0226 03:17:26.770004  1117 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.151:51934 I0226 03:17:26.770064  1118 authenticatee.cpp:258] Received SASL authentication step I0226 03:17:26.770102  1117 sched.cpp:809] Will retry registration in 1.937819802secs if necessary I0226 03:17:26.770165  1115 authenticator.cpp:231] Received SASL authentication step I0226 03:17:26.770193  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0226 03:17:26.770207  1115 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0226 03:17:26.770213  1116 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 I0226 03:17:26.770241  1115 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0226 03:17:26.770274  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0226 03:17:26.770277  1116 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1' I0226 03:17:26.770298  1115 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0226 03:17:26.770331  1115 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0226 03:17:26.770349  1115 authenticator.cpp:317] Authentication success I0226 03:17:26.770428  1118 authenticatee.cpp:298] Authentication success I0226 03:17:26.770442  1116 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(399)@172.30.2.151:51934 I0226 03:17:26.770547  1116 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(840)@172.30.2.151:51934 I0226 03:17:26.770846  1116 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ] I0226 03:17:26.770866  1118 slave.cpp:927] Successfully authenticated with master master@172.30.2.151:51934 I0226 03:17:26.770966  1118 slave.cpp:1321] Will retry registration in 1.453415ms if necessary I0226 03:17:26.771225  1115 hierarchical.cpp:265] Added framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:26.771275  1118 sched.cpp:703] Framework registered with 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:26.771299  1115 hierarchical.cpp:1434] No resources available to allocate! I0226 03:17:26.771328  1115 hierarchical.cpp:1529] No inverse offers to send out! I0226 03:17:26.771344  1118 sched.cpp:717] Scheduler::registered took 50146ns I0226 03:17:26.771356  1116 master.cpp:4240] Registering slave at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) with id 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 I0226 03:17:26.771348  1115 hierarchical.cpp:1127] Performed allocation for 0 slaves in 101438ns I0226 03:17:26.771860  1114 registrar.cpp:439] Applied 1 operations in 59672ns; attempting to update the 'registry' I0226 03:17:26.772645  1117 log.cpp:683] Attempting to append 423 bytes to the log I0226 03:17:26.772758  1112 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3 I0226 03:17:26.773435  1117 replica.cpp:537] Replica received write request for position 3 from (13824)@172.30.2.151:51934 I0226 03:17:26.773586  1111 slave.cpp:1321] Will retry registration in 2.74261ms if necessary I0226 03:17:26.773682  1115 master.cpp:4228] Ignoring register slave message from slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) as admission is already in progress I0226 03:17:26.773937  1117 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 469969ns I0226 03:17:26.773957  1117 replica.cpp:712] Persisted action at 3 I0226 03:17:26.774605  1114 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0 I0226 03:17:26.775961  1114 leveldb.cpp:341] Persisting action (444 bytes) to leveldb took 1.329435ms I0226 03:17:26.775986  1114 replica.cpp:712] Persisted action at 3 I0226 03:17:26.776008  1114 replica.cpp:697] Replica learned APPEND action at position 3 I0226 03:17:26.777228  1115 slave.cpp:1321] Will retry registration in 41.5608ms if necessary I0226 03:17:26.777300  1112 registrar.cpp:484] Successfully updated the 'registry' in 5.378048ms I0226 03:17:26.777361  1114 master.cpp:4228] Ignoring register slave message from slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) as admission is already in progress I0226 03:17:26.777505  1113 log.cpp:702] Attempting to truncate the log to 3 I0226 03:17:26.777616  1111 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4 I0226 03:17:26.778062  1114 slave.cpp:3482] Received ping from slave-observer(369)@172.30.2.151:51934 I0226 03:17:26.778139  1118 master.cpp:4308] Registered slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] I0226 03:17:26.778213  1113 replica.cpp:537] Replica received write request for position 4 from (13825)@172.30.2.151:51934 I0226 03:17:26.778291  1114 slave.cpp:971] Registered with master master@172.30.2.151:51934; given slave ID 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 I0226 03:17:26.778316  1114 fetcher.cpp:81] Clearing fetcher cache I0226 03:17:26.778367  1116 hierarchical.cpp:473] Added slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 (ip-172-30-2-151.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] (allocated: ) I0226 03:17:26.778447  1117 status_update_manager.cpp:181] Resuming sending status updates I0226 03:17:26.778617  1113 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 375414ns I0226 03:17:26.778635  1113 replica.cpp:712] Persisted action at 4 I0226 03:17:26.778650  1114 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/slave.info' I0226 03:17:26.778900  1114 slave.cpp:1030] Forwarding total oversubscribed resources  I0226 03:17:26.779109  1114 master.cpp:4649] Received update of slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) with total oversubscribed resources  I0226 03:17:26.779139  1112 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0 I0226 03:17:26.779331  1116 hierarchical.cpp:1529] No inverse offers to send out! I0226 03:17:26.779369  1116 hierarchical.cpp:1147] Performed allocation for slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 in 969593ns I0226 03:17:26.779645  1113 master.cpp:5355] Sending 1 offers to framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 I0226 03:17:26.779700  1116 hierarchical.cpp:531] Slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 (ip-172-30-2-151.mesosphere.io) updated with oversubscribed resources  (total: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000], allocated: disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000]) I0226 03:17:26.779819  1116 hierarchical.cpp:1434] No resources available to allocate! I0226 03:17:26.779847  1116 hierarchical.cpp:1529] No inverse offers to send out! I0226 03:17:26.779865  1116 hierarchical.cpp:1147] Performed allocation for slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 in 133437ns I0226 03:17:26.780025  1118 sched.cpp:873] Scheduler::resourceOffers took 102165ns I0226 03:17:26.780372  1097 resources.cpp:576] Parsing resources as JSON failed: cpus:1;mem:64; Trying semicolon-delimited string format instead I0226 03:17:26.780882  1112 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.715066ms I0226 03:17:26.780938  1112 leveldb.cpp:399] Deleting ~2 keys from leveldb took 32370ns I0226 03:17:26.780953  1112 replica.cpp:712] Persisted action at 4 I0226 03:17:26.780971  1112 replica.cpp:697] Replica learned TRUNCATE action at position 4 I0226 03:17:26.781693  1117 master.cpp:3138] Processing ACCEPT call for offers: [ 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-O0 ] on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) for framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 I0226 03:17:26.781731  1117 master.cpp:2926] Authorizing principal 'test-principal' to create volumes I0226 03:17:26.781801  1117 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root' I0226 03:17:26.782827  1114 master.cpp:3467] Applying CREATE operation for volumes disk(role1)[id1:path1]:64 from framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 to slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) I0226 03:17:26.783136  1114 master.cpp:6589] Sending checkpointed resources disk(role1)[id1:path1]:64 to slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) I0226 03:17:26.783641  1111 slave.cpp:2341] Updated checkpointed resources from  to disk(role1)[id1:path1]:64 I0226 03:17:26.783911  1114 master.hpp:176] Adding task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 (ip-172-30-2-151.mesosphere.io) I0226 03:17:26.784056  1114 master.cpp:3623] Launching task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) I0226 03:17:26.784397  1115 slave.cpp:1361] Got assigned task 1 for framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:26.784557  1115 slave.cpp:5287] Checkpointing FrameworkInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/framework.info' I0226 03:17:26.784739  1116 hierarchical.cpp:653] Updated allocation of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 from disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000] to disk(role1):1984; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64 I0226 03:17:26.784848  1115 slave.cpp:5298] Checkpointing framework pid 'scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/framework.pid' I0226 03:17:26.785078  1115 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32 Trying semicolon-delimited string format instead I0226 03:17:26.785322  1116 hierarchical.cpp:892] Recovered disk(role1):1984; cpu(*):2; mem(*):1984; cpus(*):7; ports(*):[31000-32000] (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: disk(role1)[id1:path1]:64; cpus(*):1; mem(*):64) on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 from framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:26.785658  1115 slave.cpp:1480] Launching task 1 for framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:26.785719  1115 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32 Trying semicolon-delimited string format instead I0226 03:17:26.786197  1115 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' to user 'root' I0226 03:17:26.791122  1115 slave.cpp:5739] Checkpointing ExecutorInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/executor.info' I0226 03:17:26.791543  1115 slave.cpp:5367] Launching executor 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' I0226 03:17:26.792325  1115 slave.cpp:5762] Checkpointing TaskInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/tasks/1/task.info' I0226 03:17:26.794337  1115 slave.cpp:1698] Queuing task '1' for executor '1' of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:26.794478  1115 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' I0226 03:17:26.797106  1116 docker.cpp:1023] Starting container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' for task '1' (and executor '1') of framework '5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000' I0226 03:17:26.797462  1116 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest I0226 03:17:26.910549  1111 docker.cpp:394] Docker pull alpine completed I0226 03:17:26.910800  1111 docker.cpp:483] Changing the ownership of the persistent volume at '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/volumes/roles/role1/id1' with uid 0 and gid 0 I0226 03:17:26.915712  1111 docker.cpp:504] Mounting '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/volumes/roles/role1/id1' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/path1' for persistent volume disk(role1)[id1:path1]:64 of container bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c I0226 03:17:26.919000  1117 docker.cpp:576] Checkpointing pid 9568 to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/pids/forked.pid' I0226 03:17:26.974776  1114 slave.cpp:2643] Got registration for executor '1' of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 from executor(1)@172.30.2.151:46052 I0226 03:17:26.975217  1114 slave.cpp:2729] Checkpointing executor pid 'executor(1)@172.30.2.151:46052' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/pids/libprocess.pid' I0226 03:17:26.976177  1113 docker.cpp:1303] Ignoring updating container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' with resources passed to update is identical to existing resources I0226 03:17:26.976492  1115 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 at executor(1)@172.30.2.151:46052 I0226 03:17:27.691769  1111 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 from executor(1)@172.30.2.151:46052 I0226 03:17:27.692291  1116 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:27.692327  1116 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:27.692773  1116 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:27.700090  1116 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 to the slave I0226 03:17:27.700389  1113 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 to master@172.30.2.151:51934 I0226 03:17:27.700606  1113 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:27.700644  1113 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 to executor(1)@172.30.2.151:46052 I0226 03:17:27.700742  1117 master.cpp:4794] Status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 from slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) I0226 03:17:27.700775  1117 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:27.700923  1117 master.cpp:6450] Updating the state of task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING) I0226 03:17:27.701145  1118 sched.cpp:981] Scheduler::statusUpdate took 107222ns I0226 03:17:27.701550  1112 master.cpp:3952] Processing ACKNOWLEDGE call 9f75a4e5-9ff4-4ca9-8623-8b2574796229 for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 I0226 03:17:27.701828  1114 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:27.701962  1114 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 I0226 03:17:27.701987  1112 slave.cpp:668] Slave terminating I0226 03:17:27.702256  1117 master.cpp:1174] Slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) disconnected I0226 03:17:27.702275  1117 master.cpp:2635] Disconnecting slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) I0226 03:17:27.702335  1117 master.cpp:2654] Deactivating slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) I0226 03:17:27.702492  1111 hierarchical.cpp:560] Slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 deactivated I0226 03:17:27.707713  1115 slave.cpp:193] Slave started on 400)@172.30.2.151:51934 I0226 03:17:27.707739  1115 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP"" I0226 03:17:27.708133  1115 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential' I0226 03:17:27.708282  1115 slave.cpp:324] Slave using credential for: test-principal I0226 03:17:27.708407  1115 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048 Trying semicolon-delimited string format instead I0226 03:17:27.708874  1115 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] I0226 03:17:27.708931  1115 slave.cpp:472] Slave attributes: [  ] I0226 03:17:27.708941  1115 slave.cpp:477] Slave hostname: ip-172-30-2-151.mesosphere.io I0226 03:17:27.710033  1113 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta' I0226 03:17:27.711252  1114 fetcher.cpp:81] Clearing fetcher cache I0226 03:17:27.711447  1116 status_update_manager.cpp:200] Recovering status update manager I0226 03:17:27.711727  1111 docker.cpp:726] Recovering Docker containers I0226 03:17:27.711839  1111 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a I0226 03:17:27.728170  1117 hierarchical.cpp:1434] No resources available to allocate! I0226 03:17:27.728235  1117 hierarchical.cpp:1529] No inverse offers to send out! I0226 03:17:27.728268  1117 hierarchical.cpp:1127] Performed allocation for 1 slaves in 296715ns I0226 03:17:27.817551  1113 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0.bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c I0226 03:17:27.923014  1112 docker.cpp:932] Checking if Docker container named '/mesos-5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0.bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' was started by Mesos I0226 03:17:27.923071  1112 docker.cpp:942] Checking if Mesos container with ID 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' has been orphaned I0226 03:17:27.923122  1112 docker.cpp:678] Running docker -H unix:///var/run/docker.sock stop -t 0 0a10ad8641f8e85227324a979817933322dc901706cb4430eab0bcaf979835d1 I0226 03:17:28.023885  1116 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -v 0a10ad8641f8e85227324a979817933322dc901706cb4430eab0bcaf979835d1  I0226 03:17:28.127876  1114 docker.cpp:912] Unmounting volume for container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' ../../3rdparty/libprocess/include/process/gmock.hpp:214: ERROR: this mock object (used in test DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes) should be deleted but never is. Its address is @0x5781dd8. I0226 03:17:28.127957  1114 docker.cpp:912] Unmounting volume for container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' ../../src/tests/mesos.cpp:673: ERROR: this mock object (used in test DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes) should be deleted but never is. Its address is @0x5a03260. ../../src/tests/mesos.hpp:1357: ERROR: this mock object (used in test DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes) should be deleted but never is. Its address is @0x5b477c0. Failed to perform recovery: Unable to unmount volumes for Docker container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c': Failed to unmount volume '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/path1': Failed to unmount '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/path1': Invalid argument ../../src/tests/containerizer/docker_containerizer_tests.cpp:1650: ERROR: this mock object (used in test DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes) should be deleted but never is. Its address is @0x7ffe75a8d310. To remedy this do as follows: ERROR: 4 leaked mock objects found at program exit. Step 1: rm -f /tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/latest         This ensures slave doesn't recover old live executors. Step 2: Restart the slave. Process exited with code 1 {code}  There appear to be two problems: 1) The docker containerizer should not exit on failure to clean up orphans.  The MesosContainerizer does not do this (see [MESOS-2367]). 2) Unmounting the orphan persistent volume fails for some reason.",2
"MESOS-4833","Poor allocator performance with labeled resources and/or persistent volumes","Modifying the {{HierarchicalAllocator_BENCHMARK_Test.ResourceLabels}} benchmark from https://reviews.apache.org/r/43686/ to use distinct labels between different slaves, performance regresses from ~2 seconds to ~3 minutes. The culprit seems to be the way in which the allocator merges together resources; reserved resource labels (or persistent volume IDs) inhibit merging, which causes performance to be much worse.",5
"MESOS-4834","Add 'file' fetcher plugin.","Add support for ""file"" based URI fetcher. This could be useful for container image provisioning from local file system.",2
"MESOS-4844","Add authentication to master endpoints","Before we can add authorization around operator endpoints, we need to add authentication support, so that unauthenticated requests are denied when --authenticate_http is enabled, and so that the principal is passed into `route()`.",2
"MESOS-4848","Agent Authn Research Spike","Research the master authentication flags to see what changes will be necessary for agent http authentication. Write up a 1-2 page summary/design doc.",2
"MESOS-4854","Update CHANGELOG with net_cls isolator","Need to update the CHANGELOG for 0.28 release.",1
"MESOS-4860","Add a script to install the Nvidia GDK on a host.","This script can be used to install the Nvidia GDK for Cuda 7.5 on a mesos development machine. The purpose of the Nvidia GDK is to provide all the necessary header files (nvml.h) and library files (libnvidia-ml.so) necessary to build mesos with Nvidia GPU support.  If the machine on which Mesos is being compiled doesn't have any GPUs, then libnvidia-ml.so consists only of stubs, allowing Mesos to build and run, but not actually do anything useful under the hood. This enables us to build a GPU-enabled mesos on a development machine without GPUs and then deploy it to a production machine with GPUs and be reasonably sure it will work.",2
"MESOS-4861","Add configure flags to build with Nvidia GPU support.","The configure flags can be used to enable Nvidia GPU support, as well as specify the installation directories of the nvml header and library files if not already installed in standard include/library paths on the system.  They will also be used to conditionally build support for Nvidia GPUs into Mesos.",2
"MESOS-4864","Add flag to specify available Nvidia GPUs on an agent's command line.","In the initial GPU support we will not do auto-discovery of GPUs on an agent.  As such, an operator will need to specify a flag on the command line, listing all of the GPUs available on the system.",3
"MESOS-4865","Add GPUs as an explicit resource.","We will add ""gpus"" as an explicitly recognized resource in Mesos, akin to cpus, memory, ports, and disk.  In the containerizer, we will verify that the number of GPU resources passed in via the --resources flag matches the list of GPUs passed in via the --nvidia_gpus flag.  In the future we will add autodiscovery so this matching is unnecessary.  However, we will always have to pass ""gpus"" as a resource to make any GPU available on the system (unlike for cpus and memory, where the default is probed).",3
"MESOS-4877","Mesos containerizer can't handle top level docker image like ""alpine"" (must use ""library/alpine"")","This can be demonstrated with the {{mesos-execute}} command:  # Docker containerizer with image {{alpine}}: success {code} sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=docker --name=just-a-test --command=""sleep 1000"" --master=localhost:5050 {code} # Mesos containerizer with image {{alpine}}: failure {code} sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050 {code} # Mesos containerizer with image {{library/alpine}}: success {code} sudo ./build/src/mesos-execute --docker_image=library/alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050 {code}  In the slave logs:  {code} ea-4460-83 9c-838da86af34c-0007' I0306 16:32:41.418269  3403 metadata_manager.cpp:159] Looking for image 'alpine:latest' I0306 16:32:41.418699  3403 registry_puller.cpp:194] Pulling image 'alpine:latest' from 'docker-manifest://registry-1.docker.io:443alpine?latest#https' to '/tmp/mesos-test /store/docker/staging/ka7MlQ' E0306 16:32:43.098131  3400 slave.cpp:3773] Container '4bf9132d-9a57-4baa-a78c-e7164e93ace6' for executor 'just-a-test' of framework 4f055c6f-1bea-4460-839c-838da86af34c-0 007 failed to start: Collect failed: Unexpected HTTP response '401 Unauthorized {code}  curl command executed:  {code} $ sudo sysdig -A -p ""*%evt.time %proc.cmdline"" evt.type=execve and proc.name=curl                                                                   16:42:53.198998042 curl -s -S -L -D - https://registry-1.docker.io:443/v2/alpine/manifests/latest 16:42:53.784958541 curl -s -S -L -D - https://auth.docker.io/token?service=registry.docker.io&scope=repository:alpine:pull 16:42:54.294192024 curl -s -S -L -D - -H Authorization: Bearer eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCIsIng1YyI6WyJNSUlDTHpDQ0FkU2dBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakJHTVVRd1FnWURWUVFERXp0Uk5Gb3pPa2RYTjBrNldGUlFSRHBJVFRSUk9rOVVWRmc2TmtGRlF6cFNUVE5ET2tGU01rTTZUMFkzTnpwQ1ZrVkJPa2xHUlVrNlExazFTekFlRncweE5UQTJNalV4T1RVMU5EWmFGdzB4TmpBMk1qUXhPVFUxTkRaYU1FWXhSREJDQmdOVkJBTVRPMGhHU1UwNldGZFZWam8yUVZkSU9sWlpUVEk2TTFnMVREcFNWREkxT2s5VFNrbzZTMVExUmpwWVRsSklPbFJMTmtnNlMxUkxOanBCUVV0VU1Ga3dFd1lIS29aSXpqMENBUVlJS29aSXpqMERBUWNEUWdBRXl2UzIvdEI3T3JlMkVxcGRDeFdtS1NqV1N2VmJ2TWUrWGVFTUNVMDByQjI0akNiUVhreFdmOSs0MUxQMlZNQ29BK0RMRkIwVjBGZGdwajlOWU5rL2pxT0JzakNCcnpBT0JnTlZIUThCQWY4RUJBTUNBSUF3RHdZRFZSMGxCQWd3QmdZRVZSMGxBREJFQmdOVkhRNEVQUVE3U0VaSlRUcFlWMVZXT2paQlYwZzZWbGxOTWpveldEVk1PbEpVTWpVNlQxTktTanBMVkRWR09saE9Va2c2VkVzMlNEcExWRXMyT2tGQlMxUXdSZ1lEVlIwakJEOHdQWUE3VVRSYU16cEhWemRKT2xoVVVFUTZTRTAwVVRwUFZGUllPalpCUlVNNlVrMHpRenBCVWpKRE9rOUdOemM2UWxaRlFUcEpSa1ZKT2tOWk5Vc3dDZ1lJS29aSXpqMEVBd0lEU1FBd1JnSWhBTXZiT2h4cHhrTktqSDRhMFBNS0lFdXRmTjZtRDFvMWs4ZEJOVGxuWVFudkFpRUF0YVJGSGJSR2o4ZlVSSzZ4UVJHRURvQm1ZZ3dZelR3Z3BMaGJBZzNOUmFvPSJdfQ.eyJhY2Nlc3MiOltdLCJhdWQiOiJyZWdpc3RyeS5kb2NrZXIuaW8iLCJleHAiOjE0NTcyODI4NzQsImlhdCI6MTQ1NzI4MjU3NCwiaXNzIjoiYXV0aC5kb2NrZXIuaW8iLCJqdGkiOiJaOGtyNXZXNEJMWkNIRS1IcVJIaCIsIm5iZiI6MTQ1NzI4MjU3NCwic3ViIjoiIn0.C2wtJq_P-m0buPARhmQjDfh6ztIAhcvgN3tfWIZEClSgXlVQ_sAQXAALNZKwAQL2Chj7NpHX--0GW-aeL_28Aw https://registry-1.docker.io:443/v2/alpine/manifests/latest {code}  Also got the same result with {{ubuntu}} docker image.",3
"MESOS-4888","Default cmd is executed as an incorrect command.","When mesos containerizer launch a container using a docker image, which only container default Cmd. The executable command is is a incorrect sequence. For example:  If an image default entrypoint is null, cmd is ""sh"", user defines shell=false, value is none, and arguments as [-c, echo 'hello world']. The executable command is `[sh, -c, echo 'hello world', sh]`, which is incorrect. It should be `[sh, sh, -c, echo 'hello world']` instead.  This problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. ",2
"MESOS-4889","Implement runtime isolator tests.","There different cases in docker runtime isolator. Some special cases should be tested with unique test case, to verify the docker runtime isolator logic is correct.",5
"MESOS-4891","Add a '/containers' endpoint to the agent to list all the active containers.","This endpoint will be similar to /monitor/statistics.json endpoint, but it'll also contain the 'container_status' about the container (see ContainerStatus in mesos.proto). We'll eventually deprecate the /monitor/statistics.json endpoint.",8
"MESOS-4903","Allow multiple loads of module manifests","The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure.",3
"MESOS-4912","LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails.","Observed on our CI: {noformat} [09:34:15] :  [Step 11/11] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers [09:34:19]W:  [Step 11/11] I0309 09:34:19.906719  2357 linux.cpp:81] Making '/tmp/MLVLnv' a shared mount [09:34:19]W:  [Step 11/11] I0309 09:34:19.923548  2357 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher [09:34:19]W:  [Step 11/11] I0309 09:34:19.924705  2376 containerizer.cpp:666] Starting container 'da610f7f-a709-4de8-94d3-74f4a520619b' for executor 'test_executor1' of framework '' [09:34:19]W:  [Step 11/11] I0309 09:34:19.925355  2371 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b [09:34:19]W:  [Step 11/11] I0309 09:34:19.925881  2377 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image1' to rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' [09:34:30]W:  [Step 11/11] I0309 09:34:30.835127  2376 linux.cpp:355] Bind mounting work directory from '/tmp/MLVLnv/slaves/test_slave/frameworks/executors/test_executor1/runs/da610f7f-a709-4de8-94d3-74f4a520619b' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b [09:34:30]W:  [Step 11/11] I0309 09:34:30.835392  2376 linux.cpp:683] Changing the ownership of the persistent volume at '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' with uid 0 and gid 0 [09:34:30]W:  [Step 11/11] I0309 09:34:30.840425  2376 linux.cpp:723] Mounting '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for persistent volume disk(test_role)[persistent_volume_id:volume]:32 of container da610f7f-a709-4de8-94d3-74f4a520619b [09:34:30]W:  [Step 11/11] I0309 09:34:30.843878  2374 linux_launcher.cpp:304] Cloning child process with flags = CLONE_NEWNS [09:34:30]W:  [Step 11/11] I0309 09:34:30.848302  2371 containerizer.cpp:666] Starting container 'fe4729c5-1e63-4cc6-a2e3-fe5006ffe087' for executor 'test_executor2' of framework '' [09:34:30]W:  [Step 11/11] I0309 09:34:30.848758  2371 containerizer.cpp:1392] Destroying container 'da610f7f-a709-4de8-94d3-74f4a520619b' [09:34:30]W:  [Step 11/11] I0309 09:34:30.848865  2373 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917' for container fe4729c5-1e63-4cc6-a2e3-fe5006ffe087 [09:34:30]W:  [Step 11/11] I0309 09:34:30.849449  2375 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image2' to rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917' [09:34:30]W:  [Step 11/11] I0309 09:34:30.854038  2374 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b [09:34:30]W:  [Step 11/11] I0309 09:34:30.856693  2372 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2.608128ms [09:34:30]W:  [Step 11/11] I0309 09:34:30.859237  2377 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b [09:34:30]W:  [Step 11/11] I0309 09:34:30.861454  2377 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2176us [09:34:30]W:  [Step 11/11] I0309 09:34:30.934608  2378 containerizer.cpp:1608] Executor for container 'da610f7f-a709-4de8-94d3-74f4a520619b' has exited [09:34:30]W:  [Step 11/11] I0309 09:34:30.937692  2372 linux.cpp:798] Unmounting volume '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for container da610f7f-a709-4de8-94d3-74f4a520619b [09:34:30]W:  [Step 11/11] I0309 09:34:30.937742  2372 linux.cpp:817] Unmounting sandbox/work directory '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b [09:34:30]W:  [Step 11/11] I0309 09:34:30.938129  2375 provisioner.cpp:330] Destroying container rootfs at '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b [09:34:45] :  [Step 11/11] ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1318: Failure [09:34:45] :  [Step 11/11] Failed to wait 15secs for wait1 [09:34:48] :  [Step 11/11] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers (32341 ms) {noformat}",3
"MESOS-4937","Investigate container security options for Mesos containerizer","We should investigate the following to improve the container security for Mesos containerizer and come up with a list of features that we want to support in MVP.  1) Capabilities 2) User namespace 3) Seccomp 4) SELinux 5) AppArmor  We should investigate what other container systems are doing regarding security: 1) [k8s| https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/types.go#L2905] 2) [docker|https://docs.docker.com/engine/security/security/] 3) [oci|https://github.com/opencontainers/specs/blob/master/config.md]",5
"MESOS-4942","Docker runtime isolator tests may cause disk issue.","Currently slave working directory is used as docker store dir and archive dir, which is problematic. Because slave work dir is exactly `environment->mkdtemp()`, it will get cleaned up until the end of the whole test. But the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. Cleanup has to be done by each test tear down. ",2
"MESOS-4944","Improve overlay backend so that it's writable","Currently, the overlay backend will provision a read-only FS. We can use an empty directory from the container sandbox to act as the upper layer so that it's writable.",5
"MESOS-4978","Update mesos-execute with Appc changes.","mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier.",3
"MESOS-4985","Destroy a container while it's provisioning can lead to leaked provisioned directories.","Here is the possible sequence of events: 1) containerizer->launch 2) provisioner->provision is called. it is fetching the image 3) executor registration timed out 4) containerizer->destroy is called 5) container->state is still in PREPARING 6) provisioner->destroy is called  So we can be calling provisioner->destory while provisioner->provision hasn't finished yet. provisioner->destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes.",3
"MESOS-5004","Clarify docs on '/reserve' and '/create-volumes' without authentication","For both reservations and persistent volume creation, the behavior of the HTTP endpoints differs slightly from that of the framework operations. Due to the implementation of HTTP authentication, it is not possible for a framework/operator to provide a principal when HTTP authentication is disabled. This means that when HTTP authentication is disabled, the endpoint handlers will _always_ receive {{None()}} as the principal associated with the request, and thus if authorization is enabled, the request will only succeed if the NONE principal is authorized to do stuff.  The docs should be updated to explain this behavior explicitly.",1
"MESOS-5044","Temporary directories created by environment->mkdtemp cleanup can be problematic.","Currently in mesos test, we have the temporary directories created by `environment->mkdtemp()` cleaned up until the end of the test suite, which can be problematic. For instance, if we have many tests in a test suite, each of those tests is performing large size disk read/write in its temp dir, which may lead to out of disk issue on some resource limited machines.   We should have these temp dir created by `environment->mkdtemp` cleaned up during each test teardown. Currently we only clean up the sandbox for each test.",1
"MESOS-5062","Update the long-lived-framework example to run on test clusters","There are a couple of problems with the long-lived framework that prevent it from being deployed (easily) on an actual cluster:  * The framework will greedily accept all offers; it runs one executor per agent in the cluster. * The framework assumes the {{long-lived-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments. * The framework does not specify an resources with the executor.  This is required by many isolators. * The framework has no metrics.",3
"MESOS-5109","Capture the error code in `ErrnoError` and `WindowsError`.","The {{ErrnoError}} and {{WindowsError}} classes simply construct the error string via a mechanism such as {{strerror}}. They should also capture the error code, as it is an essential piece of information for such an error type.",2
"MESOS-5111","Update `network::connect` to use the typed error state of `Try`.","{{network::connect}} function returns a {{Try<int>}} currently and the caller is required to inspect the state of {{errno}} out-of-band. {{network::connect}} should really return something like a {{Try<int, ErrnoError>}}.",2
"MESOS-5112","Introduce `WindowsSocketError`.","{{WindowsError}} invokes {{::GetLastError}} to retrieve the error code. Windows has a {{::WSAGetLastError}} function which at the interface level, is intended for failed socket operations. We should introduce a {{WindowsSocketError}} which invokes {{::WSAGetLastError}} and use them accordingly.",2
"MESOS-5113","`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag","If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump: 0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56 56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory. (gdb) bt #0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56 #1  0x00007ffff23280d8 in __GI_abort () at abort.c:89 #2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x451f5c ""isSome()"",     file=file@entry=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=line@entry=111,     function=function@entry=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:92 #3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c ""isSome()"", file=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=111,     function=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:101 #4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111 Python Exception <class 'IndexError'> list index out of range: #5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331 #6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239 #7  0x00007ffff60cd972 in std::_Function_handler<void (process::ProcessBase*), process::Future<Nothing> process::dispatch<Nothing, mesos::internal::slave::MesosIsolatorProcess, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > >(process::PID<mesos::internal::slave::MesosIsolatorProcess> const&, process::Future<Nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&), std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> >)::{lambda(process::ProcessBase*)#1}>::_M_invoke(std::_Any_data const&, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071 #8  0x00007ffff6a6bf38 in std::function<void (process::ProcessBase*)>::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471 #9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130 #10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161 #11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82 #12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570 #13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218 #14 0x00007ffff6a5bf33 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x697d70,     __args=<unknown type in /home/vagrant/mesosphere/mesos/build/src/.libs/libmesos-0.29.0.so, CU 0x45bb552, DIE 0x469efe5>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295 #15 0x00007ffff6a5bee6 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::operator()<, void>() (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353 #16 0x00007ffff6a5be95 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731 #17 0x00007ffff6a5be65 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::operator()() (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720 #18 0x00007ffff6a5be3c in std::thread::_Impl<std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()> >::_M_run() (this=0x697d58)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115 #19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6 #20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312 #21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111 (gdb) frame 4 #4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",1
"MESOS-5127","Reset `LIBPROCESS_IP` in `network\cni` isolator.","Currently the `LIBPROCESS_IP` environment variable was being set to     the Agent IP if the environment variable has not be defined by the     `Framework`. For containers having their own IP address (as with     containers on CNI networks) this becomes a problem since the command     executor tries to bind to the `LIBPROCESS_IP` that does not exist in     its network namespace, and fails. Thus, for containers launched on CNI     networks the `LIBPROCESS_IP` should not be set, or rather is set to     ""0.0.0.0"", allowing the container to bind to the IP address provided     by the CNI network.",1
"MESOS-5128","PersistentVolumeTest.AccessPersistentVolume is flaky","Observed on ASF CI:  {code} [ RUN      ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0 I0405 17:29:19.134435 31837 cluster.cpp:139] Creating default 'local' authorizer I0405 17:29:19.251143 31837 leveldb.cpp:174] Opened db in 116.386403ms I0405 17:29:19.310050 31837 leveldb.cpp:181] Compacted db in 58.80688ms I0405 17:29:19.310180 31837 leveldb.cpp:196] Created db iterator in 37145ns I0405 17:29:19.310199 31837 leveldb.cpp:202] Seeked to beginning of db in 4212ns I0405 17:29:19.310210 31837 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns I0405 17:29:19.310279 31837 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0405 17:29:19.311069 31861 recover.cpp:447] Starting replica recovery I0405 17:29:19.311362 31861 recover.cpp:473] Replica is in EMPTY status I0405 17:29:19.312641 31861 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14359)@172.17.0.4:43972 I0405 17:29:19.313045 31860 recover.cpp:193] Received a recover response from a replica in EMPTY status I0405 17:29:19.313608 31860 recover.cpp:564] Updating replica status to STARTING I0405 17:29:19.316416 31867 master.cpp:376] Master 9565ff6f-f1b6-4259-8430-690e635c391f (4090d10eba90) started on 172.17.0.4:43972 I0405 17:29:19.316470 31867 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/0A9ELu/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/0A9ELu/master"" --zk_session_timeout=""10secs"" I0405 17:29:19.316938 31867 master.cpp:427] Master only allowing authenticated frameworks to register I0405 17:29:19.316951 31867 master.cpp:432] Master only allowing authenticated agents to register I0405 17:29:19.316961 31867 credentials.hpp:37] Loading credentials for authentication from '/tmp/0A9ELu/credentials' I0405 17:29:19.317402 31867 master.cpp:474] Using default 'crammd5' authenticator I0405 17:29:19.317643 31867 master.cpp:545] Using default 'basic' HTTP authenticator I0405 17:29:19.317854 31867 master.cpp:583] Authorization enabled I0405 17:29:19.318081 31864 whitelist_watcher.cpp:77] No whitelist given I0405 17:29:19.318079 31861 hierarchical.cpp:144] Initialized hierarchical allocator process I0405 17:29:19.320838 31864 master.cpp:1826] The newly elected leader is master@172.17.0.4:43972 with id 9565ff6f-f1b6-4259-8430-690e635c391f I0405 17:29:19.320888 31864 master.cpp:1839] Elected as the leading master! I0405 17:29:19.320909 31864 master.cpp:1526] Recovering from registrar I0405 17:29:19.321218 31871 registrar.cpp:331] Recovering registrar I0405 17:29:19.347045 31860 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 33.164133ms I0405 17:29:19.347126 31860 replica.cpp:320] Persisted replica status to STARTING I0405 17:29:19.347611 31869 recover.cpp:473] Replica is in STARTING status I0405 17:29:19.349215 31871 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14361)@172.17.0.4:43972 I0405 17:29:19.349653 31870 recover.cpp:193] Received a recover response from a replica in STARTING status I0405 17:29:19.350236 31866 recover.cpp:564] Updating replica status to VOTING I0405 17:29:19.388882 31864 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.38299ms I0405 17:29:19.388993 31864 replica.cpp:320] Persisted replica status to VOTING I0405 17:29:19.389369 31856 recover.cpp:578] Successfully joined the Paxos group I0405 17:29:19.389735 31856 recover.cpp:462] Recover process terminated I0405 17:29:19.390476 31868 log.cpp:659] Attempting to start the writer I0405 17:29:19.392125 31862 replica.cpp:493] Replica received implicit promise request from (14362)@172.17.0.4:43972 with proposal 1 I0405 17:29:19.430706 31862 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.505062ms I0405 17:29:19.430816 31862 replica.cpp:342] Persisted promised to 1 I0405 17:29:19.431918 31856 coordinator.cpp:238] Coordinator attempting to fill missing positions I0405 17:29:19.433725 31861 replica.cpp:388] Replica received explicit promise request from (14363)@172.17.0.4:43972 for position 0 with proposal 2 I0405 17:29:19.472491 31861 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 38.659492ms I0405 17:29:19.472595 31861 replica.cpp:712] Persisted action at 0 I0405 17:29:19.474556 31864 replica.cpp:537] Replica received write request for position 0 from (14364)@172.17.0.4:43972 I0405 17:29:19.474652 31864 leveldb.cpp:436] Reading position from leveldb took 49423ns I0405 17:29:19.528175 31864 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 53.443616ms I0405 17:29:19.528300 31864 replica.cpp:712] Persisted action at 0 I0405 17:29:19.529389 31865 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0 I0405 17:29:19.571137 31865 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 41.676495ms I0405 17:29:19.571254 31865 replica.cpp:712] Persisted action at 0 I0405 17:29:19.571302 31865 replica.cpp:697] Replica learned NOP action at position 0 I0405 17:29:19.572322 31856 log.cpp:675] Writer started with ending position 0 I0405 17:29:19.574060 31861 leveldb.cpp:436] Reading position from leveldb took 83200ns I0405 17:29:19.575417 31864 registrar.cpp:364] Successfully fetched the registry (0B) in 0ns I0405 17:29:19.575565 31864 registrar.cpp:463] Applied 1 operations in 46419ns; attempting to update the 'registry' I0405 17:29:19.576517 31857 log.cpp:683] Attempting to append 170 bytes to the log I0405 17:29:19.576849 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1 I0405 17:29:19.578390 31857 replica.cpp:537] Replica received write request for position 1 from (14365)@172.17.0.4:43972 I0405 17:29:19.780277 31857 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 201.808617ms I0405 17:29:19.780366 31857 replica.cpp:712] Persisted action at 1 I0405 17:29:19.782024 31857 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0 I0405 17:29:19.823770 31857 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 41.667662ms I0405 17:29:19.823851 31857 replica.cpp:712] Persisted action at 1 I0405 17:29:19.823889 31857 replica.cpp:697] Replica learned APPEND action at position 1 I0405 17:29:19.825701 31867 registrar.cpp:508] Successfully updated the 'registry' in 0ns I0405 17:29:19.825929 31867 registrar.cpp:394] Successfully recovered registrar I0405 17:29:19.826015 31857 log.cpp:702] Attempting to truncate the log to 1 I0405 17:29:19.826262 31867 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2 I0405 17:29:19.827647 31867 replica.cpp:537] Replica received write request for position 2 from (14366)@172.17.0.4:43972 I0405 17:29:19.828018 31857 master.cpp:1634] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register I0405 17:29:19.828065 31861 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover I0405 17:29:19.865555 31867 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.822178ms I0405 17:29:19.865661 31867 replica.cpp:712] Persisted action at 2 I0405 17:29:19.866921 31867 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0 I0405 17:29:19.907341 31867 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.356649ms I0405 17:29:19.907531 31867 leveldb.cpp:399] Deleting ~1 keys from leveldb took 91109ns I0405 17:29:19.907560 31867 replica.cpp:712] Persisted action at 2 I0405 17:29:19.907599 31867 replica.cpp:697] Replica learned TRUNCATE action at position 2 I0405 17:29:19.923305 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:2048 Trying semicolon-delimited string format instead I0405 17:29:19.926491 31837 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix W0405 17:29:19.927836 31837 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges I0405 17:29:19.932029 31862 slave.cpp:200] Agent started on 441)@172.17.0.4:43972 I0405 17:29:19.932086 31862 slave.cpp:201] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":2048.0},""type"":""SCALAR""},{""name"":""disk"",""role"":""role1"",""scalar"":{""value"":4096.0},""type"":""SCALAR""}]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC"" I0405 17:29:19.932665 31862 credentials.hpp:86] Loading credential for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential' I0405 17:29:19.932934 31862 slave.cpp:338] Agent using credential for: test-principal I0405 17:29:19.932968 31862 credentials.hpp:37] Loading credentials for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials' I0405 17:29:19.933284 31862 slave.cpp:390] Using default 'basic' HTTP authenticator I0405 17:29:19.934916 31837 sched.cpp:222] Version: 0.29.0 I0405 17:29:19.935566 31862 slave.cpp:589] Agent resources: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000] I0405 17:29:19.935664 31862 slave.cpp:597] Agent attributes: [  ] I0405 17:29:19.935679 31862 slave.cpp:602] Agent hostname: 4090d10eba90 I0405 17:29:19.938390 31864 state.cpp:57] Recovering state from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta' I0405 17:29:19.940608 31869 sched.cpp:326] New master detected at master@172.17.0.4:43972 I0405 17:29:19.940749 31869 sched.cpp:382] Authenticating with master master@172.17.0.4:43972 I0405 17:29:19.940773 31869 sched.cpp:389] Using default CRAM-MD5 authenticatee I0405 17:29:19.942371 31869 authenticatee.cpp:121] Creating new client SASL connection I0405 17:29:19.942873 31859 master.cpp:5679] Authenticating scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:19.943156 31859 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(896)@172.17.0.4:43972 I0405 17:29:19.943507 31863 authenticator.cpp:98] Creating new server SASL connection I0405 17:29:19.943740 31859 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0405 17:29:19.943783 31859 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0405 17:29:19.943892 31859 authenticator.cpp:203] Received SASL authentication start I0405 17:29:19.943977 31859 authenticator.cpp:325] Authentication requires more steps I0405 17:29:19.944066 31859 authenticatee.cpp:258] Received SASL authentication step I0405 17:29:19.944164 31859 authenticator.cpp:231] Received SASL authentication step I0405 17:29:19.944193 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0405 17:29:19.944206 31859 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0405 17:29:19.944268 31859 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0405 17:29:19.944300 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0405 17:29:19.944313 31859 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0405 17:29:19.944321 31859 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0405 17:29:19.944339 31859 authenticator.cpp:317] Authentication success I0405 17:29:19.944541 31859 authenticatee.cpp:298] Authentication success I0405 17:29:19.944655 31859 master.cpp:5709] Successfully authenticated principal 'test-principal' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:19.944737 31859 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(896)@172.17.0.4:43972 I0405 17:29:19.945111 31859 sched.cpp:472] Successfully authenticated with master master@172.17.0.4:43972 I0405 17:29:19.945132 31859 sched.cpp:777] Sending SUBSCRIBE call to master@172.17.0.4:43972 I0405 17:29:19.945591 31859 sched.cpp:810] Will retry registration in 372.80738ms if necessary I0405 17:29:19.945744 31865 master.cpp:2346] Received SUBSCRIBE call for framework 'default' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:19.945838 31865 master.cpp:1865] Authorizing framework principal 'test-principal' to receive offers for role 'role1' I0405 17:29:19.946194 31865 master.cpp:2417] Subscribing framework default with checkpointing disabled and capabilities [  ] I0405 17:29:19.946866 31866 hierarchical.cpp:266] Added framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:19.946974 31866 hierarchical.cpp:1490] No resources available to allocate! I0405 17:29:19.947010 31866 hierarchical.cpp:1585] No inverse offers to send out! I0405 17:29:19.947054 31865 sched.cpp:704] Framework registered with 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:19.947074 31866 hierarchical.cpp:1141] Performed allocation for 0 agents in 178242ns I0405 17:29:19.947124 31865 sched.cpp:718] Scheduler::registered took 38907ns I0405 17:29:19.948712 31866 status_update_manager.cpp:200] Recovering status update manager I0405 17:29:19.948901 31866 containerizer.cpp:416] Recovering containerizer I0405 17:29:19.951021 31866 provisioner.cpp:245] Provisioner recovery complete I0405 17:29:19.951802 31866 slave.cpp:4773] Finished recovery I0405 17:29:19.952518 31866 slave.cpp:4945] Querying resource estimator for oversubscribable resources I0405 17:29:19.953248 31866 slave.cpp:928] New master detected at master@172.17.0.4:43972 I0405 17:29:19.953305 31865 status_update_manager.cpp:174] Pausing sending status updates I0405 17:29:19.953626 31866 slave.cpp:991] Authenticating with master master@172.17.0.4:43972 I0405 17:29:19.953716 31866 slave.cpp:996] Using default CRAM-MD5 authenticatee I0405 17:29:19.954074 31866 slave.cpp:964] Detecting new master I0405 17:29:19.954167 31861 authenticatee.cpp:121] Creating new client SASL connection I0405 17:29:19.954372 31866 slave.cpp:4959] Received oversubscribable resources  from the resource estimator I0405 17:29:19.954756 31866 master.cpp:5679] Authenticating slave(441)@172.17.0.4:43972 I0405 17:29:19.954944 31861 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(897)@172.17.0.4:43972 I0405 17:29:19.955368 31863 authenticator.cpp:98] Creating new server SASL connection I0405 17:29:19.955687 31861 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0405 17:29:19.955801 31861 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0405 17:29:19.956075 31861 authenticator.cpp:203] Received SASL authentication start I0405 17:29:19.956279 31861 authenticator.cpp:325] Authentication requires more steps I0405 17:29:19.956455 31861 authenticatee.cpp:258] Received SASL authentication step I0405 17:29:19.956676 31861 authenticator.cpp:231] Received SASL authentication step I0405 17:29:19.956815 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0405 17:29:19.956907 31861 auxprop.cpp:179] Looking up auxiliary property '*userPassword' I0405 17:29:19.957044 31861 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0405 17:29:19.957166 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0405 17:29:19.957264 31861 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0405 17:29:19.957353 31861 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0405 17:29:19.957449 31861 authenticator.cpp:317] Authentication success I0405 17:29:19.957664 31857 authenticatee.cpp:298] Authentication success I0405 17:29:19.957813 31857 master.cpp:5709] Successfully authenticated principal 'test-principal' at slave(441)@172.17.0.4:43972 I0405 17:29:19.958008 31861 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(897)@172.17.0.4:43972 I0405 17:29:19.958732 31857 slave.cpp:1061] Successfully authenticated with master master@172.17.0.4:43972 I0405 17:29:19.958930 31857 slave.cpp:1457] Will retry registration in 18.568334ms if necessary I0405 17:29:19.959262 31857 master.cpp:4390] Registering agent at slave(441)@172.17.0.4:43972 (4090d10eba90) with id 9565ff6f-f1b6-4259-8430-690e635c391f-S0 I0405 17:29:19.959934 31857 registrar.cpp:463] Applied 1 operations in 99197ns; attempting to update the 'registry' I0405 17:29:19.961587 31857 log.cpp:683] Attempting to append 343 bytes to the log I0405 17:29:19.961879 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3 I0405 17:29:19.963135 31857 replica.cpp:537] Replica received write request for position 3 from (14381)@172.17.0.4:43972 I0405 17:29:19.999408 31857 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 36.200109ms I0405 17:29:19.999512 31857 replica.cpp:712] Persisted action at 3 I0405 17:29:20.001049 31869 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0 I0405 17:29:20.038849 31869 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 37.709507ms I0405 17:29:20.038930 31869 replica.cpp:712] Persisted action at 3 I0405 17:29:20.038965 31869 replica.cpp:697] Replica learned APPEND action at position 3 I0405 17:29:20.041484 31869 registrar.cpp:508] Successfully updated the 'registry' in 0ns I0405 17:29:20.041785 31869 log.cpp:702] Attempting to truncate the log to 3 I0405 17:29:20.042364 31859 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4 I0405 17:29:20.043767 31859 replica.cpp:537] Replica received write request for position 4 from (14382)@172.17.0.4:43972 I0405 17:29:20.044585 31869 master.cpp:4458] Registered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000] I0405 17:29:20.044910 31864 slave.cpp:1105] Registered with master master@172.17.0.4:43972; given agent ID 9565ff6f-f1b6-4259-8430-690e635c391f-S0 I0405 17:29:20.045075 31864 fetcher.cpp:81] Clearing fetcher cache I0405 17:29:20.045140 31870 hierarchical.cpp:476] Added agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000] (allocated: ) I0405 17:29:20.045581 31864 slave.cpp:1128] Checkpointing SlaveInfo to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/slave.info' I0405 17:29:20.045974 31864 slave.cpp:1165] Forwarding total oversubscribed resources  I0405 17:29:20.046077 31864 slave.cpp:3664] Received ping from slave-observer(399)@172.17.0.4:43972 I0405 17:29:20.046193 31864 status_update_manager.cpp:181] Resuming sending status updates I0405 17:29:20.046289 31870 hierarchical.cpp:1585] No inverse offers to send out! I0405 17:29:20.046370 31870 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 1.153377ms I0405 17:29:20.046499 31864 master.cpp:4802] Received update of agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with total oversubscribed resources  I0405 17:29:20.047142 31868 hierarchical.cpp:534] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) updated with oversubscribed resources  (total: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000], allocated: disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000]) I0405 17:29:20.047960 31868 hierarchical.cpp:1490] No resources available to allocate! I0405 17:29:20.048009 31868 hierarchical.cpp:1585] No inverse offers to send out! I0405 17:29:20.048065 31868 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 866803ns I0405 17:29:20.048591 31864 master.cpp:5508] Sending 1 offers to framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:20.049188 31860 sched.cpp:874] Scheduler::resourceOffers took 114867ns I0405 17:29:20.080921 31859 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.025538ms I0405 17:29:20.081001 31859 replica.cpp:712] Persisted action at 4 I0405 17:29:20.082425 31859 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0 I0405 17:29:20.106056 31859 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 23.583037ms I0405 17:29:20.106205 31859 leveldb.cpp:399] Deleting ~2 keys from leveldb took 76995ns I0405 17:29:20.106240 31859 replica.cpp:712] Persisted action at 4 I0405 17:29:20.106278 31859 replica.cpp:697] Replica learned TRUNCATE action at position 4 I0405 17:29:20.119488 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128 Trying semicolon-delimited string format instead I0405 17:29:20.121356 31859 master.cpp:3288] Processing ACCEPT call for offers: [ 9565ff6f-f1b6-4259-8430-690e635c391f-O0 ] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:20.121485 31859 master.cpp:3046] Authorizing principal 'test-principal' to create volumes I0405 17:29:20.121692 31859 master.cpp:2891] Authorizing framework principal 'test-principal' to launch task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 as user 'mesos' I0405 17:29:20.123877 31871 master.cpp:3617] Applying CREATE operation for volumes disk(role1)[id1:path1]:2048 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) I0405 17:29:20.125424 31871 master.cpp:6747] Sending checkpointed resources disk(role1)[id1:path1]:2048 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) I0405 17:29:20.126397 31856 hierarchical.cpp:656] Updated allocation of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000] to disk(role1):2048; cpus(*):2; mem(*):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048 I0405 17:29:20.126667 31871 master.hpp:177] Adding task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) I0405 17:29:20.126875 31871 master.cpp:3773] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) I0405 17:29:20.127390 31856 slave.cpp:2523] Updated checkpointed resources from  to disk(role1)[id1:path1]:2048 I0405 17:29:20.127615 31856 slave.cpp:1497] Got assigned task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.127876 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32 Trying semicolon-delimited string format instead I0405 17:29:20.127841 31871 hierarchical.cpp:893] Recovered disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: disk(role1)[id1:path1]:2048; cpus(*):1; mem(*):128) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.127913 31871 hierarchical.cpp:930] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 filtered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for 5secs I0405 17:29:20.128667 31856 slave.cpp:1616] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.128937 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32 Trying semicolon-delimited string format instead I0405 17:29:20.129776 31856 paths.cpp:528] Trying to chown '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' to user 'mesos' I0405 17:29:20.145324 31856 slave.cpp:5575] Launching executor 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' I0405 17:29:20.146057 31858 containerizer.cpp:675] Starting container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework '9565ff6f-f1b6-4259-8430-690e635c391f-0000' I0405 17:29:20.146078 31856 slave.cpp:1834] Queuing task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.146203 31856 slave.cpp:881] Successfully attached file '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' I0405 17:29:20.147619 31859 posix.cpp:206] Changing the ownership of the persistent volume at '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' with uid 1000 and gid 1000 I0405 17:29:20.162421 31859 posix.cpp:250] Adding symlink from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09 I0405 17:29:20.172133 31861 launcher.cpp:123] Forked child with pid '7927' for container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' WARNING: Logging before InitGoogleLogging() is written to STDERR I0405 17:29:20.376197  7941 process.cpp:986] libprocess is initialized on 172.17.0.4:50952 with 16 worker threads I0405 17:29:20.378132  7941 logging.cpp:195] Logging to STDERR I0405 17:29:20.380861  7941 exec.cpp:150] Version: 0.29.0 I0405 17:29:20.396257  7966 exec.cpp:200] Executor started at: executor(1)@172.17.0.4:50952 with pid 7941 I0405 17:29:20.399426 31860 slave.cpp:2825] Got registration for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952 I0405 17:29:20.402995  7966 exec.cpp:225] Executor registered on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 I0405 17:29:20.403014 31860 slave.cpp:1999] Sending queued task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' to executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952 I0405 17:29:20.405624  7966 exec.cpp:237] Executor::registered took 393272ns I0405 17:29:20.406108  7966 exec.cpp:312] Executor asked to run task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' Registered executor on 4090d10eba90 I0405 17:29:20.406708  7966 exec.cpp:321] Executor::launchTask took 568039ns Starting task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 Forked command at 7972 sh -c 'echo abc > path1/file' I0405 17:29:20.411375  7966 exec.cpp:535] Executor sending status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.413156 31857 slave.cpp:3184] Handling status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952 I0405 17:29:20.415714 31857 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.415788 31857 status_update_manager.cpp:497] Creating StatusUpdate stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.416345 31857 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent I0405 17:29:20.416720 31870 slave.cpp:3582] Forwarding the update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:43972 I0405 17:29:20.416954 31870 slave.cpp:3476] Status update manager successfully handled status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.416997 31870 slave.cpp:3492] Sending acknowledgement for status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to executor(1)@172.17.0.4:50952 I0405 17:29:20.417505 31870 master.cpp:4947] Status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) I0405 17:29:20.417549 31870 master.cpp:4995] Forwarding status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.417724 31870 master.cpp:6608] Updating the state of task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING) I0405 17:29:20.417943  7960 exec.cpp:358] Executor received status update acknowledgement cf4f8fe9-44f2-43ce-8868-b3a09b7298cf for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.418002 31870 sched.cpp:982] Scheduler::statusUpdate took 105225ns I0405 17:29:20.418623 31870 master.cpp:4102] Processing ACKNOWLEDGE call cf4f8fe9-44f2-43ce-8868-b3a09b7298cf for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 I0405 17:29:20.419181 31860 status_update_manager.cpp:392] Received status update acknowledgement (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.419816 31860 slave.cpp:2594] Status update manager successfully handled status update acknowledgement (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.513465  7969 exec.cpp:535] Executor sending status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 Command exited with status 0 (pid: 7972) I0405 17:29:20.515449 31870 slave.cpp:3184] Handling status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952 I0405 17:29:20.516875 31860 slave.cpp:5885] Terminating task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 I0405 17:29:20.517496 31867 posix.cpp:156] Removing symlink '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09 I0405 17:29:20.519361 31864 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.519850 31864 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent I0405 17:29:20.520678 31870 slave.cpp:3582] Forwarding the update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:43972 I0405 17:29:20.520901 31870 slave.cpp:3476] Status update manager successfully handled status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.520949 31870 slave.cpp:3492] Sending acknowledgement for status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to executor(1)@172.17.0.4:50952 I0405 17:29:20.521550 31864 master.cpp:4947] Status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) I0405 17:29:20.521610 31864 master.cpp:4995] Forwarding status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.522099 31871 sched.cpp:982] Scheduler::statusUpdate took 102502ns I0405 17:29:20.522367 31864 master.cpp:6608] Updating the state of task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED) I0405 17:29:20.524288 31871 hierarchical.cpp:1676] Filtered offer with disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.524379 31871 hierarchical.cpp:1490] No resources available to allocate! I0405 17:29:20.524451 31871 hierarchical.cpp:1585] No inverse offers to send out! I0405 17:29:20.524551 31871 hierarchical.cpp:1141] Performed allocation for 1 agents in 961746ns I0405 17:29:20.525182 31858 hierarchical.cpp:893] Recovered cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: ) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.525197 31864 master.cpp:4102] Processing ACKNOWLEDGE call 128eb7af-a662-4cbb-9401-125dca38f719 for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 I0405 17:29:20.525380 31864 master.cpp:6674] Removing task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) I0405 17:29:20.526067 31864 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.526425 31864 status_update_manager.cpp:528] Cleaning up status update stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.526917 31864 slave.cpp:2594] Status update manager successfully handled status update acknowledgement (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:20.527048 31864 slave.cpp:5926] Completing task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 I0405 17:29:20.527732  7964 exec.cpp:358] Executor received status update acknowledgement 128eb7af-a662-4cbb-9401-125dca38f719 for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:21.527920 31859 slave.cpp:3710] executor(1)@172.17.0.4:50952 exited ../../src/tests/persistent_volume_tests.cpp:825: Failure Failed to wait 15secs for offers I0405 17:29:35.542609 31856 master.cpp:1269] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 disconnected I0405 17:29:35.542811 31856 master.cpp:2642] Disconnecting framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:35.542994 31856 master.cpp:2666] Deactivating framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:35.543349 31860 hierarchical.cpp:378] Deactivated framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:35.543501 31856 master.cpp:1293] Giving framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 0ns to failover I0405 17:29:35.543903 31868 master.cpp:5360] Framework failover timeout, removing framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:35.543936 31868 master.cpp:6093] Removing framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 I0405 17:29:35.544337 31861 slave.cpp:2215] Asked to shut down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 by master@172.17.0.4:43972 I0405 17:29:35.544381 31861 slave.cpp:2240] Shutting down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 I0405 17:29:35.544456 31861 slave.cpp:4398] Shutting down executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952 I0405 17:29:35.544960 31872 poll_socket.cpp:110] Socket error while connecting I0405 17:29:35.545013 31872 process.cpp:1650] Failed to send 'mesos.internal.ShutdownExecutorMessage' to '172.17.0.4:50952', connect: Socket error while connecting E0405 17:29:35.545106 31872 process.cpp:1958] Failed to shutdown socket with fd 27: Transport endpoint is not connected I0405 17:29:35.545474 31864 hierarchical.cpp:329] Removed framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 ../../src/tests/persistent_volume_tests.cpp:819: Failure Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(&driver, _))...          Expected: to be called at least once            Actual: never called - unsatisfied and active I0405 17:29:35.558538 31858 containerizer.cpp:1432] Destroying container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' ../../src/tests/cluster.cpp:453: Failure Failed to wait 15secs for wait I0405 17:29:50.565403 31870 slave.cpp:800] Agent terminating I0405 17:29:50.565512 31870 slave.cpp:2215] Asked to shut down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 by @0.0.0.0:0 W0405 17:29:50.565544 31870 slave.cpp:2236] Ignoring shutdown framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 because it is terminating I0405 17:29:50.574620 31866 master.cpp:1230] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) disconnected I0405 17:29:50.574766 31866 master.cpp:2701] Disconnecting agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) I0405 17:29:50.575003 31866 master.cpp:2720] Deactivating agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) I0405 17:29:50.575294 31865 hierarchical.cpp:563] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 deactivated I0405 17:29:50.605787 31837 master.cpp:1083] Master terminating I0405 17:29:50.606533 31866 hierarchical.cpp:508] Removed agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 [  FAILED  ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0, where GetParam() = 0 (31491 ms) {code}",3
"MESOS-5152","Add authentication to agent's /monitor/statistics endpoint","Operators may want to enforce that only authenticated users (and subsequently only specific authorized users) be able to view per-executor resource usage statistics. Since this endpoint is handled by the ResourceMonitorProcess, I would expect the work necessary to be similar to what was done for /files or /registry endpoint authn.",2
"MESOS-5157","Update webui for GPU metrics","After adding the GPU metrics and updating the resources JSON to include GPU information, the webui should be updated accordingly.",1
"MESOS-5160","Make `network/cni` enabled as the default network isolator for `MesosContainerizer`.","Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator. ",1
"MESOS-5171","Expose state/state.hpp to public headers","We want the Modules to be able to use replicated log along with the APIs to communicate with Zookeeper. This change would require us to expose at least the following headers state/storage.hpp, and any additional files that state.hpp depends on (e.g., zookeeper/authentication.hpp).",3
"MESOS-5214","Populate FrameworkInfo.principal for authenticated frameworks","If a framework authenticates and then does not provide a {{principal}} in its {{FrameworkInfo}}, we currently allow this and leave {{FrameworkInfo.principal}} unset. Instead, we should populate {{FrameworkInfo.principal}} for them automatically in that case to ensure that the two principals are equal.",2
"MESOS-5253","Isolator cleanup should not be invoked if they are not prepared yet.","If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet.   In this case, there no need to clean up any isolator, call provisioner destroy directly.",2
