issuekey,storypoint,context,codesnippet,Bug,Documentation,Improvement,Story,Task,Wish,HTTP API,allocation,build,c++ api,cmake,containerization,docker,documentation,fetcher,framework,general,isolation,java api,jenkins,leader election,libprocess,master,modules,project website,python api,replicated log,reviewbot,security,slave,statistics,stout,technical debt,test,tests,webui
MESOS-934,1.0,"'Logging and Debugging' document is out-of-date. The following is no longer correct:
http://mesos.apache.org/documentation/latest/logging-and-debugging/

We should either delete this document or re-write it entirely.",,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-998,5.0,Slave should wait until Containerizer::update() completes successfully Container resources are updated in several places in the slave and we don't check the update was successful or even wait until it completes.,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1081,1.0,"Master should not deactivate authenticated framework/slave on new AuthenticateMessage unless new authentication succeeds. Master should not deactivate an authenticated framework/slave upon receiving a new AuthenticateMessage unless new authentication succeeds. As it stands now, a malicious user could spoof the pid of an authenticated framework/slave and send an AuthenticateMessage to knock a valid framework/slave off the authenticated list, forcing the valid framework/slave to re-authenticate and re-register. This could be used in a DoS attack.
But how should we handle the scenario when the actual authenticated framework/slave sends an AuthenticateMessage that fails authentication?",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1119,2.0,"Allocator should make an allocation decision per slave instead of per framework/role. Currently the Allocator::allocate() code loops through roles and frameworks (based on DRF sort) and allocates *all* slaves resources to the first framework.

This logic should be a bit inversed. Instead, the slave should go through each slave, allocate it a role/framework and update the DRF shares.",,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1143,2.0,"Add a TASK_ERROR task status. During task validation we drop tasks that have errors and send TASK_LOST status updates. In most circumstances a framework will want to relaunch a task that has gone lost, and in the event the task is actually malformed (thus invalid) this will result in an infinite loop of sending a task and having it go lost.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1148,3.0,"Add support for rate limiting slave removal To safeguard against unforeseen bugs leading to widespread slave removal, it would be nice to allow for rate limiting of the decision to remove slaves and/or send TASK_LOST messages for tasks on those slaves.  Ideally this would allow an operator to be notified soon enough to intervene before causing cluster impact.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1236,2.0,stout's os module uses a mix of Try<Nothing> and bool returns stout's os module should use Try<nothing> for return values throughout.</nothing>,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
MESOS-1237,2.0,stout's os::ls should return a Try<> stout's os::ls returns a list that can be empty - instead it should return a Try<list...> to be consistent.</list...>,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
MESOS-1303,1.0,"ExamplesTest.{TestFramework, NoExecutorFramework} flaky I'm having trouble reproducing this but I did observe it once on my OSX system:

{noformat}
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from ExamplesTest
[ RUN      ] ExamplesTest.TestFramework
../../src/tests/script.cpp:81: Failure
Failed
test_framework_test.sh terminated with signal 'Abort trap: 6'
[  FAILED  ] ExamplesTest.TestFramework (953 ms)
[ RUN      ] ExamplesTest.NoExecutorFramework
[       OK ] ExamplesTest.NoExecutorFramework (10162 ms)
[----------] 2 tests from ExamplesTest (11115 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (11121 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ExamplesTest.TestFramework
{noformat}

when investigating a failed make check for https://reviews.apache.org/r/20971/
{noformat}
[----------] 6 tests from ExamplesTest
[ RUN      ] ExamplesTest.TestFramework
[       OK ] ExamplesTest.TestFramework (8643 ms)
[ RUN      ] ExamplesTest.NoExecutorFramework
tests/script.cpp:81: Failure
Failed
no_executor_framework_test.sh terminated with signal 'Aborted'
[  FAILED  ] ExamplesTest.NoExecutorFramework (7220 ms)
[ RUN      ] ExamplesTest.JavaFramework
[       OK ] ExamplesTest.JavaFramework (11181 ms)
[ RUN      ] ExamplesTest.JavaException
[       OK ] ExamplesTest.JavaException (5624 ms)
[ RUN      ] ExamplesTest.JavaLog
[       OK ] ExamplesTest.JavaLog (6472 ms)
[ RUN      ] ExamplesTest.PythonFramework
[       OK ] ExamplesTest.PythonFramework (14467 ms)
[----------] 6 tests from ExamplesTest (53607 ms total)
{noformat}",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1316,2.0,"Implement decent unit test coverage for the mesos-fetcher tool There are current no tests that cover the {{mesos-fetcher}} tool itself, and hence bugs like MESOS-1313 have accidentally slipped though.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0
MESOS-1332,3.0,"Improve Master and Slave metric names As we move the metrics to a new endpoint, we should consider revisiting the names of some of the current metrics to make them clearer.

It may also be worth considering changing some existing counter-style metrics to gauges.
",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-1358,1.0,Show when the leading master was elected in the webui This would be nice to have during debugging.,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
MESOS-1365,1.0,"SlaveRecoveryTest/0.MultipleFrameworks is flaky --gtest_repeat=-1 --gtest_shuffle --gtest_break_on_failure

{noformat}
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:05.931761  4320 exec.cpp:131] Version: 0.19.0
I0513 15:42:05.936698  4340 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task 51991f97-f5fd-4905-ad0f-02668083af7c
Forked command at 4367
sh -c 'sleep 1000'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:06.915061  4408 exec.cpp:131] Version: 0.19.0
I0513 15:42:06.931149  4435 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task eaf5d8d6-3a6c-4ee1-84c1-fae20fb1df83
sh -c 'sleep 1000'
Forked command at 4439
I0513 15:42:06.998332  4340 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:06.998414  4436 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:07.006350  4437 exec.cpp:228] Executor re-registered on slave 20140513-154204-16842879-51872-13062-0
Re-registered executor on artoo
I0513 15:42:07.027039  4337 exec.cpp:378] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 4367
Killing the following process trees:
[ 
-+- 4367 sh -c sleep 1000 
 \--- 4368 sleep 1000 
]
../../src/tests/slave_recovery_tests.cpp:2807: Failure
Value of: status1.get().state()
  Actual: TASK_FAILED
Expected: TASK_KILLED

Program received signal SIGSEGV, Segmentation fault.
testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::unittest::getinstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
3795          *static_cast<volatile int*="""">(NULL) = 1;
(gdb) bt
#0  testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::unittest::getinstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
#1  0x0000000000df98b9 in testing::internal::AssertHelper::operator= (this=0x7fffffffb860, message=...) at gmock-1.6.0/gtest/src/gtest.cc:356
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::mesoscontainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
#3  0x0000000000e22583 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::test, void=""""> (object=0x1954db0, method=&amp;virtual; testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#4  0x0000000000e12467 in testing::internal::HandleExceptionsInMethodIfSupported<testing::test, void=""""> (object=0x1954db0, method=&amp;virtual; testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#5  0x0000000000e010d5 in testing::Test::Run (this=0x1954db0) at gmock-1.6.0/gtest/src/gtest.cc:2161
#6  0x0000000000e01ceb in testing::TestInfo::Run (this=0x158cf80) at gmock-1.6.0/gtest/src/gtest.cc:2338
#7  0x0000000000e02387 in testing::TestCase::Run (this=0x158a880) at gmock-1.6.0/gtest/src/gtest.cc:2445
#8  0x0000000000e079ed in testing::internal::UnitTestImpl::RunAllTests (this=0x1558b40) at gmock-1.6.0/gtest/src/gtest.cc:4237
#9  0x0000000000e1ec83 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::unittestimpl, bool=""""> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::unittestimpl::runalltests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#10 0x0000000000e14217 in testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::unittestimpl, bool=""""> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::unittestimpl::runalltests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#11 0x0000000000e076d7 in testing::UnitTest::Run (this=0x154dac0 <testing::unittest::getinstance()::instance>) at gmock-1.6.0/gtest/src/gtest.cc:3872
#12 0x0000000000b99887 in main (argc=1, argv=0x7fffffffd9f8) at ../../src/tests/main.cpp:107
(gdb) frame 2
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::mesoscontainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
(gdb) p status1
$1 = {data = {<std::__shared_ptr<process::future<mesos::taskstatus>::Data, 2&gt;&gt; = {_M_ptr = 0x1963140, _M_refcount = {_M_pi = 0x198a620}}, <no data="""" fields="""">}}
(gdb) p status1.get()
$2 = (const mesos::TaskStatus &amp;) @0x7fffdc5bf5f0: {<google::protobuf::message> = {<google::protobuf::messagelite> = {_vptr$MessageLite = 0x7ffff74bc940 <vtable for="""" mesos::taskstatus+16="""">}, <no data="""" fields="""">}, static kTaskIdFieldNumber = 1, static kStateFieldNumber = 2, static kMessageFieldNumber = 4, 
  static kDataFieldNumber = 3, static kSlaveIdFieldNumber = 5, static kTimestampFieldNumber = 6, _unknown_fields_ = {fields_ = 0x0}, task_id_ = 0x7fffdc5ce9a0, message_ = 0x7fffdc5f5880, data_ = 0x154b4b0 <google::protobuf::internal::kemptystring>, slave_id_ = 0x7fffdc59c4f0, timestamp_ = 1429688582.046252, 
  state_ = 3, _cached_size_ = 0, _has_bits_ = {55}, static default_instance_ = 0x0}
(gdb) p status1.get().state()
$3 = mesos::TASK_FAILED
(gdb) list
2802      // Kill task 1.
2803      driver1.killTask(task1.task_id());
2804
2805      // Wait for TASK_KILLED update.
2806      AWAIT_READY(status1);
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
2808
2809      // Kill task 2.
2810      driver2.killTask(task2.task_id());
2811
{noformat}</google::protobuf::internal::kemptystring></no></vtable></google::protobuf::messagelite></google::protobuf::message></no></std::__shared_ptr<process::future<mesos::taskstatus></mesos::internal::slave::mesoscontainerizer></testing::unittest::getinstance()::instance></testing::internal::unittestimpl::runalltests()></testing::internal::unittestimpl,></testing::internal::unittestimpl::runalltests()></testing::internal::unittestimpl,></testing::test,></testing::test,></mesos::internal::slave::mesoscontainerizer></testing::unittest::getinstance()::instance></volatile></testing::unittest::getinstance()::instance>",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1371,1.0,Expose libprocess queue length from scheduler driver to metrics endpoint We expose the master's event queue length and we should do the same for the scheduler driver.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0
MESOS-1424,1.0,"Mesos tests should not rely on echo Triggered by MESOS-1413 I would like to propose changing our tests to not rely on {{echo}} but to use {{printf}} instead.

This seems to be useful as {{echo}} is introducing an extra linefeed after the supplied string whereas {{printf}} does not. The {{-n}} switch preventing that extra linefeed is unfortunately not portable - it is not supported by the builtin {{echo}} of the BSD / OSX {{/bin/sh}}.
",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1459,1.0,"Build failure: Ubuntu 13.10/clang due to missing virtual destructor In file included from launcher/main.cpp:19:
In file included from ./launcher/launcher.hpp:24:
In file included from ../3rdparty/libprocess/include/process/future.hpp:23:
../3rdparty/libprocess/include/process/owned.hpp:188:5: error: delete called on 'mesos::internal::launcher::Operation' that is abstract but has non-virtual destructor [-Werror,-Wdelete-non-virtual-dtor]
    delete t;
    ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:456:8: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::operation>::Data::~Data' requested here
              delete __p;
              ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:768:24: note: in instantiation of function template specialization 'std::__shared_count&lt;2&gt;::__shared_count<process::owned<mesos::internal::launcher::operation>::Data *&gt;' requested here
        : _M_ptr(__p), _M_refcount(__p)
                       ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:919:4: note: in instantiation of function template specialization 'std::__shared_ptr<process::owned<mesos::internal::launcher::operation>::Data, 2&gt;::__shared_ptr<process::owned<mesos::internal::launcher::operation>::Data&gt;' requested here
          __shared_ptr(__p).swap(*this);
          ^
../3rdparty/libprocess/include/process/owned.hpp:68:10: note: in instantiation of function template specialization 'std::__shared_ptr<process::owned<mesos::internal::launcher::operation>::Data, 2&gt;::reset<process::owned<mesos::internal::launcher::operation>::Data&gt;' requested here
    data.reset(new Data(t));
         ^
./launcher/launcher.hpp:101:7: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::operation>::Owned' requested here
  add(process::Owned<operation>(new T()));
      ^
launcher/main.cpp:26:3: note: in instantiation of function template specialization 'mesos::internal::launcher::add<mesos::internal::launcher::shelloperation>' requested here
  launcher::add<launcher::shelloperation>();
  ^
1 error generated.</launcher::shelloperation></mesos::internal::launcher::shelloperation></operation></mesos::internal::launcher::operation></process::owned<mesos::internal::launcher::operation></process::owned<mesos::internal::launcher::operation></process::owned<mesos::internal::launcher::operation></process::owned<mesos::internal::launcher::operation></process::owned<mesos::internal::launcher::operation></mesos::internal::launcher::operation>",,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1469,2.0,"No output from review bot on timeout When the mesos review build times out, likely due to a long-running failing test, we have no output to debug. We should find a way to stream the output from the build instead of waiting for the build to finish.",,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
MESOS-1472,1.0,"Improve child exit if slave dies during executor launch in MC When restarting many slaves there's a reasonable chance that a slave will be restarted between the fork and exec stages of launching an executor in the MesosContainerizer.

The forked child correctly detects this however rather than abort it should safely log and then exit non-zero cleanly.",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1545,1.0,SlaveRecoveryTest/0.MultipleFrameworks is flaky ,"<code>
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
Using temporary directory '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_6dJqxr'
I0626 00:04:39.557339  5450 leveldb.cpp:176] Opened db in 179.857593ms
I0626 00:04:39.565433  5450 leveldb.cpp:183] Compacted db in 8.071041ms
I0626 00:04:39.565457  5450 leveldb.cpp:198] Created db iterator in 4065ns
I0626 00:04:39.565466  5450 leveldb.cpp:204] Seeked to beginning of db in 596ns
I0626 00:04:39.565474  5450 leveldb.cpp:273] Iterated through 0 keys in the db in 396ns
I0626 00:04:39.565490  5450 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0626 00:04:39.565827  5476 recover.cpp:425] Starting replica recovery
I0626 00:04:39.566033  5474 recover.cpp:451] Replica is in EMPTY status
I0626 00:04:39.566504  5474 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0626 00:04:39.566686  5477 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0626 00:04:39.566905  5472 recover.cpp:542] Updating replica status to STARTING
I0626 00:04:39.568307  5471 master.cpp:288] Master 20140626-000439-1032504131-55423-5450 (juno.apache.org) started on 67.195.138.61:55423
I0626 00:04:39.568332  5471 master.cpp:325] Master only allowing authenticated frameworks to register
I0626 00:04:39.568339  5471 master.cpp:330] Master only allowing authenticated slaves to register
I0626 00:04:39.568348  5471 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_6dJqxr/credentials'
I0626 00:04:39.568461  5471 master.cpp:356] Authorization enabled
I0626 00:04:39.568739  5478 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0626 00:04:39.568814  5475 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@67.195.138.61:55423
I0626 00:04:39.569206  5478 master.cpp:1122] The newly elected leader is master@67.195.138.61:55423 with id 20140626-000439-1032504131-55423-5450
I0626 00:04:39.569223  5478 master.cpp:1135] Elected as the leading master!
I0626 00:04:39.569231  5478 master.cpp:953] Recovering from registrar
I0626 00:04:39.569286  5475 registrar.cpp:313] Recovering registrar
I0626 00:04:39.600639  5477 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 33.682136ms
I0626 00:04:39.600661  5477 replica.cpp:320] Persisted replica status to STARTING
I0626 00:04:39.600790  5476 recover.cpp:451] Replica is in STARTING status
I0626 00:04:39.601184  5474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0626 00:04:39.601274  5477 recover.cpp:188] Received a recover response from a replica in STARTING status
I0626 00:04:39.601465  5471 recover.cpp:542] Updating replica status to VOTING
I0626 00:04:39.610605  5471 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 9.076262ms
I0626 00:04:39.610638  5471 replica.cpp:320] Persisted replica status to VOTING
I0626 00:04:39.610683  5471 recover.cpp:556] Successfully joined the Paxos group
I0626 00:04:39.610780  5471 recover.cpp:440] Recover process terminated
I0626 00:04:39.610946  5474 log.cpp:656] Attempting to start the writer
I0626 00:04:39.611486  5475 replica.cpp:474] Replica received implicit promise request with proposal 1
I0626 00:04:39.618924  5475 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 7.418789ms
I0626 00:04:39.618942  5475 replica.cpp:342] Persisted promised to 1
I0626 00:04:39.619220  5476 coordinator.cpp:230] Coordinator attemping to fill missing position
I0626 00:04:39.619763  5476 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0626 00:04:39.627267  5476 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 7.485492ms
I0626 00:04:39.627295  5476 replica.cpp:676] Persisted action at 0
I0626 00:04:39.627822  5473 replica.cpp:508] Replica received write request for position 0
I0626 00:04:39.627861  5473 leveldb.cpp:438] Reading position from leveldb took 17132ns
I0626 00:04:39.635592  5473 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 7.714322ms
I0626 00:04:39.635612  5473 replica.cpp:676] Persisted action at 0
I0626 00:04:39.635797  5473 replica.cpp:655] Replica received learned notice for position 0
I0626 00:04:39.643941  5473 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 8.129347ms
I0626 00:04:39.643960  5473 replica.cpp:676] Persisted action at 0
I0626 00:04:39.643970  5473 replica.cpp:661] Replica learned NOP action at position 0
I0626 00:04:39.644207  5473 log.cpp:672] Writer started with ending position 0
I0626 00:04:39.644625  5471 leveldb.cpp:438] Reading position from leveldb took 9128ns
I0626 00:04:39.646010  5476 registrar.cpp:346] Successfully fetched the registry (0B)
I0626 00:04:39.646044  5476 registrar.cpp:422] Attempting to update the 'registry'
I0626 00:04:39.647274  5471 log.cpp:680] Attempting to append 136 bytes to the log
I0626 00:04:39.647337  5471 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0626 00:04:39.647687  5476 replica.cpp:508] Replica received write request for position 1
I0626 00:04:39.655206  5476 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 7.499736ms
I0626 00:04:39.655225  5476 replica.cpp:676] Persisted action at 1
I0626 00:04:39.655467  5476 replica.cpp:655] Replica received learned notice for position 1
I0626 00:04:39.663534  5476 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 8.054929ms
I0626 00:04:39.663554  5476 replica.cpp:676] Persisted action at 1
I0626 00:04:39.663563  5476 replica.cpp:661] Replica learned APPEND action at position 1
I0626 00:04:39.663890  5478 registrar.cpp:479] Successfully updated 'registry'
I0626 00:04:39.663947  5478 registrar.cpp:372] Successfully recovered registrar
I0626 00:04:39.663969  5476 log.cpp:699] Attempting to truncate the log to 1
I0626 00:04:39.664044  5478 master.cpp:980] Recovered 0 slaves from the Registry (98B) ; allowing 10mins for slaves to re-register
I0626 00:04:39.664057  5476 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0626 00:04:39.664341  5476 replica.cpp:508] Replica received write request for position 2
I0626 00:04:39.664681  5450 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0626 00:04:39.666721  5471 slave.cpp:168] Slave started on 173)@67.195.138.61:55423
I0626 00:04:39.666741  5471 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/credential'
I0626 00:04:39.666806  5471 slave.cpp:268] Slave using credential for: test-principal
I0626 00:04:39.666936  5471 slave.cpp:281] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0626 00:04:39.667000  5471 slave.cpp:326] Slave hostname: juno.apache.org
I0626 00:04:39.667009  5471 slave.cpp:327] Slave checkpoint: true
I0626 00:04:39.667572  5478 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta'
I0626 00:04:39.667703  5475 status_update_manager.cpp:193] Recovering status update manager
I0626 00:04:39.667840  5475 containerizer.cpp:287] Recovering containerizer
I0626 00:04:39.668478  5471 slave.cpp:3128] Finished recovery
I0626 00:04:39.668712  5471 slave.cpp:601] New master detected at master@67.195.138.61:55423
I0626 00:04:39.668738  5471 slave.cpp:677] Authenticating with master master@67.195.138.61:55423
I0626 00:04:39.668802  5471 slave.cpp:650] Detecting new master
I0626 00:04:39.668861  5471 status_update_manager.cpp:167] New master detected at master@67.195.138.61:55423
I0626 00:04:39.668916  5471 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:39.669087  5471 master.cpp:3499] Authenticating slave(173)@67.195.138.61:55423
I0626 00:04:39.669203  5471 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:39.669340  5471 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:39.669359  5471 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:39.669386  5471 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:39.669414  5471 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:39.669457  5471 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:39.669514  5471 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:39.669534  5471 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:39.669543  5471 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:39.669567  5471 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:39.669580  5471 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:39.669589  5471 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.669594  5471 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.669606  5471 authenticator.hpp:376] Authentication success
I0626 00:04:39.669641  5471 authenticatee.hpp:305] Authentication success
I0626 00:04:39.669669  5471 master.cpp:3539] Successfully authenticated principal 'test-principal' at slave(173)@67.195.138.61:55423
I0626 00:04:39.669761  5450 sched.cpp:139] Version: 0.20.0
I0626 00:04:39.669764  5478 slave.cpp:734] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:39.669826  5478 slave.cpp:972] Will retry registration in 3.190666ms if necessary
I0626 00:04:39.669950  5471 master.cpp:2781] Registering slave at slave(173)@67.195.138.61:55423 (juno.apache.org) with id 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.669960  5475 sched.cpp:235] New master detected at master@67.195.138.61:55423
I0626 00:04:39.669977  5475 sched.cpp:285] Authenticating with master master@67.195.138.61:55423
I0626 00:04:39.670073  5471 registrar.cpp:422] Attempting to update the 'registry'
I0626 00:04:39.670114  5475 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:39.670263  5475 master.cpp:3499] Authenticating scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670361  5474 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:39.670506  5475 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:39.670526  5475 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:39.670559  5475 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:39.670590  5475 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:39.670619  5475 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:39.670650  5475 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:39.670670  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:39.670677  5475 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:39.670687  5475 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:39.670697  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:39.670706  5475 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.670712  5475 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.670723  5475 authenticator.hpp:376] Authentication success
I0626 00:04:39.670749  5475 authenticatee.hpp:305] Authentication success
I0626 00:04:39.670773  5475 master.cpp:3539] Successfully authenticated principal 'test-principal' at scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670845  5475 sched.cpp:359] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:39.670858  5475 sched.cpp:478] Sending registration request to master@67.195.138.61:55423
I0626 00:04:39.670899  5475 master.cpp:1241] Received registration request from scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670922  5475 master.cpp:1201] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0626 00:04:39.671052  5475 master.cpp:1300] Registering framework 20140626-000439-1032504131-55423-5450-0000 at scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.671159  5474 sched.cpp:409] Framework registered with 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.671185  5474 sched.cpp:423] Scheduler::registered took 10223ns
I0626 00:04:39.671226  5474 hierarchical_allocator_process.hpp:331] Added framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.671241  5474 hierarchical_allocator_process.hpp:724] No resources available to allocate!
I0626 00:04:39.671247  5474 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8574ns
I0626 00:04:39.671879  5476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.48781ms
I0626 00:04:39.671900  5476 replica.cpp:676] Persisted action at 2
I0626 00:04:39.672164  5471 replica.cpp:655] Replica received learned notice for position 2
I0626 00:04:39.674092  5472 slave.cpp:972] Will retry registration in 25.467893ms if necessary
I0626 00:04:39.674108  5476 master.cpp:2769] Ignoring register slave message from slave(173)@67.195.138.61:55423 (juno.apache.org) as admission is already in progress
I0626 00:04:39.680193  5471 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.01285ms
I0626 00:04:39.680223  5471 leveldb.cpp:401] Deleting ~1 keys from leveldb took 11393ns
I0626 00:04:39.680234  5471 replica.cpp:676] Persisted action at 2
I0626 00:04:39.680245  5471 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0626 00:04:39.680585  5472 log.cpp:680] Attempting to append 326 bytes to the log
I0626 00:04:39.680670  5477 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0626 00:04:39.680953  5474 replica.cpp:508] Replica received write request for position 3
I0626 00:04:39.688521  5474 leveldb.cpp:343] Persisting action (345 bytes) to leveldb took 7.548316ms
I0626 00:04:39.688542  5474 replica.cpp:676] Persisted action at 3
I0626 00:04:39.688750  5474 replica.cpp:655] Replica received learned notice for position 3
I0626 00:04:39.696851  5474 leveldb.cpp:343] Persisting action (347 bytes) to leveldb took 8.088289ms
I0626 00:04:39.696869  5474 replica.cpp:676] Persisted action at 3
I0626 00:04:39.696878  5474 replica.cpp:661] Replica learned APPEND action at position 3
I0626 00:04:39.697268  5474 registrar.cpp:479] Successfully updated 'registry'
I0626 00:04:39.697350  5474 log.cpp:699] Attempting to truncate the log to 3
I0626 00:04:39.697412  5474 master.cpp:2821] Registered slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:39.697423  5474 master.cpp:3967] Adding slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0626 00:04:39.697535  5474 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0626 00:04:39.697618  5474 slave.cpp:768] Registered with master master@67.195.138.61:55423; given slave ID 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.697754  5474 slave.cpp:781] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/slave.info'
I0626 00:04:39.697762  5471 hierarchical_allocator_process.hpp:444] Added slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0626 00:04:39.697845  5471 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.697854  5474 slave.cpp:2325] Received ping from slave-observer(142)@67.195.138.61:55423
I0626 00:04:39.698040  5471 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140626-000439-1032504131-55423-5450-0 in 231333ns
I0626 00:04:39.698051  5474 replica.cpp:508] Replica received write request for position 4
I0626 00:04:39.698118  5471 master.hpp:794] Adding offer 20140626-000439-1032504131-55423-5450-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.698170  5471 master.cpp:3446] Sending 1 offers to framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.698318  5471 sched.cpp:546] Scheduler::resourceOffers took 24371ns
I0626 00:04:39.699718  5477 master.hpp:804] Removing offer 20140626-000439-1032504131-55423-5450-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.699787  5477 master.cpp:2125] Processing reply for offers: [ 20140626-000439-1032504131-55423-5450-0 ] on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.699812  5477 master.cpp:2211] Authorizing framework principal 'test-principal' to launch task 897522cc-4ec5-4904-aed0-00b6b8c41028 as user 'jenkins'
I0626 00:04:39.700160  5477 master.hpp:766] Adding task 897522cc-4ec5-4904-aed0-00b6b8c41028 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.700188  5477 master.cpp:2277] Launching task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:39.700392  5471 slave.cpp:1003] Got assigned task 897522cc-4ec5-4904-aed0-00b6b8c41028 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.700479  5477 hierarchical_allocator_process.hpp:546] Framework 20140626-000439-1032504131-55423-5450-0000 left cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] unused on slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.700505  5471 slave.cpp:3400] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.info'
I0626 00:04:39.700597  5477 hierarchical_allocator_process.hpp:588] Framework 20140626-000439-1032504131-55423-5450-0000 filtered slave 20140626-000439-1032504131-55423-5450-0 for 5secs
I0626 00:04:39.700686  5471 slave.cpp:3407] Checkpointing framework pid 'scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423' t",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1559,5.0,"Allow jenkins build machine to dump stack traces of all threads when timeout Many of the time, when jenkins build times out, we know that some test freezes at some place. However, most of the time, it's very hard to reproduce the deadlock on dev machines.

I would be cool if we can dump the stack traces of all threads when jenkins build times out. Some command like the following:

{noformat}
echo thread apply all bt &gt; tmp; gdb attach `pgrep lt-mesos-tests` &lt; tmp
{noformat}",,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1586,3.0,"Isolate system directories, e.g., per-container /tmp Ideally, tasks should not write outside their sandbox (executor work directory) but pragmatically they may need to write to /tmp, /var/tmp, or some other directory.

1) We should include any such files in disk usage and quota.
2) We should make these ""shared"" directories private, i.e., each container has their own.
3) We should make the lifetime of any such files the same as the executor work directory.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1587,5.0,Report disk usage from MesosContainerizer We should report disk usage for the executor work directory from MesosContainerizer and include in the ResourceStatistics protobuf.,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1594,1.0,"SlaveRecoveryTest/0.ReconcileKillTask is flaky Observed this on Jenkins.

","<code>
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG'
I0714 15:08:43.915114 27216 leveldb.cpp:176] Opened db in 474.695188ms
I0714 15:08:43.933645 27216 leveldb.cpp:183] Compacted db in 18.068942ms
I0714 15:08:43.934129 27216 leveldb.cpp:198] Created db iterator in 7860ns
I0714 15:08:43.934439 27216 leveldb.cpp:204] Seeked to beginning of db in 2560ns
I0714 15:08:43.934779 27216 leveldb.cpp:273] Iterated through 0 keys in the db in 1400ns
I0714 15:08:43.935098 27216 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0714 15:08:43.936027 27238 recover.cpp:425] Starting replica recovery
I0714 15:08:43.936225 27238 recover.cpp:451] Replica is in EMPTY status
I0714 15:08:43.936867 27238 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0714 15:08:43.937049 27238 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0714 15:08:43.937232 27238 recover.cpp:542] Updating replica status to STARTING
I0714 15:08:43.945600 27235 master.cpp:288] Master 20140714-150843-16842879-55850-27216 (quantal) started on 127.0.1.1:55850
I0714 15:08:43.945643 27235 master.cpp:325] Master only allowing authenticated frameworks to register
I0714 15:08:43.945651 27235 master.cpp:330] Master only allowing authenticated slaves to register
I0714 15:08:43.945658 27235 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG/credentials'
I0714 15:08:43.945808 27235 master.cpp:359] Authorization enabled
I0714 15:08:43.946369 27235 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@127.0.1.1:55850
I0714 15:08:43.946419 27235 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0714 15:08:43.946614 27235 master.cpp:1128] The newly elected leader is master@127.0.1.1:55850 with id 20140714-150843-16842879-55850-27216
I0714 15:08:43.946630 27235 master.cpp:1141] Elected as the leading master!
I0714 15:08:43.946637 27235 master.cpp:959] Recovering from registrar
I0714 15:08:43.946707 27235 registrar.cpp:313] Recovering registrar
I0714 15:08:43.957895 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 20.529301ms
I0714 15:08:43.957978 27238 replica.cpp:320] Persisted replica status to STARTING
I0714 15:08:43.958142 27238 recover.cpp:451] Replica is in STARTING status
I0714 15:08:43.958664 27238 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0714 15:08:43.958762 27238 recover.cpp:188] Received a recover response from a replica in STARTING status
I0714 15:08:43.958945 27238 recover.cpp:542] Updating replica status to VOTING
I0714 15:08:43.975685 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.646136ms
I0714 15:08:43.976367 27238 replica.cpp:320] Persisted replica status to VOTING
I0714 15:08:43.976824 27241 recover.cpp:556] Successfully joined the Paxos group
I0714 15:08:43.977072 27242 recover.cpp:440] Recover process terminated
I0714 15:08:43.980590 27236 log.cpp:656] Attempting to start the writer
I0714 15:08:43.981385 27236 replica.cpp:474] Replica received implicit promise request with proposal 1
I0714 15:08:43.999141 27236 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.705787ms
I0714 15:08:43.999222 27236 replica.cpp:342] Persisted promised to 1
I0714 15:08:44.004451 27240 coordinator.cpp:230] Coordinator attemping to fill missing position
I0714 15:08:44.004914 27240 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0714 15:08:44.021456 27240 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 16.499775ms
I0714 15:08:44.021533 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.022006 27240 replica.cpp:508] Replica received write request for position 0
I0714 15:08:44.022043 27240 leveldb.cpp:438] Reading position from leveldb took 21376ns
I0714 15:08:44.035969 27240 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 13.885907ms
I0714 15:08:44.036365 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.040156 27238 replica.cpp:655] Replica received learned notice for position 0
I0714 15:08:44.058082 27238 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.860707ms
I0714 15:08:44.058161 27238 replica.cpp:676] Persisted action at 0
I0714 15:08:44.058176 27238 replica.cpp:661] Replica learned NOP action at position 0
I0714 15:08:44.058526 27238 log.cpp:672] Writer started with ending position 0
I0714 15:08:44.058872 27238 leveldb.cpp:438] Reading position from leveldb took 25660ns
I0714 15:08:44.060556 27238 registrar.cpp:346] Successfully fetched the registry (0B)
I0714 15:08:44.060845 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.062304 27238 log.cpp:680] Attempting to append 120 bytes to the log
I0714 15:08:44.062866 27236 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0714 15:08:44.063154 27236 replica.cpp:508] Replica received write request for position 1
I0714 15:08:44.082813 27236 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 19.61683ms
I0714 15:08:44.082890 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.083256 27236 replica.cpp:655] Replica received learned notice for position 1
I0714 15:08:44.097398 27236 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 14.104796ms
I0714 15:08:44.097475 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.097488 27236 replica.cpp:661] Replica learned APPEND action at position 1
I0714 15:08:44.098569 27236 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.098906 27240 log.cpp:699] Attempting to truncate the log to 1
I0714 15:08:44.099608 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0714 15:08:44.100005 27240 replica.cpp:508] Replica received write request for position 2
I0714 15:08:44.100566 27236 registrar.cpp:372] Successfully recovered registrar
I0714 15:08:44.101227 27239 master.cpp:986] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register
I0714 15:08:44.118376 27240 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 18.329495ms
I0714 15:08:44.118455 27240 replica.cpp:676] Persisted action at 2
I0714 15:08:44.122258 27242 replica.cpp:655] Replica received learned notice for position 2
I0714 15:08:44.137336 27242 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.023553ms
I0714 15:08:44.137460 27242 leveldb.cpp:401] Deleting ~1 keys from leveldb took 55049ns
I0714 15:08:44.137480 27242 replica.cpp:676] Persisted action at 2
I0714 15:08:44.137492 27242 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0714 15:08:44.143729 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.145934 27242 slave.cpp:168] Slave started on 43)@127.0.1.1:55850
I0714 15:08:44.145953 27242 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.146040 27242 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.146136 27242 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.146198 27242 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.146209 27242 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.146708 27242 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.146824 27242 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.146901 27242 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.147228 27242 slave.cpp:3126] Finished recovery
I0714 15:08:44.147531 27242 slave.cpp:599] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147562 27242 slave.cpp:675] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.147614 27242 slave.cpp:648] Detecting new master
I0714 15:08:44.147652 27242 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147691 27242 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.148533 27235 master.cpp:3507] Authenticating slave(43)@127.0.1.1:55850
I0714 15:08:44.148666 27235 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.149054 27242 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.149447 27242 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.149917 27236 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.149974 27236 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.150208 27242 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.150720 27239 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.150749 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.150758 27239 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.150771 27239 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.150781 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.150787 27239 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150792 27239 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150804 27239 authenticator.hpp:376] Authentication success
I0714 15:08:44.150848 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(43)@127.0.1.1:55850
I0714 15:08:44.157696 27242 authenticatee.hpp:305] Authentication success
I0714 15:08:44.158855 27242 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.158936 27242 slave.cpp:970] Will retry registration in 10.352612ms if necessary
I0714 15:08:44.161813 27216 sched.cpp:139] Version: 0.20.0
I0714 15:08:44.162608 27236 sched.cpp:235] New master detected at master@127.0.1.1:55850
I0714 15:08:44.162637 27236 sched.cpp:285] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.162747 27236 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.163506 27239 master.cpp:2789] Registering slave at slave(43)@127.0.1.1:55850 (quantal) with id 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.164086 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.165694 27238 log.cpp:680] Attempting to append 295 bytes to the log
I0714 15:08:44.166231 27240 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0714 15:08:44.166517 27240 replica.cpp:508] Replica received write request for position 3
I0714 15:08:44.167199 27239 master.cpp:3507] Authenticating scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.167867 27241 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.168058 27241 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.168081 27241 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.168107 27241 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.168149 27241 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.168176 27241 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.168215 27241 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.168233 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.168793 27241 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.168820 27241 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.168834 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.168840 27241 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168845 27241 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168858 27241 authenticator.hpp:376] Authentication success
I0714 15:08:44.168895 27241 authenticatee.hpp:305] Authentication success
I0714 15:08:44.168970 27241 sched.cpp:359] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.168987 27241 sched.cpp:478] Sending registration request to master@127.0.1.1:55850
I0714 15:08:44.169426 27239 master.cpp:1239] Queuing up registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 because authentication is still in progress
I0714 15:08:44.169958 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.170440 27241 slave.cpp:970] Will retry registration in 8.76707ms if necessary
I0714 15:08:44.175359 27239 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.175916 27239 master.cpp:1247] Received registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.176298 27239 master.cpp:1207] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0714 15:08:44.176858 27239 master.cpp:1306] Registering framework 20140714-150843-16842879-55850-27216-0000 at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.177408 27236 sched.cpp:409] Framework registered with 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177443 27236 sched.cpp:423] Scheduler::registered took 12527ns
I0714 15:08:44.177727 27241 hierarchical_allocator_process.hpp:331] Added framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177747 27241 hierarchical_allocator_process.hpp:724] No resources available to allocate!
I0714 15:08:44.177753 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8120ns
I0714 15:08:44.179908 27241 slave.cpp:970] Will retry registration in 66.781028ms if necessary
I0714 15:08:44.180007 27241 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.183082 27240 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 16.533189ms
I0714 15:08:44.183125 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.183465 27240 replica.cpp:655] Replica received learned notice for position 3
I0714 15:08:44.203276 27240 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 19.768951ms
I0714 15:08:44.203376 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.203392 27240 replica.cpp:661] Replica learned APPEND action at position 3
I0714 15:08:44.204033 27240 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.204138 27240 log.cpp:699] Attempting to truncate the log to 3
I0714 15:08:44.204221 27240 master.cpp:2829] Registered slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.204241 27240 master.cpp:3975] Adding slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.204387 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0714 15:08:44.204489 27240 slave.cpp:766] Registered with master master@127.0.1.1:55850; given slave ID 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.204745 27240 slave.cpp:779] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/slave.info'
I0714 15:08:44.204954 27240 hierarchical_allocator_process.hpp:444] Added slave 20140714-150843-16842879-55850-27216-0 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0714 15:08:44.205023 27240 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205122 27240 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140714-150843-16842879-55850-27216-0 in 131192ns
I0714 15:08:44.205189 27240 slave.cpp:2323] Received ping from slave-observer(32)@127.0.1.1:55850
I0714 15:08:44.205258 27240 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.205303 27240 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205469 27240 sched.cpp:546] Scheduler::resourceOffers took 23591ns
I0714 15:08:44.206351 27241 replica.cpp:508] Replica received write request for position 4
I0714 15:08:44.208353 27237 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208436 27237 master.cpp:2133] Processing reply for offers: [ 20140714-150843-16842879-55850-27216-0 ] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.208472 27237 master.cpp:2219] Authorizing framework principal 'test-principal' to launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 as user 'jenkins'
I0714 15:08:44.208909 27237 master.hpp:773] Adding task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208947 27237 master.cpp:2285] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.209090 27237 slave.cpp:1001] Got assigned task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.209190 27237 slave.cpp:3398] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.info'
I0714 15:08:44.209413 27237 slave.cpp:3405] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid'
I0714 15:08:44.209710 27237 slave.cpp",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1676,1.0,"ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession is flaky {noformat:title=}
[ RUN      ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession
I0806 01:18:37.648684 17458 zookeeper_test_server.cpp:158] Started ZooKeeperTestServer on port 42069
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1682db0 flags=0
2014-08-06 01:18:37,656:17458(0x2b468638b700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:37,669:17458(0x2b468638b700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0000, negotiated timeout=6000
I0806 01:18:37.671725 17486 group.cpp:313] Group process (group(37)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:37.671758 17486 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:37.671771 17486 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
2014-08-06 01:18:39,101:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:42,441:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 01:18:42.656673 17481 contender.cpp:131] Joining the ZK group
I0806 01:18:42.662484 17484 contender.cpp:247] New candidate (id='0') has entered the contest for leadership
I0806 01:18:42.663754 17481 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:42.663884 17481 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:42.664788 17483 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x15c00f0 flags=0
2014-08-06 01:18:42,668:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:42,672:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000
I0806 01:18:42.673542 17485 group.cpp:313] Group process (group(38)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:42.673570 17485 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:42.673580 17485 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 131ms)
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 115ms)
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms
2014-08-06 01:18:46,799:17458(0x2b4687394700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 1025ms
2014-08-06 01:18:46,800:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 01:18:46.806895 17486 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:46.807857 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:47.669064 17482 contender.cpp:131] Joining the ZK group
2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2989ms
2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:47,671:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000
I0806 01:18:47.682868 17485 contender.cpp:247] New candidate (id='1') has entered the contest for leadership
I0806 01:18:47.683404 17482 group.cpp:313] Group process (group(38)@127.0.1.1:55561) reconnected to ZooKeeper
I0806 01:18:47.683445 17482 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:47.685998 17482 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:47.686142 17482 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:47.687289 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c0421c0 flags=0
2014-08-06 01:18:47,699:17458(0x2b4687de6700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:47,712:17458(0x2b4687de6700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0002, negotiated timeout=6000
I0806 01:18:47.712846 17479 group.cpp:313] Group process (group(39)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:47.712873 17479 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:47.712882 17479 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
I0806 01:18:47.714648 17479 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:47.714759 17479 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:47.716130 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:47,718:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response
I0806 01:18:47.718889 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
2014-08-06 01:18:47,720:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response
I0806 01:18:47.720788 17484 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:47.724663 17458 zookeeper_test_server.cpp:122] Shutdown ZooKeeperTestServer on port 42069
2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 4133ms
2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:49,720:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 33ms
2014-08-06 01:18:49,721:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:49,722:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:50,136:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:50,800:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:51,723:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:51,723:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:52,801:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:52.842553 17481 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0000) expiration
I0806 01:18:52.842911 17481 group.cpp:472] ZooKeeper session expired
I0806 01:18:52.843468 17485 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:52.843483 17485 detector.cpp:138] Detected a new leader: None
I0806 01:18:52.843618 17485 contender.cpp:196] Membership cancelled: 0
2014-08-06 01:18:52,843:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0000

2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1349ad0 flags=0
2014-08-06 01:18:52,844:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:53,473:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:53.720684 17480 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0001) expiration
I0806 01:18:53.721132 17480 group.cpp:472] ZooKeeper session expired
I0806 01:18:53.721516 17479 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:53.721534 17479 detector.cpp:138] Detected a new leader: None
I0806 01:18:53.721696 17479 contender.cpp:196] Membership cancelled: 1
2014-08-06 01:18:53,721:17458(0x2b46798a3700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0001

2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x16a0550 flags=0
2014-08-06 01:18:53,723:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:53,726:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:53.730258 17479 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0002) expiration
I0806 01:18:53.730736 17479 group.cpp:472] ZooKeeper session expired
I0806 01:18:53.731081 17481 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:53.731132 17481 detector.cpp:138] Detected a new leader: None
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0002

2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c035f30 flags=0
2014-08-06 01:18:53,733:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:54,512:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:55,393:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:55,403:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:56,301:17458(0x2b468698f700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 122ms
2014-08-06 01:18:56,302:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:56,809:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:57,939:17458(0x2b4686f92700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 879ms
2014-08-06 01:18:57,940:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 870ms
2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
tests/master_contender_detector_tests.cpp:574: Failure
Failed to wait 10secs for leaderReconnecting
2014-08-06 01:18:57,941:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

I0806 01:18:57.94997",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1683,2.0,Create user doc for framework rate limiting feature Create a Markdown doc under /docs,,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1695,1.0,"The stats.json endpoint on the slave exposes ""registered"" as a string. The slave is currently exposing a string value for the ""registered"" statistic, this should be a number:

","<code>
slave:5051/stats.json
{
  ""recovery_errors"": 0,
  ""registered"": ""1"",
  ""slave/executors_registering"": 0,
  ...
}
<code>

Should be a pretty straightforward fix, looks like this first originated back in 2013:

<code>
commit b8291304e1523eb67ea8dc5f195cdb0d8e7d8348
Author: Vinod Kone <vinod@twitter.com>
Date:   Wed Jul 3 12:37:36 2013 -0700

    Added a ""registered"" key/value pair to slave's stats.json.

    Review: https://reviews.apache.org/r/12256

diff --git a/src/slave/http.cpp b/src/slave/http.cpp
index dc2955f..dd51516 100644
--- a/src/slave/http.cpp
+++ b/src/slave/http.cpp
@@ -281,6 +281,8 @@ Future<response> Slave::Http::stats(const Request&amp; request)
   object.values[""lost_tasks""] = slave.stats.tasks[TASK_LOST];
   object.values[""valid_status_updates""] = slave.stats.validStatusUpdates;
   object.values[""invalid_status_updates""] = slave.stats.invalidStatusUpdates;
+  object.values[""registered""] = slave.master ? ""1"" : ""0"";
+

   return OK(object, request.query.get(""jsonp""));
 }
<code></code></response></vinod@twitter.com></code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-1698,2.0,"make check segfaults Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2331/consoleFull

It looks like the segfault happens before any tests are run. So I suspect somewhere in the setup phase of the tests.

","<code>
mv -f .deps/tests-time_tests.Tpo .deps/tests-time_tests.Po
/bin/bash ./libtool  --tag=CXX   --mode=link g++  -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11   -o tests tests-decoder_tests.o tests-encoder_tests.o tests-http_tests.o tests-io_tests.o tests-main.o tests-mutex_tests.o tests-metrics_tests.o tests-owned_tests.o tests-process_tests.o tests-queue_tests.o tests-reap_tests.o tests-sequence_tests.o tests-shared_tests.o tests-statistics_tests.o tests-subprocess_tests.o tests-system_tests.o tests-timeseries_tests.o tests-time_tests.o 3rdparty/libgmock.la libprocess.la 3rdparty/glog-0.3.3/libglog.la 3rdparty/libry_http_parser.la 3rdparty/libev-4.15/libev.la -lz  -lrt
libtool: link: g++ -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -o tests tests-decoder_tests.o tests-encoder_tests.o tests-http_tests.o tests-io_tests.o tests-main.o tests-mutex_tests.o tests-metrics_tests.o tests-owned_tests.o tests-process_tests.o tests-queue_tests.o tests-reap_tests.o tests-sequence_tests.o tests-shared_tests.o tests-statistics_tests.o tests-subprocess_tests.o tests-system_tests.o tests-timeseries_tests.o tests-time_tests.o  3rdparty/.libs/libgmock.a ./.libs/libprocess.a /home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess/3rdparty/glog-0.3.3/.libs/libglog.a /home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess/3rdparty/libev-4.15/.libs/libev.a 3rdparty/glog-0.3.3/.libs/libglog.a -lpthread 3rdparty/.libs/libry_http_parser.a 3rdparty/libev-4.15/.libs/libev.a -lm -lz -lrt
make[5]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
make  check-local
make[5]: Entering directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
./tests
Note: Google Test filter = 
[==========] Running 0 tests from 0 test cases.
[==========] 0 tests from 0 test cases ran. (0 ms total)
[  PASSED  ] 0 tests.

  YOU HAVE 3 DISABLED TESTS

make[5]: *** [check-local] Segmentation fault
make[5]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
make[4]: *** [check-am] Error 2
make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
make[3]: *** [check-recursive] Error 1
make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
make[2]: *** [check-recursive] Error 1
make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty'
make: *** [check-recursive] Error 1
Build step 'Execute shell' marked build as failure
Sending e-mails to: dev@mesos.apache.org benjamin.hindman@gmail.com dhamon@twitter.com yujie.jay@gmail.com
Finished: FAILURE
<code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1705,2.0,"SubprocessTest.Status sometimes flakes out It's a pretty rare event, but happened more then once.  

[ RUN      ] SubprocessTest.Status
*** Aborted at 1408023909 (unix time) try ""date -d @1408023909"" if you are using GNU date ***
PC: @       0x35700094b1 (unknown)
*** SIGTERM (@0x3e8000041d8) received by PID 16872 (TID 0x7fa9ea426780) from PID 16856; stack trace: ***
    @       0x3570435cb0 (unknown)
    @       0x35700094b1 (unknown)
    @       0x3570009d9f (unknown)
    @       0x357000e726 (unknown)
    @       0x3570015185 (unknown)
    @           0x5ead42 process::childMain()
    @           0x5ece8d std::_Function_handler&lt;&gt;::_M_invoke()
    @           0x5eac9c process::defaultClone()
    @           0x5ebbd4 process::subprocess()
    @           0x55a229 process::subprocess()
    @           0x55a846 process::subprocess()
    @           0x54224c SubprocessTest_Status_Test::TestBody()
    @     0x7fa9ea460323 (unknown)
    @     0x7fa9ea455b67 (unknown)
    @     0x7fa9ea455c0e (unknown)
    @     0x7fa9ea455d15 (unknown)
    @     0x7fa9ea4593a8 (unknown)
    @     0x7fa9ea459647 (unknown)
    @           0x422466 main
    @       0x3570421d65 (unknown)
    @           0x4260bd (unknown)
[       OK ] SubprocessTest.Status (153 ms)",,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1712,2.0,"Automate disallowing of commits mixing mesos/libprocess/stout For various reasons, we don't want to mix mesos/libprocess/stout changes into a single commit. Typically, it is up to the reviewee/reviewer to catch this. 

It wold be nice to automate this via the pre-commit hook .",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
MESOS-1727,2.0,"Configure fails with ../configure: line 18439: syntax error near unexpected token `PROTOBUFPREFIX,' I followed the ""Getting started"" documentation and did:
{noformat}
$ git clone http://git-wip-us.apache.org/repos/asf/mesos.git; cd mesos
$ ./bootstrap
$ mkdir build; cd build
$ ../configure
{noformat}
which aborts with
{noformat}
....
....
checking whether we are using the GNU C compiler... (cached) yes
checking whether gcc accepts -g... (cached) yes
checking for gcc option to accept ISO C89... (cached) none needed
checking dependency style of gcc... (cached) gcc3
../configure: line 18439: syntax error near unexpected token `PROTOBUFPREFIX,'
../configure: line 18439: `  PKG_CHECK_MODULES(PROTOBUFPREFIX,'
{noformat}",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1728,1.0,"Libprocess: report bind parameters on failure When you attempt to start slave or master and there's another one already running there, it is nice to report what are the actual parameters to {{bind}} call that failed.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1748,1.0,"MasterZooKeeperTest.LostZooKeeperCluster is flaky {noformat:title=}
tests/master_tests.cpp:1795: Failure
Failed to wait 10secs for slaveRegisteredMessage
{noformat}

Should have placed the FUTURE_MESSAGE that attempts to capture this messages before the slave starts...",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1758,2.0,"Freezer failure leads to lost task during container destruction. In the past we've seen numerous issues around the freezer. Lately, on the 2.6.44 kernel, we've seen issues where we're unable to freeze the cgroup:

(1) An oom occurs.
(2) No indication of oom in the kernel logs.
(3) The slave is unable to freeze the cgroup.
(4) The task is marked as lost.

{noformat}
I0903 16:46:24.956040 25469 mem.cpp:575] Memory limit exceeded: Requested: 15488MB Maximum Used: 15488MB

MEMORY STATISTICS:
cache 7958691840
rss 8281653248
mapped_file 9474048
pgpgin 4487861
pgpgout 522933
pgfault 2533780
pgmajfault 11
inactive_anon 0
active_anon 8281653248
inactive_file 7631708160
active_file 326852608
unevictable 0
hierarchical_memory_limit 16240345088
total_cache 7958691840
total_rss 8281653248
total_mapped_file 9474048
total_pgpgin 4487861
total_pgpgout 522933
total_pgfault 2533780
total_pgmajfault 11
total_inactive_anon 0
total_active_anon 8281653248
total_inactive_file 7631728640
total_active_file 326852608
total_unevictable 0
I0903 16:46:24.956848 25469 containerizer.cpp:1041] Container bbb9732a-d600-4c1b-b326-846338c608c3 has reached its limit for resource mem(*):1.62403e+10 and will be terminated
I0903 16:46:24.957427 25469 containerizer.cpp:909] Destroying container 'bbb9732a-d600-4c1b-b326-846338c608c3'
I0903 16:46:24.958664 25481 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:34.959529 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:34.962070 25482 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.710848ms
I0903 16:46:34.962658 25479 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:44.963349 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:44.965631 25472 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.588224ms
I0903 16:46:44.966356 25472 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:54.967254 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:56.008447 25475 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 2.15296ms
I0903 16:46:56.009071 25466 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:06.010329 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:06.012538 25467 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.643008ms
I0903 16:47:06.013216 25467 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:12.516348 25480 slave.cpp:3030] Current usage 9.57%. Max allowed age: 5.630238827780799days
I0903 16:47:16.015192 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:16.017043 25486 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.511168ms
I0903 16:47:16.017555 25480 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:19.862746 25483 http.cpp:245] HTTP request for '/slave(1)/stats.json'
E0903 16:47:24.960055 25472 slave.cpp:2557] Termination of executor 'E' of framework '201104070004-0000002563-0000' failed: Failed to destroy container: discarded future
I0903 16:47:24.962054 25472 slave.cpp:2087] Handling status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 from @0.0.0.0:0
I0903 16:47:24.963470 25469 mem.cpp:293] Updated 'memory.soft_limit_in_bytes' to 128MB for container bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:24.963541 25471 cpushare.cpp:338] Updated 'cpu.shares' to 256 (cpus 0.25) for container bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:24.964756 25471 cpushare.cpp:359] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 25ms (cpus 0.25) for container bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:43.406610 25476 status_update_manager.cpp:320] Received status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000
I0903 16:47:43.406991 25476 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000
I0903 16:47:43.410475 25476 status_update_manager.cpp:373] Forwarding status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 to master@<scrubbed_ip>:5050
I0903 16:47:43.439923 25480 status_update_manager.cpp:398] Received status update acknowledgement (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000
I0903 16:47:43.440115 25480 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000
I0903 16:47:43.443595 25480 slave.cpp:2709] Cleaning up executor 'E' of framework 201104070004-0000002563-0000
{noformat}

We should consider avoiding the freezer entirely in favor of a kill(2) loop. We don't have to wait for pid namespaces to remove the freezer dependency.

At the very least, when the freezer fails, we should proceed with a kill(2) loop to ensure that we destroy the cgroup.</scrubbed_ip>",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1760,1.0,"MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2355/changes

","<code>
[ RUN] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_0tw16Z'
I0903 22:04:33.520237 25565 leveldb.cpp:176] Opened db in 49.073821ms
I0903 22:04:33.538331 25565 leveldb.cpp:183] Compacted db in 18.065051ms
I0903 22:04:33.538363 25565 leveldb.cpp:198] Created db iterator in 4826ns
I0903 22:04:33.538377 25565 leveldb.cpp:204] Seeked to beginning of db in 682ns
I0903 22:04:33.538385 25565 leveldb.cpp:273] Iterated through 0 keys in the db in 312ns
I0903 22:04:33.538399 25565 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0903 22:04:33.538624 25593 recover.cpp:425] Starting replica recovery
I0903 22:04:33.538707 25598 recover.cpp:451] Replica is in EMPTY status
I0903 22:04:33.540909 25590 master.cpp:286] Master 20140903-220433-453759884-44122-25565 (hemera.apache.org) started on 140.211.11.27:44122
I0903 22:04:33.540932 25590 master.cpp:332] Master only allowing authenticated frameworks to register
I0903 22:04:33.540936 25590 master.cpp:337] Master only allowing authenticated slaves to register
I0903 22:04:33.540941 25590 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_0tw16Z/credentials'
I0903 22:04:33.541337 25590 master.cpp:366] Authorization enabled
I0903 22:04:33.541508 25597 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0903 22:04:33.542343 25582 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@140.211.11.27:44122
I0903 22:04:33.542445 25592 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0903 22:04:33.543175 25602 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0903 22:04:33.543637 25587 recover.cpp:542] Updating replica status to STARTING
I0903 22:04:33.544256 25579 master.cpp:1205] The newly elected leader is master@140.211.11.27:44122 with id 20140903-220433-453759884-44122-25565
I0903 22:04:33.544275 25579 master.cpp:1218] Elected as the leading master!
I0903 22:04:33.544282 25579 master.cpp:1036] Recovering from registrar
I0903 22:04:33.544401 25579 registrar.cpp:313] Recovering registrar
I0903 22:04:33.558487 25593 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.678563ms
I0903 22:04:33.558531 25593 replica.cpp:320] Persisted replica status to STARTING
I0903 22:04:33.558653 25593 recover.cpp:451] Replica is in STARTING status
I0903 22:04:33.559867 25588 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0903 22:04:33.560057 25602 recover.cpp:188] Received a recover response from a replica in STARTING status
I0903 22:04:33.561280 25584 recover.cpp:542] Updating replica status to VOTING
I0903 22:04:33.576900 25581 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.712427ms
I0903 22:04:33.576942 25581 replica.cpp:320] Persisted replica status to VOTING
I0903 22:04:33.577018 25581 recover.cpp:556] Successfully joined the Paxos group
I0903 22:04:33.577108 25581 recover.cpp:440] Recover process terminated
I0903 22:04:33.577401 25581 log.cpp:656] Attempting to start the writer
I0903 22:04:33.578559 25589 replica.cpp:474] Replica received implicit promise request with proposal 1
I0903 22:04:33.594611 25589 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.029152ms
I0903 22:04:33.594640 25589 replica.cpp:342] Persisted promised to 1
I0903 22:04:33.595391 25584 coordinator.cpp:230] Coordinator attemping to fill missing position
I0903 22:04:33.597512 25588 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0903 22:04:33.613037 25588 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 15.502568ms
I0903 22:04:33.613065 25588 replica.cpp:676] Persisted action at 0
I0903 22:04:33.615435 25585 replica.cpp:508] Replica received write request for position 0
I0903 22:04:33.615463 25585 leveldb.cpp:438] Reading position from leveldb took 10743ns
I0903 22:04:33.630801 25585 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 15.320225ms
I0903 22:04:33.630852 25585 replica.cpp:676] Persisted action at 0
I0903 22:04:33.631126 25585 replica.cpp:655] Replica received learned notice for position 0
I0903 22:04:33.647801 25585 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 16.652951ms
I0903 22:04:33.647830 25585 replica.cpp:676] Persisted action at 0
I0903 22:04:33.647842 25585 replica.cpp:661] Replica learned NOP action at position 0
I0903 22:04:33.648548 25583 log.cpp:672] Writer started with ending position 0
I0903 22:04:33.649235 25583 leveldb.cpp:438] Reading position from leveldb took 25209ns
I0903 22:04:33.650897 25591 registrar.cpp:346] Successfully fetched the registry (0B)
I0903 22:04:33.650930 25591 registrar.cpp:422] Attempting to update the 'registry'
I0903 22:04:33.652861 25601 log.cpp:680] Attempting to append 138 bytes to the log
I0903 22:04:33.653097 25586 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0903 22:04:33.655225 25590 replica.cpp:508] Replica received write request for position 1
I0903 22:04:33.669618 25590 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 14.337486ms
I0903 22:04:33.669663 25590 replica.cpp:676] Persisted action at 1
I0903 22:04:33.670045 25584 replica.cpp:655] Replica received learned notice for position 1
I0903 22:04:34.414243 25584 leveldb.cpp:343] Persisting action (159 bytes) to leveldb took 15.401247ms
I0903 22:04:34.414300 25584 replica.cpp:676] Persisted action at 1
I0903 22:04:34.414316 25584 replica.cpp:661] Replica learned APPEND action at position 1
I0903 22:04:34.414937 25589 registrar.cpp:479] Successfully updated 'registry'
I0903 22:04:34.415069 25585 log.cpp:699] Attempting to truncate the log to 1
I0903 22:04:34.415194 25589 registrar.cpp:372] Successfully recovered registrar
I0903 22:04:34.415284 25589 master.cpp:1063] Recovered 0 slaves from the Registry (100B) ; allowing 10mins for slaves to re-register
I0903 22:04:34.415362 25587 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0903 22:04:34.418926 25597 replica.cpp:508] Replica received write request for position 2
I0903 22:04:34.434321 25597 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 15.368147ms
I0903 22:04:34.434352 25597 replica.cpp:676] Persisted action at 2
I0903 22:04:34.435022 25582 replica.cpp:655] Replica received learned notice for position 2
I0903 22:04:34.450331 25582 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.284486ms
I0903 22:04:34.450387 25582 leveldb.cpp:401] Deleting ~1 keys from leveldb took 25774ns
I0903 22:04:34.450402 25582 replica.cpp:676] Persisted action at 2
I0903 22:04:34.450412 25582 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0903 22:04:34.460691 25565 sched.cpp:137] Version: 0.21.0
I0903 22:04:34.460927 25582 sched.cpp:233] New master detected at master@140.211.11.27:44122
I0903 22:04:34.460948 25582 sched.cpp:283] Authenticating with master master@140.211.11.27:44122
I0903 22:04:34.461359 25582 authenticatee.hpp:128] Creating new client SASL connection
I0903 22:04:34.461647 25582 master.cpp:3637] Authenticating scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.461801 25598 authenticator.hpp:156] Creating new server SASL connection
I0903 22:04:34.462172 25598 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0903 22:04:34.462185 25598 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0903 22:04:34.462257 25598 authenticator.hpp:262] Received SASL authentication start
I0903 22:04:34.462323 25598 authenticator.hpp:384] Authentication requires more steps
I0903 22:04:34.462345 25598 authenticatee.hpp:265] Received SASL authentication step
I0903 22:04:34.462417 25598 authenticator.hpp:290] Received SASL authentication step
I0903 22:04:34.462522 25598 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0903 22:04:34.462529 25598 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0903 22:04:34.462538 25598 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0903 22:04:34.462543 25598 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0903 22:04:34.462548 25598 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0903 22:04:34.462550 25598 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0903 22:04:34.462558 25598 authenticator.hpp:376] Authentication success
I0903 22:04:34.462635 25598 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.462687 25590 authenticatee.hpp:305] Authentication success
I0903 22:04:34.463219 25588 sched.cpp:357] Successfully authenticated with master master@140.211.11.27:44122
I0903 22:04:34.463243 25588 sched.cpp:476] Sending registration request to master@140.211.11.27:44122
I0903 22:04:34.463307 25588 master.cpp:1324] Received registration request from scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.463330 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0903 22:04:34.463412 25588 master.cpp:1383] Registering framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.463577 25598 sched.cpp:407] Framework registered with 20140903-220433-453759884-44122-25565-0000
I0903 22:04:34.463728 25587 hierarchical_allocator_process.hpp:329] Added framework 20140903-220433-453759884-44122-25565-0000
I0903 22:04:34.463739 25587 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0903 22:04:34.463743 25587 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5016ns
I0903 22:04:34.463755 25598 sched.cpp:421] Scheduler::registered took 165035ns
I0903 22:04:34.465558 25583 sched.cpp:227] Scheduler::disconnected took 6254ns
I0903 22:04:34.465566 25583 sched.cpp:233] New master detected at master@140.211.11.27:44122
I0903 22:04:34.465575 25583 sched.cpp:283] Authenticating with master master@140.211.11.27:44122
I0903 22:04:34.465642 25583 authenticatee.hpp:128] Creating new client SASL connection
I0903 22:04:34.465790 25583 master.cpp:1680] Deactivating framework 20140903-220433-453759884-44122-25565-0000
I0903 22:04:34.465850 25583 master.cpp:3637] Authenticating scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.465879 25601 hierarchical_allocator_process.hpp:405] Deactivated framework 20140903-220433-453759884-44122-25565-0000
I0903 22:04:34.466047 25600 authenticator.hpp:156] Creating new server SASL connection
I0903 22:04:34.466315 25600 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0903 22:04:34.466326 25600 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0903 22:04:34.466346 25600 authenticator.hpp:262] Received SASL authentication start
I0903 22:04:34.466418 25600 authenticator.hpp:384] Authentication requires more steps
I0903 22:04:34.466436 25600 authenticatee.hpp:265] Received SASL authentication step
I0903 22:04:34.466475 25600 authenticator.hpp:290] Received SASL authentication step
I0903 22:04:34.466486 25600 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0903 22:04:34.466491 25600 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0903 22:04:34.466496 25600 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0903 22:04:34.466502 25600 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0903 22:04:34.466506 25600 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0903 22:04:34.466509 25600 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0903 22:04:34.466516 25600 authenticator.hpp:376] Authentication success
I0903 22:04:34.466596 25588 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.466629 25597 authenticatee.hpp:305] Authentication success
I0903 22:04:34.467062 25594 sched.cpp:357] Successfully authenticated with master master@140.211.11.27:44122
I0903 22:04:34.467077 25594 sched.cpp:476] Sending registration request to master@140.211.11.27:44122
I0903 22:04:34.467190 25588 master.cpp:1448] Received re-registration request from framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:36.368134 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0903 22:04:34.542999 25594 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0903 22:04:35.463639 25582 sched.cpp:476] Sending registration request to master@140.211.11.27:44122
I0903 22:04:36.368185 25594 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.825177748secs
I0903 22:04:36.368302 25588 master.cpp:1448] Received re-registration request from framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:36.368330 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0903 22:04:36.368388 25582 sched.cpp:476] Sending registration request to master@140.211.11.27:44122
: Failure
Mock function called more times than expected - returning default value.
    Function call: authorize(@0x2ba11964c1b0 40-byte object <d0-ed 00-00="""" 00-3c="""" 00-6c="""" 01-3c="""" 03-00="""" 30-20="""" 39-16="""" a1-2b="""">)
    The mock function has no default action set, and its return type has no default value set.
*** Aborted at 1409781876 (unix time) try ""date -d @1409781876"" if you are using GNU date ***
I0903 22:04:36.368913 25598 sched.cpp:745] Stopping framework '20140903-220433-453759884-44122-25565-0000'
PC: @     0x2ba117a990d5 (unknown)
*** SIGABRT (@0x3ea000063dd) received by PID 25565 (TID 0x2ba11964d700) from PID 25565; stack trace: ***
    @     0x2ba117854cb0 (unknown)
    @     0x2ba117a990d5 (unknown)
    @     0x2ba117a9c83b (unknown)
    @           0x9cba9d testing::internal::GoogleTestFailureReporter::ReportFailure()
    @           0x790091 testing::internal::FunctionMockerBase&lt;&gt;::PerformDefaultAction()
    @           0x790166 testing::internal::FunctionMockerBase&lt;&gt;::UntypedPerformDefaultAction()
    @           0x9c3daa testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0x787279 mesos::internal::tests::MockAuthorizer::authorize()
    @     0x2ba1157c133d mesos::internal::master::Master::validate()
    @     0x2ba1157c2b7a mesos::internal::master::Master::reregisterFramework()
    @     0x2ba1157e0038 ProtobufProcess&lt;&gt;::handler2&lt;&gt;()
    @     0x2ba1157dde89 std::tr1::_Function_handler&lt;&gt;::_M_invoke()
    @     0x2ba1157b15f7 mesos::internal::master::Master::_visit()
    @     0x2ba1157bfa3e mesos::internal::master::Master::visit()
    @     0x2ba115caf5e7 process::ProcessManager::resume()
    @     0x2ba115cb027c process::schedule()
    @     0x2ba11784ce9a start_thread
    @     0x2ba117b5731d (unknown)
<code></code></d0-ed></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1765,5.0,"Use PID namespace to avoid freezing cgroup There is some known kernel issue when we freeze the whole cgroup upon OOM. Mesos probably can just use PID namespace so that we will only need to kill the ""init"" of the pid namespace, instead of freezing all the processes and killing them one by one. But I am not quite sure if this would break the existing code.",,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1766,2.0,MasterAuthorizationTest.DuplicateRegistration test is flaky ,"<code>
[ RUN      ] MasterAuthorizationTest.DuplicateRegistration
Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateRegistration_pVJg7m'
I0905 15:53:16.398993 25769 leveldb.cpp:176] Opened db in 2.601036ms
I0905 15:53:16.399566 25769 leveldb.cpp:183] Compacted db in 546216ns
I0905 15:53:16.399590 25769 leveldb.cpp:198] Created db iterator in 2787ns
I0905 15:53:16.399605 25769 leveldb.cpp:204] Seeked to beginning of db in 500ns
I0905 15:53:16.399617 25769 leveldb.cpp:273] Iterated through 0 keys in the db in 185ns
I0905 15:53:16.399633 25769 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0905 15:53:16.399817 25786 recover.cpp:425] Starting replica recovery
I0905 15:53:16.399952 25793 recover.cpp:451] Replica is in EMPTY status
I0905 15:53:16.400683 25795 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0905 15:53:16.400795 25787 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0905 15:53:16.401005 25783 recover.cpp:542] Updating replica status to STARTING
I0905 15:53:16.401470 25786 master.cpp:286] Master 20140905-155316-3125920579-49188-25769 (penates.apache.org) started on 67.195.81.186:49188
I0905 15:53:16.401521 25786 master.cpp:332] Master only allowing authenticated frameworks to register
I0905 15:53:16.401533 25786 master.cpp:337] Master only allowing authenticated slaves to register
I0905 15:53:16.401543 25786 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateRegistration_pVJg7m/credentials'
I0905 15:53:16.401558 25793 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 474683ns
I0905 15:53:16.401582 25793 replica.cpp:320] Persisted replica status to STARTING
I0905 15:53:16.401667 25793 recover.cpp:451] Replica is in STARTING status
I0905 15:53:16.401669 25786 master.cpp:366] Authorization enabled
I0905 15:53:16.401898 25795 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0905 15:53:16.401936 25796 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.186:49188
I0905 15:53:16.402160 25784 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0905 15:53:16.402333 25790 master.cpp:1205] The newly elected leader is master@67.195.81.186:49188 with id 20140905-155316-3125920579-49188-25769
I0905 15:53:16.402359 25790 master.cpp:1218] Elected as the leading master!
I0905 15:53:16.402371 25790 master.cpp:1036] Recovering from registrar
I0905 15:53:16.402472 25798 registrar.cpp:313] Recovering registrar
I0905 15:53:16.402529 25791 recover.cpp:188] Received a recover response from a replica in STARTING status
I0905 15:53:16.402782 25788 recover.cpp:542] Updating replica status to VOTING
I0905 15:53:16.403002 25795 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 116403ns
I0905 15:53:16.403020 25795 replica.cpp:320] Persisted replica status to VOTING
I0905 15:53:16.403081 25791 recover.cpp:556] Successfully joined the Paxos group
I0905 15:53:16.403197 25791 recover.cpp:440] Recover process terminated
I0905 15:53:16.403388 25796 log.cpp:656] Attempting to start the writer
I0905 15:53:16.403993 25784 replica.cpp:474] Replica received implicit promise request with proposal 1
I0905 15:53:16.404147 25784 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 132156ns
I0905 15:53:16.404167 25784 replica.cpp:342] Persisted promised to 1
I0905 15:53:16.404542 25795 coordinator.cpp:230] Coordinator attemping to fill missing position
I0905 15:53:16.405498 25787 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0905 15:53:16.405868 25787 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 347231ns
I0905 15:53:16.405886 25787 replica.cpp:676] Persisted action at 0
I0905 15:53:16.406553 25788 replica.cpp:508] Replica received write request for position 0
I0905 15:53:16.406582 25788 leveldb.cpp:438] Reading position from leveldb took 11402ns
I0905 15:53:16.529067 25788 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 535803ns
I0905 15:53:16.529088 25788 replica.cpp:676] Persisted action at 0
I0905 15:53:16.529355 25784 replica.cpp:655] Replica received learned notice for position 0
I0905 15:53:16.529784 25784 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 406036ns
I0905 15:53:16.529806 25784 replica.cpp:676] Persisted action at 0
I0905 15:53:16.529817 25784 replica.cpp:661] Replica learned NOP action at position 0
I0905 15:53:16.530108 25783 log.cpp:672] Writer started with ending position 0
I0905 15:53:16.530597 25792 leveldb.cpp:438] Reading position from leveldb took 14594ns
I0905 15:53:16.532060 25787 registrar.cpp:346] Successfully fetched the registry (0B)
I0905 15:53:16.532091 25787 registrar.cpp:422] Attempting to update the 'registry'
I0905 15:53:16.533537 25785 log.cpp:680] Attempting to append 140 bytes to the log
I0905 15:53:16.533596 25785 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0905 15:53:16.533998 25798 replica.cpp:508] Replica received write request for position 1
I0905 15:53:16.534397 25798 leveldb.cpp:343] Persisting action (159 bytes) to leveldb took 372452ns
I0905 15:53:16.534416 25798 replica.cpp:676] Persisted action at 1
I0905 15:53:16.534808 25793 replica.cpp:655] Replica received learned notice for position 1
I0905 15:53:16.534996 25793 leveldb.cpp:343] Persisting action (161 bytes) to leveldb took 164609ns
I0905 15:53:16.535014 25793 replica.cpp:676] Persisted action at 1
I0905 15:53:16.535025 25793 replica.cpp:661] Replica learned APPEND action at position 1
I0905 15:53:16.535368 25784 registrar.cpp:479] Successfully updated 'registry'
I0905 15:53:16.535419 25784 registrar.cpp:372] Successfully recovered registrar
I0905 15:53:16.535452 25785 log.cpp:699] Attempting to truncate the log to 1
I0905 15:53:16.535555 25791 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0905 15:53:16.535553 25792 master.cpp:1063] Recovered 0 slaves from the Registry (102B) ; allowing 10mins for slaves to re-register
I0905 15:53:16.536038 25784 replica.cpp:508] Replica received write request for position 2
I0905 15:53:16.536166 25784 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 101619ns
I0905 15:53:16.536185 25784 replica.cpp:676] Persisted action at 2
I0905 15:53:16.536497 25791 replica.cpp:655] Replica received learned notice for position 2
I0905 15:53:16.536633 25791 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 109281ns
I0905 15:53:16.536664 25791 leveldb.cpp:401] Deleting ~1 keys from leveldb took 14164ns
I0905 15:53:16.536677 25791 replica.cpp:676] Persisted action at 2
I0905 15:53:16.536689 25791 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0905 15:53:16.548408 25769 sched.cpp:137] Version: 0.21.0
I0905 15:53:16.548627 25792 sched.cpp:233] New master detected at master@67.195.81.186:49188
I0905 15:53:16.548653 25792 sched.cpp:283] Authenticating with master master@67.195.81.186:49188
I0905 15:53:16.548857 25797 authenticatee.hpp:128] Creating new client SASL connection
I0905 15:53:16.548950 25797 master.cpp:3637] Authenticating scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.549041 25797 authenticator.hpp:156] Creating new server SASL connection
I0905 15:53:16.549120 25797 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0905 15:53:16.549141 25797 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0905 15:53:16.549180 25797 authenticator.hpp:262] Received SASL authentication start
I0905 15:53:16.549229 25797 authenticator.hpp:384] Authentication requires more steps
I0905 15:53:16.549268 25797 authenticatee.hpp:265] Received SASL authentication step
I0905 15:53:16.549351 25787 authenticator.hpp:290] Received SASL authentication step
I0905 15:53:16.549378 25787 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0905 15:53:16.549391 25787 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0905 15:53:16.549403 25787 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0905 15:53:16.549415 25787 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0905 15:53:16.549424 25787 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0905 15:53:16.549432 25787 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0905 15:53:16.549448 25787 authenticator.hpp:376] Authentication success
I0905 15:53:16.549489 25787 authenticatee.hpp:305] Authentication success
I0905 15:53:16.549525 25787 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.549669 25783 sched.cpp:357] Successfully authenticated with master master@67.195.81.186:49188
I0905 15:53:16.549690 25783 sched.cpp:476] Sending registration request to master@67.195.81.186:49188
I0905 15:53:16.549751 25787 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.549782 25787 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0905 15:53:16.551250 25791 sched.cpp:233] New master detected at master@67.195.81.186:49188
I0905 15:53:16.551273 25791 sched.cpp:283] Authenticating with master master@67.195.81.186:49188
I0905 15:53:16.551357 25788 authenticatee.hpp:128] Creating new client SASL connection
I0905 15:53:16.551456 25791 master.cpp:3637] Authenticating scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.551553 25788 authenticator.hpp:156] Creating new server SASL connection
I0905 15:53:16.551673 25786 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0905 15:53:16.551697 25786 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0905 15:53:16.551755 25792 authenticator.hpp:262] Received SASL authentication start
I0905 15:53:16.551808 25792 authenticator.hpp:384] Authentication requires more steps
I0905 15:53:16.551856 25792 authenticatee.hpp:265] Received SASL authentication step
I0905 15:53:16.551920 25786 authenticator.hpp:290] Received SASL authentication step
I0905 15:53:16.551949 25786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0905 15:53:16.551966 25786 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0905 15:53:16.551985 25786 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0905 15:53:16.551997 25786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0905 15:53:16.552006 25786 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0905 15:53:16.552014 25786 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0905 15:53:16.552031 25786 authenticator.hpp:376] Authentication success
I0905 15:53:16.552081 25792 authenticatee.hpp:305] Authentication success
I0905 15:53:16.552100 25786 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.552249 25792 sched.cpp:357] Successfully authenticated with master master@67.195.81.186:49188
I0905 15:53:17.402861 25793 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0905 15:53:18.874348 25792 sched.cpp:476] Sending registration request to master@67.195.81.186:49188
I0905 15:53:18.874364 25793 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.471501003secs
I0905 15:53:18.874420 25792 sched.cpp:476] Sending registration request to master@67.195.81.186:49188
I0905 15:53:18.874451 25793 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:18.874480 25793 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0905 15:53:18.874565 25793 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:18.874588 25793 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
: Failure
Mock function called more times than expected - returning default value.
    Function call: authorize(@0x2b9ed7fe9350 40-byte object &lt;90-BA B4-D4 9E-2B 00-00 00-00 00-00 00-00 00-00 A0-FA 06-F4 9E-2B 00-00 80-17 09-F4 9E-2B 00-00 00-00 00-00 03-00 00-00&gt;)
    The mock function has no default action set, and its return type has no default value set.
*** Aborted at 1409932398 (unix time) try ""date -d @1409932398"" if you are using GNU date ***
PC: @     0x2b9ed6233f79 (unknown)
*** SIGABRT (@0x95c000064a9) received by PID 25769 (TID 0x2b9ed7fea700) from PID 25769; stack trace: ***
    @     0x2b9ed5fef340 (unknown)
    @     0x2b9ed6233f79 (unknown)
    @     0x2b9ed6237388 (unknown)
    @           0x93a5ec testing::internal::GoogleTestFailureReporter::ReportFailure()
    @           0x7296c5 testing::internal::FunctionMockerBase&lt;&gt;::UntypedPerformDefaultAction()
    @           0x933094 testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0x71fbde mesos::internal::tests::MockAuthorizer::authorize()
    @     0x2b9ed4038caf mesos::internal::master::Master::validate()
    @     0x2b9ed4039763 mesos::internal::master::Master::registerFramework()
    @     0x2b9ed40a0c0f ProtobufProcess&lt;&gt;::handler1&lt;&gt;()
    @     0x2b9ed4050c57 std::_Function_handler&lt;&gt;::_M_invoke()
    @     0x2b9ed407d202 ProtobufProcess&lt;&gt;::visit()
    @     0x2b9ed402af1a mesos::internal::master::Master::_visit()
    @     0x2b9ed4037eb8 mesos::internal::master::Master::visit()
    @     0x2b9ed44cb792 process::ProcessManager::resume()
    @     0x2b9ed44cba9c process::schedule()
    @     0x2b9ed5fe7182 start_thread
    @     0x2b9ed62f830d (unknown)
<code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1778,3.0,"Provide an option to validate flag value in stout/flags.  Currently we can provide the default value for a flag, but cannot check if the flag is set to a reasonable value and, e.g., issue a warning. Passing an optional lambda checker to {{FlagBase::add()}} can be a possible solution.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-1782,1.0,"AllocatorTest/0.FrameworkExited is flaky {noformat:title=}
[ RUN      ] AllocatorTest/0.FrameworkExited
Using temporary directory '/tmp/AllocatorTest_0_FrameworkExited_B6WZng'
I0909 08:02:35.116555 18112 leveldb.cpp:176] Opened db in 31.64686ms
I0909 08:02:35.126065 18112 leveldb.cpp:183] Compacted db in 9.449823ms
I0909 08:02:35.126118 18112 leveldb.cpp:198] Created db iterator in 5858ns
I0909 08:02:35.126137 18112 leveldb.cpp:204] Seeked to beginning of db in 1136ns
I0909 08:02:35.126150 18112 leveldb.cpp:273] Iterated through 0 keys in the db in 560ns
I0909 08:02:35.126178 18112 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0909 08:02:35.126502 18133 recover.cpp:425] Starting replica recovery
I0909 08:02:35.126601 18133 recover.cpp:451] Replica is in EMPTY status
I0909 08:02:35.127012 18133 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0909 08:02:35.127094 18133 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0909 08:02:35.127223 18133 recover.cpp:542] Updating replica status to STARTING
I0909 08:02:35.226631 18133 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 99.308134ms
I0909 08:02:35.226690 18133 replica.cpp:320] Persisted replica status to STARTING
I0909 08:02:35.226812 18131 recover.cpp:451] Replica is in STARTING status
I0909 08:02:35.227246 18131 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0909 08:02:35.227308 18131 recover.cpp:188] Received a recover response from a replica in STARTING status
I0909 08:02:35.227409 18131 recover.cpp:542] Updating replica status to VOTING
I0909 08:02:35.228540 18129 master.cpp:286] Master 20140909-080235-16842879-44005-18112 (precise) started on 127.0.1.1:44005
I0909 08:02:35.228593 18129 master.cpp:332] Master only allowing authenticated frameworks to register
I0909 08:02:35.228607 18129 master.cpp:337] Master only allowing authenticated slaves to register
I0909 08:02:35.228620 18129 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_FrameworkExited_B6WZng/credentials'
I0909 08:02:35.228754 18129 master.cpp:366] Authorization enabled
I0909 08:02:35.229560 18129 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 08:02:35.229933 18129 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@127.0.1.1:44005
I0909 08:02:35.230057 18127 master.cpp:1212] The newly elected leader is master@127.0.1.1:44005 with id 20140909-080235-16842879-44005-18112
I0909 08:02:35.230129 18127 master.cpp:1225] Elected as the leading master!
I0909 08:02:35.230144 18127 master.cpp:1043] Recovering from registrar
I0909 08:02:35.230257 18127 registrar.cpp:313] Recovering registrar
I0909 08:02:35.232461 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 4.999384ms
I0909 08:02:35.232489 18131 replica.cpp:320] Persisted replica status to VOTING
I0909 08:02:35.232544 18131 recover.cpp:556] Successfully joined the Paxos group
I0909 08:02:35.232611 18131 recover.cpp:440] Recover process terminated
I0909 08:02:35.232727 18131 log.cpp:656] Attempting to start the writer
I0909 08:02:35.233012 18131 replica.cpp:474] Replica received implicit promise request with proposal 1
I0909 08:02:35.238785 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.749504ms
I0909 08:02:35.238818 18131 replica.cpp:342] Persisted promised to 1
I0909 08:02:35.244056 18131 coordinator.cpp:230] Coordinator attemping to fill missing position
I0909 08:02:35.244580 18131 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0909 08:02:35.250143 18131 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.382351ms
I0909 08:02:35.250319 18131 replica.cpp:676] Persisted action at 0
I0909 08:02:35.250901 18131 replica.cpp:508] Replica received write request for position 0
I0909 08:02:35.251137 18131 leveldb.cpp:438] Reading position from leveldb took 18689ns
I0909 08:02:35.256597 18131 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.274169ms
I0909 08:02:35.256764 18131 replica.cpp:676] Persisted action at 0
I0909 08:02:35.263712 18126 replica.cpp:655] Replica received learned notice for position 0
I0909 08:02:35.269613 18126 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.417225ms
I0909 08:02:35.351641 18126 replica.cpp:676] Persisted action at 0
I0909 08:02:35.351655 18126 replica.cpp:661] Replica learned NOP action at position 0
I0909 08:02:35.351889 18126 log.cpp:672] Writer started with ending position 0
I0909 08:02:35.352165 18126 leveldb.cpp:438] Reading position from leveldb took 25215ns
I0909 08:02:35.353163 18126 registrar.cpp:346] Successfully fetched the registry (0B)
I0909 08:02:35.353185 18126 registrar.cpp:422] Attempting to update the 'registry'
I0909 08:02:35.354152 18126 log.cpp:680] Attempting to append 120 bytes to the log
I0909 08:02:35.354195 18126 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0909 08:02:35.354416 18126 replica.cpp:508] Replica received write request for position 1
I0909 08:02:35.351579 18127 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0909 08:02:35.354558 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 2.984795ms
I0909 08:02:35.360254 18126 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.811986ms
I0909 08:02:35.360285 18126 replica.cpp:676] Persisted action at 1
I0909 08:02:35.364126 18132 replica.cpp:655] Replica received learned notice for position 1
I0909 08:02:35.369856 18132 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 5.702756ms
I0909 08:02:35.369899 18132 replica.cpp:676] Persisted action at 1
I0909 08:02:35.369910 18132 replica.cpp:661] Replica learned APPEND action at position 1
I0909 08:02:35.370209 18132 registrar.cpp:479] Successfully updated 'registry'
I0909 08:02:35.370311 18132 registrar.cpp:372] Successfully recovered registrar
I0909 08:02:35.370477 18132 log.cpp:699] Attempting to truncate the log to 1
I0909 08:02:35.370553 18132 master.cpp:1070] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register
I0909 08:02:35.370594 18132 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0909 08:02:35.371201 18127 replica.cpp:508] Replica received write request for position 2
I0909 08:02:35.376760 18127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.264501ms
I0909 08:02:35.377105 18127 replica.cpp:676] Persisted action at 2
I0909 08:02:35.377770 18127 replica.cpp:655] Replica received learned notice for position 2
I0909 08:02:35.383363 18127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.272769ms
I0909 08:02:35.383818 18127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 28148ns
I0909 08:02:35.384137 18127 replica.cpp:676] Persisted action at 2
I0909 08:02:35.384399 18127 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0909 08:02:35.396512 18127 slave.cpp:167] Slave started on 64)@127.0.1.1:44005
I0909 08:02:35.654770 18131 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0909 08:02:35.654847 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 104933ns
I0909 08:02:35.654974 18127 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/credential'
I0909 08:02:35.655097 18127 slave.cpp:274] Slave using credential for: test-principal
I0909 08:02:35.655203 18127 slave.cpp:287] Slave resources: cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000]
I0909 08:02:35.655274 18127 slave.cpp:315] Slave hostname: precise
I0909 08:02:35.655285 18127 slave.cpp:316] Slave checkpoint: false
I0909 08:02:35.655804 18127 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/meta'
I0909 08:02:35.655913 18127 status_update_manager.cpp:193] Recovering status update manager
I0909 08:02:35.656005 18127 slave.cpp:3202] Finished recovery
I0909 08:02:35.656251 18127 slave.cpp:598] New master detected at master@127.0.1.1:44005
I0909 08:02:35.656285 18127 slave.cpp:672] Authenticating with master master@127.0.1.1:44005
I0909 08:02:35.656325 18127 slave.cpp:645] Detecting new master
I0909 08:02:35.656358 18127 status_update_manager.cpp:167] New master detected at master@127.0.1.1:44005
I0909 08:02:35.656389 18127 authenticatee.hpp:128] Creating new client SASL connection
I0909 08:02:35.656563 18127 master.cpp:3653] Authenticating slave(64)@127.0.1.1:44005
I0909 08:02:35.656651 18127 authenticator.hpp:156] Creating new server SASL connection
I0909 08:02:35.656770 18127 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 08:02:35.656796 18127 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 08:02:35.656822 18127 authenticator.hpp:262] Received SASL authentication start
I0909 08:02:35.656858 18127 authenticator.hpp:384] Authentication requires more steps
I0909 08:02:35.656883 18127 authenticatee.hpp:265] Received SASL authentication step
I0909 08:02:35.656924 18127 authenticator.hpp:290] Received SASL authentication step
I0909 08:02:35.656960 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 08:02:35.656971 18127 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 08:02:35.656982 18127 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 08:02:35.656997 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 08:02:35.657004 18127 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.657008 18127 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.657019 18127 authenticator.hpp:376] Authentication success
I0909 08:02:35.657047 18127 authenticatee.hpp:305] Authentication success
I0909 08:02:35.657073 18127 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(64)@127.0.1.1:44005
I0909 08:02:35.657145 18127 slave.cpp:729] Successfully authenticated with master master@127.0.1.1:44005
I0909 08:02:35.657183 18127 slave.cpp:980] Will retry registration in 19.238717ms if necessary
I0909 08:02:35.657276 18128 master.cpp:2843] Registering slave at slave(64)@127.0.1.1:44005 (precise) with id 20140909-080235-16842879-44005-18112-0
I0909 08:02:35.657389 18128 registrar.cpp:422] Attempting to update the 'registry'
I0909 08:02:35.658382 18130 log.cpp:680] Attempting to append 295 bytes to the log
I0909 08:02:35.658432 18130 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0909 08:02:35.658635 18130 replica.cpp:508] Replica received write request for position 3
I0909 08:02:35.660959 18112 sched.cpp:137] Version: 0.21.0
I0909 08:02:35.661093 18126 sched.cpp:233] New master detected at master@127.0.1.1:44005
I0909 08:02:35.661111 18126 sched.cpp:283] Authenticating with master master@127.0.1.1:44005
I0909 08:02:35.661175 18126 authenticatee.hpp:128] Creating new client SASL connection
I0909 08:02:35.661306 18126 master.cpp:3653] Authenticating scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005
I0909 08:02:35.661376 18126 authenticator.hpp:156] Creating new server SASL connection
I0909 08:02:35.661466 18126 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 08:02:35.661483 18126 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 08:02:35.661504 18126 authenticator.hpp:262] Received SASL authentication start
I0909 08:02:35.661530 18126 authenticator.hpp:384] Authentication requires more steps
I0909 08:02:35.661552 18126 authenticatee.hpp:265] Received SASL authentication step
I0909 08:02:35.661579 18126 authenticator.hpp:290] Received SASL authentication step
I0909 08:02:35.661592 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 08:02:35.661598 18126 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 08:02:35.661607 18126 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 08:02:35.661613 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 08:02:35.661619 18126 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.661623 18126 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.661633 18126 authenticator.hpp:376] Authentication success
I0909 08:02:35.661653 18126 authenticatee.hpp:305] Authentication success
I0909 08:02:35.661672 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005
I0909 08:02:35.661730 18126 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005
I0909 08:02:35.661741 18126 sched.cpp:476] Sending registration request to master@127.0.1.1:44005
I0909 08:02:35.661782 18126 master.cpp:1331] Received registration request from scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005
I0909 08:02:35.661798 18126 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0909 08:02:35.661917 18126 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0000 at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005
I0909 08:02:35.662017 18126 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.662039 18126 sched.cpp:421] Scheduler::registered took 9070ns
I0909 08:02:35.662119 18126 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.662130 18126 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0909 08:02:35.662135 18126 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5558ns
I0909 08:02:35.672230 18130 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 13.567526ms
I0909 08:02:35.672268 18130 replica.cpp:676] Persisted action at 3
I0909 08:02:35.672483 18130 replica.cpp:655] Replica received learned notice for position 3
I0909 08:02:35.677322 18132 slave.cpp:980] Will retry registration in 14.890338ms if necessary
I0909 08:02:35.677399 18132 master.cpp:2831] Ignoring register slave message from slave(64)@127.0.1.1:44005 (precise) as admission is already in progress
I0909 08:02:35.680881 18130 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 8.376798ms
I0909 08:02:35.680908 18130 replica.cpp:676] Persisted action at 3
I0909 08:02:35.680917 18130 replica.cpp:661] Replica learned APPEND action at position 3
I0909 08:02:35.681252 18130 registrar.cpp:479] Successfully updated 'registry'
I0909 08:02:35.681330 18130 log.cpp:699] Attempting to truncate the log to 3
I0909 08:02:35.681385 18130 master.cpp:2883] Registered slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise)
I0909 08:02:35.681399 18130 master.cpp:4126] Adding slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000]
I0909 08:02:35.681504 18130 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0909 08:02:35.681570 18130 slave.cpp:763] Registered with master master@127.0.1.1:44005; given slave ID 20140909-080235-16842879-44005-18112-0
I0909 08:02:35.681689 18130 slave.cpp:2329] Received ping from slave-observer(50)@127.0.1.1:44005
I0909 08:02:35.681753 18130 hierarchical_allocator_process.hpp:442] Added slave 20140909-080235-16842879-44005-18112-0 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] (and cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] available)
I0909 08:02:35.681808 18130 hierarchical_allocator_process.hpp:734] Offering cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.681892 18130 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140909-080235-16842879-44005-18112-0 in 109580ns
I0909 08:02:35.681968 18130 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.682014 18130 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.682443 18130 sched.cpp:544] Scheduler::resourceOffers took 254258ns
I0909 08:02:35.682633 18130 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.682684 18130 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-0 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.682708 18130 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I0909 08:02:35.682971 18130 replica.cpp:508] Replica received write request for position 4
I0909 08:02:35.683132 18132 master.hpp:833] Adding task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.683159 18132 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0000 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise)
I0909 08:02:35.683363 18132 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.683580 18132 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.684833 18133 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.684864 18133 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0000 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs
I0909 08:02:35.686401 18132 exec.cpp:132] Version: 0.21.0
I0909 08:02:35.686848 18128 exec.cpp:182] Executor started at: executor(8)@127.0.1.1:44005 with pid 18112
I0909 08:02:35.687095 18132 slave.cpp:1231] Queuing task '0' for executor executor-1 of framework '20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.687302 18132 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-1",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1813,1.0,"Fail fast in example frameworks if task goes into unexpected state Most of the example frameworks launch a bunch of tasks and exit if *all* of them reach FINISHED state. But if there is a bug in the code resulting in TASK_LOST, the framework waits forever. Instead the framework should abort if an un-expected task state is encountered.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1815,3.0,"Create a guide to becoming a committer We have a committer's guide, but the process by which one becomes a committer is unclear. We should set some guidelines and a process by which we can grow contributors into committers.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1830,5.0,Expose master stats differentiating between master-generated and slave-generated LOST tasks The master exports a monotonically-increasing counter of tasks transitioned to TASK_LOST.  This loses fidelity of the source of the lost task.  A first step in exposing the source of lost tasks might be to just differentiate between TASK_LOST transitions initiated by the master vs the slave (and maybe bad input from the scheduler).,,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1844,1.0,AllocatorTest/0.SlaveLost is flaky ,"<code>
[ RUN      ] AllocatorTest/0.SlaveLost
Using temporary directory '/tmp/AllocatorTest_0_SlaveLost_Z2oazw'
I0929 16:58:29.484141  3486 leveldb.cpp:176] Opened db in 604109ns
I0929 16:58:29.484629  3486 leveldb.cpp:183] Compacted db in 172697ns
I0929 16:58:29.484912  3486 leveldb.cpp:198] Created db iterator in 6429ns
I0929 16:58:29.485133  3486 leveldb.cpp:204] Seeked to beginning of db in 1618ns
I0929 16:58:29.485337  3486 leveldb.cpp:273] Iterated through 0 keys in the db in 752ns
I0929 16:58:29.485595  3486 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0929 16:58:29.486017  3500 recover.cpp:425] Starting replica recovery
I0929 16:58:29.486304  3500 recover.cpp:451] Replica is in EMPTY status
I0929 16:58:29.486793  3500 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0929 16:58:29.487205  3500 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0929 16:58:29.487540  3500 recover.cpp:542] Updating replica status to STARTING
I0929 16:58:29.487911  3500 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 36629ns
I0929 16:58:29.488173  3500 replica.cpp:320] Persisted replica status to STARTING
I0929 16:58:29.488438  3500 recover.cpp:451] Replica is in STARTING status
I0929 16:58:29.488891  3500 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0929 16:58:29.489187  3500 recover.cpp:188] Received a recover response from a replica in STARTING status
I0929 16:58:29.489516  3500 recover.cpp:542] Updating replica status to VOTING
I0929 16:58:29.489887  3502 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 32099ns
I0929 16:58:29.490124  3502 replica.cpp:320] Persisted replica status to VOTING
I0929 16:58:29.490381  3500 recover.cpp:556] Successfully joined the Paxos group
I0929 16:58:29.490713  3500 recover.cpp:440] Recover process terminated
I0929 16:58:29.493401  3506 master.cpp:312] Master 20140929-165829-2759502016-55618-3486 (fedora-20) started on 192.168.122.164:55618
I0929 16:58:29.493700  3506 master.cpp:358] Master only allowing authenticated frameworks to register
I0929 16:58:29.493921  3506 master.cpp:363] Master only allowing authenticated slaves to register
I0929 16:58:29.494123  3506 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveLost_Z2oazw/credentials'
I0929 16:58:29.494500  3506 master.cpp:392] Authorization enabled
I0929 16:58:29.495249  3506 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0929 16:58:29.495728  3502 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@192.168.122.164:55618
I0929 16:58:29.496196  3506 master.cpp:1241] The newly elected leader is master@192.168.122.164:55618 with id 20140929-165829-2759502016-55618-3486
I0929 16:58:29.496469  3506 master.cpp:1254] Elected as the leading master!
I0929 16:58:29.496713  3506 master.cpp:1072] Recovering from registrar
I0929 16:58:29.497020  3506 registrar.cpp:312] Recovering registrar
I0929 16:58:29.497486  3506 log.cpp:656] Attempting to start the writer
I0929 16:58:29.498105  3506 replica.cpp:474] Replica received implicit promise request with proposal 1
I0929 16:58:29.498373  3506 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 27145ns
I0929 16:58:29.498605  3506 replica.cpp:342] Persisted promised to 1
I0929 16:58:29.500880  3500 coordinator.cpp:230] Coordinator attemping to fill missing position
I0929 16:58:29.501404  3500 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0929 16:58:29.501687  3500 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 57971ns
I0929 16:58:29.501935  3500 replica.cpp:676] Persisted action at 0
I0929 16:58:29.504905  3507 replica.cpp:508] Replica received write request for position 0
I0929 16:58:29.505130  3507 leveldb.cpp:438] Reading position from leveldb took 18418ns
I0929 16:58:29.505377  3507 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 19998ns
I0929 16:58:29.505571  3507 replica.cpp:676] Persisted action at 0
I0929 16:58:29.505957  3507 replica.cpp:655] Replica received learned notice for position 0
I0929 16:58:29.506186  3507 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 21648ns
I0929 16:58:29.506433  3507 replica.cpp:676] Persisted action at 0
I0929 16:58:29.506767  3507 replica.cpp:661] Replica learned NOP action at position 0
I0929 16:58:29.507199  3507 log.cpp:672] Writer started with ending position 0
I0929 16:58:29.507730  3507 leveldb.cpp:438] Reading position from leveldb took 11532ns
I0929 16:58:29.508915  3507 registrar.cpp:345] Successfully fetched the registry (0B)
I0929 16:58:29.509230  3507 registrar.cpp:421] Attempting to update the 'registry'
I0929 16:58:29.510516  3500 log.cpp:680] Attempting to append 130 bytes to the log
I0929 16:58:29.510949  3500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0929 16:58:29.511363  3500 replica.cpp:508] Replica received write request for position 1
I0929 16:58:29.511697  3500 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 66530ns
I0929 16:58:29.512039  3500 replica.cpp:676] Persisted action at 1
I0929 16:58:29.512460  3500 replica.cpp:655] Replica received learned notice for position 1
I0929 16:58:29.512778  3500 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 24121ns
I0929 16:58:29.513013  3500 replica.cpp:676] Persisted action at 1
I0929 16:58:29.513239  3500 replica.cpp:661] Replica learned APPEND action at position 1
I0929 16:58:29.513674  3500 log.cpp:699] Attempting to truncate the log to 1
I0929 16:58:29.513954  3500 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0929 16:58:29.514385  3500 replica.cpp:508] Replica received write request for position 2
I0929 16:58:29.514680  3500 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 65014ns
I0929 16:58:29.514991  3500 replica.cpp:676] Persisted action at 2
I0929 16:58:29.516978  3501 replica.cpp:655] Replica received learned notice for position 2
I0929 16:58:29.517319  3501 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 24103ns
I0929 16:58:29.517546  3501 leveldb.cpp:401] Deleting ~1 keys from leveldb took 16533ns
I0929 16:58:29.517801  3501 replica.cpp:676] Persisted action at 2
I0929 16:58:29.518039  3501 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0929 16:58:29.518539  3507 registrar.cpp:478] Successfully updated 'registry'
I0929 16:58:29.518885  3507 registrar.cpp:371] Successfully recovered registrar
I0929 16:58:29.519201  3507 master.cpp:1099] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0929 16:58:29.533073  3505 slave.cpp:169] Slave started on 57)@192.168.122.164:55618
I0929 16:58:29.533500  3505 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/credential'
I0929 16:58:29.533834  3505 slave.cpp:276] Slave using credential for: test-principal
I0929 16:58:29.534168  3505 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000]
I0929 16:58:29.534751  3505 slave.cpp:317] Slave hostname: fedora-20
I0929 16:58:29.534965  3505 slave.cpp:318] Slave checkpoint: false
I0929 16:58:29.535557  3505 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/meta'
I0929 16:58:29.535951  3505 status_update_manager.cpp:193] Recovering status update manager
I0929 16:58:29.536290  3505 slave.cpp:3271] Finished recovery
I0929 16:58:29.536782  3505 slave.cpp:598] New master detected at master@192.168.122.164:55618
I0929 16:58:29.537122  3505 slave.cpp:672] Authenticating with master master@192.168.122.164:55618
I0929 16:58:29.537492  3505 slave.cpp:645] Detecting new master
I0929 16:58:29.537294  3506 status_update_manager.cpp:167] New master detected at master@192.168.122.164:55618
I0929 16:58:29.537642  3507 authenticatee.hpp:128] Creating new client SASL connection
I0929 16:58:29.538769  3502 master.cpp:3737] Authenticating slave(57)@192.168.122.164:55618
I0929 16:58:29.539091  3502 authenticator.hpp:156] Creating new server SASL connection
I0929 16:58:29.539710  3503 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0929 16:58:29.539943  3503 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0929 16:58:29.540206  3502 authenticator.hpp:262] Received SASL authentication start
I0929 16:58:29.540457  3502 authenticator.hpp:384] Authentication requires more steps
I0929 16:58:29.540757  3502 authenticatee.hpp:265] Received SASL authentication step
I0929 16:58:29.541121  3502 authenticator.hpp:290] Received SASL authentication step
I0929 16:58:29.541368  3502 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0929 16:58:29.541599  3502 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0929 16:58:29.541874  3502 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0929 16:58:29.542129  3502 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0929 16:58:29.542333  3502 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.542553  3502 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.542785  3502 authenticator.hpp:376] Authentication success
I0929 16:58:29.543047  3502 authenticatee.hpp:305] Authentication success
I0929 16:58:29.543381  3502 slave.cpp:729] Successfully authenticated with master master@192.168.122.164:55618
I0929 16:58:29.543707  3502 slave.cpp:992] Will retry registration in 11.795692ms if necessary
I0929 16:58:29.543179  3503 master.cpp:3777] Successfully authenticated principal 'test-principal' at slave(57)@192.168.122.164:55618
I0929 16:58:29.544255  3503 master.cpp:2930] Registering slave at slave(57)@192.168.122.164:55618 (fedora-20) with id 20140929-165829-2759502016-55618-3486-0
I0929 16:58:29.544587  3503 registrar.cpp:421] Attempting to update the 'registry'
I0929 16:58:29.545816  3500 log.cpp:680] Attempting to append 299 bytes to the log
I0929 16:58:29.546267  3500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0929 16:58:29.546749  3500 replica.cpp:508] Replica received write request for position 3
I0929 16:58:29.547030  3500 leveldb.cpp:343] Persisting action (318 bytes) to leveldb took 31759ns
I0929 16:58:29.547236  3500 replica.cpp:676] Persisted action at 3
I0929 16:58:29.548902  3506 replica.cpp:655] Replica received learned notice for position 3
I0929 16:58:29.549139  3506 leveldb.cpp:343] Persisting action (320 bytes) to leveldb took 25595ns
I0929 16:58:29.549343  3506 replica.cpp:676] Persisted action at 3
I0929 16:58:29.549607  3506 replica.cpp:661] Replica learned APPEND action at position 3
I0929 16:58:29.550081  3506 log.cpp:699] Attempting to truncate the log to 3
I0929 16:58:29.550497  3506 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0929 16:58:29.550943  3506 replica.cpp:508] Replica received write request for position 4
I0929 16:58:29.551198  3506 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 20852ns
I0929 16:58:29.551409  3506 replica.cpp:676] Persisted action at 4
I0929 16:58:29.551795  3506 replica.cpp:655] Replica received learned notice for position 4
I0929 16:58:29.552094  3506 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 22182ns
I0929 16:58:29.552320  3506 leveldb.cpp:401] Deleting ~2 keys from leveldb took 18503ns
I0929 16:58:29.552525  3506 replica.cpp:676] Persisted action at 4
I0929 16:58:29.552781  3506 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0929 16:58:29.550289  3503 registrar.cpp:478] Successfully updated 'registry'
I0929 16:58:29.553553  3503 master.cpp:2970] Registered slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.553807  3503 master.cpp:4180] Adding slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) with cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000]
I0929 16:58:29.554152  3503 slave.cpp:763] Registered with master master@192.168.122.164:55618; given slave ID 20140929-165829-2759502016-55618-3486-0
I0929 16:58:29.554455  3503 slave.cpp:2345] Received ping from slave-observer(56)@192.168.122.164:55618
I0929 16:58:29.554707  3504 hierarchical_allocator_process.hpp:442] Added slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) with cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] available)
I0929 16:58:29.555064  3504 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140929-165829-2759502016-55618-3486-0 in 13111ns
I0929 16:58:29.558220  3486 sched.cpp:137] Version: 0.21.0
I0929 16:58:29.558821  3501 sched.cpp:233] New master detected at master@192.168.122.164:55618
I0929 16:58:29.559054  3501 sched.cpp:283] Authenticating with master master@192.168.122.164:55618
I0929 16:58:29.559360  3501 authenticatee.hpp:128] Creating new client SASL connection
I0929 16:58:29.560096  3501 master.cpp:3737] Authenticating scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618
I0929 16:58:29.560430  3501 authenticator.hpp:156] Creating new server SASL connection
I0929 16:58:29.561141  3501 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0929 16:58:29.561465  3501 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0929 16:58:29.561743  3501 authenticator.hpp:262] Received SASL authentication start
I0929 16:58:29.562098  3501 authenticator.hpp:384] Authentication requires more steps
I0929 16:58:29.562353  3501 authenticatee.hpp:265] Received SASL authentication step
I0929 16:58:29.562721  3507 authenticator.hpp:290] Received SASL authentication step
I0929 16:58:29.563022  3507 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0929 16:58:29.563254  3507 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0929 16:58:29.563484  3507 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0929 16:58:29.563736  3507 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0929 16:58:29.563976  3507 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.564188  3507 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.564415  3507 authenticator.hpp:376] Authentication success
I0929 16:58:29.564673  3507 master.cpp:3777] Successfully authenticated principal 'test-principal' at scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618
I0929 16:58:29.568681  3501 authenticatee.hpp:305] Authentication success
I0929 16:58:29.569046  3501 sched.cpp:357] Successfully authenticated with master master@192.168.122.164:55618
I0929 16:58:29.569286  3501 sched.cpp:476] Sending registration request to master@192.168.122.164:55618
I0929 16:58:29.569581  3507 master.cpp:1360] Received registration request from scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618
I0929 16:58:29.569846  3507 master.cpp:1320] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0929 16:58:29.570219  3507 master.cpp:1419] Registering framework 20140929-165829-2759502016-55618-3486-0000 at scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618
I0929 16:58:29.570543  3506 sched.cpp:407] Framework registered with 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.570811  3506 sched.cpp:421] Scheduler::registered took 13811ns
I0929 16:58:29.571135  3502 hierarchical_allocator_process.hpp:329] Added framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.571393  3502 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 to framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.571723  3502 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 368547ns
I0929 16:58:29.572125  3507 master.hpp:868] Adding offer 20140929-165829-2759502016-55618-3486-0 with resources cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20)
I0929 16:58:29.572374  3507 master.cpp:3679] Sending 1 offers to framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.572841  3503 sched.cpp:544] Scheduler::resourceOffers took 114306ns
I0929 16:58:29.573197  3507 master.hpp:877] Removing offer 20140929-165829-2759502016-55618-3486-0 with resources cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20)
I0929 16:58:29.573457  3507 master.cpp:2274] Processing reply for offers: [ 20140929-165829-2759502016-55618-3486-0 ] on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) for framework 20140929-165829-2759502016-55618-3486-0000
W0929 16:58:29.573717  3507 master.cpp:1944] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0929 16:58:29.573953  3507 master.cpp:1955] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0929 16:58:29.574177  3507 master.cpp:2357] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I0929 16:58:29.574745  3507 master.hpp:845] Adding task 0 with resources cpus(*):2; mem(*):512 on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20)
I0929 16:58:29.574992  3507 master.cpp:2423] Launching task 0 of framework 20140929-165829-2759502016-55618-3486-0000 with resources cpus(*):2; mem(*):512 on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.575315  3503 slave.cpp:1023] Got assigned task 0 for framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.575724  3503 slave.cpp:1133] Launching task 0 for framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.578129  3503 exec.cpp:132] Version: 0.21.0
I0929 16:58:29.578505  3504 exec.cpp:182] Executor started at: executor(30)@192.168.122.164:55618 with pid 3486
I0929 16:58:29.578867  3503 slave.cpp:1246] Queuing task '0' for executor default of framework '20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.579144  3503 slave.cpp:554] Successfully attached file '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/slaves/20140929-165829-2759502016-55618-3486-0/frameworks/20140929-165829-2759502016-55618-3486-0000/executors/default/runs/b0de9759-7054-4763-90f4-889ddc3a8524'
I0929 16:58:29.579401  3503 slave.cpp:1756] Got registration for executor 'default' of framework 20140929-165829-2759502016-55618-3486-0000 ",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-1853,3.0,"Remove /proc and /sys remounts from port_mapping isolator /proc/net reflects a new network namespace regardless and remount doesn't actually do what we expected anyway, i.e., it's not sufficient for a new pid namespace and a new mount is required.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1855,1.0,"Mesos 0.20.1 doesn't compile The compilation of Mesos 0.20.1 fails on Ubuntu Trusty with the following error -

slave/containerizer/mesos/containerizer.cpp  -fPIC -DPIC -o slave/containerizer/mesos/.libs/libmesos_no_3rdparty_la-containerizer.o
In file included from ./linux/routing/filter/ip.hpp:36:0,
                 from ./slave/containerizer/isolators/network/port_mapping.hpp:42,
                 from slave/containerizer/mesos/containerizer.cpp:44:
./linux/routing/filter/filter.hpp:29:43: fatal error: linux/routing/filter/handle.hpp: No such file or directory
 #include ""linux/routing/filter/handle.hpp""
                                           ^",,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1862,3.0,"Performance regression in the Master's http metrics. As part of the change to hold on to terminal unacknowledged tasks in the master, we introduced a performance regression during the following patch:

https://github.com/apache/mesos/commit/0760b007ad65bc91e8cea377339978c78d36d247
{noformat}
commit 0760b007ad65bc91e8cea377339978c78d36d247
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Thu Sep 11 10:48:20 2014 -0700

    Minor cleanups to the Master code.

    Review: https://reviews.apache.org/r/25566
{noformat}

Rather than keeping a running count of allocated resources, we now compute resources on-demand. This was done in order to ignore terminal task's resources.

As a result of this change, the /stats.json and /metrics/snapshot endpoints on the master have slowed down substantially on large clusters.

{noformat}
$ time curl localhost:5050/health
real	0m0.004s
user	0m0.001s
sys	0m0.002s

$ time curl localhost:5050/stats.json &gt; /dev/null
real	0m15.402s
user	0m0.001s
sys	0m0.003s

$ time curl localhost:5050/metrics/snapshot &gt; /dev/null
real	0m6.059s
user	0m0.002s
sys	0m0.002s
{noformat}

{{perf top}} reveals some of the resource computation during a request to stats.json:
{noformat: perf top}
Events: 36K cycles
 10.53%  libc-2.5.so             [.] _int_free
  9.90%  libc-2.5.so             [.] malloc
  8.56%  libmesos-0.21.0.so  [.] std::_Rb_tree<process::processbase*, process::processbase*,="""" std::_identity<process::processbase*="""">, std::less<process::processbase*>, std::allocator<process::processbase*> &gt;::
  8.23%  libc-2.5.so             [.] _int_malloc
  5.80%  libstdc++.so.6.0.8      [.] std::_Rb_tree_increment(std::_Rb_tree_node_base*)
  5.33%  [kernel]                [k] _raw_spin_lock
  3.13%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&amp;)
  2.95%  libmesos-0.21.0.so  [.] process::SocketManager::exited(process::ProcessBase*)
  2.43%  libmesos-0.21.0.so  [.] mesos::Resource::MergeFrom(mesos::Resource const&amp;)
  1.88%  libmesos-0.21.0.so  [.] mesos::internal::master::Slave::used() const
  1.48%  libstdc++.so.6.0.8      [.] __gnu_cxx::__atomic_add(int volatile*, int)
  1.45%  [kernel]                [k] find_busiest_group
  1.41%  libc-2.5.so             [.] free
  1.38%  libmesos-0.21.0.so  [.] mesos::Value_Range::MergeFrom(mesos::Value_Range const&amp;)
  1.13%  libmesos-0.21.0.so  [.] mesos::Value_Scalar::MergeFrom(mesos::Value_Scalar const&amp;)
  1.12%  libmesos-0.21.0.so  [.] mesos::Resource::SharedDtor()
  1.07%  libstdc++.so.6.0.8      [.] __gnu_cxx::__exchange_and_add(int volatile*, int)
  0.94%  libmesos-0.21.0.so  [.] google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&amp;)
  0.92%  libstdc++.so.6.0.8      [.] operator new(unsigned long)
  0.88%  libmesos-0.21.0.so  [.] mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&amp;)
  0.75%  libmesos-0.21.0.so  [.] mesos::matches(mesos::Resource const&amp;, mesos::Resource const&amp;)
{noformat}</process::processbase*></process::processbase*></process::processbase*,></bmahler@twitter.com>",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-1941,3.0,"Make executor's user owner of executor's cgroup directory Currently, when cgroups are enabled, and executor is spawned, it's mounted under, for ex: /sys/fs/cgroup/cpu/mesos/<mesos-id>. This directory in current implementation is only writable by root user. This prevents process launched by executor to mount its child processes under this cgroup, because the cgroup directory is only writable by root.

To enable a executor spawned process to mount it's child processes under it's cgroup directory, the cgroup directory should be made writable by the user which spawns the executor.</mesos-id>",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2007,2.0,"AllocatorTest/0.SlaveReregistersFirst is flaky {noformat:title=}
[ RUN      ] AllocatorTest/0.SlaveReregistersFirst
Using temporary directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d'
I1028 23:48:22.360447 31190 leveldb.cpp:176] Opened db in 2.192575ms
I1028 23:48:22.361253 31190 leveldb.cpp:183] Compacted db in 760753ns
I1028 23:48:22.361320 31190 leveldb.cpp:198] Created db iterator in 22188ns
I1028 23:48:22.361340 31190 leveldb.cpp:204] Seeked to beginning of db in 1950ns
I1028 23:48:22.361351 31190 leveldb.cpp:273] Iterated through 0 keys in the db in 345ns
I1028 23:48:22.361403 31190 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I1028 23:48:22.362185 31217 recover.cpp:437] Starting replica recovery
I1028 23:48:22.362764 31219 recover.cpp:463] Replica is in EMPTY status
I1028 23:48:22.363955 31210 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I1028 23:48:22.364320 31217 recover.cpp:188] Received a recover response from a replica in EMPTY status
I1028 23:48:22.364820 31211 recover.cpp:554] Updating replica status to STARTING
I1028 23:48:22.365365 31215 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418991ns
I1028 23:48:22.365391 31215 replica.cpp:320] Persisted replica status to STARTING
I1028 23:48:22.365617 31217 recover.cpp:463] Replica is in STARTING status
I1028 23:48:22.366328 31206 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043
I1028 23:48:22.366377 31206 master.cpp:358] Master only allowing authenticated frameworks to register
I1028 23:48:22.366391 31206 master.cpp:363] Master only allowing authenticated slaves to register
I1028 23:48:22.366402 31206 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials'
I1028 23:48:22.366708 31206 master.cpp:392] Authorization enabled
I1028 23:48:22.366886 31209 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I1028 23:48:22.367311 31208 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1028 23:48:22.367312 31207 recover.cpp:188] Received a recover response from a replica in STARTING status
I1028 23:48:22.367686 31211 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043
I1028 23:48:22.367863 31212 recover.cpp:554] Updating replica status to VOTING
I1028 23:48:22.368477 31218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 375527ns
I1028 23:48:22.368505 31218 replica.cpp:320] Persisted replica status to VOTING
I1028 23:48:22.368517 31204 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190
I1028 23:48:22.368549 31204 master.cpp:1255] Elected as the leading master!
I1028 23:48:22.368567 31204 master.cpp:1073] Recovering from registrar
I1028 23:48:22.368621 31215 recover.cpp:568] Successfully joined the Paxos group
I1028 23:48:22.368716 31219 registrar.cpp:313] Recovering registrar
I1028 23:48:22.369000 31215 recover.cpp:452] Recover process terminated
I1028 23:48:22.369523 31208 log.cpp:656] Attempting to start the writer
I1028 23:48:22.370909 31205 replica.cpp:474] Replica received implicit promise request with proposal 1
I1028 23:48:22.371266 31205 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 325016ns
I1028 23:48:22.371290 31205 replica.cpp:342] Persisted promised to 1
I1028 23:48:22.371979 31218 coordinator.cpp:230] Coordinator attemping to fill missing position
I1028 23:48:22.373378 31210 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I1028 23:48:22.373746 31210 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 329018ns
I1028 23:48:22.373772 31210 replica.cpp:676] Persisted action at 0
I1028 23:48:22.374897 31214 replica.cpp:508] Replica received write request for position 0
I1028 23:48:22.374951 31214 leveldb.cpp:438] Reading position from leveldb took 26002ns
I1028 23:48:22.375272 31214 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 289094ns
I1028 23:48:22.375298 31214 replica.cpp:676] Persisted action at 0
I1028 23:48:22.375886 31204 replica.cpp:655] Replica received learned notice for position 0
I1028 23:48:22.376258 31204 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 346650ns
I1028 23:48:22.376277 31204 replica.cpp:676] Persisted action at 0
I1028 23:48:22.376298 31204 replica.cpp:661] Replica learned NOP action at position 0
I1028 23:48:22.376843 31215 log.cpp:672] Writer started with ending position 0
I1028 23:48:22.378056 31205 leveldb.cpp:438] Reading position from leveldb took 28265ns
I1028 23:48:22.380323 31217 registrar.cpp:346] Successfully fetched the registry (0B) in 11.55584ms
I1028 23:48:22.380466 31217 registrar.cpp:445] Applied 1 operations in 50632ns; attempting to update the 'registry'
I1028 23:48:22.382472 31217 log.cpp:680] Attempting to append 139 bytes to the log
I1028 23:48:22.382715 31210 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I1028 23:48:22.383463 31210 replica.cpp:508] Replica received write request for position 1
I1028 23:48:22.383857 31210 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 363758ns
I1028 23:48:22.383875 31210 replica.cpp:676] Persisted action at 1
I1028 23:48:22.384397 31218 replica.cpp:655] Replica received learned notice for position 1
I1028 23:48:22.384840 31218 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 420161ns
I1028 23:48:22.384862 31218 replica.cpp:676] Persisted action at 1
I1028 23:48:22.384882 31218 replica.cpp:661] Replica learned APPEND action at position 1
I1028 23:48:22.385684 31211 registrar.cpp:490] Successfully updated the 'registry' in 5.158144ms
I1028 23:48:22.385818 31211 registrar.cpp:376] Successfully recovered registrar
I1028 23:48:22.385912 31214 log.cpp:699] Attempting to truncate the log to 1
I1028 23:48:22.386101 31218 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I1028 23:48:22.386124 31211 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I1028 23:48:22.387398 31209 replica.cpp:508] Replica received write request for position 2
I1028 23:48:22.387758 31209 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 334969ns
I1028 23:48:22.387776 31209 replica.cpp:676] Persisted action at 2
I1028 23:48:22.388272 31204 replica.cpp:655] Replica received learned notice for position 2
I1028 23:48:22.388453 31204 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 159390ns
I1028 23:48:22.388501 31204 leveldb.cpp:401] Deleting ~1 keys from leveldb took 30409ns
I1028 23:48:22.388516 31204 replica.cpp:676] Persisted action at 2
I1028 23:48:22.388531 31204 replica.cpp:661] Replica learned TRUNCATE action at position 2
I1028 23:48:22.400737 31207 slave.cpp:169] Slave started on 34)@67.195.81.190:50043
I1028 23:48:22.400786 31207 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/credential'
I1028 23:48:22.400996 31207 slave.cpp:276] Slave using credential for: test-principal
I1028 23:48:22.401304 31207 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I1028 23:48:22.401413 31207 slave.cpp:318] Slave hostname: pietas.apache.org
I1028 23:48:22.401520 31207 slave.cpp:319] Slave checkpoint: false
W1028 23:48:22.401535 31207 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I1028 23:48:22.402349 31207 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/meta'
I1028 23:48:22.402678 31207 status_update_manager.cpp:197] Recovering status update manager
I1028 23:48:22.403048 31211 slave.cpp:3456] Finished recovery
I1028 23:48:22.403815 31215 slave.cpp:602] New master detected at master@67.195.81.190:50043
I1028 23:48:22.403852 31215 slave.cpp:665] Authenticating with master master@67.195.81.190:50043
I1028 23:48:22.403875 31206 status_update_manager.cpp:171] Pausing sending status updates
I1028 23:48:22.403961 31215 slave.cpp:638] Detecting new master
I1028 23:48:22.404016 31211 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:22.404230 31204 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043
I1028 23:48:22.404464 31205 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:22.404613 31211 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:22.404649 31211 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:22.404734 31211 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:22.404783 31211 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:22.404898 31215 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:22.404999 31215 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:22.405030 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1028 23:48:22.405047 31215 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1028 23:48:22.405086 31215 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1028 23:48:22.405109 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1028 23:48:22.405122 31215 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.405129 31215 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.405146 31215 authenticator.hpp:381] Authentication success
I1028 23:48:22.405243 31213 authenticatee.hpp:310] Authentication success
I1028 23:48:22.405253 31214 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043
I1028 23:48:22.405505 31213 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043
I1028 23:48:22.405619 31213 slave.cpp:1050] Will retry registration in 17.050994ms if necessary
I1028 23:48:22.405819 31215 master.cpp:3032] Registering slave at slave(34)@67.195.81.190:50043 (pietas.apache.org) with id 20141028-234822-3193029443-50043-31190-S0
I1028 23:48:22.406262 31216 registrar.cpp:445] Applied 1 operations in 52647ns; attempting to update the 'registry'
I1028 23:48:22.406697 31190 sched.cpp:137] Version: 0.21.0
I1028 23:48:22.407083 31211 sched.cpp:233] New master detected at master@67.195.81.190:50043
I1028 23:48:22.407114 31211 sched.cpp:283] Authenticating with master master@67.195.81.190:50043
I1028 23:48:22.407290 31214 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:22.407424 31214 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.407659 31207 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:22.407757 31207 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:22.407774 31207 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:22.407830 31207 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:22.407868 31207 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:22.407927 31207 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:22.408015 31212 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:22.408037 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1028 23:48:22.408046 31212 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1028 23:48:22.408072 31212 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1028 23:48:22.408092 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1028 23:48:22.408100 31212 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.408105 31212 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.408116 31212 authenticator.hpp:381] Authentication success
I1028 23:48:22.408192 31210 authenticatee.hpp:310] Authentication success
I1028 23:48:22.408210 31217 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.408419 31210 sched.cpp:357] Successfully authenticated with master master@67.195.81.190:50043
I1028 23:48:22.408460 31210 sched.cpp:476] Sending registration request to master@67.195.81.190:50043
I1028 23:48:22.408568 31217 master.cpp:1362] Received registration request for framework 'default' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.408617 31217 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1028 23:48:22.408937 31214 master.cpp:1426] Registering framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.409265 31213 sched.cpp:407] Framework registered with 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.409267 31212 hierarchical_allocator_process.hpp:329] Added framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.409312 31212 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1028 23:48:22.409324 31215 log.cpp:680] Attempting to append 316 bytes to the log
I1028 23:48:22.409333 31213 sched.cpp:421] Scheduler::registered took 38591ns
I1028 23:48:22.409327 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 24107ns
I1028 23:48:22.409518 31205 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I1028 23:48:22.410127 31206 replica.cpp:508] Replica received write request for position 3
I1028 23:48:22.410706 31206 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554098ns
I1028 23:48:22.410725 31206 replica.cpp:676] Persisted action at 3
I1028 23:48:22.411151 31217 replica.cpp:655] Replica received learned notice for position 3
I1028 23:48:22.411499 31217 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 326572ns
I1028 23:48:22.411519 31217 replica.cpp:676] Persisted action at 3
I1028 23:48:22.411533 31217 replica.cpp:661] Replica learned APPEND action at position 3
I1028 23:48:22.412292 31219 registrar.cpp:490] Successfully updated the 'registry' in 5.972992ms
I1028 23:48:22.412518 31218 log.cpp:699] Attempting to truncate the log to 3
I1028 23:48:22.412621 31213 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I1028 23:48:22.412734 31219 slave.cpp:2522] Received ping from slave-observer(38)@67.195.81.190:50043
I1028 23:48:22.412787 31206 master.cpp:3086] Registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I1028 23:48:22.412858 31219 slave.cpp:756] Registered with master master@67.195.81.190:50043; given slave ID 20141028-234822-3193029443-50043-31190-S0
I1028 23:48:22.412994 31210 status_update_manager.cpp:178] Resuming sending status updates
I1028 23:48:22.413014 31211 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I1028 23:48:22.413159 31211 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] on slave 20141028-234822-3193029443-50043-31190-S0 to framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.413290 31208 replica.cpp:508] Replica received write request for position 4
I1028 23:48:22.413421 31211 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 346658ns
I1028 23:48:22.413650 31208 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 336067ns
I1028 23:48:22.413668 31208 replica.cpp:676] Persisted action at 4
I1028 23:48:22.413797 31216 master.cpp:3795] Sending 1 offers to framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.414077 31212 replica.cpp:655] Replica received learned notice for position 4
I1028 23:48:22.414356 31212 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260401ns
I1028 23:48:22.414403 31212 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28541ns
I1028 23:48:22.414417 31212 replica.cpp:676] Persisted action at 4
I1028 23:48:22.414446 31212 replica.cpp:661] Replica learned TRUNCATE action at position 4
I1028 23:48:22.414422 31207 sched.cpp:544] Scheduler::resourceOffers took 310278ns
I1028 23:48:22.415086 31214 master.cpp:2321] Processing reply for offers: [ 20141028-234822-3193029443-50043-31190-O0 ] on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) for framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
W1028 23:48:22.415163 31214 master.cpp:1969] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1028 23:48:22.415186 31214 master.cpp:1980] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1028 23:48:22.415256 31214 master.cpp:2417] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I1028 23:48:22.416033 31219 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org)
I1028 23:48:22.416084 31219 master.cpp:2480] Launching task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.416317 31214 slave.cpp:1081] Got assigned task 0 for framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.416679 31215 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.416721 31215 hierarchical_allocator_process.hpp:599] Framework 20141028-2",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2008,2.0,"MasterAuthorizationTest.DuplicateReregistration is flaky {noformat:title=}
[ RUN      ] MasterAuthorizationTest.DuplicateReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX'
I1029 08:25:26.021766 32232 leveldb.cpp:176] Opened db in 3.066621ms
I1029 08:25:26.022734 32232 leveldb.cpp:183] Compacted db in 935019ns
I1029 08:25:26.022766 32232 leveldb.cpp:198] Created db iterator in 4350ns
I1029 08:25:26.022785 32232 leveldb.cpp:204] Seeked to beginning of db in 902ns
I1029 08:25:26.022799 32232 leveldb.cpp:273] Iterated through 0 keys in the db in 387ns
I1029 08:25:26.022831 32232 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I1029 08:25:26.023305 32248 recover.cpp:437] Starting replica recovery
I1029 08:25:26.023598 32248 recover.cpp:463] Replica is in EMPTY status
I1029 08:25:26.025059 32260 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I1029 08:25:26.025320 32247 recover.cpp:188] Received a recover response from a replica in EMPTY status
I1029 08:25:26.025585 32256 recover.cpp:554] Updating replica status to STARTING
I1029 08:25:26.026546 32249 master.cpp:312] Master 20141029-082526-3142697795-40696-32232 (pomona.apache.org) started on 67.195.81.187:40696
I1029 08:25:26.026561 32261 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 694444ns
I1029 08:25:26.026592 32249 master.cpp:358] Master only allowing authenticated frameworks to register
I1029 08:25:26.026592 32261 replica.cpp:320] Persisted replica status to STARTING
I1029 08:25:26.026605 32249 master.cpp:363] Master only allowing authenticated slaves to register
I1029 08:25:26.026639 32249 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX/credentials'
I1029 08:25:26.026877 32249 master.cpp:392] Authorization enabled
I1029 08:25:26.026901 32260 recover.cpp:463] Replica is in STARTING status
I1029 08:25:26.027498 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1029 08:25:26.027541 32248 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40696
I1029 08:25:26.028055 32252 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I1029 08:25:26.028451 32247 recover.cpp:188] Received a recover response from a replica in STARTING status
I1029 08:25:26.028733 32249 master.cpp:1242] The newly elected leader is master@67.195.81.187:40696 with id 20141029-082526-3142697795-40696-32232
I1029 08:25:26.028764 32249 master.cpp:1255] Elected as the leading master!
I1029 08:25:26.028781 32249 master.cpp:1073] Recovering from registrar
I1029 08:25:26.028904 32246 recover.cpp:554] Updating replica status to VOTING
I1029 08:25:26.029163 32257 registrar.cpp:313] Recovering registrar
I1029 08:25:26.029556 32251 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 485711ns
I1029 08:25:26.029588 32251 replica.cpp:320] Persisted replica status to VOTING
I1029 08:25:26.029726 32253 recover.cpp:568] Successfully joined the Paxos group
I1029 08:25:26.029932 32253 recover.cpp:452] Recover process terminated
I1029 08:25:26.030436 32250 log.cpp:656] Attempting to start the writer
I1029 08:25:26.032152 32248 replica.cpp:474] Replica received implicit promise request with proposal 1
I1029 08:25:26.032778 32248 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 597030ns
I1029 08:25:26.032807 32248 replica.cpp:342] Persisted promised to 1
I1029 08:25:26.033481 32254 coordinator.cpp:230] Coordinator attemping to fill missing position
I1029 08:25:26.035429 32247 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I1029 08:25:26.036154 32247 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 690208ns
I1029 08:25:26.036181 32247 replica.cpp:676] Persisted action at 0
I1029 08:25:26.037344 32249 replica.cpp:508] Replica received write request for position 0
I1029 08:25:26.037395 32249 leveldb.cpp:438] Reading position from leveldb took 22607ns
I1029 08:25:26.038074 32249 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 647429ns
I1029 08:25:26.038105 32249 replica.cpp:676] Persisted action at 0
I1029 08:25:26.038683 32247 replica.cpp:655] Replica received learned notice for position 0
I1029 08:25:26.039378 32247 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 664911ns
I1029 08:25:26.039407 32247 replica.cpp:676] Persisted action at 0
I1029 08:25:26.039433 32247 replica.cpp:661] Replica learned NOP action at position 0
I1029 08:25:26.040045 32252 log.cpp:672] Writer started with ending position 0
I1029 08:25:26.041378 32251 leveldb.cpp:438] Reading position from leveldb took 25625ns
I1029 08:25:26.044642 32246 registrar.cpp:346] Successfully fetched the registry (0B) in 15.433984ms
I1029 08:25:26.044742 32246 registrar.cpp:445] Applied 1 operations in 16444ns; attempting to update the 'registry'
I1029 08:25:26.047538 32256 log.cpp:680] Attempting to append 139 bytes to the log
I1029 08:25:26.156330 32247 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I1029 08:25:26.158460 32261 replica.cpp:508] Replica received write request for position 1
I1029 08:25:26.159277 32261 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 782308ns
I1029 08:25:26.159328 32261 replica.cpp:676] Persisted action at 1
I1029 08:25:26.160267 32255 replica.cpp:655] Replica received learned notice for position 1
I1029 08:25:26.161070 32255 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 750259ns
I1029 08:25:26.161100 32255 replica.cpp:676] Persisted action at 1
I1029 08:25:26.161125 32255 replica.cpp:661] Replica learned APPEND action at position 1
I1029 08:25:26.162199 32253 registrar.cpp:490] Successfully updated the 'registry' in 117.40416ms
I1029 08:25:26.162400 32253 registrar.cpp:376] Successfully recovered registrar
I1029 08:25:26.162724 32249 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I1029 08:25:26.162757 32253 log.cpp:699] Attempting to truncate the log to 1
I1029 08:25:26.162919 32256 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I1029 08:25:26.163949 32250 replica.cpp:508] Replica received write request for position 2
I1029 08:25:26.164589 32250 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 603175ns
I1029 08:25:26.164618 32250 replica.cpp:676] Persisted action at 2
I1029 08:25:26.165385 32251 replica.cpp:655] Replica received learned notice for position 2
I1029 08:25:26.166007 32251 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 594003ns
I1029 08:25:26.166056 32251 leveldb.cpp:401] Deleting ~1 keys from leveldb took 23309ns
I1029 08:25:26.166077 32251 replica.cpp:676] Persisted action at 2
I1029 08:25:26.166100 32251 replica.cpp:661] Replica learned TRUNCATE action at position 2
I1029 08:25:26.178493 32232 sched.cpp:137] Version: 0.21.0
I1029 08:25:26.179029 32256 sched.cpp:233] New master detected at master@67.195.81.187:40696
I1029 08:25:26.179078 32256 sched.cpp:283] Authenticating with master master@67.195.81.187:40696
I1029 08:25:26.179424 32246 authenticatee.hpp:133] Creating new client SASL connection
I1029 08:25:26.179678 32259 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.179970 32250 authenticator.hpp:161] Creating new server SASL connection
I1029 08:25:26.180165 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1029 08:25:26.180191 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1029 08:25:26.180272 32250 authenticator.hpp:267] Received SASL authentication start
I1029 08:25:26.180378 32250 authenticator.hpp:389] Authentication requires more steps
I1029 08:25:26.180557 32260 authenticatee.hpp:270] Received SASL authentication step
I1029 08:25:26.180704 32254 authenticator.hpp:295] Received SASL authentication step
I1029 08:25:26.180737 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1029 08:25:26.180748 32254 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1029 08:25:26.180780 32254 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1029 08:25:26.180804 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1029 08:25:26.180816 32254 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:26.180824 32254 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:26.180841 32254 authenticator.hpp:381] Authentication success
I1029 08:25:26.180937 32259 authenticatee.hpp:310] Authentication success
I1029 08:25:26.180991 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.181422 32259 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696
I1029 08:25:26.181449 32259 sched.cpp:476] Sending registration request to master@67.195.81.187:40696
I1029 08:25:26.181697 32260 master.cpp:1362] Received registration request for framework 'default' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.181758 32260 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1029 08:25:26.182063 32260 master.cpp:1426] Registering framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.182430 32248 hierarchical_allocator_process.hpp:329] Added framework 20141029-082526-3142697795-40696-32232-0000
I1029 08:25:26.182462 32248 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:26.182462 32261 sched.cpp:407] Framework registered with 20141029-082526-3142697795-40696-32232-0000
I1029 08:25:26.182473 32248 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 15372ns
I1029 08:25:26.182554 32261 sched.cpp:421] Scheduler::registered took 60059ns
I1029 08:25:26.185515 32260 sched.cpp:227] Scheduler::disconnected took 16607ns
I1029 08:25:26.185538 32260 sched.cpp:233] New master detected at master@67.195.81.187:40696
I1029 08:25:26.185567 32260 sched.cpp:283] Authenticating with master master@67.195.81.187:40696
I1029 08:25:26.185783 32246 authenticatee.hpp:133] Creating new client SASL connection
I1029 08:25:26.186218 32250 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.186456 32247 authenticator.hpp:161] Creating new server SASL connection
I1029 08:25:26.186594 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1029 08:25:26.186621 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1029 08:25:26.186745 32259 authenticator.hpp:267] Received SASL authentication start
I1029 08:25:26.186800 32259 authenticator.hpp:389] Authentication requires more steps
I1029 08:25:26.186936 32260 authenticatee.hpp:270] Received SASL authentication step
I1029 08:25:26.187062 32249 authenticator.hpp:295] Received SASL authentication step
I1029 08:25:26.187095 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1029 08:25:26.187108 32249 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1029 08:25:26.187137 32249 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1029 08:25:26.187162 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1029 08:25:26.187175 32249 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:26.187182 32249 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:26.187199 32249 authenticator.hpp:381] Authentication success
I1029 08:25:26.187327 32249 authenticatee.hpp:310] Authentication success
I1029 08:25:26.187366 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.187631 32249 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696
I1029 08:25:26.187659 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696
I1029 08:25:27.028445 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:28.045682 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.017231941secs
I1029 08:25:28.045760 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696
I1029 08:25:28.045900 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.045989 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1029 08:25:28.046455 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.046529 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1029 08:25:28.050155 32247 sched.cpp:233] New master detected at master@67.195.81.187:40696
I1029 08:25:28.050217 32247 sched.cpp:283] Authenticating with master master@67.195.81.187:40696
I1029 08:25:28.050405 32252 master.cpp:1552] Re-registering framework 20141029-082526-3142697795-40696-32232-0000 (default)  at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.050509 32253 authenticatee.hpp:133] Creating new client SASL connection
I1029 08:25:28.050566 32252 master.cpp:1592] Allowing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 to re-register with an already used id
I1029 08:25:28.051084 32257 sched.cpp:449] Framework re-registered with 20141029-082526-3142697795-40696-32232-0000
I1029 08:25:28.051151 32252 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.051167 32257 sched.cpp:463] Scheduler::reregistered took 52801ns
I1029 08:25:28.051723 32261 authenticator.hpp:161] Creating new server SASL connection
I1029 08:25:28.052042 32249 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1029 08:25:28.052077 32249 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1029 08:25:28.052170 32249 master.cpp:1534] Dropping re-registration request of framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 because new authentication attempt is in progress
I1029 08:25:28.052218 32257 authenticator.hpp:267] Received SASL authentication start
I1029 08:25:28.052325 32257 authenticator.hpp:389] Authentication requires more steps
I1029 08:25:28.052428 32257 authenticatee.hpp:270] Received SASL authentication step
I1029 08:25:28.052641 32246 authenticator.hpp:295] Received SASL authentication step
I1029 08:25:28.052685 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1029 08:25:28.052701 32246 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1029 08:25:28.052739 32246 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1029 08:25:28.052767 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1029 08:25:28.052779 32246 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:28.052788 32246 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:28.052804 32246 authenticator.hpp:381] Authentication success
I1029 08:25:28.052947 32252 authenticatee.hpp:310] Authentication success
I1029 08:25:28.053020 32246 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.053462 32247 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696
I1029 08:25:29.046855 32261 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:29.046880 32261 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 35632ns
I1029 08:25:30.047458 32253 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:30.047487 32253 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 43031ns
I1029 08:25:31.028373 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1029 08:25:31.048673 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:31.048702 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 44769ns
I1029 08:25:32.049576 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:32.049604 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 51919ns
I1029 08:25:33.050864 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:33.050896 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38019ns
I1029 08:25:34.051961 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:34.051993 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 64619ns
I1029 08:25:35.052196 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:35.052223 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 34475ns
I1029 08:25:36.029101 32259 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1029 08:25:36.053067 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:36.053095 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38354ns
I1029 08:25:37.053506 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:37.053536 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38249ns
tests/master_authorization_tests.cpp:877: Failure
Failed to wait 10secs for frameworkReregisteredMessage",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2017,5.0,"Segfault with ""Pure virtual method called"" when tests fail The most recent one:

{noformat:title=DRFAllocatorTest.DRFAllocatorProcess}
[ RUN      ] DRFAllocatorTest.DRFAllocatorProcess
Using temporary directory '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j'
I1030 05:55:06.934813 24459 leveldb.cpp:176] Opened db in 3.175202ms
I1030 05:55:06.935925 24459 leveldb.cpp:183] Compacted db in 1.077924ms
I1030 05:55:06.935976 24459 leveldb.cpp:198] Created db iterator in 16460ns
I1030 05:55:06.935995 24459 leveldb.cpp:204] Seeked to beginning of db in 2018ns
I1030 05:55:06.936005 24459 leveldb.cpp:273] Iterated through 0 keys in the db in 335ns
I1030 05:55:06.936039 24459 replica.cpp:741] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I1030 05:55:06.936705 24480 recover.cpp:437] Starting replica recovery
I1030 05:55:06.937023 24480 recover.cpp:463] Replica is in EMPTY status
I1030 05:55:06.938158 24475 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I1030 05:55:06.938859 24482 recover.cpp:188] Received a recover response from a replica in EMPTY status
I1030 05:55:06.939486 24474 recover.cpp:554] Updating replica status to STARTING
I1030 05:55:06.940249 24489 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 591981ns
I1030 05:55:06.940274 24489 replica.cpp:320] Persisted replica status to STARTING
I1030 05:55:06.940752 24481 recover.cpp:463] Replica is in STARTING status
I1030 05:55:06.940820 24489 master.cpp:312] Master 20141030-055506-3142697795-40429-24459 (pomona.apache.org) started on 67.195.81.187:40429
I1030 05:55:06.940871 24489 master.cpp:358] Master only allowing authenticated frameworks to register
I1030 05:55:06.940891 24489 master.cpp:363] Master only allowing authenticated slaves to register
I1030 05:55:06.940908 24489 credentials.hpp:36] Loading credentials for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j/credentials'
I1030 05:55:06.941215 24489 master.cpp:392] Authorization enabled
I1030 05:55:06.941751 24475 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1030 05:55:06.942227 24474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I1030 05:55:06.942401 24476 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40429
I1030 05:55:06.942895 24483 recover.cpp:188] Received a recover response from a replica in STARTING status
I1030 05:55:06.943035 24474 master.cpp:1242] The newly elected leader is master@67.195.81.187:40429 with id 20141030-055506-3142697795-40429-24459
I1030 05:55:06.943063 24474 master.cpp:1255] Elected as the leading master!
I1030 05:55:06.943079 24474 master.cpp:1073] Recovering from registrar
I1030 05:55:06.943313 24480 registrar.cpp:313] Recovering registrar
I1030 05:55:06.943455 24475 recover.cpp:554] Updating replica status to VOTING
I1030 05:55:06.944144 24474 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 536365ns
I1030 05:55:06.944172 24474 replica.cpp:320] Persisted replica status to VOTING
I1030 05:55:06.944355 24489 recover.cpp:568] Successfully joined the Paxos group
I1030 05:55:06.944576 24489 recover.cpp:452] Recover process terminated
I1030 05:55:06.945155 24486 log.cpp:656] Attempting to start the writer
I1030 05:55:06.947013 24473 replica.cpp:474] Replica received implicit promise request with proposal 1
I1030 05:55:06.947854 24473 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 806463ns
I1030 05:55:06.947883 24473 replica.cpp:342] Persisted promised to 1
I1030 05:55:06.948547 24481 coordinator.cpp:230] Coordinator attemping to fill missing position
I1030 05:55:06.950269 24479 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I1030 05:55:06.950933 24479 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 603843ns
I1030 05:55:06.950961 24479 replica.cpp:676] Persisted action at 0
I1030 05:55:06.952180 24476 replica.cpp:508] Replica received write request for position 0
I1030 05:55:06.952239 24476 leveldb.cpp:438] Reading position from leveldb took 28437ns
I1030 05:55:06.952896 24476 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 623980ns
I1030 05:55:06.952926 24476 replica.cpp:676] Persisted action at 0
I1030 05:55:06.953543 24485 replica.cpp:655] Replica received learned notice for position 0
I1030 05:55:06.954082 24485 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 511807ns
I1030 05:55:06.954107 24485 replica.cpp:676] Persisted action at 0
I1030 05:55:06.954128 24485 replica.cpp:661] Replica learned NOP action at position 0
I1030 05:55:06.954710 24473 log.cpp:672] Writer started with ending position 0
I1030 05:55:06.956215 24478 leveldb.cpp:438] Reading position from leveldb took 33085ns
I1030 05:55:06.959481 24475 registrar.cpp:346] Successfully fetched the registry (0B) in 16.11904ms
I1030 05:55:06.959616 24475 registrar.cpp:445] Applied 1 operations in 28239ns; attempting to update the 'registry'
I1030 05:55:06.962514 24487 log.cpp:680] Attempting to append 139 bytes to the log
I1030 05:55:06.962646 24474 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I1030 05:55:06.964146 24486 replica.cpp:508] Replica received write request for position 1
I1030 05:55:06.964962 24486 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 743389ns
I1030 05:55:06.964993 24486 replica.cpp:676] Persisted action at 1
I1030 05:55:06.965895 24473 replica.cpp:655] Replica received learned notice for position 1
I1030 05:55:06.966531 24473 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 607242ns
I1030 05:55:06.966555 24473 replica.cpp:676] Persisted action at 1
I1030 05:55:06.966578 24473 replica.cpp:661] Replica learned APPEND action at position 1
I1030 05:55:06.967706 24481 registrar.cpp:490] Successfully updated the 'registry' in 8.036096ms
I1030 05:55:06.967895 24481 registrar.cpp:376] Successfully recovered registrar
I1030 05:55:06.967993 24482 log.cpp:699] Attempting to truncate the log to 1
I1030 05:55:06.968258 24479 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I1030 05:55:06.968268 24475 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I1030 05:55:06.969156 24476 replica.cpp:508] Replica received write request for position 2
I1030 05:55:06.969678 24476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 491913ns
I1030 05:55:06.969703 24476 replica.cpp:676] Persisted action at 2
I1030 05:55:06.970459 24478 replica.cpp:655] Replica received learned notice for position 2
I1030 05:55:06.971060 24478 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 573076ns
I1030 05:55:06.971124 24478 leveldb.cpp:401] Deleting ~1 keys from leveldb took 35339ns
I1030 05:55:06.971145 24478 replica.cpp:676] Persisted action at 2
I1030 05:55:06.971168 24478 replica.cpp:661] Replica learned TRUNCATE action at position 2
I1030 05:55:06.980211 24459 containerizer.cpp:100] Using isolation: posix/cpu,posix/mem
I1030 05:55:06.984153 24473 slave.cpp:169] Slave started on 203)@67.195.81.187:40429
I1030 05:55:07.055308 24473 credentials.hpp:84] Loading credential for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/credential'
I1030 05:55:06.988750 24459 sched.cpp:137] Version: 0.21.0
I1030 05:55:07.055521 24473 slave.cpp:276] Slave using credential for: test-principal
I1030 05:55:07.055726 24473 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):0; ports(*):[31000-32000]
I1030 05:55:07.055865 24473 slave.cpp:318] Slave hostname: pomona.apache.org
I1030 05:55:07.055881 24473 slave.cpp:319] Slave checkpoint: false
W1030 05:55:07.055889 24473 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I1030 05:55:07.056172 24485 sched.cpp:233] New master detected at master@67.195.81.187:40429
I1030 05:55:07.056222 24485 sched.cpp:283] Authenticating with master master@67.195.81.187:40429
I1030 05:55:07.056717 24485 state.cpp:33] Recovering state from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/meta'
I1030 05:55:07.056851 24475 authenticatee.hpp:133] Creating new client SASL connection
I1030 05:55:07.057003 24473 status_update_manager.cpp:197] Recovering status update manager
I1030 05:55:07.057252 24488 master.cpp:3853] Authenticating scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:07.057502 24489 containerizer.cpp:281] Recovering containerizer
I1030 05:55:07.057524 24475 authenticator.hpp:161] Creating new server SASL connection
I1030 05:55:07.057688 24475 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1030 05:55:07.057719 24475 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1030 05:55:07.057919 24481 authenticator.hpp:267] Received SASL authentication start
I1030 05:55:07.057968 24481 authenticator.hpp:389] Authentication requires more steps
I1030 05:55:07.058070 24473 authenticatee.hpp:270] Received SASL authentication step
I1030 05:55:07.058199 24485 authenticator.hpp:295] Received SASL authentication step
I1030 05:55:07.058223 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1030 05:55:07.058233 24485 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1030 05:55:07.058259 24485 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1030 05:55:07.058290 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1030 05:55:07.058302 24485 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1030 05:55:07.058307 24485 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1030 05:55:07.058320 24485 authenticator.hpp:381] Authentication success
I1030 05:55:07.058467 24480 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:07.058493 24485 slave.cpp:3456] Finished recovery
I1030 05:55:07.058593 24478 authenticatee.hpp:310] Authentication success
I1030 05:55:07.058838 24478 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40429
I1030 05:55:07.058861 24478 sched.cpp:476] Sending registration request to master@67.195.81.187:40429
I1030 05:55:07.058969 24475 slave.cpp:602] New master detected at master@67.195.81.187:40429
I1030 05:55:07.058969 24487 status_update_manager.cpp:171] Pausing sending status updates
I1030 05:55:07.059026 24475 slave.cpp:665] Authenticating with master master@67.195.81.187:40429
I1030 05:55:07.059061 24481 master.cpp:1362] Received registration request for framework 'framework1' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:07.059131 24481 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I1030 05:55:07.059171 24475 slave.cpp:638] Detecting new master
I1030 05:55:07.059214 24482 authenticatee.hpp:133] Creating new client SASL connection
I1030 05:55:07.059550 24481 master.cpp:3853] Authenticating slave(203)@67.195.81.187:40429
I1030 05:55:07.059787 24487 authenticator.hpp:161] Creating new server SASL connection
I1030 05:55:07.059922 24481 master.cpp:1426] Registering framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:07.059996 24474 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1030 05:55:07.060034 24474 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1030 05:55:07.060117 24474 authenticator.hpp:267] Received SASL authentication start
I1030 05:55:07.060165 24474 authenticator.hpp:389] Authentication requires more steps
I1030 05:55:07.060377 24476 hierarchical_allocator_process.hpp:329] Added framework 20141030-055506-3142697795-40429-24459-0000
I1030 05:55:07.060394 24488 sched.cpp:407] Framework registered with 20141030-055506-3142697795-40429-24459-0000
I1030 05:55:07.060403 24476 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1030 05:55:07.060431 24476 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 29857ns
I1030 05:55:07.060443 24488 sched.cpp:421] Scheduler::registered took 19407ns
I1030 05:55:07.060545 24478 authenticatee.hpp:270] Received SASL authentication step
I1030 05:55:07.060645 24478 authenticator.hpp:295] Received SASL authentication step
I1030 05:55:07.060673 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1030 05:55:07.060685 24478 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1030 05:55:07.060714 24478 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1030 05:55:07.060740 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1030 05:55:07.060760 24478 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1030 05:55:07.060770 24478 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1030 05:55:07.060788 24478 authenticator.hpp:381] Authentication success
I1030 05:55:07.060920 24474 authenticatee.hpp:310] Authentication success
I1030 05:55:07.060945 24485 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(203)@67.195.81.187:40429
I1030 05:55:07.061388 24489 slave.cpp:722] Successfully authenticated with master master@67.195.81.187:40429
I1030 05:55:07.061504 24489 slave.cpp:1050] Will retry registration in 4.778336ms if necessary
I1030 05:55:07.061718 24480 master.cpp:3032] Registering slave at slave(203)@67.195.81.187:40429 (pomona.apache.org) with id 20141030-055506-3142697795-40429-24459-S0
I1030 05:55:07.062119 24489 registrar.cpp:445] Applied 1 operations in 53691ns; attempting to update the 'registry'
I1030 05:55:07.065182 24479 log.cpp:680] Attempting to append 316 bytes to the log
I1030 05:55:07.065337 24487 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I1030 05:55:07.066359 24474 replica.cpp:508] Replica received write request for position 3
I1030 05:55:07.066643 24474 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 249579ns
I1030 05:55:07.066671 24474 replica.cpp:676] Persisted action at 3
I../../src/tests/allocator_tests.cpp:120: Failure
Failed to wait 10secs for offers1
1030 05:55:07.067101 24477 slave.cpp:1050] Will retry registration in 24.08243ms if necessary
I1030 05:55:07.067140 24473 master.cpp:3020] Ignoring register slave message from slave(203)@67.195.81.187:40429 (pomona.apache.org) as admission is already in progress
I1030 05:55:07.067395 24488 replica.cpp:655] Replica received learned notice for position 3
I1030 05:55:07.943416 24478 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1030 05:55:19.804687 24478 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 11.861261123secs
I1030 05:55:11.942713 24474 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1030 05:55:19.805850 24488 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 1.067224ms
I1030 05:55:19.806012 24488 replica.cpp:676] Persisted action at 3
../../src/tests/allocator_tests.cpp:115: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I1030 05:55:19.806144 24488 replica.cpp:661] Replica learned APPEND action at position 3
I1030 05:55:19.806695 24473 master.cpp:768] Framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 disconnected
I1030 05:55:19.806726 24473 master.cpp:1731] Disconnecting framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:19.806751 24473 master.cpp:1747] Deactivating framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:19.806967 24473 master.cpp:790] Giving framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 0ns to failover
../../src/tests/allocator_tests.cpp:94: Failure
Actual function call count doesn't match EXPECT_CALL(allocator, slaveAdded(_, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
F1030 05:55:19.806967 24480 logging.cpp:57] RAW: Pure virtual method called
I1030 05:55:19.807348 24488 master.cpp:3665] Framework failover timeout, removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:19.807370 24488 master.cpp:4201] Removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
*** Aborted at 1414648519 (unix time) try ""date -d @1414648519"" if you are using GNU date ***
PC: @           0x91bc86 process::PID&lt;&gt;::PID()
*** SIGSEGV (@0x0) received by PID 24459 (TID 0x2b86c919a700) from PID 0; stack trace: ***
I1030 05:55:19.808631 24489 registrar.cpp:490] Successfully updated the 'registry' in 12.746377984secs
    @     0x2b86c55fc340 (unknown)
I1030 05:55:19.808938 24473 log.cpp:699] Attempting to truncate the log to 3
    @     0x2b86c3327174  google::LogMessage::Fail()
I1030 05:55:19.809084 24481 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
    @           0x91bc86 process::PID&lt;&gt;::PID()
    @     0x2b86c332c868  google::RawLog__()
I1030 05:55:19.810191 24479 replica.cpp:508] Replica received write request for position 4
I1030 05:55:19.810899 24479 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 678090ns
I1030 05:55:19.810919 24479 replica.cpp:676] Persisted action at 4
    @           0x91bf24 process::Process&lt;&gt;::self()
I1030 05:55:19.811635 24485 replica.cpp:655] Replica received learned notice for position 4
I1030 05:55:19.812180 24485 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 523927ns
I1030 05:55:19.812228 24485 leveldb.cpp:401] Deleting ~2 keys from leveldb took 29523ns
I1030 05:55:19.812242 24485 replica.cpp:676] Persisted action at 4
I    @     0x2b86c29d2a36  __cxa_pure_virtual
1030 05:55:19.812258 24485 replica.cpp:661] Replica learned TRUNCATE action at position 4
    @          0x1046936  testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
I1030 05:55:19.829655 24474 slave.cpp:1050] Will retry registration in 31.785967ms if necessary
    @           0x9c0633  testing::internal::FunctionMockerBase&lt;&gt;::InvokeWith()
    @           0x9b6152  testing::internal::Functio",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2032,13.0,"Update Maintenance design to account for persistent resources. With persistent resources and dynamic reservations, frameworks need to know how long the resources will be unavailable for maintenance operations.

This is because for persistent resources, the framework needs to understand how long the persistent resource will be unavailable. For example, if there will be a 10 minute reboot for a kernel upgrade, the framework will not want to re-replicate all of it's persistent data on the machine. Rather, tolerating one unavailable replica for the maintenance window would be preferred.

I'd like to do a revisit of the design to ensure it works well for persistent resources as well.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2052,1.0,"RunState::recover should always recover 'completed' RunState::recover() will return partial state if it cannot find or open the libprocess pid file. Specifically, it does not recover the 'completed' flag.

However, if the slave has removed the executor (because launch failed or the executor failed to register) the sentinel flag will be set and this fact should be recovered. This ensures that container recovery is not attempted later.

This was discovered when the LinuxLauncher failed to recover because it was asked to recover two containers with the same forkedPid. Investigation showed the executors both OOM'ed before registering, i.e., no libprocess pid file was present. However, the containerizer had detected the OOM, destroyed the container, and notified the slave which cleaned everything up: failing the task and calling removeExecutor (which writes the completed sentinel file.)",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2056,1.0,"Refactor fetcher code in preparation for fetcher cache Refactor/rearrange fetcher-related code so that cache functionality can be dropped in. One could do both together in one go. This is splitting up reviews into smaller chunks. It will not immediately be obvious how this change will be used later, but it will look better-factored and still do the exact same thing as before. In particular, a download routine to be reused several times in launcher/fetcher will be factored out and the remainder of fetcher-related code can be moved from the containerizer realm into fetcher.cpp.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2058,1.0,"Deprecate stats.json endpoints for Master and Slave With the introduction of the libprocess {{/metrics/snapshot}} endpoint, metrics are now duplicated in the Master and Slave between this and {{stats.json}}. We should deprecate the {{stats.json}} endpoints.

Manual inspection of {{stats.json}} shows that all metrics are now covered by the new endpoint for Master and Slave.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2062,3.0,"Add InverseOffer to Event/Call API. The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.

One way to add this is to tack it on to the OFFERS Event:

","<code>
message Offers {
  repeated Offer offers = 1;
  repeated InverseOffer inverse_offers = 2;
}
<code></code></code>",0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2070,2.0,"Implement simple slave recovery behavior for fetcher cache Clean the fetcher cache completely upon slave restart/recovery. This implements correct, albeit not ideal behavior. More efficient schemes that restore knowledge about cached files or even resume downloads can be added later. ",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2072,8.0,"Fetcher cache eviction Delete files from the fetcher cache so that a given cache size is never exceeded. Succeed in doing so while concurrent downloads are on their way and new requests are pouring in.

Idea: measure the size of each download before it begins, make enough room before the download. This means that only download mechanisms that divulge the size before the main download will be supported. AFAWK, those in use so far have this property. 

The calculation of how much space to free needs to be under concurrency control, accumulating all space needed for competing, incomplete download requests. (The Python script that performs fetcher caching for Aurora does not seem to implement this. See https://gist.github.com/zmanji/f41df77510ef9d00265a, imagine several of these programs running concurrently, each one's _cache_eviction() call succeeding, each perceiving the SAME free space being available.)

Ultimately, a conflict resolution strategy is needed if just the downloads underway already exceed the cache capacity. Then, as a fallback, direct download into the work directory will be used for some tasks. TBD how to pick which task gets treated how. 

At first, only support copying of any downloaded files to the work directory for task execution. This isolates the task life cycle after starting a task from cache eviction considerations. 

(Later, we can add symbolic links that avoid copying. But then eviction of fetched files used by ongoing tasks must be blocked, which adds complexity. another future extension is MESOS-1667 ""Extract from URI while downloading into work dir"").
",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2074,5.0,"Fetcher cache test fixture To accelerate providing good test coverage for the fetcher cache (MESOS-336), we can provide a framework that canonicalizes creating and running a number of tasks and allows easy parametrization with combinations of the following:
- whether to cache or not
- whether make what has been downloaded executable or not
- whether to extract from an archive or not
- whether to download from a file system, http, or...

We can create a simple HHTP server in the test fixture to support the latter.

Furthermore, the tests need to be robust wrt. varying numbers of StatusUpdate messages. An accumulating update message sink that reports the final state is needed.

All this has already been programmed in this patch, just needs to be rebased:
https://reviews.apache.org/r/21316/",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2083,8.0,"Add documentation for maintenance primitives. We should provide some guiding documentation around the upcoming maintenance primitives in Mesos.

Specifically, we should ensure that general users, framework developers, and operators understand the notion of maintenance in Mesos. Some guidance and recommendations for the latter two audiences will be necessary.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2099,8.0,"Support acquiring/releasing resources with DiskInfo in allocator. The allocator needs to be changed because the resources are changing while we acquiring or releasing persistent disk resources (resources with DiskInfo). For example, when we release a persistent disk resource, we are changing the release with DiskInfo to a resource with the DiskInfo.",,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2100,8.0,"Implement master to slave protocol for persistent disk resources. We need to do the following:
1) Slave needs to send persisted resources when registering (or re-registering).
2) Master needs to send total persisted resources to slave by either re-using RunTask/UpdateFrameworkInfo or introduce new type of messages (like UpdateResources).",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2104,3.0,"Correct naming of cgroup memory statistics mem_rss_bytes is *not* RSS but is the total memory usage (memory.usage_in_bytes) of the cgroup, including file cache etc. Actual RSS is reported as mem_anon_bytes. These, and others, should be consistently named.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2128,2.0,"Turning on cgroups_limit_swap effectively disables memory isolation Our test runs show that enabling cgroups_limit_swap effectively disables memory isolation altogether.

Per: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html

""It is important to set the memory.limit_in_bytes parameter before setting the memory.memsw.limit_in_bytes parameter: attempting to do so in the reverse order results in an error. This is because memory.memsw.limit_in_bytes becomes available only after all memory limitations (previously set in memory.limit_in_bytes) are exhausted.""

Looks like the flag sets ""memory.memsw.limit_in_bytes"" if true and ""memory.limit_in_bytes"" if false, but should always set ""memory.limit_in_bytes"" and in addition set ""memory.memsw.limit_in_bytes"" if true. Otherwise the limits won't be set and enforced.

See: https://github.com/apache/mesos/blob/c8598f7f5a24a01b6a68e0f060b79662ee97af89/src/slave/containerizer/isolators/cgroups/mem.cpp#L365
",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2144,8.0,"Segmentation Fault in ExamplesTest.LowLevelSchedulerPthread Occured on review bot review of: https://reviews.apache.org/r/28262/#review62333

The review doesn't touch code related to the test (And doesn't break libprocess in general)

[ RUN      ] ExamplesTest.LowLevelSchedulerPthread
../../src/tests/script.cpp:83: Failure
Failed
low_level_scheduler_pthread_test.sh terminated with signal Segmentation fault
[  FAILED  ] ExamplesTest.LowLevelSchedulerPthread (7561 ms)

The test ",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2176,5.0,"Hierarchical allocator inconsistently accounts for reserved resources.  Looking through the allocator code for MESOS-2099, I see an issue with respect to accounting reserved resources in the sorters:

Within {{HierarchicalAllocatorProcess::allocate}}, only unreserved resources are accounted for in the sorters, whereas everywhere else (add/remove framework, add/remove slave) we account for both reserved and unreserved.

From git blame, it looks like this issue was introduced over a long course of refactoring and fixes to the allocator. My guess is that this was never caught due to the lack of unit-testability of the allocator (unnecessarily requires a master PID to use an allocator).

From my understanding, the two levels of the hierarchical sorter should have the following semantics:

# Level 1 sorts across roles. Only unreserved resources are shared across roles, and therefore the ""role sorter"" for level 1 should only account for the unreserved resource pool.
# Level 2 sorts across frameworks, within a role. Both unreserved and reserved resources are shared across frameworks within a role, and therefore the ""framework sorters"" for level 2 should each account for the reserved resource pool for the role, as well as the unreserved resources _allocated_ inside the role.",,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2182,3.0,"Performance issue in libprocess SocketManager. Noticed an issue in production under which the master is slow to respond after failover for ~15 minutes.

After looking at some perf data, the top offender is:

{noformat}
    12.02%  mesos-master  libmesos-0.21.0-rc3.so  [.] std::_Rb_tree<process::processbase*, process::processbase*,="""" std::_identity<process::processbase*="""">, std::less<process::processbase*>, std::allocator<process::processbase*> &gt;::erase(process::ProcessBase* const&amp;)
...
     3.29%  mesos-master  libmesos-0.21.0-rc3.so  [.] process::SocketManager::exited(process::ProcessBase*)
{noformat}

It appears that in the SocketManager, whenever an internal Process exits, we loop over all the links unnecessarily:

</process::processbase*></process::processbase*></process::processbase*,>","<code>
void SocketManager::exited(ProcessBase* process)
{
  // An exited event is enough to cause the process to get deleted
  // (e.g., by the garbage collector), which means we can't
  // dereference process (or even use the address) after we enqueue at
  // least one exited event. Thus, we save the process pid.
  const UPID pid = process-&gt;pid;

  // Likewise, we need to save the current time of the process so we
  // can update the clocks of linked processes as appropriate.
  const Time time = Clock::now(process);

  synchronized (this) {
    // Iterate through the links, removing any links the process might
    // have had and creating exited events for any linked processes.
    foreachpair (const UPID&amp; linkee, set<processbase*>&amp; processes, links) {
      processes.erase(process);

      if (linkee == pid) {
        foreach (ProcessBase* linker, processes) {
          CHECK(linker != process) &lt;&lt; ""Process linked with itself"";
          synchronized (timeouts) {
            if (Clock::paused()) {
              Clock::update(linker, time);
            }
          }
          linker-&gt;enqueue(new ExitedEvent(linkee));
        }
      }
    }

    links.erase(pid);
  }
}
<code>

On clusters with 10,000s of slaves, this means we hold the socket manager lock for a very expensive loop erasing nothing from a set! This is because, the master contains links from the Master Process to each slave. However, when a random ephemeral Process terminates, we don't need to loop over each slave link.

While we hold this lock, the following calls will block:

<code>
class SocketManager
{
public:
  Socket accepted(int s);
  void link(ProcessBase* process, const UPID&amp; to);
  PID<httpproxy> proxy(const Socket&amp; socket);
  void send(Encoder* encoder, bool persist);
  void send(const Response&amp; response,
            const Request&amp; request,
            const Socket&amp; socket);
  void send(Message* message);
  Encoder* next(int s);
  void close(int s);
  void exited(const Node&amp; node);
  void exited(ProcessBase* process);
...
<code>

As a result, the slave observers and the master can block calling send()!

Short term, we will try to fix this issue by removing the unnecessary looping. Longer term, it would be nice to avoid all this locking when sending on independent sockets.</code></httpproxy></code></code></processbase*></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2184,1.0,deprecate unused flag 'cgroups_subsystems' cgroups_subsystems is a slave flag that is no longer used and should be deprecated.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2225,2.0,"FaultToleranceTest.ReregisterFrameworkExitedExecutor is flaky Observed this on internal CI.

","<code>
[ RUN      ] FaultToleranceTest.ReregisterFrameworkExitedExecutor
Using temporary directory '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_yNprKi'
I0114 18:50:51.461186  4720 leveldb.cpp:176] Opened db in 4.866948ms
I0114 18:50:51.462057  4720 leveldb.cpp:183] Compacted db in 472256ns
I0114 18:50:51.462514  4720 leveldb.cpp:198] Created db iterator in 42905ns
I0114 18:50:51.462784  4720 leveldb.cpp:204] Seeked to beginning of db in 21630ns
I0114 18:50:51.463068  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 19967ns
I0114 18:50:51.463485  4720 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0114 18:50:51.464555  4737 recover.cpp:449] Starting replica recovery
I0114 18:50:51.465188  4737 recover.cpp:475] Replica is in EMPTY status
I0114 18:50:51.467324  4741 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0114 18:50:51.470118  4736 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0114 18:50:51.475424  4739 recover.cpp:566] Updating replica status to STARTING
I0114 18:50:51.476553  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 107545ns
I0114 18:50:51.476862  4739 replica.cpp:323] Persisted replica status to STARTING
I0114 18:50:51.477309  4739 recover.cpp:475] Replica is in STARTING status
I0114 18:50:51.479109  4734 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0114 18:50:51.481274  4738 recover.cpp:195] Received a recover response from a replica in STARTING status
I0114 18:50:51.482324  4738 recover.cpp:566] Updating replica status to VOTING
I0114 18:50:51.482913  4738 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 66011ns
I0114 18:50:51.483186  4738 replica.cpp:323] Persisted replica status to VOTING
I0114 18:50:51.483608  4738 recover.cpp:580] Successfully joined the Paxos group
I0114 18:50:51.484031  4738 recover.cpp:464] Recover process terminated
I0114 18:50:51.554949  4734 master.cpp:262] Master 20150114-185051-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018
I0114 18:50:51.555785  4734 master.cpp:308] Master only allowing authenticated frameworks to register
I0114 18:50:51.556046  4734 master.cpp:313] Master only allowing authenticated slaves to register
I0114 18:50:51.556426  4734 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_yNprKi/credentials'
I0114 18:50:51.557003  4734 master.cpp:357] Authorization enabled
I0114 18:50:51.558007  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0114 18:50:51.558521  4741 whitelist_watcher.cpp:65] No whitelist given
I0114 18:50:51.562185  4734 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185051-2272962752-57018-4720
I0114 18:50:51.562680  4734 master.cpp:1232] Elected as the leading master!
I0114 18:50:51.562950  4734 master.cpp:1050] Recovering from registrar
I0114 18:50:51.564506  4736 registrar.cpp:313] Recovering registrar
I0114 18:50:51.566162  4737 log.cpp:660] Attempting to start the writer
I0114 18:50:51.568691  4741 replica.cpp:477] Replica received implicit promise request with proposal 1
I0114 18:50:51.569154  4741 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 106885ns
I0114 18:50:51.569504  4741 replica.cpp:345] Persisted promised to 1
I0114 18:50:51.573277  4740 coordinator.cpp:230] Coordinator attemping to fill missing position
I0114 18:50:51.575623  4739 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0114 18:50:51.576133  4739 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 86360ns
I0114 18:50:51.576449  4739 replica.cpp:679] Persisted action at 0
I0114 18:50:51.586966  4736 replica.cpp:511] Replica received write request for position 0
I0114 18:50:51.587666  4736 leveldb.cpp:438] Reading position from leveldb took 60621ns
I0114 18:50:51.588043  4736 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 81094ns
I0114 18:50:51.588374  4736 replica.cpp:679] Persisted action at 0
I0114 18:50:51.589418  4736 replica.cpp:658] Replica received learned notice for position 0
I0114 18:50:51.590428  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 106648ns
I0114 18:50:51.590840  4736 replica.cpp:679] Persisted action at 0
I0114 18:50:51.591104  4736 replica.cpp:664] Replica learned NOP action at position 0
I0114 18:50:51.592260  4734 log.cpp:676] Writer started with ending position 0
I0114 18:50:51.594172  4739 leveldb.cpp:438] Reading position from leveldb took 52163ns
I0114 18:50:51.600744  4736 registrar.cpp:346] Successfully fetched the registry (0B) in 35968us
I0114 18:50:51.601646  4736 registrar.cpp:445] Applied 1 operations in 184502ns; attempting to update the 'registry'
I0114 18:50:51.604329  4737 log.cpp:684] Attempting to append 130 bytes to the log
I0114 18:50:51.604966  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0114 18:50:51.606449  4737 replica.cpp:511] Replica received write request for position 1
I0114 18:50:51.606937  4737 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 84877ns
I0114 18:50:51.607199  4737 replica.cpp:679] Persisted action at 1
I0114 18:50:51.611934  4741 replica.cpp:658] Replica received learned notice for position 1
I0114 18:50:51.612423  4741 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 113059ns
I0114 18:50:51.612794  4741 replica.cpp:679] Persisted action at 1
I0114 18:50:51.613056  4741 replica.cpp:664] Replica learned APPEND action at position 1
I0114 18:50:51.614598  4741 log.cpp:703] Attempting to truncate the log to 1
I0114 18:50:51.615157  4741 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0114 18:50:51.616458  4737 replica.cpp:511] Replica received write request for position 2
I0114 18:50:51.616902  4737 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71716ns
I0114 18:50:51.617168  4737 replica.cpp:679] Persisted action at 2
I0114 18:50:51.618505  4740 replica.cpp:658] Replica received learned notice for position 2
I0114 18:50:51.619031  4740 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 78481ns
I0114 18:50:51.619567  4740 leveldb.cpp:401] Deleting ~1 keys from leveldb took 59638ns
I0114 18:50:51.619832  4740 replica.cpp:679] Persisted action at 2
I0114 18:50:51.620101  4740 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0114 18:50:51.621757  4736 registrar.cpp:490] Successfully updated the 'registry' in 19.78496ms
I0114 18:50:51.622658  4736 registrar.cpp:376] Successfully recovered registrar
I0114 18:50:51.623261  4736 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0114 18:50:51.670349  4739 slave.cpp:173] Slave started on 115)@192.168.122.135:57018
I0114 18:50:51.671133  4739 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/credential'
I0114 18:50:51.671685  4739 slave.cpp:282] Slave using credential for: test-principal
I0114 18:50:51.672245  4739 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:50:51.673360  4739 slave.cpp:329] Slave hostname: fedora-19
I0114 18:50:51.673660  4739 slave.cpp:330] Slave checkpoint: false
W0114 18:50:51.674052  4739 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0114 18:50:51.677234  4737 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/meta'
I0114 18:50:51.684973  4739 status_update_manager.cpp:197] Recovering status update manager
I0114 18:50:51.687644  4739 slave.cpp:3519] Finished recovery
I0114 18:50:51.688698  4737 slave.cpp:613] New master detected at master@192.168.122.135:57018
I0114 18:50:51.688902  4734 status_update_manager.cpp:171] Pausing sending status updates
I0114 18:50:51.689482  4737 slave.cpp:676] Authenticating with master master@192.168.122.135:57018
I0114 18:50:51.689910  4737 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0114 18:50:51.690577  4741 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:50:51.691453  4737 slave.cpp:649] Detecting new master
I0114 18:50:51.691864  4741 master.cpp:4130] Authenticating slave(115)@192.168.122.135:57018
I0114 18:50:51.692369  4741 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:50:51.693208  4741 authenticator.hpp:170] Creating new server SASL connection
I0114 18:50:51.694598  4738 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:50:51.694893  4738 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:50:51.695329  4741 authenticator.hpp:276] Received SASL authentication start
I0114 18:50:51.695641  4741 authenticator.hpp:398] Authentication requires more steps
I0114 18:50:51.696028  4736 authenticatee.hpp:275] Received SASL authentication step
I0114 18:50:51.696486  4741 authenticator.hpp:304] Received SASL authentication step
I0114 18:50:51.696753  4741 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:50:51.697041  4741 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:50:51.697343  4741 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:50:51.697685  4741 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:50:51.697998  4741 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:51.698251  4741 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:51.698580  4741 authenticator.hpp:390] Authentication success
I0114 18:50:51.698927  4735 authenticatee.hpp:315] Authentication success
I0114 18:50:51.705123  4741 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(115)@192.168.122.135:57018
I0114 18:50:51.705847  4720 sched.cpp:151] Version: 0.22.0
I0114 18:50:51.707159  4736 sched.cpp:248] New master detected at master@192.168.122.135:57018
I0114 18:50:51.707523  4736 sched.cpp:304] Authenticating with master master@192.168.122.135:57018
I0114 18:50:51.707792  4736 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0114 18:50:51.708412  4736 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:50:51.709316  4735 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:50:51.709723  4737 master.cpp:4130] Authenticating scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.710274  4737 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:50:51.710739  4735 slave.cpp:1075] Will retry registration in 17.028024ms if necessary
I0114 18:50:51.711304  4737 master.cpp:3276] Registering slave at slave(115)@192.168.122.135:57018 (fedora-19) with id 20150114-185051-2272962752-57018-4720-S0
I0114 18:50:51.711459  4738 authenticator.hpp:170] Creating new server SASL connection
I0114 18:50:51.713142  4739 registrar.cpp:445] Applied 1 operations in 100530ns; attempting to update the 'registry'
I0114 18:50:51.713465  4738 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:50:51.715435  4738 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:50:51.715963  4740 authenticator.hpp:276] Received SASL authentication start
I0114 18:50:51.716258  4740 authenticator.hpp:398] Authentication requires more steps
I0114 18:50:51.716524  4740 authenticatee.hpp:275] Received SASL authentication step
I0114 18:50:51.716784  4740 authenticator.hpp:304] Received SASL authentication step
I0114 18:50:51.716979  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:50:51.717139  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:50:51.717315  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:50:51.717542  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:50:51.717703  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:51.717864  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:51.718040  4740 authenticator.hpp:390] Authentication success
I0114 18:50:51.718292  4740 authenticatee.hpp:315] Authentication success
I0114 18:50:51.718454  4738 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.719012  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:50:51.719364  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018
I0114 18:50:51.719702  4740 sched.cpp:548] Will retry registration in 746.539282ms if necessary
I0114 18:50:51.719902  4735 master.cpp:1417] Received registration request for framework 'default' at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.720232  4735 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0114 18:50:51.722206  4735 master.cpp:1481] Registering framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.720927  4737 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:50:51.722924  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0114 18:50:51.724269  4737 replica.cpp:511] Replica received write request for position 3
I0114 18:50:51.724817  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 116638ns
I0114 18:50:51.728560  4737 replica.cpp:679] Persisted action at 3
I0114 18:50:51.726066  4736 sched.cpp:442] Framework registered with 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.728879  4736 sched.cpp:456] Scheduler::registered took 34885ns
I0114 18:50:51.725520  4735 hierarchical_allocator_process.hpp:319] Added framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.731864  4735 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0114 18:50:51.732038  4735 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 214728ns
I0114 18:50:51.733106  4738 replica.cpp:658] Replica received learned notice for position 3
I0114 18:50:51.733340  4738 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 83165ns
I0114 18:50:51.733538  4738 replica.cpp:679] Persisted action at 3
I0114 18:50:51.733705  4738 replica.cpp:664] Replica learned APPEND action at position 3
I0114 18:50:51.735610  4738 registrar.cpp:490] Successfully updated the 'registry' in 21.936128ms
I0114 18:50:51.735805  4739 log.cpp:703] Attempting to truncate the log to 3
I0114 18:50:51.736445  4739 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0114 18:50:51.737664  4739 replica.cpp:511] Replica received write request for position 4
I0114 18:50:51.738013  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 72906ns
I0114 18:50:51.738255  4739 replica.cpp:679] Persisted action at 4
I0114 18:50:51.743397  4734 replica.cpp:658] Replica received learned notice for position 4
I0114 18:50:51.743628  4734 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 78832ns
I0114 18:50:51.743837  4734 leveldb.cpp:401] Deleting ~2 keys from leveldb took 63991ns
I0114 18:50:51.744004  4734 replica.cpp:679] Persisted action at 4
I0114 18:50:51.744168  4734 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0114 18:50:51.745537  4738 master.cpp:3330] Registered slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:50:51.745968  4734 hierarchical_allocator_process.hpp:453] Added slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0114 18:50:51.746070  4735 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185051-2272962752-57018-4720-S0
I0114 18:50:51.751437  4741 status_update_manager.cpp:178] Resuming sending status updates
I0114 18:50:51.752428  4740 master.cpp:4072] Sending 1 offers to framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.753764  4740 sched.cpp:605] Scheduler::resourceOffers took 751714ns
I0114 18:50:51.754812  4740 master.cpp:2541] Processing reply for offers: [ 20150114-185051-2272962752-57018-4720-O0 ] on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) for framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.755040  4740 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
W0114 18:50:51.756431  4741 master.cpp:2124] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0114 18:50:51.756652  4741 master.cpp:2136] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0114 18:50:51.757284  4741 master.hpp:766] Adding task 0 with resources cpus(*):1; mem(*):16 on slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19)
I0114 18:50:51.757733  4734 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185051-2272962752-57018-4720-S0 in 9.535066ms
I0114 18:50:51.758117  4735 slave.cpp:2588] Received ping from slave-observer(95)@192.168.122.135:57018
I0114 18:50:51.758630  4741 master.cpp:2897] Launching task 0 of framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 with resources cpus(*):1; mem(*):16 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:51.759526  4741 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:50:51.759796  4737 slave.cpp:1130] Got assigned task 0 for framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.761184  4737 slave.cpp:1245] Launching task 0 for framework 20150114-185051-2272962752-57018-47",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2226,3.0,"HookTest.VerifySlaveLaunchExecutorHook is flaky Observed this on internal CI

","<code>
[ RUN      ] HookTest.VerifySlaveLaunchExecutorHook
Using temporary directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME'
I0114 18:51:34.659353  4720 leveldb.cpp:176] Opened db in 1.255951ms
I0114 18:51:34.662112  4720 leveldb.cpp:183] Compacted db in 596090ns
I0114 18:51:34.662364  4720 leveldb.cpp:198] Created db iterator in 177877ns
I0114 18:51:34.662719  4720 leveldb.cpp:204] Seeked to beginning of db in 19709ns
I0114 18:51:34.663010  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 18208ns
I0114 18:51:34.663312  4720 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0114 18:51:34.664266  4735 recover.cpp:449] Starting replica recovery
I0114 18:51:34.664908  4735 recover.cpp:475] Replica is in EMPTY status
I0114 18:51:34.667842  4734 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0114 18:51:34.669117  4735 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0114 18:51:34.677913  4735 recover.cpp:566] Updating replica status to STARTING
I0114 18:51:34.683157  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 137939ns
I0114 18:51:34.683507  4735 replica.cpp:323] Persisted replica status to STARTING
I0114 18:51:34.684013  4735 recover.cpp:475] Replica is in STARTING status
I0114 18:51:34.685554  4738 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0114 18:51:34.696512  4736 recover.cpp:195] Received a recover response from a replica in STARTING status
I0114 18:51:34.700552  4735 recover.cpp:566] Updating replica status to VOTING
I0114 18:51:34.701128  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 115624ns
I0114 18:51:34.701478  4735 replica.cpp:323] Persisted replica status to VOTING
I0114 18:51:34.701817  4735 recover.cpp:580] Successfully joined the Paxos group
I0114 18:51:34.702569  4735 recover.cpp:464] Recover process terminated
I0114 18:51:34.716439  4736 master.cpp:262] Master 20150114-185134-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018
I0114 18:51:34.716913  4736 master.cpp:308] Master only allowing authenticated frameworks to register
I0114 18:51:34.717136  4736 master.cpp:313] Master only allowing authenticated slaves to register
I0114 18:51:34.717488  4736 credentials.hpp:36] Loading credentials for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME/credentials'
I0114 18:51:34.718077  4736 master.cpp:357] Authorization enabled
I0114 18:51:34.719238  4738 whitelist_watcher.cpp:65] No whitelist given
I0114 18:51:34.719755  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0114 18:51:34.722584  4736 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185134-2272962752-57018-4720
I0114 18:51:34.722865  4736 master.cpp:1232] Elected as the leading master!
I0114 18:51:34.723310  4736 master.cpp:1050] Recovering from registrar
I0114 18:51:34.723760  4734 registrar.cpp:313] Recovering registrar
I0114 18:51:34.725229  4740 log.cpp:660] Attempting to start the writer
I0114 18:51:34.727893  4739 replica.cpp:477] Replica received implicit promise request with proposal 1
I0114 18:51:34.728425  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 114781ns
I0114 18:51:34.728662  4739 replica.cpp:345] Persisted promised to 1
I0114 18:51:34.731271  4741 coordinator.cpp:230] Coordinator attemping to fill missing position
I0114 18:51:34.733223  4734 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0114 18:51:34.734076  4734 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 87441ns
I0114 18:51:34.734441  4734 replica.cpp:679] Persisted action at 0
I0114 18:51:34.740272  4739 replica.cpp:511] Replica received write request for position 0
I0114 18:51:34.740910  4739 leveldb.cpp:438] Reading position from leveldb took 59846ns
I0114 18:51:34.741672  4739 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 189259ns
I0114 18:51:34.741919  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.743000  4739 replica.cpp:658] Replica received learned notice for position 0
I0114 18:51:34.746844  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 328487ns
I0114 18:51:34.747118  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.747553  4739 replica.cpp:664] Replica learned NOP action at position 0
I0114 18:51:34.751344  4737 log.cpp:676] Writer started with ending position 0
I0114 18:51:34.753504  4734 leveldb.cpp:438] Reading position from leveldb took 61183ns
I0114 18:51:34.762962  4737 registrar.cpp:346] Successfully fetched the registry (0B) in 38.907904ms
I0114 18:51:34.763610  4737 registrar.cpp:445] Applied 1 operations in 67206ns; attempting to update the 'registry'
I0114 18:51:34.766079  4736 log.cpp:684] Attempting to append 130 bytes to the log
I0114 18:51:34.766769  4736 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0114 18:51:34.768215  4741 replica.cpp:511] Replica received write request for position 1
I0114 18:51:34.768759  4741 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 87970ns
I0114 18:51:34.768995  4741 replica.cpp:679] Persisted action at 1
I0114 18:51:34.770691  4736 replica.cpp:658] Replica received learned notice for position 1
I0114 18:51:34.771273  4736 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 83590ns
I0114 18:51:34.771579  4736 replica.cpp:679] Persisted action at 1
I0114 18:51:34.771917  4736 replica.cpp:664] Replica learned APPEND action at position 1
I0114 18:51:34.773252  4738 log.cpp:703] Attempting to truncate the log to 1
I0114 18:51:34.773756  4735 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0114 18:51:34.775552  4736 replica.cpp:511] Replica received write request for position 2
I0114 18:51:34.775846  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71503ns
I0114 18:51:34.776695  4736 replica.cpp:679] Persisted action at 2
I0114 18:51:34.785259  4739 replica.cpp:658] Replica received learned notice for position 2
I0114 18:51:34.786252  4737 registrar.cpp:490] Successfully updated the 'registry' in 22.340864ms
I0114 18:51:34.787094  4737 registrar.cpp:376] Successfully recovered registrar
I0114 18:51:34.787749  4737 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0114 18:51:34.787282  4739 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 707150ns
I0114 18:51:34.788692  4739 leveldb.cpp:401] Deleting ~1 keys from leveldb took 60262ns
I0114 18:51:34.789048  4739 replica.cpp:679] Persisted action at 2
I0114 18:51:34.789329  4739 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0114 18:51:34.819548  4738 slave.cpp:173] Slave started on 171)@192.168.122.135:57018
I0114 18:51:34.820530  4738 credentials.hpp:84] Loading credential for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/credential'
I0114 18:51:34.820952  4738 slave.cpp:282] Slave using credential for: test-principal
I0114 18:51:34.821516  4738 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.822217  4738 slave.cpp:329] Slave hostname: fedora-19
I0114 18:51:34.822502  4738 slave.cpp:330] Slave checkpoint: false
W0114 18:51:34.822857  4738 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0114 18:51:34.824998  4737 state.cpp:33] Recovering state from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/meta'
I0114 18:51:34.834015  4738 status_update_manager.cpp:197] Recovering status update manager
I0114 18:51:34.834810  4738 slave.cpp:3519] Finished recovery
I0114 18:51:34.835906  4734 status_update_manager.cpp:171] Pausing sending status updates
I0114 18:51:34.836423  4738 slave.cpp:613] New master detected at master@192.168.122.135:57018
I0114 18:51:34.836908  4738 slave.cpp:676] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.837190  4738 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0114 18:51:34.837820  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.838784  4738 slave.cpp:649] Detecting new master
I0114 18:51:34.839306  4740 master.cpp:4130] Authenticating slave(171)@192.168.122.135:57018
I0114 18:51:34.839957  4740 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.841236  4740 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.842681  4741 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.843118  4741 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.843581  4740 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.843962  4740 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.844357  4740 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.844780  4740 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.845113  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.845507  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.845835  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.846238  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.846542  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.846806  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.847110  4740 authenticator.hpp:390] Authentication success
I0114 18:51:34.847808  4734 authenticatee.hpp:315] Authentication success
I0114 18:51:34.851029  4734 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.851608  4737 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(171)@192.168.122.135:57018
I0114 18:51:34.854962  4720 sched.cpp:151] Version: 0.22.0
I0114 18:51:34.856674  4734 slave.cpp:1075] Will retry registration in 3.085482ms if necessary
I0114 18:51:34.857434  4739 sched.cpp:248] New master detected at master@192.168.122.135:57018
I0114 18:51:34.861433  4739 sched.cpp:304] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.861693  4739 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0114 18:51:34.857795  4737 master.cpp:3276] Registering slave at slave(171)@192.168.122.135:57018 (fedora-19) with id 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.862951  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.863919  4735 registrar.cpp:445] Applied 1 operations in 120272ns; attempting to update the 'registry'
I0114 18:51:34.864645  4738 master.cpp:4130] Authenticating scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.865033  4738 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.866904  4738 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.868840  4737 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.869125  4737 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.869523  4737 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.869835  4737 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.870213  4737 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.870622  4737 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.870946  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.871219  4737 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.871554  4737 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.871968  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.872297  4737 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.872655  4737 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.873024  4737 authenticator.hpp:390] Authentication success
I0114 18:51:34.873428  4737 authenticatee.hpp:315] Authentication success
I0114 18:51:34.873632  4739 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.875006  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.875319  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018
I0114 18:51:34.876200  4740 sched.cpp:548] Will retry registration in 1.952991346secs if necessary
I0114 18:51:34.876729  4738 master.cpp:1417] Received registration request for framework 'default' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.877040  4738 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0114 18:51:34.878059  4738 master.cpp:1481] Registering framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.878473  4739 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:51:34.879464  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0114 18:51:34.880116  4734 hierarchical_allocator_process.hpp:319] Added framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.880470  4734 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0114 18:51:34.882331  4734 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 1.901284ms
I0114 18:51:34.884024  4741 sched.cpp:442] Framework registered with 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.884454  4741 sched.cpp:456] Scheduler::registered took 44320ns
I0114 18:51:34.881965  4737 replica.cpp:511] Replica received write request for position 3
I0114 18:51:34.885218  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 134480ns
I0114 18:51:34.885716  4737 replica.cpp:679] Persisted action at 3
I0114 18:51:34.886034  4739 slave.cpp:1075] Will retry registration in 22.947772ms if necessary
I0114 18:51:34.886291  4740 master.cpp:3264] Ignoring register slave message from slave(171)@192.168.122.135:57018 (fedora-19) as admission is already in progress
I0114 18:51:34.894690  4736 replica.cpp:658] Replica received learned notice for position 3
I0114 18:51:34.898638  4736 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 215501ns
I0114 18:51:34.899055  4736 replica.cpp:679] Persisted action at 3
I0114 18:51:34.899416  4736 replica.cpp:664] Replica learned APPEND action at position 3
I0114 18:51:34.911782  4736 registrar.cpp:490] Successfully updated the 'registry' in 46.176768ms
I0114 18:51:34.912286  4740 log.cpp:703] Attempting to truncate the log to 3
I0114 18:51:34.913108  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0114 18:51:34.915027  4736 master.cpp:3330] Registered slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.915642  4735 hierarchical_allocator_process.hpp:453] Added slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0114 18:51:34.917809  4735 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185134-2272962752-57018-4720-S0 in 514027ns
I0114 18:51:34.916689  4738 replica.cpp:511] Replica received write request for position 4
I0114 18:51:34.915784  4741 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.919293  4741 slave.cpp:2588] Received ping from slave-observer(156)@192.168.122.135:57018
I0114 18:51:34.919775  4740 status_update_manager.cpp:178] Resuming sending status updates
I0114 18:51:34.920374  4736 master.cpp:4072] Sending 1 offers to framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.920569  4738 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 1.540136ms
I0114 18:51:34.921092  4738 replica.cpp:679] Persisted action at 4
I0114 18:51:34.927111  4735 replica.cpp:658] Replica received learned notice for position 4
I0114 18:51:34.927299  4734 sched.cpp:605] Scheduler::resourceOffers took 1.335524ms
I0114 18:51:34.930418  4735 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.596377ms
I0114 18:51:34.930882  4735 leveldb.cpp:401] Deleting ~2 keys from leveldb took 67578ns
I0114 18:51:34.931115  4735 replica.cpp:679] Persisted action at 4
I0114 18:51:34.931529  4735 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0114 18:51:34.930356  4734 master.cpp:2541] Processing reply for offers: [ 20150114-185134-2272962752-57018-4720-O0 ] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) for framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.932834  4734 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0114 18:51:34.934442  4736 master.cpp:2124] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0114 18:51:34.934960  4736 master.cpp:2136] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0114 18:51:34.935878  4736 master.hpp:766] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19)
I0114 18:51:34.939453  4738 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.939950  4736 master.cpp:2897] Launching task 1 of framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2230,3.0,Update RateLimiter to allow the acquired future to be discarded Currently there is no way for the future returned by RateLimiter's acquire() to be discarded by the user of the limiter. This is useful in cases where the user is no longer interested in the permit. See MESOS-1148 for an example use case.,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2232,3.0,"Suppress MockAllocator::transformAllocation() warnings. After transforming allocated resources feature was added to allocator, a number of warnings are popping out for allocator tests. Commits leading to this behaviour:
{{dacc88292cc13d4b08fe8cda4df71110a96cb12a}}
{{5a02d5bdc75d3b1149dcda519016374be06ec6bd}}
corresponding reviews:
https://reviews.apache.org/r/29083
https://reviews.apache.org/r/29084

Here is an example:
","<code>
[ RUN ] MasterAllocatorTest/0.FrameworkReregistersFirst GMOCK WARNING: Uninteresting mock function call - taking default action specified at: ../../../src/tests/mesos.hpp:719: Function call: transformAllocation(@0x7fd3bb5274d8 20150115-185632-1677764800-59671-44186-0000, @0x7fd3bb5274f8 20150115-185632-1677764800-59671-44186-S0, @0x1119140e0 16-byte object <f0-5e 00-00="""" 52-bb="""" c0-5f="""" d3-7f="""">) Stack trace: [ OK ] MasterAllocatorTest/0.FrameworkReregistersFirst (204 ms)
<code></code></f0-5e></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2233,5.0,"Run ASF CI mesos builds inside docker There are several limitations to mesos projects current state of CI, which is run on builds.a.o

--&gt; Only runs on Ubuntu
--&gt; Doesn't run any tests that deal with cgroups
--&gt; Doesn't run any tests that need root permissions

Now that ASF CI supports docker (https://issues.apache.org/jira/browse/BUILDS-25), it would be great for the Mesos project to use it.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
MESOS-2241,1.0,"DiskUsageCollectorTest.SymbolicLink test is flaky Observed this on a local machine running linux w/ sudo.

","<code>
[ RUN      ] DiskUsageCollectorTest.SymbolicLink
../../src/tests/disk_quota_tests.cpp:138: Failure
Expected: (usage1.get()) &lt; (Kilobytes(16)), actual: 24KB vs 8-byte object &lt;00-40 00-00 00-00 00-00&gt;
[  FAILED  ] DiskUsageCollectorTest.SymbolicLink (201 ms)
<code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2273,1.0,"Add ""tests"" target to Makefile for building-but-not-running tests. 'make check' allows one to build and run the test suite. However, often we just want to build the tests.  Currently, this is done by setting GTEST_FILTER to an empty string.

It will be nice to have a dedicated target such as 'make tests' that allows one to build the test suite without running it.",,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2283,1.0,"SlaveRecoveryTest.ReconcileKillTask is flaky. Saw this on an internal CI:

{noformat}
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_D5wSwg'
I0126 19:10:52.005317 13291 leveldb.cpp:176] Opened db in 978670ns
I0126 19:10:52.006155 13291 leveldb.cpp:183] Compacted db in 541346ns
I0126 19:10:52.006494 13291 leveldb.cpp:198] Created db iterator in 24562ns
I0126 19:10:52.006798 13291 leveldb.cpp:204] Seeked to beginning of db in 3254ns
I0126 19:10:52.007036 13291 leveldb.cpp:273] Iterated through 0 keys in the db in 949ns
I0126 19:10:52.007369 13291 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0126 19:10:52.008362 13308 recover.cpp:449] Starting replica recovery
I0126 19:10:52.009141 13308 recover.cpp:475] Replica is in EMPTY status
I0126 19:10:52.016494 13308 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0126 19:10:52.017333 13309 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0126 19:10:52.018244 13309 recover.cpp:566] Updating replica status to STARTING
I0126 19:10:52.019064 13305 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 113577ns
I0126 19:10:52.019487 13305 replica.cpp:323] Persisted replica status to STARTING
I0126 19:10:52.019937 13309 recover.cpp:475] Replica is in STARTING status
I0126 19:10:52.021492 13307 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0126 19:10:52.022665 13309 recover.cpp:195] Received a recover response from a replica in STARTING status
I0126 19:10:52.027971 13312 recover.cpp:566] Updating replica status to VOTING
I0126 19:10:52.028590 13312 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 78452ns
I0126 19:10:52.028869 13312 replica.cpp:323] Persisted replica status to VOTING
I0126 19:10:52.029252 13312 recover.cpp:580] Successfully joined the Paxos group
I0126 19:10:52.030828 13307 recover.cpp:464] Recover process terminated
I0126 19:10:52.049947 13306 master.cpp:262] Master 20150126-191052-2272962752-35545-13291 (fedora-19) started on 192.168.122.135:35545
I0126 19:10:52.050499 13306 master.cpp:308] Master only allowing authenticated frameworks to register
I0126 19:10:52.050765 13306 master.cpp:313] Master only allowing authenticated slaves to register
I0126 19:10:52.051048 13306 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_D5wSwg/credentials'
I0126 19:10:52.051589 13306 master.cpp:357] Authorization enabled
I0126 19:10:52.052531 13305 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0126 19:10:52.052881 13311 whitelist_watcher.cpp:65] No whitelist given
I0126 19:10:52.055524 13306 master.cpp:1219] The newly elected leader is master@192.168.122.135:35545 with id 20150126-191052-2272962752-35545-13291
I0126 19:10:52.056226 13306 master.cpp:1232] Elected as the leading master!
I0126 19:10:52.056639 13306 master.cpp:1050] Recovering from registrar
I0126 19:10:52.057045 13307 registrar.cpp:313] Recovering registrar
I0126 19:10:52.058554 13312 log.cpp:660] Attempting to start the writer
I0126 19:10:52.060868 13309 replica.cpp:477] Replica received implicit promise request with proposal 1
I0126 19:10:52.061691 13309 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 91680ns
I0126 19:10:52.062261 13309 replica.cpp:345] Persisted promised to 1
I0126 19:10:52.064559 13310 coordinator.cpp:230] Coordinator attemping to fill missing position
I0126 19:10:52.069105 13311 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0126 19:10:52.069860 13311 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 94858ns
I0126 19:10:52.070350 13311 replica.cpp:679] Persisted action at 0
I0126 19:10:52.080348 13305 replica.cpp:511] Replica received write request for position 0
I0126 19:10:52.081153 13305 leveldb.cpp:438] Reading position from leveldb took 62247ns
I0126 19:10:52.081676 13305 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 81487ns
I0126 19:10:52.082053 13305 replica.cpp:679] Persisted action at 0
I0126 19:10:52.083566 13309 replica.cpp:658] Replica received learned notice for position 0
I0126 19:10:52.085734 13309 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 283144ns
I0126 19:10:52.086067 13309 replica.cpp:679] Persisted action at 0
I0126 19:10:52.086448 13309 replica.cpp:664] Replica learned NOP action at position 0
I0126 19:10:52.089784 13306 log.cpp:676] Writer started with ending position 0
I0126 19:10:52.093415 13309 leveldb.cpp:438] Reading position from leveldb took 66744ns
I0126 19:10:52.104814 13306 registrar.cpp:346] Successfully fetched the registry (0B) in 47.451136ms
I0126 19:10:52.105731 13306 registrar.cpp:445] Applied 1 operations in 42124ns; attempting to update the 'registry'
I0126 19:10:52.111935 13305 log.cpp:684] Attempting to append 131 bytes to the log
I0126 19:10:52.112754 13305 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0126 19:10:52.114297 13308 replica.cpp:511] Replica received write request for position 1
I0126 19:10:52.114908 13308 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 98332ns
I0126 19:10:52.115387 13308 replica.cpp:679] Persisted action at 1
I0126 19:10:52.117277 13305 replica.cpp:658] Replica received learned notice for position 1
I0126 19:10:52.118142 13305 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 227799ns
I0126 19:10:52.118621 13305 replica.cpp:679] Persisted action at 1
I0126 19:10:52.118979 13305 replica.cpp:664] Replica learned APPEND action at position 1
I0126 19:10:52.121311 13305 registrar.cpp:490] Successfully updated the 'registry' in 15.161088ms
I0126 19:10:52.121548 13311 log.cpp:703] Attempting to truncate the log to 1
I0126 19:10:52.122697 13311 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0126 19:10:52.124316 13307 replica.cpp:511] Replica received write request for position 2
I0126 19:10:52.124913 13307 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 87281ns
I0126 19:10:52.125334 13307 replica.cpp:679] Persisted action at 2
I0126 19:10:52.127018 13311 replica.cpp:658] Replica received learned notice for position 2
I0126 19:10:52.127835 13311 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 201050ns
I0126 19:10:52.128232 13311 leveldb.cpp:401] Deleting ~1 keys from leveldb took 78012ns
I0126 19:10:52.128835 13305 registrar.cpp:376] Successfully recovered registrar
I0126 19:10:52.128551 13311 replica.cpp:679] Persisted action at 2
I0126 19:10:52.130105 13311 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0126 19:10:52.131479 13312 master.cpp:1077] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register
I0126 19:10:52.143465 13291 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
I0126 19:10:52.170471 13309 slave.cpp:173] Slave started on 101)@192.168.122.135:35545
I0126 19:10:52.171723 13309 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/credential'
I0126 19:10:52.172286 13309 slave.cpp:282] Slave using credential for: test-principal
I0126 19:10:52.172821 13309 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0126 19:10:52.173982 13309 slave.cpp:329] Slave hostname: fedora-19
I0126 19:10:52.174505 13309 slave.cpp:330] Slave checkpoint: true
I0126 19:10:52.179308 13309 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta'
I0126 19:10:52.180075 13308 status_update_manager.cpp:197] Recovering status update manager
I0126 19:10:52.180611 13308 containerizer.cpp:300] Recovering containerizer
I0126 19:10:52.182473 13309 slave.cpp:3519] Finished recovery
I0126 19:10:52.184403 13312 slave.cpp:613] New master detected at master@192.168.122.135:35545
I0126 19:10:52.184916 13312 slave.cpp:676] Authenticating with master master@192.168.122.135:35545
I0126 19:10:52.185230 13312 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0126 19:10:52.185715 13312 slave.cpp:649] Detecting new master
I0126 19:10:52.186420 13312 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:10:52.186002 13311 status_update_manager.cpp:171] Pausing sending status updates
I0126 19:10:52.188293 13312 master.cpp:4129] Authenticating slave(101)@192.168.122.135:35545
I0126 19:10:52.188748 13312 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:10:52.189525 13312 authenticator.hpp:170] Creating new server SASL connection
I0126 19:10:52.191082 13305 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:10:52.191550 13305 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:10:52.191990 13312 authenticator.hpp:276] Received SASL authentication start
I0126 19:10:52.192365 13312 authenticator.hpp:398] Authentication requires more steps
I0126 19:10:52.192800 13311 authenticatee.hpp:275] Received SASL authentication step
I0126 19:10:52.193244 13312 authenticator.hpp:304] Received SASL authentication step
I0126 19:10:52.193565 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:10:52.193902 13312 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:10:52.194301 13312 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:10:52.195669 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:10:52.196048 13312 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.196395 13312 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.196723 13312 authenticator.hpp:390] Authentication success
I0126 19:10:52.197206 13305 authenticatee.hpp:315] Authentication success
I0126 19:10:52.204121 13305 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:35545
I0126 19:10:52.204676 13310 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(101)@192.168.122.135:35545
I0126 19:10:52.205729 13305 slave.cpp:1075] Will retry registration in 5.608661ms if necessary
I0126 19:10:52.206451 13310 master.cpp:3275] Registering slave at slave(101)@192.168.122.135:35545 (fedora-19) with id 20150126-191052-2272962752-35545-13291-S0
I0126 19:10:52.210019 13310 registrar.cpp:445] Applied 1 operations in 235087ns; attempting to update the 'registry'
I0126 19:10:52.220736 13308 slave.cpp:1075] Will retry registration in 9.28397ms if necessary
I0126 19:10:52.221309 13311 master.cpp:3263] Ignoring register slave message from slave(101)@192.168.122.135:35545 (fedora-19) as admission is already in progress
I0126 19:10:52.224818 13307 log.cpp:684] Attempting to append 302 bytes to the log
I0126 19:10:52.225554 13307 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0126 19:10:52.227422 13305 replica.cpp:511] Replica received write request for position 3
I0126 19:10:52.227969 13305 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 100350ns
I0126 19:10:52.228276 13305 replica.cpp:679] Persisted action at 3
I0126 19:10:52.232475 13312 replica.cpp:658] Replica received learned notice for position 3
I0126 19:10:52.233280 13312 leveldb.cpp:343] Persisting action (323 bytes) to leveldb took 546567ns
I0126 19:10:52.233726 13312 replica.cpp:679] Persisted action at 3
I0126 19:10:52.234035 13312 replica.cpp:664] Replica learned APPEND action at position 3
I0126 19:10:52.236556 13310 registrar.cpp:490] Successfully updated the 'registry' in 26.040064ms
I0126 19:10:52.237330 13305 log.cpp:703] Attempting to truncate the log to 3
I0126 19:10:52.238056 13311 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0126 19:10:52.239594 13311 replica.cpp:511] Replica received write request for position 4
I0126 19:10:52.240129 13311 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 92868ns
I0126 19:10:52.240458 13311 replica.cpp:679] Persisted action at 4
I0126 19:10:52.241976 13308 replica.cpp:658] Replica received learned notice for position 4
I0126 19:10:52.242645 13308 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 95635ns
I0126 19:10:52.242990 13308 leveldb.cpp:401] Deleting ~2 keys from leveldb took 58066ns
I0126 19:10:52.243337 13308 replica.cpp:679] Persisted action at 4
I0126 19:10:52.243695 13308 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0126 19:10:52.245657 13291 sched.cpp:151] Version: 0.22.0
I0126 19:10:52.247625 13305 master.cpp:3329] Registered slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0126 19:10:52.248942 13307 slave.cpp:781] Registered with master master@192.168.122.135:35545; given slave ID 20150126-191052-2272962752-35545-13291-S0
I0126 19:10:52.250396 13307 slave.cpp:797] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/slave.info'
I0126 19:10:52.250731 13309 status_update_manager.cpp:178] Resuming sending status updates
I0126 19:10:52.251765 13307 slave.cpp:2588] Received ping from slave-observer(99)@192.168.122.135:35545
I0126 19:10:52.247951 13310 hierarchical_allocator_process.hpp:453] Added slave 20150126-191052-2272962752-35545-13291-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0126 19:10:52.252810 13310 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:10:52.254365 13310 hierarchical_allocator_process.hpp:756] Performed allocation for slave 20150126-191052-2272962752-35545-13291-S0 in 1.732701ms
I0126 19:10:52.254137 13307 sched.cpp:248] New master detected at master@192.168.122.135:35545
I0126 19:10:52.257863 13307 sched.cpp:304] Authenticating with master master@192.168.122.135:35545
I0126 19:10:52.258249 13307 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:10:52.258908 13306 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:10:52.261397 13309 master.cpp:4129] Authenticating scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.261776 13309 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:10:52.264528 13309 authenticator.hpp:170] Creating new server SASL connection
I0126 19:10:52.266248 13312 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:10:52.266749 13312 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:10:52.267143 13312 authenticator.hpp:276] Received SASL authentication start
I0126 19:10:52.267525 13312 authenticator.hpp:398] Authentication requires more steps
I0126 19:10:52.267917 13312 authenticatee.hpp:275] Received SASL authentication step
I0126 19:10:52.268404 13312 authenticator.hpp:304] Received SASL authentication step
I0126 19:10:52.268725 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:10:52.269078 13312 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:10:52.269498 13312 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:10:52.269881 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:10:52.270385 13312 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.271015 13312 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.271599 13312 authenticator.hpp:390] Authentication success
I0126 19:10:52.272126 13312 authenticatee.hpp:315] Authentication success
I0126 19:10:52.272415 13305 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.273998 13307 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:35545
I0126 19:10:52.274415 13307 sched.cpp:515] Sending registration request to master@192.168.122.135:35545
I0126 19:10:52.274842 13307 sched.cpp:548] Will retry registration in 674.656506ms if necessary
I0126 19:10:52.275235 13305 master.cpp:1420] Received registration request for framework 'default' at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.276017 13305 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:10:52.277027 13305 master.cpp:1484] Registering framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.278285 13308 hierarchical_allocator_process.hpp:319] Added framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.279575 13308 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 697902ns
I0126 19:10:52.287966 13305 master.cpp:4071] Sending 1 offers to framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.288776 13307 sched.cpp:442] Framework registered with 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.289373 13307 sched.cpp:456] Scheduler::registered took 21674ns
I0126 19:10:52.289932 13307 sched.cpp:605] Scheduler::resourceOffers took 76147ns
I0126 19:10:52.293220 13311 master.cpp:2677] Processing ACCEPT call for offers: [ 20150126-191052-2272962752-35545-13291-O0 ] on slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) for framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.293586 13311 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e as user 'jenkins'
I0126 19:10:52.295825 13311 master.hpp:782] Adding task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150126-191052-2272962752-35545-13291-S0 (fedora-19)
I0126 19:10:52.296272 13311 master.cpp:2885] Launching task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.296886 13309 slave.cpp:1130] Got assigned task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.297324 13309 slave.cpp:3846] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-227296",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2302,1.0,"FaultToleranceTest.SchedulerFailoverFrameworkMessage is flaky. Bad Run:
{noformat}
[ RUN      ] FaultToleranceTest.SchedulerFailoverFrameworkMessage
Using temporary directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_f3jYkr'
I0123 18:50:11.669674 15688 leveldb.cpp:176] Opened db in 31.920683ms
I0123 18:50:11.678328 15688 leveldb.cpp:183] Compacted db in 8.580569ms
I0123 18:50:11.678455 15688 leveldb.cpp:198] Created db iterator in 38478ns
I0123 18:50:11.678478 15688 leveldb.cpp:204] Seeked to beginning of db in 3057ns
I0123 18:50:11.678489 15688 leveldb.cpp:273] Iterated through 0 keys in the db in 427ns
I0123 18:50:11.678539 15688 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0123 18:50:11.682271 15705 recover.cpp:449] Starting replica recovery
I0123 18:50:11.682634 15705 recover.cpp:475] Replica is in EMPTY status
I0123 18:50:11.684389 15708 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0123 18:50:11.685132 15708 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0123 18:50:11.689842 15708 recover.cpp:566] Updating replica status to STARTING
I0123 18:50:11.702548 15708 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 12.484558ms
I0123 18:50:11.702615 15708 replica.cpp:323] Persisted replica status to STARTING
I0123 18:50:11.703531 15708 recover.cpp:475] Replica is in STARTING status
I0123 18:50:11.705080 15704 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0123 18:50:11.712587 15708 recover.cpp:195] Received a recover response from a replica in STARTING status
I0123 18:50:11.722898 15708 recover.cpp:566] Updating replica status to VOTING
I0123 18:50:11.725427 15703 master.cpp:262] Master 20150123-185011-16777343-37526-15688 (localhost.localdomain) started on 127.0.0.1:37526
W0123 18:50:11.725464 15703 master.cpp:266] 
**************************************************
Master bound to loopback interface! Cannot communicate with remote schedulers or slaves. You might want to set '--ip' flag to a routable IP address.
**************************************************
I0123 18:50:11.725502 15703 master.cpp:308] Master only allowing authenticated frameworks to register
I0123 18:50:11.725513 15703 master.cpp:313] Master only allowing authenticated slaves to register
I0123 18:50:11.725543 15703 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_f3jYkr/credentials'
I0123 18:50:11.725774 15703 master.cpp:357] Authorization enabled
I0123 18:50:11.728428 15707 whitelist_watcher.cpp:65] No whitelist given
I0123 18:50:11.729169 15707 master.cpp:1219] The newly elected leader is master@127.0.0.1:37526 with id 20150123-185011-16777343-37526-15688
I0123 18:50:11.729200 15707 master.cpp:1232] Elected as the leading master!
I0123 18:50:11.729223 15707 master.cpp:1050] Recovering from registrar
I0123 18:50:11.729595 15706 registrar.cpp:313] Recovering registrar
I0123 18:50:11.730715 15703 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0123 18:50:11.737431 15708 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.259597ms
I0123 18:50:11.737511 15708 replica.cpp:323] Persisted replica status to VOTING
I0123 18:50:11.737768 15708 recover.cpp:580] Successfully joined the Paxos group
I0123 18:50:11.737977 15708 recover.cpp:464] Recover process terminated
I0123 18:50:11.739083 15706 log.cpp:660] Attempting to start the writer
I0123 18:50:11.741236 15706 replica.cpp:477] Replica received implicit promise request with proposal 1
I0123 18:50:11.750435 15706 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 8.813783ms
I0123 18:50:11.750514 15706 replica.cpp:345] Persisted promised to 1
I0123 18:50:11.752239 15708 coordinator.cpp:230] Coordinator attemping to fill missing position
I0123 18:50:11.754176 15706 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0123 18:50:11.763464 15706 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 8.799822ms
I0123 18:50:11.763535 15706 replica.cpp:679] Persisted action at 0
I0123 18:50:11.765697 15709 replica.cpp:511] Replica received write request for position 0
I0123 18:50:11.766293 15709 leveldb.cpp:438] Reading position from leveldb took 54028ns
I0123 18:50:11.776468 15709 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 9.789169ms
I0123 18:50:11.776561 15709 replica.cpp:679] Persisted action at 0
I0123 18:50:11.777515 15709 replica.cpp:658] Replica received learned notice for position 0
I0123 18:50:11.785459 15709 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.897242ms
I0123 18:50:11.785531 15709 replica.cpp:679] Persisted action at 0
I0123 18:50:11.785565 15709 replica.cpp:664] Replica learned NOP action at position 0
I0123 18:50:11.786633 15709 log.cpp:676] Writer started with ending position 0
I0123 18:50:11.788460 15709 leveldb.cpp:438] Reading position from leveldb took 266087ns
I0123 18:50:11.801141 15709 registrar.cpp:346] Successfully fetched the registry (0B) in 71.491072ms
I0123 18:50:11.801300 15709 registrar.cpp:445] Applied 1 operations in 41795ns; attempting to update the 'registry'
I0123 18:50:11.805186 15707 log.cpp:684] Attempting to append 136 bytes to the log
I0123 18:50:11.805454 15707 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0123 18:50:11.806677 15703 replica.cpp:511] Replica received write request for position 1
I0123 18:50:11.815621 15703 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 8.89177ms
I0123 18:50:11.815692 15703 replica.cpp:679] Persisted action at 1
I0123 18:50:11.817358 15704 replica.cpp:658] Replica received learned notice for position 1
I0123 18:50:11.825014 15704 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 7.578558ms
I0123 18:50:11.825088 15704 replica.cpp:679] Persisted action at 1
I0123 18:50:11.825124 15704 replica.cpp:664] Replica learned APPEND action at position 1
I0123 18:50:11.827008 15705 registrar.cpp:490] Successfully updated the 'registry' in 25.629952ms
I0123 18:50:11.827143 15705 registrar.cpp:376] Successfully recovered registrar
I0123 18:50:11.827517 15705 master.cpp:1077] Recovered 0 slaves from the Registry (98B) ; allowing 10mins for slaves to re-register
I0123 18:50:11.828515 15704 log.cpp:703] Attempting to truncate the log to 1
I0123 18:50:11.829074 15704 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0123 18:50:11.830546 15709 replica.cpp:511] Replica received write request for position 2
I0123 18:50:11.837752 15709 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.142431ms
I0123 18:50:11.837826 15709 replica.cpp:679] Persisted action at 2
I0123 18:50:11.839334 15709 replica.cpp:658] Replica received learned notice for position 2
I0123 18:50:11.847069 15709 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 7.116607ms
I0123 18:50:11.847214 15709 leveldb.cpp:401] Deleting ~1 keys from leveldb took 74008ns
I0123 18:50:11.847241 15709 replica.cpp:679] Persisted action at 2
I0123 18:50:11.847295 15709 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0123 18:50:11.870337 15710 slave.cpp:173] Slave started on 94)@127.0.0.1:37526
W0123 18:50:11.870980 15710 slave.cpp:176] 
**************************************************
Slave bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0123 18:50:11.871412 15710 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/credential'
I0123 18:50:11.871819 15710 slave.cpp:282] Slave using credential for: test-principal
I0123 18:50:11.873178 15710 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0123 18:50:11.873620 15710 slave.cpp:329] Slave hostname: localhost.localdomain
I0123 18:50:11.873837 15710 slave.cpp:330] Slave checkpoint: false
W0123 18:50:11.874068 15710 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0123 18:50:11.879103 15705 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/meta'
W0123 18:50:11.882972 15688 sched.cpp:1246] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0123 18:50:11.884106 15709 status_update_manager.cpp:197] Recovering status update manager
I0123 18:50:11.884703 15710 slave.cpp:3519] Finished recovery
I0123 18:50:11.892076 15704 status_update_manager.cpp:171] Pausing sending status updates
I0123 18:50:11.892590 15710 slave.cpp:613] New master detected at master@127.0.0.1:37526
I0123 18:50:11.892937 15710 slave.cpp:676] Authenticating with master master@127.0.0.1:37526
I0123 18:50:11.893165 15710 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0123 18:50:11.893754 15708 authenticatee.hpp:138] Creating new client SASL connection
I0123 18:50:11.894120 15708 master.cpp:4129] Authenticating slave(94)@127.0.0.1:37526
I0123 18:50:11.894153 15708 master.cpp:4140] Using default CRAM-MD5 authenticator
I0123 18:50:11.894628 15708 authenticator.hpp:170] Creating new server SASL connection
I0123 18:50:11.894913 15708 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0123 18:50:11.894942 15708 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0123 18:50:11.895043 15708 authenticator.hpp:276] Received SASL authentication start
I0123 18:50:11.895095 15708 authenticator.hpp:398] Authentication requires more steps
I0123 18:50:11.895165 15708 authenticatee.hpp:275] Received SASL authentication step
I0123 18:50:11.895261 15708 authenticator.hpp:304] Received SASL authentication step
I0123 18:50:11.895292 15708 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0123 18:50:11.895305 15708 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0123 18:50:11.895354 15708 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0123 18:50:11.895881 15710 slave.cpp:649] Detecting new master
I0123 18:50:11.898449 15708 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0123 18:50:11.899024 15708 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.899106 15708 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.899190 15708 authenticator.hpp:390] Authentication success
I0123 18:50:11.899569 15706 authenticatee.hpp:315] Authentication success
I0123 18:50:11.902299 15706 slave.cpp:747] Successfully authenticated with master master@127.0.0.1:37526
I0123 18:50:11.902847 15706 slave.cpp:1075] Will retry registration in 19.809649ms if necessary
I0123 18:50:11.903264 15705 master.cpp:3214] Queuing up registration request from slave(94)@127.0.0.1:37526 because authentication is still in progress
I0123 18:50:11.903497 15705 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(94)@127.0.0.1:37526
I0123 18:50:11.903940 15705 master.cpp:3275] Registering slave at slave(94)@127.0.0.1:37526 (localhost.localdomain) with id 20150123-185011-16777343-37526-15688-S0
I0123 18:50:11.904398 15705 registrar.cpp:445] Applied 1 operations in 63679ns; attempting to update the 'registry'
I0123 18:50:11.917883 15688 sched.cpp:151] Version: 0.22.0
I0123 18:50:11.919347 15703 log.cpp:684] Attempting to append 315 bytes to the log
I0123 18:50:11.921039 15703 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0123 18:50:11.919992 15706 sched.cpp:248] New master detected at master@127.0.0.1:37526
I0123 18:50:11.921352 15706 sched.cpp:304] Authenticating with master master@127.0.0.1:37526
I0123 18:50:11.921408 15706 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0123 18:50:11.921773 15706 authenticatee.hpp:138] Creating new client SASL connection
I0123 18:50:11.922266 15706 master.cpp:4129] Authenticating scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.922301 15706 master.cpp:4140] Using default CRAM-MD5 authenticator
I0123 18:50:11.923928 15703 replica.cpp:511] Replica received write request for position 3
I0123 18:50:11.924285 15707 authenticator.hpp:170] Creating new server SASL connection
I0123 18:50:11.925091 15707 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0123 18:50:11.925122 15707 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0123 18:50:11.925194 15707 authenticator.hpp:276] Received SASL authentication start
I0123 18:50:11.925257 15707 authenticator.hpp:398] Authentication requires more steps
I0123 18:50:11.925325 15707 authenticatee.hpp:275] Received SASL authentication step
I0123 18:50:11.925442 15707 authenticator.hpp:304] Received SASL authentication step
I0123 18:50:11.925473 15707 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0123 18:50:11.925487 15707 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0123 18:50:11.925532 15707 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0123 18:50:11.925559 15707 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0123 18:50:11.925571 15707 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.925580 15707 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.925595 15707 authenticator.hpp:390] Authentication success
I0123 18:50:11.925695 15707 authenticatee.hpp:315] Authentication success
I0123 18:50:11.925792 15707 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.926127 15707 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:37526
I0123 18:50:11.926154 15707 sched.cpp:515] Sending registration request to master@127.0.0.1:37526
I0123 18:50:11.926215 15707 sched.cpp:548] Will retry registration in 866.81063ms if necessary
I0123 18:50:11.926640 15707 master.cpp:1420] Received registration request for framework 'default' at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.926960 15707 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0123 18:50:11.927691 15707 master.cpp:1484] Registering framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.928292 15708 hierarchical_allocator_process.hpp:319] Added framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.928326 15708 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0123 18:50:11.928340 15708 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 21080ns
I0123 18:50:11.934458 15707 sched.cpp:442] Framework registered with 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.934927 15707 sched.cpp:456] Scheduler::registered took 112885ns
I0123 18:50:11.935747 15709 slave.cpp:1075] Will retry registration in 19.609252ms if necessary
I0123 18:50:11.935981 15709 master.cpp:3263] Ignoring register slave message from slave(94)@127.0.0.1:37526 (localhost.localdomain) as admission is already in progress
I0123 18:50:11.938997 15703 leveldb.cpp:343] Persisting action (334 bytes) to leveldb took 10.171709ms
I0123 18:50:11.939049 15703 replica.cpp:679] Persisted action at 3
I0123 18:50:11.940630 15709 replica.cpp:658] Replica received learned notice for position 3
I0123 18:50:11.945473 15709 leveldb.cpp:343] Persisting action (336 bytes) to leveldb took 4.804742ms
I0123 18:50:11.945521 15709 replica.cpp:679] Persisted action at 3
I0123 18:50:11.945550 15709 replica.cpp:664] Replica learned APPEND action at position 3
I0123 18:50:11.947105 15709 registrar.cpp:490] Successfully updated the 'registry' in 42.637056ms
I0123 18:50:11.948020 15703 master.cpp:3329] Registered slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0123 18:50:11.948318 15703 hierarchical_allocator_process.hpp:453] Added slave 20150123-185011-16777343-37526-15688-S0 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0123 18:50:11.948719 15703 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150123-185011-16777343-37526-15688-S0 in 355831ns
I0123 18:50:11.948813 15703 slave.cpp:781] Registered with master master@127.0.0.1:37526; given slave ID 20150123-185011-16777343-37526-15688-S0
I0123 18:50:11.948969 15703 slave.cpp:2588] Received ping from slave-observer(92)@127.0.0.1:37526
I0123 18:50:11.949324 15703 master.cpp:4071] Sending 1 offers to framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.949571 15706 status_update_manager.cpp:178] Resuming sending status updates
I0123 18:50:11.950023 15709 log.cpp:703] Attempting to truncate the log to 3
I0123 18:50:11.950810 15705 sched.cpp:605] Scheduler::resourceOffers took 135580ns
I0123 18:50:11.952793 15708 master.cpp:2677] Processing ACCEPT call for offers: [ 20150123-185011-16777343-37526-15688-O0 ] on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) for framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.952852 15708 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0123 18:50:11.954649 15708 master.cpp:2130] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0123 18:50:11.954988 15708 master.cpp:2142] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0123 18:50:11.955579 15708 master.hpp:782] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150123-185011-16777343-37526-15688-S0 (localhost.localdomain)
I0123 18:50:11.956035 15703 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0123 18:50:11.957592 15704 replica.cpp:511] Replica received write request for position 4
I0123 18:50:11.958485 15708 master.cpp:2885] Launching task 1 of framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb10",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2306,1.0,"MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky. Good run:

{noformat}
[ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD'
I0122 19:23:06.481690 17483 leveldb.cpp:176] Opened db in 21.058723ms
I0122 19:23:06.488590 17483 leveldb.cpp:183] Compacted db in 6.6715ms
I0122 19:23:06.488816 17483 leveldb.cpp:198] Created db iterator in 30034ns
I0122 19:23:06.489053 17483 leveldb.cpp:204] Seeked to beginning of db in 2908ns
I0122 19:23:06.489073 17483 leveldb.cpp:273] Iterated through 0 keys in the db in 492ns
I0122 19:23:06.489148 17483 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0122 19:23:06.490272 17504 recover.cpp:449] Starting replica recovery
I0122 19:23:06.490900 17504 recover.cpp:475] Replica is in EMPTY status
I0122 19:23:06.492422 17504 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0122 19:23:06.492694 17504 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0122 19:23:06.493185 17504 recover.cpp:566] Updating replica status to STARTING
I0122 19:23:06.514881 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 21.459963ms
I0122 19:23:06.514920 17504 replica.cpp:323] Persisted replica status to STARTING
I0122 19:23:06.515861 17501 master.cpp:262] Master 20150122-192306-16842879-46283-17483 (lucid) started on 127.0.1.1:46283
I0122 19:23:06.515910 17501 master.cpp:308] Master only allowing authenticated frameworks to register
I0122 19:23:06.515923 17501 master.cpp:313] Master only allowing authenticated slaves to register
I0122 19:23:06.515946 17501 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD/credentials'
I0122 19:23:06.516150 17501 master.cpp:357] Authorization enabled
I0122 19:23:06.517511 17501 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0122 19:23:06.517607 17501 whitelist_watcher.cpp:65] No whitelist given
I0122 19:23:06.518066 17498 master.cpp:1219] The newly elected leader is master@127.0.1.1:46283 with id 20150122-192306-16842879-46283-17483
I0122 19:23:06.518095 17498 master.cpp:1232] Elected as the leading master!
I0122 19:23:06.518121 17498 master.cpp:1050] Recovering from registrar
I0122 19:23:06.518333 17498 registrar.cpp:313] Recovering registrar
I0122 19:23:06.523987 17504 recover.cpp:475] Replica is in STARTING status
I0122 19:23:06.525090 17504 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0122 19:23:06.525337 17504 recover.cpp:195] Received a recover response from a replica in STARTING status
I0122 19:23:06.525693 17504 recover.cpp:566] Updating replica status to VOTING
I0122 19:23:06.532680 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 6.810884ms
I0122 19:23:06.532714 17504 replica.cpp:323] Persisted replica status to VOTING
I0122 19:23:06.532835 17504 recover.cpp:580] Successfully joined the Paxos group
I0122 19:23:06.533004 17504 recover.cpp:464] Recover process terminated
I0122 19:23:06.533833 17500 log.cpp:660] Attempting to start the writer
I0122 19:23:06.535225 17500 replica.cpp:477] Replica received implicit promise request with proposal 1
I0122 19:23:06.540340 17500 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.086139ms
I0122 19:23:06.540371 17500 replica.cpp:345] Persisted promised to 1
I0122 19:23:06.541502 17504 coordinator.cpp:230] Coordinator attemping to fill missing position
I0122 19:23:06.543021 17504 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0122 19:23:06.548140 17504 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.083443ms
I0122 19:23:06.548171 17504 replica.cpp:679] Persisted action at 0
I0122 19:23:06.549746 17500 replica.cpp:511] Replica received write request for position 0
I0122 19:23:06.549926 17500 leveldb.cpp:438] Reading position from leveldb took 31962ns
I0122 19:23:06.555033 17500 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.065823ms
I0122 19:23:06.555064 17500 replica.cpp:679] Persisted action at 0
I0122 19:23:06.556094 17504 replica.cpp:658] Replica received learned notice for position 0
I0122 19:23:06.558815 17504 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.688382ms
I0122 19:23:06.558847 17504 replica.cpp:679] Persisted action at 0
I0122 19:23:06.558868 17504 replica.cpp:664] Replica learned NOP action at position 0
I0122 19:23:06.559917 17500 log.cpp:676] Writer started with ending position 0
I0122 19:23:06.560995 17500 leveldb.cpp:438] Reading position from leveldb took 27742ns
I0122 19:23:06.563467 17500 registrar.cpp:346] Successfully fetched the registry (0B) in 45.095936ms
I0122 19:23:06.563551 17500 registrar.cpp:445] Applied 1 operations in 19686ns; attempting to update the 'registry'
I0122 19:23:06.566107 17500 log.cpp:684] Attempting to append 118 bytes to the log
I0122 19:23:06.566267 17500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0122 19:23:06.567126 17500 replica.cpp:511] Replica received write request for position 1
I0122 19:23:06.582588 17500 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 15.425511ms
I0122 19:23:06.582631 17500 replica.cpp:679] Persisted action at 1
I0122 19:23:06.583425 17500 replica.cpp:658] Replica received learned notice for position 1
I0122 19:23:06.589001 17500 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.549486ms
I0122 19:23:06.589200 17500 replica.cpp:679] Persisted action at 1
I0122 19:23:06.589416 17500 replica.cpp:664] Replica learned APPEND action at position 1
I0122 19:23:06.596420 17500 registrar.cpp:490] Successfully updated the 'registry' in 32.815104ms
I0122 19:23:06.596551 17500 registrar.cpp:376] Successfully recovered registrar
I0122 19:23:06.596923 17500 master.cpp:1077] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0122 19:23:06.597007 17500 log.cpp:703] Attempting to truncate the log to 1
I0122 19:23:06.597239 17500 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0122 19:23:06.598464 17501 replica.cpp:511] Replica received write request for position 2
I0122 19:23:06.604038 17501 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.536264ms
I0122 19:23:06.604084 17501 replica.cpp:679] Persisted action at 2
I0122 19:23:06.608747 17503 replica.cpp:658] Replica received learned notice for position 2
I0122 19:23:06.614094 17503 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.315347ms
I0122 19:23:06.614171 17503 leveldb.cpp:401] Deleting ~1 keys from leveldb took 33021ns
I0122 19:23:06.614188 17503 replica.cpp:679] Persisted action at 2
I0122 19:23:06.614208 17503 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0122 19:23:06.628820 17483 sched.cpp:151] Version: 0.22.0
I0122 19:23:06.629879 17505 sched.cpp:248] New master detected at master@127.0.1.1:46283
I0122 19:23:06.629973 17505 sched.cpp:304] Authenticating with master master@127.0.1.1:46283
I0122 19:23:06.629995 17505 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:23:06.630314 17505 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:23:06.630722 17505 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.630750 17505 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:23:06.631115 17505 authenticator.hpp:170] Creating new server SASL connection
I0122 19:23:06.631423 17505 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:23:06.631459 17505 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:23:06.631563 17505 authenticator.hpp:276] Received SASL authentication start
I0122 19:23:06.631605 17505 authenticator.hpp:398] Authentication requires more steps
I0122 19:23:06.631671 17505 authenticatee.hpp:275] Received SASL authentication step
I0122 19:23:06.631748 17505 authenticator.hpp:304] Received SASL authentication step
I0122 19:23:06.631774 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:23:06.631784 17505 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:23:06.631822 17505 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:23:06.631856 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:23:06.631870 17505 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.631877 17505 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.631892 17505 authenticator.hpp:390] Authentication success
I0122 19:23:06.631988 17505 authenticatee.hpp:315] Authentication success
I0122 19:23:06.632066 17505 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.632359 17505 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283
I0122 19:23:06.632382 17505 sched.cpp:515] Sending registration request to master@127.0.1.1:46283
I0122 19:23:06.632432 17505 sched.cpp:548] Will retry registration in 598.155756ms if necessary
I0122 19:23:06.632575 17505 master.cpp:1420] Received registration request for framework 'default' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.632639 17505 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:23:06.632912 17505 master.cpp:1484] Registering framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.633421 17505 hierarchical_allocator_process.hpp:319] Added framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.633448 17505 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0122 19:23:06.633458 17505 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 17704ns
I0122 19:23:06.633919 17505 sched.cpp:442] Framework registered with 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.633980 17505 sched.cpp:456] Scheduler::registered took 37063ns
I0122 19:23:06.636554 17500 sched.cpp:242] Scheduler::disconnected took 14843ns
I0122 19:23:06.636579 17500 sched.cpp:248] New master detected at master@127.0.1.1:46283
I0122 19:23:06.636625 17500 sched.cpp:304] Authenticating with master master@127.0.1.1:46283
I0122 19:23:06.636641 17500 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:23:06.636914 17500 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:23:06.637313 17500 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.637341 17500 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:23:06.637675 17500 authenticator.hpp:170] Creating new server SASL connection
I0122 19:23:06.638056 17501 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:23:06.638083 17501 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:23:06.638182 17501 authenticator.hpp:276] Received SASL authentication start
I0122 19:23:06.638221 17501 authenticator.hpp:398] Authentication requires more steps
I0122 19:23:06.638286 17501 authenticatee.hpp:275] Received SASL authentication step
I0122 19:23:06.638360 17501 authenticator.hpp:304] Received SASL authentication step
I0122 19:23:06.638383 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:23:06.638393 17501 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:23:06.638422 17501 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:23:06.638447 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:23:06.638458 17501 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.638464 17501 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.638478 17501 authenticator.hpp:390] Authentication success
I0122 19:23:06.638566 17501 authenticatee.hpp:315] Authentication success
I0122 19:23:06.638643 17501 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.638919 17501 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283
I0122 19:23:06.638942 17501 sched.cpp:515] Sending registration request to master@127.0.1.1:46283
I0122 19:23:06.638994 17501 sched.cpp:548] Will retry registration in 489.304713ms if necessary
I0122 19:23:06.639169 17501 master.cpp:1557] Received re-registration request from framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.639242 17501 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:23:06.639839 17483 sched.cpp:1471] Asked to stop the driver
I0122 19:23:06.640379 17499 sched.cpp:808] Stopping framework '20150122-192306-16842879-46283-17483-0000'
I0122 19:23:06.640697 17499 master.cpp:745] Framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 disconnected
I0122 19:23:06.640723 17499 master.cpp:1789] Disconnecting framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.640744 17499 master.cpp:1805] Deactivating framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.640806 17499 master.cpp:767] Giving framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 0ns to failover
I0122 19:23:06.640951 17499 hierarchical_allocator_process.hpp:398] Deactivated framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.646342 17498 master.cpp:1604] Dropping re-registration request of framework 20150122-192306-16842879-46283-17483-0000 (default)  at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 because it is not authenticated
I0122 19:23:06.648844 17498 master.cpp:3941] Framework failover timeout, removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.648871 17498 master.cpp:4499] Removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.649624 17498 hierarchical_allocator_process.hpp:352] Removed framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.656532 17483 master.cpp:654] Master terminating
[       OK ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration (216 ms)
{noformat}

Bad run:

{noformat}
[ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm'
I0126 19:19:55.517570  2381 leveldb.cpp:176] Opened db in 34.341401ms
I0126 19:19:55.529630  2381 leveldb.cpp:183] Compacted db in 11.824435ms
I0126 19:19:55.529878  2381 leveldb.cpp:198] Created db iterator in 26176ns
I0126 19:19:55.530200  2381 leveldb.cpp:204] Seeked to beginning of db in 3457ns
I0126 19:19:55.530455  2381 leveldb.cpp:273] Iterated through 0 keys in the db in 902ns
I0126 19:19:55.530658  2381 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0126 19:19:55.531492  2397 recover.cpp:449] Starting replica recovery
I0126 19:19:55.531793  2397 recover.cpp:475] Replica is in EMPTY status
I0126 19:19:55.533327  2397 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0126 19:19:55.533608  2397 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0126 19:19:55.534101  2397 recover.cpp:566] Updating replica status to STARTING
I0126 19:19:55.550417  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.106821ms
I0126 19:19:55.550472  2397 replica.cpp:323] Persisted replica status to STARTING
I0126 19:19:55.551434  2397 recover.cpp:475] Replica is in STARTING status
I0126 19:19:55.552846  2397 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0126 19:19:55.553099  2397 recover.cpp:195] Received a recover response from a replica in STARTING status
I0126 19:19:55.553565  2397 recover.cpp:566] Updating replica status to VOTING
I0126 19:19:55.564590  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 10.719218ms
I0126 19:19:55.564919  2397 replica.cpp:323] Persisted replica status to VOTING
I0126 19:19:55.565982  2397 recover.cpp:580] Successfully joined the Paxos group
I0126 19:19:55.566231  2397 recover.cpp:464] Recover process terminated
I0126 19:19:55.567878  2401 master.cpp:262] Master 20150126-191955-16842879-51862-2381 (lucid) started on 127.0.1.1:51862
I0126 19:19:55.567927  2401 master.cpp:308] Master only allowing authenticated frameworks to register
I0126 19:19:55.567950  2401 master.cpp:313] Master only allowing authenticated slaves to register
I0126 19:19:55.567978  2401 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm/credentials'
I0126 19:19:55.568220  2401 master.cpp:357] Authorization enabled
I0126 19:19:55.569890  2401 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0126 19:19:55.569999  2401 whitelist_watcher.cpp:65] No whitelist given
I0126 19:19:55.570694  2401 master.cpp:1219] The newly elected leader is master@127.0.1.1:51862 with id 20150126-191955-16842879-51862-2381
I0126 19:19:55.570721  2401 master.cpp:1232] Elected as the leading master!
I0126 19:19:55.570742  2401 master.cpp:1050] Recovering from registrar
I0126 19:19:55.570977  2401 registrar.cpp:313] Recovering registrar
I0126 19:19:55.571959  2401 log.cpp:660] Attempting to start the writer
I0126 19:19:55.573441  2401 replica.cpp:477] Replica received implicit promise request with proposal 1
I0126 19:19:55.590724  2401 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.243964ms
I0126 19:19:55.590785  2401 replica.cpp:345] Persisted promised to 1
I0126 19:19:55.592140  2396 coordinator.cpp:230] Coordinator attemping to fill missing position
I0126 19:19:55.593834  2396 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0126 19:19:55.603837  2396 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 9.955824ms
I0126 19:19:55.603902  2396 replica.cpp:679] Persisted action at 0
I0126 19:19:55.606082  2401 replica.cpp:511] Replica received write request for position 0
I0126 19:19:55.606331  2401 leveldb.cpp:438] Reading position from leveldb took 44524ns
I0126",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2314,2.0,"remove unnecessary constants In {{src/slave/paths.cpp}} a number of string constants are defined to describe the formats of various paths. However, given there is a 1:1 mapping between the string constant and the functions that build the paths, the code would be more readable if the format strings were inline in the functions.

In the cases where one constant depends on another (see the {{EXECUTOR_INFO_PATH, EXECUTOR_PATH, FRAMEWORK_PATH, SLAVE_PATH, ROOT_PATH}} chain, for example) the function calls can just be chained together.

This will have the added benefit of removing some statically constructed string constants, which are dangerous.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0
MESOS-2319,2.0,"Unable to set --work_dir to a non /tmp device When starting mesos-slave with --work_dir set to a directory which is not the same device as /tmp results in mesos-slave throwing a core dump:
","<code>
mesos # GLOG_v=1 sbin/mesos-slave --master=zk://10.171.59.83:2181/mesos --work_dir=/var/lib/mesos/
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0204 18:24:49.274619 22922 process.cpp:958] libprocess is initialized on 10.169.146.67:5051 for 8 cpus
I0204 18:24:49.274978 22922 logging.cpp:177] Logging to STDERR
I0204 18:24:49.275111 22922 main.cpp:152] Build: 2015-02-03 22:59:30 by 
I0204 18:24:49.275233 22922 main.cpp:154] Version: 0.22.0
I0204 18:24:49.275485 22922 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
2015-02-04 18:24:49,275:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2015-02-04 18:24:49,275:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@716: Client environment:host.name=ip-10-169-146-67.ec2.internal
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@724: Client environment:os.arch=3.18.2
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@725: Client environment:os.version=#2 SMP Tue Jan 27 23:34:36 UTC 2015
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@733: Client environment:user.name=core
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@741: Client environment:user.home=/root
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@753: Client environment:user.dir=/opt/mesosphere/dcos/0.0.1-0.1.20150203225612/mesos
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=10.171.59.83:2181 sessionTimeout=10000 watcher=0x7ffdd97bccf0 sessionId=0 sessionPasswd=<null> context=0x7ffdc8000ba0 flags=0
I0204 18:24:49.276793 22922 main.cpp:180] Starting Mesos slave
2015-02-04 18:24:49,307:22922(0x7ffdd151f700):ZOO_INFO@check_events@1703: initiated connection to server [10.171.59.83:2181]
I0204 18:24:49.307548 22922 slave.cpp:173] Slave started on 1)@10.169.146.67:5051
I0204 18:24:49.307955 22922 slave.cpp:300] Slave resources: cpus(*):1; mem(*):2728; disk(*):24736; ports(*):[31000-32000]
I0204 18:24:49.308404 22922 slave.cpp:329] Slave hostname: ip-10-169-146-67.ec2.internal
I0204 18:24:49.308459 22922 slave.cpp:330] Slave checkpoint: true
I0204 18:24:49.310431 22924 state.cpp:33] Recovering state from '/var/lib/mesos/meta'
I0204 18:24:49.310583 22924 state.cpp:668] Failed to find resources file '/var/lib/mesos/meta/resources/resources.info'
I0204 18:24:49.310670 22924 state.cpp:74] Failed to find the latest slave from '/var/lib/mesos/meta'
I0204 18:24:49.310803 22924 status_update_manager.cpp:197] Recovering status update manager
I0204 18:24:49.310916 22924 containerizer.cpp:300] Recovering containerizer
I0204 18:24:49.311110 22924 slave.cpp:3527] Finished recovery
F0204 18:24:49.311312 22924 slave.cpp:3537] CHECK_SOME(state::checkpoint(path, bootId.get())): Failed to rename '/tmp/PSHLqV' to '/var/lib/mesos/meta/boot_id': Invalid cross-device link 
2015-02-04 18:24:49,310:22922(0x7ffdd151f700):ZOO_INFO@check_events@1750: session establishment complete on server [10.171.59.83:2181], sessionId=0x14b51bc8506039a, negotiated timeout=10000
*** Check failure stack trace: ***
    @     0x7ffdd9a6596d  google::LogMessage::Fail()
I0204 18:24:49.313356 22930 group.cpp:313] Group process (group(1)@10.169.146.67:5051) connected to ZooKeeper
    @     0x7ffdd9a677ad  google::LogMessage::SendToLog()
I0204 18:24:49.313786 22930 group.cpp:790] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0204 18:24:49.314487 22930 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
I0204 18:24:49.323668 22930 group.cpp:717] Found non-sequence node 'log_replicas' at '/mesos' in ZooKeeper
I0204 18:24:49.323806 22930 detector.cpp:138] Detected a new leader: (id='1')
I0204 18:24:49.323958 22930 group.cpp:659] Trying to get '/mesos/info_0000000001' in ZooKeeper
I0204 18:24:49.324595 22930 detector.cpp:433] A new leading master (UPID=master@10.171.59.83:5050) is detected
    @     0x7ffdd9a6555c  google::LogMessage::Flush()
    @     0x7ffdd9a680a9  google::LogMessageFatal::~LogMessageFatal()
    @     0x7ffdd94b7179  _CheckFatal::~_CheckFatal()
    @     0x7ffdd96718e2  mesos::internal::slave::Slave::__recover()
    @     0x7ffdd9a1524a  process::ProcessManager::resume()
    @     0x7ffdd9a1550c  process::schedule()
    @     0x7ffdd83832ad  (unknown)
    @     0x7ffdd80b834d  (unknown)
Aborted (core dumped)
<code>

Removing the --work_dir option results in the slave starting successfully.</code></null></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2332,5.0,"Report per-container metrics for network bandwidth throttling Export metrics from the network isolation to identify scope and duration of container throttling.  

Packet loss can be identified from the overlimits and requeues fields of the htb qdisc report for the virtual interface, e.g.

{noformat}
$ tc -s -d qdisc show dev mesos19223
qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
 Sent 158213287452 bytes 1030876393 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
qdisc ingress ffff: parent ffff:fff1 ----------------
 Sent 119381747824 bytes 1144549901 pkt (dropped 2044879, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
{noformat}

Note that since a packet can be examined multiple times before transmission, overlimits can exceed total packets sent.  

Add to the port_mapping isolator usage() and the container statistics protobuf. Carefully consider the naming (esp tx/rx) + commenting of the protobuf fields so it's clear what these represent and how they are different to the existing dropped packet counts from the network stack.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2347,8.0,"Add ability for schedulers to explicitly acknowledge status updates on the driver. In order for schedulers to be able to handle status updates in a scalable manner, they need the ability to send acknowledgements through the driver. This enables optimizations in schedulers (e.g. process status updates asynchronously w/o backing up the driver, processing/acking updates in batch).

Without this, an implicit reconciliation can overload a scheduler (hence the motivation for MESOS-2308).",,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
MESOS-2350,5.0,"Add support for MesosContainerizerLaunch to chroot to a specified path In preparation for the MesosContainerizer to support a filesystem isolator the MesosContainerizerLauncher must support chrooting. Optionally, it should also configure the chroot environment by (re-)mounting special filesystems such as /proc and /sys and making device nodes such as /dev/zero, etc., such that the chroot environment is functional.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2367,5.0,"Improve slave resiliency in the face of orphan containers  Right now there's a case where a misbehaving executor can cause a slave process to flap:

{panel:title=Quote From [~jieyu]}
{quote}
1) User tries to kill an instance
2) Slave sends {{KillTaskMessage}} to executor
3) Executor sends kill signals to task processes
4) Executor sends {{TASK_KILLED}} to slave
5) Slave updates container cpu limit to be 0.01 cpus
6) A user-process is still processing the kill signal
7) the task process cannot exit since it has too little cpu share and is throttled
8) Executor itself terminates
9) Slave tries to destroy the container, but cannot because the user-process is stuck in the exit path.
10) Slave restarts, and is constantly flapping because it cannot kill orphan containers
{quote}
{panel}

The slave's orphan container handling should be improved to deal with this case despite ill-behaved users (framework writers).",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2387,1.0,"SlaveTest.TaskLaunchContainerizerUpdateFails is flaky Observed on internal CI

","<code>
[ RUN      ] SlaveTest.TaskLaunchContainerizerUpdateFails
Using temporary directory '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_tUjtcI'
I0222 04:59:56.568491 21813 process.cpp:2117] Dropped / Lost event for PID: slave(52)@192.168.122.68:39461
I0222 04:59:56.595433 21791 leveldb.cpp:175] Opened db in 27.59732ms
I0222 04:59:56.603965 21791 leveldb.cpp:182] Compacted db in 8.49192ms
I0222 04:59:56.604019 21791 leveldb.cpp:197] Created db iterator in 19206ns
I0222 04:59:56.604037 21791 leveldb.cpp:203] Seeked to beginning of db in 1802ns
I0222 04:59:56.604046 21791 leveldb.cpp:272] Iterated through 0 keys in the db in 467ns
I0222 04:59:56.604081 21791 replica.cpp:743] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0222 04:59:56.607413 21809 recover.cpp:448] Starting replica recovery
I0222 04:59:56.607687 21809 recover.cpp:474] Replica is in 4 status
I0222 04:59:56.609011 21809 replica.cpp:640] Replica in 4 status received a broadcasted recover request
I0222 04:59:56.609262 21809 recover.cpp:194] Received a recover response from a replica in 4 status
I0222 04:59:56.609709 21809 recover.cpp:565] Updating replica status to 3
I0222 04:59:56.610749 21811 master.cpp:347] Master 20150222-045956-1148889280-39461-21791 (centos-7) started on 192.168.122.68:39461
I0222 04:59:56.610791 21811 master.cpp:393] Master only allowing authenticated frameworks to register
I0222 04:59:56.610802 21811 master.cpp:398] Master only allowing authenticated slaves to register
I0222 04:59:56.610821 21811 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_tUjtcI/credentials'
I0222 04:59:56.611042 21811 master.cpp:440] Authorization enabled
I0222 04:59:56.612329 21811 hierarchical.hpp:286] Initialized hierarchical allocator process
I0222 04:59:56.612416 21811 whitelist_watcher.cpp:78] No whitelist given
I0222 04:59:56.613005 21811 master.cpp:1354] The newly elected leader is master@192.168.122.68:39461 with id 20150222-045956-1148889280-39461-21791
I0222 04:59:56.613034 21811 master.cpp:1367] Elected as the leading master!
I0222 04:59:56.613050 21811 master.cpp:1185] Recovering from registrar
I0222 04:59:56.613229 21811 registrar.cpp:312] Recovering registrar
I0222 04:59:56.622866 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 12.988429ms
I0222 04:59:56.622913 21809 replica.cpp:322] Persisted replica status to 3
I0222 04:59:56.623118 21809 recover.cpp:474] Replica is in 3 status
I0222 04:59:56.624419 21809 replica.cpp:640] Replica in 3 status received a broadcasted recover request
I0222 04:59:56.624685 21809 recover.cpp:194] Received a recover response from a replica in 3 status
I0222 04:59:56.625200 21809 recover.cpp:565] Updating replica status to 1
I0222 04:59:56.635154 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 9.799671ms
I0222 04:59:56.635197 21809 replica.cpp:322] Persisted replica status to 1
I0222 04:59:56.635296 21809 recover.cpp:579] Successfully joined the Paxos group
I0222 04:59:56.635426 21809 recover.cpp:463] Recover process terminated
I0222 04:59:56.635812 21809 log.cpp:659] Attempting to start the writer
I0222 04:59:56.637075 21809 replica.cpp:476] Replica received implicit promise request with proposal 1
I0222 04:59:56.648674 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 11.566146ms
I0222 04:59:56.648717 21809 replica.cpp:344] Persisted promised to 1
I0222 04:59:56.649456 21809 coordinator.cpp:229] Coordinator attemping to fill missing position
I0222 04:59:56.650800 21809 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2
I0222 04:59:56.659916 21809 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 9.078258ms
I0222 04:59:56.659981 21809 replica.cpp:678] Persisted action at 0
I0222 04:59:56.661075 21809 replica.cpp:510] Replica received write request for position 0
I0222 04:59:56.661129 21809 leveldb.cpp:437] Reading position from leveldb took 26387ns
I0222 04:59:56.671227 21809 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 10.064302ms
I0222 04:59:56.671262 21809 replica.cpp:678] Persisted action at 0
I0222 04:59:56.671821 21809 replica.cpp:657] Replica received learned notice for position 0
I0222 04:59:56.684200 21809 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 12.346897ms
I0222 04:59:56.684242 21809 replica.cpp:678] Persisted action at 0
I0222 04:59:56.684262 21809 replica.cpp:663] Replica learned 1 action at position 0
I0222 04:59:56.684875 21809 log.cpp:675] Writer started with ending position 0
I0222 04:59:56.685932 21809 leveldb.cpp:437] Reading position from leveldb took 27308ns
I0222 04:59:56.688256 21809 registrar.cpp:345] Successfully fetched the registry (0B) in 74.992128ms
I0222 04:59:56.688344 21809 registrar.cpp:444] Applied 1 operations in 19566ns; attempting to update the 'registry'
I0222 04:59:56.690690 21809 log.cpp:683] Attempting to append 129 bytes to the log
I0222 04:59:56.690848 21809 coordinator.cpp:339] Coordinator attempting to write 2 action at position 1
I0222 04:59:56.691661 21809 replica.cpp:510] Replica received write request for position 1
I0222 04:59:56.701247 21809 leveldb.cpp:342] Persisting action (148 bytes) to leveldb took 9.550768ms
I0222 04:59:56.701292 21809 replica.cpp:678] Persisted action at 1
I0222 04:59:56.702066 21809 replica.cpp:657] Replica received learned notice for position 1
I0222 04:59:56.712136 21809 leveldb.cpp:342] Persisting action (150 bytes) to leveldb took 10.041696ms
I0222 04:59:56.712175 21809 replica.cpp:678] Persisted action at 1
I0222 04:59:56.712198 21809 replica.cpp:663] Replica learned 2 action at position 1
I0222 04:59:56.713289 21809 registrar.cpp:489] Successfully updated the 'registry' in 24.890112ms
I0222 04:59:56.713397 21809 registrar.cpp:375] Successfully recovered registrar
I0222 04:59:56.713537 21809 log.cpp:702] Attempting to truncate the log to 1
I0222 04:59:56.713795 21809 master.cpp:1212] Recovered 0 slaves from the Registry (93B) ; allowing 10mins for slaves to re-register
I0222 04:59:56.713871 21809 coordinator.cpp:339] Coordinator attempting to write 3 action at position 2
I0222 04:59:56.714879 21809 replica.cpp:510] Replica received write request for position 2
I0222 04:59:56.725225 21809 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 10.311704ms
I0222 04:59:56.725270 21809 replica.cpp:678] Persisted action at 2
I0222 04:59:56.726066 21809 replica.cpp:657] Replica received learned notice for position 2
I0222 04:59:56.734110 21809 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 8.012327ms
I0222 04:59:56.734180 21809 leveldb.cpp:400] Deleting ~1 keys from leveldb took 36578ns
I0222 04:59:56.734201 21809 replica.cpp:678] Persisted action at 2
I0222 04:59:56.734221 21809 replica.cpp:663] Replica learned 3 action at position 2
I0222 04:59:56.747556 21809 slave.cpp:173] Slave started on 53)@192.168.122.68:39461
I0222 04:59:56.747601 21809 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/credential'
I0222 04:59:56.747774 21809 slave.cpp:280] Slave using credential for: test-principal
I0222 04:59:56.748021 21809 slave.cpp:298] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0222 04:59:56.748682 21809 slave.cpp:327] Slave hostname: centos-7
I0222 04:59:56.748705 21809 slave.cpp:328] Slave checkpoint: false
W0222 04:59:56.748714 21809 slave.cpp:330] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0222 04:59:56.749826 21809 state.cpp:34] Recovering state from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/meta'
I0222 04:59:56.750191 21809 status_update_manager.cpp:196] Recovering status update manager
I0222 04:59:56.750465 21809 slave.cpp:3775] Finished recovery
I0222 04:59:56.751260 21809 slave.cpp:623] New master detected at master@192.168.122.68:39461
I0222 04:59:56.751349 21809 slave.cpp:686] Authenticating with master master@192.168.122.68:39461
I0222 04:59:56.751369 21809 slave.cpp:691] Using default CRAM-MD5 authenticatee
I0222 04:59:56.751502 21809 slave.cpp:659] Detecting new master
I0222 04:59:56.751596 21809 status_update_manager.cpp:170] Pausing sending status updates
I0222 04:59:56.751668 21809 authenticatee.hpp:138] Creating new client SASL connection
I0222 04:59:56.752781 21809 master.cpp:3811] Authenticating slave(53)@192.168.122.68:39461
I0222 04:59:56.752820 21809 master.cpp:3822] Using default CRAM-MD5 authenticator
I0222 04:59:56.753124 21809 authenticator.hpp:169] Creating new server SASL connection
I0222 04:59:56.755609 21809 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0222 04:59:56.755641 21809 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 04:59:56.755708 21809 authenticator.hpp:275] Received SASL authentication start
I0222 04:59:56.755751 21809 authenticator.hpp:397] Authentication requires more steps
I0222 04:59:56.755813 21809 authenticatee.hpp:275] Received SASL authentication step
I0222 04:59:56.755887 21809 authenticator.hpp:303] Received SASL authentication step
I0222 04:59:56.755920 21809 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 04:59:56.755934 21809 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0222 04:59:56.756005 21809 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 04:59:56.756036 21809 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 04:59:56.756047 21809 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 04:59:56.756054 21809 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 04:59:56.756068 21809 authenticator.hpp:389] Authentication success
I0222 04:59:56.756155 21809 authenticatee.hpp:315] Authentication success
I0222 04:59:56.756219 21809 master.cpp:3869] Successfully authenticated principal 'test-principal' at slave(53)@192.168.122.68:39461
I0222 04:59:56.756503 21809 slave.cpp:757] Successfully authenticated with master master@192.168.122.68:39461
I0222 04:59:56.756611 21809 slave.cpp:1089] Will retry registration in 11.221976ms if necessary
I0222 04:59:56.756876 21809 master.cpp:2936] Registering slave at slave(53)@192.168.122.68:39461 (centos-7) with id 20150222-045956-1148889280-39461-21791-S0
I0222 04:59:56.757323 21809 registrar.cpp:444] Applied 1 operations in 70787ns; attempting to update the 'registry'
I0222 04:59:56.759790 21809 log.cpp:683] Attempting to append 299 bytes to the log
I0222 04:59:56.760000 21809 coordinator.cpp:339] Coordinator attempting to write 2 action at position 3
I0222 04:59:56.760920 21809 replica.cpp:510] Replica received write request for position 3
I0222 04:59:56.762037 21791 sched.cpp:154] Version: 0.22.0
I0222 04:59:56.762763 21806 sched.cpp:251] New master detected at master@192.168.122.68:39461
I0222 04:59:56.762835 21806 sched.cpp:307] Authenticating with master master@192.168.122.68:39461
I0222 04:59:56.762856 21806 sched.cpp:314] Using default CRAM-MD5 authenticatee
I0222 04:59:56.763082 21806 authenticatee.hpp:138] Creating new client SASL connection
I0222 04:59:56.763753 21806 master.cpp:3811] Authenticating scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.763784 21806 master.cpp:3822] Using default CRAM-MD5 authenticator
I0222 04:59:56.764040 21806 authenticator.hpp:169] Creating new server SASL connection
I0222 04:59:56.764624 21806 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0222 04:59:56.764653 21806 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 04:59:56.764719 21806 authenticator.hpp:275] Received SASL authentication start
I0222 04:59:56.764758 21806 authenticator.hpp:397] Authentication requires more steps
I0222 04:59:56.764819 21806 authenticatee.hpp:275] Received SASL authentication step
I0222 04:59:56.764889 21806 authenticator.hpp:303] Received SASL authentication step
I0222 04:59:56.764911 21806 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 04:59:56.764922 21806 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0222 04:59:56.764974 21806 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 04:59:56.765005 21806 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 04:59:56.765017 21806 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 04:59:56.765023 21806 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 04:59:56.765036 21806 authenticator.hpp:389] Authentication success
I0222 04:59:56.765120 21806 authenticatee.hpp:315] Authentication success
I0222 04:59:56.765182 21806 master.cpp:3869] Successfully authenticated principal 'test-principal' at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.765442 21806 sched.cpp:395] Successfully authenticated with master master@192.168.122.68:39461
I0222 04:59:56.765465 21806 sched.cpp:518] Sending registration request to master@192.168.122.68:39461
I0222 04:59:56.765522 21806 sched.cpp:551] Will retry registration in 1.283564292secs if necessary
I0222 04:59:56.765637 21806 master.cpp:1572] Received registration request for framework 'default' at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.765699 21806 master.cpp:1433] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0222 04:59:56.766120 21806 master.cpp:1636] Registering framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.766572 21806 hierarchical.hpp:320] Added framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.766598 21806 hierarchical.hpp:831] No resources available to allocate!
I0222 04:59:56.766609 21806 hierarchical.hpp:738] Performed allocation for 0 slaves in 15902ns
I0222 04:59:56.766753 21806 sched.cpp:445] Framework registered with 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.766790 21806 sched.cpp:459] Scheduler::registered took 15076ns
I0222 04:59:56.773710 21806 slave.cpp:1089] Will retry registration in 3.454005ms if necessary
I0222 04:59:56.773900 21806 master.cpp:2924] Ignoring register slave message from slave(53)@192.168.122.68:39461 (centos-7) as admission is already in progress
I0222 04:59:56.775297 21809 leveldb.cpp:342] Persisting action (318 bytes) to leveldb took 14.319807ms
I0222 04:59:56.775344 21809 replica.cpp:678] Persisted action at 3
I0222 04:59:56.776139 21809 replica.cpp:657] Replica received learned notice for position 3
I0222 04:59:56.778630 21806 slave.cpp:1089] Will retry registration in 32.764468ms if necessary
I0222 04:59:56.778779 21806 master.cpp:2924] Ignoring register slave message from slave(53)@192.168.122.68:39461 (centos-7) as admission is already in progress
I0222 04:59:56.783778 21809 leveldb.cpp:342] Persisting action (320 bytes) to leveldb took 7.609533ms
I0222 04:59:56.783828 21809 replica.cpp:678] Persisted action at 3
I0222 04:59:56.783849 21809 replica.cpp:663] Replica learned 2 action at position 3
I0222 04:59:56.785058 21809 registrar.cpp:489] Successfully updated the 'registry' in 27.669248ms
I0222 04:59:56.785274 21809 log.cpp:702] Attempting to truncate the log to 3
I0222 04:59:56.785815 21809 master.cpp:2993] Registered slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0222 04:59:56.785913 21809 coordinator.cpp:339] Coordinator attempting to write 3 action at position 4
I0222 04:59:56.786267 21809 hierarchical.hpp:452] Added slave 20150222-045956-1148889280-39461-21791-S0 (centos-7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0222 04:59:56.786600 21809 hierarchical.hpp:756] Performed allocation for slave 20150222-045956-1148889280-39461-21791-S0 in 292298ns
I0222 04:59:56.786684 21809 slave.cpp:791] Registered with master master@192.168.122.68:39461; given slave ID 20150222-045956-1148889280-39461-21791-S0
I0222 04:59:56.786792 21809 slave.cpp:2830] Received ping from slave-observer(52)@192.168.122.68:39461
I0222 04:59:56.787230 21809 master.cpp:3753] Sending 1 offers to framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.787334 21809 status_update_manager.cpp:177] Resuming sending status updates
I0222 04:59:56.788156 21809 sched.cpp:608] Scheduler::resourceOffers took 557128ns
I0222 04:59:56.788936 21809 master.cpp:2266] Processing ACCEPT call for offers: [ 20150222-045956-1148889280-39461-21791-O0 ] on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) for framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.789000 21809 master.cpp:2110] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
W0222 04:59:56.790506 21809 validation.cpp:327] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0222 04:59:56.790546 21809 validation.cpp:339] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0222 04:59:56.790808 21809 master.hpp:821] Adding task 0 with resources cpus(*):1; mem(*):128 on slave 20150222-045956-1148889280-39461-21791-S0 (centos-7)
I0222 04:59:56.790885 21809 master.cpp:2543] Launching task 0 of framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 with resources cpus(*):1; mem(*):128 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7)
I0222 04:59:56.791201 21809 replica.cpp:510] Replica received write request for position 4
I0222 04:59:56.791610 21806 slave.cpp:1120] Got assigned task 0 for framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.792140 21806 slave.cpp:1230] Launching task 0 for framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.794872 21806 slave.cpp:4177] Launching executor default of framework 20150222-045956-1148889280-39461-21791-0000 in work directory '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default/runs/753232b5-43ff-4fbf-b29a-0f76161132ab'
I0222 04:59:56.796846 21806 exec.cpp:130] Version: 0.22.0
I0222 04:59:56.797173 21806 slave.cpp:1377] Queuing task '0' for executor default of framework ",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2401,1.0,"MasterTest.ShutdownFrameworkWhileTaskRunning is flaky Looks like the executorShutdownTimeout() was called immediately after executorShutdown() was called!

","<code>
[ RUN      ] MasterTest.ShutdownFrameworkWhileTaskRunning
Using temporary directory '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_sBd6vK'
I0224 18:51:17.385068 30213 leveldb.cpp:176] Opened db in 1.262442ms
I0224 18:51:17.386360 30213 leveldb.cpp:183] Compacted db in 985102ns
I0224 18:51:17.387025 30213 leveldb.cpp:198] Created db iterator in 78043ns
I0224 18:51:17.387420 30213 leveldb.cpp:204] Seeked to beginning of db in 25814ns
I0224 18:51:17.387804 30213 leveldb.cpp:273] Iterated through 0 keys in the db in 25025ns
I0224 18:51:17.388270 30213 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0224 18:51:17.389760 30227 recover.cpp:449] Starting replica recovery
I0224 18:51:17.395699 30227 recover.cpp:475] Replica is in 4 status
I0224 18:51:17.398294 30227 replica.cpp:641] Replica in 4 status received a broadcasted recover request
I0224 18:51:17.398816 30227 recover.cpp:195] Received a recover response from a replica in 4 status
I0224 18:51:17.402415 30230 recover.cpp:566] Updating replica status to 3
I0224 18:51:17.403473 30229 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 273857ns
I0224 18:51:17.404093 30229 replica.cpp:323] Persisted replica status to 3
I0224 18:51:17.404930 30229 recover.cpp:475] Replica is in 3 status
I0224 18:51:17.407995 30233 replica.cpp:641] Replica in 3 status received a broadcasted recover request
I0224 18:51:17.410697 30231 recover.cpp:195] Received a recover response from a replica in 3 status
I0224 18:51:17.415710 30230 recover.cpp:566] Updating replica status to 1
I0224 18:51:17.416987 30227 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 221966ns
I0224 18:51:17.417579 30227 replica.cpp:323] Persisted replica status to 1
I0224 18:51:17.418803 30234 recover.cpp:580] Successfully joined the Paxos group
I0224 18:51:17.419699 30227 recover.cpp:464] Recover process terminated
I0224 18:51:17.430594 30234 master.cpp:349] Master 20150224-185117-2272962752-44950-30213 (fedora-19) started on 192.168.122.135:44950
I0224 18:51:17.431082 30234 master.cpp:395] Master only allowing authenticated frameworks to register
I0224 18:51:17.431453 30234 master.cpp:400] Master only allowing authenticated slaves to register
I0224 18:51:17.431828 30234 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_sBd6vK/credentials'
I0224 18:51:17.432740 30234 master.cpp:442] Authorization enabled
I0224 18:51:17.434224 30229 hierarchical.hpp:287] Initialized hierarchical allocator process
I0224 18:51:17.434994 30233 whitelist_watcher.cpp:79] No whitelist given
I0224 18:51:17.440687 30234 master.cpp:1356] The newly elected leader is master@192.168.122.135:44950 with id 20150224-185117-2272962752-44950-30213
I0224 18:51:17.441764 30234 master.cpp:1369] Elected as the leading master!
I0224 18:51:17.442430 30234 master.cpp:1187] Recovering from registrar
I0224 18:51:17.443053 30229 registrar.cpp:313] Recovering registrar
I0224 18:51:17.445468 30228 log.cpp:660] Attempting to start the writer
I0224 18:51:17.449970 30233 replica.cpp:477] Replica received implicit promise request with proposal 1
I0224 18:51:17.451359 30233 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 339488ns
I0224 18:51:17.451949 30233 replica.cpp:345] Persisted promised to 1
I0224 18:51:17.456845 30235 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(154)@192.168.122.135:44950
I0224 18:51:17.461741 30231 coordinator.cpp:230] Coordinator attemping to fill missing position
I0224 18:51:17.464686 30228 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0224 18:51:17.465515 30228 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 170261ns
I0224 18:51:17.465991 30228 replica.cpp:679] Persisted action at 0
I0224 18:51:17.470512 30229 replica.cpp:511] Replica received write request for position 0
I0224 18:51:17.471437 30229 leveldb.cpp:438] Reading position from leveldb took 139178ns
I0224 18:51:17.472129 30229 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 141560ns
I0224 18:51:17.472705 30229 replica.cpp:679] Persisted action at 0
I0224 18:51:17.476305 30228 replica.cpp:658] Replica received learned notice for position 0
I0224 18:51:17.477991 30228 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 208112ns
I0224 18:51:17.478574 30228 replica.cpp:679] Persisted action at 0
I0224 18:51:17.479044 30228 replica.cpp:664] Replica learned 1 action at position 0
I0224 18:51:17.484371 30233 log.cpp:676] Writer started with ending position 0
I0224 18:51:17.487396 30233 leveldb.cpp:438] Reading position from leveldb took 96498ns
I0224 18:51:17.498906 30233 registrar.cpp:346] Successfully fetched the registry (0B) in 55.234048ms
I0224 18:51:17.499781 30233 registrar.cpp:445] Applied 1 operations in 97308ns; attempting to update the 'registry'
I0224 18:51:17.503955 30231 log.cpp:684] Attempting to append 131 bytes to the log
I0224 18:51:17.505009 30231 coordinator.cpp:340] Coordinator attempting to write 2 action at position 1
I0224 18:51:17.507428 30228 replica.cpp:511] Replica received write request for position 1
I0224 18:51:17.508517 30228 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 316570ns
I0224 18:51:17.508985 30228 replica.cpp:679] Persisted action at 1
I0224 18:51:17.512902 30229 replica.cpp:658] Replica received learned notice for position 1
I0224 18:51:17.517261 30229 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 427860ns
I0224 18:51:17.517470 30229 replica.cpp:679] Persisted action at 1
I0224 18:51:17.517796 30229 replica.cpp:664] Replica learned 2 action at position 1
I0224 18:51:17.532624 30232 registrar.cpp:490] Successfully updated the 'registry' in 32.31104ms
I0224 18:51:17.533957 30228 log.cpp:703] Attempting to truncate the log to 1
I0224 18:51:17.534366 30228 coordinator.cpp:340] Coordinator attempting to write 3 action at position 2
I0224 18:51:17.536684 30227 replica.cpp:511] Replica received write request for position 2
I0224 18:51:17.537406 30227 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 196455ns
I0224 18:51:17.537946 30227 replica.cpp:679] Persisted action at 2
I0224 18:51:17.537695 30232 registrar.cpp:376] Successfully recovered registrar
I0224 18:51:17.544136 30231 master.cpp:1214] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register
I0224 18:51:17.546041 30227 replica.cpp:658] Replica received learned notice for position 2
I0224 18:51:17.546728 30227 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 192442ns
I0224 18:51:17.547058 30227 leveldb.cpp:401] Deleting ~1 keys from leveldb took 61064ns
I0224 18:51:17.547363 30227 replica.cpp:679] Persisted action at 2
I0224 18:51:17.547669 30227 replica.cpp:664] Replica learned 3 action at position 2
I0224 18:51:17.565460 30234 slave.cpp:174] Slave started on 138)@192.168.122.135:44950
I0224 18:51:17.566038 30234 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/credential'
I0224 18:51:17.566584 30234 slave.cpp:281] Slave using credential for: test-principal
I0224 18:51:17.567198 30234 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 18:51:17.567930 30234 slave.cpp:328] Slave hostname: fedora-19
I0224 18:51:17.568172 30234 slave.cpp:329] Slave checkpoint: false
W0224 18:51:17.568435 30234 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0224 18:51:17.570539 30227 state.cpp:35] Recovering state from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/meta'
I0224 18:51:17.573499 30232 status_update_manager.cpp:197] Recovering status update manager
I0224 18:51:17.574209 30234 slave.cpp:3775] Finished recovery
I0224 18:51:17.576277 30229 status_update_manager.cpp:171] Pausing sending status updates
I0224 18:51:17.576680 30234 slave.cpp:624] New master detected at master@192.168.122.135:44950
I0224 18:51:17.577131 30234 slave.cpp:687] Authenticating with master master@192.168.122.135:44950
I0224 18:51:17.577385 30234 slave.cpp:692] Using default CRAM-MD5 authenticatee
I0224 18:51:17.577945 30228 authenticatee.hpp:139] Creating new client SASL connection
I0224 18:51:17.578837 30234 slave.cpp:660] Detecting new master
I0224 18:51:17.579270 30228 master.cpp:3813] Authenticating slave(138)@192.168.122.135:44950
I0224 18:51:17.579900 30228 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 18:51:17.580572 30228 authenticator.hpp:170] Creating new server SASL connection
I0224 18:51:17.581501 30231 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 18:51:17.581805 30231 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 18:51:17.582222 30228 authenticator.hpp:276] Received SASL authentication start
I0224 18:51:17.582531 30228 authenticator.hpp:398] Authentication requires more steps
I0224 18:51:17.582945 30230 authenticatee.hpp:276] Received SASL authentication step
I0224 18:51:17.583351 30228 authenticator.hpp:304] Received SASL authentication step
I0224 18:51:17.583643 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 18:51:17.583911 30228 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 18:51:17.584241 30228 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 18:51:17.584517 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 18:51:17.584787 30228 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 18:51:17.585075 30228 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 18:51:17.585358 30228 authenticator.hpp:390] Authentication success
I0224 18:51:17.585750 30233 authenticatee.hpp:316] Authentication success
I0224 18:51:17.586354 30232 master.cpp:3871] Successfully authenticated principal 'test-principal' at slave(138)@192.168.122.135:44950
I0224 18:51:17.590953 30234 slave.cpp:758] Successfully authenticated with master master@192.168.122.135:44950
I0224 18:51:17.591686 30233 master.cpp:2938] Registering slave at slave(138)@192.168.122.135:44950 (fedora-19) with id 20150224-185117-2272962752-44950-30213-S0
I0224 18:51:17.592718 30233 registrar.cpp:445] Applied 1 operations in 100358ns; attempting to update the 'registry'
I0224 18:51:17.595989 30227 log.cpp:684] Attempting to append 302 bytes to the log
I0224 18:51:17.596757 30227 coordinator.cpp:340] Coordinator attempting to write 2 action at position 3
I0224 18:51:17.599280 30227 replica.cpp:511] Replica received write request for position 3
I0224 18:51:17.599481 30234 slave.cpp:1090] Will retry registration in 12.331173ms if necessary
I0224 18:51:17.601940 30227 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 999045ns
I0224 18:51:17.602339 30227 replica.cpp:679] Persisted action at 3
I0224 18:51:17.612349 30229 replica.cpp:658] Replica received learned notice for position 3
I0224 18:51:17.612934 30229 leveldb.cpp:343] Persisting action (323 bytes) to leveldb took 152139ns
I0224 18:51:17.613471 30229 replica.cpp:679] Persisted action at 3
I0224 18:51:17.613796 30229 replica.cpp:664] Replica learned 2 action at position 3
I0224 18:51:17.615980 30229 master.cpp:2926] Ignoring register slave message from slave(138)@192.168.122.135:44950 (fedora-19) as admission is already in progress
I0224 18:51:17.614302 30233 slave.cpp:1090] Will retry registration in 11.014835ms if necessary
I0224 18:51:17.617490 30234 registrar.cpp:490] Successfully updated the 'registry' in 24.179968ms
I0224 18:51:17.618989 30234 master.cpp:2995] Registered slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 18:51:17.619567 30233 hierarchical.hpp:455] Added slave 20150224-185117-2272962752-44950-30213-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0224 18:51:17.621080 30233 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:17.621441 30233 hierarchical.hpp:759] Performed allocation for slave 20150224-185117-2272962752-44950-30213-S0 in 544608ns
I0224 18:51:17.619704 30229 slave.cpp:792] Registered with master master@192.168.122.135:44950; given slave ID 20150224-185117-2272962752-44950-30213-S0
I0224 18:51:17.622195 30229 slave.cpp:2830] Received ping from slave-observer(125)@192.168.122.135:44950
I0224 18:51:17.622385 30227 status_update_manager.cpp:178] Resuming sending status updates
I0224 18:51:17.620266 30232 log.cpp:703] Attempting to truncate the log to 3
I0224 18:51:17.623522 30232 coordinator.cpp:340] Coordinator attempting to write 3 action at position 4
I0224 18:51:17.624835 30229 replica.cpp:511] Replica received write request for position 4
I0224 18:51:17.625727 30229 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 259831ns
I0224 18:51:17.626122 30229 replica.cpp:679] Persisted action at 4
I0224 18:51:17.627686 30227 replica.cpp:658] Replica received learned notice for position 4
I0224 18:51:17.628228 30227 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 93777ns
I0224 18:51:17.628785 30227 leveldb.cpp:401] Deleting ~2 keys from leveldb took 57660ns
I0224 18:51:17.629176 30227 replica.cpp:679] Persisted action at 4
I0224 18:51:17.629443 30227 replica.cpp:664] Replica learned 3 action at position 4
I0224 18:51:17.636715 30213 sched.cpp:157] Version: 0.23.0
I0224 18:51:17.638003 30229 sched.cpp:254] New master detected at master@192.168.122.135:44950
I0224 18:51:17.638602 30229 sched.cpp:310] Authenticating with master master@192.168.122.135:44950
I0224 18:51:17.639024 30229 sched.cpp:317] Using default CRAM-MD5 authenticatee
I0224 18:51:17.639580 30228 authenticatee.hpp:139] Creating new client SASL connection
I0224 18:51:17.640455 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950
I0224 18:51:17.641150 30228 master.cpp:3813] Authenticating scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.641597 30228 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 18:51:17.642643 30228 authenticator.hpp:170] Creating new server SASL connection
I0224 18:51:17.643698 30234 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 18:51:17.644296 30234 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 18:51:17.644739 30228 authenticator.hpp:276] Received SASL authentication start
I0224 18:51:17.645143 30228 authenticator.hpp:398] Authentication requires more steps
I0224 18:51:17.645654 30230 authenticatee.hpp:276] Received SASL authentication step
I0224 18:51:17.646122 30228 authenticator.hpp:304] Received SASL authentication step
I0224 18:51:17.646421 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 18:51:17.646746 30228 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 18:51:17.647203 30228 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 18:51:17.647644 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 18:51:17.648454 30228 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 18:51:17.648788 30228 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 18:51:17.649210 30228 authenticator.hpp:390] Authentication success
I0224 18:51:17.649705 30231 authenticatee.hpp:316] Authentication success
I0224 18:51:17.653314 30231 sched.cpp:398] Successfully authenticated with master master@192.168.122.135:44950
I0224 18:51:17.653766 30232 master.cpp:3871] Successfully authenticated principal 'test-principal' at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.654683 30231 sched.cpp:521] Sending registration request to master@192.168.122.135:44950
I0224 18:51:17.655138 30231 sched.cpp:554] Will retry registration in 1.028970132secs if necessary
I0224 18:51:17.657112 30232 master.cpp:1574] Received registration request for framework 'default' at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.658509 30232 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 18:51:17.659765 30232 master.cpp:1638] Registering framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.660727 30233 hierarchical.hpp:321] Added framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.661730 30233 hierarchical.hpp:741] Performed allocation for 1 slaves in 529369ns
I0224 18:51:17.662911 30229 sched.cpp:448] Framework registered with 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.663374 30229 sched.cpp:462] Scheduler::registered took 35637ns
I0224 18:51:17.664552 30232 master.cpp:3755] Sending 1 offers to framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.668009 30234 sched.cpp:611] Scheduler::resourceOffers took 2.574292ms
I0224 18:51:17.671038 30232 master.cpp:2268] Processing ACCEPT call for offers: [ 20150224-185117-2272962752-44950-30213-O0 ] on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) for framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.672071 30232 master.cpp:2112] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0224 18:51:17.674675 30232 validation.cpp:326] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 18:51:17.675395 30232 validation.cpp:338] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 18:51:17.676460 30232 master.hpp:822] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150224-185117-2272962752-44950-30213-S0 (fedora-19)
I0224 18:51:17.677078 30232 master.cpp:2545] Launching task 1 of framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19)
I0224 18:51:17.678084 30230 slave.cpp:1121] Got assigned task 1 for framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.680057 30230 s",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2403,2.0,MasterAllocatorTest/0.FrameworkReregistersFirst is flaky ,"<code>
[ RUN      ] MasterAllocatorTest/0.FrameworkReregistersFirst
Using temporary directory '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_Vy5Nml'
I0224 23:22:31.681670 30589 leveldb.cpp:176] Opened db in 2.943518ms
I0224 23:22:31.682152 30619 process.cpp:2117] Dropped / Lost event for PID: slave(65)@67.195.81.187:38391
I0224 23:22:31.682732 30589 leveldb.cpp:183] Compacted db in 1.029469ms
I0224 23:22:31.682777 30589 leveldb.cpp:198] Created db iterator in 15460ns
I0224 23:22:31.682792 30589 leveldb.cpp:204] Seeked to beginning of db in 1832ns
I0224 23:22:31.682802 30589 leveldb.cpp:273] Iterated through 0 keys in the db in 319ns
I0224 23:22:31.682833 30589 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0224 23:22:31.683228 30605 recover.cpp:449] Starting replica recovery
I0224 23:22:31.683537 30605 recover.cpp:475] Replica is in 4 status
I0224 23:22:31.684624 30615 replica.cpp:641] Replica in 4 status received a broadcasted recover request
I0224 23:22:31.684978 30616 recover.cpp:195] Received a recover response from a replica in 4 status
I0224 23:22:31.685405 30610 recover.cpp:566] Updating replica status to 3
I0224 23:22:31.686249 30609 master.cpp:349] Master 20150224-232231-3142697795-38391-30589 (pomona.apache.org) started on 67.195.81.187:38391
I0224 23:22:31.686265 30617 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 717897ns
I0224 23:22:31.686319 30617 replica.cpp:323] Persisted replica status to 3
I0224 23:22:31.686336 30609 master.cpp:395] Master only allowing authenticated frameworks to register
I0224 23:22:31.686357 30609 master.cpp:400] Master only allowing authenticated slaves to register
I0224 23:22:31.686390 30609 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_Vy5Nml/credentials'
I0224 23:22:31.686511 30606 recover.cpp:475] Replica is in 3 status
I0224 23:22:31.686563 30609 master.cpp:442] Authorization enabled
I0224 23:22:31.686929 30607 whitelist_watcher.cpp:79] No whitelist given
I0224 23:22:31.686954 30603 hierarchical.hpp:287] Initialized hierarchical allocator process
I0224 23:22:31.687134 30605 replica.cpp:641] Replica in 3 status received a broadcasted recover request
I0224 23:22:31.687731 30609 master.cpp:1356] The newly elected leader is master@67.195.81.187:38391 with id 20150224-232231-3142697795-38391-30589
I0224 23:22:31.839818 30609 master.cpp:1369] Elected as the leading master!
I0224 23:22:31.839834 30609 master.cpp:1187] Recovering from registrar
I0224 23:22:31.839926 30605 registrar.cpp:313] Recovering registrar
I0224 23:22:31.840000 30613 recover.cpp:195] Received a recover response from a replica in 3 status
I0224 23:22:31.840504 30606 recover.cpp:566] Updating replica status to 1
I0224 23:22:31.841599 30611 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 990330ns
I0224 23:22:31.841627 30611 replica.cpp:323] Persisted replica status to 1
I0224 23:22:31.841743 30611 recover.cpp:580] Successfully joined the Paxos group
I0224 23:22:31.841904 30611 recover.cpp:464] Recover process terminated
I0224 23:22:31.842366 30608 log.cpp:660] Attempting to start the writer
I0224 23:22:31.843557 30607 replica.cpp:477] Replica received implicit promise request with proposal 1
I0224 23:22:31.844312 30607 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 722368ns
I0224 23:22:31.844337 30607 replica.cpp:345] Persisted promised to 1
I0224 23:22:31.844889 30615 coordinator.cpp:230] Coordinator attemping to fill missing position
I0224 23:22:31.846043 30614 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0224 23:22:31.846729 30614 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 660024ns
I0224 23:22:31.846746 30614 replica.cpp:679] Persisted action at 0
I0224 23:22:31.847671 30611 replica.cpp:511] Replica received write request for position 0
I0224 23:22:31.847723 30611 leveldb.cpp:438] Reading position from leveldb took 27349ns
I0224 23:22:31.848429 30611 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 671461ns
I0224 23:22:31.848454 30611 replica.cpp:679] Persisted action at 0
I0224 23:22:31.849041 30615 replica.cpp:658] Replica received learned notice for position 0
I0224 23:22:31.849762 30615 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 690386ns
I0224 23:22:31.849787 30615 replica.cpp:679] Persisted action at 0
I0224 23:22:31.849808 30615 replica.cpp:664] Replica learned 1 action at position 0
I0224 23:22:31.850416 30612 log.cpp:676] Writer started with ending position 0
I0224 23:22:31.851490 30615 leveldb.cpp:438] Reading position from leveldb took 30659ns
I0224 23:22:31.854452 30610 registrar.cpp:346] Successfully fetched the registry (0B) in 14.491136ms
I0224 23:22:31.854543 30610 registrar.cpp:445] Applied 1 operations in 18024ns; attempting to update the 'registry'
I0224 23:22:31.857095 30604 log.cpp:684] Attempting to append 139 bytes to the log
I0224 23:22:31.857208 30608 coordinator.cpp:340] Coordinator attempting to write 2 action at position 1
I0224 23:22:31.858073 30609 replica.cpp:511] Replica received write request for position 1
I0224 23:22:31.858808 30609 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 701708ns
I0224 23:22:31.858835 30609 replica.cpp:679] Persisted action at 1
I0224 23:22:31.859508 30618 replica.cpp:658] Replica received learned notice for position 1
I0224 23:22:31.860267 30618 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 731035ns
I0224 23:22:31.860309 30618 replica.cpp:679] Persisted action at 1
I0224 23:22:31.860332 30618 replica.cpp:664] Replica learned 2 action at position 1
I0224 23:22:31.860983 30609 registrar.cpp:490] Successfully updated the 'registry' in 6.39616ms
I0224 23:22:31.861071 30609 registrar.cpp:376] Successfully recovered registrar
I0224 23:22:31.861126 30608 log.cpp:703] Attempting to truncate the log to 1
I0224 23:22:31.861249 30603 coordinator.cpp:340] Coordinator attempting to write 3 action at position 2
I0224 23:22:31.861248 30617 master.cpp:1214] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I0224 23:22:31.861831 30613 replica.cpp:511] Replica received write request for position 2
I0224 23:22:31.862504 30613 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 648125ns
I0224 23:22:31.862531 30613 replica.cpp:679] Persisted action at 2
I0224 23:22:31.863067 30603 replica.cpp:658] Replica received learned notice for position 2
I0224 23:22:31.863689 30603 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 602784ns
I0224 23:22:31.863737 30603 leveldb.cpp:401] Deleting ~1 keys from leveldb took 28697ns
I0224 23:22:31.863751 30603 replica.cpp:679] Persisted action at 2
I0224 23:22:31.863767 30603 replica.cpp:664] Replica learned 3 action at position 2
I0224 23:22:31.875962 30610 slave.cpp:174] Slave started on 66)@67.195.81.187:38391
I0224 23:22:31.876008 30610 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/credential'
I0224 23:22:31.876144 30610 slave.cpp:281] Slave using credential for: test-principal
I0224 23:22:31.876404 30610 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I0224 23:22:31.876489 30610 slave.cpp:328] Slave hostname: pomona.apache.org
I0224 23:22:31.876502 30610 slave.cpp:329] Slave checkpoint: false
W0224 23:22:31.876507 30610 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0224 23:22:31.877014 30603 state.cpp:35] Recovering state from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/meta'
I0224 23:22:31.877230 30610 status_update_manager.cpp:197] Recovering status update manager
I0224 23:22:31.877495 30609 slave.cpp:3776] Finished recovery
I0224 23:22:31.877879 30607 status_update_manager.cpp:171] Pausing sending status updates
I0224 23:22:31.877879 30604 slave.cpp:624] New master detected at master@67.195.81.187:38391
I0224 23:22:31.877959 30604 slave.cpp:687] Authenticating with master master@67.195.81.187:38391
I0224 23:22:31.877975 30604 slave.cpp:692] Using default CRAM-MD5 authenticatee
I0224 23:22:31.878069 30604 slave.cpp:660] Detecting new master
I0224 23:22:31.878093 30608 authenticatee.hpp:139] Creating new client SASL connection
I0224 23:22:31.878223 30604 master.cpp:3813] Authenticating slave(66)@67.195.81.187:38391
I0224 23:22:31.878244 30604 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 23:22:31.878412 30613 authenticator.hpp:170] Creating new server SASL connection
I0224 23:22:31.878525 30603 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 23:22:31.878551 30603 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 23:22:31.878625 30617 authenticator.hpp:276] Received SASL authentication start
I0224 23:22:31.878662 30617 authenticator.hpp:398] Authentication requires more steps
I0224 23:22:31.878727 30603 authenticatee.hpp:276] Received SASL authentication step
I0224 23:22:31.878815 30617 authenticator.hpp:304] Received SASL authentication step
I0224 23:22:31.878839 30617 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 23:22:31.878847 30617 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 23:22:31.878875 30617 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 23:22:31.878891 30617 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 23:22:31.878900 30617 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:31.878906 30617 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:31.878916 30617 authenticator.hpp:390] Authentication success
I0224 23:22:31.880717 30589 sched.cpp:157] Version: 0.23.0
I0224 23:22:32.017823 30611 authenticatee.hpp:316] Authentication success
I0224 23:22:32.017901 30618 master.cpp:3871] Successfully authenticated principal 'test-principal' at slave(66)@67.195.81.187:38391
I0224 23:22:32.018156 30615 sched.cpp:254] New master detected at master@67.195.81.187:38391
I0224 23:22:32.018240 30615 sched.cpp:310] Authenticating with master master@67.195.81.187:38391
I0224 23:22:32.018263 30615 sched.cpp:317] Using default CRAM-MD5 authenticatee
I0224 23:22:32.018496 30613 slave.cpp:758] Successfully authenticated with master master@67.195.81.187:38391
I0224 23:22:32.018579 30611 authenticatee.hpp:139] Creating new client SASL connection
I0224 23:22:32.018620 30613 slave.cpp:1090] Will retry registration in 363167ns if necessary
I0224 23:22:32.018811 30615 master.cpp:2938] Registering slave at slave(66)@67.195.81.187:38391 (pomona.apache.org) with id 20150224-232231-3142697795-38391-30589-S0
I0224 23:22:32.019122 30615 master.cpp:3813] Authenticating scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.019156 30615 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 23:22:32.019232 30612 registrar.cpp:445] Applied 1 operations in 57599ns; attempting to update the 'registry'
I0224 23:22:32.019394 30603 authenticator.hpp:170] Creating new server SASL connection
I0224 23:22:32.019541 30611 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 23:22:32.019568 30611 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 23:22:32.019666 30605 authenticator.hpp:276] Received SASL authentication start
I0224 23:22:32.019717 30605 authenticator.hpp:398] Authentication requires more steps
I0224 23:22:32.019805 30615 authenticatee.hpp:276] Received SASL authentication step
I0224 23:22:32.019942 30605 authenticator.hpp:304] Received SASL authentication step
I0224 23:22:32.019979 30605 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 23:22:32.019994 30605 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 23:22:32.020025 30605 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 23:22:32.020036 30610 slave.cpp:1090] Will retry registration in 10.850555ms if necessary
I0224 23:22:32.020053 30605 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 23:22:32.020102 30605 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:32.020117 30605 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:32.020133 30605 authenticator.hpp:390] Authentication success
I0224 23:22:32.020151 30611 master.cpp:2926] Ignoring register slave message from slave(66)@67.195.81.187:38391 (pomona.apache.org) as admission is already in progress
I0224 23:22:32.020226 30603 authenticatee.hpp:316] Authentication success
I0224 23:22:32.020256 30611 master.cpp:3871] Successfully authenticated principal 'test-principal' at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.020534 30615 sched.cpp:398] Successfully authenticated with master master@67.195.81.187:38391
I0224 23:22:32.020561 30615 sched.cpp:521] Sending registration request to master@67.195.81.187:38391
I0224 23:22:32.020635 30615 sched.cpp:554] Will retry registration in 490.035142ms if necessary
I0224 23:22:32.020720 30613 master.cpp:1574] Received registration request for framework 'default' at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.020787 30613 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 23:22:32.021122 30607 master.cpp:1638] Registering framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.021502 30611 hierarchical.hpp:321] Added framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.021531 30611 hierarchical.hpp:834] No resources available to allocate!
I0224 23:22:32.021543 30611 hierarchical.hpp:741] Performed allocation for 0 slaves in 18915ns
I0224 23:22:32.021618 30609 sched.cpp:448] Framework registered with 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.021673 30609 sched.cpp:462] Scheduler::registered took 26310ns
I0224 23:22:32.022400 30613 log.cpp:684] Attempting to append 316 bytes to the log
I0224 23:22:32.022523 30608 coordinator.cpp:340] Coordinator attempting to write 2 action at position 3
I0224 23:22:32.023232 30607 replica.cpp:511] Replica received write request for position 3
I0224 23:22:32.024055 30607 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 798548ns
I0224 23:22:32.024073 30607 replica.cpp:679] Persisted action at 3
I0224 23:22:32.024651 30610 replica.cpp:658] Replica received learned notice for position 3
I0224 23:22:32.025252 30610 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 580525ns
I0224 23:22:32.025271 30610 replica.cpp:679] Persisted action at 3
I0224 23:22:32.025297 30610 replica.cpp:664] Replica learned 2 action at position 3
I0224 23:22:32.025995 30618 registrar.cpp:490] Successfully updated the 'registry' in 6.586112ms
I0224 23:22:32.026228 30604 log.cpp:703] Attempting to truncate the log to 3
I0224 23:22:32.026360 30609 coordinator.cpp:340] Coordinator attempting to write 3 action at position 4
I0224 23:22:32.026669 30609 slave.cpp:2831] Received ping from slave-observer(66)@67.195.81.187:38391
I0224 23:22:32.026772 30609 slave.cpp:792] Registered with master master@67.195.81.187:38391; given slave ID 20150224-232231-3142697795-38391-30589-S0
I0224 23:22:32.026737 30603 master.cpp:2995] Registered slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I0224 23:22:32.026867 30603 status_update_manager.cpp:178] Resuming sending status updates
I0224 23:22:32.026868 30617 hierarchical.hpp:455] Added slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I0224 23:22:32.026921 30615 replica.cpp:511] Replica received write request for position 4
I0224 23:22:32.027276 30617 hierarchical.hpp:759] Performed allocation for slave 20150224-232231-3142697795-38391-30589-S0 in 351257ns
I0224 23:22:32.027580 30615 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 624249ns
I0224 23:22:32.027604 30615 replica.cpp:679] Persisted action at 4
I0224 23:22:32.027642 30618 master.cpp:3755] Sending 1 offers to framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.028223 30617 replica.cpp:658] Replica received learned notice for position 4
I0224 23:22:32.028621 30607 sched.cpp:611] Scheduler::resourceOffers took 648326ns
I0224 23:22:32.028916 30617 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 662416ns
I0224 23:22:32.028991 30617 leveldb.cpp:401] Deleting ~2 keys from leveldb took 47386ns
I0224 23:22:32.029021 30617 replica.cpp:679] Persisted action at 4
I0224 23:22:32.029044 30617 replica.cpp:664] Replica learned 3 action at position 4
I0224 23:22:32.029534 30613 master.cpp:2268] Processing ACCEPT call for offers: [ 20150224-232231-3142697795-38391-30589-O0 ] on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) for framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.190521 30613 master.cpp:2112] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
W0224 23:22:32.191864 30604 validation.cpp:328] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 23:22:32.191905 30604 validation.cpp:340] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 23:22:32.192206 30604 master.hpp:822] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org)
I0224 23:22:32.192318 30604 master.cpp:2545] Launching task 0 of framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 with resources cpus(*):1; mem(*):500 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)
I0224 23:22:32.192659 30611 slave.cpp:1121] Got assigned task 0 for framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.192847 30609 hierarchical.hpp:648] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):52",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2427,2.0,Add Java binding for the acceptOffers API. We introduced the new acceptOffers API in C++ driver. We need to provide Java binding for this API as well.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2428,2.0,Add Python bindings for the acceptOffers API. We introduced the new acceptOffers API in C++ driver. We need to provide Python binding for this API as well.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0
MESOS-2438,8.0,"Improve support for streaming HTTP Responses in libprocess. Currently libprocess' HTTP::Response supports a PIPE construct for doing streaming responses:

","<code>
struct Response
{
  ...

  // Either provide a ""body"", an absolute ""path"" to a file, or a
  // ""pipe"" for streaming a response. Distinguish between the cases
  // using 'type' below.
  //
  // BODY: Uses 'body' as the body of the response. These may be
  // encoded using gzip for efficiency, if 'Content-Encoding' is not
  // already specified.
  //
  // PATH: Attempts to perform a 'sendfile' operation on the file
  // found at 'path'.
  //
  // PIPE: Splices data from 'pipe' using 'Transfer-Encoding=chunked'.
  // Note that the read end of the pipe will be closed by libprocess
  // either after the write end has been closed or if the socket the
  // data is being spliced to has been closed (i.e., nobody is
  // listening any longer). This can cause writes to the pipe to
  // generate a SIGPIPE (which will terminate your program unless you
  // explicitly ignore them or handle them).
  //
  // In all cases (BODY, PATH, PIPE), you are expected to properly
  // specify the 'Content-Type' header, but the 'Content-Length' and
  // or 'Transfer-Encoding' headers will be filled in for you.
  enum {
    NONE,
    BODY,
    PATH,
    PIPE
  } type;

  ...
};
<code>

This interface is too low level and difficult to program against:

* Connection closure is signaled with SIGPIPE, which is difficult for callers to deal with (must suppress SIGPIPE locally or globally in order to get EPIPE instead).
* Pipes are generally for inter-process communication, and the pipe has finite size. With a blocking pipe the caller must deal with blocking when the pipe's buffer limit is exceeded. With a non-blocking pipe, the caller must deal with retrying the write.

We'll want to consider a few use cases:
# Sending an HTTP::Response with streaming data.
# Making a request with http::get and http::post in which the data is returned in a streaming manner.
# Making a request in which the request content is streaming.

This ticket will focus on 1 as it is required for the HTTP API.</code></code>",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2461,1.0,"Slave should provide details on processes running in its cgroups The slave can optionally be put into its own cgroups for a list of subsystems, e.g., for monitoring of memory and cpu. See the slave flag: --slave_subsystems

It currently refuses to start if there are any processes in its cgroups - this could be another slave or some subprocess started by a previous slave - and only logs the pids of those processes.

Improve this to log details about the processes: suggest at least the process command, uid running it, and perhaps its start time.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2485,3.0,"Add ability to distinguish slave removals metrics by reason. Currently we only expose a single removal metric ({{""master/slave_removals""}}) which makes it difficult to distinguish between removal reasons in the alerting.

Currently, a slave can be removed for the following reasons:

# Health checks failed.
# Slave unregistered.
# Slave was replaced by a new slave (on the same endpoint).

In the case of (2), we expect this to be due to maintenance and don't want to be notified as strongly as with health check failures.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2491,5.0,"Persist the reservation state on the slave h3. Goal

The goal for this task is to persist the reservation state stored on the master on the corresponding slave. The {{needCheckpointing}} predicate is used to capture the condition for which a resource needs to be checkpointed. Currently the only condition is {{isPersistentVolume}}. We'll update this to include dynamically reserved resources.

h3. Expected Outcome

* The dynamically reserved resources will be persisted on the slave.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2500,2.0,"Doxygen setup for libprocess Goals: 
- Initial doxygen setup. 
- Enable interested developers to generate already available doxygen content locally in their workspace and view it.
- Form the basis for future contributions of more doxygen content.

1. Devise a way to use Doxygen with Mesos source code. (For example, solve this by adding optional brew/apt-get installation to the ""Getting Started"" doc.)
2. Create a make target for libprocess documentation that can be manually triggered.
3. Create initial library top level documentation.
4. Enhance one header file with Doxygen. Make sure the generated output has all necessary links to navigate from the lib to the file and back, etc.
",,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2501,1.0,"Doxygen style for libprocess Create a description of the Doxygen style to use for libprocess documentation. 

It is expected that this will later also become the Doxygen style for stout and Mesos, but we are working on libprocess only for now.

Possible outcome: a file named docs/doxygen-style.md

We hope for much input and expect a lot of discussion!
",,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2507,5.0,"Performance issue in the master when a large number of slaves are registering. For large clusters, when a lot of slaves are registering, the master gets backlogged processing registration requests. {{perf}} revealed the following:

","<code>
Events: 14K cycles
 25.44%  libmesos-0.22.0-x.so  [.] mesos::internal::master::Master::registerSlave(process::UPID const&amp;, mesos::SlaveInfo const&amp;, std::vector<mesos::resource, std::allocator<mesos::resource=""""> &gt; cons
 11.18%  libmesos-0.22.0-x.so  [.] pipecb
  5.88%  libc-2.5.so             [.] malloc_consolidate
  5.33%  libc-2.5.so             [.] _int_free
  5.25%  libc-2.5.so             [.] malloc
  5.23%  libc-2.5.so             [.] _int_malloc
  4.11%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&amp;)
  3.22%  libmesos-0.22.0-x.so  [.] mesos::Resource::SharedDtor()
  3.10%  [kernel]                [k] _raw_spin_lock
  1.97%  libmesos-0.22.0-x.so  [.] mesos::Attribute::SharedDtor()
  1.28%  libc-2.5.so             [.] memcmp
  1.08%  libc-2.5.so             [.] free
<code>

This is likely because we loop over all the slaves for each registration:

<code>
void Master::registerSlave(
    const UPID&amp; from,
    const SlaveInfo&amp; slaveInfo,
    const vector<resource>&amp; checkpointedResources,
    const string&amp; version)
{
  // ...

  // Check if this slave is already registered (because it retries).
  foreachvalue (Slave* slave, slaves.registered) {
    if (slave-&gt;pid == from) {
      // ...
    }
  }
  // ...
}
<code></code></resource></code></code></mesos::resource,></code>",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2534,2.0,"PerfTest.ROOT_SampleInit test fails. From MESOS-2300 as well, it looks like this test is not reliable:

","<code>
[ RUN      ] PerfTest.ROOT_SampleInit
../../src/tests/perf_tests.cpp:147: Failure
Expected: (0u) &lt; (statistics.get().cycles()), actual: 0 vs 0
../../src/tests/perf_tests.cpp:150: Failure
Expected: (0.0) &lt; (statistics.get().task_clock()),
<code>

It looks like this test samples PID 1, which is either {{init}} or {{systemd}}. Per a chat with [~idownes] this should probably sample something that is guaranteed to be consuming cycles.</code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2581,3.0,"Document tips, best practices, guidelines for doing code reviews. We currently have a [""Committers Guide""|https://github.com/apache/mesos/blob/0.22.0/docs/committers-guide.md], however most of this information is relevant to all contributors looking to be participating in the code review process.

I'm proposing we extract much of this information into a more general ""Code Reviewing"" document, and include additional tips, best practices, lessons learned from members of the community.

This would be a great pre-requisite for on-boarding more committers and adding [MAINTAINERS|http://mail-archives.apache.org/mod_mbox/mesos-dev/201502.mbox/%3CCA+8RcoReugMVqoOpsnB8WGYBELa5fHwPA=J=YHJE22iwZvsbeQ@mail.gmail.com%3E].

The committers guide can be more specific to our expectations of committers, so we may want to make this into a ""committership"" document to help set expectations for contributors looking to become committers.",,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
MESOS-2591,2.0,Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2595,8.0,"Create docker executor Currently we're reusing the command executor to wait on the progress of the docker executor, but has the following drawback:

- We need to launch a seperate docker log process just to forward logs, where we can just simply reattach stdout/stderr if we create a specific executor for docker
- In general, Mesos slave is assuming that the executor is the one starting the actual task. But the current docker containerizer, the containerizer is actually starting the docker container first then launches the command executor to wait on it. This can cause problems if the container failed before the command executor was able to launch, as slave will try to update the limits of the containerizer on executor registration but then the docker containerizer will fail to do so since the container failed. 

Overall it's much simpler to tie the container lifecycle with the executor and simplfies logic and log management.",,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2615,1.0,"Pipe 'updateFramework' path from master to Allocator to support framework re-registration Pipe the 'updateFramework' call from the master through the allocator, as described in the design doc in the epic: MESOS-703",,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2622,1.0,"Document the semantic change in decorator return values In order to enable decorator modules to _remove_ metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks.

The Result<t> return values means:

||State||Before||After||
|Error|Error is propagated to the call-site|No change|
|None|The result of the decorator is not applied|No change|
|Some|The result of the decorator is *appended*|The result of the decorator *overwrites* the final labels/environment object|</t>",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2680,1.0,"Update modules doc with hook usage example Modules doc states the possibility of using hooks, but doesn't refer to necessary flags and usage example.",,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2696,5.0,Explore exposing stats from kernel Exploratory work.  Additional tickets to follow.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2700,13.0,"Determine CFS behavior with biased cpu.shares subtrees See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.

* Understand the relationship between cpu.shares and CFS quota.
* Determine range of possible bias splits
* Determine how to achieve bias, e.g., should 20:1 be 20480:1024 or ~1024:50
* Rigorous testing of behavior with varying loads, particularly the combination of latency sensitive loads for high biased tasks (non-revokable), and cpu intensive loads for the low biased tasks (revokable).
* Discover any performance edge cases?",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2701,8.0,"Implement bi-level cpu.shares subtrees in cgroups/cpu isolator. See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.

# Configurable bias
# Change cgroup layout
** Implement roll-forward migration path in isolator recover
** Document roll-back migration path",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2702,3.0,Compare split/flattened cgroup hierarchy for CPU oversubscription Investigate if a flat hierarchy is sufficient for oversubscription of CPU or if a two-way split is necessary/preferred.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2709,3.0,"Design Master discovery functionality for HTTP-only clients When building clients that do not bind to {{libmesos}} and only use the HTTP API (via ""pure"" language bindings - eg, Java-only) there is no simple way to discover the Master's IP address to connect to.

Rather than relying on 'out-of-band' configuration mechanisms, we would like to enable the ability of interrogating the ZooKeeper ensemble to discover the Master's IP address (and, possibly, other information) to which the HTTP API requests can be addressed to.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2722,13.0,"Create access to the Mesos ""state abstraction"" that does not require linking with libmesos See ""src/state/state.hpp"" and ""src/java/src/org/apache/mesos/state/*.java"" for what the ""state abstraction"" is.

With the new HTTP API (see MESOS-2288, MESOS-2289), there will be no need to link to libmesos to a framework for it to communicate with a Mesos master. However, if a framework uses the Mesos ""state abstraction"", either directly in C++ or through other language bindings (e.g., Java), it still needs to link with libmesos. So, in order to achieve libmesos-free frameworks that can leverage all APIs Mesos has to offer, we need a different way to access the ""state abstraction"". 

---

One approach is to provide an HTTP API for state queries that get routed through the Mesos master, which relays them by making calls into libmesos. Details TBD, including how separate this will be from the general HTTP API.
",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2726,13.0,"Add support for enabling network namespace without enabling the network isolator Following the discussion Kapil started, it is currently not possible to enable the linux network namespace for a container without enabling the network isolator (which requires certain kernel capabilities and dependencies).
Following the pattern of enabling pid namespaces (--isolation=""namespaces/pid""). One possible solution could be to add another one for network i.e. ""namespaces/network"".

",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2737,3.0,"Add documentation for maintainers. In order to scale the number of committers in the project, we proposed the concept of maintainers here:

http://markmail.org/thread/cjmdn3d7qfzbxhpm

To follow up on that proposal, we'll need some documentation to capture the concept of maintainers. Both how contributors can benefit from maintainer feedback and the expectations of ""maintainer-ship"".

In order to not enforce an excessive amount of process, maintainers will initially only serve as an encouraged means to help contributors find reviewers and get meaningful feedback.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2783,5.0,"document the fetcher For framework developers specifically, Mesos provides a fetcher to move binaries. This needs MVP documentation.

- What is it
- How does it help
- What protocols or schemas are supported
- Can it be extended

This is important to get framework developers over the hump of learning to code against Mesos and grow the ecosystem.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2793,1.0,Add support for container rootfs to Mesos isolators Mesos containers can have a different rootfs to the host. Update Isolator interface to pass rootfs during Isolator::prepare(). Update Isolators where  necessary.,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2794,13.0,"Implement filesystem isolators Move persistent volume support from Mesos containerizer to separate filesystem isolators, including support for container rootfs, where possible.

Use symlinks for posix systems without container rootfs. Use bind mounts for Linux with/without container rootfs.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2800,3.0,"Rename Option<T>::get(const T& _t) to getOrElse() and refactor the original function As suggested, if we want to change the name then we should refactor the original function as opposed to having 2 copies. 
If we did have 2 versions of the same function, would it make more sense to delegate one of them to the other.

As of today, there is only one file need to be refactor: 3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp at line 151, 161",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-2801,3.0,Remove dynamic allocation from Future<T> Remove the dynamic allocation of `T*` inside `Future::Data`,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2804,1.0,"Log framework capabilities in the master. Now that {{Capabilities}} has been added to FrameworkInfo, we should log these in the master when a framework (re-)registers (i.e. which capabilities are enabled and disabled). This would make debugging easier for framework developers.

Ideally, folding in the old {{checkpoint}} capability and logging that as well. In the past, the fact that {{checkpoint}} defaults to false has tripped up a lot of developers.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2805,8.0,"Make synchronized as primary form of synchronization. Re-organize Synchronized to allow {{synchronized(m)}} to work on:
  1. {{std::mutex}}
  2. {{std::recursive_mutex}}
  3. {{std::atomic_flag}}

Move synchronized.hpp into stout, so that developers don't think it's part of the utility suite for actors in libprocess.

Remove references to internal.hpp and replace them with {{std::atomic_flag}} synchronization.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-2807,3.0,"As a developer I need an easy way to convert MasterInfo protobuf to/from JSON As a preliminary to MESOS-2340, this requires the implementation of a simple (de)serialization mechanism to JSON from/to {{MasterInfo}} protobuf.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2815,2.0,"Flaky test: FetcherCacheHttpTest.HttpCachedSerialized FetcherCacheHttpTest.HttpCachedSerialized has been observed to fail (once  so far), but normally works fine. Here is the failure output:

[ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x3cca8e0, @0x2b1053422b20 { 128-byte object <d0-e1 00-00="""" 00-68="""" 04-00="""" 0f-00="""" 10-2b="""" 10-a1="""" 20-df="""" 40-9c="""" 59-49="""" 90-2d="""" a0-de=""""> })
Stack trace:
F0604 13:08:16.377907  6813 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
*** Check failure stack trace: ***
    @     0x2b10488ff6c0  google::LogMessage::Fail()
    @     0x2b10488ff60c  google::LogMessage::SendToLog()
    @     0x2b10488ff00e  google::LogMessage::Flush()
    @     0x2b1048901f22  google::LogMessageFatal::~LogMessageFatal()
    @           0x9721e4  _CheckFatal::~_CheckFatal()
    @           0xb4da86  mesos::internal::tests::FetcherCacheTest::launchTask()
    @           0xb53f8d  mesos::internal::tests::FetcherCacheHttpTest_HttpCachedSerialized_Test::TestBody()
    @          0x116ac21  testing::internal::HandleSehExceptionsInMethodIfSupported&lt;&gt;()
    @          0x1165e1e  testing::internal::HandleExceptionsInMethodIfSupported&lt;&gt;()
    @          0x114e1df  testing::Test::Run()
    @          0x114e902  testing::TestInfo::Run()
    @          0x114ee8a  testing::TestCase::Run()
    @          0x1153b54  testing::internal::UnitTestImpl::RunAllTests()
    @          0x116ba93  testing::internal::HandleSehExceptionsInMethodIfSupported&lt;&gt;()
    @          0x1166b0f  testing::internal::HandleExceptionsInMethodIfSupported&lt;&gt;()
    @          0x1152a60  testing::UnitTest::Run()
    @           0xcbc50f  main
    @     0x2b104af78ec5  (unknown)
    @           0x867559  (unknown)
make[4]: *** [check-local] Aborted
make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build'
make: *** [distcheck] Error 1
</d0-e1>",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2817,3.0,"Support revocable/non-revocable CPU updates in Mesos containerizer MESOS-2652 provided preliminary support for revocable cpu resources only when specified in the initial resources for a container. Improve this to support updates to/from revocable cpu. Note, *any* revocable cpu will result in the entire container's cpu being treated as revocable at the cpu isolator level. Higher level logic is responsible for adding/removing based on some policy.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2830,8.0,"Add an endpoint to slaves to allow launching system administration tasks As a System Administrator often times I need to run a organization-mandated task on every machine in the cluster. Ideally I could do this within the framework of mesos resources if it is a ""cleanup"" or auditing task, but sometimes I just have to run something, and run it now, regardless if a machine has un-accounted resources  (Ex: Adding/removing a user).

Currently to do this I have to completely bypass Mesos and SSH to the box. Ideally I could tell a mesos slave (With proper authentication) to run a container with the limited special permissions needed to get the task done.",,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2831,0.0,"FetcherCacheTest.SimpleEviction is flaky Saw this when reviewbot was testing an unrelated review https://reviews.apache.org/r/35119/

","<code>
[ RUN      ] FetcherCacheTest.SimpleEviction

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x5365320, @0x2b7bef9f1b20 { 128-byte object <b0-c0 00-00="""" 00-18="""" 00-77="""" 04-00="""" 0f-00="""" 20-75="""" 36-e6="""" 40-3a="""" 60-76="""" 7b-2b="""" 7c-2b="""" c0-75=""""> })
Stack trace:
F0607 21:19:23.181392  4246 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
*** Check failure stack trace: ***
    @     0x2b7be56c5972  google::LogMessage::Fail()
    @     0x2b7be56c58be  google::LogMessage::SendToLog()
    @     0x2b7be56c52c0  google::LogMessage::Flush()
    @     0x2b7be56c81d4  google::LogMessageFatal::~LogMessageFatal()
    @           0x97d182  _CheckFatal::~_CheckFatal()
    @           0xb58a28  mesos::internal::tests::FetcherCacheTest::launchTask()
    @           0xb65b50  mesos::internal::tests::FetcherCacheTest_SimpleEviction_Test::TestBody()
    @          0x11923b7  testing::internal::HandleSehExceptionsInMethodIfSupported&lt;&gt;()
    @          0x118d5b4  testing::internal::HandleExceptionsInMethodIfSupported&lt;&gt;()
    @          0x1175975  testing::Test::Run()
    @          0x1176098  testing::TestInfo::Run()
    @          0x1176620  testing::TestCase::Run()
    @          0x117b2ea  testing::internal::UnitTestImpl::RunAllTests()
    @          0x1193229  testing::internal::HandleSehExceptionsInMethodIfSupported&lt;&gt;()
    @          0x118e2a5  testing::internal::HandleExceptionsInMethodIfSupported&lt;&gt;()
    @          0x117a1f6  testing::UnitTest::Run()
    @           0xcc832b  main
    @     0x2b7be7d46ec5  (unknown)
    @           0x872379  (unknown)
<code></code></b0-c0></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2857,1.0,"FetcherCacheTest.LocalCachedExtract is flaky. From jenkins:

{noformat}
[ RUN      ] FetcherCacheTest.LocalCachedExtract
Using temporary directory '/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj'
I0610 20:04:48.591573 24561 leveldb.cpp:176] Opened db in 3.512525ms
I0610 20:04:48.592456 24561 leveldb.cpp:183] Compacted db in 828630ns
I0610 20:04:48.592512 24561 leveldb.cpp:198] Created db iterator in 32992ns
I0610 20:04:48.592531 24561 leveldb.cpp:204] Seeked to beginning of db in 8967ns
I0610 20:04:48.592545 24561 leveldb.cpp:273] Iterated through 0 keys in the db in 7762ns
I0610 20:04:48.592604 24561 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0610 20:04:48.593438 24587 recover.cpp:449] Starting replica recovery
I0610 20:04:48.593698 24587 recover.cpp:475] Replica is in EMPTY status
I0610 20:04:48.595641 24580 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0610 20:04:48.596086 24590 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0610 20:04:48.596607 24590 recover.cpp:566] Updating replica status to STARTING
I0610 20:04:48.597507 24590 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 717888ns
I0610 20:04:48.597535 24590 replica.cpp:323] Persisted replica status to STARTING
I0610 20:04:48.597697 24590 recover.cpp:475] Replica is in STARTING status
I0610 20:04:48.599165 24584 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0610 20:04:48.599434 24584 recover.cpp:195] Received a recover response from a replica in STARTING status
I0610 20:04:48.599915 24590 recover.cpp:566] Updating replica status to VOTING
I0610 20:04:48.600545 24590 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 432335ns
I0610 20:04:48.600574 24590 replica.cpp:323] Persisted replica status to VOTING
I0610 20:04:48.600659 24590 recover.cpp:580] Successfully joined the Paxos group
I0610 20:04:48.600797 24590 recover.cpp:464] Recover process terminated
I0610 20:04:48.602905 24594 master.cpp:363] Master 20150610-200448-3875541420-32907-24561 (dbade881e927) started on 172.17.0.231:32907
I0610 20:04:48.602957 24594 master.cpp:365] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --credentials=""/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.23.0/_inst/share/mesos/webui"" --work_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/master"" --zk_session_timeout=""10secs""
I0610 20:04:48.603374 24594 master.cpp:410] Master only allowing authenticated frameworks to register
I0610 20:04:48.603392 24594 master.cpp:415] Master only allowing authenticated slaves to register
I0610 20:04:48.603404 24594 credentials.hpp:37] Loading credentials for authentication from '/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/credentials'
I0610 20:04:48.603751 24594 master.cpp:454] Using default 'crammd5' authenticator
I0610 20:04:48.604928 24594 master.cpp:491] Authorization enabled
I0610 20:04:48.606034 24593 hierarchical.hpp:309] Initialized hierarchical allocator process
I0610 20:04:48.606106 24593 whitelist_watcher.cpp:79] No whitelist given
I0610 20:04:48.607430 24594 master.cpp:1476] The newly elected leader is master@172.17.0.231:32907 with id 20150610-200448-3875541420-32907-24561
I0610 20:04:48.607466 24594 master.cpp:1489] Elected as the leading master!
I0610 20:04:48.607481 24594 master.cpp:1259] Recovering from registrar
I0610 20:04:48.607712 24594 registrar.cpp:313] Recovering registrar
I0610 20:04:48.608543 24588 log.cpp:661] Attempting to start the writer
I0610 20:04:48.610231 24588 replica.cpp:477] Replica received implicit promise request with proposal 1
I0610 20:04:48.611335 24588 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.086439ms
I0610 20:04:48.611382 24588 replica.cpp:345] Persisted promised to 1
I0610 20:04:48.612303 24588 coordinator.cpp:230] Coordinator attemping to fill missing position
I0610 20:04:48.613883 24593 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0610 20:04:48.619205 24593 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.228235ms
I0610 20:04:48.619257 24593 replica.cpp:679] Persisted action at 0
I0610 20:04:48.621919 24593 replica.cpp:511] Replica received write request for position 0
I0610 20:04:48.621987 24593 leveldb.cpp:438] Reading position from leveldb took 49394ns
I0610 20:04:48.622689 24593 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 668412ns
I0610 20:04:48.622716 24593 replica.cpp:679] Persisted action at 0
I0610 20:04:48.623507 24584 replica.cpp:658] Replica received learned notice for position 0
I0610 20:04:48.624155 24584 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 612283ns
I0610 20:04:48.624186 24584 replica.cpp:679] Persisted action at 0
I0610 20:04:48.624215 24584 replica.cpp:664] Replica learned NOP action at position 0
I0610 20:04:48.625144 24593 log.cpp:677] Writer started with ending position 0
I0610 20:04:48.626724 24589 leveldb.cpp:438] Reading position from leveldb took 72013ns
I0610 20:04:48.629276 24591 registrar.cpp:346] Successfully fetched the registry (0B) in 21.520128ms
I0610 20:04:48.629663 24591 registrar.cpp:445] Applied 1 operations in 129587ns; attempting to update the 'registry'
I0610 20:04:48.632237 24579 log.cpp:685] Attempting to append 131 bytes to the log
I0610 20:04:48.632624 24579 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0610 20:04:48.633739 24579 replica.cpp:511] Replica received write request for position 1
I0610 20:04:48.634351 24579 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 583937ns
I0610 20:04:48.634382 24579 replica.cpp:679] Persisted action at 1
I0610 20:04:48.635073 24583 replica.cpp:658] Replica received learned notice for position 1
I0610 20:04:48.635442 24583 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 357122ns
I0610 20:04:48.635469 24583 replica.cpp:679] Persisted action at 1
I0610 20:04:48.635494 24583 replica.cpp:664] Replica learned APPEND action at position 1
I0610 20:04:48.636337 24583 registrar.cpp:490] Successfully updated the 'registry' in 6.534144ms
I0610 20:04:48.636725 24594 log.cpp:704] Attempting to truncate the log to 1
I0610 20:04:48.636858 24583 registrar.cpp:376] Successfully recovered registrar
I0610 20:04:48.637073 24594 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0610 20:04:48.637789 24594 master.cpp:1286] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register
I0610 20:04:48.638630 24583 replica.cpp:511] Replica received write request for position 2
I0610 20:04:48.639127 24583 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 396272ns
I0610 20:04:48.639153 24583 replica.cpp:679] Persisted action at 2
I0610 20:04:48.639804 24583 replica.cpp:658] Replica received learned notice for position 2
I0610 20:04:48.640965 24583 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.147322ms
I0610 20:04:48.641054 24583 leveldb.cpp:401] Deleting ~1 keys from leveldb took 72395ns
I0610 20:04:48.641197 24583 replica.cpp:679] Persisted action at 2
I0610 20:04:48.641345 24583 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0610 20:04:48.652274 24561 containerizer.cpp:111] Using isolation: posix/cpu,posix/mem
I0610 20:04:48.658994 24590 slave.cpp:188] Slave started on 42)@172.17.0.231:32907
I0610 20:04:48.659049 24590 slave.cpp:189] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_sandbox_directory=""/mnt/mesos/sandbox"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.23.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus(*):1000; mem(*):1000"" --revocable_cpu_low_priority=""true"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM""
I0610 20:04:48.659570 24590 credentials.hpp:85] Loading credential for authentication from '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/credential'
I0610 20:04:48.659803 24590 slave.cpp:319] Slave using credential for: test-principal
I0610 20:04:48.660441 24590 slave.cpp:352] Slave resources: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0610 20:04:48.660555 24590 slave.cpp:382] Slave hostname: dbade881e927
I0610 20:04:48.660578 24590 slave.cpp:387] Slave checkpoint: true
I0610 20:04:48.661550 24588 state.cpp:35] Recovering state from '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/meta'
I0610 20:04:48.661913 24590 status_update_manager.cpp:201] Recovering status update manager
I0610 20:04:48.662253 24590 containerizer.cpp:312] Recovering containerizer
I0610 20:04:48.663207 24581 slave.cpp:3950] Finished recovery
I0610 20:04:48.663761 24581 slave.cpp:4104] Querying resource estimator for oversubscribable resources
I0610 20:04:48.664077 24581 slave.cpp:678] New master detected at master@172.17.0.231:32907
I0610 20:04:48.664088 24586 status_update_manager.cpp:175] Pausing sending status updates
I0610 20:04:48.664245 24581 slave.cpp:741] Authenticating with master master@172.17.0.231:32907
I0610 20:04:48.664388 24581 slave.cpp:746] Using default CRAM-MD5 authenticatee
I0610 20:04:48.664611 24581 slave.cpp:714] Detecting new master
I0610 20:04:48.664647 24594 authenticatee.hpp:139] Creating new client SASL connection
I0610 20:04:48.664813 24581 slave.cpp:4125] Received oversubscribable resources  from the resource estimator
I0610 20:04:48.665060 24581 slave.cpp:4129] No master detected. Re-querying resource estimator after 15secs
I0610 20:04:48.665096 24594 master.cpp:4181] Authenticating slave(42)@172.17.0.231:32907
I0610 20:04:48.665247 24581 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(130)@172.17.0.231:32907
I0610 20:04:48.665657 24581 authenticator.cpp:92] Creating new server SASL connection
I0610 20:04:48.666013 24581 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0610 20:04:48.666159 24581 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0610 20:04:48.666443 24592 authenticator.cpp:197] Received SASL authentication start
I0610 20:04:48.666591 24592 authenticator.cpp:319] Authentication requires more steps
I0610 20:04:48.666779 24592 authenticatee.hpp:276] Received SASL authentication step
I0610 20:04:48.667007 24585 authenticator.cpp:225] Received SASL authentication step
I0610 20:04:48.667043 24585 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0610 20:04:48.667058 24585 auxprop.cpp:173] Looking up auxiliary property '*userPassword'
I0610 20:04:48.667110 24585 auxprop.cpp:173] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0610 20:04:48.667142 24585 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0610 20:04:48.667155 24585 auxprop.cpp:123] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0610 20:04:48.667163 24585 auxprop.cpp:123] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0610 20:04:48.667181 24585 authenticator.cpp:311] Authentication success
I0610 20:04:48.667331 24585 authenticatee.hpp:316] Authentication success
I0610 20:04:48.667414 24585 master.cpp:4211] Successfully authenticated principal 'test-principal' at slave(42)@172.17.0.231:32907
I0610 20:04:48.667505 24585 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(130)@172.17.0.231:32907
I0610 20:04:48.667809 24585 slave.cpp:812] Successfully authenticated with master master@172.17.0.231:32907
I0610 20:04:48.667982 24585 slave.cpp:1146] Will retry registration in 7.257154ms if necessary
I0610 20:04:48.668226 24585 master.cpp:3157] Registering slave at slave(42)@172.17.0.231:32907 (dbade881e927) with id 20150610-200448-3875541420-32907-24561-S0
I0610 20:04:48.668737 24585 registrar.cpp:445] Applied 1 operations in 90255ns; attempting to update the 'registry'
I0610 20:04:48.672297 24585 log.cpp:685] Attempting to append 305 bytes to the log
I0610 20:04:48.672541 24585 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0610 20:04:48.673528 24593 replica.cpp:511] Replica received write request for position 3
I0610 20:04:48.674321 24593 leveldb.cpp:343] Persisting action (324 bytes) to leveldb took 766804ns
I0610 20:04:48.674355 24593 replica.cpp:679] Persisted action at 3
I0610 20:04:48.675138 24587 replica.cpp:658] Replica received learned notice for position 3
I0610 20:04:48.675866 24587 leveldb.cpp:343] Persisting action (326 bytes) to leveldb took 714643ns
I0610 20:04:48.675897 24587 replica.cpp:679] Persisted action at 3
I0610 20:04:48.675922 24587 replica.cpp:664] Replica learned APPEND action at position 3
I0610 20:04:48.677471 24587 registrar.cpp:490] Successfully updated the 'registry' in 8.656128ms
I0610 20:04:48.677759 24587 log.cpp:704] Attempting to truncate the log to 3
I0610 20:04:48.678423 24593 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0610 20:04:48.678621 24587 master.cpp:3214] Registered slave 20150610-200448-3875541420-32907-24561-S0 at slave(42)@172.17.0.231:32907 (dbade881e927) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0610 20:04:48.678959 24593 hierarchical.hpp:496] Added slave 20150610-200448-3875541420-32907-24561-S0 (dbade881e927) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I0610 20:04:48.679157 24593 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:48.679183 24593 hierarchical.hpp:852] Performed allocation for slave 20150610-200448-3875541420-32907-24561-S0 in 175519ns
I0610 20:04:48.679805 24593 replica.cpp:511] Replica received write request for position 4
I0610 20:04:48.684160 24587 slave.cpp:846] Registered with master master@172.17.0.231:32907; given slave ID 20150610-200448-3875541420-32907-24561-S0
I0610 20:04:48.684229 24587 fetcher.cpp:77] Clearing fetcher cache
I0610 20:04:48.684666 24587 slave.cpp:869] Checkpointing SlaveInfo to '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/meta/slaves/20150610-200448-3875541420-32907-24561-S0/slave.info'
I0610 20:04:48.687366 24587 slave.cpp:2895] Received ping from slave-observer(42)@172.17.0.231:32907
I0610 20:04:48.687453 24584 status_update_manager.cpp:182] Resuming sending status updates
I0610 20:04:48.690901 24593 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 3.385583ms
I0610 20:04:48.690975 24593 replica.cpp:679] Persisted action at 4
I0610 20:04:48.692137 24593 replica.cpp:658] Replica received learned notice for position 4
I0610 20:04:48.692603 24593 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 449838ns
I0610 20:04:48.692674 24593 leveldb.cpp:401] Deleting ~2 keys from leveldb took 52471ns
I0610 20:04:48.692699 24593 replica.cpp:679] Persisted action at 4
I0610 20:04:48.692726 24593 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0610 20:04:48.693544 24561 sched.cpp:157] Version: 0.23.0
I0610 20:04:48.695550 24590 sched.cpp:254] New master detected at master@172.17.0.231:32907
I0610 20:04:48.697090 24590 sched.cpp:310] Authenticating with master master@172.17.0.231:32907
I0610 20:04:48.697136 24590 sched.cpp:317] Using default CRAM-MD5 authenticatee
I0610 20:04:48.697511 24586 authenticatee.hpp:139] Creating new client SASL connection
I0610 20:04:48.697937 24586 master.cpp:4181] Authenticating scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907
I0610 20:04:48.698185 24584 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(131)@172.17.0.231:32907
I0610 20:04:48.698575 24584 authenticator.cpp:92] Creating new server SASL connection
I0610 20:04:48.698807 24584 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0610 20:04:48.699898 24584 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0610 20:04:48.700040 24584 authenticator.cpp:197] Received SASL authentication start
I0610 20:04:48.700119 24584 authenticator.cpp:319] Authentication requires more steps
I0610 20:04:48.700193 24584 authenticatee.hpp:276] Received SASL authentication step
I0610 20:04:48.700287 24584 authenticator.cpp:225] Received SASL authentication step
I0610 20:04:48.700320 24584 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0610 20:04:48.700333 24584 auxprop.cpp:173] Looking up auxiliary property '*userPassword'
I0610 20:04:48.700392 24584 auxprop.cpp:173] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0610 20:04:48.700425 24584 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0610 20:04:48.700439 24584 auxprop.cpp:123] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0610 20:04:48.700448 24584 auxprop.cpp:123] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0610 20:04:48.700467 24584 authenticator.cpp:311] Authentication success
I0610 20:04:48.700640 24584 authenticatee.hpp:316] Authentication success
I0610 20:04:48.700742 24584 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(131)@172.17.0.231:32907
I0610 20:04:48.701282 24590 sched.cpp:398] Successfully authenticated with master master@172.17.0.231:32907
I0610 20:04:48.701315 24590 sched.cpp:521] Sending registration request to master@172.17.0.231:32907
I0610 20:04:48.701386 24590 sched.cpp:554] Will re",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2862,2.0,"mesos-fetcher won't fetch uris which begin with a "" "" Discovered while running mesos with marathon on top. If I launch a marathon task with a URI which is "" http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz"" mesos will log to stderr:

","<code>
I0611 22:39:22.815636 35673 logging.cpp:177] Logging to STDERR
I0611 22:39:25.643889 35673 fetcher.cpp:214] Fetching URI ' http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz'
I0611 22:39:25.648111 35673 fetcher.cpp:94] Hadoop Client not available, skipping fetch with Hadoop Client
Failed to fetch:  http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz
Failed to synchronize with slave (it's probably exited)
<code>

It would be nice if mesos trimmed leading whitespace before doing protocol detection so that simple mistakes are just fixed. </code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2866,3.0,"Slave should send oversubscribed resource information after master failover. After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2874,2.0,"Convert PortMappingStatistics to use automatic JSON encoding/decoding Simplify PortMappingStatistics by using JSON::Protocol and protobuf::parse to convert ResourceStatistics to/from line format.

This change will simplify the implementation of MESOS-2332.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-2879,1.0,"Random recursive_mutex errors in when running make check While running make check on OS X, from time to time {{recursive_mutex}} errors appear after running all the test successfully. Just one of the experience messages actually stops {{make check}} reporting an error.

The following error messages have been experienced:

","<code>
libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument


*** Aborted at 1434553937 (unix time) try ""date -d @1434553937"" if you are using GNU date ***
<code>
<code>
libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument
*** Aborted at 1434557001 (unix time) try ""date -d @1434557001"" if you are using GNU date ***
libc++abi.dylib: PC: @     0x7fff93855286 __pthread_kill
libc++abi.dylib: *** SIGABRT (@0x7fff93855286) received by PID 88060 (TID 0x10fc40000) stack trace: ***
    @     0x7fff8e1d6f1a _sigtramp
libc++abi.dylib:     @        0x10fc3f1a8 (unknown)
libc++abi.dylib:     @     0x7fff979deb53 abort
libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentMaking check in include
<code>
<code>
Assertion failed: (e == 0), function ~recursive_mutex, file /SourceCache/libcxx/libcxx-120/src/mutex.cpp, line 82.
*** Aborted at 1434555685 (unix time) try ""date -d @1434555685"" if you are using GNU date ***
PC: @     0x7fff93855286 __pthread_kill
*** SIGABRT (@0x7fff93855286) received by PID 60235 (TID 0x7fff7ebdc300) stack trace: ***
    @     0x7fff8e1d6f1a _sigtramp
    @        0x10b512350 google::CheckNotNull&lt;&gt;()
    @     0x7fff979deb53 abort
    @     0x7fff979a6c39 __assert_rtn
    @     0x7fff9bffdcc9 std::__1::recursive_mutex::~recursive_mutex()
    @        0x10b881928 process::ProcessManager::~ProcessManager()
    @        0x10b874445 process::ProcessManager::~ProcessManager()
    @        0x10b874418 process::finalize()
    @        0x10b2f7aec main
    @     0x7fff98edc5c9 start
make[5]: *** [check-local] Abort trap: 6
make[4]: *** [check-am] Error 2
make[3]: *** [check-recursive] Error 1
make[2]: *** [check-recursive] Error 1
make[1]: *** [check] Error 2
make: *** [check-recursive] Error 1
<code></code></code></code></code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2883,2.0,"Do not call hook manager if no hooks installed Hooks modules allow us to provide decorators during various aspects of a task lifecycle such as label decorator, environment decorator, etc.  Often the call into such a decorator hooks results in a new copy of labels, environment, etc., being returned to the call site. This is an unnecessary overhead if there are no hooks installed.

The proper way would be to call decorators via the hook manager only if there are some hooks installed. This would prevent unnecessary copying overhead if no hooks are available.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2884,5.0,"Allow isolators to specify required namespaces Currently, the LinuxLauncher looks into SlaveFlags to compute the namespaces that should be enabled when launching the executor. This means that a custom Isolator module doesn't have any way to specify dependency on a set of namespaces.

The proposed solution is to extend the Isolator interface to also export the namespaces dependency. This way the MesosContainerizer can directly query all loaded Isolators (inbuilt and custom modules) to compute the set of namespaces required by the executor. This set of namespaces is then passed on to the LinuxLauncher.
",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2891,3.0,"Performance regression in hierarchical allocator. For large clusters, the 0.23.0 allocator cannot keep up with the volume of slaves. After the following slave was re-registered, it took the allocator a long time to work through the backlog of slaves to add:

{noformat:title=45 minute delay}
I0618 18:55:40.738399 10172 master.cpp:3419] Re-registered slave 20150422-211121-2148346890-5050-3253-S4695
I0618 19:40:14.960636 10164 hierarchical.hpp:496] Added slave 20150422-211121-2148346890-5050-3253-S4695
{noformat}

Empirically, [addSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L462] and [updateSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L533] have become expensive.

Some timings from a production cluster reveal that the allocator spending in the low tens of milliseconds for each call to {{addSlave}} and {{updateSlave}}, when there are tens of thousands of slaves this amounts to the large delay seen above.

We also saw a slow steady increase in memory consumption, hinting further at a queue backup in the allocator.

A synthetic benchmark like we did for the registrar would be prudent here, along with visibility into the allocator's queue size.",,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2892,3.0,"Add benchmark for hierarchical allocator. In light of the performance regression in MESOS-2891, we'd like to have a synthetic benchmark of the allocator code, in order to analyze and direct improvements.",,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2893,1.0,"Add queue size metrics for the allocator. In light of the performance regression in MESOS-2891, we'd like to have visibility into the queue size of the allocator. This will enable alerting on performance problems.

We currently have no metrics in the allocator.

I will also look into MESOS-1286 now that we have gcc 4.8, current queue size gauges require a trip through the Process' queue.",,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2902,5.0,"Enable Mesos to use arbitrary script / module to figure out IP, HOSTNAME Currently Mesos tries to guess the IP, HOSTNAME by doing a reverse DNS lookup. This doesn't work on a lot of clouds as we want things like public IPs (which aren't the default DNS), there aren't FQDN names (Azure), or the correct way to figure it out is to call some cloud-specific endpoint.

If Mesos / Libprocess could load a mesos-module (Or run a script) which is provided per-cloud, we can figure out perfectly the IP / Hostname for the given environment. It also means we can ship one identical set of files to all hosts in a given provider which doesn't happen to have the DNS scheme + hostnames that libprocess/Mesos expects. Currently we have to generate host-specific config files which Mesos uses to guess.

The host-specific files break / fall apart if machines change IP / hostname without being reinstalled.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2903,3.0,"Network isolator should not fail when target state already exists Network isolator has multiple instances of the following pattern:

{noformat}
  Try<bool> something = ....::create();                                  
  if (something.isError()) {                                                   
    ++metrics.something_errors;                                      
    return Failure(""Failed to create something ..."")
  } else if (!icmpVethToEth0.get()) {                                               
    ++metrics.adding_veth_icmp_filters_already_exist;                               
    return Failure(""Something already exists"");
  }                                                                                 
{noformat}

These failures have occurred in operation due to the failure to recover or delete an orphan, causing the slave to remain on line but unable to create new resources.    We should convert the second failure message in this pattern to an information message since the final state of the system is the state that we requested.</bool>",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2904,1.0,"Add slave metric to count container launch failures We have seen circumstances where a machine has been consistently unable to launch containers due to an inconsistent state (for example, unexpected network configuration).   Adding a metric to track container launch failures will allow us to detect and alert on slaves in such a state.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0
MESOS-2917,1.0,"Specify correct libnl version for configure check Currently configure.ac lists 3.2.24 as the required libnl version. However, https://reviews.apache.org/r/31503 caused the minimum required version to be bumped to 3.2.26. The configure check thus fails to error out during execution and the dependency is captured only during the build step.",,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2925,1.0,"Invalid usage of ATOMIC_FLAG_INIT in member initialization The C++ specification states:

The macro ATOMIC_FLAG_INIT shall be defined in such a way that it can be used to initialize an object of type atomic_flag to the clear state. The macro can be used in the form: ""atomic_flag guard = ATOMIC_FLAG_INIT; ""It is unspecified whether the macro can be used in other initialization contexts."" 

Clang catches this (although reports it erroneously as a braced scaled init issue) and refuses to compile libprocess.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2938,1.0,"Linux docker inspect crashes On linux,  when a simple task is being executed on docker container executor, the sandbox stderr shows a backtrace:

*** Aborted at 1435254156 (unix time) try ""date -d @1435254156"" if you are using GNU date ***
PC: @     0x7ffff2b1364d (unknown)
*** SIGSEGV (@0xfffffffffffffff8) received by PID 88424 (TID 0x7fffe88fb700) from PID 18446744073709551608; stack trace: ***
    @     0x7ffff25a4340 (unknown)
    @     0x7ffff2b1364d (unknown)
    @     0x7ffff2b724df (unknown)
    @           0x4a6466 Docker::Container::~Container()
    @     0x7ffff5bfa49a Option&lt;&gt;::~Option()
    @     0x7ffff5c15989 Option&lt;&gt;::operator=()
    @     0x7ffff5c09e9f Try&lt;&gt;::operator=()
    @     0x7ffff5c09ee3 Result&lt;&gt;::operator=()
    @     0x7ffff5c0a938 process::Future&lt;&gt;::set()
    @     0x7ffff5bff412 process::Promise&lt;&gt;::set()
    @     0x7ffff5be53e3 Docker::___inspect()
    @     0x7ffff5be3cf8 _ZZN6Docker9__inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationENS2_6FutureISsEERKNS2_10SubprocessEENKUlRKSG_E1_clESL_
    @     0x7ffff5be91e9 _ZZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferEENUlSM_E_clESM_
    @     0x7ffff5be9d9d _ZNSt17_Function_handlerIFvRKN7process6FutureISsEEEZNKS2_5onAnyIZN6Docker9__inspectERKSsRKNS0_5OwnedINS0_7PromiseINS7_9ContainerEEEEERK6OptionI8DurationES2_RKNS0_10SubprocessEEUlS4_E1_vEES4_OT_NS2_6PreferEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_
    @     0x7ffff5c1eadd std::function&lt;&gt;::operator()()
    @     0x7ffff5c15e07 process::Future&lt;&gt;::onAny()
    @     0x7ffff5be93a1 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferE
    @     0x7ffff5be87f6 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_EESM_OT_
    @     0x7ffff5be459c Docker::__inspect()
    @     0x7ffff5be337c _ZZN6Docker8_inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationEENKUlvE_clEv
    @     0x7ffff5be8c5a _ZZNK7process6FutureI6OptionIiEE5onAnyIZN6Docker8_inspectERKSsRKNS_5OwnedINS_7PromiseINS5_9ContainerEEEEERKS1_I8DurationEEUlvE_vEERKS3_OT_NS3_10LessPreferEENUlSL_E_clESL_
    @     0x7ffff5be9b36 _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyIZN6Docker8_inspectERKSsRKNS0_5OwnedINS0_7PromiseINS9_9ContainerEEEEERKS2_I8DurationEEUlvE_vEES6_OT_NS4_10LessPreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_
    @     0x7ffff5c1e9b3 std::function&lt;&gt;::operator()()
    @     0x7ffff6184a1a _ZN7process8internal3runISt8functionIFvRKNS_6FutureI6OptionIiEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_
    @     0x7ffff617e64d process::Future&lt;&gt;::set()
    @     0x7ffff6752e46 process::Promise&lt;&gt;::set()
    @     0x7ffff675faec process::internal::cleanup()
    @     0x7ffff6765293 _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EE6__callIvIS6_EILm0ELm1ELm2EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x7ffff6764bcd _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EEclIJS6_EvEET0_DpOT_
    @     0x7ffff67642a5 _ZZNK7process6FutureI6OptionIiEE5onAnyISt5_BindIFPFvRKS3_PNS_7PromiseIS2_EERKNS_10SubprocessEESt12_PlaceholderILi1EESA_SB_EEvEES7_OT_NS3_6PreferEENUlS7_E_clES7_
    @     0x7ffff676531d _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyISt5_BindIFPFvS6_PNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EESC_SD_EEvEES6_OT_NS4_6PreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_
    @     0x7ffff5c1e9b3 std::function&lt;&gt;::operator()()
(END)
 ",,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2940,3.0,"Reconciliation is expensive for large numbers of tasks. We've observed that both implicit and explicit reconciliation are expensive for large numbers of tasks:

{noformat: title=Explicit O(100,000) tasks: 70secs}
I0625 20:55:23.716320 21937 master.cpp:3863] Performing explicit task state reconciliation for N tasks of framework F (NAME) at S@IP:PORT
I0625 20:56:34.812464 21937 master.cpp:5041] Removing task T with resources R of framework F on slave S at slave(1)@IP:PORT (HOST)
{noformat}

{noformat: title=Implicit with O(100,000) tasks: 60secs}
I0625 20:25:22.310601 21936 master.cpp:3802] Performing implicit task state reconciliation for framework F (NAME) at S@IP:PORT
I0625 20:26:23.874528 21921 master.cpp:218] Scheduling shutdown of slave S due to health check timeout
{noformat}

Let's add a benchmark to see if there are any bottlenecks here, and to guide improvements.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2950,8.0,"Implement current mesos Authorizer in terms of generalized Authorizer interface In order to maintain compatibility with existent versions of Mesos, as well as to prove the flexibility of the generalized {{mesos::Authorizer}} design, the current authorization mechanism through ACL definitions needs to run under the updated interface without any changes being noticeable by the current authorization users.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0
MESOS-2951,3.0,"Inefficient container usage collection docker containerizer currently collects usage statistics by calling os's process statistics (eg ps ). There is scope for making this efficient, say by querying cgroups file system.

",,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2957,1.0,Add version to MasterInfo This will help schedulers figure out the version of the master that they are interacting with. See MESOS-2736 for additional context.,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2962,1.0,"Slave fails with Abort stacktrace when DNS cannot resolve hostname If the DNS cannot resolve the hostname-to-IP for a slave node, we correctly return an {{Error}} object, but we then fail with a segfault.

This code adds a more user-friendly message and exits normally (with an {{EXIT_FAILURE}} code).

For example, forcing {{net::getIp()}} to always return an {{Error}}, now causes the slave to exit like this:

{noformat}
$ ./bin/mesos-slave.sh --master=10.10.1.121:5405
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0630 11:31:45.777465 1944417024 process.cpp:899] Could not obtain the IP address for stratos.local; the DNS service may not be able to resolve it: &gt;&gt;&gt; Marco was here!!!

$ echo $?
1
{noformat}",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-2966,5.0,socket::peer() and socket::address() might fail with SSL enabled libevent SSL currently uses a secondary FD so we need to virtualize the get() function on socket interface. ,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2967,5.0,Missing doxygen documentation for libprocess socket interface  Convert existing comments to doxygen format.  ,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2986,1.0,"Docker version output is not compatible with Mesos We currently use docker version to get Docker version, in Docker master branch and soon in Docker 1.8 [1] the output for this command changes. The solution for now will be to use the unchanged docker --version output, in the long term we should consider stop using the cli and use the API instead. 

[1] https://github.com/docker/docker/pull/14047",,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2993,3.0,Document  per container unique egress flow and network queueing statistics Document new network isolation capabilities in 0.23,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-2997,3.0,SSL connection failure causes failed CHECK. ,"<code>
[ RUN      ] SSLTest.BasicSameProcess
F0706 18:32:28.465451 238583808 libevent_ssl_socket.cpp:507] Check failed: 'self-&gt;bev' Must be non NULL
<code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3002,1.0,"Rename Option<T>::get(const T& _t) to getOrElse() broke network isolator Change to Option from get() to getOrElse() breaks network isolator.  Building with '../configure --with-network-isolator' generates the following error:

../../src/slave/containerizer/isolators/network/port_mapping.cpp: In static member function 'static Try<mesos::slave::isolator*> mesos::internal::slave::PortMappingIsolatorProcess::create(const mesos::internal::slave::Flags&amp;)':
../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: error: no matching function for call to 'Option<std::basic_string<char> &gt;::get(const char [1]) const'
       flags.resources.get(""""),
                             ^
../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: note: candidates are:
In file included from ../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp:26:0,
                 from ../../3rdparty/libprocess/include/process/check.hpp:19,
                 from ../../3rdparty/libprocess/include/process/collect.hpp:7,
                 from ../../src/slave/containerizer/isolators/network/port_mapping.cpp:30:
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note: const T&amp; Option<t>::get() const [with T = std::basic_string<char>]
   const T&amp; get() const { assert(isSome()); return t; }
            ^
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note:   candidate expects 0 arguments, 1 provided
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note: T&amp; Option<t>::get() [with T = std::basic_string<char>]
   T&amp; get() { assert(isSome()); return t; }
      ^
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note:   candidate expects 0 arguments, 1 provided
make[2]: *** [slave/containerizer/isolators/network/libmesos_no_3rdparty_la-port_mapping.lo] Error 1
make[2]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src'
make: *** [check-recursive] Error 1
</char></t></char></t></std::basic_string<char></mesos::slave::isolator*>",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3005,3.0,"SSL tests can fail depending on hostname configuration Depending on how /etc/hosts is configured, the SSL tests can fail with a bad hostname match for the certificate.
We can avoid this by explicitly matching the hostname for the certificate to the IP that will be used during the test.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3032,3.0,"Document containerizer launch  We currently dont have enough documentation for the containerizer component. This task adds documentation for containerizer launch sequence.
The mail goals are:
- Have diagrams (state, sequence, class etc) depicting the containerizer launch process.
- Make the documentation newbie friendly.
- Usable for future design discussions.",,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3046,3.0,"Stout's UUID re-seeds a new random generator during each call to UUID::random. Per [~StephanErb] and [~kevints]'s observations on MESOS-2940, stout's UUID abstraction is re-seeding the random generator during each call to {{UUID::random()}}, which is really expensive.

This is confirmed in the perf graph from MESOS-2940.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3051,8.0,"performance issues with port ranges comparison Testing in an environment with lots of frameworks (&gt;200), where the frameworks permanently decline resources they don't need. The allocator ends up spending a lot of time figuring out whether offers are refused (the code path through {{HierarchicalAllocatorProcess::isFiltered()}}.

In profiling a synthetic benchmark, it turns out that comparing port ranges is very expensive, involving many temporary allocations. 61% of Resources::contains() run time is in operator -= (Resource). 35% of Resources::contains() run time is in Resources::_contains().

The heaviest call chain through {{Resources::_contains}} is:

","<code>
Running Time          Self (ms)         Symbol Name
7237.0ms   35.5%          4.0            mesos::Resources::_contains(mesos::Resource const&amp;) const
7200.0ms   35.3%          1.0             mesos::contains(mesos::Resource const&amp;, mesos::Resource const&amp;)
7133.0ms   35.0%        121.0              mesos::operator&lt;=(mesos::Value_Ranges const&amp;, mesos::Value_Ranges const&amp;)
6319.0ms   31.0%          7.0               mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Ranges const&amp;)
6240.0ms   30.6%        161.0                mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&amp;)
1867.0ms    9.1%         25.0                 mesos::Value_Ranges::add_range()
1694.0ms    8.3%          4.0                 mesos::Value_Ranges::~Value_Ranges()
1495.0ms    7.3%         16.0                 mesos::Value_Ranges::operator=(mesos::Value_Ranges const&amp;)
 445.0ms    2.1%         94.0                 mesos::Value_Range::MergeFrom(mesos::Value_Range const&amp;)
 154.0ms    0.7%         24.0                 mesos::Value_Ranges::range(int) const
 103.0ms    0.5%         24.0                 mesos::Value_Ranges::range_size() const
  95.0ms    0.4%          2.0                 mesos::Value_Range::Value_Range(mesos::Value_Range const&amp;)
  59.0ms    0.2%          4.0                 mesos::Value_Ranges::Value_Ranges()
  50.0ms    0.2%         50.0                 mesos::Value_Range::begin() const
  28.0ms    0.1%         28.0                 mesos::Value_Range::end() const
  26.0ms    0.1%          0.0                 mesos::Value_Range::~Value_Range()
<code>

mesos::coalesce(Value_Ranges) gets done a lot and ends up being really expensive. The heaviest parts of the inverted call chain are:

<code>
Running Time	Self (ms)		Symbol Name
3209.0ms   15.7%	3209.0	 	mesos::Value_Range::~Value_Range()
3209.0ms   15.7%	0.0	 	 google::protobuf::internal::GenericTypeHandler<mesos::value_range>::Delete(mesos::Value_Range*)
3209.0ms   15.7%	0.0	 	  void google::protobuf::internal::RepeatedPtrFieldBase::Destroy<google::protobuf::repeatedptrfield<mesos::value_range>::TypeHandler&gt;()
3209.0ms   15.7%	0.0	 	   google::protobuf::RepeatedPtrField<mesos::value_range>::~RepeatedPtrField()
3209.0ms   15.7%	0.0	 	    google::protobuf::RepeatedPtrField<mesos::value_range>::~RepeatedPtrField()
3209.0ms   15.7%	0.0	 	     mesos::Value_Ranges::~Value_Ranges()
3209.0ms   15.7%	0.0	 	      mesos::Value_Ranges::~Value_Ranges()
2441.0ms   11.9%	0.0	 	       mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&amp;)
 452.0ms    2.2%	0.0	 	       mesos::remove(mesos::Value_Ranges*, mesos::Value_Range const&amp;)
 169.0ms    0.8%	0.0	 	       mesos::operator&lt;=(mesos::Value_Ranges const&amp;, mesos::Value_Ranges const&amp;)
  82.0ms    0.4%	0.0	 	       mesos::operator-=(mesos::Value_Ranges&amp;, mesos::Value_Ranges const&amp;)
  65.0ms    0.3%	0.0	 	       mesos::Value_Ranges::~Value_Ranges()

2541.0ms   12.4%	2541.0	 	google::protobuf::internal::GenericTypeHandler<mesos::value_range>::New()
2541.0ms   12.4%	0.0	 	 google::protobuf::RepeatedPtrField<mesos::value_range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::repeatedptrfield<mesos::value_range>::TypeHandler&gt;()
2305.0ms   11.3%	0.0	 	  google::protobuf::RepeatedPtrField<mesos::value_range>::Add()
2305.0ms   11.3%	0.0	 	   mesos::Value_Ranges::add_range()
1962.0ms    9.6%	0.0	 	    mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&amp;)
 343.0ms    1.6%	0.0	 	    mesos::ranges::add(mesos::Value_Ranges*, long long, long long)

236.0ms    1.1%	0.0	 	  void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::repeatedptrfield<mesos::value_range>::TypeHandler&gt;(google::protobuf::internal::RepeatedPtrFieldBase const&amp;)
1471.0ms    7.2%	1471.0	 	google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)
1333.0ms    6.5%	0.0	 	 google::protobuf::RepeatedPtrField<mesos::value_range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::repeatedptrfield<mesos::value_range>::TypeHandler&gt;()
1333.0ms    6.5%	0.0	 	  google::protobuf::RepeatedPtrField<mesos::value_range>::Add()
1333.0ms    6.5%	0.0	 	   mesos::Value_Ranges::add_range()
1086.0ms    5.3%	0.0	 	    mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&amp;)
 247.0ms    1.2%	0.0	 	    mesos::ranges::add(mesos::Value_Ranges*, long long, long long)

107.0ms    0.5%	0.0	 	 void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::repeatedptrfield<mesos::value_range>::TypeHandler&gt;(google::protobuf::internal::RepeatedPtrFieldBase const&amp;)
107.0ms    0.5%	0.0	 	  google::protobuf::RepeatedPtrField<mesos::value_range>::MergeFrom(google::protobuf::RepeatedPtrField<mesos::value_range> const&amp;)
107.0ms    0.5%	0.0	 	   mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&amp;)
105.0ms    0.5%	0.0	 	    mesos::Value_Ranges::CopyFrom(mesos::Value_Ranges const&amp;)
105.0ms    0.5%	0.0	 	     mesos::Value_Ranges::operator=(mesos::Value_Ranges const&amp;)
104.0ms    0.5%	0.0	 	      mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&amp;)
  1.0ms    0.0%	0.0	 	      mesos::remove(mesos::Value_Ranges*, mesos::Value_Range const&amp;)
  2.0ms    0.0%	0.0	 	    mesos::Resource::MergeFrom(mesos::Resource const&amp;)
  2.0ms    0.0%	0.0	 	     google::protobuf::internal::GenericTypeHandler<mesos::resource>::Merge(mesos::Resource const&amp;, mesos::Resource*)
  2.0ms    0.0%	0.0	 	      void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::repeatedptrfield<mesos::resource>::TypeHandler&gt;(google::protobuf::internal::RepeatedPtrFieldBase const&amp;)
 29.0ms    0.1%	0.0	 	 void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::repeatedptrfield<mesos::resource>::TypeHandler&gt;(google::protobuf::internal::RepeatedPtrFieldBase const&amp;)

898.0ms    4.4%	898.0	 	google::protobuf::RepeatedPtrField<mesos::value_range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::repeatedptrfield<mesos::value_range>::TypeHandler&gt;()
517.0ms    2.5%	0.0	 	 google::protobuf::RepeatedPtrField<mesos::value_range>::Add()
517.0ms    2.5%	0.0	 	  mesos::Value_Ranges::add_range()
429.0ms    2.1%	0.0	 	   mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&amp;)
 88.0ms    0.4%	0.0	 	   mesos::ranges::add(mesos::Value_Ranges*, long long, long long)
379.0ms    1.8%	0.0	 	 void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::repeatedptrfield<mesos::value_range>::TypeHandler&gt;(google::protobuf::internal::RepeatedPtrFieldBase const&amp;)
<code>
</code></google::protobuf::repeatedptrfield<mesos::value_range></mesos::value_range></google::protobuf::repeatedptrfield<mesos::value_range></mesos::value_range></google::protobuf::repeatedptrfield<mesos::resource></google::protobuf::repeatedptrfield<mesos::resource></mesos::resource></mesos::value_range></mesos::value_range></google::protobuf::repeatedptrfield<mesos::value_range></mesos::value_range></google::protobuf::repeatedptrfield<mesos::value_range></mesos::value_range></google::protobuf::repeatedptrfield<mesos::value_range></mesos::value_range></google::protobuf::repeatedptrfield<mesos::value_range></mesos::value_range></mesos::value_range></mesos::value_range></mesos::value_range></google::protobuf::repeatedptrfield<mesos::value_range></mesos::value_range></code></code></code>",1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3060,1.0,"FTP response code for success not recognized by fetcher. The response code for successful HTTP requests is 200, the response code for successful FTP file transfers is 226. The fetcher currently only checks for a response code of 200 even for FTP URIs. This results in failed fetching even though the resource gets downloaded successfully. This has been found by a dedicated external test using an FTP server.
",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3101,3.0,"Standardize separation of Windows/Linux-specific OS code There are 50+ files that must be touched to separate OS-specific code.

First, we will standardize the changes by using stout/abort.hpp as an example.
The review/discussion can be found here:
https://reviews.apache.org/r/36625/",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3102,5.0,"Separate OS-specific code in the stout library This issue tracks changes for all files under {{3rdparty/libprocess/3rdparty/stout/}}

The changes will be based on this commit:
https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3112,8.0,"Fetcher should perform cache eviction based on cache file usage patterns. Currently, the fetcher uses a trivial strategy to select eviction victims: it picks the first cache file it finds in linear iteration. This means that potentially a file that has just been used gets evicted the next moment. This performance loss can be avoided by even the simplest enhancement of the selection procedure.

Proposed approach: determine an effective yet relatively uncomplex and quick algorithm and implement it in `FetcherProcess::Cache::selectVictims(const Bytes&amp; requiredSpace)`. Suggestion: approximate MRU-retention somehow.

Unit-test what actually happens!",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3121,2.0,"Always disable SSLV2 The SSL protocol mismatch tests are failing on Centos7 when matching SSLV2 with SSLV2. Since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3122,2.0,"Add configurable UNIMPLEMENTED macro to stout During the transition to support for windows, it would be great if we had the ability to use a macro that marks functions as un-implemented.
To support being able to find all the unimplemented functions easily at compile time, while also being able to run the tests at the same time, we can add a configuration flag that controls whether this macro aborts or expands to a static assertion.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3127,1.0,Improve task reconciliation documentation. Include additional information about task reconciliation that explain why the master may not return the states of all tasks immediately and why an explicit task reconciliation algorithm is necessary.,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3129,2.0,"Move all MesosContainerizer related files under src/slave/containerizer/mesos Currently, some MesosContainerizer specific files are not in the correct location. For example:
{noformat} 
src/slave/containerizer/isolators/*
src/slave/containerizer/provisioner.hpp|cpp
{noformat}

They should be put under src/slave/containerizer/mesos/",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3132,5.0,"Allow slave to forward messages through the master for HTTP schedulers. The master currently has no install handler for {{ExecutorToFramework}} messages and the slave directly sends these messages to the scheduler driver, bypassing the master entirely.

We need to preserve this behavior for the driver, but HTTP schedulers will not have a libprocess 'pid'. We'll have to ensure that the {{RunTaskMessage}} and {{UpdateFrameworkMessage}} have an optional pid. For now the master will continue to set the pid, but 0.24.0 slaves will know to send messages through the master when the 'pid' is not available.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3134,5.0,"Port bootstrap to CMake Bootstrap does a lot of significant things, like setting up the git commit hooks. We will want something like bootstrap to run also on systems that don't have bash -- ideally this should just run in CMake itself.",,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3135,2.0,"Publish MasterInfo to ZK using JSON Following from MESOS-2340, which now allows Master to correctly decode JSON information ({{MasterInfo}}) published to Zookeeper, we can now enable the Master Leader Contender to serialize it too in JSON.",,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3142,2.0,"As a Developer I want a better way to run shell commands When reviewing the code in [r/36425|https://reviews.apache.org/r/36425/] [~benjaminhindman] noticed that there is a better abstraction that is possible to introduce for {{os::shell()}} that will simplify the caller's life.

Instead of having to handle all possible outcomes, we propose to refactor {{os::shell()}} as follows:

","<code>
/**
 * Returns the output from running the specified command with the shell.
 */
Try<std::string> shell(const string&amp; command)
{
  // Actually handle the WIFEXITED, WIFSIGNALED here!
}
<code>

where the returned string is {{stdout}} and, should the program be signaled, or exit with a non-zero exit code, we will simply return a {{Failure}} with an error message that will encapsulate both the returned/signaled state, and, possibly {{stderr}}.

And some test driven development:
<code>
EXPECT_ERROR(os::shell(""false""));
EXPECT_SOME(os::shell(""true""));

EXPECT_SOME_EQ(""hello world"", os::shell(""echo hello world""));
<code>

Alternatively, the caller can ask to have {{stderr}} conflated with {{stdout}}:
<code>
Try<string> outAndErr = os::shell(""myCmd --foo 2&gt;&amp;1"");
<code>

However, {{stderr}} will be ignored by default:
<code>
// We don't read standard error by default.
EXPECT_SOME_EQ("""", os::shell(""echo hello world 1&gt;&amp;2""));

// We don't even read stderr if something fails (to return in Try::error).
Try<string> output = os::shell(""echo hello world 1&gt;&amp;2 &amp;&amp; false"");
EXPECT_ERROR(output);
EXPECT_FALSE(strings::contains(output.error(), ""hello world""));
<code>

An analysis of existing usage shows that in almost all cases, the caller only cares {{if not error}}; in fact, the actual exit code is read only once, and even then, in a test case.

We believe this will simplify the API to the caller, and will significantly reduce the length and complexity at the calling sites (&lt;6 LOC against the current 20+).</code></string></code></code></string></code></code></code></code></std::string></code>",0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3143,2.0,"Disable endpoints rule fails to recognize HTTP path delegates In mesos, one can use the flag {{--firewall_rules}} to disable endpoints. Disabled endpoints will return a _403 Forbidden_ response whenever someone tries to access endpoints.

Libprocess support adding one default delegate for endpoints, which is the default process id which handles endpoints if no process id was given. For example, the default id of the master libprocess process is {{master}} which is also set as the delegate for the master system process, so a request to the endpoint {{http://master-address:5050/state.json}} will effectively be resolved by {{http://master-address:5050/master/state.json}}. But if one disables  {{/state.json}} because of how delegates work, it can still access {{/master/state.json}}.

The only workaround is to disabled both enpoints.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3154,1.0,"Enable Mesos Agent Node to use arbitrary script / module to figure out IP, HOSTNAME Following from MESOS-2902 we want to enable the same functionality in the Mesos Agents too.

This is probably best done once we implement the new {{os::shell}} semantics, as described in MESOS-3142.",,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3158,3.0,"Libprocess Process: Join runqueue workers during finalization The lack of synchronization between ProcessManager destruction and the thread pool threads running the queued processes means that the shared state that is part of the ProcessManager gets destroyed prematurely.
Synchronizing the ProcessManager destructor with draining the work queues and stopping the workers will allow us to not require leaking the shared state to avoid use beyond destruction.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3162,3.0,"Provide a means to check http connection equality for streaming connections. If one uses an http::Pipe::Writer to stream a response, one cannot compare the writer with another to see if the connection has changed.

This is useful for example, in the master's http api when there is asynchronous disconnection logic. When we handle the disconnection, it's possible for the scheduler to have re-subscribed, and so the master needs to tell if the disconnection event is relevant for the current connection before taking action.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3164,3.0,Introduce QuotaInfo message A {{QuotaInfo}} protobuf message is internal representation for quota related information (e.g. for persisting quota). The protobuf message should be extendable for future needs and allows for easy aggregation across roles and operator principals. It may also be used to pass quota information to allocators.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3168,2.0,"MesosZooKeeperTest fixture can have side effects across tests MesosZooKeeperTest fixture doesn't restart the ZooKeeper server for each test. This means if a test shuts down the ZooKeeper server, the next test (using the same fixture) might fail. 

For an example see https://reviews.apache.org/r/36807/",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-3169,2.0,"FrameworkInfo should only be updated if the re-registration is valid See Ben Mahler's comment in https://reviews.apache.org/r/32961/
FrameworkInfo should not be updated if the re-registration is invalid. This can happen in a few cases under the branching logic, so this requires some refactoring.
Notice that a ","<code>FrameworkErrorMessage<code> can be generated  both inside <code>else if (from != framework-&gt;pid)<code> as well as from inside <code>failoverFramework(framework, from);<code></code></code></code></code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3173,1.0,"Mark Path::basename, Path::dirname as const functions. The functions Path::basename and Path::dirname in stout/path.hpp are not marked const, although they could. Marking them const would remove some ambiguities in the usage of these functions.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3174,1.0,"Fetcher logs erroneous message when successfully extracting an archive When fetching an asset while not using the cache, the fetcher may erroneously report this: ""Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: "".

This message appears in the stderr log in the sandbox no matter whether extraction succeeded or not. It should be absent after successful extraction.
",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3185,3.0,"Refactor Subprocess logic in linux/perf.cpp to use common subroutine MESOS-2834 will enhance the perf isolator to support the different output formats provided by difference kernel versions.  In order to achieve this, it requires to execute the ""perf --version"" command. 

We should decompose the existing Subcommand processing in perf so that we can share the implementation between the multiple uses of perf.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3189,2.0,"TimeTest.Now fails with --enable-libevent [ RUN      ] TimeTest.Now
../../../3rdparty/libprocess/src/tests/time_tests.cpp:50: Failure
Expected: (Microseconds(10)) &lt; (Clock::now() - t1), actual: 8-byte object &lt;10-27 00-00 00-00 00-00&gt; vs 0ns
[  FAILED  ] TimeTest.Now (0 ms)",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3200,1.0,"Remove unused 'fatal' and 'fatalerror' macros There exist {{fatal}} and {{fatalerror}} macros in both {{libprocess}} and {{stout}}. None of them are currently used as we favor {{glog}}'s {{LOG(FATAL)}}, and therefore should be removed.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3201,3.0,"Libev handle_async can deadlock with run_in_event_loop Due to the arbitrary nature of the functions that are executed in handle_async, invoking them under the (A) {{watchers_mutex}} can lead to deadlocks if (B) is acquired before calling {{run_in_event_loop}} and (B) is also acquired within the arbitrary function.
","<code>
==82679== Thread #10: lock order ""0x60774F8 before 0x60768C0"" violated
==82679== 
==82679== Observed (incorrect) order is: acquisition of lock at 0x60768C0
==82679==    at 0x4C32145: pthread_mutex_lock (in /usr/lib/valgrind/vgpreload_helgrind-amd64-linux.so)
==82679==    by 0x692C9B: __gthread_mutex_lock(pthread_mutex_t*) (gthr-default.h:748)
==82679==    by 0x6950BF: std::mutex::lock() (mutex:134)
==82679==    by 0x696219: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*)::{lambda(std::mutex*)#1}::operator()(std::mutex*) const (synchronized.hpp:58)
==82679==    by 0x696238: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*)::{lambda(std::mutex*)#1}::_FUN(std::mutex*) (synchronized.hpp:58)
==82679==    by 0x6984CF: Synchronized<std::mutex>::Synchronized(std::mutex*, void (*)(std::mutex*), void (*)(std::mutex*)) (synchronized.hpp:35)
==82679==    by 0x6962DE: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*) (synchronized.hpp:60)
==82679==    by 0x728FE1: process::handle_async(ev_loop*, ev_async*, int) (libev.cpp:48)
==82679==    by 0x761384: ev_invoke_pending (ev.c:2994)
==82679==    by 0x7643C4: ev_run (ev.c:3394)
==82679==    by 0x728E37: ev_loop (ev.h:826)
==82679==    by 0x729469: process::EventLoop::run() (libev.cpp:135)
==82679== 
==82679==  followed by a later acquisition of lock at 0x60774F8
==82679==    at 0x4C32145: pthread_mutex_lock (in /usr/lib/valgrind/vgpreload_helgrind-amd64-linux.so)
==82679==    by 0x4C6F9D: __gthread_mutex_lock(pthread_mutex_t*) (gthr-default.h:748)
==82679==    by 0x4C6FED: __gthread_recursive_mutex_lock(pthread_mutex_t*) (gthr-default.h:810)
==82679==    by 0x4F5D3D: std::recursive_mutex::lock() (mutex:175)
==82679==    by 0x516513: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*)::{lambda(std::recursive_mutex*)#1}::operator()(std::recursive_mutex*) const (synchronized.hpp:58)
==82679==    by 0x516532: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*)::{lambda(std::recursive_mutex*)#1}::_FUN(std::recursive_mutex*) (synchronized.hpp:58)
==82679==    by 0x52E619: Synchronized<std::recursive_mutex>::Synchronized(std::recursive_mutex*, void (*)(std::recursive_mutex*), void (*)(std::recursive_mutex*)) (synchronized.hpp:35)
==82679==    by 0x5165D4: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*) (synchronized.hpp:60)
==82679==    by 0x6BF4E1: process::ProcessManager::use(process::UPID const&amp;) (process.cpp:2127)
==82679==    by 0x6C2B8C: process::ProcessManager::terminate(process::UPID const&amp;, bool, process::ProcessBase*) (process.cpp:2604)
==82679==    by 0x6C6C3C: process::terminate(process::UPID const&amp;, bool) (process.cpp:3107)
==82679==    by 0x692B65: process::Latch::trigger() (latch.cpp:53)
<code>

This was introduced in https://github.com/apache/mesos/commit/849fc4d361e40062073324153ba97e98e294fdf2</code></std::recursive_mutex></std::recursive_mutex></std::recursive_mutex></std::recursive_mutex></std::recursive_mutex></std::recursive_mutex></std::recursive_mutex></std::mutex></std::mutex></std::mutex></std::mutex></std::mutex></std::mutex></std::mutex></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3203,1.0,"MasterAuthorizationTest.DuplicateRegistration test is flaky [ RUN      ] MasterAuthorizationTest.DuplicateRegistration
Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7'
I0804 22:16:01.578500 26185 leveldb.cpp:176] Opened db in 2.188338ms
I0804 22:16:01.579172 26185 leveldb.cpp:183] Compacted db in 645075ns
I0804 22:16:01.579211 26185 leveldb.cpp:198] Created db iterator in 15766ns
I0804 22:16:01.579227 26185 leveldb.cpp:204] Seeked to beginning of db in 1658ns
I0804 22:16:01.579238 26185 leveldb.cpp:273] Iterated through 0 keys in the db in 313ns
I0804 22:16:01.579282 26185 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0804 22:16:01.579787 26212 recover.cpp:449] Starting replica recovery
I0804 22:16:01.580075 26212 recover.cpp:475] Replica is in EMPTY status
I0804 22:16:01.581014 26205 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0804 22:16:01.581357 26211 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0804 22:16:01.581761 26207 recover.cpp:566] Updating replica status to STARTING
I0804 22:16:01.582334 26218 master.cpp:377] Master 20150804-221601-2550141356-59302-26185 (d6d349cd895b) started on 172.17.0.152:59302
I0804 22:16:01.582355 26218 master.cpp:379] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --credentials=""/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.24.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/master"" --zk_session_timeout=""10secs""
I0804 22:16:01.582711 26218 master.cpp:424] Master only allowing authenticated frameworks to register
I0804 22:16:01.582722 26218 master.cpp:429] Master only allowing authenticated slaves to register
I0804 22:16:01.582728 26218 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials'
I0804 22:16:01.582929 26204 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421543ns
I0804 22:16:01.582950 26204 replica.cpp:323] Persisted replica status to STARTING
I0804 22:16:01.583032 26218 master.cpp:468] Using default 'crammd5' authenticator
I0804 22:16:01.583132 26211 recover.cpp:475] Replica is in STARTING status
I0804 22:16:01.583154 26218 master.cpp:505] Authorization enabled
I0804 22:16:01.583356 26214 whitelist_watcher.cpp:79] No whitelist given
I0804 22:16:01.583411 26217 hierarchical.hpp:346] Initialized hierarchical allocator process
I0804 22:16:01.583976 26213 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0804 22:16:01.584187 26209 recover.cpp:195] Received a recover response from a replica in STARTING status
I0804 22:16:01.584581 26213 master.cpp:1495] The newly elected leader is master@172.17.0.152:59302 with id 20150804-221601-2550141356-59302-26185
I0804 22:16:01.584609 26213 master.cpp:1508] Elected as the leading master!
I0804 22:16:01.584627 26213 master.cpp:1278] Recovering from registrar
I0804 22:16:01.584656 26204 recover.cpp:566] Updating replica status to VOTING
I0804 22:16:01.584770 26212 registrar.cpp:313] Recovering registrar
I0804 22:16:01.585261 26218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 370526ns
I0804 22:16:01.585285 26218 replica.cpp:323] Persisted replica status to VOTING
I0804 22:16:01.585412 26216 recover.cpp:580] Successfully joined the Paxos group
I0804 22:16:01.585667 26216 recover.cpp:464] Recover process terminated
I0804 22:16:01.586047 26213 log.cpp:661] Attempting to start the writer
I0804 22:16:01.587164 26211 replica.cpp:477] Replica received implicit promise request with proposal 1
I0804 22:16:01.587549 26211 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 358261ns
I0804 22:16:01.587568 26211 replica.cpp:345] Persisted promised to 1
I0804 22:16:01.588173 26209 coordinator.cpp:230] Coordinator attemping to fill missing position
I0804 22:16:01.589316 26208 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0804 22:16:01.589700 26208 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 351778ns
I0804 22:16:01.589721 26208 replica.cpp:679] Persisted action at 0
I0804 22:16:01.590698 26213 replica.cpp:511] Replica received write request for position 0
I0804 22:16:01.590754 26213 leveldb.cpp:438] Reading position from leveldb took 31557ns
I0804 22:16:01.591147 26213 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 321842ns
I0804 22:16:01.591167 26213 replica.cpp:679] Persisted action at 0
I0804 22:16:01.591790 26217 replica.cpp:658] Replica received learned notice for position 0
I0804 22:16:01.592133 26217 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 315281ns
I0804 22:16:01.592155 26217 replica.cpp:679] Persisted action at 0
I0804 22:16:01.592180 26217 replica.cpp:664] Replica learned NOP action at position 0
I0804 22:16:01.592686 26211 log.cpp:677] Writer started with ending position 0
I0804 22:16:01.593729 26205 leveldb.cpp:438] Reading position from leveldb took 26394ns
I0804 22:16:01.596165 26209 registrar.cpp:346] Successfully fetched the registry (0B) in 11.343104ms
I0804 22:16:01.596281 26209 registrar.cpp:445] Applied 1 operations in 26242ns; attempting to update the 'registry'
I0804 22:16:01.598415 26212 log.cpp:685] Attempting to append 178 bytes to the log
I0804 22:16:01.598563 26215 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0804 22:16:01.599324 26215 replica.cpp:511] Replica received write request for position 1
I0804 22:16:01.599778 26215 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 420523ns
I0804 22:16:01.599800 26215 replica.cpp:679] Persisted action at 1
I0804 22:16:01.600349 26204 replica.cpp:658] Replica received learned notice for position 1
I0804 22:16:01.600684 26204 leveldb.cpp:343] Persisting action (199 bytes) to leveldb took 310315ns
I0804 22:16:01.600706 26204 replica.cpp:679] Persisted action at 1
I0804 22:16:01.600723 26204 replica.cpp:664] Replica learned APPEND action at position 1
I0804 22:16:01.601632 26213 registrar.cpp:490] Successfully updated the 'registry' in 5.287936ms
I0804 22:16:01.601747 26213 registrar.cpp:376] Successfully recovered registrar
I0804 22:16:01.601826 26215 log.cpp:704] Attempting to truncate the log to 1
I0804 22:16:01.601948 26210 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0804 22:16:01.602145 26208 master.cpp:1305] Recovered 0 slaves from the Registry (139B) ; allowing 10mins for slaves to re-register
I0804 22:16:01.602859 26219 replica.cpp:511] Replica received write request for position 2
I0804 22:16:01.603181 26219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284713ns
I0804 22:16:01.603209 26219 replica.cpp:679] Persisted action at 2
I0804 22:16:01.603984 26211 replica.cpp:658] Replica received learned notice for position 2
I0804 22:16:01.604313 26211 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 302445ns
I0804 22:16:01.604365 26211 leveldb.cpp:401] Deleting ~1 keys from leveldb took 29354ns
I0804 22:16:01.604387 26211 replica.cpp:679] Persisted action at 2
I0804 22:16:01.604408 26211 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0804 22:16:01.616402 26185 sched.cpp:164] Version: 0.24.0
I0804 22:16:01.616902 26209 sched.cpp:262] New master detected at master@172.17.0.152:59302
I0804 22:16:01.617000 26209 sched.cpp:318] Authenticating with master master@172.17.0.152:59302
I0804 22:16:01.617019 26209 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0804 22:16:01.617324 26212 authenticatee.cpp:115] Creating new client SASL connection
I0804 22:16:01.617550 26209 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.617641 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(259)@172.17.0.152:59302
I0804 22:16:01.617858 26208 authenticator.cpp:92] Creating new server SASL connection
I0804 22:16:01.618140 26216 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0804 22:16:01.618191 26216 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0804 22:16:01.618324 26213 authenticator.cpp:197] Received SASL authentication start
I0804 22:16:01.618413 26213 authenticator.cpp:319] Authentication requires more steps
I0804 22:16:01.618557 26216 authenticatee.cpp:252] Received SASL authentication step
I0804 22:16:01.618664 26216 authenticator.cpp:225] Received SASL authentication step
I0804 22:16:01.618703 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0804 22:16:01.618719 26216 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0804 22:16:01.618778 26216 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0804 22:16:01.618820 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0804 22:16:01.618834 26216 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0804 22:16:01.618839 26216 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0804 22:16:01.618857 26216 authenticator.cpp:311] Authentication success
I0804 22:16:01.618954 26219 authenticatee.cpp:292] Authentication success
I0804 22:16:01.619035 26204 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.619083 26219 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(259)@172.17.0.152:59302
I0804 22:16:01.619309 26208 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302
I0804 22:16:01.619335 26208 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302
I0804 22:16:01.619494 26208 sched.cpp:746] Will retry registration in 439203ns if necessary
I0804 22:16:01.619627 26217 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.619695 26217 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0804 22:16:01.620848 26217 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302
I0804 22:16:01.620929 26217 sched.cpp:746] Will retry registration in 2.099193326secs if necessary
I0804 22:16:01.621036 26210 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.621083 26210 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0804 22:16:01.621727 26217 master.cpp:1876] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0804 22:16:01.621981 26208 sched.cpp:262] New master detected at master@172.17.0.152:59302
I0804 22:16:01.622131 26208 sched.cpp:318] Authenticating with master master@172.17.0.152:59302
I0804 22:16:01.622153 26208 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0804 22:16:01.622323 26212 authenticatee.cpp:115] Creating new client SASL connection
I0804 22:16:01.622324 26210 hierarchical.hpp:391] Added framework 20150804-221601-2550141356-59302-26185-0000
I0804 22:16:01.622369 26210 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:01.622386 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 28592ns
I0804 22:16:01.622511 26210 sched.cpp:640] Framework registered with 20150804-221601-2550141356-59302-26185-0000
I0804 22:16:01.622586 26210 sched.cpp:654] Scheduler::registered took 48005ns
I0804 22:16:01.622592 26208 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.622673 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(260)@172.17.0.152:59302
I0804 22:16:01.622923 26205 authenticator.cpp:92] Creating new server SASL connection
I0804 22:16:01.623112 26204 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0804 22:16:01.623133 26216 master.cpp:1870] Dropping SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302: Re-authentication in progress
I0804 22:16:01.623144 26204 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0804 22:16:01.623258 26215 authenticator.cpp:197] Received SASL authentication start
I0804 22:16:01.623313 26215 authenticator.cpp:319] Authentication requires more steps
I0804 22:16:01.623394 26215 authenticatee.cpp:252] Received SASL authentication step
I0804 22:16:01.623512 26212 authenticator.cpp:225] Received SASL authentication step
I0804 22:16:01.623546 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0804 22:16:01.623564 26212 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0804 22:16:01.623603 26212 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0804 22:16:01.623622 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0804 22:16:01.623631 26212 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0804 22:16:01.623636 26212 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0804 22:16:01.623649 26212 authenticator.cpp:311] Authentication success
I0804 22:16:01.623777 26212 authenticatee.cpp:292] Authentication success
I0804 22:16:01.623846 26212 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.623913 26212 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(260)@172.17.0.152:59302
I0804 22:16:01.624130 26212 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302
I0804 22:16:02.583772 26218 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:02.583818 26218 hierarchical.hpp:908] Performed allocation for 0 slaves in 80538ns
I0804 22:16:03.585110 26211 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:03.585156 26211 hierarchical.hpp:908] Performed allocation for 0 slaves in 69272ns
I0804 22:16:04.586539 26214 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:04.586586 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 79232ns
I0804 22:16:05.587239 26209 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:05.587293 26209 hierarchical.hpp:908] Performed allocation for 0 slaves in 85128ns
I0804 22:16:06.587935 26212 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:06.587985 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 78141ns
I0804 22:16:07.588817 26214 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:07.588865 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 81433ns
I0804 22:16:08.589857 26214 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:08.589906 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 71929ns
I0804 22:16:09.591085 26207 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:09.591133 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 78223ns
I0804 22:16:10.591737 26207 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:10.591785 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71894ns
I0804 22:16:11.593166 26210 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:11.593221 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 89782ns
I0804 22:16:12.593647 26212 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:12.593689 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 69426ns
I0804 22:16:13.594154 26210 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:13.594202 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 70581ns
I0804 22:16:14.594712 26207 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:14.594758 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71201ns
I0804 22:16:15.595412 26219 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:15.595464 26219 hierarchical.hpp:908] Performed allocation for 0 slaves in 85183ns
I0804 22:16:16.596201 26217 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:16.596247 26217 hierarchical.hpp:908] Performed allocation for 0 slaves in 95132ns
../../src/tests/master_authorization_tests.cpp:794: Failure
Failed to wait 15secs for frameworkRegisteredMessage
I0804 22:16:16.624354 26212 master.cpp:966] Framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 disconnected
I0804 22:16:16.624398 26212 master.cpp:2092] Disconnecting framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:16.624445 26212 master.cpp:2116] Deactivating framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:16.624686 26212 master.cpp:988] Giving framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 0ns to failover
I0804 22:16:16.625641 26219 hierarchical.hpp:474] Deactivated framework 20150804-221601-2550141356-59302-26185-0000
I0804 22:16:16.626688 26218 master.cpp:4180] Framework failover timeout, removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:16.626734 26218 master.cpp:4759] Removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:16.627074 26218 master.cpp:858] Master terminating
I0804 22:16:16.627218 26215 hierarchical.hpp:428] Removed framework 20150804-221601-2550141356-59302-26185-0000
../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure
Actual function call count doesn't match EXPECT_CALL(filter-&gt;mock, filter(testing::A<const messageevent&="""">()))...
    Expected args: message matcher (8-byte object &lt;98-98 02-AC 54-2B 00-00&",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-3207,1.0,"C++ style guide is not rendered correctly (code section syntax disregarded) Some paragraphs at the bottom of docs/mesos-c++-style-guide.md containing code sections are not rendered correctly by the web site generator. It looks fine in a github gist and apparently the syntax used is correct. 

",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1
MESOS-3213,2.0,Design doc for docker registry token manager Create design document for describing the component and interaction between Docker Registry Client and remote Docker Registry for token based authorization.,,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3222,5.0,"Implement docker registry client Implement the following functionality:

- fetch manifest from remote registry based on authorization method dictated by the registry.
- fetch image layers from remote registry  based on authorization method dictated by the registry..
",,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3252,2.0,"Ignore no statistics condition for containers with no qdisc In PortMappingStatistics::execute, we log the following errors to stderr if the egress rate limiting qdiscs are not configured inside the container.

","<code>
Failed to get the network statistics for the htb qdisc on eth0
Failed to get the network statistics for the fq_codel qdisc on eth0
<code>

This can occur because of an error reading the qdisc (statistics function return an error) or because the qdisc does not exist (function returns none).  

We should not log an error when the qdisc does not exist since this is normal behaviour if the container is created without rate limiting.  We do not want to gate this function on the slave rate limiting flag since we would have to compare the behaviour against the flag value at the time the container was created.</code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3265,8.0,"Starting maintenance needs to deactivate agents and kill tasks. After using the {{/maintenance/start}} endpoint to begin maintenance on a machine, agents running on said machine should:
* Be deactivated such that no offers are sent from that agent.  (Investigate if {{Master::deactivate(Slave*)}} can be used or modified for this purpose.)
* Kill all tasks still running on the agent (See MESOS-1475).
* Prevent other agents on that machine from registering or sending out offers.  This will likely involve some modifications to {{Master::register}} and {{Master::reregister}}.

",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3266,5.0,"Stopping/Completing maintenance needs to reactivate agents. After using the {{/maintenance/stop}} endpoint to end maintenance on a machine, any deactivated agents must be reactivated and allowed to register with the master.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3273,5.0,"EventCall Test Framework is flaky Observed this on ASF CI. h/t [~haosdent@gmail.com]

Looks like the HTTP scheduler never sent a SUBSCRIBE request to the master.

","<code>
[ RUN      ] ExamplesTest.EventCallFramework
Using temporary directory '/tmp/ExamplesTest_EventCallFramework_k4vXkx'
I0813 19:55:15.643579 26085 exec.cpp:443] Ignoring exited event because the driver is aborted!
Shutting down
Sending SIGTERM to process tree at pid 26061
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26062
Shutting down
Killing the following process trees:
[ 

]
Sending SIGTERM to process tree at pid 26063
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26098
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26099
Killing the following process trees:
[ 

]
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0813 19:55:17.161726 26100 process.cpp:1012] libprocess is initialized on 172.17.2.10:60249 for 16 cpus
I0813 19:55:17.161888 26100 logging.cpp:177] Logging to STDERR
I0813 19:55:17.163625 26100 scheduler.cpp:157] Version: 0.24.0
I0813 19:55:17.175302 26100 leveldb.cpp:176] Opened db in 3.167446ms
I0813 19:55:17.176393 26100 leveldb.cpp:183] Compacted db in 1.047996ms
I0813 19:55:17.176496 26100 leveldb.cpp:198] Created db iterator in 77155ns
I0813 19:55:17.176518 26100 leveldb.cpp:204] Seeked to beginning of db in 8429ns
I0813 19:55:17.176527 26100 leveldb.cpp:273] Iterated through 0 keys in the db in 4219ns
I0813 19:55:17.176708 26100 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0813 19:55:17.178951 26136 recover.cpp:449] Starting replica recovery
I0813 19:55:17.179934 26136 recover.cpp:475] Replica is in EMPTY status
I0813 19:55:17.181970 26126 master.cpp:378] Master 20150813-195517-167907756-60249-26100 (297daca2d01a) started on 172.17.2.10:60249
I0813 19:55:17.182317 26126 master.cpp:380] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --credentials=""/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.24.0/src/webui"" --work_dir=""/tmp/mesos-II8Gua"" --zk_session_timeout=""10secs""
I0813 19:55:17.183475 26126 master.cpp:427] Master allowing unauthenticated frameworks to register
I0813 19:55:17.183536 26126 master.cpp:432] Master allowing unauthenticated slaves to register
I0813 19:55:17.183615 26126 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials'
W0813 19:55:17.183859 26126 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0813 19:55:17.183969 26123 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0813 19:55:17.184306 26126 master.cpp:469] Using default 'crammd5' authenticator
I0813 19:55:17.184661 26126 authenticator.cpp:512] Initializing server SASL
I0813 19:55:17.185104 26138 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0813 19:55:17.185972 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.186058 26135 recover.cpp:566] Updating replica status to STARTING
I0813 19:55:17.187001 26138 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 654586ns
I0813 19:55:17.187037 26138 replica.cpp:323] Persisted replica status to STARTING
I0813 19:55:17.187499 26134 recover.cpp:475] Replica is in STARTING status
I0813 19:55:17.187605 26126 auxprop.cpp:66] Initialized in-memory auxiliary property plugin
I0813 19:55:17.187710 26126 master.cpp:506] Authorization enabled
I0813 19:55:17.188657 26138 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0813 19:55:17.188853 26131 hierarchical.hpp:346] Initialized hierarchical allocator process
I0813 19:55:17.189252 26132 whitelist_watcher.cpp:79] No whitelist given
I0813 19:55:17.189321 26134 recover.cpp:195] Received a recover response from a replica in STARTING status
I0813 19:55:17.190001 26125 recover.cpp:566] Updating replica status to VOTING
I0813 19:55:17.190696 26124 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 357331ns
I0813 19:55:17.190775 26124 replica.cpp:323] Persisted replica status to VOTING
I0813 19:55:17.190970 26133 recover.cpp:580] Successfully joined the Paxos group
I0813 19:55:17.192183 26129 recover.cpp:464] Recover process terminated
I0813 19:55:17.192699 26123 slave.cpp:190] Slave started on 1)@172.17.2.10:60249
I0813 19:55:17.192741 26123 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/0""
I0813 19:55:17.194514 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.194658 26123 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.194854 26123 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.194877 26123 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.196751 26132 master.cpp:1524] The newly elected leader is master@172.17.2.10:60249 with id 20150813-195517-167907756-60249-26100
I0813 19:55:17.196797 26132 master.cpp:1537] Elected as the leading master!
I0813 19:55:17.196815 26132 master.cpp:1307] Recovering from registrar
I0813 19:55:17.197032 26138 registrar.cpp:311] Recovering registrar
I0813 19:55:17.197845 26132 slave.cpp:190] Slave started on 2)@172.17.2.10:60249
I0813 19:55:17.198420 26125 log.cpp:661] Attempting to start the writer
I0813 19:55:17.197948 26132 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/1""
I0813 19:55:17.199121 26132 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.199235 26138 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/0/meta'
I0813 19:55:17.199322 26132 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.199345 26132 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.199676 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.200085 26135 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/1/meta'
I0813 19:55:17.200317 26132 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.200371 26129 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.202003 26129 replica.cpp:477] Replica received implicit promise request with proposal 1
I0813 19:55:17.202585 26131 slave.cpp:190] Slave started on 3)@172.17.2.10:60249
I0813 19:55:17.202596 26129 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 523191ns
I0813 19:55:17.202756 26129 replica.cpp:345] Persisted promised to 1
I0813 19:55:17.202770 26132 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.203061 26135 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.202663 26131 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/2""
I0813 19:55:17.203819 26131 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.203930 26131 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.203948 26131 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.204674 26137 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/2/meta'
I0813 19:55:17.205178 26135 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.205323 26135 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.205521 26136 slave.cpp:4069] Finished recovery
I0813 19:55:17.206074 26136 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.206424 26128 slave.cpp:4069] Finished recovery
I0813 19:55:17.206722 26137 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.206858 26136 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.206902 26138 slave.cpp:4069] Finished recovery
I0813 19:55:17.206962 26128 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.208312 26134 scheduler.cpp:272] New master detected at master@172.17.2.10:60249
I0813 19:55:17.208364 26136 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.208608 26136 slave.cpp:720] Detecting new master
I0813 19:55:17.208839 26138 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.209216 26123 coordinator.cpp:231] Coordinator attemping to fill missing position
I0813 19:55:17.209247 26127 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.209259 26128 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.209322 26127 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.209364 26128 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.209344 26138 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.209455 26128 slave.cpp:720] Detecting new master
I0813 19:55:17.209492 26138 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.209573 26128 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.209601 26138 slave.cpp:720] Detecting new master
I0813 19:55:17.209730 26138 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.209883 26136 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.211266 26136 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0813 19:55:17.211771 26136 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 462128ns
I0813 19:55:17.211797 26136 replica.cpp:679] Persisted action at 0
I0813 19:55:17.212980 26130 replica.cpp:511] Replica received write request for position 0
I0813 19:55:17.213124 26130 leveldb.cpp:438] Reading position from leveldb took 67075ns
I0813 19:55:17.213580 26130 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 301649ns
I0813 19:55:17.213603 26130 replica.cpp:679] Persisted action at 0
I0813 19:55:17.214284 26123 replica.cpp:658] Replica received learned notice for position 0
I0813 19:55:17.214622 26123 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284547ns
I0813 19:55:17.214648 26123 replica.cpp:679] Persisted action at 0
I0813 19:55:17.214675 26123 replica.cpp:664] Replica learned NOP action at position 0
I0813 19:55:17.215420 26136 log.cpp:677] Writer started with ending position 0
I0813 19:55:17.217463 26133 leveldb.cpp:438] Reading position from leveldb took 47943ns
I0813 19:55:17.220762 26125 registrar.cpp:344] Successfully fetched the registry (0B) in 23.649024ms
I0813 19:55:17.221081 26125 registrar.cpp:443] Applied 1 operations in 136902ns; attempting to update the 'registry'
I0813 19:55:17.223667 26133 log.cpp:685] Attempting to append 174 bytes to the log
I0813 19:55:17.223778 26125 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0813 19:55:17.224516 26127 replica.cpp:511] Replica received write request for position 1
I0813 19:55:17.225009 26127 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 466230ns
I0813 19:55:17.225042 26127 replica.cpp:679] Persisted action at 1
I0813 19:55:17.225653 26126 replica.cpp:658] Replica received learned notice for position 1
I0813 19:55:17.225953 26126 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 286966ns
I0813 19:55:17.225975 26126 replica.cpp:679] Persisted action at 1
I0813 19:55:17.226013 26126 replica.cpp:664] Replica learned APPEND action at position 1
I0813 19:55:17.227545 26137 registrar.cpp:488] Successfully updated the 'registry' in 6.328064ms
I0813 19:55:17.227722 26137 registrar.cpp:374] Successfully recovered registrar
I0813 19:55:17.227918 26124 log.cpp:704] Attempting to truncate the log to 1
I0813 19:55:17.228024 26133 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0813 19:55:17.228193 26131 master.cpp:1334] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register
I0813 19:55:17.228659 26127 replica.cpp:511] Replica received write request for position 2
I0813 19:55:17.228972 26127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 297903ns
I0813 19:55:17.229004 26127 replica.cpp:679] Persisted action at 2
I0813 19:55:17.229565 26127 replica.cpp:658] Replica received learned notice for position 2
I0813 19:55:17.229837 26127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260326ns
I0813 19:55:17.229899 26127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 48697ns
I0813 19:55:17.229923 26127 replica.cpp:679] Persisted action at 2
I0813 19:55:17.229956 26127 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0813 19:55:17.325634 26138 slave.cpp:1209] Will retry registration in 445.955946ms if necessary
I0813 19:55:17.326088 26124 master.cpp:3635] Registering slave at slave(2)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S0
I0813 19:55:17.327446 26124 registrar.cpp:443] Applied 1 operations in 231072ns; attempting to update the 'registry'
I0813 19:55:17.330252 26136 log.cpp:685] Attempting to append 344 bytes to the log
I0813 19:55:17.330407 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0813 19:55:17.331418 26128 replica.cpp:511] Replica received write request for position 3
I0813 19:55:17.331753 26128 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 264140ns
I0813 19:55:17.331778 26128 replica.cpp:679] Persisted action at 3
I0813 19:55:17.332324 26133 replica.cpp:658] Replica received learned notice for position 3
I0813 19:55:17.332809 26133 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 313064ns
I0813 19:55:17.332834 26133 replica.cpp:679] Persisted action at 3
I0813 19:55:17.332865 26133 replica.cpp:664] Replica learned APPEND action at position 3
I0813 19:55:17.334211 26132 registrar.cpp:488] Successfully updated the 'registry' in 6.668032ms
I0813 19:55:17.334430 26127 log.cpp:704] Attempting to truncate the log to 3
I0813 19:55:17.334566 26132 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0813 19:55:17.335283 26129 replica.cpp:511] Replica received write request for position 4
I0813 19:55:17.335615 26127 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249
I0813 19:55:17.335816 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 458268ns
I0813 19:55:17.335908 26137 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.335983 26129 replica.cpp:679] Persisted action at 4
I0813 19:55:17.336019 26136 slave.c",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3280,8.0,"Master fails to access replicated log after network partition In a 5 node cluster with 3 masters and 2 slaves, and ZK on each node, when a network partition is forced, all the masters apparently lose access to their replicated log. The leading master halts. Unknown reasons, but presumably related to replicated log access. The others fail to recover from the replicated log. Unknown reasons. This could have to do with ZK setup, but it might also be a Mesos bug. 

This was observed in a Chronos test drive scenario described in detail here:
https://github.com/mesos/chronos/issues/511

With setup instructions here:
https://github.com/mesos/chronos/issues/508

",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0
MESOS-3284,3.0,"JSON representation of Protobuf should use base64 encoding for 'bytes' fields. Currently we encode 'bytes' fields as UTF-8 strings, which is lossy for binary data due to invalid byte sequences! In order to encode binary data in a lossless fashion, we can encode 'bytes' fields in base64.

Note that this is also how proto3 does its encoding (see [here|https://developers.google.com/protocol-buffers/docs/proto3?hl=en#json]), so this would make migration easier as well.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3288,5.0,"Implement docker registry client Implement the docker registry client as per design document:

https://docs.google.com/document/d/1kE-HXPQl4lQgamPIiaD4Ytdr-N4HeQc4fnE93WHR4X4/edit",,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3289,5.0,"Add DockerRegistry unit tests Add unit tests suite for docker registry implementation.  This could include:

- Creating mock docker registry server
- Using openssl library for digest functions.",,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3311,2.0,"SlaveTest.HTTPSchedulerSlaveRestart Observed on ASF CI

","<code>
[ RUN      ] SlaveTest.HTTPSchedulerSlaveRestart
Using temporary directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA'
I0825 22:07:36.809872 27610 leveldb.cpp:176] Opened db in 3.751801ms
I0825 22:07:36.811115 27610 leveldb.cpp:183] Compacted db in 1.2194ms
I0825 22:07:36.811175 27610 leveldb.cpp:198] Created db iterator in 30669ns
I0825 22:07:36.811197 27610 leveldb.cpp:204] Seeked to beginning of db in 7829ns
I0825 22:07:36.811208 27610 leveldb.cpp:273] Iterated through 0 keys in the db in 6017ns
I0825 22:07:36.811245 27610 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0825 22:07:36.811722 27638 recover.cpp:449] Starting replica recovery
I0825 22:07:36.811980 27638 recover.cpp:475] Replica is in EMPTY status
I0825 22:07:36.813033 27641 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0825 22:07:36.813355 27635 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0825 22:07:36.813756 27628 recover.cpp:566] Updating replica status to STARTING
I0825 22:07:36.814434 27636 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 570160ns
I0825 22:07:36.814471 27636 replica.cpp:323] Persisted replica status to STARTING
I0825 22:07:36.814743 27642 recover.cpp:475] Replica is in STARTING status
I0825 22:07:36.814965 27638 master.cpp:378] Master 20150825-220736-234885548-51219-27610 (09c6504e3a31) started on 172.17.0.14:51219
I0825 22:07:36.814999 27638 master.cpp:380] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.25.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/master"" --zk_session_timeout=""10secs""
I0825 22:07:36.815347 27638 master.cpp:425] Master only allowing authenticated frameworks to register
I0825 22:07:36.815371 27638 master.cpp:430] Master only allowing authenticated slaves to register
I0825 22:07:36.815402 27638 credentials.hpp:37] Loading credentials for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/credentials'
I0825 22:07:36.815634 27632 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0825 22:07:36.815752 27638 master.cpp:469] Using default 'crammd5' authenticator
I0825 22:07:36.815904 27638 master.cpp:506] Authorization enabled
I0825 22:07:36.815979 27643 recover.cpp:195] Received a recover response from a replica in STARTING status
I0825 22:07:36.816185 27637 whitelist_watcher.cpp:79] No whitelist given
I0825 22:07:36.816186 27641 hierarchical.hpp:346] Initialized hierarchical allocator process
I0825 22:07:36.816519 27630 recover.cpp:566] Updating replica status to VOTING
I0825 22:07:36.817258 27639 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 475231ns
I0825 22:07:36.817296 27639 replica.cpp:323] Persisted replica status to VOTING
I0825 22:07:36.817420 27637 master.cpp:1525] The newly elected leader is master@172.17.0.14:51219 with id 20150825-220736-234885548-51219-27610
I0825 22:07:36.817467 27637 master.cpp:1538] Elected as the leading master!
I0825 22:07:36.817483 27637 master.cpp:1308] Recovering from registrar
I0825 22:07:36.817509 27635 recover.cpp:580] Successfully joined the Paxos group
I0825 22:07:36.817708 27633 registrar.cpp:311] Recovering registrar
I0825 22:07:36.817844 27635 recover.cpp:464] Recover process terminated
I0825 22:07:36.818439 27631 log.cpp:661] Attempting to start the writer
I0825 22:07:36.819694 27636 replica.cpp:477] Replica received implicit promise request with proposal 1
I0825 22:07:36.820133 27636 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421255ns
I0825 22:07:36.820168 27636 replica.cpp:345] Persisted promised to 1
I0825 22:07:36.820804 27630 coordinator.cpp:231] Coordinator attemping to fill missing position
I0825 22:07:36.822105 27638 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0825 22:07:36.822597 27638 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 468065ns
I0825 22:07:36.822625 27638 replica.cpp:679] Persisted action at 0
I0825 22:07:36.823737 27637 replica.cpp:511] Replica received write request for position 0
I0825 22:07:36.823796 27637 leveldb.cpp:438] Reading position from leveldb took 39603ns
I0825 22:07:36.824267 27637 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 446655ns
I0825 22:07:36.824296 27637 replica.cpp:679] Persisted action at 0
I0825 22:07:36.824961 27634 replica.cpp:658] Replica received learned notice for position 0
I0825 22:07:36.825340 27634 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 362236ns
I0825 22:07:36.825369 27634 replica.cpp:679] Persisted action at 0
I0825 22:07:36.825388 27634 replica.cpp:664] Replica learned NOP action at position 0
I0825 22:07:36.825975 27642 log.cpp:677] Writer started with ending position 0
I0825 22:07:36.826997 27628 leveldb.cpp:438] Reading position from leveldb took 56us
I0825 22:07:36.829946 27639 registrar.cpp:344] Successfully fetched the registry (0B) in 12.187136ms
I0825 22:07:36.830077 27639 registrar.cpp:443] Applied 1 operations in 40874ns; attempting to update the 'registry'
I0825 22:07:36.832870 27635 log.cpp:685] Attempting to append 174 bytes to the log
I0825 22:07:36.833088 27641 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0825 22:07:36.833845 27636 replica.cpp:511] Replica received write request for position 1
I0825 22:07:36.834293 27636 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 425175ns
I0825 22:07:36.834324 27636 replica.cpp:679] Persisted action at 1
I0825 22:07:36.835077 27643 replica.cpp:658] Replica received learned notice for position 1
I0825 22:07:36.835500 27643 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 404831ns
I0825 22:07:36.835532 27643 replica.cpp:679] Persisted action at 1
I0825 22:07:36.835574 27643 replica.cpp:664] Replica learned APPEND action at position 1
I0825 22:07:36.836545 27643 registrar.cpp:488] Successfully updated the 'registry' in 6.393088ms
I0825 22:07:36.836707 27643 registrar.cpp:374] Successfully recovered registrar
I0825 22:07:36.836874 27639 log.cpp:704] Attempting to truncate the log to 1
I0825 22:07:36.837174 27632 master.cpp:1335] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register
I0825 22:07:36.837291 27634 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0825 22:07:36.838249 27639 replica.cpp:511] Replica received write request for position 2
I0825 22:07:36.838685 27639 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 412214ns
I0825 22:07:36.838716 27639 replica.cpp:679] Persisted action at 2
I0825 22:07:36.839735 27628 replica.cpp:658] Replica received learned notice for position 2
I0825 22:07:36.840304 27628 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 547841ns
I0825 22:07:36.840375 27628 leveldb.cpp:401] Deleting ~1 keys from leveldb took 51256ns
I0825 22:07:36.840401 27628 replica.cpp:679] Persisted action at 2
I0825 22:07:36.840428 27628 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0825 22:07:36.849371 27610 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0825 22:07:36.856500 27633 slave.cpp:190] Slave started on 286)@172.17.0.14:51219
I0825 22:07:36.856541 27633 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L""
I0825 22:07:36.857074 27633 credentials.hpp:85] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential'
I0825 22:07:36.857275 27633 slave.cpp:321] Slave using credential for: test-principal
I0825 22:07:36.857822 27633 slave.cpp:354] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0825 22:07:36.857936 27633 slave.cpp:384] Slave hostname: 09c6504e3a31
I0825 22:07:36.857959 27633 slave.cpp:389] Slave checkpoint: true
I0825 22:07:36.858886 27637 state.cpp:54] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta'
I0825 22:07:36.859130 27638 status_update_manager.cpp:202] Recovering status update manager
I0825 22:07:36.859465 27636 containerizer.cpp:379] Recovering containerizer
I0825 22:07:36.860631 27634 slave.cpp:4069] Finished recovery
I0825 22:07:36.861034 27634 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0825 22:07:36.861239 27643 status_update_manager.cpp:176] Pausing sending status updates
I0825 22:07:36.861240 27634 slave.cpp:684] New master detected at master@172.17.0.14:51219
I0825 22:07:36.861322 27634 slave.cpp:747] Authenticating with master master@172.17.0.14:51219
I0825 22:07:36.861343 27634 slave.cpp:752] Using default CRAM-MD5 authenticatee
I0825 22:07:36.861450 27634 slave.cpp:720] Detecting new master
I0825 22:07:36.861495 27628 authenticatee.cpp:115] Creating new client SASL connection
I0825 22:07:36.861569 27634 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0825 22:07:36.861716 27632 master.cpp:4694] Authenticating slave(286)@172.17.0.14:51219
I0825 22:07:36.861799 27629 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(665)@172.17.0.14:51219
I0825 22:07:36.862045 27642 authenticator.cpp:92] Creating new server SASL connection
I0825 22:07:36.862308 27635 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0825 22:07:36.862337 27635 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0825 22:07:36.862421 27629 authenticator.cpp:197] Received SASL authentication start
I0825 22:07:36.862478 27629 authenticator.cpp:319] Authentication requires more steps
I0825 22:07:36.862579 27633 authenticatee.cpp:252] Received SASL authentication step
I0825 22:07:36.862679 27628 authenticator.cpp:225] Received SASL authentication step
I0825 22:07:36.862707 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0825 22:07:36.862717 27628 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0825 22:07:36.862754 27628 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0825 22:07:36.862785 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0825 22:07:36.862797 27628 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.862802 27628 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.862817 27628 authenticator.cpp:311] Authentication success
I0825 22:07:36.862884 27629 authenticatee.cpp:292] Authentication success
I0825 22:07:36.862921 27630 master.cpp:4724] Successfully authenticated principal 'test-principal' at slave(286)@172.17.0.14:51219
I0825 22:07:36.862969 27642 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(665)@172.17.0.14:51219
I0825 22:07:36.863139 27639 slave.cpp:815] Successfully authenticated with master master@172.17.0.14:51219
I0825 22:07:36.863256 27639 slave.cpp:1209] Will retry registration in 15.028678ms if necessary
I0825 22:07:36.863382 27643 master.cpp:3636] Registering slave at slave(286)@172.17.0.14:51219 (09c6504e3a31) with id 20150825-220736-234885548-51219-27610-S0
I0825 22:07:36.863899 27610 sched.cpp:164] Version: 0.25.0
I0825 22:07:36.863940 27636 registrar.cpp:443] Applied 1 operations in 94492ns; attempting to update the 'registry'
I0825 22:07:36.864670 27632 sched.cpp:262] New master detected at master@172.17.0.14:51219
I0825 22:07:36.864790 27632 sched.cpp:318] Authenticating with master master@172.17.0.14:51219
I0825 22:07:36.864821 27632 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0825 22:07:36.865095 27637 authenticatee.cpp:115] Creating new client SASL connection
I0825 22:07:36.865453 27643 master.cpp:4694] Authenticating scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.865603 27629 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(666)@172.17.0.14:51219
I0825 22:07:36.865840 27638 authenticator.cpp:92] Creating new server SASL connection
I0825 22:07:36.866217 27630 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0825 22:07:36.866260 27630 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0825 22:07:36.866433 27639 authenticator.cpp:197] Received SASL authentication start
I0825 22:07:36.866513 27639 authenticator.cpp:319] Authentication requires more steps
I0825 22:07:36.866710 27630 authenticatee.cpp:252] Received SASL authentication step
I0825 22:07:36.866999 27638 authenticator.cpp:225] Received SASL authentication step
I0825 22:07:36.867051 27638 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0825 22:07:36.867077 27638 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0825 22:07:36.867130 27638 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0825 22:07:36.867162 27638 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0825 22:07:36.867175 27638 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.867183 27638 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.867202 27638 authenticator.cpp:311] Authentication success
I0825 22:07:36.867426 27636 authenticatee.cpp:292] Authentication success
I0825 22:07:36.867434 27633 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(666)@172.17.0.14:51219
I0825 22:07:36.867627 27630 master.cpp:4724] Successfully authenticated principal 'test-principal' at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.867951 27641 sched.cpp:407] Successfully authenticated with master master@172.17.0.14:51219
I0825 22:07:36.867986 27641 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.14:51219
I0825 22:07:36.868114 27641 sched.cpp:746] Will retry registration in 1.352726078secs if necessary
I0825 22:07:36.868233 27634 log.cpp:685] Attempting to append 344 bytes to the log
I0825 22:07:36.868268 27638 master.cpp:2094] Received SUBSCRIBE call for framework 'default' at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.868305 27638 master.cpp:1564] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0825 22:07:36.868373 27631 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0825 22:07:36.868614 27642 master.cpp:2164] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0825 22:07:36.868999 27643 hierarchical.hpp:391] Added framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.869030 27643 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:36.869046 27643 hierarchical.hpp:910] Performed allocation for 0 slaves in 34654ns
I0825 22:07:36.869215 27631 sched.cpp:640] Framework registered with 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.869215 27643 replica.cpp:511] Replica received write request for position 3
I0825 22:07:36.869268 27631 sched.cpp:654] Scheduler::registered took 29976ns
I0825 22:07:36.869453 27643 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 181689ns
I0825 22:07:36.869477 27643 replica.cpp:679] Persisted action at 3
I0825 22:07:36.870075 27629 replica.cpp:658] Replica received learned notice for position 3
I0825 22:07:36.870542 27629 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 469081ns
I0825 22:07:36.870589 27629 replica.cpp:679] Persisted action at 3
I0825 22:07:36.870622 27629 replica.cpp:664] Replica learned APPEND action at position 3
I0825 22:07:36.872133 27632 registrar.cpp:488] Successfully updated the 'registry' in 8.113152ms
I0825 22:07:36.872354 27639 log.cpp:704] Attempting to truncate the log to 3
I0825 22:07:36.872470 27632 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0825 22:07:36.872879 27637 slave.cpp:3058] Received ping from slave-observer(274)@172.17.0.14:51219
I0825 22:07:36.873015 27636 master.cpp:3699] Registered slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0825 22:07:36.873180 27637 slave.cpp:859] Registered with master master@172.17.0.14:51219; given slave ID 20150825-220736-234885548-51219-27610-S0
I0825 22:07:36.873219 27637 fetcher.cpp:77] Clearing fetcher cache
I0825 22:07:36.873410 27634 status_update_manager.cpp:183] Resuming sending status updates
I0825 22:07:36.873379 27628 hierarchical.hpp:542] Added slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0825 22:07:36.873482 27642 replica.cpp:511] Replica received write request for position 4
I0825 22:07:36.873661 27637 s",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-3321,1.0,"Spurious fetcher message about extracting an archive The fetcher emits a spurious log message about not extracting an archive with "".tgz"" extension, even though the tarball is extracted correctly.

","<code>
I0826 19:02:08.304914  2109 logging.cpp:172] INFO level logging started!
I0826 19:02:08.305253  2109 fetcher.cpp:413] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/20150826-185716-251662764-5050-1-S0\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""file:\/\/\/mesos\/sampleflaskapp.tgz""}}],""sandbox_directory"":""\/tmp\/mesos\/slaves\/20150826-185716-251662764-5050-1-S0\/frameworks\/20150826-185716-251662764-5050-1-0000\/executors\/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011\/runs\/e71f50b8-816d-46d5-bcc6-f9850a0402ed"",""user"":""root""}
I0826 19:02:08.306834  2109 fetcher.cpp:368] Fetching URI 'file:///mesos/sampleflaskapp.tgz'
I0826 19:02:08.306864  2109 fetcher.cpp:242] Fetching directly into the sandbox directory
I0826 19:02:08.306884  2109 fetcher.cpp:179] Fetching URI 'file:///mesos/sampleflaskapp.tgz'
I0826 19:02:08.306900  2109 fetcher.cpp:159] Copying resource with command:cp '/mesos/sampleflaskapp.tgz' '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
I0826 19:02:08.309063  2109 fetcher.cpp:76] Extracting with command: tar -C '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed' -xf '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
I0826 19:02:08.315313  2109 fetcher.cpp:84] Extracted '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz' into '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed'
W0826 19:02:08.315381  2109 fetcher.cpp:264] Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: file:///mesos/sampleflaskapp.tgz
I0826 19:02:08.315604  2109 fetcher.cpp:445] Fetched 'file:///mesos/sampleflaskapp.tgz' to '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
<code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3340,2.0,"Command-line flags should take precedence over OS Env variables Currently, it appears that re-defining a flag on the command-line that was already defined via a OS Env var ({{MESOS_*}}) causes the Master to fail with a not very helpful message.

For example, if one has {{MESOS_QUORUM}} defined, this happens:
{noformat}
$ ./mesos-master --zk=zk://192.168.1.4/mesos --quorum=1 --hostname=192.168.1.4 --ip=192.168.1.4
Duplicate flag 'quorum' on command line
{noformat}

which is not very helpful.

Ideally, we would parse the flags with a ""well-known"" priority (command-line first, environment last) - but at the very least, the error message should be more helpful in explaining what the issue is.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3366,3.0,"Allow resources/attributes discovery In heterogeneous clusters, tasks sometimes have strong constraints on the type of hardware they need to execute on. The current solution is to use custom resources and attributes on the agents. Detecting non-standard resources/attributes requires wrapping the ""mesos-slave"" binary behind a script and use custom code to probe the agent. Unfortunately, this approach doesn't allow composition. The solution would be to provide a hook/module mechanism to allow users to use custom code performing resources/attributes discovery.

Please review the detailed document below:
https://docs.google.com/document/d/15OkebDezFxzeyLsyQoU0upB0eoVECAlzEkeg0HQAX9w

Feel free to express comments/concerns by annotating the document or by replying to this issue.
",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3375,1.0,"Add executor protobuf to v1 A new protobuf for Executor was introduced in Mesos for the HTTP API, it needs to be added to /v1 so it reflects changes made on v1/mesos.proto. This protobuf is ought to be changed as the executor HTTP API design evolves.",,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3378,3.0,Document a test pattern for expediting event firing We use {{Clock::advance()}} extensively in tests to expedite event firing and minimize overall {{make check}} time. Document this pattern for posterity.,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-3393,1.0,"Remove unused executor protobuf The executor protobuf definition living outside the v1/ directory is unused, it should  be removed to avoid confusion.",,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3413,5.0,"Docker containerizer does not symlink persistent volumes into sandbox For the ArangoDB framework I am trying to use the persistent primitives. nearly all is working, but I am missing a crucial piece at the end: I have successfully created a persistent disk resource and have set the persistence and volume information in the DiskInfo message. However, I do not see any way to find out what directory on the host the mesos slave has reserved for us. I know it is ${MESOS_SLAVE_WORKDIR}/volumes/roles/<myrole>/<name>_<uuid> but we have no way to query this information anywhere. The docker containerizer does not automatically mount this directory into our docker container, or symlinks it into our sandbox. Therefore, I have essentially no access to it. Note that the mesos containerizer (which I cannot use for other reasons) seems to create a symlink in the sandbox to the actual path for the persistent volume. With that, I could mount the volume into our docker container and all would be well.</uuid></name></myrole>",,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3417,2.0,"Log source address replicated log recieved broadcasts Currently Mesos doesn't log what machine a replicated log status broadcast was recieved from:
","<code>
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.320164 15637 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
Sep 11 21:41:14 master-01 mesos-dns[15583]: I0911 21:41:14.321097   15583 detect.go:118] ignoring children-changed event, leader has not changed: /mesos
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.353914 15639 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.479132 15639 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
<code>

It would be really useful for debugging replicated log startup issues to have info about where the message came from (libprocess address, ip, or hostname) the message came from</code></code>",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
MESOS-3426,3.0,"process::collect and process::await do not perform discard propagation. When aggregating futures with collect, one may discard the outer future:

","<code>
Promise<int> p1;
Promise<string> p2;

Future<int, string=""""> collect = process::collect(p1.future(), p2.future());

collect.discard();

// collect will transition to DISCARDED

// However, p{1,2}.future().hasDiscard() remains false
// as there is no discard propagation!
<code>

Discard requests should propagate down into the inner futures being collected.</code></int,></string></int></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3458,1.0,"Segfault when accepting or declining inverse offers Discovered while writing a test for filters (in regards to inverse offers).

Fix here: https://reviews.apache.org/r/38470/",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3485,3.0,"Make hook execution order deterministic Currently, when using multiple hooks of the same type, the execution order is implementation-defined. 

This is because in src/hook/manager.cpp, the list of available hooks is stored in a {{hashmap<string, hook*="""">}}. A hashmap is probably unnecessary for this task since the number of hooks should remain reasonable. A data structure preserving ordering should be used instead to allow the user to predict the execution order of the hooks. I suggest that the execution order should be the order in which hooks are specified with {{--hooks}} when starting an agent/master.

This will be useful when combining multiple hooks after MESOS-3366 is done.</string,>",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3492,1.0,"Expose maintenance user doc via the documentation home page The committed docs can be found here:
http://mesos.apache.org/documentation/latest/maintenance/

We need to add a link to {{docs/home.md}}
Also, the doc needs some minor formatting tweaks.",,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3496,2.0,"Create interface for digest verifier Add interface for digest verifier so that we can add implementations for digest types like sha256, sha512 etc",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3513,1.0,"Cgroups Test Filters aborts tests on Centos 6.6  Running make check on centos 6.6 causes all tests to abort due to CHECK_SOME test in CgroupsFIlter:

","<code>
Build directory: /home/jenkins/workspace/mesos-config-centos6/build
F0923 23:00:49.748896 27362 environment.cpp:132] CHECK_SOME(hierarchies_): Failed to determine canonical path of /sys/fs/cgroup/freezer: No such file or directory 
*** Check failure stack trace: ***
    @     0x7fb786ca0c4d  google::LogMessage::Fail()
    @     0x7fb786ca298c  google::LogMessage::SendToLog()
    @     0x7fb786ca083c  google::LogMessage::Flush()
    @     0x7fb786ca3289  google::LogMessageFatal::~LogMessageFatal()
    @           0x58e66c  mesos::internal::tests::CgroupsFilter::CgroupsFilter()
    @           0x58712f  mesos::internal::tests::Environment::Environment()
    @           0x4c882f  main
    @     0x7fb782767d5d  __libc_start_main
    @           0x4d6331  (unknown)
make[3]: *** [check-local] Aborted
<code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0
MESOS-3525,3.0,"Figure out how to enforce 64-bit builds on Windows. We need to make sure people don't try to compile Mesos on 32-bit architectures. We don't want a Windows repeat of something like this:

https://issues.apache.org/jira/browse/MESOS-267",,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3540,2.0,"Libevent termination triggers Broken Pipe When the libevent loop terminates and we unblock the {{SIGPIPE}} signal, the pending {{SIGPIPE}} instantly triggers and causes a broken pipe when the test binary stops running.
","<code>
Program received signal SIGPIPE, Broken pipe.
[Switching to Thread 0x7ffff18b4700 (LWP 16270)]
pthread_sigmask (how=1, newmask=<optimized out="""">, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthread_sigmask.c:53
53	../sysdeps/unix/sysv/linux/pthread_sigmask.c: No such file or directory.
(gdb) bt
#0  pthread_sigmask (how=1, newmask=<optimized out="""">, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthread_sigmask.c:53
#1  0x00000000006fd9a4 in unblock () at ../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/signals.hpp:90
#2  0x00000000007d7915 in run () at ../../../3rdparty/libprocess/src/libevent.cpp:125
#3  0x00000000007950cb in _M_invoke&lt;&gt;(void) () at /usr/include/c++/4.9/functional:1700
#4  0x0000000000795000 in operator() () at /usr/include/c++/4.9/functional:1688
#5  0x0000000000794f6e in _M_run () at /usr/include/c++/4.9/thread:115
#6  0x00007ffff668de30 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#7  0x00007ffff79a16aa in start_thread (arg=0x7ffff18b4700) at pthread_create.c:333
#8  0x00007ffff5df1eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
<code></code></optimized></optimized></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3551,3.0,"Replace use of strerror with thread-safe alternatives strerror_r / strerror_l. {{strerror()}} is not required to be thread safe by POSIX and is listed as unsafe on Linux:

http://pubs.opengroup.org/onlinepubs/9699919799/
http://man7.org/linux/man-pages/man3/strerror.3.html

I don't believe we've seen any issues reported due to this. We should replace occurrences of strerror accordingly, possibly offering a wrapper in stout to simplify callsites.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3554,3.0,"Allocator changes trigger large re-compiles Due to the templatized nature of the allocator, even small changes trigger large recompiles of the code-base. This make iterating on changes expensive for developers.",,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3571,5.0,"Refactor registry_client Refactor registry client component to:

- Make methods shorter for readability
- Pull out structs not related to registry client into common namespace.",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3573,5.0,"Mesos does not kill orphaned docker containers After upgrade to 0.24.0 we noticed hanging containers appearing. Looks like there were changes between 0.23.0 and 0.24.0 that broke cleanup.

Here's how to trigger this bug:

1. Deploy app in docker container.
2. Kill corresponding mesos-docker-executor process
3. Observe hanging container

Here are the logs after kill:

{noformat}
slave_1    | I1002 12:12:59.362002  7791 docker.cpp:1576] Executor for container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8' has exited
slave_1    | I1002 12:12:59.362284  7791 docker.cpp:1374] Destroying container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8'
slave_1    | I1002 12:12:59.363404  7791 docker.cpp:1478] Running docker stop on container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8'
slave_1    | I1002 12:12:59.363876  7791 slave.cpp:3399] Executor 'sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c' of framework 20150923-122130-2153451692-5050-1-0000 terminated with signal Terminated
slave_1    | I1002 12:12:59.367570  7791 slave.cpp:2696] Handling status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 from @0.0.0.0:0
slave_1    | I1002 12:12:59.367842  7791 slave.cpp:5094] Terminating task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c
slave_1    | W1002 12:12:59.368484  7791 docker.cpp:986] Ignoring updating unknown container: f083aaa2-d5c3-43c1-b6ba-342de8829fa8
slave_1    | I1002 12:12:59.368671  7791 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000
slave_1    | I1002 12:12:59.368741  7791 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000
slave_1    | I1002 12:12:59.370636  7791 status_update_manager.cpp:376] Forwarding update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to the slave
slave_1    | I1002 12:12:59.371335  7791 slave.cpp:2975] Forwarding the update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to master@172.16.91.128:5050
slave_1    | I1002 12:12:59.371908  7791 slave.cpp:2899] Status update manager successfully handled status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000
master_1   | I1002 12:12:59.372047    11 master.cpp:4069] Status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 from slave 20151002-120829-2153451692-5050-1-S0 at slave(1)@172.16.91.128:5051 (172.16.91.128)
master_1   | I1002 12:12:59.372534    11 master.cpp:4108] Forwarding status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000
master_1   | I1002 12:12:59.373018    11 master.cpp:5576] Updating the latest state of task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to TASK_FAILED
master_1   | I1002 12:12:59.373447    11 hierarchical.hpp:814] Recovered cpus(*):0.1; mem(*):16; ports(*):[31685-31685] (total: cpus(*):4; mem(*):1001; disk(*):52869; ports(*):[31000-32000], allocated: cpus(*):8.32667e-17) on slave 20151002-120829-2153451692-5050-1-S0 from framework 20150923-122130-2153451692-5050-1-0000
{noformat}

Another issue: if you restart mesos-slave on the host with orphaned docker containers, they are not getting killed. This was the case before and I hoped for this trick to kill hanging containers, but it doesn't work now.

Marking this as critical because it hoards cluster resources and blocks scheduling.",,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3579,2.0,"FetcherCacheTest.LocalUncachedExtract is flaky From ASF CI:
https://builds.apache.org/job/Mesos/866/COMPILER=clang,CONFIGURATION=--verbose,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/console

","<code>
[ RUN      ] FetcherCacheTest.LocalUncachedExtract
Using temporary directory '/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA'
I0925 19:15:39.541198 27410 leveldb.cpp:176] Opened db in 3.43934ms
I0925 19:15:39.542362 27410 leveldb.cpp:183] Compacted db in 1.136184ms
I0925 19:15:39.542428 27410 leveldb.cpp:198] Created db iterator in 35866ns
I0925 19:15:39.542448 27410 leveldb.cpp:204] Seeked to beginning of db in 8807ns
I0925 19:15:39.542459 27410 leveldb.cpp:273] Iterated through 0 keys in the db in 6325ns
I0925 19:15:39.542505 27410 replica.cpp:744] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0925 19:15:39.543143 27438 recover.cpp:449] Starting replica recovery
I0925 19:15:39.543393 27438 recover.cpp:475] Replica is in EMPTY status
I0925 19:15:39.544373 27436 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0925 19:15:39.544791 27433 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0925 19:15:39.545284 27433 recover.cpp:566] Updating replica status to STARTING
I0925 19:15:39.546155 27436 master.cpp:376] Master c8bf1c95-50f4-4832-a570-c560f0b466ae (f57fd4291168) started on 172.17.1.195:41781
I0925 19:15:39.546257 27433 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 747249ns
I0925 19:15:39.546288 27433 replica.cpp:323] Persisted replica status to STARTING
I0925 19:15:39.546483 27434 recover.cpp:475] Replica is in STARTING status
I0925 19:15:39.546187 27436 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/master"" --zk_session_timeout=""10secs""
I0925 19:15:39.546567 27436 master.cpp:423] Master only allowing authenticated frameworks to register
I0925 19:15:39.546617 27436 master.cpp:428] Master only allowing authenticated slaves to register
I0925 19:15:39.546632 27436 credentials.hpp:37] Loading credentials for authentication from '/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/credentials'
I0925 19:15:39.546931 27436 master.cpp:467] Using default 'crammd5' authenticator
I0925 19:15:39.547044 27436 master.cpp:504] Authorization enabled
I0925 19:15:39.547276 27441 whitelist_watcher.cpp:79] No whitelist given
I0925 19:15:39.547320 27434 hierarchical.hpp:468] Initialized hierarchical allocator process
I0925 19:15:39.547471 27438 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0925 19:15:39.548318 27443 recover.cpp:195] Received a recover response from a replica in STARTING status
I0925 19:15:39.549067 27435 recover.cpp:566] Updating replica status to VOTING
I0925 19:15:39.549115 27440 master.cpp:1603] The newly elected leader is master@172.17.1.195:41781 with id c8bf1c95-50f4-4832-a570-c560f0b466ae
I0925 19:15:39.549162 27440 master.cpp:1616] Elected as the leading master!
I0925 19:15:39.549190 27440 master.cpp:1376] Recovering from registrar
I0925 19:15:39.549342 27434 registrar.cpp:309] Recovering registrar
I0925 19:15:39.549666 27430 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418187ns
I0925 19:15:39.549753 27430 replica.cpp:323] Persisted replica status to VOTING
I0925 19:15:39.550089 27442 recover.cpp:580] Successfully joined the Paxos group
I0925 19:15:39.550320 27442 recover.cpp:464] Recover process terminated
I0925 19:15:39.550904 27430 log.cpp:661] Attempting to start the writer
I0925 19:15:39.551955 27434 replica.cpp:477] Replica received implicit promise request with proposal 1
I0925 19:15:39.552351 27434 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 380746ns
I0925 19:15:39.552372 27434 replica.cpp:345] Persisted promised to 1
I0925 19:15:39.552896 27436 coordinator.cpp:231] Coordinator attemping to fill missing position
I0925 19:15:39.554003 27432 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0925 19:15:39.554534 27432 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 510572ns
I0925 19:15:39.554558 27432 replica.cpp:679] Persisted action at 0
I0925 19:15:39.555516 27443 replica.cpp:511] Replica received write request for position 0
I0925 19:15:39.555595 27443 leveldb.cpp:438] Reading position from leveldb took 65355ns
I0925 19:15:39.556177 27443 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 542757ns
I0925 19:15:39.556200 27443 replica.cpp:679] Persisted action at 0
I0925 19:15:39.556813 27429 replica.cpp:658] Replica received learned notice for position 0
I0925 19:15:39.557251 27429 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 422272ns
I0925 19:15:39.557281 27429 replica.cpp:679] Persisted action at 0
I0925 19:15:39.557315 27429 replica.cpp:664] Replica learned NOP action at position 0
I0925 19:15:39.558061 27442 log.cpp:677] Writer started with ending position 0
I0925 19:15:39.559294 27434 leveldb.cpp:438] Reading position from leveldb took 56536ns
I0925 19:15:39.560333 27432 registrar.cpp:342] Successfully fetched the registry (0B) in 10.936064ms
I0925 19:15:39.560469 27432 registrar.cpp:441] Applied 1 operations in 41340ns; attempting to update the 'registry'
I0925 19:15:39.561244 27441 log.cpp:685] Attempting to append 176 bytes to the log
I0925 19:15:39.561378 27436 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0925 19:15:39.562126 27439 replica.cpp:511] Replica received write request for position 1
I0925 19:15:39.562515 27439 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 364968ns
I0925 19:15:39.562539 27439 replica.cpp:679] Persisted action at 1
I0925 19:15:39.563160 27438 replica.cpp:658] Replica received learned notice for position 1
I0925 19:15:39.563699 27438 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 455933ns
I0925 19:15:39.563730 27438 replica.cpp:679] Persisted action at 1
I0925 19:15:39.563753 27438 replica.cpp:664] Replica learned APPEND action at position 1
I0925 19:15:39.564749 27434 registrar.cpp:486] Successfully updated the 'registry' in 4.214016ms
I0925 19:15:39.564893 27434 registrar.cpp:372] Successfully recovered registrar
I0925 19:15:39.564950 27442 log.cpp:704] Attempting to truncate the log to 1
I0925 19:15:39.565039 27429 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0925 19:15:39.565172 27430 master.cpp:1413] Recovered 0 slaves from the Registry (137B) ; allowing 10mins for slaves to re-register
I0925 19:15:39.565946 27429 replica.cpp:511] Replica received write request for position 2
I0925 19:15:39.566349 27429 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 375473ns
I0925 19:15:39.566371 27429 replica.cpp:679] Persisted action at 2
I0925 19:15:39.566994 27431 replica.cpp:658] Replica received learned notice for position 2
I0925 19:15:39.567440 27431 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 437095ns
I0925 19:15:39.567483 27431 leveldb.cpp:401] Deleting ~1 keys from leveldb took 31954ns
I0925 19:15:39.567498 27431 replica.cpp:679] Persisted action at 2
I0925 19:15:39.567514 27431 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0925 19:15:39.576660 27410 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0925 19:15:39.577055 27410 backend.cpp:50] Failed to create 'bind' backend: BindBackend requires root privileges
I0925 19:15:39.583020 27443 slave.cpp:190] Slave started on 46)@172.17.1.195:41781
I0925 19:15:39.583062 27443 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus(*):1000; mem(*):1000"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4""
I0925 19:15:39.583472 27443 credentials.hpp:85] Loading credential for authentication from '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/credential'
I0925 19:15:39.583752 27443 slave.cpp:321] Slave using credential for: test-principal
I0925 19:15:39.584249 27443 slave.cpp:354] Slave resources: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0925 19:15:39.584344 27443 slave.cpp:390] Slave hostname: f57fd4291168
I0925 19:15:39.584362 27443 slave.cpp:395] Slave checkpoint: true
I0925 19:15:39.585180 27428 state.cpp:54] Recovering state from '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta'
I0925 19:15:39.585383 27440 status_update_manager.cpp:202] Recovering status update manager
I0925 19:15:39.585636 27435 containerizer.cpp:386] Recovering containerizer
I0925 19:15:39.586380 27438 slave.cpp:4110] Finished recovery
I0925 19:15:39.586845 27438 slave.cpp:4267] Querying resource estimator for oversubscribable resources
I0925 19:15:39.587059 27430 status_update_manager.cpp:176] Pausing sending status updates
I0925 19:15:39.587064 27438 slave.cpp:705] New master detected at master@172.17.1.195:41781
I0925 19:15:39.587139 27438 slave.cpp:768] Authenticating with master master@172.17.1.195:41781
I0925 19:15:39.587163 27438 slave.cpp:773] Using default CRAM-MD5 authenticatee
I0925 19:15:39.587321 27438 slave.cpp:741] Detecting new master
I0925 19:15:39.587357 27434 authenticatee.cpp:115] Creating new client SASL connection
I0925 19:15:39.587574 27438 slave.cpp:4281] Received oversubscribable resources  from the resource estimator
I0925 19:15:39.587739 27442 master.cpp:5138] Authenticating slave(46)@172.17.1.195:41781
I0925 19:15:39.587853 27441 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(139)@172.17.1.195:41781
I0925 19:15:39.588052 27439 authenticator.cpp:92] Creating new server SASL connection
I0925 19:15:39.588248 27431 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0925 19:15:39.588297 27431 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0925 19:15:39.588443 27437 authenticator.cpp:197] Received SASL authentication start
I0925 19:15:39.588506 27437 authenticator.cpp:319] Authentication requires more steps
I0925 19:15:39.588677 27443 authenticatee.cpp:252] Received SASL authentication step
I0925 19:15:39.588814 27436 authenticator.cpp:225] Received SASL authentication step
I0925 19:15:39.588855 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0925 19:15:39.588876 27436 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0925 19:15:39.588937 27436 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0925 19:15:39.588979 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0925 19:15:39.588997 27436 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.589011 27436 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.589036 27436 authenticator.cpp:311] Authentication success
I0925 19:15:39.589126 27443 authenticatee.cpp:292] Authentication success
I0925 19:15:39.589192 27437 master.cpp:5168] Successfully authenticated principal 'test-principal' at slave(46)@172.17.1.195:41781
I0925 19:15:39.589238 27433 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(139)@172.17.1.195:41781
I0925 19:15:39.589412 27440 slave.cpp:836] Successfully authenticated with master master@172.17.1.195:41781
I0925 19:15:39.589540 27440 slave.cpp:1230] Will retry registration in 13.562027ms if necessary
I0925 19:15:39.589745 27436 master.cpp:3862] Registering slave at slave(46)@172.17.1.195:41781 (f57fd4291168) with id c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:39.590121 27438 registrar.cpp:441] Applied 1 operations in 70627ns; attempting to update the 'registry'
I0925 19:15:39.590831 27430 log.cpp:685] Attempting to append 345 bytes to the log
I0925 19:15:39.590927 27439 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0925 19:15:39.591809 27430 replica.cpp:511] Replica received write request for position 3
I0925 19:15:39.592072 27430 leveldb.cpp:343] Persisting action (364 bytes) to leveldb took 221734ns
I0925 19:15:39.592099 27430 replica.cpp:679] Persisted action at 3
I0925 19:15:39.592643 27442 replica.cpp:658] Replica received learned notice for position 3
I0925 19:15:39.593215 27442 leveldb.cpp:343] Persisting action (366 bytes) to leveldb took 560946ns
I0925 19:15:39.593237 27442 replica.cpp:679] Persisted action at 3
I0925 19:15:39.593255 27442 replica.cpp:664] Replica learned APPEND action at position 3
I0925 19:15:39.594663 27433 registrar.cpp:486] Successfully updated the 'registry' in 4.472832ms
I0925 19:15:39.594874 27431 log.cpp:704] Attempting to truncate the log to 3
I0925 19:15:39.595407 27429 slave.cpp:3138] Received ping from slave-observer(45)@172.17.1.195:41781
I0925 19:15:39.595450 27433 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0925 19:15:39.596017 27442 replica.cpp:511] Replica received write request for position 4
I0925 19:15:39.596029 27429 hierarchical.hpp:675] Added slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0925 19:15:39.595952 27441 master.cpp:3930] Registered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0925 19:15:39.596240 27429 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:39.596263 27439 slave.cpp:880] Registered with master master@172.17.1.195:41781; given slave ID c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:39.596341 27439 fetcher.cpp:77] Clearing fetcher cache
I0925 19:15:39.596345 27429 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:39.596367 27429 hierarchical.hpp:1239] Performed allocation for slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 in 299337ns
I0925 19:15:39.596524 27434 status_update_manager.cpp:183] Resuming sending status updates
I0925 19:15:39.596571 27442 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 575374ns
I0925 19:15:39.596662 27442 replica.cpp:679] Persisted action at 4
I0925 19:15:39.596984 27439 slave.cpp:903] Checkpointing SlaveInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/slave.info'
I0925 19:15:39.597522 27434 replica.cpp:658] Replica received learned notice for position 4
I0925 19:15:39.597553 27410 sched.cpp:164] Version: 0.26.0
I0925 19:15:39.597746 27439 slave.cpp:939] Forwarding total oversubscribed resources 
I0925 19:15:39.598021 27429 master.cpp:4272] Received update of slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with total oversubscribed resources 
I0925 19:15:39.598070 27434 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 531503ns
I0925 19:15:39.598162 27434 leveldb.cpp:401] Deleting ~2 keys from leveldb took 79081ns
I0925 19:15:39.598170 27428 sched.cpp:262] New master detected at master@172.17.1.195:41781
I0925 19:15:39.598206 27434 replica.cpp:679] Persisted action at 4
I0925 19:15:39.598238 27434 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0925 19:15:39.598276 27428 sched.cpp:318] Authenticating with master master@172.17.1.195:41781
I0925 19:15:39.598296 27428 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0925 19:15:39.598950 27430 hierarchical.hpp:735] Slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) updated with oversubscribed resources  (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0925 19:15:39.599242 27430 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:39.599282 27430 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:39.599341 27430 hierarchical.hpp:1239] Performed allocation for slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 in 327742ns
I0925 19:15:39.599632 27437 authenticatee.cpp:115] Creating new client SASL connection
I0925 19:15:39.600005 27428 master.cpp:5138] Authenticating scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.600170 27435 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(140)@172.17.1.195:41781
I0925 19:15:39.600518 27433 authenticator.cpp:92] Creating new server SASL connection
I0925 19:15:39.600788 27436 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0925 19:15:39.600831 27436 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0925 19:15:39.600944 27433 authenticator.cpp:197] Received SASL authentication start
I0925 19:15:39.601019 27433 authenticator.cpp:319] Authentication requires more steps
I0925 19:15:39.601150 27436 authenticatee.cpp:252] Received SASL authentication step
I0925 19:15:39.601284 27436 authenticator.cpp:225] Received SASL authentication step
I0925 19:15:39.601326 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0925 19:15:39.601341 27436 auxpro",1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-3584,1.0,"rename libprocess tests to ""libprocess-tests"" Stout tests are in a binary named {{stout-tests}}, Mesos tests are in {{mesos-tests}}, but libprocess tests are just {{tests}}. It would be helpful to name them {{libprocess-tests}} ",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3604,3.0,"ExamplesTest.PersistentVolumeFramework does not work in OS X El Capitan The example persistent volume framework test does not pass in OS X El Capitan. It seems to be executing the {{<build_dir>/src/.libs/mesos-executor}} directly while it should be executing the wrapper script at {{<build_dir>/src/mesos-executor}} instead. The no-executor framework passes however, which seem to have a very similar configuration with the persistent volume framework. The following is the output that shows the {{dyld}} load error:

{noformat}
I1008 01:22:52.280140 4284416 launcher.cpp:132] Forked child with pid '1706' for contain
er 'b6d3bd96-2ebd-47b1-a16a-a22ffba992aa'
I1008 01:22:52.280300 4284416 containerizer.cpp:873] Checkpointing executor's forked pid
 1706 to '/var/folders/p6/nfxknpz52dzfc6zqnz23tq180000gn/T/mesos-XXXXXX.5OZ3locB/0/meta/
slaves/34d6329e-69cb-4a72-aee4-fe892bf1c70b-S2/frameworks/34d6329e-69cb-4a72-aee4-fe892b
f1c70b-0000/executors/dec188d4-d2dc-40c5-ac4d-881adc3d81c0/runs/b6d3bd96-2ebd-47b1-a16a-
a22ffba992aa/pids/forked.pid'
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
I1008 01:22:52.365397 3211264 containerizer.cpp:1284] Executor for container '06b649be-88c8-4047-8fb5-e89bdd096b66' has exited
I1008 01:22:52.365433 3211264 containerizer.cpp:1097] Destroying container '06b649be-88c8-4047-8fb5-e89bdd096b66'
{noformat}</build_dir></build_dir>",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-3615,3.0,"Port slave/state.cpp Important subset of changes this depends on:

slave/state.cpp: pid, os, path, protobuf, paths, state
pid.hpp: address.hpp, ip.hpp
address.hpp: ip.hpp, net.hpp
net.hpp: ip, networking stuff
state: type_utils, pid, os, path, protobuf, uuid
type_utils.hpp: uuid.hpp",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-3692,1.0,"Clarify error message 'could not chown work directory' When deploying a framework I encountered the error message 'could not chown work directory'.

It took me a while to figure out that this happened because my framework was registered as a user on my host machine which did not exist on the Docker container and the agent was running as root.

I suggest to clarify this message by pointing out to either set {{--switch-user}}  to {{false}} or to run the framework as the same user as the agent.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3716,3.0,"Update Allocator interface to support quota An allocator should be notified when a quota is being set/updated or removed. Also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota.",,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3718,5.0,"Implement Quota support in allocator The built-in Hierarchical DRF allocator should support Quota. This includes (but not limited to): adding, updating, removing and satisfying quota; avoiding both overcomitting resources and handing them to non-quota'ed roles in presence of master failover.

A [design doc for Quota support in Allocator|https://issues.apache.org/jira/browse/MESOS-2937] provides an overview of a feature set required to be implemented.",,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3743,2.0,"Provide diagnostic output in agent log when fetching fails When fetching fails, the fetcher has written log output to stderr in the task sandbox, but it is not easy to get to. It may even be impossible to get to if one only has the agent log available and no more access to the sandbox. This is for instance the case when looking at output from a CI run.

The fetcher actor in the agent detects if the external fetcher program claims to have succeeded or not. When it exits with an error code, we could grab the fetcher log from the stderr file in the sandbox and append it to the agent log.

This is similar to this patch: https://reviews.apache.org/r/37813/

The difference is that the output of the latter is triggered by test failures outside the fetcher, whereas what is proposed here is triggering upon failures inside the fetcher.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3785,5.0,"Use URI content modification time to trigger fetcher cache updates. Instead of using checksums to trigger fetcher cache updates, we can for starters use the content modification time (mtime), which is available for a number of download protocols, e.g. HTTP and HDFS.

Proposal: Instead of just fetching the content size, we fetch both size  and mtime together. As before, if there is no size, then caching fails and we fall back on direct downloading to the sandbox. 

Assuming a size is given, we compare the mtime from the fetch URI with the mtime known to the cache. If it differs, we update the cache. (As a defensive measure, a difference in size should also trigger an update.) 

Not having an mtime available at the fetch URI is simply treated as a unique valid mtime value that differs from all others. This means that when initially there is no mtime, cache content remains valid until there is one. Thereafter,  anew lack of an mtime invalidates the cache once. In other words: any change from no mtime to having one or back is the same as encountering a new mtime.

Note that this scheme does not require any new protobuf fields.
",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3833,2.0,"/help endpoints do not work for nested paths Mesos displays the list of all supported endpoints starting at a given path prefix using the {{/help}} suffix, e.g. {{master:5050/help}}.

It seems that the {{help}} functionality is broken for URL's having nested paths e.g. {{master:5050/help/master/machine/down}}. The response returned is:
{quote}
Malformed URL, expecting '/help/id/name/'
{quote}",,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3849,1.0,Corrected style in Makefiles Order of files in Makefiles is not strictly alphabetic,,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3864,1.0,"Simplify and/or document the libprocess initialization synchronization logic Tracks this [TODO|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L749].

The [synchronization logic of libprocess|https://github.com/apache/mesos/commit/cd757cf75637c92c438bf4cd22f21ba1b5be702f#diff-128d3b56fc8c9ec0176fdbadcfd11fc2] [predates abstractions|https://github.com/apache/mesos/commit/6c3b107e4e02d5ba0673eb3145d71ec9d256a639#diff-0eebc8689450916990abe080d86c2acb] like {{process::Once}}, which is used in almost all other one-time initialization blocks.  

The logic should be documented.  It can also be simplified (see the [review description|https://reviews.apache.org/r/39949/]).  Or it can be replaced with {{process::Once}}.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3873,3.0,Enhance allocator interface with the recovery() method There are some scenarios (e.g. quota is set for some roles) when it makes sense to notify an allocator about the recovery. Introduce a method into the allocator interface that allows for this.,,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3882,3.0,"Libprocess: Implement process::Clock::finalize Tracks this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L974-L975].

The {{Clock}} is initialized with a callback that, among other things, will dereference the global {{process_manager}} object.

When libprocess is shutting down, the {{process_manager}} is cleaned up.  Between cleanup and termination of libprocess, there is some chance that a {{Timer}} will time out and result in dereferencing {{process_manager}}.

*Proposal* 
* Implement {{Clock::finalize}}.  This would clear:
** existing timers
** process-specific clocks
** ticks
* Change {{process::finalize}}.
*# Resume the clock.  (The clock is only paused during some tests.)  When the clock is not paused, the callback does not dereference {{process_manager}}.
*# Clean up {{process_manager}}.  This terminates all the processes that would potentially interact with {{Clock}}.
*# Call {{Clock::finalize}}.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-3884,1.0,"Corrected style in hierarchical allocator The built-in allocator code has some style issues (namespaces in the .cpp file, unfortunate formatting) which should be corrected for readability.",,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3899,1.0,"Wrong syntax and inconsistent formatting of JSON examples in flag documentation The JSON examples in the documentation of the commandline flags ({{mesos-master.sh --help}} and {{mesos-slave.sh --help}}) don't have a consistent formatting. Furthermore, some examples aren't even compliant JSON because they have trailing commas were they shouldn't.",,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3900,3.0,"Enable mesos-reviewbot project on jenkins to use docker As a first step to adding capability for building multiple configurations on reviewbot, we need to change the build scripts to use docker. ",,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3909,3.0,"isolator module headers depend on picojson headers When trying to build an isolator module, stout headers end up depending on {{picojson.hpp}} which is not installed.

","<code>
In file included from /opt/mesos/include/mesos/module/isolator.hpp:25:
In file included from /opt/mesos/include/mesos/slave/isolator.hpp:30:
In file included from /opt/mesos/include/process/dispatch.hpp:22:
In file included from /opt/mesos/include/process/process.hpp:26:
In file included from /opt/mesos/include/process/event.hpp:21:
In file included from /opt/mesos/include/process/http.hpp:39:
/opt/mesos/include/stout/json.hpp:23:10: fatal error: 'picojson.h' file not found
#include <picojson.h>
         ^
8 warnings and 1 error generated.
<code></code></picojson.h></code>",1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3911,1.0,"Add a `--force` flag to disable sanity check in quota There are use cases when an operator may want to disable the sanity check for quota endpoints (MESOS-3074), even if this renders the cluster under quota. For example, an operator sets quota before adding more agents in order to make sure that no non-quota allocations from new agents are made. ",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3912,3.0,"Rescind offers in order to satisfy quota When a quota request comes in, we may need to rescind a certain amount of outstanding offers in order to satisfy it. Because resources are allocated in the allocator, there can be a race between rescinding and allocating. This race makes it hard to determine the exact amount of offers that should be rescinded in the master.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3936,5.0,"Document possible task state transitions for framework authors We should document the possible ways in which the state of a task can evolve over time; what happens when an agent is partitioned from the master; and more generally, how we recommend that framework authors develop fault-tolerant schedulers and do task state reconciliation.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3939,2.0,"ubsan error in net::IP::create(sockaddr const&): misaligned address Running ubsan from GCC 5.2 on the current Mesos unit tests yields this, among other problems:

{noformat}
/mesos/3rdparty/libprocess/3rdparty/stout/include/stout/ip.hpp:230:56: runtime error: reference binding to misaligned address 0x00000199629c for type 'const struct sockaddr_storage', which requires 8 byte alignment
0x00000199629c: note: pointer points here
  00 00 00 00 02 00 00 00  ff ff ff 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00
              ^
    #0 0x5950cb in net::IP::create(sockaddr const&amp;) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x5950cb)
    #1 0x5970cd in net::IPNetwork::fromLinkDevice(std::__cxx11::basic_string<char, std::char_traits<char="""">, std::allocator<char> &gt; const&amp;, int) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x5970cd)
    #2 0x58e006 in NetTest_LinkDevice_Test::TestBody() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x58e006)
    #3 0x85abd5 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::test, void="""">(testing::Test*, void (testing::Test::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x85abd5)
    #4 0x848abc in void testing::internal::HandleExceptionsInMethodIfSupported<testing::test, void="""">(testing::Test*, void (testing::Test::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x848abc)
    #5 0x7e2755 in testing::Test::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e2755)
    #6 0x7e44a0 in testing::TestInfo::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e44a0)
    #7 0x7e5ffa in testing::TestCase::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e5ffa)
    #8 0x7ffe21 in testing::internal::UnitTestImpl::RunAllTests() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7ffe21)
    #9 0x85d7a5 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::unittestimpl, bool="""">(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x85d7a5)
    #10 0x84b37a in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::unittestimpl, bool="""">(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x84b37a)
    #11 0x7f8a4a in testing::UnitTest::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7f8a4a)
    #12 0x608a96 in RUN_ALL_TESTS() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x608a96)
    #13 0x60896b in main (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x60896b)
    #14 0x7fd0f0c7fa3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x20a3f)
    #15 0x4145c8 in _start (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x4145c8)
{noformat}</testing::internal::unittestimpl,></testing::internal::unittestimpl,></testing::test,></testing::test,></char></char,>",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-3949,3.0,"User CGroup Isolation tests fail on Centos 6. UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup and UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup fail on CentOS 6.6 with similar output when libevent and SSL are enabled.

{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup"" --verbose
{noformat}
{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from UserCgroupIsolatorTest/0, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup
I1118 16:53:35.273717 30249 mem.cpp:605] Started listening for OOM events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.274538 30249 mem.cpp:725] Started listening on low memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.275164 30249 mem.cpp:725] Started listening on medium memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.275784 30249 mem.cpp:725] Started listening on critical memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.276448 30249 mem.cpp:356] Updated 'memory.soft_limit_in_bytes' to 1GB for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.277331 30249 mem.cpp:391] Updated 'memory.limit_in_bytes' to 1GB for container 867a829e-4a26-43f5-86e0-938bf1f47688
-bash: /sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ &gt;"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
[  FAILED  ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess (149 ms)
{noformat}

{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup"" --verbose
{noformat}
{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from UserCgroupIsolatorTest/1, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup
I1118 17:01:00.550706 30357 cpushare.cpp:392] Updated 'cpu.shares' to 1024 (cpus 1) for container e57f4343-1a97-4b44-b347-803be47ace80
-bash: /sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ &gt;"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ &gt;"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess (116 ms)
{noformat}",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3964,2.0,"LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs and LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs_Big_Quota fail on Debian 8. sudo ./bin/mesos-test.sh --gtest_filter=""LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs""

{noformat}
...
F1119 14:34:52.514742 30706 isolator_tests.cpp:455] CHECK_SOME(isolator): Failed to find 'cpu.cfs_quota_us'. Your kernel might be too old to use the CFS cgroups feature.
{noformat}
",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-3965,3.0,"Ensure resources in `QuotaInfo` protobuf do not contain `role` {{QuotaInfo}} protobuf currently stores per-role quotas, including {{Resource}} objects. These resources are neither statically nor dynamically reserved, hence they may not contain {{role}} field. We should ensure this field is unset, as well as update validation routine for {{QuotaInfo}}",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3981,3.0,Implement recovery in the Hierarchical allocator The built-in Hierarchical allocator should implement the recovery (in the presence of quota).,,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-3983,3.0,"Tests for quota request validation Tests should include:
* JSON validation;
* Absence of irrelevant fields;
* Semantic validation.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-4004,3.0,Support default entrypoint and command runtime config in Mesos containerizer We need to use the entrypoint and command runtime configuration returned from image to be used in Mesos containerizer.,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4005,2.0,Support workdir runtime configuration from image  We need to support workdir runtime configuration returned from image such as Dockerfile.,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4046,3.0,"Enable `Env` specified in docker image can be returned from docker pull Currently docker pull only return an image structure, which only contains entrypoint info. We have docker inspect as a subprocess inside docker pull, which contains many other useful information of a docker image. We should be able to support returning environment variables information from the image.",,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4047,1.0,"MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky {code:title=Output from passed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000430889 s, 2.4 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:14.319327  5062 exec.cpp:134] Version: 0.27.0
I1202 11:09:14.333317  5079 exec.cpp:208] Executor registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Registered executor on ubuntu
Starting task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 5085
I1202 11:09:14.391739  5077 exec.cpp:254] Received reconnect request from slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
I1202 11:09:14.398598  5082 exec.cpp:231] Executor re-registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Re-registered executor on ubuntu
Shutting down
Sending SIGTERM to process tree at pid 5085
Killing the following process trees:
[ 
-+- 5085 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done 
 \--- 5086 dd count=512 bs=1M if=/dev/zero of=./temp 
]
[       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (1096 ms)
","<code>

{code:title=Output from failed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000404489 s, 2.6 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:15.509950  5109 exec.cpp:134] Version: 0.27.0
I1202 11:09:15.568183  5123 exec.cpp:208] Executor registered on slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
Registered executor on ubuntu
Starting task 14b6bab9-9f60-4130-bdc4-44efba262bc6
Forked command at 5132
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
I1202 11:09:15.665498  5129 exec.cpp:254] Received reconnect request from slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
I1202 11:09:15.670995  5123 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 5132
../../src/tests/containerizer/memory_pressure_tests.cpp:283: Failure
(usage).failure(): Unknown container: ebe90e15-72fa-4519-837b-62f43052c913
*** Aborted at 1449083355 (unix time) try ""date -d @1449083355"" if you are using GNU date ***
<code>

Notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent.</code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-4087,5.0,"Introduce a module for logging executor/task output Existing executor/task logs are logged to files in their sandbox directory, with some nuances based on which containerizer is used (see background section in linked document).

A logger for executor/task logs has the following requirements:
* The logger is given a command to run and must handle the stdout/stderr of the command.
* The handling of stdout/stderr must be resilient across agent failover.  Logging should not stop if the agent fails.
* Logs should be readable, presumably via the web UI, or via some other module-specific UI.",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4107,1.0,`os::strerror_r` breaks the Windows build `os::strerror_r` does not exist on Windows.,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-4108,5.0,"Implement `os::mkdtemp` for Windows Used basically exclusively for testing, this insecure and otherwise-not-quite-suitable-for-prod function needs to work to run what will eventually become the FS tests.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-4109,1.0,"HTTPConnectionTest.ClosingResponse is flaky Output of the test:
","<code>
[ RUN      ] HTTPConnectionTest.ClosingResponse
I1210 01:20:27.048532 26671 process.cpp:3077] Handling HTTP event for process '(22)' with path: '/(22)/get'
../../../3rdparty/libprocess/src/tests/http_tests.cpp:919: Failure
Actual function call count doesn't match EXPECT_CALL(*http.process, get(_))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
[  FAILED  ] HTTPConnectionTest.ClosingResponse (43 ms)
<code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-4110,5.0,"Implement `WindowsError` to correspond with `ErrnoError`. In the C standard library, `errno` records the last error on a thread. You can pretty-print it with `strerror`.

In Stout, we report these errors with `ErrnoError`.

The Windows API has something similar, called `GetLastError()`. The way to pretty-print this is hilariously unintuitive and terrible, so in this case it is actually very beneficial to wrap it with something similar to `ErrnoError`, maybe called `WindowsError`.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-4130,1.0,"Document how the fetcher can reach across a proxy connection. The fetcher uses libcurl for downloading content from HTTP, HTTPS, etc. There is no source code in the pertinent parts of ""net.hpp"" that deals with proxy settings. However, libcurl automatically picks up certain environment variables and adjusts its settings accordingly. See ""man libcurl-tutorial"" for details. See section ""Proxies"", subsection ""Environment Variables"". If you follow this recipe in your Mesos agent startup script, you can use a proxy. 

We should document this in the fetcher (cache) doc (http://mesos.apache.org/documentation/latest/fetcher/).
",,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4136,3.0,"Add a ContainerLogger module that restrains log sizes One of the major problems this logger module aims to solve is overflowing executor/task log files.  Log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).

We should add a {{ContainerLogger}} module that truncates logs as it reaches a configurable maximum size.  Additionally, we should determine if the web UI's {{pailer}} needs to be changed to deal with logs that are not append-only.

This will be a non-default module which will also serve as an example for how to implement the module.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4150,3.0,"Implement container logger module metadata recovery The {{ContainerLoggers}} are intended to be isolated from agent failover, in the same way that executors do not crash when the agent process crashes.

For default {{ContainerLogger}} s, like the {{SandboxContainerLogger}} and the (tentatively named) {{TruncatingSandboxContainerLogger}}, the log files are exposed during agent recovery regardless.

For non-default {{ContainerLogger}} s, the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs.  This can be implemented as part of {{Containerizer::recover}}.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4192,3.0,"Add documentation for API Versioning Currently, we don't have any documentation for:

- How Mesos implements API versioning ?
- How are protobufs versioned and how does mesos handle them internally ?
- What do contributors need to do when they make a change to a external user facing protobuf ?

The relevant design doc:
https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b
",,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4200,2.0,"Test case(s) for weights + allocation behavior As far as I can see, we currently have NO test cases for behavior when weights are defined.",,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-4207,2.0,"Add an example bug due to a lack of defer() to the defer() documentation In the past, some bugs have been introduced into the codebase due to a lack of {{defer()}} where it should have been used. It would be useful to add an example of this to the {{defer()}} documentation.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4209,3.0,"Document ""how to program with dynamic reservations and persistent volumes"" Specifically, some of the gotchas around:

* Retrying reservation attempts after a timeout
* Fuzzy-matching resources to determine whether a reservation/PV is successful
* Represent client state as a state machine and repeatedly move ""toward"" successful terminate stats

Should also point to persistent volume example framework. We should also ask Gabriel and others (Arango?) who have built frameworks with PVs/DRs for feedback.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4222,3.0,"Document containerizer from user perspective. Add documentation that covers:

* Purpose of containerizers from a use case perspective.
* What purpose does each containerizer (mesos. docker, compose) serve.
* What criteria could be used to choose a containerizer.",,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4226,1.0,"Enable passing docker image environment variables runtime config to provisioner Collect environment variables runtime config information from a docker image, and save as a map. Pass it back to provisioner, and handling environment variables merge issue.",,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4241,3.0,"Consolidate docker store slave flags Currently there are too many slave flags for configuring the docker store/puller.
We can remove the following flags:

docker_auth_server_port
docker_local_archives_dir
docker_registry_port
docker_puller

And consolidate them into the existing flags.",,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4257,3.0,"ExamplesTest.NoExecutorFramework runs forever. {noformat: title=Good Run}
[ RUN      ] ExamplesTest.NoExecutorFramework
I1221 23:10:02.721617 32528 exec.cpp:444] Ignoring exited event because the driver is aborted!
Using temporary directory '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn'
I1221 23:10:02.721675 32539 exec.cpp:444] Ignoring exited event because the driver is aborted!
I1221 23:10:02.722024 32554 exec.cpp:444] Ignoring exited event because the driver is aborted!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 23:10:05.179466 32569 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32;disk:32
Trying semicolon-delimited string format instead
I1221 23:10:05.180269 32569 logging.cpp:172] Logging to STDERR
I1221 23:10:05.185768 32569 process.cpp:998] libprocess is initialized on 172.17.0.2:40874 for 16 cpus
I1221 23:10:05.200728 32569 leveldb.cpp:174] Opened db in 4.184362ms
I1221 23:10:05.202234 32569 leveldb.cpp:181] Compacted db in 1.459268ms
I1221 23:10:05.202353 32569 leveldb.cpp:196] Created db iterator in 73761ns
I1221 23:10:05.202383 32569 leveldb.cpp:202] Seeked to beginning of db in 3382ns
I1221 23:10:05.202405 32569 leveldb.cpp:271] Iterated through 0 keys in the db in 633ns
I1221 23:10:05.202674 32569 replica.cpp:779] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I1221 23:10:05.205301 32604 recover.cpp:447] Starting replica recovery
I1221 23:10:05.206414 32569 local.cpp:239] Using 'local' authorizer
I1221 23:10:05.206405 32604 recover.cpp:473] Replica is in EMPTY status
I1221 23:10:05.209595 32594 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.2:40874
I1221 23:10:05.210916 32596 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1221 23:10:05.211515 32597 master.cpp:365] Master 3931c1a8-1cd6-49eb-94c8-d01b33bb008e (6ccf2ee56b13) started on 172.17.0.2:40874
I1221 23:10:05.211699 32605 recover.cpp:564] Updating replica status to STARTING
I1221 23:10:05.211539 32597 master.cpp:367] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-otpdch"" --zk_session_timeout=""10secs""
I1221 23:10:05.212323 32597 master.cpp:412] Master only allowing authenticated frameworks to register
I1221 23:10:05.212337 32597 master.cpp:419] Master allowing unauthenticated slaves to register
I1221 23:10:05.212347 32597 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials'
W1221 23:10:05.212442 32597 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I1221 23:10:05.212606 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 656857ns
I1221 23:10:05.212620 32597 master.cpp:456] Using default 'crammd5' authenticator
I1221 23:10:05.212631 32600 replica.cpp:320] Persisted replica status to STARTING
I1221 23:10:05.212893 32597 authenticator.cpp:518] Initializing server SASL
I1221 23:10:05.213091 32608 recover.cpp:473] Replica is in STARTING status
I1221 23:10:05.213958 32595 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.2:40874
I1221 23:10:05.214323 32594 recover.cpp:193] Received a recover response from a replica in STARTING status
I1221 23:10:05.214689 32595 recover.cpp:564] Updating replica status to VOTING
I1221 23:10:05.215353 32596 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 487419ns
I1221 23:10:05.215384 32596 replica.cpp:320] Persisted replica status to VOTING
I1221 23:10:05.215481 32605 recover.cpp:578] Successfully joined the Paxos group
I1221 23:10:05.215867 32605 recover.cpp:462] Recover process terminated
I1221 23:10:05.216111 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
W1221 23:10:05.217021 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 23:10:05.221482 32608 slave.cpp:191] Slave started on 1)@172.17.0.2:40874
I1221 23:10:05.221521 32608 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/0""
I1221 23:10:05.222578 32608 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 23:10:05.223465 32608 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.223621 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1221 23:10:05.223610 32608 slave.cpp:400] Slave attributes: [  ]
I1221 23:10:05.223677 32608 slave.cpp:405] Slave hostname: 6ccf2ee56b13
I1221 23:10:05.223697 32608 slave.cpp:410] Slave checkpoint: true
W1221 23:10:05.224143 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 23:10:05.226668 32604 slave.cpp:191] Slave started on 2)@172.17.0.2:40874
I1221 23:10:05.226692 32604 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/1""
I1221 23:10:05.227520 32604 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 23:10:05.228037 32604 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.228148 32604 slave.cpp:400] Slave attributes: [  ]
I1221 23:10:05.228169 32604 slave.cpp:405] Slave hostname: 6ccf2ee56b13
I1221 23:10:05.228184 32604 slave.cpp:410] Slave checkpoint: true
I1221 23:10:05.229123 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1221 23:10:05.229641 32605 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/0/meta'
W1221 23:10:05.229645 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 23:10:05.229636 32595 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/1/meta'
I1221 23:10:05.230242 32605 status_update_manager.cpp:200] Recovering status update manager
I1221 23:10:05.230254 32598 status_update_manager.cpp:200] Recovering status update manager
I1221 23:10:05.230515 32601 containerizer.cpp:383] Recovering containerizer
I1221 23:10:05.230562 32602 containerizer.cpp:383] Recovering containerizer
I1221 23:10:05.232681 32597 auxprop.cpp:71] Initialized in-memory auxiliary property plugin
I1221 23:10:05.232803 32597 master.cpp:493] Authorization enabled
I1221 23:10:05.232867 32600 slave.cpp:4427] Finished recovery
I1221 23:10:05.232980 32598 slave.cpp:191] Slave started on 3)@172.17.0.2:40874
I1221 23:10:05.233039 32594 slave.cpp:4427] Finished recovery
I1221 23:10:05.233376 32599 whitelist_watcher.cpp:77] No whitelist given
I1221 23:10:05.233428 32601 hierarchical.cpp:147] Initialized hierarchical allocator process
I1221 23:10:05.233003 32598 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/2""
I1221 23:10:05.233744 32600 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 23:10:05.233749 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 23:10:05.234222 32598 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.234284 32598 slave.cpp:400] Slave attributes: [  ]
I1221 23:10:05.234299 32598 slave.cpp:405] Slave hostname: 6ccf2ee56b13
I1221 23:10:05.234311 32598 slave.cpp:410] Slave checkpoint: true
I1221 23:10:05.234338 32600 slave.cpp:729] New master detected at master@172.17.0.2:40874
I1221 23:10:05.234376 32604 status_update_manager.cpp:174] Pausing sending status updates
I1221 23:10:05.234424 32600 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 23:10:05.234522 32600 slave.cpp:765] Detecting new master
I1221 23:10:05.234616 32569 sched.cpp:164] Version: 0.27.0
I1221 23:10:05.234658 32600 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 23:10:05.234671 32594 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 23:10:05.234884 32606 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 23:10:05.235038 32595 status_update_manager.cpp:174] Pausing sending status updates
I1221 23:10:05.235043 32606 slave.cpp:729] New master detected at master@172.17.0.2:40874
I1221 23:10:05.235111 32606 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 23:10:05.235147 32606 slave.cpp:765] Detecting new master
I1221 23:10:05.235240 32594 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/2/meta'
I1221 23:10:05.235443 32608 status_update_manager.cpp:200] Recovering status update manager
I1221 23:10:05.235625 32594 containerizer.cpp:383] Recovering containerizer
I1221 23:10:05.236549 32599 slave.cpp:4427] Finished recovery
I1221 23:10:05.236984 32593 sched.cpp:262] New master detected at master@172.17.0.2:40874
I1221 23:10:05.237004 32599 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 23:10:05.237221 32593 sched.cpp:318] Authenticating with master master@172.17.0.2:40874
I1221 23:10:05.237277 32593 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1221 23:10:05.237285 32604 status_update_manager.cpp:174] Pausing sending status updates
I1221 23:10:05.237288 32599 slave.cpp:729] New master detected at master@172.17.0.2:40874
I1221 23:10:05.237361 32599 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 23:10:05.237433 32599 slave.cpp:765] Detecting new master
I1221 23:10:05.237565 32599 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 23:10:05.238154 32605 authenticatee.cpp:97] Initializing client SASL
I1221 23:10:05.238315 32605 authenticatee.cpp:121] Creating new client SASL connection
I1221 23:10:05.239640 32597 master.cpp:1200] Dropping 'mesos.internal.AuthenticateMessage' message since not elected yet
I1221 23:10:05.239765 32597 master.cpp:1629] The newly elected leader is master@172.17.0.2:40874 with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e
I1221 23:10:05.239794 32597 master.cpp:1642] Elected as the leading master!
I1221 23:10:05.239843 32597 master.cpp:1387] Recovering from registrar
I1221 23:10:05.240056 32600 registrar.cpp:307] Recovering registrar
I1221 23:10:05.241477 32608 log.cpp:659] Attempting to start the writer
I1221 23:10:05.244540 32600 replica.cpp:493] Replica received implicit promise request from (39)@172.17.0.2:40874 with proposal 1
I1221 23:10:05.245358 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 776937ns
I1221 23:10:05.245393 32600 replica.cpp:342] Persisted promised to 1
I1221 23:10:05.246625 32601 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1221 23:10:05.248757 32605 replica.cpp:388] Replica received explicit promise request from (40)@172.17.0.2:40874 for position 0 with proposal 2
I1221 23:10:05.249214 32605 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 366567ns
I1221 23:10:05.249246 32605 replica.cpp:712] Persisted action at 0
I1221 23:10:05.250998 32599 replica.cpp:537] Replica received write request for position 0 from (41)@172.17.0.2:40874
I1221 23:10:05.251111 32599 leveldb.cpp:436] Reading position from leveldb took 66773ns
I1221 23:10:05.251734 32599 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 379612ns
I1221 23:10:05.251759 32599 replica.cpp:712] Persisted action at 0
I1221 23:10:05.252555 32601 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1221 23:10:05.253010 32601 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 381858ns
I1221 23:10:05.253036 32601 replica.cpp:712] Persisted action at 0
I1221 23:10:05.253068 32601 replica.cpp:697] Replica learned NOP action at position 0
I1221 23:10:05.254043 32595 log.cpp:675] Writer started with ending position 0
I1221 23:10:05.256741 32595 leveldb.cpp:436] Reading position from leveldb took 48607ns
I1221 23:10:05.260617 32601 registrar.cpp:340] Successfully fetched the registry (0B) in 20.47616ms
I1221 23:10:05.260988 32601 registrar.cpp:439] Applied 1 operations in 103123ns; attempting to update the 'registry'
I1221 23:10:05.264700 32604 log.cpp:683] Attempting to append 170 bytes to the log
I1221 23:10:05.265138 32601 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1221 23:10:05.266208 32603 replica.cpp:537] Replica received write request for position 1 from (42)@172.17.0.2:40874
I1221 23:10:05.266829 32603 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 551087ns
I1221 23:10:05.266861 32603 replica.cpp:712] Persisted action at 1
I1221 23:10:05.267918 32605 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1221 23:10:05.268442 32605 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 453416ns
I1221 23:10:05.268470 32605 replica.cpp:712] Persisted action at 1
I1221 23:10:05.268506 32605 replica.cpp:697] Replica learned APPEND action at position 1
I1221 23:10:05.270512 32606 registrar.cpp:484] Successfully updated the 'registry' in 9.375232ms
I1221 23:10:05.270705 32606 registrar.cpp:370] Successfully recovered registrar
I1221 23:10:05.271045 32602 log.cpp:702] Attempting to truncate the log to 1
I1221 23:10:05.271178 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0
MESOS-4261,3.0,"Remove docker auth server flag We currently use a configured docker auth server from a slave flag to get token auth for docker registry. However this doesn't work for private registries as docker registry supports sending down the correct auth server to contact.

We should remove docker auth server flag completely and ask the docker registry for auth server.",,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4262,5.0,"Enable net_cls subsytem in cgroup infrastructure Currently the control group infrastructure within mesos supports only the memory and CPU subsystems. We need to enhance this infrastructure to support the net_cls subsystem as well. Details of the net_cls subsystem and its use-cases can be found here:
https://www.kernel.org/doc/Documentation/cgroups/net_cls.txt

Enabling the net_cls will allow us to provide operators to, potentially, regulate framework traffic on a per-container basis.  ",,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4282,2.0,"Update isolator prepare function to use ContainerLaunchInfo Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers. 

By doing this (ContainerPrepareInfo -&gt; ContainerLaunchInfo), we can select any necessary information and passing then to launcher.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4284,8.0,Draft design doc for multi-role frameworks Create a document that describes the problems with having only single-role frameworks and proposes an MVP solution and implementation approach.,,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4285,3.0,Mesos command task doesn't support volumes with image Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. ,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4289,5.0,"Design doc for simple appc image discovery Create a design document describing the following:

- Model and abstraction of the Discoverer
- Workflow of the discovery process
",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4300,3.0,Add AuthN and AuthZ to maintenance endpoints. Maintenance endpoints are currently only restricted by firewall settings.  They should also support authentication/authorization like other HTTP endpoints.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0
MESOS-4304,1.0,"hdfs operations fail due to prepended / on path for non-hdfs hadoop clients. This bug was resolved for the hdfs protocol for MESOS-3602 but since the process checks for the ""hdfs"" protocol at the beginning of the URI, the fix does not extend itself to non-hdfs hadoop clients.
","<code>
I0107 01:22:01.259490 17678 logging.cpp:172] INFO level logging started!
I0107 01:22:01.259856 17678 fetcher.cpp:422] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""maprfs:\/\/\/mesos\/storm-mesos-0.9.3.tgz""}},{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""http:\/\/s0121.stag.urbanairship.com:36373\/conf\/storm.yaml""}}],""sandbox_directory"":""\/mnt\/data\/mesos\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/frameworks\/530dda5a-481a-4117-8154-3aee637d3b38-0000\/executors\/word-count-1-1452129714\/runs\/4443d5ac-d034-49b3-bf12-08fb9b0d92d0"",""user"":""root""}
I0107 01:22:01.262171 17678 fetcher.cpp:377] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz'
I0107 01:22:01.262212 17678 fetcher.cpp:248] Fetching directly into the sandbox directory
I0107 01:22:01.262243 17678 fetcher.cpp:185] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz'
I0107 01:22:01.671777 17678 fetcher.cpp:110] Downloading resource with Hadoop client from 'maprfs:///mesos/storm-mesos-0.9.3.tgz' to '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'
copyToLocal: java.net.URISyntaxException: Expected scheme-specific part at index 7: maprfs:
Usage: java FsShell [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>]
E0107 01:22:02.435556 17678 shell.hpp:90] Command 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'' failed; this is the output:
Failed to fetch 'maprfs:///mesos/storm-mesos-0.9.3.tgz': HDFS copyToLocal failed: Failed to execute 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz''; the command was either not found or exited with a non-zero exit status: 255
Failed to synchronize with slave (it's probably exited)
<code>

After a brief chat with [~jieyu], it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it.</code></localdst></src></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4333,2.0,Refactor Appc provisioner tests   Current tests can be refactored so that we can reuse some common tasks like test image creation. This will benefit future tests like appc image puller tests.,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4348,1.0,"GMock warning in HookTest.VerifySlaveRunTaskHook, HookTest.VerifySlaveTaskStatusDecorator {noformat}
[ RUN      ] HookTest.VerifySlaveRunTaskHook

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7ff079cb2420)
Stack trace:
[       OK ] HookTest.VerifySlaveRunTaskHook (51 ms)
[ RUN      ] HookTest.VerifySlaveTaskStatusDecorator

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7ff079cbb790)
Stack trace:
[       OK ] HookTest.VerifySlaveTaskStatusDecorator (54 ms)
{noformat}

Occurs non-deterministically for me. OSX 10.10.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-4349,1.0,"GMock warning in SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor {noformat}
[ RUN      ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fe189cae850)
Stack trace:
[       OK ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor (51 ms)
{noformat}

Occurs non-deterministically for me on OSX 10.10, perhaps one run in ten.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-4353,1.0,"Limit the number of processes created by libprocess Currently libprocess will create {{max(8, number of CPU cores)}} processes during the initialization, see https://github.com/apache/mesos/blob/0.26.0/3rdparty/libprocess/src/process.cpp#L2146 for details. This should be OK for a normal machine which has no much cores (e.g., 16, 32), but for a powerful machine which may have a large number of cores (e.g., an IBM Power machine may have 192 cores), this will cause too much worker threads which are not necessary.

And since libprocess is widely used in Mesos (master, agent, scheduler, executor), it may also cause some performance issue. For example, when user creates a Docker container via Mesos in a Mesos agent which is running on a powerful machine with 192 cores, the DockerContainerizer in Mesos agent will create a dedicated executor for the container, and there will be 192 worker threads in that executor. And if user creates 1000 Docker containers in that machine, then there will be 1000 executors, i.e., 1000 * 192 worker threads which is a large number and may thrash the OS.

",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4357,1.0,"GMock warning in RoleTest.ImplicitRoleStaticReservation {noformat}
[ RUN      ] RoleTest.ImplicitRoleStaticReservation

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fe37a4752f0)
Stack trace:
[       OK ] RoleTest.ImplicitRoleStaticReservation (52 ms)
{noformat}",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-4358,2.0,"Expose net_cls network handles in agent's state endpoint We need to expose net_cls network handles, associated with containers, to operators and network utilities that would use these network handles to enforce network policy. 

In order to achieve the above we need to add a new field in the `NetworkInfo` protobuf (say NetHandles) and update this field when a container gets assigned to a net_cls cgroup. The `ContainerStatus` protobuf already has the `NetworkInfo` protobuf as a nested message, and the `ContainerStatus` itself is exposed to operators as part of TaskInfo (for tasks associated with the container) in an agent's state.json. ",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4377,1.0,Document units associated with resource types We should document the units associated with memory and disk resources.,,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4410,3.0,"Introduce protobuf for quota set request. To document quota request JSON schema and simplify request processing, introduce a {{QuotaRequest}} protobuf wrapper.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4421,3.0,"Document that /reserve, /create-volumes endpoints can return misleading ""success"" The docs for the {{/reserve}} endpoint say:

{noformat}
200 OK: Success (the requested resources have been reserved).
{noformat}

This is not true: the master returns {{200}} when the request has been validated and a {{CheckpointResourcesMessage}} has been sent to the agent, but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed. Same behavior applies to {{/unreserve}}, {{/create-volumes}}, and {{/destroy-volumes}}.

We should _either_:

1. Accurately document what {{200}} return code means.
2. Change the implementation to wait for the agent's next checkpoint to succeed (and to include the effect of the operation) before returning success to the HTTP client.",,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4425,3.0,"Introduce filtering test abstractions for HTTP events to libprocess We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.

The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4435,3.0,Update `Master::Http::stateSummary` to use `jsonify`. Update {{state-summary}} to use {{jsonify}} to stay consistent with {{state}} HTTP endpoint.,,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4437,1.0,"Disable the test RegistryClientTest.BadTokenServerAddress. As we are retiring registry client, disable this test which looks flaky.",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4438,1.0,Add 'dependency' message to 'AppcImageManifest' protobuf. AppcImageManifest protobuf currently lacks 'dependencies' which is necessary for image discovery.,,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4454,2.0,Create common sha512 compute utility function. Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. ,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4488,1.0,"Define a CgroupInfo protobuf to expose cgroup isolator configuration. Within `MesosContainerizer` we have an isolator associated with each linux cgroup subsystem. The isolators apply subsystem specific configuration on the containers before launching the containers. For e.g cgroup/net_cls isolator applies net_cls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu-share isolator configures cpu shares. 

Currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container. We therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container. This protobuf will be filled in by the cgroup isolator and will be stored as part of `ContainerConfig` in the containerizer. ",,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4505,3.0,"Hierarchical allocator performance is slow due to Quota Since we do not strip the non-scalar resources during the resource arithmetic for quota, the performance can degrade significantly, as currently resource arithmetic is expensive.

One approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars. This is valid as quota can currently only be set for scalar resource types.",,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4517,3.0,"Introduce docker runtime isolator. Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4520,1.0,"Introduce a status() interface for isolators While launching a container mesos isolators end up configuring/modifying various properties of the container. For e.g., cgroup isolators (mem, cpu, net_cls) configure/change the properties associated with their respective subsystems before launching a container. Similary network isolator (net-modules, port mapping) configure the IP address and ports associated with a container. 

Currently, there are not interface in the isolator to extract the run time state of these properties for a given container. Therefore a status() method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator. ",,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4529,2.0,"Update the allocator to not offer unreserved resources beyond quota. Eventually, we will want to offer unreserved resources as revocable beyond the role's quota. Rather than offering non-revocable resources beyond the role's quota's guarantee, in the short term, we choose to not offer resources beyond a role's quota.",,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4535,1.0,"Logrotate ContainerLogger may not handle FD ownership correctly One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.

The way the logrotate module uses this is slightly incorrect:
# The module starts a subprocess with an output {{Subprocess::PIPE()}}.
# That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}.
# When the second subprocess starts, the pipe's FD is closed in the parent.
# When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4546,3.0,"Mesos Agents needs to re-resolve hosts in zk string on leader change / failure to connect Sample Mesos Agent log: https://gist.github.com/brndnmtthws/fb846fa988487250a809

Note, zookeeper has a function to change the list of servers at runtime: https://github.com/apache/zookeeper/blob/735ea78909e67c648a4978c8d31d63964986af73/src/c/src/zookeeper.c#L1207-L1232

This comes up when using an AWS AutoScalingGroup for managing the set of masters. 

The agent when it comes up the first time, resolves the zk:// string. Once all the hosts that were in the original string fail (Each fails, is replaced by a new machine, which has the same DNS name), the agent just keeps spinning in an internal loop, never re-resolving the DNS names.

Two solutions I see are 
1. Update the list of servers / re-resolve
2. Have the agent detect it hasn't connected recently, and kill itself (Which will force a re-resolution when the agent starts back up)",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-4562,2.0,"Mesos UI shows wrong count for ""started"" tasks The task started field shows the number of tasks in state ""TASKS_STARTING"" as opposed to those in ""TASK_RUNNING"" state.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
MESOS-4564,2.0,Separate Appc protobuf messages to its own file. It would be cleaner to keep the Appc protobuf messages separate from other mesos messages.,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4566,1.0,"Avoid unnecessary temporary `std::string` constructions and copies in `jsonify`. A few of the critical code paths in {{jsonify}} involve unnecessary temporary string construction and copies (inherited from the {{JSON::*}}). For example, {{strings::trim}} is used to remove trailing 0s from printing {{double}}s. We print {{double}}s a lot, and therefore constructing a temporary {{std::string}} on printing of every double is extremely costly. This ticket captures the work involved in avoiding them.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-4576,2.0,"Introduce a stout helper for ""which"" We may want to add a helper to {{stout/os.hpp}} that will natively emulate the functionality of the Linux utility {{which}}.  i.e.
","<code>
Option<string> which(const string&amp; command)
{
  Option<string> path = os::getenv(""PATH"");

  // Loop through path and return the first one which os::exists(...).

  return None();
}
<code>

This helper may be useful:
* for test filters in {{src/tests/environment.cpp}}
* a few tests in {{src/tests/containerizer/port_mapping_tests.cpp}}
* the {{sha512}} utility in {{src/common/command_utils.cpp}}
* as runtime checks in the {{LogrotateContainerLogger}}
* etc.</code></string></string></code>",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-4584,2.0,Update Rakefile for mesos site generation The stuff in site/ directory needs some updates to make it easier to generate updates for mesos.apache.org site.,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0
MESOS-4590,2.0,"Add test case for reservations with same role, different principals We don't have a test case that covers $SUBJECT; we probably should.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-4614,3.0,"SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky Just saw this failure on the ASF CI:

","<code>
[ RUN      ] SlaveRecoveryTest/0.CleanupHTTPExecutor
I0206 00:22:44.791671  2824 leveldb.cpp:174] Opened db in 2.539372ms
I0206 00:22:44.792459  2824 leveldb.cpp:181] Compacted db in 740473ns
I0206 00:22:44.792510  2824 leveldb.cpp:196] Created db iterator in 24164ns
I0206 00:22:44.792532  2824 leveldb.cpp:202] Seeked to beginning of db in 1831ns
I0206 00:22:44.792548  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 342ns
I0206 00:22:44.792605  2824 replica.cpp:779] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0206 00:22:44.793256  2847 recover.cpp:447] Starting replica recovery
I0206 00:22:44.793480  2847 recover.cpp:473] Replica is in EMPTY status
I0206 00:22:44.794538  2847 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9472)@172.17.0.2:43484
I0206 00:22:44.795040  2848 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 00:22:44.795644  2848 recover.cpp:564] Updating replica status to STARTING
I0206 00:22:44.796519  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 752810ns
I0206 00:22:44.796545  2850 replica.cpp:320] Persisted replica status to STARTING
I0206 00:22:44.796725  2848 recover.cpp:473] Replica is in STARTING status
I0206 00:22:44.797828  2857 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9473)@172.17.0.2:43484
I0206 00:22:44.798355  2850 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 00:22:44.799193  2850 recover.cpp:564] Updating replica status to VOTING
I0206 00:22:44.799583  2855 master.cpp:376] Master 0b206a40-a9c3-4d44-a5bd-8032d60a32ca (6632562f1ade) started on 172.17.0.2:43484
I0206 00:22:44.799609  2855 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/n2FxQV/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/n2FxQV/master"" --zk_session_timeout=""10secs""
I0206 00:22:44.799991  2855 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 00:22:44.800009  2855 master.cpp:428] Master only allowing authenticated slaves to register
I0206 00:22:44.800020  2855 credentials.hpp:35] Loading credentials for authentication from '/tmp/n2FxQV/credentials'
I0206 00:22:44.800245  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 679345ns
I0206 00:22:44.800370  2850 replica.cpp:320] Persisted replica status to VOTING
I0206 00:22:44.800397  2855 master.cpp:468] Using default 'crammd5' authenticator
I0206 00:22:44.800693  2855 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 00:22:44.800815  2855 master.cpp:571] Authorization enabled
I0206 00:22:44.801216  2850 recover.cpp:578] Successfully joined the Paxos group
I0206 00:22:44.801604  2850 recover.cpp:462] Recover process terminated
I0206 00:22:44.801759  2856 whitelist_watcher.cpp:77] No whitelist given
I0206 00:22:44.801725  2847 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 00:22:44.803982  2855 master.cpp:1712] The newly elected leader is master@172.17.0.2:43484 with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca
I0206 00:22:44.804026  2855 master.cpp:1725] Elected as the leading master!
I0206 00:22:44.804059  2855 master.cpp:1470] Recovering from registrar
I0206 00:22:44.804424  2855 registrar.cpp:307] Recovering registrar
I0206 00:22:44.805202  2855 log.cpp:659] Attempting to start the writer
I0206 00:22:44.806782  2856 replica.cpp:493] Replica received implicit promise request from (9475)@172.17.0.2:43484 with proposal 1
I0206 00:22:44.807368  2856 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547939ns
I0206 00:22:44.807395  2856 replica.cpp:342] Persisted promised to 1
I0206 00:22:44.808375  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 00:22:44.809460  2848 replica.cpp:388] Replica received explicit promise request from (9476)@172.17.0.2:43484 for position 0 with proposal 2
I0206 00:22:44.809929  2848 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 427561ns
I0206 00:22:44.809967  2848 replica.cpp:712] Persisted action at 0
I0206 00:22:44.811035  2850 replica.cpp:537] Replica received write request for position 0 from (9477)@172.17.0.2:43484
I0206 00:22:44.811149  2850 leveldb.cpp:436] Reading position from leveldb took 36452ns
I0206 00:22:44.811532  2850 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 318924ns
I0206 00:22:44.811615  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.812532  2850 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 00:22:44.813117  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 476530ns
I0206 00:22:44.813143  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.813166  2850 replica.cpp:697] Replica learned NOP action at position 0
I0206 00:22:44.813984  2848 log.cpp:675] Writer started with ending position 0
I0206 00:22:44.815549  2848 leveldb.cpp:436] Reading position from leveldb took 31800ns
I0206 00:22:44.817061  2848 registrar.cpp:340] Successfully fetched the registry (0B) in 12.591104ms
I0206 00:22:44.817319  2848 registrar.cpp:439] Applied 1 operations in 63480ns; attempting to update the 'registry'
I0206 00:22:44.818780  2845 log.cpp:683] Attempting to append 170 bytes to the log
I0206 00:22:44.818981  2845 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 00:22:44.819941  2845 replica.cpp:537] Replica received write request for position 1 from (9478)@172.17.0.2:43484
I0206 00:22:44.820582  2845 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 600949ns
I0206 00:22:44.820608  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821552  2845 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 00:22:44.821934  2845 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 352813ns
I0206 00:22:44.821960  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821979  2845 replica.cpp:697] Replica learned APPEND action at position 1
I0206 00:22:44.823447  2845 registrar.cpp:484] Successfully updated the 'registry' in 5.987072ms
I0206 00:22:44.823580  2845 registrar.cpp:370] Successfully recovered registrar
I0206 00:22:44.823833  2845 log.cpp:702] Attempting to truncate the log to 1
I0206 00:22:44.824203  2845 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 00:22:44.824291  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 00:22:44.824645  2845 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 00:22:44.825222  2850 replica.cpp:537] Replica received write request for position 2 from (9479)@172.17.0.2:43484
I0206 00:22:44.825742  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 481617ns
I0206 00:22:44.825772  2850 replica.cpp:712] Persisted action at 2
I0206 00:22:44.826748  2852 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 00:22:44.827368  2852 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 588591ns
I0206 00:22:44.827432  2852 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33059ns
I0206 00:22:44.827450  2852 replica.cpp:712] Persisted action at 2
I0206 00:22:44.827468  2852 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 00:22:44.838011  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 00:22:44.838873  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 00:22:44.843785  2857 slave.cpp:193] Slave started on 172.17.0.2:43484
I0206 00:22:44.843819  2857 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw""
I0206 00:22:44.844292  2857 credentials.hpp:83] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential'
I0206 00:22:44.844518  2857 slave.cpp:324] Slave using credential for: test-principal
I0206 00:22:44.844696  2857 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 00:22:44.845243  2857 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.845326  2857 slave.cpp:472] Slave attributes: [  ]
I0206 00:22:44.845342  2857 slave.cpp:477] Slave hostname: 6632562f1ade
I0206 00:22:44.845953  2824 sched.cpp:222] Version: 0.28.0
I0206 00:22:44.846853  2848 sched.cpp:326] New master detected at master@172.17.0.2:43484
I0206 00:22:44.846936  2848 sched.cpp:382] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.846958  2848 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 00:22:44.847692  2858 state.cpp:58] Recovering state from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta'
I0206 00:22:44.848108  2850 status_update_manager.cpp:200] Recovering status update manager
I0206 00:22:44.848325  2852 containerizer.cpp:397] Recovering containerizer
I0206 00:22:44.848603  2845 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.849719  2845 master.cpp:5523] Authenticating scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.850052  2852 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.850227  2854 provisioner.cpp:245] Provisioner recovery complete
I0206 00:22:44.850410  2852 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.850692  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.850720  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.850805  2852 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.850862  2852 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.850939  2852 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.851027  2852 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.851052  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.851063  2852 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.851102  2852 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.851121  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.851130  2852 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851136  2852 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851150  2852 authenticator.cpp:317] Authentication success
I0206 00:22:44.851219  2850 authenticatee.cpp:298] Authentication success
I0206 00:22:44.851310  2850 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.851485  2849 slave.cpp:4496] Finished recovery
I0206 00:22:44.852154  2843 sched.cpp:471] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.852175  2843 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.2:43484
I0206 00:22:44.852262  2843 sched.cpp:809] Will retry registration in 939.183679ms if necessary
I0206 00:22:44.852375  2844 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.852448  2844 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 00:22:44.852699  2852 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.852782  2844 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0206 00:22:44.853056  2849 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 00:22:44.853421  2856 hierarchical.cpp:265] Added framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853513  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.853582  2844 sched.cpp:703] Framework registered with 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853613  2852 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 00:22:44.853663  2844 sched.cpp:717] Scheduler::registered took 53762ns
I0206 00:22:44.853899  2843 slave.cpp:796] New master detected at master@172.17.0.2:43484
I0206 00:22:44.853955  2854 status_update_manager.cpp:174] Pausing sending status updates
I0206 00:22:44.853997  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.853960  2843 slave.cpp:859] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.854035  2843 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 00:22:44.854030  2856 hierarchical.cpp:1096] Performed allocation for 0 slaves in 581355ns
I0206 00:22:44.854182  2843 slave.cpp:832] Detecting new master
I0206 00:22:44.854277  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.854517  2843 master.cpp:5523] Authenticating slave@172.17.0.2:43484
I0206 00:22:44.854603  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.854836  2855 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.855013  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.855044  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.855139  2855 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.855186  2855 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.855263  2855 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.855352  2855 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.855381  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.855389  2855 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.855419  2855 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.855438  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.855448  2855 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855453  2855 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855464  2855 authenticator.cpp:317] Authentication success
I0206 00:22:44.855540  2851 authenticatee.cpp:298] Authentication success
I0206 00:22:44.855721  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.855832  2852 slave.cpp:927] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.855615  2855 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave@172.17.0.2:43484
I0206 00:22:44.855973  2852 slave.cpp:1321] Will retry registration in 9.327708ms if necessary
I0206 00:22:44.856145  2854 master.cpp:4237] Registering slave at slave@172.17.0.2:43484 (6632562f1ade) with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.856598  2851 registrar.cpp:439] Applied 1 operations in 59112ns; attempting to update the 'registry'
I0206 00:22:44.857403  2851 log.cpp:683] Attempting to append 339 bytes to the log
I0206 00:22:44.857525  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 00:22:44.858482  2844 replica.cpp:537] Replica received write request for position 3 from (9493)@172.17.0.2:43484
I0206 00:22:44.858755  2844 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 228484ns
I0206 00:22:44.858855  2844 replica.cpp:712] Persisted action at 3
I0206 00:22:44.859751  2852 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 00:22:44.860332  2852 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 549638ns
I0206 00:22:44.860358  2852 replica.cpp:712] Persisted action at 3
I0206 00:22:44.860411  2852 replica.cpp:697] Replica learned APPEND action at position 3
I0206 00:22:44.862709  2856 registrar.cpp:484] Successfully updated the 'registry' in 6.0",1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0
MESOS-4615,1.0,"ContainerLoggerTest.DefaultToSandbox is flaky Just saw this failure on the ASF CI:

","<code>
[ RUN      ] ContainerLoggerTest.DefaultToSandbox
I0206 01:25:03.766458  2824 leveldb.cpp:174] Opened db in 72.979786ms
I0206 01:25:03.811712  2824 leveldb.cpp:181] Compacted db in 45.162067ms
I0206 01:25:03.811810  2824 leveldb.cpp:196] Created db iterator in 26090ns
I0206 01:25:03.811828  2824 leveldb.cpp:202] Seeked to beginning of db in 3173ns
I0206 01:25:03.811839  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 497ns
I0206 01:25:03.811900  2824 replica.cpp:779] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0206 01:25:03.812785  2849 recover.cpp:447] Starting replica recovery
I0206 01:25:03.813043  2849 recover.cpp:473] Replica is in EMPTY status
I0206 01:25:03.814668  2854 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (371)@172.17.0.8:37843
I0206 01:25:03.815210  2849 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 01:25:03.815732  2854 recover.cpp:564] Updating replica status to STARTING
I0206 01:25:03.819664  2857 master.cpp:376] Master 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de (74ef606c4063) started on 172.17.0.8:37843
I0206 01:25:03.819703  2857 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/h5vu5I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/h5vu5I/master"" --zk_session_timeout=""10secs""
I0206 01:25:03.820241  2857 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 01:25:03.820257  2857 master.cpp:428] Master only allowing authenticated slaves to register
I0206 01:25:03.820269  2857 credentials.hpp:35] Loading credentials for authentication from '/tmp/h5vu5I/credentials'
I0206 01:25:03.821110  2857 master.cpp:468] Using default 'crammd5' authenticator
I0206 01:25:03.821311  2857 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 01:25:03.821636  2857 master.cpp:571] Authorization enabled
I0206 01:25:03.821979  2846 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 01:25:03.822057  2846 whitelist_watcher.cpp:77] No whitelist given
I0206 01:25:03.825460  2847 master.cpp:1712] The newly elected leader is master@172.17.0.8:37843 with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de
I0206 01:25:03.825512  2847 master.cpp:1725] Elected as the leading master!
I0206 01:25:03.825533  2847 master.cpp:1470] Recovering from registrar
I0206 01:25:03.825835  2847 registrar.cpp:307] Recovering registrar
I0206 01:25:03.848212  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 32.226093ms
I0206 01:25:03.848299  2854 replica.cpp:320] Persisted replica status to STARTING
I0206 01:25:03.848702  2854 recover.cpp:473] Replica is in STARTING status
I0206 01:25:03.850728  2858 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (373)@172.17.0.8:37843
I0206 01:25:03.851230  2854 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 01:25:03.852018  2854 recover.cpp:564] Updating replica status to VOTING
I0206 01:25:03.881681  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.184163ms
I0206 01:25:03.881772  2854 replica.cpp:320] Persisted replica status to VOTING
I0206 01:25:03.882058  2854 recover.cpp:578] Successfully joined the Paxos group
I0206 01:25:03.882258  2854 recover.cpp:462] Recover process terminated
I0206 01:25:03.883076  2854 log.cpp:659] Attempting to start the writer
I0206 01:25:03.885040  2854 replica.cpp:493] Replica received implicit promise request from (374)@172.17.0.8:37843 with proposal 1
I0206 01:25:03.915132  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.980589ms
I0206 01:25:03.915215  2854 replica.cpp:342] Persisted promised to 1
I0206 01:25:03.916038  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 01:25:03.917659  2856 replica.cpp:388] Replica received explicit promise request from (375)@172.17.0.8:37843 for position 0 with proposal 2
I0206 01:25:03.948698  2856 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.974607ms
I0206 01:25:03.948786  2856 replica.cpp:712] Persisted action at 0
I0206 01:25:03.950920  2849 replica.cpp:537] Replica received write request for position 0 from (376)@172.17.0.8:37843
I0206 01:25:03.951011  2849 leveldb.cpp:436] Reading position from leveldb took 44263ns
I0206 01:25:03.982026  2849 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 30.947321ms
I0206 01:25:03.982225  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:03.983867  2849 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 01:25:04.015499  2849 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 30.957888ms
I0206 01:25:04.015591  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:04.015682  2849 replica.cpp:697] Replica learned NOP action at position 0
I0206 01:25:04.016666  2849 log.cpp:675] Writer started with ending position 0
I0206 01:25:04.017881  2855 leveldb.cpp:436] Reading position from leveldb took 56779ns
I0206 01:25:04.018934  2852 registrar.cpp:340] Successfully fetched the registry (0B) in 193.048064ms
I0206 01:25:04.019076  2852 registrar.cpp:439] Applied 1 operations in 38180ns; attempting to update the 'registry'
I0206 01:25:04.020100  2844 log.cpp:683] Attempting to append 170 bytes to the log
I0206 01:25:04.020288  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 01:25:04.021323  2844 replica.cpp:537] Replica received write request for position 1 from (377)@172.17.0.8:37843
I0206 01:25:04.054726  2844 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.309419ms
I0206 01:25:04.054818  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.055933  2844 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 01:25:04.088142  2844 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 32.116643ms
I0206 01:25:04.088230  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.088265  2844 replica.cpp:697] Replica learned APPEND action at position 1
I0206 01:25:04.090070  2856 registrar.cpp:484] Successfully updated the 'registry' in 70.90816ms
I0206 01:25:04.090338  2851 log.cpp:702] Attempting to truncate the log to 1
I0206 01:25:04.090358  2856 registrar.cpp:370] Successfully recovered registrar
I0206 01:25:04.090507  2847 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 01:25:04.090867  2858 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 01:25:04.091449  2858 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 01:25:04.092280  2857 replica.cpp:537] Replica received write request for position 2 from (378)@172.17.0.8:37843
I0206 01:25:04.125702  2857 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 33.192265ms
I0206 01:25:04.125804  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.127400  2857 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 01:25:04.157727  2857 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 30.268594ms
I0206 01:25:04.157905  2857 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88436ns
I0206 01:25:04.157941  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.157984  2857 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 01:25:04.166174  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 01:25:04.166954  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 01:25:04.172008  2844 slave.cpp:193] Slave started on 9)@172.17.0.8:37843
I0206 01:25:04.172046  2844 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw""
I0206 01:25:04.172569  2844 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential'
I0206 01:25:04.172886  2844 slave.cpp:324] Slave using credential for: test-principal
I0206 01:25:04.173141  2844 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 01:25:04.173620  2844 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.173686  2844 slave.cpp:472] Slave attributes: [  ]
I0206 01:25:04.173702  2844 slave.cpp:477] Slave hostname: 74ef606c4063
I0206 01:25:04.174816  2847 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta'
I0206 01:25:04.175441  2847 status_update_manager.cpp:200] Recovering status update manager
I0206 01:25:04.175678  2858 containerizer.cpp:397] Recovering containerizer
I0206 01:25:04.177573  2858 provisioner.cpp:245] Provisioner recovery complete
I0206 01:25:04.178231  2847 slave.cpp:4496] Finished recovery
I0206 01:25:04.178834  2847 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 01:25:04.179405  2847 slave.cpp:796] New master detected at master@172.17.0.8:37843
I0206 01:25:04.179500  2847 slave.cpp:859] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.179525  2847 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 01:25:04.179656  2858 status_update_manager.cpp:174] Pausing sending status updates
I0206 01:25:04.179798  2847 slave.cpp:832] Detecting new master
I0206 01:25:04.179891  2852 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.179916  2847 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 01:25:04.180286  2847 master.cpp:5523] Authenticating slave(9)@172.17.0.8:37843
I0206 01:25:04.180569  2847 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.181000  2847 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.181315  2847 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.181387  2847 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.181562  2847 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.181648  2847 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.181843  2847 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.182034  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.182071  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.182093  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.182145  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.182173  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.182185  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182193  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182211  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.182333  2849 authenticatee.cpp:298] Authentication success
I0206 01:25:04.182422  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(9)@172.17.0.8:37843
I0206 01:25:04.182510  2853 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.182945  2849 slave.cpp:927] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.183178  2849 slave.cpp:1321] Will retry registration in 9.87937ms if necessary
I0206 01:25:04.183466  2852 master.cpp:4237] Registering slave at slave(9)@172.17.0.8:37843 (74ef606c4063) with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.184039  2845 registrar.cpp:439] Applied 1 operations in 89453ns; attempting to update the 'registry'
I0206 01:25:04.185288  2856 log.cpp:683] Attempting to append 339 bytes to the log
I0206 01:25:04.185672  2850 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 01:25:04.186674  2846 replica.cpp:537] Replica received write request for position 3 from (392)@172.17.0.8:37843
I0206 01:25:04.195863  2856 slave.cpp:1321] Will retry registration in 11.038094ms if necessary
I0206 01:25:04.196233  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.208094  2856 slave.cpp:1321] Will retry registration in 27.881223ms if necessary
I0206 01:25:04.208472  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.216698  2846 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 29.961291ms
I0206 01:25:04.216789  2846 replica.cpp:712] Persisted action at 3
I0206 01:25:04.218246  2845 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 01:25:04.237861  2846 slave.cpp:1321] Will retry registration in 1.006941ms if necessary
I0206 01:25:04.238221  2846 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.239858  2856 slave.cpp:1321] Will retry registration in 167.305686ms if necessary
I0206 01:25:04.240044  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.241482  2845 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 23.193162ms
I0206 01:25:04.241524  2845 replica.cpp:712] Persisted action at 3
I0206 01:25:04.241557  2845 replica.cpp:697] Replica learned APPEND action at position 3
I0206 01:25:04.243746  2844 registrar.cpp:484] Successfully updated the 'registry' in 59.587072ms
I0206 01:25:04.244210  2857 log.cpp:702] Attempting to truncate the log to 3
I0206 01:25:04.244344  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 01:25:04.244597  2856 master.cpp:4305] Registered slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.244746  2843 slave.cpp:3436] Received ping from slave-observer(8)@172.17.0.8:37843
I0206 01:25:04.244976  2845 hierarchical.cpp:473] Added slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 01:25:04.245072  2843 slave.cpp:971] Registered with master master@172.17.0.8:37843; given slave ID 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.245121  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 01:25:04.245146  2845 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.245178  2845 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 159744ns
I0206 01:25:04.245465  2846 status_update_manager.cpp:181] Resuming sending status updates
I0206 01:25:04.245776  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/slave.info'
I0206 01:25:04.245745  2846 replica.cpp:537] Replica received write request for position 4 from (393)@172.17.0.8:37843
I0206 01:25:04.246273  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 01:25:04.246507  2850 master.cpp:4646] Received update of slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with total oversubscribed resources 
I0206 01:25:04.247180  2824 sched.cpp:222] Version: 0.28.0
I0206 01:25:04.247155  2850 hierarchical.cpp:531] Slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0206 01:25:04.247357  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.247406  2850 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 183250ns
I0206 01:25:04.247938  2854 sched.cpp:326] New master detected at master@172.17.0.8:37843
I0206 01:25:04.248157  2854 sched.cpp:382] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.248265  2854 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 01:25:04.248769  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.249311  2854 master.cpp:5523] Authenticating scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.249646  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(33)@172",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-4619,1.0,Remove markdown files from doxygen pages The doxygen html pages corresponding to doc/* markdown files are redundant and have broken links. They don't serve any reasonable purpose in doxygen site.,,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4623,3.0,"Add a stub Nvidia GPU isolator. We'll first wire up a skeleton Nvidia GPU isolator, which needs to be guarded by a configure flag due to the dependency on NVML.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4624,1.0,"Add allocation metrics for ""gpus"" resources. Allocation metrics are currently hard-coded to include only {{\[""cpus"", ""mem"", ""disk""\]}} resources. We'll need to add ""gpus"" to the list to start, possibly following up on the TODO to remove the hard-coding.

See:
https://github.com/apache/mesos/blob/0.27.0/src/master/metrics.cpp#L266-L269
https://github.com/apache/mesos/blob/0.27.0/src/slave/metrics.cpp#L123-L126
",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-4625,5.0,"Implement Nvidia GPU isolation w/o filesystem isolation enabled. The Nvidia GPU isolator will need to use the device cgroup to restrict access to GPU resources, and will need to recover this information after agent failover. For now this will require that the operator specifies the GPU devices via a flag.

To handle filesystem isolation requires that we provide mechanisms for operators to inject volumes with the necessary libraries into all containers using GPU resources, we'll tackle this in a separate ticket.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4657,1.0,Add LOG(INFO) in `cgroups/net_cls` for debugging allocation of net_cls handles. We need to add LOG(INFO) during the prepare phase of `cgroups/net_cls` for debugging management of `net_cls` handles within the isolator. ,,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4669,2.0,Add common compression utility We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4683,2.0,"Document docker runtime isolator. Should include the following information:

*What features are currently supported in docker runtime isolator.
*How to use the docker runtime isolator (user manual).
*Compare the different semantics v.s. docker containerizer, and explain why.",,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4684,3.0,"Create base docker image for test suite. This should be widely used for unified containerizer testing. Should basically include:

*at least one layer.
*repositories.

For each layer:
*root file system as a layer tar ball.
*docker image json (manifest).
*docker version.",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4702,1.0,"Document default value of ""offer_timeout"" There isn't a default value (i.e., offers do not timeout by default), but we should clarify this in {{flags.cpp}} and {{configuration.md}}.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4703,1.0,"Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent) Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.

We should make a StoutConfigure.cmake that can be included by any package downstream.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-4713,2.0,"ReviewBot should not fail hard if there are circular dependencies in a review chain Instead of failing hard, ReviewBot should post an error to the review that a circular dependency is detected.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0
MESOS-4714,2.0,"""make DESTDIR=<path> install"" broken There is a missing '$(DESTDIR)' prefix in the install-data-hook that causes DESTDIR builds to be broken.",,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4731,3.0,Update /frameworks to use jsonify This should let us remove the duplicated code in {{http.cpp}} between {{model(Framework)}} and {{json(Full<framework>)}}.</framework>,,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4747,1.0,"ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation Some cleanup of spawned processes is missing in {{ContainerLoggerTest.MesosContainerizerRecover}} so that when the test is run in isolation the global teardown might find lingering processes.
","<code>
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from ContainerLoggerTest
[ RUN      ] ContainerLoggerTest.MesosContainerizerRecover
[       OK ] ContainerLoggerTest.MesosContainerizerRecover (13 ms)
[----------] 1 test from ContainerLoggerTest (13 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:728: Failure
Failed
Tests completed with child processes remaining:
-+- 7112 /SOME/PATH/src/mesos/build/src/.libs/mesos-tests --gtest_filter=ContainerLoggerTest.MesosContainerizerRecover
 \--- 7130 (sh)
[==========] 1 test from 1 test case ran. (23 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 0 tests, listed below:

 0 FAILED TESTS
<code>

Observered on OS X with clang-trunk and an unoptimized build.
</code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-4748,3.0,Add Appc image fetcher tests. Mesos now has support for fetching Appc images. Add tests that verifies the new component.,,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4754,2.0,"The ""executors"" field is exposed under a backwards incompatible schema. In 0.26.0, the master's {{/state}} endpoint generated the following:

","<code>
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""argv"": [],
            ""uris"": [],
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": ""default"",
          ""framework_id"": ""0ea528a9-64ba-417f-98ea-9c4b8d418db6-0000"",
          ""name"": ""Long Lived Executor (C++)"",
          ""resources"": {
            ""cpus"": 0,
            ""disk"": 0,
            ""mem"": 0
          },
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
<code>

In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:

<code>
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""shell"": true,
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": {
            ""value"": ""default""
          },
          ""framework_id"": {
            ""value"": ""368a5a49-480b-41f6-a13b-24a69c92a72e-0000""
          },
          ""name"": ""Long Lived Executor (C++)"",
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0"",
          ""source"": ""cpp_long_lived_framework""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
<code>

This is a backwards incompatible API change.</code></code></code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4768,1.0,"MasterMaintenanceTest.InverseOffers is flaky [MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].

Verbose logs from ASF Centos7 build:
","<code>
[ RUN      ] MasterMaintenanceTest.InverseOffers
I0224 22:35:53.714018  1948 leveldb.cpp:174] Opened db in 2.034387ms
I0224 22:35:53.714663  1948 leveldb.cpp:181] Compacted db in 608839ns
I0224 22:35:53.714709  1948 leveldb.cpp:196] Created db iterator in 19043ns
I0224 22:35:53.714844  1948 leveldb.cpp:202] Seeked to beginning of db in 2330ns
I0224 22:35:53.714956  1948 leveldb.cpp:271] Iterated through 0 keys in the db in 518ns
I0224 22:35:53.715092  1948 replica.cpp:779] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0224 22:35:53.715646  1968 recover.cpp:447] Starting replica recovery
I0224 22:35:53.715915  1981 recover.cpp:473] Replica is in EMPTY status
I0224 22:35:53.717067  1972 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4533)@172.17.0.1:36678
I0224 22:35:53.717445  1981 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0224 22:35:53.717888  1978 recover.cpp:564] Updating replica status to STARTING
I0224 22:35:53.718585  1979 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 525061ns
I0224 22:35:53.718618  1979 replica.cpp:320] Persisted replica status to STARTING
I0224 22:35:53.718827  1982 recover.cpp:473] Replica is in STARTING status
I0224 22:35:53.719728  1969 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4534)@172.17.0.1:36678
I0224 22:35:53.719974  1971 recover.cpp:193] Received a recover response from a replica in STARTING status
I0224 22:35:53.720369  1970 recover.cpp:564] Updating replica status to VOTING
I0224 22:35:53.720789  1982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 322308ns
I0224 22:35:53.720823  1982 replica.cpp:320] Persisted replica status to VOTING
I0224 22:35:53.720968  1982 recover.cpp:578] Successfully joined the Paxos group
I0224 22:35:53.721101  1982 recover.cpp:462] Recover process terminated
I0224 22:35:53.721698  1982 master.cpp:376] Master aab18b61-7811-4c43-a672-d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678
I0224 22:35:53.721719  1982 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MjbcWP/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MjbcWP/master"" --zk_session_timeout=""10secs""
I0224 22:35:53.722039  1982 master.cpp:425] Master allowing unauthenticated frameworks to register
I0224 22:35:53.722053  1982 master.cpp:428] Master only allowing authenticated slaves to register
I0224 22:35:53.722061  1982 credentials.hpp:35] Loading credentials for authentication from '/tmp/MjbcWP/credentials'
I0224 22:35:53.722394  1982 master.cpp:468] Using default 'crammd5' authenticator
I0224 22:35:53.722525  1982 master.cpp:537] Using default 'basic' HTTP authenticator
I0224 22:35:53.722661  1982 master.cpp:571] Authorization enabled
I0224 22:35:53.722813  1968 hierarchical.cpp:144] Initialized hierarchical allocator process
I0224 22:35:53.722846  1980 whitelist_watcher.cpp:77] No whitelist given
I0224 22:35:53.724957  1977 master.cpp:1712] The newly elected leader is master@172.17.0.1:36678 with id aab18b61-7811-4c43-a672-d1a63818c880
I0224 22:35:53.725000  1977 master.cpp:1725] Elected as the leading master!
I0224 22:35:53.725023  1977 master.cpp:1470] Recovering from registrar
I0224 22:35:53.725306  1967 registrar.cpp:307] Recovering registrar
I0224 22:35:53.725808  1977 log.cpp:659] Attempting to start the writer
I0224 22:35:53.727145  1973 replica.cpp:493] Replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1
I0224 22:35:53.727728  1973 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 424560ns
I0224 22:35:53.727828  1973 replica.cpp:342] Persisted promised to 1
I0224 22:35:53.729080  1973 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0224 22:35:53.731009  1979 replica.cpp:388] Replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2
I0224 22:35:53.731580  1979 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 478479ns
I0224 22:35:53.731613  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.734354  1979 replica.cpp:537] Replica received write request for position 0 from (4538)@172.17.0.1:36678
I0224 22:35:53.734485  1979 leveldb.cpp:436] Reading position from leveldb took 60879ns
I0224 22:35:53.735877  1979 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.324061ms
I0224 22:35:53.735930  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.737061  1970 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0224 22:35:53.738881  1970 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.772814ms
I0224 22:35:53.738939  1970 replica.cpp:712] Persisted action at 0
I0224 22:35:53.738975  1970 replica.cpp:697] Replica learned NOP action at position 0
I0224 22:35:53.740136  1976 log.cpp:675] Writer started with ending position 0
I0224 22:35:53.741750  1976 leveldb.cpp:436] Reading position from leveldb took 74863ns
I0224 22:35:53.743479  1976 registrar.cpp:340] Successfully fetched the registry (0B) in 18.11968ms
I0224 22:35:53.743755  1976 registrar.cpp:439] Applied 1 operations in 56670ns; attempting to update the 'registry'
I0224 22:35:53.745604  1978 log.cpp:683] Attempting to append 170 bytes to the log
I0224 22:35:53.745905  1977 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0224 22:35:53.746968  1981 replica.cpp:537] Replica received write request for position 1 from (4539)@172.17.0.1:36678
I0224 22:35:53.747480  1981 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 456947ns
I0224 22:35:53.747609  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.750448  1981 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0224 22:35:53.751158  1981 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 535163ns
I0224 22:35:53.751258  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.751389  1981 replica.cpp:697] Replica learned APPEND action at position 1
I0224 22:35:53.753149  1979 registrar.cpp:484] Successfully updated the 'registry' in 9.228032ms
I0224 22:35:53.753324  1979 registrar.cpp:370] Successfully recovered registrar
I0224 22:35:53.753593  1979 log.cpp:702] Attempting to truncate the log to 1
I0224 22:35:53.753805  1979 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0224 22:35:53.754055  1981 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0224 22:35:53.754349  1979 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0224 22:35:53.755764  1977 replica.cpp:537] Replica received write request for position 2 from (4540)@172.17.0.1:36678
I0224 22:35:53.756459  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 488559ns
I0224 22:35:53.756561  1977 replica.cpp:712] Persisted action at 2
I0224 22:35:53.757932  1972 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0224 22:35:53.758400  1972 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 343827ns
I0224 22:35:53.758539  1972 leveldb.cpp:399] Deleting ~1 keys from leveldb took 34231ns
I0224 22:35:53.758658  1972 replica.cpp:712] Persisted action at 2
I0224 22:35:53.758782  1972 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0224 22:35:53.778059  1978 slave.cpp:193] Slave started on 115)@172.17.0.1:36678
I0224 22:35:53.778105  1978 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF""
I0224 22:35:53.778609  1978 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential'
I0224 22:35:53.779175  1978 slave.cpp:324] Slave using credential for: test-principal
I0224 22:35:53.779520  1978 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0224 22:35:53.780192  1978 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.780362  1978 slave.cpp:472] Slave attributes: [  ]
I0224 22:35:53.780483  1978 slave.cpp:477] Slave hostname: maintenance-host
I0224 22:35:53.782126  1967 state.cpp:58] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta'
I0224 22:35:53.782892  1969 status_update_manager.cpp:200] Recovering status update manager
I0224 22:35:53.783242  1969 slave.cpp:4565] Finished recovery
I0224 22:35:53.784001  1969 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0224 22:35:53.784678  1969 slave.cpp:796] New master detected at master@172.17.0.1:36678
I0224 22:35:53.784874  1967 status_update_manager.cpp:174] Pausing sending status updates
I0224 22:35:53.784808  1969 slave.cpp:859] Authenticating with master master@172.17.0.1:36678
I0224 22:35:53.784945  1969 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0224 22:35:53.785181  1969 slave.cpp:832] Detecting new master
I0224 22:35:53.785326  1969 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0224 22:35:53.785557  1969 authenticatee.cpp:121] Creating new client SASL connection
I0224 22:35:53.786227  1969 master.cpp:5526] Authenticating slave(115)@172.17.0.1:36678
I0224 22:35:53.786492  1969 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.786962  1969 authenticator.cpp:98] Creating new server SASL connection
I0224 22:35:53.787274  1969 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0224 22:35:53.787308  1969 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 22:35:53.787400  1969 authenticator.cpp:203] Received SASL authentication start
I0224 22:35:53.787470  1969 authenticator.cpp:325] Authentication requires more steps
I0224 22:35:53.787884  1972 authenticatee.cpp:258] Received SASL authentication step
I0224 22:35:53.787992  1972 authenticator.cpp:231] Received SASL authentication step
I0224 22:35:53.788027  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 22:35:53.788040  1972 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0224 22:35:53.788090  1972 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 22:35:53.788122  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 22:35:53.788136  1972 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788146  1972 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788164  1972 authenticator.cpp:317] Authentication success
I0224 22:35:53.788331  1972 authenticatee.cpp:298] Authentication success
I0224 22:35:53.788439  1972 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(115)@172.17.0.1:36678
I0224 22:35:53.788529  1972 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.788988  1972 slave.cpp:927] Successfully authenticated with master master@172.17.0.1:36678
I0224 22:35:53.789139  1972 slave.cpp:1321] Will retry registration in 1.535786ms if necessary
I0224 22:35:53.789515  1972 master.cpp:4240] Registering slave at slave(115)@172.17.0.1:36678 (maintenance-host) with id aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.790577  1972 registrar.cpp:439] Applied 1 operations in 78745ns; attempting to update the 'registry'
I0224 22:35:53.791128  1971 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule'
I0224 22:35:53.791877  1971 http.cpp:501] HTTP POST for /master/maintenance/schedule from 172.17.0.1:45095
I0224 22:35:53.793313  1972 log.cpp:683] Attempting to append 343 bytes to the log
I0224 22:35:53.793586  1972 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0224 22:35:53.794533  1971 replica.cpp:537] Replica received write request for position 3 from (4547)@172.17.0.1:36678
I0224 22:35:53.794862  1971 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 283614ns
I0224 22:35:53.794893  1971 replica.cpp:712] Persisted action at 3
I0224 22:35:53.796646  1979 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0224 22:35:53.797102  1972 slave.cpp:1321] Will retry registration in 17.198963ms if necessary
I0224 22:35:53.797186  1979 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 498502ns
I0224 22:35:53.797230  1979 replica.cpp:712] Persisted action at 3
I0224 22:35:53.797260  1979 replica.cpp:697] Replica learned APPEND action at position 3
I0224 22:35:53.797417  1972 master.cpp:4228] Ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenance-host) as admission is already in progress
I0224 22:35:53.799119  1978 registrar.cpp:484] Successfully updated the 'registry' in 8.45824ms
I0224 22:35:53.799613  1978 registrar.cpp:439] Applied 1 operations in 176193ns; attempting to update the 'registry'
I0224 22:35:53.800472  1972 master.cpp:4308] Registered slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.800623  1978 log.cpp:702] Attempting to truncate the log to 3
I0224 22:35:53.801255  1969 hierarchical.cpp:473] Added slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0224 22:35:53.801301  1978 slave.cpp:971] Registered with master master@172.17.0.1:36678; given slave ID aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.801331  1978 fetcher.cpp:81] Clearing fetcher cache
I0224 22:35:53.801431  1969 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.801466  1969 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 162751ns
I0224 22:35:53.801532  1969 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0224 22:35:53.801867  1978 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/slave.info'
I0224 22:35:53.801877  1969 status_update_manager.cpp:181] Resuming sending status updates
I0224 22:35:53.802898  1977 replica.cpp:537] Replica received write request for position 4 from (4548)@172.17.0.1:36678
I0224 22:35:53.803252  1978 slave.cpp:1030] Forwarding total oversubscribed resources 
I0224 22:35:53.803640  1970 master.cpp:4649] Received update of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with total oversubscribed resources 
I0224 22:35:53.803858  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 912626ns
I0224 22:35:53.803889  1977 replica.cpp:712] Persisted action at 4
I0224 22:35:53.804144  1978 slave.cpp:3482] Received ping from slave-observer(117)@172.17.0.1:36678
I0224 22:35:53.804535  1971 hierarchical.cpp:531] Slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0224 22:35:53.804684  1971 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.804714  1971 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 131453ns
I0224 22:35:53.805541  1967 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0224 22:35:53.805941  1967 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 366444ns
I0224 22:35:53.806015  1967 leveldb.cpp:399] Deleting ~2 keys from leveldb took 42808ns
I0224 22:35:53.806041  1967 replica.cpp:712] Persisted action at 4
I0224 22:35:53.806066  1967 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0224 22:35:53.807355  1978 log.cpp:683] Attempting to append 465 bytes to the log
I0224 22:35:53.807551  1978 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0224 22:35:53.809638  1979 replica.cpp:537] Replica received write request for position 5 from (4549)@172.17.0.1:36678
I0224 22:35:53.810858  1979 leveldb.cpp:341] Persisting action (484 bytes) to leveldb took 1.167663ms
I0224 22:35:53.810904  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.811997  1979 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0224 22:35:53.812348  1979 leveldb.cpp:341] Persisting action (486 bytes) to leveldb took 318928ns
I0224 22:35:53.812376  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.81239",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-4783,3.0,"Disable rate limiting of the global metrics endpoint for mesos-tests execution Once we can optionally disable rate limiting in the global metrics endpoint with MESOS-4776 we should disable the rate limiting during the execution of mesos-tests.

* rate limiting makes it cumbersome to repeatedly hit the endpoint since one would not want to interfere with the rate limiting
* rate limiting might incur additional wait time which might slown down tests",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-4784,1.0,"SlaveTest.MetricsSlaveLaunchErrors test relies on implicit blocking behavior hitting the global metrics endpoint The test attempts to observe a change in the {{slave/container_launch_errors}} metric, but does not wait for the triggering action to take place. Currently the test passes since hitting the endpoint blocks for some rate limit-related time which provides under many circumstances enough wait time for the action to take place. ",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-4807,1.0,"IOTest.BufferedRead writes to the current directory libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,

* should the test fail data might be leaked to random locations,
* the test cannot be executed from a write-only directory, or
* executing the same test in parallel would race on the existence of the created file, and show bogus behavior.

The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-4820,1.0,"Need to set `EXPOSED` ports from docker images into `ContainerConfig` Most docker images have an `EXPOSE` command associated with them. This tells the container run-time the TCP ports that the micro-service ""wishes"" to expose to the outside world. 

With the `Unified containerizer` project since `MesosContainerizer` is going to natively support docker images it is imperative that the Mesos container run time have a mechanism to expose ports listed in a Docker image. The first step to achieve this is to extract this information from the `Docker` image and set in the `ContainerConfig` . The `ContainerConfig` can then be used to pass this information to any isolator (for e.g. `network/cni` isolator) that will install port forwarding rules to expose the desired ports.",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4822,2.0,Add support for local image fetching in Appc provisioner. Currently Appc image provisioner supports http(s) fetching. It would be valuable to add support for local file path(URI) based  fetching.,,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4824,2.0,"""filesystem/linux"" isolator does not unmount orphaned persistent volumes A persistent volume can be orphaned when:
# A framework registers with checkpointing enabled.
# The framework starts a task + a persistent volume.
# The agent exits.  The task continues to run.
# Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent.
# The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.

The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}}) 
","<code>
I0229 23:55:42.078940  5635 linux.cpp:711] Ignoring cleanup request for unknown container: a35189d3-85d5-4d02-b568-67f675b6dc97
<code>

Test implemented here: https://reviews.apache.org/r/44122/</code></code>",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4825,1.0,Master's slave reregister logic does not update version field The master's logic for reregistering a slave does not update the version field if the slave re-registers with a new version.,,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4832,2.0,"DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes exits when the /tmp directory is bind-mounted If the {{/tmp}} directory (where Mesos tests create temporary directories) is a bind mount, the test suite will exit here:
","<code>
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes
I0226 03:17:26.722806  1097 leveldb.cpp:174] Opened db in 12.587676ms
I0226 03:17:26.723496  1097 leveldb.cpp:181] Compacted db in 636999ns
I0226 03:17:26.723536  1097 leveldb.cpp:196] Created db iterator in 18271ns
I0226 03:17:26.723547  1097 leveldb.cpp:202] Seeked to beginning of db in 1555ns
I0226 03:17:26.723554  1097 leveldb.cpp:271] Iterated through 0 keys in the db in 363ns
I0226 03:17:26.723593  1097 replica.cpp:779] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0226 03:17:26.724128  1117 recover.cpp:447] Starting replica recovery
I0226 03:17:26.724367  1117 recover.cpp:473] Replica is in EMPTY status
I0226 03:17:26.725237  1117 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13810)@172.30.2.151:51934
I0226 03:17:26.725744  1114 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0226 03:17:26.726356  1111 master.cpp:376] Master 5cc57c0e-f1ad-4107-893f-420ed1a1db1a (ip-172-30-2-151.mesosphere.io) started on 172.30.2.151:51934
I0226 03:17:26.726369  1118 recover.cpp:564] Updating replica status to STARTING
I0226 03:17:26.726378  1111 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/djHTVQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/djHTVQ/master"" --zk_session_timeout=""10secs""
I0226 03:17:26.726605  1111 master.cpp:423] Master only allowing authenticated frameworks to register
I0226 03:17:26.726616  1111 master.cpp:428] Master only allowing authenticated slaves to register
I0226 03:17:26.726632  1111 credentials.hpp:35] Loading credentials for authentication from '/tmp/djHTVQ/credentials'
I0226 03:17:26.726860  1111 master.cpp:468] Using default 'crammd5' authenticator
I0226 03:17:26.726977  1111 master.cpp:537] Using default 'basic' HTTP authenticator
I0226 03:17:26.727092  1111 master.cpp:571] Authorization enabled
I0226 03:17:26.727243  1118 hierarchical.cpp:144] Initialized hierarchical allocator process
I0226 03:17:26.727285  1116 whitelist_watcher.cpp:77] No whitelist given
I0226 03:17:26.728852  1114 master.cpp:1712] The newly elected leader is master@172.30.2.151:51934 with id 5cc57c0e-f1ad-4107-893f-420ed1a1db1a
I0226 03:17:26.728876  1114 master.cpp:1725] Elected as the leading master!
I0226 03:17:26.728891  1114 master.cpp:1470] Recovering from registrar
I0226 03:17:26.728977  1117 registrar.cpp:307] Recovering registrar
I0226 03:17:26.731503  1112 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 4.977811ms
I0226 03:17:26.731539  1112 replica.cpp:320] Persisted replica status to STARTING
I0226 03:17:26.731711  1111 recover.cpp:473] Replica is in STARTING status
I0226 03:17:26.732501  1114 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13812)@172.30.2.151:51934
I0226 03:17:26.732862  1111 recover.cpp:193] Received a recover response from a replica in STARTING status
I0226 03:17:26.733264  1117 recover.cpp:564] Updating replica status to VOTING
I0226 03:17:26.733836  1118 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 388246ns
I0226 03:17:26.733855  1118 replica.cpp:320] Persisted replica status to VOTING
I0226 03:17:26.733979  1113 recover.cpp:578] Successfully joined the Paxos group
I0226 03:17:26.734149  1113 recover.cpp:462] Recover process terminated
I0226 03:17:26.734478  1111 log.cpp:659] Attempting to start the writer
I0226 03:17:26.735523  1114 replica.cpp:493] Replica received implicit promise request from (13813)@172.30.2.151:51934 with proposal 1
I0226 03:17:26.736130  1114 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 576451ns
I0226 03:17:26.736150  1114 replica.cpp:342] Persisted promised to 1
I0226 03:17:26.736709  1115 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0226 03:17:26.737771  1114 replica.cpp:388] Replica received explicit promise request from (13814)@172.30.2.151:51934 for position 0 with proposal 2
I0226 03:17:26.738386  1114 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 583184ns
I0226 03:17:26.738404  1114 replica.cpp:712] Persisted action at 0
I0226 03:17:26.739312  1118 replica.cpp:537] Replica received write request for position 0 from (13815)@172.30.2.151:51934
I0226 03:17:26.739367  1118 leveldb.cpp:436] Reading position from leveldb took 26157ns
I0226 03:17:26.740638  1118 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.238477ms
I0226 03:17:26.740669  1118 replica.cpp:712] Persisted action at 0
I0226 03:17:26.741158  1118 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0226 03:17:26.742878  1118 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.697254ms
I0226 03:17:26.742902  1118 replica.cpp:712] Persisted action at 0
I0226 03:17:26.742916  1118 replica.cpp:697] Replica learned NOP action at position 0
I0226 03:17:26.743393  1117 log.cpp:675] Writer started with ending position 0
I0226 03:17:26.744370  1112 leveldb.cpp:436] Reading position from leveldb took 34329ns
I0226 03:17:26.745240  1117 registrar.cpp:340] Successfully fetched the registry (0B) in 16.21888ms
I0226 03:17:26.745350  1117 registrar.cpp:439] Applied 1 operations in 30460ns; attempting to update the 'registry'
I0226 03:17:26.746016  1111 log.cpp:683] Attempting to append 210 bytes to the log
I0226 03:17:26.746119  1116 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0226 03:17:26.746798  1114 replica.cpp:537] Replica received write request for position 1 from (13816)@172.30.2.151:51934
I0226 03:17:26.747251  1114 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 411333ns
I0226 03:17:26.747269  1114 replica.cpp:712] Persisted action at 1
I0226 03:17:26.747808  1113 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0226 03:17:26.749511  1113 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.673488ms
I0226 03:17:26.749534  1113 replica.cpp:712] Persisted action at 1
I0226 03:17:26.749550  1113 replica.cpp:697] Replica learned APPEND action at position 1
I0226 03:17:26.750422  1111 registrar.cpp:484] Successfully updated the 'registry' in 5.021952ms
I0226 03:17:26.750560  1111 registrar.cpp:370] Successfully recovered registrar
I0226 03:17:26.750635  1112 log.cpp:702] Attempting to truncate the log to 1
I0226 03:17:26.750751  1113 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0226 03:17:26.751096  1116 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
I0226 03:17:26.751126  1111 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0226 03:17:26.751561  1118 replica.cpp:537] Replica received write request for position 2 from (13817)@172.30.2.151:51934
I0226 03:17:26.751999  1118 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406823ns
I0226 03:17:26.752018  1118 replica.cpp:712] Persisted action at 2
I0226 03:17:26.752521  1113 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0226 03:17:26.754161  1113 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.614888ms
I0226 03:17:26.754210  1113 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26384ns
I0226 03:17:26.754225  1113 replica.cpp:712] Persisted action at 2
I0226 03:17:26.754240  1113 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0226 03:17:26.765103  1115 slave.cpp:193] Slave started on 399)@172.30.2.151:51934
I0226 03:17:26.765130  1115 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP""
I0226 03:17:26.765403  1115 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential'
I0226 03:17:26.765573  1115 slave.cpp:324] Slave using credential for: test-principal
I0226 03:17:26.765733  1115 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048
Trying semicolon-delimited string format instead
I0226 03:17:26.766185  1115 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0226 03:17:26.766242  1115 slave.cpp:472] Slave attributes: [  ]
I0226 03:17:26.766250  1115 slave.cpp:477] Slave hostname: ip-172-30-2-151.mesosphere.io
I0226 03:17:26.767325  1097 sched.cpp:222] Version: 0.28.0
I0226 03:17:26.767390  1111 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta'
I0226 03:17:26.767603  1115 status_update_manager.cpp:200] Recovering status update manager
I0226 03:17:26.767865  1113 docker.cpp:726] Recovering Docker containers
I0226 03:17:26.767971  1111 sched.cpp:326] New master detected at master@172.30.2.151:51934
I0226 03:17:26.768045  1111 sched.cpp:382] Authenticating with master master@172.30.2.151:51934
I0226 03:17:26.768059  1111 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0226 03:17:26.768070  1118 slave.cpp:4565] Finished recovery
I0226 03:17:26.768273  1112 authenticatee.cpp:121] Creating new client SASL connection
I0226 03:17:26.768435  1118 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0226 03:17:26.768565  1111 master.cpp:5526] Authenticating scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934
I0226 03:17:26.768661  1118 slave.cpp:796] New master detected at master@172.30.2.151:51934
I0226 03:17:26.768659  1115 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(839)@172.30.2.151:51934
I0226 03:17:26.768679  1113 status_update_manager.cpp:174] Pausing sending status updates
I0226 03:17:26.768728  1118 slave.cpp:859] Authenticating with master master@172.30.2.151:51934
I0226 03:17:26.768743  1118 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0226 03:17:26.768865  1118 slave.cpp:832] Detecting new master
I0226 03:17:26.768868  1112 authenticator.cpp:98] Creating new server SASL connection
I0226 03:17:26.768908  1114 authenticatee.cpp:121] Creating new client SASL connection
I0226 03:17:26.769003  1118 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0226 03:17:26.769103  1115 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0226 03:17:26.769131  1115 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0226 03:17:26.769209  1116 master.cpp:5526] Authenticating slave(399)@172.30.2.151:51934
I0226 03:17:26.769253  1114 authenticator.cpp:203] Received SASL authentication start
I0226 03:17:26.769295  1115 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(840)@172.30.2.151:51934
I0226 03:17:26.769307  1114 authenticator.cpp:325] Authentication requires more steps
I0226 03:17:26.769403  1117 authenticatee.cpp:258] Received SASL authentication step
I0226 03:17:26.769495  1114 authenticator.cpp:98] Creating new server SASL connection
I0226 03:17:26.769531  1115 authenticator.cpp:231] Received SASL authentication step
I0226 03:17:26.769554  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0226 03:17:26.769562  1115 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0226 03:17:26.769608  1115 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0226 03:17:26.769629  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0226 03:17:26.769637  1115 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0226 03:17:26.769642  1115 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0226 03:17:26.769654  1115 authenticator.cpp:317] Authentication success
I0226 03:17:26.769728  1117 authenticatee.cpp:298] Authentication success
I0226 03:17:26.769769  1112 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0226 03:17:26.769767  1118 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934
I0226 03:17:26.769803  1112 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0226 03:17:26.769798  1114 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(839)@172.30.2.151:51934
I0226 03:17:26.769881  1112 authenticator.cpp:203] Received SASL authentication start
I0226 03:17:26.769932  1112 authenticator.cpp:325] Authentication requires more steps
I0226 03:17:26.769981  1117 sched.cpp:471] Successfully authenticated with master master@172.30.2.151:51934
I0226 03:17:26.770004  1117 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.151:51934
I0226 03:17:26.770064  1118 authenticatee.cpp:258] Received SASL authentication step
I0226 03:17:26.770102  1117 sched.cpp:809] Will retry registration in 1.937819802secs if necessary
I0226 03:17:26.770165  1115 authenticator.cpp:231] Received SASL authentication step
I0226 03:17:26.770193  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0226 03:17:26.770207  1115 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0226 03:17:26.770213  1116 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934
I0226 03:17:26.770241  1115 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0226 03:17:26.770274  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0226 03:17:26.770277  1116 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0226 03:17:26.770298  1115 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0226 03:17:26.770331  1115 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0226 03:17:26.770349  1115 authenticator.cpp:317] Authentication success
I0226 03:17:26.770428  1118 authenticatee.cpp:298] Authentication success
I0226 03:17:26.770442  1116 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(399)@172.30.2.151:51934
I0226 03:17:26.770547  1116 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(840)@172.30.2.151:51934
I0226 03:17:26.770846  1116 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0226 03:17:26.770866  1118 slave.cpp:927] Successfully authenticated with master master@172.30.2.151:51934
I0226 03:17:26.770966  1118 slave.cpp:1321] Will retry registration in 1.453415ms if necessary
I0226 03:17:26.771225  1115 hierarchical.cpp:265] Added framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:26.771275  1118 sched.cpp:703] Framework registered with 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:26.771299  1115 hierarchical.cpp:1434] No resources available to allocate!
I0226 03:17:26.771328  1115 hierarchical.cpp:1529] No inverse offers to send out!
I0226 03:17:26.771344  1118 sched.cpp:717] Scheduler::registered took 50146ns
I0226 03:17:26.771356  1116 master.cpp:4240] Registering slave at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) with id 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0
I0226 03:17:26.771348  1115 hierarchical.cpp:1127] Performed allocation for 0 slaves in 101438ns
I0226 03:17:26.771860  1114 registrar.cpp:439] Applied 1 operations in 59672ns; attempting to update the 'registry'
I0226 03:17:26.772645  1117 log.cpp:683] Attempting to append 423 bytes to the log
I0226 03:17:26.772758  1112 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0226 03:17:26.773435  1117 replica.cpp:537] Replica received write request for position 3 from (13824)@172.30.2.151:51934
I0226 03:17:26.773586  1111 slave.cpp:1321] Will retry registration in 2.74261ms if necessary
I0226 03:17:26.773682  1115 master.cpp:4228] Ignoring register slave message from slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) as admission is already in progress
I0226 03:17:26.773937  1117 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 469969ns
I0226 03:17:26.773957  1117 replica.cpp:712] Persisted action at 3
I0226 03:17:26.774605  1114 replica",1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4833,5.0,"Poor allocator performance with labeled resources and/or persistent volumes Modifying the {{HierarchicalAllocator_BENCHMARK_Test.ResourceLabels}} benchmark from https://reviews.apache.org/r/43686/ to use distinct labels between different slaves, performance regresses from ~2 seconds to ~3 minutes. The culprit seems to be the way in which the allocator merges together resources; reserved resource labels (or persistent volume IDs) inhibit merging, which causes performance to be much worse.",,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4834,2.0,"Add 'file' fetcher plugin. Add support for ""file"" based URI fetcher. This could be useful for container image provisioning from local file system.",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4844,2.0,"Add authentication to master endpoints Before we can add authorization around operator endpoints, we need to add authentication support, so that unauthenticated requests are denied when --authenticate_http is enabled, and so that the principal is passed into `route()`.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0
MESOS-4848,2.0,"Agent Authn Research Spike Research the master authentication flags to see what changes will be necessary for agent http authentication.
Write up a 1-2 page summary/design doc.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0
MESOS-4854,1.0,Update CHANGELOG with net_cls isolator Need to update the CHANGELOG for 0.28 release.,,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4860,2.0,"Add a script to install the Nvidia GDK on a host. This script can be used to install the Nvidia GDK for Cuda 7.5 on a
mesos development machine. The purpose of the Nvidia GDK is to provide
all the necessary header files (nvml.h) and library files
(libnvidia-ml.so) necessary to build mesos with Nvidia GPU support.

If the machine on which Mesos is being compiled doesn't have any GPUs,
then libnvidia-ml.so consists only of stubs, allowing Mesos to build
and run, but not actually do anything useful under the hood. This
enables us to build a GPU-enabled mesos on a development machine
without GPUs and then deploy it to a production machine with GPUs and
be reasonably sure it will work.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4861,2.0,"Add configure flags to build with Nvidia GPU support. The configure flags can be used to enable Nvidia GPU support, as well as specify the installation directories of the nvml header and library files if not already installed in standard include/library paths on the system.

They will also be used to conditionally build support for Nvidia GPUs into Mesos.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4864,3.0,"Add flag to specify available Nvidia GPUs on an agent's command line. In the initial GPU support we will not do auto-discovery of GPUs on an agent.  As such, an operator will need to specify a flag on the command line, listing all of the GPUs available on the system.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4865,3.0,"Add GPUs as an explicit resource. We will add ""gpus"" as an explicitly recognized resource in Mesos, akin to cpus, memory, ports, and disk.  In the containerizer, we will verify that the number of GPU resources passed in via the --resources flag matches the list of GPUs passed in via the --nvidia_gpus flag.  In the future we will add autodiscovery so this matching is unnecessary.  However, we will always have to pass ""gpus"" as a resource to make any GPU available on the system (unlike for cpus and memory, where the default is probed).",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4877,3.0,"Mesos containerizer can't handle top level docker image like ""alpine"" (must use ""library/alpine"") This can be demonstrated with the {{mesos-execute}} command:

# Docker containerizer with image {{alpine}}: success
","<code>
sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=docker --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
<code>
# Mesos containerizer with image {{alpine}}: failure
<code>
sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
<code>
# Mesos containerizer with image {{library/alpine}}: success
<code>
sudo ./build/src/mesos-execute --docker_image=library/alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
<code>

In the slave logs:

<code>
ea-4460-83
9c-838da86af34c-0007'
I0306 16:32:41.418269  3403 metadata_manager.cpp:159] Looking for image 'alpine:latest'
I0306 16:32:41.418699  3403 registry_puller.cpp:194] Pulling image 'alpine:latest' from 'docker-manifest://registry-1.docker.io:443alpine?latest#https' to '/tmp/mesos-test
/store/docker/staging/ka7MlQ'
E0306 16:32:43.098131  3400 slave.cpp:3773] Container '4bf9132d-9a57-4baa-a78c-e7164e93ace6' for executor 'just-a-test' of framework 4f055c6f-1bea-4460-839c-838da86af34c-0
007 failed to start: Collect failed: Unexpected HTTP response '401 Unauthorized
<code>

curl command executed:

<code>
$ sudo sysdig -A -p ""*%evt.time %proc.cmdline"" evt.type=execve and proc.name=curl                                                                   16:42:53.198998042 curl -s -S -L -D - https://registry-1.docker.io:443/v2/alpine/manifests/latest
16:42:53.784958541 curl -s -S -L -D - https://auth.docker.io/token?service=registry.docker.io&amp;scope;=repository:alpine:pull
16:42:54.294192024 curl -s -S -L -D - -H Authorization: Bearer eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCIsIng1YyI6WyJNSUlDTHpDQ0FkU2dBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakJHTVVRd1FnWURWUVFERXp0Uk5Gb3pPa2RYTjBrNldGUlFSRHBJVFRSUk9rOVVWRmc2TmtGRlF6cFNUVE5ET2tGU01rTTZUMFkzTnpwQ1ZrVkJPa2xHUlVrNlExazFTekFlRncweE5UQTJNalV4T1RVMU5EWmFGdzB4TmpBMk1qUXhPVFUxTkRaYU1FWXhSREJDQmdOVkJBTVRPMGhHU1UwNldGZFZWam8yUVZkSU9sWlpUVEk2TTFnMVREcFNWREkxT2s5VFNrbzZTMVExUmpwWVRsSklPbFJMTmtnNlMxUkxOanBCUVV0VU1Ga3dFd1lIS29aSXpqMENBUVlJS29aSXpqMERBUWNEUWdBRXl2UzIvdEI3T3JlMkVxcGRDeFdtS1NqV1N2VmJ2TWUrWGVFTUNVMDByQjI0akNiUVhreFdmOSs0MUxQMlZNQ29BK0RMRkIwVjBGZGdwajlOWU5rL2pxT0JzakNCcnpBT0JnTlZIUThCQWY4RUJBTUNBSUF3RHdZRFZSMGxCQWd3QmdZRVZSMGxBREJFQmdOVkhRNEVQUVE3U0VaSlRUcFlWMVZXT2paQlYwZzZWbGxOTWpveldEVk1PbEpVTWpVNlQxTktTanBMVkRWR09saE9Va2c2VkVzMlNEcExWRXMyT2tGQlMxUXdSZ1lEVlIwakJEOHdQWUE3VVRSYU16cEhWemRKT2xoVVVFUTZTRTAwVVRwUFZGUllPalpCUlVNNlVrMHpRenBCVWpKRE9rOUdOemM2UWxaRlFUcEpSa1ZKT2tOWk5Vc3dDZ1lJS29aSXpqMEVBd0lEU1FBd1JnSWhBTXZiT2h4cHhrTktqSDRhMFBNS0lFdXRmTjZtRDFvMWs4ZEJOVGxuWVFudkFpRUF0YVJGSGJSR2o4ZlVSSzZ4UVJHRURvQm1ZZ3dZelR3Z3BMaGJBZzNOUmFvPSJdfQ.eyJhY2Nlc3MiOltdLCJhdWQiOiJyZWdpc3RyeS5kb2NrZXIuaW8iLCJleHAiOjE0NTcyODI4NzQsImlhdCI6MTQ1NzI4MjU3NCwiaXNzIjoiYXV0aC5kb2NrZXIuaW8iLCJqdGkiOiJaOGtyNXZXNEJMWkNIRS1IcVJIaCIsIm5iZiI6MTQ1NzI4MjU3NCwic3ViIjoiIn0.C2wtJq_P-m0buPARhmQjDfh6ztIAhcvgN3tfWIZEClSgXlVQ_sAQXAALNZKwAQL2Chj7NpHX--0GW-aeL_28Aw https://registry-1.docker.io:443/v2/alpine/manifests/latest
<code>

Also got the same result with {{ubuntu}} docker image.</code></code></code></code></code></code></code></code></code></code>",1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4888,2.0,"Default cmd is executed as an incorrect command. When mesos containerizer launch a container using a docker image, which only container default Cmd. The executable command is is a incorrect sequence. For example:

If an image default entrypoint is null, cmd is ""sh"", user defines shell=false, value is none, and arguments as [-c, echo 'hello world']. The executable command is `[sh, -c, echo 'hello world', sh]`, which is incorrect. It should be `[sh, sh, -c, echo 'hello world']` instead.

This problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. ",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4889,5.0,"Implement runtime isolator tests. There different cases in docker runtime isolator. Some special cases should be tested with unique test case, to verify the docker runtime isolator logic is correct.",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4891,8.0,"Add a '/containers' endpoint to the agent to list all the active containers. This endpoint will be similar to /monitor/statistics.json endpoint, but it'll also contain the 'container_status' about the container (see ContainerStatus in mesos.proto). We'll eventually deprecate the /monitor/statistics.json endpoint.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0
MESOS-4903,3.0,"Allow multiple loads of module manifests The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure.",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4912,3.0,"LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails. Observed on our CI:
{noformat}
[09:34:15] :	 [Step 11/11] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.906719  2357 linux.cpp:81] Making '/tmp/MLVLnv' a shared mount
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.923548  2357 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.924705  2376 containerizer.cpp:666] Starting container 'da610f7f-a709-4de8-94d3-74f4a520619b' for executor 'test_executor1' of framework ''
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.925355  2371 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.925881  2377 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image1' to rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.835127  2376 linux.cpp:355] Bind mounting work directory from '/tmp/MLVLnv/slaves/test_slave/frameworks/executors/test_executor1/runs/da610f7f-a709-4de8-94d3-74f4a520619b' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.835392  2376 linux.cpp:683] Changing the ownership of the persistent volume at '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' with uid 0 and gid 0
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.840425  2376 linux.cpp:723] Mounting '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for persistent volume disk(test_role)[persistent_volume_id:volume]:32 of container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.843878  2374 linux_launcher.cpp:304] Cloning child process with flags = CLONE_NEWNS
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848302  2371 containerizer.cpp:666] Starting container 'fe4729c5-1e63-4cc6-a2e3-fe5006ffe087' for executor 'test_executor2' of framework ''
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848758  2371 containerizer.cpp:1392] Destroying container 'da610f7f-a709-4de8-94d3-74f4a520619b'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848865  2373 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917' for container fe4729c5-1e63-4cc6-a2e3-fe5006ffe087
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.849449  2375 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image2' to rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.854038  2374 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.856693  2372 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2.608128ms
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.859237  2377 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.861454  2377 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2176us
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.934608  2378 containerizer.cpp:1608] Executor for container 'da610f7f-a709-4de8-94d3-74f4a520619b' has exited
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.937692  2372 linux.cpp:798] Unmounting volume '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.937742  2372 linux.cpp:817] Unmounting sandbox/work directory '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.938129  2375 provisioner.cpp:330] Destroying container rootfs at '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:45] :	 [Step 11/11] ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1318: Failure
[09:34:45] :	 [Step 11/11] Failed to wait 15secs for wait1
[09:34:48] :	 [Step 11/11] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers (32341 ms)
{noformat}",,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4937,5.0,"Investigate container security options for Mesos containerizer We should investigate the following to improve the container security for Mesos containerizer and come up with a list of features that we want to support in MVP.

1) Capabilities
2) User namespace
3) Seccomp
4) SELinux
5) AppArmor

We should investigate what other container systems are doing regarding security:
1) [k8s| https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/types.go#L2905]
2) [docker|https://docs.docker.com/engine/security/security/]
3) [oci|https://github.com/opencontainers/specs/blob/master/config.md]",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4942,2.0,"Docker runtime isolator tests may cause disk issue. Currently slave working directory is used as docker store dir and archive dir, which is problematic. Because slave work dir is exactly `environment-&gt;mkdtemp()`, it will get cleaned up until the end of the whole test. But the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. Cleanup has to be done by each test tear down. ",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4944,5.0,"Improve overlay backend so that it's writable Currently, the overlay backend will provision a read-only FS. We can use an empty directory from the container sandbox to act as the upper layer so that it's writable.",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4978,3.0,Update mesos-execute with Appc changes. mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier.,,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-4985,3.0,"Destroy a container while it's provisioning can lead to leaked provisioned directories. Here is the possible sequence of events:
1) containerizer-&gt;launch
2) provisioner-&gt;provision is called. it is fetching the image
3) executor registration timed out
4) containerizer-&gt;destroy is called
5) container-&gt;state is still in PREPARING
6) provisioner-&gt;destroy is called

So we can be calling provisioner-&gt;destory while provisioner-&gt;provision hasn't finished yet. provisioner-&gt;destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes.",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-5004,1.0,"Clarify docs on '/reserve' and '/create-volumes' without authentication For both reservations and persistent volume creation, the behavior of the HTTP endpoints differs slightly from that of the framework operations. Due to the implementation of HTTP authentication, it is not possible for a framework/operator to provide a principal when HTTP authentication is disabled. This means that when HTTP authentication is disabled, the endpoint handlers will _always_ receive {{None()}} as the principal associated with the request, and thus if authorization is enabled, the request will only succeed if the NONE principal is authorized to do stuff.

The docs should be updated to explain this behavior explicitly.",,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-5044,1.0,"Temporary directories created by environment->mkdtemp cleanup can be problematic. Currently in mesos test, we have the temporary directories created by `environment-&gt;mkdtemp()` cleaned up until the end of the test suite, which can be problematic. For instance, if we have many tests in a test suite, each of those tests is performing large size disk read/write in its temp dir, which may lead to out of disk issue on some resource limited machines. 

We should have these temp dir created by `environment-&gt;mkdtemp` cleaned up during each test teardown. Currently we only clean up the sandbox for each test.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0
MESOS-5062,3.0,"Update the long-lived-framework example to run on test clusters There are a couple of problems with the long-lived framework that prevent it from being deployed (easily) on an actual cluster:

* The framework will greedily accept all offers; it runs one executor per agent in the cluster.
* The framework assumes the {{long-lived-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments.
* The framework does not specify an resources with the executor.  This is required by many isolators.
* The framework has no metrics.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
MESOS-5109,2.0,"Capture the error code in `ErrnoError` and `WindowsError`. The {{ErrnoError}} and {{WindowsError}} classes simply construct the error string via a mechanism such as {{strerror}}. They should also capture the error code, as it is an essential piece of information for such an error type.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-5111,2.0,"Update `network::connect` to use the typed error state of `Try`. {{network::connect}} function returns a {{Try<int>}} currently and the caller is required to inspect the state of {{errno}} out-of-band. {{network::connect}} should really return something like a {{Try<int, errnoerror="""">}}.</int,></int>",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-5112,2.0,"Introduce `WindowsSocketError`. {{WindowsError}} invokes {{::GetLastError}} to retrieve the error code. Windows has a {{::WSAGetLastError}} function which at the interface level, is intended for failed socket operations. We should introduce a {{WindowsSocketError}} which invokes {{::WSAGetLastError}} and use them accordingly.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0
MESOS-5113,1.0,"`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump:
0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff23280d8 in __GI_abort () at abort.c:89
#2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x451f5c ""isSome()"",
    file=file@entry=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=line@entry=111,
    function=function@entry=0x45294a ""const T &amp;Option;<std::basic_string<char> &gt;::get() const &amp; [T = std::basic_string<char>]"") at assert.c:92
#3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c ""isSome()"", file=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=111,
    function=0x45294a ""const T &amp;Option;<std::basic_string<char> &gt;::get() const &amp; [T = std::basic_string<char>]"") at assert.c:101
#4  0x0000000000432c0d in Option<std::string>::get() const &amp; (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111
Python Exception <class 'indexerror'=""""> list index out of range:
#5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331
#6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239
#7  0x00007ffff60cd972 in std::_Function_handler<void (process::processbase*),="""" process::future<nothing=""""> process::dispatch<nothing, mesos::internal::slave::mesosisolatorprocess,="""" std::allocator<mesos::slave::containerstate="""" std::list<mesos::slave::containerstate,=""""> &gt; const&amp;, hashset<mesos::containerid, std::hash<mesos::containerid="""">, std::equal_to<mesos::containerid> &gt; const&amp;, std::list<mesos::slave::containerstate, std::allocator<mesos::slave::containerstate=""""> &gt;, hashset<mesos::containerid, std::hash<mesos::containerid="""">, std::equal_to<mesos::containerid> &gt; &gt;(process::PID<mesos::internal::slave::mesosisolatorprocess> const&amp;, process::Future<nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::containerstate, std::allocator<mesos::slave::containerstate=""""> &gt; const&amp;, hashset<mesos::containerid, std::hash<mesos::containerid="""">, std::equal_to<mesos::containerid> &gt; const&amp;), std::list<mesos::slave::containerstate, std::allocator<mesos::slave::containerstate=""""> &gt;, hashset<mesos::containerid, std::hash<mesos::containerid="""">, std::equal_to<mesos::containerid> &gt;)::{lambda(process::ProcessBase*)#1}&gt;::_M_invoke(std::_Any_data const&amp;, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071
#8  0x00007ffff6a6bf38 in std::function<void (process::processbase*)="""">::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471
#9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130
#10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161
#11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82
#12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570
#13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218
#14 0x00007ffff6a5bf33 in std::_Bind<process::processmanager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool="""" const="""">)&gt;::__call<void, ,="""" 0ul="""">(std::tuple&lt;&gt;&amp;&amp;, std::_Index_tuple&lt;0ul&gt;) (this=0x697d70,
    __args=<unknown .libs="""" 0x45bb552,="""" 0x469efe5="""" build="""" cu="""" die="""" home="""" in="""" libmesos-0.29.0.so,="""" mesos="""" mesosphere="""" src="""" type="""" vagrant="""">) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295
#15 0x00007ffff6a5bee6 in std::_Bind<process::processmanager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool="""" const="""">)&gt;::operator()&lt;, void&gt;() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353
#16 0x00007ffff6a5be95 in std::_Bind_simple<std::_bind<process::processmanager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool="""" const="""">)&gt; ()&gt;::_M_invoke&lt;&gt;(std::_Index_tuple&lt;&gt;) (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731
#17 0x00007ffff6a5be65 in std::_Bind_simple<std::_bind<process::processmanager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool="""" const="""">)&gt; ()&gt;::operator()() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720
#18 0x00007ffff6a5be3c in std::thread::_Impl<std::_bind_simple<std::_bind<process::processmanager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool="""" const="""">)&gt; ()&gt; &gt;::_M_run() (this=0x697d58)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115
#19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312
#21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) frame 4
#4  0x0000000000432c0d in Option<std::string>::get() const &amp; (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111</std::string></std::_bind_simple<std::_bind<process::processmanager::init_threads()::$_1></std::_bind<process::processmanager::init_threads()::$_1></std::_bind<process::processmanager::init_threads()::$_1></process::processmanager::init_threads()::$_1></unknown></void,></process::processmanager::init_threads()::$_1></void></mesos::containerid></mesos::containerid,></mesos::slave::containerstate,></mesos::containerid></mesos::containerid,></mesos::slave::containerstate,></nothing></mesos::internal::slave::mesosisolatorprocess></mesos::containerid></mesos::containerid,></mesos::slave::containerstate,></mesos::containerid></mesos::containerid,></nothing,></void></class></std::string></char></std::basic_string<char></char></std::basic_string<char>",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-5127,1.0,"Reset `LIBPROCESS_IP` in `network\cni` isolator. Currently the `LIBPROCESS_IP` environment variable was being set to
    the Agent IP if the environment variable has not be defined by the
    `Framework`. For containers having their own IP address (as with
    containers on CNI networks) this becomes a problem since the command
    executor tries to bind to the `LIBPROCESS_IP` that does not exist in
    its network namespace, and fails. Thus, for containers launched on CNI
    networks the `LIBPROCESS_IP` should not be set, or rather is set to
    ""0.0.0.0"", allowing the container to bind to the IP address provided
    by the CNI network.",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-5128,3.0,"PersistentVolumeTest.AccessPersistentVolume is flaky Observed on ASF CI:

","<code>
[ RUN      ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0
I0405 17:29:19.134435 31837 cluster.cpp:139] Creating default 'local' authorizer
I0405 17:29:19.251143 31837 leveldb.cpp:174] Opened db in 116.386403ms
I0405 17:29:19.310050 31837 leveldb.cpp:181] Compacted db in 58.80688ms
I0405 17:29:19.310180 31837 leveldb.cpp:196] Created db iterator in 37145ns
I0405 17:29:19.310199 31837 leveldb.cpp:202] Seeked to beginning of db in 4212ns
I0405 17:29:19.310210 31837 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0405 17:29:19.310279 31837 replica.cpp:779] Replica recovered with log positions 0 -&gt; 0 with 1 holes and 0 unlearned
I0405 17:29:19.311069 31861 recover.cpp:447] Starting replica recovery
I0405 17:29:19.311362 31861 recover.cpp:473] Replica is in EMPTY status
I0405 17:29:19.312641 31861 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14359)@172.17.0.4:43972
I0405 17:29:19.313045 31860 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0405 17:29:19.313608 31860 recover.cpp:564] Updating replica status to STARTING
I0405 17:29:19.316416 31867 master.cpp:376] Master 9565ff6f-f1b6-4259-8430-690e635c391f (4090d10eba90) started on 172.17.0.4:43972
I0405 17:29:19.316470 31867 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/0A9ELu/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/0A9ELu/master"" --zk_session_timeout=""10secs""
I0405 17:29:19.316938 31867 master.cpp:427] Master only allowing authenticated frameworks to register
I0405 17:29:19.316951 31867 master.cpp:432] Master only allowing authenticated agents to register
I0405 17:29:19.316961 31867 credentials.hpp:37] Loading credentials for authentication from '/tmp/0A9ELu/credentials'
I0405 17:29:19.317402 31867 master.cpp:474] Using default 'crammd5' authenticator
I0405 17:29:19.317643 31867 master.cpp:545] Using default 'basic' HTTP authenticator
I0405 17:29:19.317854 31867 master.cpp:583] Authorization enabled
I0405 17:29:19.318081 31864 whitelist_watcher.cpp:77] No whitelist given
I0405 17:29:19.318079 31861 hierarchical.cpp:144] Initialized hierarchical allocator process
I0405 17:29:19.320838 31864 master.cpp:1826] The newly elected leader is master@172.17.0.4:43972 with id 9565ff6f-f1b6-4259-8430-690e635c391f
I0405 17:29:19.320888 31864 master.cpp:1839] Elected as the leading master!
I0405 17:29:19.320909 31864 master.cpp:1526] Recovering from registrar
I0405 17:29:19.321218 31871 registrar.cpp:331] Recovering registrar
I0405 17:29:19.347045 31860 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 33.164133ms
I0405 17:29:19.347126 31860 replica.cpp:320] Persisted replica status to STARTING
I0405 17:29:19.347611 31869 recover.cpp:473] Replica is in STARTING status
I0405 17:29:19.349215 31871 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14361)@172.17.0.4:43972
I0405 17:29:19.349653 31870 recover.cpp:193] Received a recover response from a replica in STARTING status
I0405 17:29:19.350236 31866 recover.cpp:564] Updating replica status to VOTING
I0405 17:29:19.388882 31864 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.38299ms
I0405 17:29:19.388993 31864 replica.cpp:320] Persisted replica status to VOTING
I0405 17:29:19.389369 31856 recover.cpp:578] Successfully joined the Paxos group
I0405 17:29:19.389735 31856 recover.cpp:462] Recover process terminated
I0405 17:29:19.390476 31868 log.cpp:659] Attempting to start the writer
I0405 17:29:19.392125 31862 replica.cpp:493] Replica received implicit promise request from (14362)@172.17.0.4:43972 with proposal 1
I0405 17:29:19.430706 31862 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.505062ms
I0405 17:29:19.430816 31862 replica.cpp:342] Persisted promised to 1
I0405 17:29:19.431918 31856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0405 17:29:19.433725 31861 replica.cpp:388] Replica received explicit promise request from (14363)@172.17.0.4:43972 for position 0 with proposal 2
I0405 17:29:19.472491 31861 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 38.659492ms
I0405 17:29:19.472595 31861 replica.cpp:712] Persisted action at 0
I0405 17:29:19.474556 31864 replica.cpp:537] Replica received write request for position 0 from (14364)@172.17.0.4:43972
I0405 17:29:19.474652 31864 leveldb.cpp:436] Reading position from leveldb took 49423ns
I0405 17:29:19.528175 31864 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 53.443616ms
I0405 17:29:19.528300 31864 replica.cpp:712] Persisted action at 0
I0405 17:29:19.529389 31865 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0405 17:29:19.571137 31865 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 41.676495ms
I0405 17:29:19.571254 31865 replica.cpp:712] Persisted action at 0
I0405 17:29:19.571302 31865 replica.cpp:697] Replica learned NOP action at position 0
I0405 17:29:19.572322 31856 log.cpp:675] Writer started with ending position 0
I0405 17:29:19.574060 31861 leveldb.cpp:436] Reading position from leveldb took 83200ns
I0405 17:29:19.575417 31864 registrar.cpp:364] Successfully fetched the registry (0B) in 0ns
I0405 17:29:19.575565 31864 registrar.cpp:463] Applied 1 operations in 46419ns; attempting to update the 'registry'
I0405 17:29:19.576517 31857 log.cpp:683] Attempting to append 170 bytes to the log
I0405 17:29:19.576849 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0405 17:29:19.578390 31857 replica.cpp:537] Replica received write request for position 1 from (14365)@172.17.0.4:43972
I0405 17:29:19.780277 31857 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 201.808617ms
I0405 17:29:19.780366 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.782024 31857 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0405 17:29:19.823770 31857 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 41.667662ms
I0405 17:29:19.823851 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.823889 31857 replica.cpp:697] Replica learned APPEND action at position 1
I0405 17:29:19.825701 31867 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:19.825929 31867 registrar.cpp:394] Successfully recovered registrar
I0405 17:29:19.826015 31857 log.cpp:702] Attempting to truncate the log to 1
I0405 17:29:19.826262 31867 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0405 17:29:19.827647 31867 replica.cpp:537] Replica received write request for position 2 from (14366)@172.17.0.4:43972
I0405 17:29:19.828018 31857 master.cpp:1634] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0405 17:29:19.828065 31861 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0405 17:29:19.865555 31867 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.822178ms
I0405 17:29:19.865661 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.866921 31867 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0405 17:29:19.907341 31867 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.356649ms
I0405 17:29:19.907531 31867 leveldb.cpp:399] Deleting ~1 keys from leveldb took 91109ns
I0405 17:29:19.907560 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.907599 31867 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0405 17:29:19.923305 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:2048
Trying semicolon-delimited string format instead
I0405 17:29:19.926491 31837 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0405 17:29:19.927836 31837 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0405 17:29:19.932029 31862 slave.cpp:200] Agent started on 441)@172.17.0.4:43972
I0405 17:29:19.932086 31862 slave.cpp:201] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":2048.0},""type"":""SCALAR""},{""name"":""disk"",""role"":""role1"",""scalar"":{""value"":4096.0},""type"":""SCALAR""}]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC""
I0405 17:29:19.932665 31862 credentials.hpp:86] Loading credential for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential'
I0405 17:29:19.932934 31862 slave.cpp:338] Agent using credential for: test-principal
I0405 17:29:19.932968 31862 credentials.hpp:37] Loading credentials for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials'
I0405 17:29:19.933284 31862 slave.cpp:390] Using default 'basic' HTTP authenticator
I0405 17:29:19.934916 31837 sched.cpp:222] Version: 0.29.0
I0405 17:29:19.935566 31862 slave.cpp:589] Agent resources: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:19.935664 31862 slave.cpp:597] Agent attributes: [  ]
I0405 17:29:19.935679 31862 slave.cpp:602] Agent hostname: 4090d10eba90
I0405 17:29:19.938390 31864 state.cpp:57] Recovering state from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta'
I0405 17:29:19.940608 31869 sched.cpp:326] New master detected at master@172.17.0.4:43972
I0405 17:29:19.940749 31869 sched.cpp:382] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.940773 31869 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0405 17:29:19.942371 31869 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.942873 31859 master.cpp:5679] Authenticating scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.943156 31859 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.943507 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.943740 31859 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.943783 31859 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.943892 31859 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.943977 31859 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.944066 31859 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.944164 31859 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.944193 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.944206 31859 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.944268 31859 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.944300 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.944313 31859 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944321 31859 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944339 31859 authenticator.cpp:317] Authentication success
I0405 17:29:19.944541 31859 authenticatee.cpp:298] Authentication success
I0405 17:29:19.944655 31859 master.cpp:5709] Successfully authenticated principal 'test-principal' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.944737 31859 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.945111 31859 sched.cpp:472] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.945132 31859 sched.cpp:777] Sending SUBSCRIBE call to master@172.17.0.4:43972
I0405 17:29:19.945591 31859 sched.cpp:810] Will retry registration in 372.80738ms if necessary
I0405 17:29:19.945744 31865 master.cpp:2346] Received SUBSCRIBE call for framework 'default' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.945838 31865 master.cpp:1865] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0405 17:29:19.946194 31865 master.cpp:2417] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0405 17:29:19.946866 31866 hierarchical.cpp:266] Added framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.946974 31866 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:19.947010 31866 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:19.947054 31865 sched.cpp:704] Framework registered with 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.947074 31866 hierarchical.cpp:1141] Performed allocation for 0 agents in 178242ns
I0405 17:29:19.947124 31865 sched.cpp:718] Scheduler::registered took 38907ns
I0405 17:29:19.948712 31866 status_update_manager.cpp:200] Recovering status update manager
I0405 17:29:19.948901 31866 containerizer.cpp:416] Recovering containerizer
I0405 17:29:19.951021 31866 provisioner.cpp:245] Provisioner recovery complete
I0405 17:29:19.951802 31866 slave.cpp:4773] Finished recovery
I0405 17:29:19.952518 31866 slave.cpp:4945] Querying resource estimator for oversubscribable resources
I0405 17:29:19.953248 31866 slave.cpp:928] New master detected at master@172.17.0.4:43972
I0405 17:29:19.953305 31865 status_update_manager.cpp:174] Pausing sending status updates
I0405 17:29:19.953626 31866 slave.cpp:991] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.953716 31866 slave.cpp:996] Using default CRAM-MD5 authenticatee
I0405 17:29:19.954074 31866 slave.cpp:964] Detecting new master
I0405 17:29:19.954167 31861 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.954372 31866 slave.cpp:4959] Received oversubscribable resources  from the resource estimator
I0405 17:29:19.954756 31866 master.cpp:5679] Authenticating slave(441)@172.17.0.4:43972
I0405 17:29:19.954944 31861 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.955368 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.955687 31861 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.955801 31861 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.956075 31861 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.956279 31861 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.956455 31861 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.956676 31861 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.956815 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.956907 31861 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.957044 31861 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.957166 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.957264 31861 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957353 31861 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957449 31861 authenticator.cpp:317] Authentication success
I0405 17:29:19.957664 31857 authenticatee.cpp:298] Authentication success
I0405 17:29:19.957813 31857 master.cpp:5709] Successfully authenticated principal 'test-principal' at slave(441)@172.17.0.4:43972
I0405 17:29:19.958008 31861 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.958732 31857 slave.cpp:1061] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.958930 31857 slave.cpp:1457] Will retry registration in 18.568334ms if necessary
I0405 17:29:19.959262 31857 master.cpp:4390] Registering agent at slave(441)@172.17.0.4:43972 (4090d10eba90) with id 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:19.959934 31857 registrar.cpp:463] Applied 1 operations in 99197ns; attempting to update the 'registry'
I0405 17:29:19.961587 31857 log.cpp:683] Attempting to append 343 bytes to the log
I0405 17:29:19.961879 31857 coordina",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0
MESOS-5152,2.0,"Add authentication to agent's /monitor/statistics endpoint Operators may want to enforce that only authenticated users (and subsequently only specific authorized users) be able to view per-executor resource usage statistics.
Since this endpoint is handled by the ResourceMonitorProcess, I would expect the work necessary to be similar to what was done for /files or /registry endpoint authn.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0
MESOS-5157,1.0,"Update webui for GPU metrics After adding the GPU metrics and updating the resources JSON to include GPU information, the webui should be updated accordingly.",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
MESOS-5160,1.0,"Make `network/cni` enabled as the default network isolator for `MesosContainerizer`. Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator. ",,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-5171,3.0,"Expose state/state.hpp to public headers We want the Modules to be able to use replicated log along with the APIs to communicate with Zookeeper. This change would require us to expose at least the following headers state/storage.hpp, and any additional files that state.hpp depends on (e.g., zookeeper/authentication.hpp).",,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0
MESOS-5214,2.0,"Populate FrameworkInfo.principal for authenticated frameworks If a framework authenticates and then does not provide a {{principal}} in its {{FrameworkInfo}}, we currently allow this and leave {{FrameworkInfo.principal}} unset. Instead, we should populate {{FrameworkInfo.principal}} for them automatically in that case to ensure that the two principals are equal.",,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0
MESOS-5253,2.0,"Isolator cleanup should not be invoked if they are not prepared yet. If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet. 

In this case, there no need to clean up any isolator, call provisioner destroy directly.",,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
